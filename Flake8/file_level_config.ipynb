{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d91edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\otten\\AppData\\Local\\Temp\\ipykernel_15880\\3860527935.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tokenized_data = torch.load(load_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# the following cell is used to load tokenized data for testing, note that this loads the train test split, not the unsplit data. Unsplit data is used for testing our \n",
    "# code parrot jupyter errors dataset and the jupyter errors dataset. To load the unsplit data, you can uncomment the lines below and comment out the above lines.\n",
    "\n",
    "# To load tokenized data, ensure the path is correct. Tokenizer as well as code to save tokenized content is in the run model file.\n",
    "\n",
    "load_path = \"dataset\\\\tokenized_content\\\\file_name.pt\"\n",
    "\n",
    "tokenized_data = torch.load(load_path)\n",
    "\n",
    "train_ids = tokenized_data['train_ids']\n",
    "test_ids = tokenized_data['test_ids']\n",
    "train_masks = tokenized_data['train_masks']\n",
    "test_masks = tokenized_data['test_masks']\n",
    "train_labels = tokenized_data['train_labels']\n",
    "test_labels = tokenized_data['test_labels']\n",
    "\n",
    "# Uncomment the lines below to load unsplit data\n",
    "# test_ids = tokenized_data['test_ids']\n",
    "# test_masks = tokenized_data['test_masks']\n",
    "# test_labels = tokenized_data['test_labels']\n",
    "\n",
    "print(\"Tokenized data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35034b7",
   "metadata": {},
   "source": [
    "The following cell is used to configure Flake8 for file level bug detection in Jupyter Notebooks. It works by decoding tokenized content into single Python files removing special tokens. It then uses Flake8 on the Python file to make a prediction on whether or not the file contains a bug. Errors we used in our Flake8 configuration were selected to reduce false positives in bug detection avoiding things such as stylistic recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28adf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import subprocess\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import re\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "\n",
    "# tokenizer setup for decoding\n",
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "\n",
    "# setting up the special tokens use for finding cell boundaries in tokenized content\n",
    "start_special_tokens = [f\"<CELL_{i}>\" for i in range(1, 1024)]\n",
    "end_special_tokens = [f\"<END_CELL_{i}>\" for i in range(1, 1024)]\n",
    "all_special_tokens = start_special_tokens + end_special_tokens\n",
    "\n",
    "# Add tokens if not already in the vocabulary.\n",
    "for token in all_special_tokens:\n",
    "    if token not in tokenizer.get_vocab():\n",
    "        tokenizer.add_tokens([token])\n",
    "\n",
    "correctnessOfPredictions = [] # holds true or false for each prediction, true if the prediction is correct, false otherwise.\n",
    "\n",
    "\n",
    "#### starting decoding notebooks\n",
    "flat_codes, flat_labels = [], []\n",
    "for chunks_ids, chunks_masks, chunk_label_lists in tqdm(\n",
    "    zip(test_ids, test_masks, test_labels),\n",
    "    total=len(test_ids),\n",
    "    desc=\"Decoding & cleaning notebooks\",\n",
    "    dynamic_ncols=True,\n",
    "):\n",
    "    file_ids = chunks_ids[:4] # use same chunk size as JupOtter-base and JupOtter-small\n",
    "    is_buggy = int(any((lbls == 1).any().item() for lbls in chunk_label_lists[:4]))\n",
    "    flat_list = file_ids.reshape(-1).tolist()\n",
    "    decoded = tokenizer.decode(flat_list, skip_special_tokens=True)\n",
    "    \n",
    "    # after this is just ensuring special tokens are removed even though we set skip_special_tokens=True\n",
    "    for token in tokenizer.all_special_tokens:\n",
    "        pattern = re.escape(token)\n",
    "        decoded = re.sub(pattern, \"\", decoded)\n",
    "\n",
    "    decoded = re.sub(r\"<CELL_\\d+>\", \"\", decoded)\n",
    "    decoded = re.sub(r\"<END_CELL_\\d+>\", \"\", decoded)\n",
    "    flat_codes.append(decoded)\n",
    "    flat_labels.append(is_buggy)\n",
    "#### end of decoding notebooks\n",
    "\n",
    "results = [] # holds the results of the predictions, each element is a tuple for its notebook (is_buggy, label)\n",
    "\n",
    "tq = tqdm(\n",
    "    enumerate(zip(flat_codes, flat_labels)),\n",
    "    total=len(flat_codes),\n",
    "    desc=\"Static analysis Eval\",\n",
    "    dynamic_ncols=True,\n",
    "    leave=True,\n",
    ")\n",
    "buggy_pred = 0\n",
    "non_buggy_pred = 0\n",
    "skipped = 0\n",
    "for i, (code, label) in tq:\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", encoding=\"utf-8\", delete=False) as tmp_file:\n",
    "        tmp_file.write(code)\n",
    "        tmp_filename = tmp_file.name\n",
    "\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [ \n",
    "                    \"flake8\",\n",
    "                    \"--select=E9,F402,F405,F406,F407,F501,F502,F503,F505,F506,F507,F508,F509,F521,F524,F525,F621,F622,F633,F701,F702,F704,F706,F707,F821,F822,F823,F831,F901\",\n",
    "                    tmp_filename\n",
    "                ],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        encoding='utf-8' \n",
    "    )\n",
    "\n",
    "        is_buggy = 0 if result.returncode == 0 else 1\n",
    "        if is_buggy:\n",
    "            buggy_pred += 1 \n",
    "        else:\n",
    "            non_buggy_pred += 1\n",
    "\n",
    "        if is_buggy != label:\n",
    "            correctnessOfPredictions.append(False)\n",
    "        else:\n",
    "            correctnessOfPredictions.append(True)\n",
    "\n",
    "    except subprocess.TimeoutExpired:\n",
    "        skipped += 1\n",
    "        continue  # skip this file and move on\n",
    "\n",
    "    os.remove(tmp_filename)\n",
    "    results.append((is_buggy, label))\n",
    "\n",
    "    # Live metrics, only used to display progress in the tqdm bar\n",
    "    preds_so_far = [pred for pred, _ in results]\n",
    "    labels_so_far = [true for _, true in results]\n",
    "    f1 = f1_score(labels_so_far, preds_so_far, zero_division=0)\n",
    "    acc = accuracy_score(labels_so_far, preds_so_far)\n",
    "    tq.set_postfix({'F1': f\"{f1:.3f}\", 'Acc': f\"{acc:.3f}\", 'Recall': f\"{recall_score(labels_so_far, preds_so_far, zero_division=0):.3f}\"})\n",
    "    tq.refresh()  \n",
    "\n",
    "\n",
    "# Evaluate\n",
    "correct = sum([pred == true for pred, true in results])\n",
    "total = len(results)\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f\"\\nFile-level Bug Detection via Flake8:\")\n",
    "print(f\"Accuracy: {accuracy:.4f} ({correct}/{total})\")\n",
    "\n",
    "# Split predictions and labels\n",
    "predictions = [pred for pred, _ in results]\n",
    "labels = [true for _, true in results]\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "precision = precision_score(labels, predictions)\n",
    "recall = recall_score(labels, predictions)\n",
    "f1 = f1_score(labels, predictions)\n",
    "\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")\n",
    "print(f\"Skipped {skipped} files due to timeout.\")\n",
    "print(f\"Buggy predictions: {buggy_pred}, Non-buggy predictions: {non_buggy_pred}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
