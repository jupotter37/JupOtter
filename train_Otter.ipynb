{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c9454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\otten\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports used\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm  \n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torch\n",
    "import buggy_cell_vector_evalualtion_clean\n",
    "from torch.utils.data import Dataset\n",
    "import gc\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torch.cuda as cuda\n",
    "import time\n",
    "from transformers import T5EncoderModel, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c9b367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset to wrap notebook data to easily use with DataLoader\n",
    "\n",
    "class NotebookDataset(Dataset):\n",
    "    def __init__(self, all_ids, all_masks, all_labels):\n",
    "        # all_ids is a tensor of chunnks where each row is a chunk of a notebook\n",
    "        self.ids = all_ids\n",
    "        self.masks = all_masks\n",
    "        self.labels = all_labels\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, i): # to get the data for a single book, can be in multiple chunks\n",
    "        return {\n",
    "          \"input_ids\": self.ids[i],           \n",
    "          \"attention_mask\": self.masks[i],    \n",
    "          \"labels\": self.labels[i],         \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5510822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom collate function that creates batches of notebooks.\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    input_ids = ([item['input_ids'] for item in batch])\n",
    "    attention_mask = ([item['attention_mask'] for item in batch])\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    return {'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe228e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section prepares JupOtter for cell-level bug detection in Jupyter notebooks, more in depth documentation avalable in run_model.ipynb.\n",
    "\n",
    "class CodeT5TokenClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_labels=1):\n",
    "        \"\"\"\n",
    "        model_name: e.g., 'Salesforce/codet5-base'\n",
    "        \"\"\"\n",
    "        super(CodeT5TokenClassifier, self).__init__() # get the base encoder model\n",
    "        self.encoder = T5EncoderModel.from_pretrained(model_name)\n",
    "        hidden_size = self.encoder.config.d_model\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)  # intitialize linear laryer that will map hidden states to a single logit\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, start_token_ids, end_token_ids, labels=None, calc_loss=1):\n",
    "        \"\"\"\n",
    "        Loss calculation:\n",
    "        calc_loss: 0 for no loss, 1 for chunk weighted binary cross entropy loss, 2 for cell weighted binary cross entropy loss.\n",
    "        \"\"\"\n",
    "        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask) #getting the hidden states\n",
    "        hidden_states = encoder_outputs.last_hidden_state  # getting the last hidden state\n",
    "\n",
    "        # Create a mask to identify positions of the target tokens, one for the start tokens,\n",
    "        # one for the end token\n",
    "        start_mask = torch.zeros_like(input_ids, dtype=torch.bool)\n",
    "        end_mask = torch.zeros_like(input_ids, dtype=torch.bool)\n",
    "        for token_id in start_token_ids:\n",
    "            start_mask |= (input_ids == token_id) # getting all of the start tokens present in the code\n",
    "        for token_id in end_token_ids:\n",
    "            end_mask |= (input_ids == token_id) # getting all of the end tokens presentin in the code\n",
    "        \n",
    "        # will hold the prediction vector of each chunk\n",
    "        logits_list = []\n",
    "\n",
    "        for i in range(hidden_states.size(0)):  # iterate over hidden states for each sample in the batch\n",
    "            hs = hidden_states[i]\n",
    "\n",
    "             # Find positions of start and end tokens in this example\n",
    "            start_positions = (start_mask[i]).nonzero(as_tuple=True)[0]\n",
    "            end_positions = (end_mask[i]).nonzero(as_tuple=True)[0]\n",
    "            \n",
    "           \n",
    "            cell_logits = []  # one logit per cell, stores logits for cells in this chunk\n",
    "            for start_token_pos, end_token_pos in zip(start_positions, end_positions):\n",
    "\n",
    "                cell_hidden_state = hs[start_token_pos:end_token_pos+1]  # getting the hidden state between the special token bounds\n",
    "                cell_rep = cell_hidden_state.mean(dim=0)  # this averages the array of tokens into a vector where each entrie is the average of the features in a token.\n",
    "                logit = self.classifier(cell_rep)  # using the classifier on the vector of averaged tokens\n",
    "                cell_logits.append(logit)\n",
    "\n",
    "            if cell_logits: # for if logits generated for the sample\n",
    "                logits_list.append(torch.stack(cell_logits)) # add logit to logits list\n",
    "    \n",
    "            else:\n",
    "                # if no cell pairs are found, append an empty tensor \n",
    "                logits_list.append(torch.empty(0, self.classifier.out_features, device=hs.device))\n",
    "\n",
    "        # starting loss calculation\n",
    "        loss = None\n",
    "        if labels is None or calc_loss == 0: # if no labels are provided or we do not want to calculate loss\n",
    "            return {\"logits\": logits_list}\n",
    "        elif calc_loss == 1:  # if labels are provided and we want to calculate loss chunk weighted binary cross entropy loss\n",
    "            loss_fct = nn.BCEWithLogitsLoss() # useing binary cross entorphy loss, this is what the paper this idea was based on uses\n",
    "            losses = []\n",
    "            # loop over each examples logits and corresponding labels\n",
    "            for logits, lbl in zip(logits_list, labels):\n",
    "                if len(logits) != len(lbl):\n",
    "                    lbl = lbl[:len(logits)]  # trim lbl to match logits length\n",
    "                    print(f\"Trimmed lbl to match logits length: {len(lbl)}\")\n",
    "                logits = logits.squeeze(-1)\n",
    "                if logits.numel() > 0:  # only calculate loss if logits are not empty\n",
    "                    losses.append(loss_fct(logits, lbl.float()))  # calculate loss\n",
    "\n",
    "                    \n",
    "            if losses:\n",
    "                loss = torch.stack(losses).mean()\n",
    "            return {\"loss\": loss, \"logits\": logits_list}\n",
    "        \n",
    "        elif calc_loss == 2:  # to calculate loss cell weighted binary cross entropy loss\n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "            list_logits = torch.cat(logits_list).squeeze(1)\n",
    "            list_lbl = torch.cat(labels).float()\n",
    "\n",
    "            if len(list_logits) != len(list_lbl): # trim if lenghths do not match\n",
    "                print(f\"Trimming labels from {len(list_lbl)} to {len(list_logits)}\")\n",
    "                list_lbl = list_lbl[:len(list_logits)]\n",
    "\n",
    "                # calculate the loss and scale it by the batch size\n",
    "            loss = loss_fct(list_logits, list_lbl)\n",
    "\n",
    "            return {\"loss\": loss, \"logits\": logits_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91edb99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Model loading and special token setup\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "\n",
    "\n",
    "# setting up the special tokens use\n",
    "start_special_tokens = [f\"<CELL_{i}>\" for i in range(1, 1024)]\n",
    "end_special_tokens = [f\"<END_CELL_{i}>\" for i in range(1, 1024)]\n",
    "all_special_tokens = start_special_tokens + end_special_tokens\n",
    "\n",
    "# Add tokens if not already in the vocabulary.\n",
    "for token in all_special_tokens:\n",
    "    if token not in tokenizer.get_vocab():\n",
    "        tokenizer.add_tokens([token])\n",
    "\n",
    "# Get token IDs\n",
    "start_token_ids = [tokenizer.convert_tokens_to_ids(token) for token in start_special_tokens]\n",
    "end_token_ids = [tokenizer.convert_tokens_to_ids(token) for token in end_special_tokens]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Instantiate model and resize embeddings to account for new tokens.\n",
    "model = CodeT5TokenClassifier('Salesforce/codet5-base').to(device)\n",
    "\n",
    "model.encoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2995086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\otten\\AppData\\Local\\Temp\\ipykernel_15764\\1024729650.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  tokenized_data = torch.load(load_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# To load tokenized data, ensure the path is correct. Tokenizer as well as code to save tokenized content is in the run model file.\n",
    "\n",
    "load_path = \"dataset\\\\tokenized_content\\\\name_of_file.pt\"\n",
    "\n",
    "tokenized_data = torch.load(load_path)\n",
    "\n",
    "train_ids = tokenized_data['train_ids']\n",
    "test_ids = tokenized_data['test_ids']\n",
    "train_masks = tokenized_data['train_masks']\n",
    "test_masks = tokenized_data['test_masks']\n",
    "train_labels = tokenized_data['train_labels']\n",
    "test_labels = tokenized_data['test_labels']\n",
    "\n",
    "print(\"Tokenized data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f7599c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Starting training\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/4113 [00:24<28:05:53, 24.60s/it, loss=0.619]"
     ]
    }
   ],
   "source": [
    "# Training loop setup\n",
    "\n",
    "# Set checkpoint directory\n",
    "checkpoint_dir = \"models\\\\checkpoints\"\n",
    "\n",
    "   \n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "train_dataset = NotebookDataset(train_ids, train_masks, train_labels)\n",
    "test_dataset = NotebookDataset(test_ids, test_masks, test_labels)\n",
    "\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "model_tester = buggy_cell_vector_evalualtion_clean.VectorEval()\n",
    "\n",
    "print(\"---------------------- BASE LINE ----------------------\")\n",
    "model_tester.eval_vector_batched(test_loader, model, start_token_ids, end_token_ids, device, chunk_size=4)\n",
    "model_tester.print_results()\n",
    "model_tester.reset()\n",
    "\n",
    "scaler = GradScaler()\n",
    "epochs = 10\n",
    "loss_fct = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(\"Starting training\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    train_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    train_start = time.time()\n",
    "\n",
    "    for batch in train_bar: # looping through the batches of notebooks\n",
    "\n",
    "        input_ids_batch = batch['input_ids']\n",
    "        attention_mask_batch = batch['attention_mask']\n",
    "        labels_batch = batch['labels']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss = 0.0 # initialize the loss for this batch\n",
    "\n",
    "        # going thorugh each notebook in the batch\n",
    "        for batch_ids, batch_masks, batch_labels in zip(input_ids_batch, attention_mask_batch, labels_batch):\n",
    "            # get first n chunks in the notebook, we use chunk size of 4\n",
    "            batch_ids = batch_ids[:4].to(device)\n",
    "            batch_masks = batch_masks[:4].to(device)\n",
    "            batch_labels = [lbl.to(device) for lbl in batch_labels[:4]]\n",
    "\n",
    "            # Use autocast for mixed precision training\n",
    "            with autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(\n",
    "                    input_ids=batch_ids,\n",
    "                    attention_mask=batch_masks,\n",
    "                    start_token_ids=start_token_ids,\n",
    "                    end_token_ids=end_token_ids,\n",
    "                    labels=batch_labels,\n",
    "                    calc_loss=2\n",
    "                )\n",
    "\n",
    "                \n",
    "                # calculate the loss and scale it by the batch size\n",
    "                loss = outputs[\"loss\"] / batch_size\n",
    "                scaler.scale(loss).backward()\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            del batch_ids, batch_masks, batch_labels\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += train_loss\n",
    "        train_bar.set_postfix(loss=train_loss)\n",
    "\n",
    "    epoch_time = time.time() - train_start\n",
    "    print(f\"Epoch {epoch + 1} finished. Total Loss: {total_loss:.4f}. Time: {epoch_time:.2f} sec\")\n",
    "\n",
    "    # validation step\n",
    "    model.eval()\n",
    "    total_eval_loss = 0.0\n",
    "    test_bar = tqdm(test_loader, desc=\"Validating\", leave=False)\n",
    "\n",
    "    val_start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_bar: # looping through the batches of notebooks\n",
    "            input_ids_batch = batch['input_ids']\n",
    "            attention_mask_batch = batch['attention_mask']\n",
    "            labels_batch = batch['labels']\n",
    "\n",
    "            batch_loss = 0.0\n",
    "\n",
    "            # go through each notebook in the batch\n",
    "            for batch_ids, batch_masks, batch_labels in zip(input_ids_batch, attention_mask_batch, labels_batch):\n",
    "                # get first n chunks in the notebook, we use chunk size of 4\n",
    "                batch_ids = batch_ids[:4].to(device)\n",
    "                batch_masks = batch_masks[:4].to(device)\n",
    "                batch_labels = [lbl.to(device) for lbl in batch_labels[:4]]\n",
    "                # Use autocast for mixed precision\n",
    "                with autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    outputs = model(\n",
    "                        input_ids=batch_ids,\n",
    "                        attention_mask=batch_masks,\n",
    "                        start_token_ids=start_token_ids,\n",
    "                        end_token_ids=end_token_ids,\n",
    "                        labels=batch_labels,\n",
    "                        calc_loss=2\n",
    "                    )\n",
    "\n",
    "                    # calculate the loss and scale it by the batch size to get validation loss\n",
    "                    loss = outputs[\"loss\"] / batch_size\n",
    "                    batch_loss += loss.item()\n",
    "\n",
    "                del batch_ids, batch_masks, batch_labels\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "            total_eval_loss += batch_loss\n",
    "            test_bar.set_postfix(val_loss=batch_loss)\n",
    "\n",
    "    avg_eval_loss = total_eval_loss / len(test_loader)\n",
    "    val_time = time.time() - val_start\n",
    "    print(f\"Validation Loss per batch: {avg_eval_loss:.4f}, Time: {val_time:.2f} seconds\")\n",
    "\n",
    "        \n",
    "\n",
    "    # after validation in each epoch get the results:\n",
    "    eval_start = time.time()\n",
    "    model_tester.eval_vector_batched(test_loader, model, start_token_ids, end_token_ids, device, chunk_size=4)\n",
    "    model_tester.print_results()\n",
    "    model_tester.reset()\n",
    "    eval_time = time.time() - eval_start\n",
    "    print(f\"Evaluation Time (F1/Recall/Precision): {eval_time:.2f}s time: {eval_time:.2f} seconds\")\n",
    "\n",
    "    # save checkpoint at the end of each epoch to the checkpoint directory\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch + 1}.pt\")\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scaler_state_dict': scaler.state_dict(),\n",
    "    }, checkpoint_path)\n",
    "    print(f\"Checkpoint saved to {checkpoint_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
