{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "346cc9af",
   "metadata": {},
   "source": [
    "### Model Loading and Token Classification Setup\n",
    "\n",
    "This section loads and prepares JupOtter based on **CodeT5** for **cell-level bug detection** in Jupyter notebooks.\n",
    "\n",
    "- **Model**: A custom PyTorch model class 'CodeT5TokenClassifier' is used to set up JupOtter. This class uses a CodeT5 encoder and a linear classification layer to predict bugs. The CodeT5 model used can be configured through changing 'default_encoder_path' at the top of the file.\n",
    "- **Input/Output**: For each input sample (chunk of notebook), the model extracts hidden states corresponding to special tokens `<CELL_i>` and `<END_CELL_i>`. It averages the token embeddings between these boundaries and applies a classifier to predict bugs.\n",
    "- **Loss Modes**:\n",
    "  - This setup contains 3 loss modes choosen through the 'calc_loss' parameter. `calc_loss = 0`: No loss is calculated, only logits are returned. `calc_loss = 1`: Chunk-weighted binary cross-entropy loss. `calc_loss = 2`: Cell-weighted binary cross-entropy loss.\n",
    "- **Special Tokens**: `2046` start/end tokens (`<CELL_1>`, ..., `<END_CELL_1023>`) are added to the tokenizer to delimit code cells.\n",
    "- **Model Loading**: The model is loaded from a saved checkpoint configured through the 'saved_model_path' parameter. Tokenizer embeddings are resized to account for the new special tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167a0b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\otten\\AppData\\Local\\Temp\\ipykernel_13708\\2326339799.py:124: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(saved_model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from models\\JupOtter-base_epoch_9.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import T5EncoderModel, RobertaTokenizer\n",
    "\n",
    "saved_model_path = \"models\\\\JupOtter-base_epoch_9.pt\"\n",
    "tokenizer_path = 'Salesforce/codet5-base'\n",
    "default_encoder_path = 'Salesforce/codet5-base'\n",
    "num_max_cells = 1024  # Number of special tokens to use\n",
    "\n",
    "class CodeT5TokenClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_labels=1):\n",
    "        \"\"\"\n",
    "        model_name: e.g., 'Salesforce/codet5-base'\n",
    "        num_labels: Number of labels per cell\n",
    "        \"\"\"\n",
    "        super(CodeT5TokenClassifier, self).__init__() # get the base encoder model\n",
    "        self.encoder = T5EncoderModel.from_pretrained(model_name)\n",
    "        hidden_size = self.encoder.config.d_model\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)  # intitialize linear laryer that will map hidden states to a single logit\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, start_token_ids, end_token_ids, labels=None, calc_loss=1):\n",
    "\n",
    "        encoder_outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask) #getting the hidden states\n",
    "        hidden_states = encoder_outputs.last_hidden_state  # getting the last hidden state\n",
    "\n",
    "        # Create a mask to identify positions of the target tokens, one for the start tokens,\n",
    "        # one for the end token\n",
    "        start_mask = torch.zeros_like(input_ids, dtype=torch.bool)\n",
    "        end_mask = torch.zeros_like(input_ids, dtype=torch.bool)\n",
    "        for token_id in start_token_ids:\n",
    "            start_mask |= (input_ids == token_id) # getting all of the start tokens present in the code\n",
    "        for token_id in end_token_ids:\n",
    "            end_mask |= (input_ids == token_id) # getting all of the end tokens presentin in the code\n",
    "      \n",
    "        # will hold the prediction vector of each chunk\n",
    "        logits_list = []\n",
    "\n",
    "        for i in range(hidden_states.size(0)):  # iterate over hidden states for each batch\n",
    "            hs = hidden_states[i]\n",
    "\n",
    "             # Find positions of start and end tokens in this sample to find the cells\n",
    "            start_positions = (start_mask[i]).nonzero(as_tuple=True)[0]\n",
    "            end_positions = (end_mask[i]).nonzero(as_tuple=True)[0]\n",
    "            \n",
    "           \n",
    "            cell_logits = []  # one logit per cell, this will hold the logits\n",
    "            for start_token_pos, end_token_pos in zip(start_positions, end_positions):\n",
    "\n",
    "                cell_hidden_state = hs[start_token_pos:end_token_pos+1]  # getting the hidden state between the special token bounds, corresponds to one cell\n",
    "                cell_rep = cell_hidden_state.mean(dim=0)  # this averages the array of tokens into a vector where each entrie is the average of the features in a token.\n",
    "                logit = self.classifier(cell_rep)  # using the classifier on the vector of averaged tokens to get the prediction for the cell\n",
    "                cell_logits.append(logit)\n",
    "\n",
    "            if cell_logits: # for if logits generated for the sample\n",
    "                logits_list.append(torch.stack(cell_logits)) # add logit to logits list\n",
    "            else:\n",
    "                # if no cell pairs are found, append an empty tensor \n",
    "                logits_list.append(torch.empty(0, self.classifier.out_features, device=hs.device))\n",
    "\n",
    "        # starting loss calculation\n",
    "        loss = None\n",
    "        if labels is None or calc_loss == 0: # if no labels are provided or we do not want to calculate loss\n",
    "            return {\"logits\": logits_list}\n",
    "        elif calc_loss == 1:  # if labels are provided and we want to calculate loss chunk weighted binary cross entropy loss\n",
    "            loss_fct = nn.BCEWithLogitsLoss() # useing binary cross entorphy loss, this is what the paper this idea was based on uses\n",
    "            losses = []\n",
    "            # loop over each examples logits and corresponding labels\n",
    "            for logits, lbl in zip(logits_list, labels):\n",
    "                if len(logits) != len(lbl):\n",
    "                    lbl = lbl[:len(logits)]  # trim lbl to match logits length\n",
    "                    print(f\"Trimmed lbl to match logits length: {len(lbl)}\")\n",
    "                logits = logits.squeeze(-1)\n",
    "                if logits.numel() > 0:  # only calculate loss if logits are not empty\n",
    "                    losses.append(loss_fct(logits, lbl.float()))  # calculate loss\n",
    "\n",
    "                    \n",
    "            if losses:\n",
    "                loss = torch.stack(losses).mean()\n",
    "            return {\"loss\": loss, \"logits\": logits_list}\n",
    "        \n",
    "        elif calc_loss == 2:  # to calculate loss cell weighted binary cross entropy loss\n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "            list_logits = torch.cat(logits_list).squeeze(1)\n",
    "            list_lbl = torch.cat(labels).float()\n",
    "\n",
    "            if len(list_logits) != len(list_lbl): # trim if lenghths do not match\n",
    "                print(f\"Trimming labels from {len(list_lbl)} to {len(list_logits)}\")\n",
    "                list_lbl = list_lbl[:len(list_logits)]\n",
    "\n",
    "                # calculate the loss and scale it by the batch size\n",
    "            loss = loss_fct(list_logits, list_lbl)\n",
    "\n",
    "            return {\"loss\": loss, \"logits\": logits_list}\n",
    "    \n",
    "tokenizer = RobertaTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# setting up special tokens for cell boundaries\n",
    "start_special_tokens = [f\"<CELL_{i}>\" for i in range(1, num_max_cells)]\n",
    "end_special_tokens = [f\"<END_CELL_{i}>\" for i in range(1, num_max_cells)]\n",
    "all_special_tokens = start_special_tokens + end_special_tokens\n",
    "\n",
    "# Add tokens if not already in the vocabulary.\n",
    "for token in all_special_tokens:\n",
    "    if token not in tokenizer.get_vocab():\n",
    "        tokenizer.add_tokens([token])\n",
    "\n",
    "# Get token IDs\n",
    "start_token_ids = [tokenizer.convert_tokens_to_ids(token) for token in start_special_tokens]\n",
    "end_token_ids = [tokenizer.convert_tokens_to_ids(token) for token in end_special_tokens]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Instantiate model and resize embeddings to account for new tokens.\n",
    "model = CodeT5TokenClassifier(default_encoder_path).to(device)\n",
    "model.encoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "# load model checkpoint\n",
    "try:\n",
    "    checkpoint = torch.load(saved_model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint)\n",
    "    print(f\"Loaded model from {saved_model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model from {saved_model_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab78ac3",
   "metadata": {},
   "source": [
    "### Data Cleaning and Loading\n",
    "\n",
    "This cell performs the following steps:\n",
    "\n",
    "- **Data Cleaning**:  \n",
    "  Uses the `cleanData_csv` module to process raw Jupyter notebooks stored in the directory `\"test_notebooks\"`.  \n",
    "  The cleaned and parsed data is saved as a CSV file `\"parsed_notebook_data.csv\"` in the dataset directory.  \n",
    "\n",
    "- **Data Loading**:  \n",
    "  Opens the cleaned CSV file and reads the notebook code samples and their corresponding labels.  \n",
    "  Code samples are stored in `codeSamples` and raw label strings in `raw_labels`.\n",
    "\n",
    "- **Label Processing**:  \n",
    "  After csv dataset is created, the string labels are converted from the CSV into Python lists of integers using `ast.literal_eval`.  \n",
    "  The resulting list of integer labels is stored in `labels_ints` for later use in model training or evaluation. `labels_ints` are cell level\n",
    "  labels, while codeSamples contains the corresponding notebook code cells.\n",
    "\n",
    "'cleanData_csv.Data_manage()' can be used to retrive different statistics about the dataset after the dataset has been labeled and stored in a CSV file. OtterDataset is already labeld and stored in a CSV file in the OtterDataset directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea40729d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import cleanData_csv\n",
    "import csv\n",
    "import ast\n",
    "\n",
    "\n",
    "\n",
    "# file_path_of_notebooks = \"test_notebooks\"\n",
    "data_file_path = \"dataset\\\\CodeParrot_Subset\\\\CodeParrot_dataset.csv\" # path to where the cleaned data will be saved, must be a csv file\n",
    "# data_file_path = \"dataset\\\\OtterDataset\\\\OtterDataset.csv\" \n",
    "# data_file_path = \"dataset\\\\JupyterErrors_dataset\\\\JupyterError_dataset.csv\"\n",
    "\n",
    "\n",
    "clean = cleanData_csv.Data_Clean()\n",
    "manage = cleanData_csv.Data_manage()\n",
    "\n",
    "# clean.create_notebook_train_data(file_path_of_notebooks, data_file_path, 2) # uncomment this line to create dataset, csv file must be moved from the folder of notebooks\n",
    "print(manage.ensureNoDuplicates(data_file_path)) # ensure no duplicates in the dataset\n",
    "\n",
    "# print(f\"Data cleaned and saved to {data_file_path}\")\n",
    "\n",
    "\n",
    "codeSamples, raw_labels, skipped_files = [], [], []\n",
    "\n",
    "first = True\n",
    "csv.field_size_limit(10000000)\n",
    "with open(data_file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "    fileReader = csv.reader(csvfile)\n",
    "    next(fileReader)\n",
    "    for row in fileReader:\n",
    "        try:\n",
    "            codeSamples.append(row[1]) # rq2 filtered code samples based on column 4, ensuring the specific error types were present in the book\n",
    "            raw_labels.append(row[-1])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {row}: {e}\")\n",
    "            skipped_files.append(row[0]) \n",
    "            \n",
    "\n",
    "# turn labels to a list of ints\n",
    "labels_ints = [ast.literal_eval(label.strip()) for label in raw_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100e0451",
   "metadata": {},
   "source": [
    "### Tokenization and Chunking of Notebook Cells\n",
    "\n",
    "This cell performs the following steps to prepare the data for model input:\n",
    "\n",
    "- **Padding Functions**:  \n",
    "  Defines helper functions `pad` and `pad_mask` to pad token sequences and attention masks to a fixed length (chunk token limit), ensuring consistent input sizes for the model.\n",
    "\n",
    "- **Tokenization and Chunking**:  \n",
    "  Iterates over each notebook’s code samples and their labels:  \n",
    "  - Each notebook is split into individual cells using the regex pattern.  \n",
    "  - Cells are tokenized individually using the pretrained tokenizer, with no padding or truncation beyond 2,500 tokens.  \n",
    "  - Cells longer than 2,500 tokens are skipped to avoid excessively large inputs.  \n",
    "  - Tokenized cells are grouped into chunks up to 2,500 tokens in length. If adding a new cell would exceed this limit, the current chunk is saved (with padding), and a new chunk is started.  \n",
    "  - Corresponding attention masks and label tensors for the chunks are created and stored.\n",
    "\n",
    "- **Data Structures for Tokenized Content**:  \n",
    "  The tokenized and padded input IDs, attention masks, and labels for all notebooks are stored in:  \n",
    "  - `tokenized_chunks_ids`  \n",
    "  - `tokenized_chunks_attention_mask`  \n",
    "  - `tokenized_chunk_labels`\n",
    "\n",
    "This cell contains optional code to create a train test split, during our evaluation we used a random state of 42 to split OtterDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19b681eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4892/4892 [00:24<00:00, 203.43it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cell_pattern = re.compile(r\"<CELL_\\d+>(.*?)<END_CELL_\\d+>\", re.DOTALL)\n",
    "\n",
    "def pad(seq, max_len=2500, pad_id=tokenizer.pad_token_id):\n",
    "    padding = torch.full((max_len - seq.size(0),), pad_id, dtype=seq.dtype)\n",
    "    return torch.cat((seq, padding))\n",
    "\n",
    "def pad_mask(mask, max_len=2500):\n",
    "    padding = torch.zeros(max_len - mask.size(0), dtype=mask.dtype)\n",
    "    return torch.cat((mask, padding))\n",
    "\n",
    "# to hold final tokenized data\n",
    "tokenized_chunks_ids = []       \n",
    "tokenized_chunks_attention_mask = []\n",
    "tokenized_chunk_labels = []   \n",
    "skipped = 0\n",
    "for code, label in tqdm(zip(codeSamples, labels_ints), total=len(codeSamples)):\n",
    "    cells = [match.group(0) for match in cell_pattern.finditer(code)]\n",
    "\n",
    "    current_notebook_chunks = [] #to hold the chunks of the current notebook as we build them, chunks up to 2048 tokens\n",
    "    current_notebook_masks = []\n",
    "    current_notebook_labels = [] #to hold the labels of each chunk\n",
    "\n",
    "    current_chunk = [] # current one we are building\n",
    "    current_mask_chunk = []\n",
    "    current_labels = []\n",
    "    current_length = 0\n",
    "    label_idx = 0\n",
    "    if len(cells) != len(label):\n",
    "        print(f\"Label mismatch with cell num\")\n",
    "    for cell_num, cell in enumerate(cells):\n",
    "        encoding = tokenizer(cell, padding=False, truncation=True, max_length=2501, return_tensors=\"pt\")\n",
    "        next_cell = encoding[\"input_ids\"].squeeze(0)\n",
    "        next_mask = encoding[\"attention_mask\"].squeeze(0)\n",
    "        next_cell_len = next_cell.size(0)\n",
    "\n",
    "        next_cell_len = next_cell.size(0)\n",
    "\n",
    "        # skip overly long cells and their labels to maintain consistency\n",
    "        if next_cell_len > 2500: \n",
    "            label_idx += 1\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        # if current chunk length + length to add is greater than 2048, save the current chunk and start a new one\n",
    "        if current_length + next_cell_len > 2500:\n",
    "            if current_chunk: #as long as we had a chunk build at this point append what he had\n",
    "                current_notebook_chunks.append(pad(torch.cat(current_chunk))) # concationate the chunk together and add it to list of chunks\n",
    "                current_notebook_masks.append(pad_mask(torch.cat(current_mask_chunk)))\n",
    "                current_notebook_labels.append(torch.tensor(current_labels, dtype=torch.long))\n",
    "                \n",
    "            current_chunk = [next_cell] # update next chunk with what we just tokenized\n",
    "            current_mask_chunk = [next_mask]\n",
    "            current_labels = [label[label_idx]]\n",
    "            current_length = next_cell_len\n",
    "        else:\n",
    "            current_chunk.append(next_cell) #ortherwise if adding current chunk does not exceed the limit, add it to the current chunk\n",
    "            current_mask_chunk.append(next_mask)\n",
    "            current_labels.append(label[label_idx])\n",
    "            current_length += next_cell_len\n",
    "\n",
    "        label_idx += 1\n",
    "\n",
    "    # save the final chunk of current notebook if it exists after for loop\n",
    "    if current_chunk:\n",
    "        current_notebook_chunks.append(pad(torch.cat(current_chunk))) # concationate current chunk together and add it to list of chunks for this book\n",
    "        current_notebook_masks.append(pad_mask(torch.cat(current_mask_chunk)))\n",
    "        current_notebook_labels.append(torch.tensor(current_labels, dtype=torch.long))\n",
    "        \n",
    "\n",
    "    if len(current_notebook_chunks) > 0: # if we have any chunks in the current notebook, add them to the list of all notebooks\n",
    "        tokenized_chunks_ids.append(torch.stack(current_notebook_chunks)) # add the current notebook chunks to the list of all notebooks\n",
    "        tokenized_chunks_attention_mask.append(torch.stack(current_notebook_masks))\n",
    "        tokenized_chunk_labels.append(current_notebook_labels)\n",
    "\n",
    "\n",
    "\n",
    "# uncomment for train test split\n",
    "# train_ids, test_ids, train_masks, test_masks, train_labels, test_labels = train_test_split(\n",
    "#     tokenized_chunks_ids,\n",
    "#     tokenized_chunks_attention_mask,\n",
    "#     tokenized_chunk_labels,\n",
    "#     test_size=0.2,\n",
    "#     random_state=42\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9daeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following cell is used to load split tokenized data to test. To load the unsplit data, uncomment the lines below and comment out the above lines.\n",
    "\n",
    "import torch\n",
    "\n",
    "load_path = \"dataset\\\\tokenized_content\\\\name_of_file.pt\"\n",
    "\n",
    "tokenized_data = torch.load(load_path)\n",
    "\n",
    "train_ids = tokenized_data['train_ids']\n",
    "test_ids = tokenized_data['test_ids']\n",
    "train_masks = tokenized_data['train_masks']\n",
    "test_masks = tokenized_data['test_masks']\n",
    "train_labels = tokenized_data['train_labels']\n",
    "test_labels = tokenized_data['test_labels']\n",
    "\n",
    "# Uncomment the lines below to load unsplit data\n",
    "# test_ids = tokenized_data['test_ids']\n",
    "# test_masks = tokenized_data['test_masks']\n",
    "# test_labels = tokenized_data['test_labels']\n",
    "\n",
    "print(\"Tokenized data loaded successfully.\")\n",
    "print(f\"Train IDs: {len(train_ids)}, Test IDs: {len(test_ids)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7a8984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following cell is used to save tokenized data for testing, note that this saves the train test split, not the unsplit data. Unsplit data is used for testing our \n",
    "# code parrot jupyter errors dataset and the jupyter errors dataset. To save the unsplit data, uncomment the lines below and comment out the above lines.\n",
    "\n",
    "# To load tokenized data, ensure the path is correct. Tokenizer as well as code to save tokenized content is in the run model file.\n",
    "\n",
    "save_path = \"dataset\\\\tokenized_content\\\\name_of_file.pt\"\n",
    "\n",
    "tokenized_data = {\n",
    "    'train_ids': train_ids,\n",
    "    'test_ids': test_ids,\n",
    "    'train_masks': train_masks,\n",
    "    'test_masks': test_masks,\n",
    "    'train_labels': train_labels,\n",
    "    'test_labels': test_labels\n",
    "}\n",
    "\n",
    "# to save unsplit data, uncomment the following lines and comment out the above lines\n",
    "# tokenized_data = {\n",
    "#     'test_ids': tokenized_chunks_ids,\n",
    "#     'test_masks': tokenized_chunks_attention_mask,\n",
    "#     'test_labels': tokenized_chunk_labels\n",
    "# }\n",
    "torch.save(tokenized_data, save_path)\n",
    "print(f\"Tokenized data saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463150f3",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "This cell prepares the tokenized data for batching and evaluates the trained model. We have included commented out code to perform cell-level, file-level, and single notebook analysis. Analysis is done using 'buggy_cell_vector_evalualtion_clean' which contains many functions to aid in evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb2bbab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- Evaluation Results ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating batches: 100%|██████████| 4770/4770 [29:27<00:00,  2.70batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 1: 4056, Total 0: 74352\n",
      "\n",
      "===== Model Evaluation Metrics Totals =====\n",
      "Total books: 4770, Total cells: 78408, Total buggy cells: 3816\n",
      "True positives: 2981, True negatives: 73517, Total correct: 76498\n",
      "False positives: 1075, False negatives: 835, Total incorrect: 1910\n",
      "\n",
      "===== Cell-aggregated =====\n",
      "Precision: 0.7350\n",
      "Recall: 0.7812\n",
      "F1 Score: 0.7574\n",
      "Accuracy: 0.9756\n",
      "Buggy Cell Ratio: 0.0487\n",
      "===================================\n",
      "\n",
      "\n",
      "===== File-aggregated =====\n",
      "Precision Score: 0.8639\n",
      "Recall Score: 0.9186\n",
      "F1 Score: 0.8904\n",
      "Accuracy Score: 0.9620\n",
      "===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import buggy_cell_vector_evalualtion_clean\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NotebookDataset(Dataset):\n",
    "    def __init__(self, all_ids, all_masks, all_labels):\n",
    "        # here all_ids is a tensor of chunnks where each row is a chunk of a notebook\n",
    "        self.ids = all_ids\n",
    "        self.masks = all_masks\n",
    "        self.labels = all_labels\n",
    "\n",
    "    def __len__(self): # just num samples\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, i): # to get the data for a single book, can be in multiple chunks\n",
    "        return {\n",
    "          \"input_ids\": self.ids[i],           \n",
    "          \"attention_mask\": self.masks[i],    \n",
    "          \"labels\": self.labels[i],         \n",
    "        }\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Stack fixed-size tensors for inputs and attention masks.\n",
    "    input_ids = ([item['input_ids'] for item in batch])\n",
    "    attention_mask = ([item['attention_mask'] for item in batch])\n",
    "    # Leave labels as a list of tensors, since they are variable-length.\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    return {'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels}\n",
    "\n",
    "\n",
    "# Create the dataset for testing from whole tokenized dataset\n",
    "test_dataset = NotebookDataset(tokenized_chunks_ids, tokenized_chunks_attention_mask, tokenized_chunk_labels)\n",
    "\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "\n",
    "model_tester = buggy_cell_vector_evalualtion_clean.VectorEval()\n",
    "print(\"---------------------- Evaluation Results ----------------------\")\n",
    "# the following is for evaluating the model on the test set at cell-level\n",
    "model_tester.eval_vector_batched(test_loader, model, start_token_ids, end_token_ids, device, chunk_size=4)\n",
    "model_tester.print_results()\n",
    "model_tester.reset()\n",
    "\n",
    "# the following is for evaluating the model on the test set at file-level\n",
    "# model_tester.eval_vector_batched(test_loader, model, start_token_ids, end_token_ids, device, chunk_size=4, eval_type=2)\n",
    "# model_tester.print_results_file_level()\n",
    "# model_tester.reset()\n",
    "\n",
    "# the following is for evaluating a single book at cell-level\n",
    "# model_tester.eval_single_book(test_loader, model, start_token_ids, end_token_ids, device, chunk_size=4)\n",
    "# model_tester.print_results()\n",
    "#11"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
