{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [nzw0301/keras-examples](https://github.com/nzw0301/keras-examples)\n",
    "* [fchollet/keras](https://github.com/fchollet/keras/blob/master/examples/imdb_fasttext.py)\n",
    "* [kemaswill/fasttext_benchmark](https://github.com/kemaswill/fasttext_benchmark/blob/master/fasttext.py)\n",
    "* [Keras Blog](https://blog.keras.io/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.1\n",
      "0.18.1\n",
      "0.7.0\n",
      "0.18.1\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "print(keras.__version__)\n",
    "import scipy\n",
    "print(scipy.__version__)\n",
    "import theano\n",
    "print(theano.__version__)\n",
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Flatten, Activation, Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "\n",
    "EmbeddingDim    = 50\n",
    "MaxWords        = 30000\n",
    "SequenceLength  = 50\n",
    "Epochs          = 5\n",
    "SamplesPerEpoch = 1000\n",
    "BatchSize       = 64\n",
    "Labels          = 3\n",
    "LabelMapping    = {\n",
    "  1: 0,\n",
    "  2: 0,\n",
    "  3: 1,\n",
    "  4: 2,\n",
    "  5: 2\n",
    "}\n",
    "\n",
    "def oneHot(dictionarySize, wordIndex):\n",
    "\tvect = np.zeros(dictionarySize)\n",
    "\tif wordIndex > 0: vect[wordIndex] = 1\n",
    "\treturn vect\n",
    "\n",
    "# From https://primes.utm.edu/lists/small/100ktwins.txt\n",
    "Prime1 = 15327749\n",
    "Prime2 = 18409199\n",
    "\n",
    "# `sequence` must refer to zero-padded sequence.\n",
    "# From http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf, equation 6.6\n",
    "def biGramHash(sequence, t, buckets):\n",
    "\tt1 = sequence[t - 1] if t - 1 >= 0 else 0\n",
    "\treturn (t1 * Prime1) % buckets\n",
    "\n",
    "def triGramHash(sequence, t, buckets):\n",
    "\tt1 = sequence[t - 1] if t - 1 >= 0 else 0\n",
    "\tt2 = sequence[t - 2] if t - 2 >= 0 else 0\n",
    "\n",
    "\treturn (t2 * Prime1 * Prime2 + t1 * Prime1) % buckets\n",
    "\n",
    "def sentenceVector(tokeniser, dictionarySize, sentence, oneHotVectors, oneHotAveraged, contextHashes):\n",
    "\tresult    = np.array([])\n",
    "\tsequences = tokeniser.texts_to_sequences([sentence])\n",
    "\t# Zero-pad every string\n",
    "\tpadded    = pad_sequences(sequences, maxlen=SequenceLength)[0]\n",
    "\n",
    "\tif oneHotVectors:\n",
    "\t\tiptOneHot = [oneHot(dictionarySize, i) for i in padded]\n",
    "\t\tresult = np.append(\n",
    "\t\t\tresult,\n",
    "\t\t\tnp.mean(iptOneHot, axis=0) if oneHotAveraged else np.concatenate(iptOneHot)\n",
    "\t\t)\n",
    "\n",
    "\tif contextHashes:\n",
    "\t\tbuckets = np.zeros(dictionarySize * 2)\n",
    "\t\tfor t in range(SequenceLength): buckets[biGramHash(padded, t, dictionarySize)] = 1\n",
    "\t\tfor t in range(SequenceLength): buckets[dictionarySize + triGramHash(padded, t, dictionarySize)] = 1\n",
    "\t\tresult = np.append(result, buckets)\n",
    "\n",
    "\treturn result\n",
    "\n",
    "\n",
    "def mapGenerator(generator, tokeniser, dictionarySize, oneHot, oneHotAveraged, contextHashes):\n",
    "\tfor row in generator:\n",
    "\t\tsentence = row[0]\n",
    "\t\tlabel    = row[1]\n",
    "\n",
    "\t\tx = sentenceVector(tokeniser, dictionarySize, sentence, oneHot, oneHotAveraged, contextHashes)\n",
    "\t\ty = np.zeros(Labels)\n",
    "\t\ty[LabelMapping[label]] = 1\n",
    "\t\tyield (x[np.newaxis], y[np.newaxis])\n",
    "\n",
    "def train(oneHot, oneHotAveraged, contextHashes):\n",
    "\tn = (Epochs + 1) * SamplesPerEpoch  # TODO + 1 should not be needed\n",
    "\n",
    "\ttokeniser = Tokenizer(nb_words=MaxWords)\n",
    "\ttokeniser.fit_on_texts((row[0] for row in trainingData(n)))\n",
    "\n",
    "\t# `word_index` maps each word to its unique index\n",
    "\tdictionarySize = len(tokeniser.word_index) + 1\n",
    "\n",
    "\toneHotDimension        = (1 if oneHotAveraged else SequenceLength) * dictionarySize if oneHot else 0\n",
    "\tcontextHashesDimension = dictionarySize * 2 if contextHashes else 0\n",
    "\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(EmbeddingDim, input_dim=(oneHotDimension + contextHashesDimension)))\n",
    "\tmodel.add(Dense(Labels, activation='softmax'))\n",
    "\tmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\ttrainingGenerator   = mapGenerator(trainingData(n),   tokeniser, dictionarySize, oneHot, oneHotAveraged, contextHashes)\n",
    "\tvalidationGenerator = mapGenerator(validationData(n), tokeniser, dictionarySize, oneHot, oneHotAveraged, contextHashes)\n",
    "\n",
    "\tmodel.fit_generator(trainingGenerator,\n",
    "\t\tnb_epoch=Epochs,\n",
    "\t\tsamples_per_epoch=SamplesPerEpoch,\n",
    "\t\tvalidation_data=validationGenerator,\n",
    "\t\tnb_val_samples=SamplesPerEpoch)\n",
    "\n",
    "\tmodel2 = Sequential()\n",
    "\tmodel2.add(Dense(EmbeddingDim, input_dim=(oneHotDimension + contextHashesDimension), weights=model.layers[0].get_weights()))\n",
    "\n",
    "\treturn model, model2, tokeniser, dictionarySize\n",
    "\n",
    "# TODO Fix\n",
    "def query(model, tokeniser, dictionarySize, sentence):\n",
    "\tconcat = sentenceVector(tokeniser, dictionarySize, sentence)\n",
    "\treturn model.predict(np.asarray(concat)[np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs\n",
    "\n",
    "DataSetPath = '/home/data/sentiment-analysis-and-text-classification/yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_review.json'\n",
    "\n",
    "\n",
    "def processFile(n, validation):\n",
    "  with codecs.open(DataSetPath, encoding='iso-8859-1') as f:\n",
    "    if validation:\n",
    "      for _ in range(n): next(f)\n",
    "\n",
    "    for _ in range(n):\n",
    "      line   = next(f).strip()\n",
    "      review = json.loads(line)\n",
    "\n",
    "      while len(review['text'].split()) > 50:\n",
    "        line   = next(f).strip()\n",
    "        review = json.loads(line)\n",
    "\n",
    "      yield (review['text'], int(review['stars']))\n",
    "\n",
    "def trainingData(n):\n",
    "  return processFile(n, validation = False)\n",
    "\n",
    "def validationData(n):\n",
    "  return processFile(n, validation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import six.moves.cPickle\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 24s - loss: 0.6913 - acc: 0.7510 - val_loss: 0.8134 - val_acc: 0.7270\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 24s - loss: 0.6052 - acc: 0.7840 - val_loss: 0.5645 - val_acc: 0.7980\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 25s - loss: 0.6514 - acc: 0.7590 - val_loss: 0.7004 - val_acc: 0.7820\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 25s - loss: 0.5726 - acc: 0.8090 - val_loss: 0.6256 - val_acc: 0.7840\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 26s - loss: 0.6181 - acc: 0.7660 - val_loss: 0.5753 - val_acc: 0.7740\n"
     ]
    }
   ],
   "source": [
    "model, model2, tokeniser, dictionarySize = train(oneHot = True, oneHotAveraged = True, contextHashes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
