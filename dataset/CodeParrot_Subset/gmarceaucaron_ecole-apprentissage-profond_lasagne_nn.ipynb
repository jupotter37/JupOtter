{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explains how to define a classification task for vision with Lasagne and theano.\n",
    "\n",
    "To execute a cell: Ctrl-Enter.\n",
    "\n",
    "The code was executed with the default configuration of Theano: `floatX=float64`, `device=cpu` and the configuration for GPU `floatX=float32,device=cuda`.\n",
    "\n",
    "Tested with:\n",
    "- Python 3.6.2, \n",
    "- Theano 0.10.0beta1.dev,\n",
    "- Lasagne 0.2.dev1,\n",
    "- cuDNN version 6021,\n",
    "- GeForce GTX TITAN Black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['THEANO_FLAGS'] = 'floatX=float32,device=cuda,mode=FAST_RUN'\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "seed = 1\n",
    "lasagne.random.set_rng(np.random.RandomState(seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are hyperparameters that will have an impact on the learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Architecture\n",
    "N_HIDDEN = [800,800]\n",
    "NON_LINEARITY = lasagne.nonlinearities.rectify\n",
    "\n",
    "# Dropout parameters\n",
    "#DROP_INPUT = 0.2\n",
    "#DROP_HIDDEN = 0.5\n",
    "DROP_INPUT = None\n",
    "DROP_HIDDEN = None\n",
    "\n",
    "# Number of epochs to train the net\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "# Optimization learning rate\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# Batch Size\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Optimizer\n",
    "eta = theano.shared(lasagne.utils.floatX(LEARNING_RATE))\n",
    "my_optimizer = lambda loss, params: lasagne.updates.nesterov_momentum(\n",
    "        loss, params, learning_rate=eta, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An optimizer can be seen as a function that takes a gradient, obtained by backpropagation, and returns an update to be applied to the current parameters. Other optimizers can be found in: [optimizer reference](http://lasagne.readthedocs.io/en/latest/modules/updates.html?highlight=update). In order to be able to change the learning rate dynamically, we must use a shared variable that will be accessible afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "In this example, we are using the celebrated [MNIST dataset](http://yann.lecun.com/exdb/mnist/). The following are functions that download the MNIST dataset, resize it into a convenient numpy array for images of size `(n_example, n_channel, img_width, img_height)` and split the dataset into a train set (50k images) and a validation set (10k images). The pixels are normalized by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def load_mnist():\n",
    "    \"\"\"\n",
    "    A dataloader for MNIST\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    from urllib.request import urlretrieve\n",
    "  \n",
    "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "        print(\"Downloading %s\" % filename)\n",
    "        urlretrieve(source + filename, filename)\n",
    "\n",
    "    # We then define functions for loading MNIST images and labels.\n",
    "    # For convenience, they also download the requested files if needed.\n",
    "    import gzip\n",
    "\n",
    "    def load_mnist_images(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the inputs in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
    "        # following the shape convention: (examples, channels, rows, columns)\n",
    "        data = data.reshape(-1, 1, 28, 28)\n",
    "        # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
    "        return data / np.float32(255)\n",
    "\n",
    "    def load_mnist_labels(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the labels in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        # The labels are vectors of integers now, that's exactly what we want.\n",
    "        return data\n",
    "\n",
    "    # We can now download and read the training and test set images and labels.\n",
    "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
    "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
    "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "    # We reserve the last 10000 training examples for validation.\n",
    "    X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "    # We just return all the arrays in order, as expected in main().\n",
    "    # (It doesn't matter how we do this as long as we can read them again.)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "print(\"Loading data...\")\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_mnist()\n",
    "n_train = X_train.shape[0]\n",
    "input_shape = X_train[0].shape\n",
    "print(input_shape)\n",
    "input_shape = (None, input_shape[0], input_shape[1], input_shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "n_img_row = 3\n",
    "n_img_col = 3\n",
    "plt.rcParams['figure.figsize'] = (12,12) # Make the figures a bit bigger\n",
    "for i in range(n_img_row*n_img_col):\n",
    "    plt.subplot(n_img_row,n_img_col,i+1)\n",
    "    plt.axis('off')\n",
    "    idx = np.random.randint(n_train)\n",
    "    plt.imshow(X_train[idx][0], cmap='gray')\n",
    "    plt.title(\"Label {}\".format(y_train[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following auxiliary function creates a minibatch in a 3D tensor (batch_size, img_width, img_height)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    \"\"\"\n",
    "    Return a minibatch of images with the associated targets\n",
    "\n",
    "    Keyword arguments:\n",
    "    :type inputs: numpy.ndarray\n",
    "    :param inputs: the dataset of images\n",
    "    :type targets: numpy.ndarray\n",
    "    :param targets: the targets associated to the dataset\n",
    "    :type batchsize: int\n",
    "    :param batchsize: the number of datapoints in the minibatch\n",
    "    :type shuffle: bool\n",
    "    :param shuffle: a flag if we want to shuffle the dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two functions are general functions for creating multi-layer perceptron (mlp) and convolutional neural networks (cnn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_mlp(\n",
    "    input_shape,\n",
    "    input_var=None,\n",
    "    nonlinearity = lasagne.nonlinearities.rectify,\n",
    "    n_hidden=[800], \n",
    "    drop_input=.2,\n",
    "    drop_hidden=.5):\n",
    "    \"\"\"\n",
    "    A generic function for creating a multi-layer perceptron.\n",
    "    If n_hidden is given as a list, then depth is ignored.\n",
    "    \n",
    "    :type input_shape: tuple\n",
    "    :param input_shape: a tuple containing the shape of the input\n",
    "    :type input_var: theano.tensor.var.TensorVariable\n",
    "    :param input_var: a theano symbolic variable, created automatically if None\n",
    "    :type nonlinearity: lasagne.nonlinearities\n",
    "    :param nonlinearity: a nonlinearity function that follows all dense layers\n",
    "    :type n_hidden: list\n",
    "    :param n_hidden: number of hidden units per layer\n",
    "    :type drop_input: float\n",
    "    :param drop_input: the probability of dropout for the input\n",
    "    :type drop_hidden: float\n",
    "    :param drop_hidden: the probability of dropout for the hidden units\n",
    "    \"\"\"\n",
    "\n",
    "    # if input_shape is None, then the mlp is used on top of an existing model\n",
    "    if input_shape:\n",
    "        \n",
    "        # if input_var is None, lasagne create \n",
    "        # automatically the associated theano variable\n",
    "        network = lasagne.layers.InputLayer(\n",
    "            shape=input_shape,\n",
    "            input_var=input_var)\n",
    "        \n",
    "        if drop_input:\n",
    "            network = lasagne.layers.dropout(\n",
    "            incoming=network,\n",
    "            p=drop_input)\n",
    "    else:\n",
    "        network = input_var\n",
    "    \n",
    "    for i in range(len(n_hidden)):\n",
    "        network = lasagne.layers.DenseLayer(\n",
    "            incoming=network, \n",
    "            num_units=n_hidden[i],\n",
    "            nonlinearity=nonlinearity\n",
    "        )\n",
    "        if drop_hidden:\n",
    "            network = lasagne.layers.dropout(\n",
    "                incoming=network, \n",
    "                p=drop_hidden\n",
    "            )\n",
    "\n",
    "    network = lasagne.layers.DenseLayer(\n",
    "        incoming=network, \n",
    "        num_units=10, \n",
    "        nonlinearity=lasagne.nonlinearities.softmax\n",
    "    )\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a network\n",
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "\n",
    "network = create_mlp(\n",
    "    input_shape,\n",
    "    input_var=input_var,\n",
    "    nonlinearity=NON_LINEARITY,\n",
    "    n_hidden=N_HIDDEN, \n",
    "    drop_input=DROP_INPUT, \n",
    "    drop_hidden=DROP_HIDDEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we want to maximize the probability to output the right digit given the image. To do this, we retrieve the output of our model, which is a softmax (probability distribution) over the 10 digits, and we compare it to the actual target. Finally, since we are using minibatches of size `BATCH_SIZE`, we compute the mean over the examples of the minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a loss expression for training\n",
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.categorical_crossentropy(prediction, target_var).mean()\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = my_optimizer(loss, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile a function performing a training step on a mini-batch (by giving\n",
    "# the updates dictionary) and returning the corresponding training loss:\n",
    "train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "\n",
    "# Create a loss expression for validation/testing. The crucial difference\n",
    "# here is that we do a deterministic forward pass through the network,\n",
    "# disabling dropout layers.\n",
    "valid_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "valid_loss = lasagne.objectives.categorical_crossentropy(valid_prediction, target_var).mean()\n",
    "\n",
    "# We also create an expression for the classification accuracy:\n",
    "valid_acc = lasagne.objectives.categorical_accuracy(valid_prediction, target_var).mean()\n",
    "\n",
    "# Compile a second function computing the validation loss and accuracy:\n",
    "valid_fn = theano.function([input_var, target_var], [valid_loss, valid_acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following training loop is minimal and often insufficient for real-world purposes.\n",
    "The idea here is to show the minimal requirements for training a neural network.\n",
    "Also, we plot to show the evolution of the train and validation losses.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "plt.rcParams['figure.figsize'] = (4,4) # Make the figures a bit bigger\n",
    "\n",
    "import time\n",
    "def train(\n",
    "    train_fn,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    valid_fn,\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    num_epochs=50,\n",
    "    batchsize=100):\n",
    "    \n",
    "    ###################\n",
    "    # code for plotting\n",
    "    ###################\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('NLL')\n",
    "    ax.set_xlim(0,50)\n",
    "    ax.set_ylim(0,0.5)\n",
    "    \n",
    "    train_log = []\n",
    "    valid_log = []\n",
    "    ###################\n",
    "    \n",
    "    n_train_batches = X_train.shape[0] // batchsize # Warning last examples are not used\n",
    "    n_valid_batches = X_valid.shape[0] // batchsize\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        train_loss = 0\n",
    "        for inputs, targets in iterate_minibatches(X_train, y_train, batchsize, shuffle=True):\n",
    "            train_loss += train_fn(inputs, targets)\n",
    "\n",
    "        valid_loss = 0\n",
    "        for inputs, targets in iterate_minibatches(X_valid, y_valid, batchsize, shuffle=False):\n",
    "            loss,_ = valid_fn(inputs, targets)\n",
    "            valid_loss += loss\n",
    "        \n",
    "        ###################\n",
    "        # code for plotting\n",
    "        ###################\n",
    "        train_log.append(train_loss/n_train_batches)\n",
    "        valid_log.append(valid_loss/n_valid_batches)\n",
    "        #print(train_loss/n_train_batches, valid_loss/n_valid_batches)\n",
    "        if ax.lines:\n",
    "            ax.lines[0].set_xdata(range(0,epoch+1))\n",
    "            ax.lines[0].set_ydata(train_log)\n",
    "            ax.lines[1].set_xdata(range(0,epoch+1))\n",
    "            ax.lines[1].set_ydata(valid_log)\n",
    "        else:\n",
    "            ax.plot(epoch, train_log[epoch], 'b', label='train')\n",
    "            ax.plot(epoch, valid_log[epoch], 'r', label='valid')\n",
    "            ax.legend()\n",
    "            ax.grid()\n",
    "        fig.canvas.draw()\n",
    "        time.sleep(0.1)\n",
    "        ###################\n",
    "\n",
    "train(train_fn, X_train, y_train, valid_fn, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following training loop contains features that are interesting to consider:\n",
    "- early-stopping\n",
    "- logging and filenames\n",
    "- checkpointing\n",
    "- adaptive step-size (optional)\n",
    "\n",
    "The first three are the most important ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "def train(\n",
    "    train_fn,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    valid_fn,\n",
    "    X_valid,\n",
    "    y_valid,\n",
    "    num_epochs=100,\n",
    "    batchsize=64):\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    train_loss_array = []\n",
    "    valid_loss_array = []\n",
    "    \n",
    "    # early-stopping parameters\n",
    "    n_iter = 0\n",
    "    n_train_batches = X_train.shape[0] // batchsize # Warning last examples are not used\n",
    "    n_valid_batches = X_valid.shape[0] // batchsize\n",
    "    patience = 10 * n_train_batches # look as this many examples regardless\n",
    "    patience_increase = 2.  # wait this much longer when a new best is\n",
    "                           # found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                   # considered significant\n",
    "    validation_frequency = min(n_train_batches, patience // 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "    best_valid_loss = np.inf\n",
    "    best_iter = 0\n",
    "    test_score = 0.\n",
    "\n",
    "    epoch = 0\n",
    "    done_looping = False\n",
    "    \n",
    "    if not isinstance(N_HIDDEN, list):\n",
    "        n_hidden = [N_HIDDEN] * DEPTH\n",
    "    else:\n",
    "        n_hidden = N_HIDDEN\n",
    "    \n",
    "    exp_log_filename = 'mlp_lr-{}_arch-{}_bs-{}_seed-{}.log'.format(\n",
    "        LEARNING_RATE, \n",
    "        '-'.join(str(i) for i in n_hidden),\n",
    "        batchsize,\n",
    "        seed\n",
    "    )\n",
    "    with open(exp_log_filename, 'w') as f:\n",
    "        log_line = '{} \\t\\t{} \\t\\t{} \\t\\t{} \\n'.format('epoch', 'train_loss', 'valid_loss', 'valid_acc')\n",
    "        f.write(log_line)\n",
    "    \n",
    "    while epoch < num_epochs and not done_looping:\n",
    "        \n",
    "        train_loss = 0\n",
    "        start_time = time.time()\n",
    "        for inputs, targets in iterate_minibatches(X_train, y_train, batchsize, shuffle=True):\n",
    "            train_loss += train_fn(inputs, targets)\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        valid_loss = 0\n",
    "        valid_acc = 0\n",
    "        for inputs, targets in iterate_minibatches(X_valid, y_valid, batchsize, shuffle=False):\n",
    "            loss, acc = valid_fn(inputs, targets)\n",
    "            valid_loss += loss\n",
    "            valid_acc += acc\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        avg_train_loss = train_loss / n_train_batches\n",
    "        avg_valid_loss = valid_loss / n_valid_batches\n",
    "        avg_valid_acc = valid_acc / n_valid_batches * 100\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(avg_train_loss))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(avg_valid_loss))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(avg_valid_acc))\n",
    "        train_loss_array.append(avg_train_loss)\n",
    "        valid_loss_array.append(avg_valid_loss)\n",
    "        \n",
    "        with open(exp_log_filename, 'a') as f:\n",
    "            log_line = '{} \\t\\t{:.6f} \\t\\t{:.6f} \\t\\t{:.2f} \\n'.format(epoch, avg_train_loss, avg_valid_loss, avg_valid_acc)\n",
    "            f.write(log_line)\n",
    "        \n",
    "        # if we got the best validation score until now\n",
    "        n_iter += n_train_batches\n",
    "        if valid_loss < best_valid_loss:\n",
    "\n",
    "            #improve patience if loss improvement is good enough\n",
    "            if valid_loss < best_valid_loss * improvement_threshold:\n",
    "                patience = max(patience, n_iter * patience_increase)\n",
    "\n",
    "                best_valid_loss = valid_loss\n",
    "                best_iter = n_iter\n",
    "\n",
    "                # save the best model\n",
    "                with open('best_model.pkl', 'wb') as f:\n",
    "                    all_params_values = lasagne.layers.get_all_param_values(network)\n",
    "                    pickle.dump(all_params_values, f)\n",
    "                eta.set_value(lasagne.utils.floatX(eta.get_value() * 1.2))\n",
    "                    \n",
    "            if patience <= n_iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "                \n",
    "        else:\n",
    "            all_params_values = pickle.load(open('best_model.pkl','rb'))\n",
    "            lasagne.layers.set_all_param_values(network, all_params_values)\n",
    "            eta.set_value(lasagne.utils.floatX(eta.get_value() * 0.5))\n",
    "            \n",
    "        epoch += 1\n",
    "\n",
    "train_log, valid_log = train(train_fn, X_train, y_train, valid_fn, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!tail mlp_lr-0.01_arch-800-800_bs-64_seed-1.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the saved model\n",
    "all_params_values = pickle.load(open('best_model.pkl','rb'))\n",
    "lasagne.layers.set_all_param_values(network, all_params_values)\n",
    "\n",
    "# After training, we compute the test error.\n",
    "test_loss, test_acc = valid_fn(X_test, y_test)\n",
    "print(\"Final results:\")\n",
    "print(\"  test loss:\\t\\t\\t {:.6f}\".format(np.asscalar(test_loss)))\n",
    "print(\"  test accuracy:\\t\\t {:.2f} %\".format(np.asscalar(test_acc*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
