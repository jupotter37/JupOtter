{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use vgg19 to train the cifar-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import pakages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, AveragePooling2D\n",
    "from keras.initializers import he_normal\n",
    "from keras import optimizers\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard\n",
    "from keras.utils.data_utils import get_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# force to use gpu and limit the use of gpu memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 1.0\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes  = 10\n",
    "batch_size   = 128\n",
    "epochs       = 170\n",
    "iterations   = 391\n",
    "dropout      = 0.5\n",
    "weight_decay = 0.0015\n",
    "log_filepath = r'./vgg19_retrain_WHE_wd/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# do some precessing with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def color_preprocessing(x_train,x_test):\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    # data preprocessing \n",
    "    x_train[:,:,:,0] = (x_train[:,:,:,0]-123.680)\n",
    "    x_train[:,:,:,1] = (x_train[:,:,:,1]-116.779)\n",
    "    x_train[:,:,:,2] = (x_train[:,:,:,2]-103.939)\n",
    "    x_test[:,:,:,0] = (x_test[:,:,:,0]-123.680)\n",
    "    x_test[:,:,:,1] = (x_test[:,:,:,1]-116.779)\n",
    "    x_test[:,:,:,2] = (x_test[:,:,:,2]-103.939)\n",
    "\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set the learning rate changes strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "  learning_rate_init = 0.1\n",
    "  if epoch > 80:\n",
    "    learning_rate_init = 0.01\n",
    "  if epoch > 120:\n",
    "    learning_rate_init = 0.001\n",
    "  return learning_rate_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def VGG19():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Block 1\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block1_conv1', input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block1_conv2'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool'))\n",
    "\n",
    "    # Block 2\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block2_conv1'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block2_conv2'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool'))\n",
    "\n",
    "    # Block 3\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block3_conv1'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block3_conv2'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block3_conv3'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block3_conv4'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool'))\n",
    "\n",
    "    # Block 4\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block4_conv1'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block4_conv2'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block4_conv3'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block4_conv4'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool'))\n",
    "\n",
    "    # Block 5\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block5_conv1'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block5_conv2'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block5_conv3'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='block5_conv4'))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # model modification for cifar-10\n",
    "    model.add(Flatten(name='flatten'))\n",
    "    model.add(Dense(4096, use_bias = True, kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='fc_cifa10'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(4096, kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='fc2'))  \n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout))      \n",
    "    model.add(Dense(10, kernel_regularizer=keras.regularizers.l2(weight_decay), kernel_initializer=he_normal(), name='predictions_cifa10'))        \n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data and build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "fc_cifa10 (Dense)            (None, 4096)              8392704   \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "predictions_cifa10 (Dense)   (None, 10)                40970     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 45,239,370\n",
      "Trainable params: 45,239,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "# color preprocessing\n",
    "x_train, x_test = color_preprocessing(x_train, x_test)\n",
    "\n",
    "# build network\n",
    "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "filepath = get_file('vgg19_weights_tf_dim_ordering_tf_kernels.h5', WEIGHTS_PATH, cache_subdir='models')\n",
    "\n",
    "model = VGG19()\n",
    "print(model.summary())\n",
    "\n",
    "# load pretrained weight from VGG19 by name      \n",
    "model.load_weights(filepath, by_name=True)\n",
    "\n",
    "# -------- optimizer setting -------- #\n",
    "sgd = optimizers.SGD(lr=.1, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb_cb = TensorBoard(log_dir=log_filepath, histogram_freq=0)\n",
    "change_lr = LearningRateScheduler(scheduler)\n",
    "cbks = [change_lr,tb_cb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# processing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n"
     ]
    }
   ],
   "source": [
    "print('Using real-time data augmentation.')\n",
    "datagen = ImageDataGenerator(horizontal_flip=True,\n",
    "        width_shift_range=0.125,height_shift_range=0.125,fill_mode='constant',cval=0.)\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/170\n",
      "391/391 [==============================] - 48s - loss: 6.8878 - acc: 0.6139 - val_loss: 3.5075 - val_acc: 0.4492\n",
      "Epoch 2/170\n",
      "391/391 [==============================] - 45s - loss: 2.1108 - acc: 0.6682 - val_loss: 2.2759 - val_acc: 0.4886\n",
      "Epoch 3/170\n",
      "391/391 [==============================] - 45s - loss: 1.6630 - acc: 0.6808 - val_loss: 2.8778 - val_acc: 0.3666\n",
      "Epoch 4/170\n",
      "391/391 [==============================] - 45s - loss: 1.6069 - acc: 0.6889 - val_loss: 2.2547 - val_acc: 0.4986\n",
      "Epoch 5/170\n",
      "391/391 [==============================] - 45s - loss: 1.5893 - acc: 0.6978 - val_loss: 1.7269 - val_acc: 0.6550\n",
      "Epoch 6/170\n",
      "391/391 [==============================] - 45s - loss: 1.5958 - acc: 0.7011 - val_loss: 2.6924 - val_acc: 0.3892\n",
      "Epoch 7/170\n",
      "391/391 [==============================] - 45s - loss: 1.5822 - acc: 0.7062 - val_loss: 2.4918 - val_acc: 0.4452\n",
      "Epoch 8/170\n",
      "391/391 [==============================] - 45s - loss: 1.5929 - acc: 0.7060 - val_loss: 2.1708 - val_acc: 0.5210\n",
      "Epoch 9/170\n",
      "391/391 [==============================] - 45s - loss: 1.5911 - acc: 0.7083 - val_loss: 1.9936 - val_acc: 0.5769\n",
      "Epoch 10/170\n",
      "391/391 [==============================] - 45s - loss: 1.5936 - acc: 0.7140 - val_loss: 2.0213 - val_acc: 0.5507\n",
      "Epoch 11/170\n",
      "391/391 [==============================] - 45s - loss: 1.5887 - acc: 0.7147 - val_loss: 2.2894 - val_acc: 0.4830\n",
      "Epoch 12/170\n",
      "391/391 [==============================] - 45s - loss: 1.5989 - acc: 0.7148 - val_loss: 3.1557 - val_acc: 0.3229\n",
      "Epoch 13/170\n",
      "391/391 [==============================] - 45s - loss: 1.5874 - acc: 0.7194 - val_loss: 2.4012 - val_acc: 0.5076\n",
      "Epoch 14/170\n",
      "391/391 [==============================] - 45s - loss: 1.6052 - acc: 0.7156 - val_loss: 3.7248 - val_acc: 0.2863\n",
      "Epoch 15/170\n",
      "391/391 [==============================] - 45s - loss: 1.5982 - acc: 0.7158 - val_loss: 2.7553 - val_acc: 0.3899\n",
      "Epoch 16/170\n",
      "391/391 [==============================] - 45s - loss: 1.6067 - acc: 0.7190 - val_loss: 2.3964 - val_acc: 0.4573\n",
      "Epoch 17/170\n",
      "391/391 [==============================] - 45s - loss: 1.6110 - acc: 0.7228 - val_loss: 2.5093 - val_acc: 0.4269\n",
      "Epoch 18/170\n",
      "391/391 [==============================] - 45s - loss: 1.6119 - acc: 0.7208 - val_loss: 2.9080 - val_acc: 0.4681\n",
      "Epoch 19/170\n",
      "391/391 [==============================] - 45s - loss: 1.6164 - acc: 0.7209 - val_loss: 2.1174 - val_acc: 0.5694\n",
      "Epoch 20/170\n",
      "391/391 [==============================] - 44s - loss: 1.6058 - acc: 0.7242 - val_loss: 2.7051 - val_acc: 0.3764\n",
      "Epoch 21/170\n",
      "391/391 [==============================] - 44s - loss: 1.6146 - acc: 0.7236 - val_loss: 3.2420 - val_acc: 0.3711\n",
      "Epoch 22/170\n",
      "391/391 [==============================] - 45s - loss: 1.6265 - acc: 0.7227 - val_loss: 2.4697 - val_acc: 0.4456\n",
      "Epoch 23/170\n",
      "391/391 [==============================] - 45s - loss: 1.6153 - acc: 0.7249 - val_loss: 2.4554 - val_acc: 0.4455\n",
      "Epoch 24/170\n",
      "391/391 [==============================] - 44s - loss: 1.6359 - acc: 0.7216 - val_loss: 2.8173 - val_acc: 0.3943\n",
      "Epoch 25/170\n",
      "391/391 [==============================] - 44s - loss: 1.6264 - acc: 0.7268 - val_loss: 2.0038 - val_acc: 0.5731\n",
      "Epoch 26/170\n",
      "391/391 [==============================] - 45s - loss: 1.6331 - acc: 0.7253 - val_loss: 2.4279 - val_acc: 0.4687\n",
      "Epoch 27/170\n",
      "391/391 [==============================] - 45s - loss: 1.6361 - acc: 0.7252 - val_loss: 2.9963 - val_acc: 0.3576\n",
      "Epoch 28/170\n",
      "391/391 [==============================] - 45s - loss: 1.6291 - acc: 0.7271 - val_loss: 2.4251 - val_acc: 0.5070\n",
      "Epoch 29/170\n",
      "391/391 [==============================] - 44s - loss: 1.6352 - acc: 0.7280 - val_loss: 2.4878 - val_acc: 0.4652\n",
      "Epoch 30/170\n",
      "391/391 [==============================] - 44s - loss: 1.6241 - acc: 0.7294 - val_loss: 2.2547 - val_acc: 0.5298\n",
      "Epoch 31/170\n",
      "391/391 [==============================] - 44s - loss: 1.6290 - acc: 0.7270 - val_loss: 2.0977 - val_acc: 0.5632\n",
      "Epoch 32/170\n",
      "391/391 [==============================] - 45s - loss: 1.6315 - acc: 0.7270 - val_loss: 2.7400 - val_acc: 0.3834\n",
      "Epoch 33/170\n",
      "391/391 [==============================] - 44s - loss: 1.6283 - acc: 0.7262 - val_loss: 2.0775 - val_acc: 0.5628\n",
      "Epoch 34/170\n",
      "391/391 [==============================] - 44s - loss: 1.6155 - acc: 0.7297 - val_loss: 2.4855 - val_acc: 0.4750\n",
      "Epoch 35/170\n",
      "391/391 [==============================] - 45s - loss: 1.6210 - acc: 0.7299 - val_loss: 2.5215 - val_acc: 0.4396\n",
      "Epoch 36/170\n",
      "391/391 [==============================] - 45s - loss: 1.6286 - acc: 0.7279 - val_loss: 3.1068 - val_acc: 0.3895\n",
      "Epoch 37/170\n",
      "391/391 [==============================] - 45s - loss: 1.6404 - acc: 0.7284 - val_loss: 2.4683 - val_acc: 0.4433\n",
      "Epoch 38/170\n",
      "391/391 [==============================] - 45s - loss: 1.6233 - acc: 0.7314 - val_loss: 2.2438 - val_acc: 0.5579\n",
      "Epoch 39/170\n",
      "391/391 [==============================] - 45s - loss: 1.6292 - acc: 0.7286 - val_loss: 2.1861 - val_acc: 0.5306\n",
      "Epoch 40/170\n",
      "391/391 [==============================] - 45s - loss: 1.6136 - acc: 0.7328 - val_loss: 2.4955 - val_acc: 0.4440\n",
      "Epoch 41/170\n",
      "391/391 [==============================] - 45s - loss: 1.6273 - acc: 0.7292 - val_loss: 2.6906 - val_acc: 0.4575\n",
      "Epoch 42/170\n",
      "391/391 [==============================] - 45s - loss: 1.6268 - acc: 0.7293 - val_loss: 2.3609 - val_acc: 0.4941\n",
      "Epoch 43/170\n",
      "391/391 [==============================] - 45s - loss: 1.6183 - acc: 0.7318 - val_loss: 2.4815 - val_acc: 0.4693\n",
      "Epoch 44/170\n",
      "391/391 [==============================] - 45s - loss: 1.6233 - acc: 0.7293 - val_loss: 2.3069 - val_acc: 0.4750\n",
      "Epoch 45/170\n",
      "391/391 [==============================] - 45s - loss: 1.6239 - acc: 0.7300 - val_loss: 2.5219 - val_acc: 0.4626\n",
      "Epoch 46/170\n",
      "391/391 [==============================] - 45s - loss: 1.6120 - acc: 0.7337 - val_loss: 2.0765 - val_acc: 0.5481\n",
      "Epoch 47/170\n",
      "391/391 [==============================] - 44s - loss: 1.6070 - acc: 0.7325 - val_loss: 2.5972 - val_acc: 0.4426\n",
      "Epoch 48/170\n",
      "391/391 [==============================] - 45s - loss: 1.6140 - acc: 0.7302 - val_loss: 2.3313 - val_acc: 0.5036\n",
      "Epoch 49/170\n",
      "391/391 [==============================] - 45s - loss: 1.6109 - acc: 0.7314 - val_loss: 2.4352 - val_acc: 0.4775\n",
      "Epoch 50/170\n",
      "391/391 [==============================] - 45s - loss: 1.6054 - acc: 0.7344 - val_loss: 1.9621 - val_acc: 0.5994\n",
      "Epoch 51/170\n",
      "391/391 [==============================] - 44s - loss: 1.6300 - acc: 0.7307 - val_loss: 6.0263 - val_acc: 0.1273\n",
      "Epoch 52/170\n",
      "391/391 [==============================] - 45s - loss: 1.6103 - acc: 0.7311 - val_loss: 2.5825 - val_acc: 0.4361\n",
      "Epoch 53/170\n",
      "391/391 [==============================] - 44s - loss: 1.6193 - acc: 0.7336 - val_loss: 2.3532 - val_acc: 0.4965\n",
      "Epoch 54/170\n",
      "391/391 [==============================] - 45s - loss: 1.6151 - acc: 0.7342 - val_loss: 2.8765 - val_acc: 0.3680\n",
      "Epoch 55/170\n",
      "391/391 [==============================] - 45s - loss: 1.6219 - acc: 0.7302 - val_loss: 2.3603 - val_acc: 0.5124\n",
      "Epoch 56/170\n",
      "391/391 [==============================] - 45s - loss: 1.6107 - acc: 0.7304 - val_loss: 2.5536 - val_acc: 0.4310\n",
      "Epoch 57/170\n",
      "391/391 [==============================] - 45s - loss: 1.6155 - acc: 0.7341 - val_loss: 2.4213 - val_acc: 0.4873\n",
      "Epoch 58/170\n",
      "391/391 [==============================] - 45s - loss: 1.5933 - acc: 0.7342 - val_loss: 2.5767 - val_acc: 0.3996\n",
      "Epoch 59/170\n",
      "391/391 [==============================] - 45s - loss: 1.6072 - acc: 0.7293 - val_loss: 2.7264 - val_acc: 0.4714\n",
      "Epoch 60/170\n",
      "391/391 [==============================] - 44s - loss: 1.6019 - acc: 0.7347 - val_loss: 2.1024 - val_acc: 0.5680\n",
      "Epoch 61/170\n",
      "391/391 [==============================] - 44s - loss: 1.6020 - acc: 0.7320 - val_loss: 3.4962 - val_acc: 0.3448\n",
      "Epoch 62/170\n",
      "391/391 [==============================] - 44s - loss: 1.6017 - acc: 0.7343 - val_loss: 3.0863 - val_acc: 0.3589\n",
      "Epoch 63/170\n",
      "391/391 [==============================] - 45s - loss: 1.5970 - acc: 0.7303 - val_loss: 2.0296 - val_acc: 0.5620\n",
      "Epoch 64/170\n",
      "391/391 [==============================] - 45s - loss: 1.5918 - acc: 0.7337 - val_loss: 2.1089 - val_acc: 0.5558\n",
      "Epoch 65/170\n",
      "391/391 [==============================] - 45s - loss: 1.6085 - acc: 0.7278 - val_loss: 1.9820 - val_acc: 0.5989\n",
      "Epoch 66/170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 45s - loss: 1.5918 - acc: 0.7330 - val_loss: 2.7357 - val_acc: 0.4175\n",
      "Epoch 67/170\n",
      "391/391 [==============================] - 45s - loss: 1.5946 - acc: 0.7336 - val_loss: 2.5758 - val_acc: 0.3925\n",
      "Epoch 68/170\n",
      "391/391 [==============================] - 45s - loss: 1.5931 - acc: 0.7326 - val_loss: 2.8285 - val_acc: 0.3382\n",
      "Epoch 69/170\n",
      "391/391 [==============================] - 45s - loss: 1.5978 - acc: 0.7331 - val_loss: 2.5085 - val_acc: 0.4478\n",
      "Epoch 70/170\n",
      "391/391 [==============================] - 45s - loss: 1.5879 - acc: 0.7353 - val_loss: 2.4009 - val_acc: 0.4973\n",
      "Epoch 71/170\n",
      "391/391 [==============================] - 45s - loss: 1.5942 - acc: 0.7368 - val_loss: 2.4004 - val_acc: 0.4152\n",
      "Epoch 72/170\n",
      "391/391 [==============================] - 45s - loss: 1.5944 - acc: 0.7313 - val_loss: 2.1285 - val_acc: 0.5658\n",
      "Epoch 73/170\n",
      "391/391 [==============================] - 44s - loss: 1.5846 - acc: 0.7349 - val_loss: 2.0390 - val_acc: 0.5717\n",
      "Epoch 74/170\n",
      "391/391 [==============================] - 44s - loss: 1.5990 - acc: 0.7326 - val_loss: 2.3853 - val_acc: 0.4744\n",
      "Epoch 75/170\n",
      "391/391 [==============================] - 44s - loss: 1.5958 - acc: 0.7358 - val_loss: 2.6013 - val_acc: 0.4367\n",
      "Epoch 76/170\n",
      "391/391 [==============================] - 44s - loss: 1.6033 - acc: 0.7332 - val_loss: 2.5766 - val_acc: 0.4145\n",
      "Epoch 77/170\n",
      "391/391 [==============================] - 44s - loss: 1.6014 - acc: 0.7324 - val_loss: 3.2699 - val_acc: 0.3168\n",
      "Epoch 78/170\n",
      "391/391 [==============================] - 44s - loss: 1.5880 - acc: 0.7332 - val_loss: 2.2472 - val_acc: 0.5362\n",
      "Epoch 79/170\n",
      "391/391 [==============================] - 44s - loss: 1.5973 - acc: 0.7338 - val_loss: 2.6982 - val_acc: 0.3678\n",
      "Epoch 80/170\n",
      "391/391 [==============================] - 45s - loss: 1.5955 - acc: 0.7320 - val_loss: 2.3848 - val_acc: 0.5045\n",
      "Epoch 81/170\n",
      "391/391 [==============================] - 45s - loss: 1.5764 - acc: 0.7337 - val_loss: 2.0337 - val_acc: 0.5618\n",
      "Epoch 82/170\n",
      "391/391 [==============================] - 45s - loss: 1.3197 - acc: 0.8007 - val_loss: 1.3066 - val_acc: 0.7887\n",
      "Epoch 83/170\n",
      "391/391 [==============================] - 45s - loss: 1.0416 - acc: 0.8472 - val_loss: 1.1027 - val_acc: 0.8112\n",
      "Epoch 84/170\n",
      "391/391 [==============================] - 45s - loss: 0.8996 - acc: 0.8631 - val_loss: 1.0225 - val_acc: 0.8130\n",
      "Epoch 85/170\n",
      "391/391 [==============================] - 45s - loss: 0.8106 - acc: 0.8702 - val_loss: 0.9317 - val_acc: 0.8239\n",
      "Epoch 86/170\n",
      "391/391 [==============================] - 45s - loss: 0.7498 - acc: 0.8741 - val_loss: 0.9299 - val_acc: 0.8169\n",
      "Epoch 87/170\n",
      "391/391 [==============================] - 45s - loss: 0.7157 - acc: 0.8774 - val_loss: 0.8181 - val_acc: 0.8343\n",
      "Epoch 88/170\n",
      "391/391 [==============================] - 45s - loss: 0.6914 - acc: 0.8801 - val_loss: 0.9621 - val_acc: 0.7938\n",
      "Epoch 89/170\n",
      "391/391 [==============================] - 45s - loss: 0.6827 - acc: 0.8805 - val_loss: 0.9323 - val_acc: 0.8049\n",
      "Epoch 90/170\n",
      "391/391 [==============================] - 45s - loss: 0.6788 - acc: 0.8824 - val_loss: 0.9320 - val_acc: 0.8006\n",
      "Epoch 91/170\n",
      "391/391 [==============================] - 45s - loss: 0.6720 - acc: 0.8844 - val_loss: 0.8394 - val_acc: 0.8367\n",
      "Epoch 92/170\n",
      "391/391 [==============================] - 45s - loss: 0.6748 - acc: 0.8859 - val_loss: 1.1221 - val_acc: 0.7608\n",
      "Epoch 93/170\n",
      "391/391 [==============================] - 45s - loss: 0.6697 - acc: 0.8870 - val_loss: 0.9130 - val_acc: 0.8160\n",
      "Epoch 94/170\n",
      "391/391 [==============================] - 45s - loss: 0.6677 - acc: 0.8917 - val_loss: 0.8731 - val_acc: 0.8317\n",
      "Epoch 95/170\n",
      "391/391 [==============================] - 45s - loss: 0.6697 - acc: 0.8935 - val_loss: 0.9115 - val_acc: 0.8248\n",
      "Epoch 96/170\n",
      "391/391 [==============================] - 45s - loss: 0.6743 - acc: 0.8936 - val_loss: 0.9579 - val_acc: 0.8131\n",
      "Epoch 97/170\n",
      "391/391 [==============================] - 45s - loss: 0.6739 - acc: 0.8966 - val_loss: 0.9413 - val_acc: 0.8205\n",
      "Epoch 98/170\n",
      "391/391 [==============================] - 45s - loss: 0.6797 - acc: 0.8953 - val_loss: 0.9161 - val_acc: 0.8226\n",
      "Epoch 99/170\n",
      "391/391 [==============================] - 45s - loss: 0.6807 - acc: 0.8975 - val_loss: 0.8980 - val_acc: 0.8344\n",
      "Epoch 100/170\n",
      "391/391 [==============================] - 45s - loss: 0.6808 - acc: 0.8979 - val_loss: 0.8584 - val_acc: 0.8465\n",
      "Epoch 101/170\n",
      "391/391 [==============================] - 45s - loss: 0.6795 - acc: 0.8994 - val_loss: 0.8751 - val_acc: 0.8359\n",
      "Epoch 102/170\n",
      "391/391 [==============================] - 45s - loss: 0.6771 - acc: 0.9008 - val_loss: 0.8976 - val_acc: 0.8302\n",
      "Epoch 103/170\n",
      "391/391 [==============================] - 45s - loss: 0.6727 - acc: 0.9042 - val_loss: 0.9247 - val_acc: 0.8257\n",
      "Epoch 104/170\n",
      "391/391 [==============================] - 45s - loss: 0.6802 - acc: 0.9022 - val_loss: 0.8823 - val_acc: 0.8394\n",
      "Epoch 105/170\n",
      "391/391 [==============================] - 45s - loss: 0.6807 - acc: 0.9036 - val_loss: 1.1629 - val_acc: 0.7608\n",
      "Epoch 106/170\n",
      "391/391 [==============================] - 46s - loss: 0.6805 - acc: 0.9058 - val_loss: 0.9777 - val_acc: 0.8223\n",
      "Epoch 107/170\n",
      "391/391 [==============================] - 46s - loss: 0.6783 - acc: 0.9081 - val_loss: 0.8959 - val_acc: 0.8405\n",
      "Epoch 108/170\n",
      "391/391 [==============================] - 46s - loss: 0.6792 - acc: 0.9064 - val_loss: 1.0403 - val_acc: 0.7902\n",
      "Epoch 109/170\n",
      "391/391 [==============================] - 46s - loss: 0.6861 - acc: 0.9061 - val_loss: 1.2975 - val_acc: 0.7517\n",
      "Epoch 110/170\n",
      "391/391 [==============================] - 46s - loss: 0.6854 - acc: 0.9071 - val_loss: 0.8764 - val_acc: 0.8512\n",
      "Epoch 111/170\n",
      "391/391 [==============================] - 46s - loss: 0.6892 - acc: 0.9067 - val_loss: 0.9311 - val_acc: 0.8287\n",
      "Epoch 112/170\n",
      "391/391 [==============================] - 46s - loss: 0.6824 - acc: 0.9104 - val_loss: 1.1703 - val_acc: 0.7850\n",
      "Epoch 113/170\n",
      "391/391 [==============================] - 46s - loss: 0.6943 - acc: 0.9091 - val_loss: 0.9857 - val_acc: 0.8165\n",
      "Epoch 114/170\n",
      "391/391 [==============================] - 46s - loss: 0.6943 - acc: 0.9074 - val_loss: 1.0189 - val_acc: 0.8120\n",
      "Epoch 115/170\n",
      "391/391 [==============================] - 46s - loss: 0.6908 - acc: 0.9085 - val_loss: 0.9910 - val_acc: 0.8250\n",
      "Epoch 116/170\n",
      "391/391 [==============================] - 46s - loss: 0.6954 - acc: 0.9096 - val_loss: 1.1163 - val_acc: 0.7998\n",
      "Epoch 117/170\n",
      "391/391 [==============================] - 46s - loss: 0.6955 - acc: 0.9103 - val_loss: 1.3160 - val_acc: 0.7321\n",
      "Epoch 118/170\n",
      "391/391 [==============================] - 46s - loss: 0.6826 - acc: 0.9137 - val_loss: 0.9815 - val_acc: 0.8312\n",
      "Epoch 119/170\n",
      "391/391 [==============================] - 46s - loss: 0.6953 - acc: 0.9113 - val_loss: 1.1597 - val_acc: 0.7902\n",
      "Epoch 120/170\n",
      "391/391 [==============================] - 46s - loss: 0.6941 - acc: 0.9125 - val_loss: 1.0074 - val_acc: 0.8210\n",
      "Epoch 121/170\n",
      "391/391 [==============================] - 46s - loss: 0.6919 - acc: 0.9138 - val_loss: 1.0106 - val_acc: 0.8271\n",
      "Epoch 122/170\n",
      "391/391 [==============================] - 46s - loss: 0.5955 - acc: 0.9437 - val_loss: 0.7098 - val_acc: 0.9073\n",
      "Epoch 123/170\n",
      "391/391 [==============================] - 46s - loss: 0.5343 - acc: 0.9617 - val_loss: 0.6960 - val_acc: 0.9124\n",
      "Epoch 124/170\n",
      "391/391 [==============================] - 46s - loss: 0.5044 - acc: 0.9682 - val_loss: 0.6885 - val_acc: 0.9139\n",
      "Epoch 125/170\n",
      "391/391 [==============================] - 47s - loss: 0.4797 - acc: 0.9739 - val_loss: 0.6774 - val_acc: 0.9169\n",
      "Epoch 126/170\n",
      "391/391 [==============================] - 46s - loss: 0.4663 - acc: 0.9757 - val_loss: 0.6719 - val_acc: 0.9164\n",
      "Epoch 127/170\n",
      "391/391 [==============================] - 46s - loss: 0.4471 - acc: 0.9785 - val_loss: 0.6743 - val_acc: 0.9170\n",
      "Epoch 128/170\n",
      "391/391 [==============================] - 46s - loss: 0.4344 - acc: 0.9799 - val_loss: 0.6679 - val_acc: 0.9159\n",
      "Epoch 129/170\n",
      "391/391 [==============================] - 46s - loss: 0.4204 - acc: 0.9825 - val_loss: 0.6741 - val_acc: 0.9156\n",
      "Epoch 130/170\n",
      "391/391 [==============================] - 46s - loss: 0.4094 - acc: 0.9837 - val_loss: 0.6752 - val_acc: 0.9131\n",
      "Epoch 131/170\n",
      "391/391 [==============================] - 46s - loss: 0.3990 - acc: 0.9834 - val_loss: 0.6565 - val_acc: 0.9187\n",
      "Epoch 132/170\n",
      "391/391 [==============================] - 46s - loss: 0.3864 - acc: 0.9863 - val_loss: 0.6580 - val_acc: 0.9155\n",
      "Epoch 133/170\n",
      "391/391 [==============================] - 46s - loss: 0.3786 - acc: 0.9860 - val_loss: 0.6555 - val_acc: 0.9149\n",
      "Epoch 134/170\n",
      "391/391 [==============================] - 46s - loss: 0.3665 - acc: 0.9875 - val_loss: 0.6570 - val_acc: 0.9181\n",
      "Epoch 135/170\n",
      "391/391 [==============================] - 46s - loss: 0.3591 - acc: 0.9882 - val_loss: 0.6614 - val_acc: 0.9162\n",
      "Epoch 136/170\n",
      "391/391 [==============================] - 46s - loss: 0.3521 - acc: 0.9884 - val_loss: 0.6504 - val_acc: 0.9131\n",
      "Epoch 137/170\n",
      "391/391 [==============================] - 46s - loss: 0.3455 - acc: 0.9886 - val_loss: 0.6467 - val_acc: 0.9160\n",
      "Epoch 138/170\n",
      "391/391 [==============================] - 46s - loss: 0.3391 - acc: 0.9883 - val_loss: 0.6419 - val_acc: 0.9129\n",
      "Epoch 139/170\n",
      "391/391 [==============================] - 46s - loss: 0.3317 - acc: 0.9890 - val_loss: 0.6316 - val_acc: 0.9160\n",
      "Epoch 140/170\n",
      "391/391 [==============================] - 46s - loss: 0.3207 - acc: 0.9908 - val_loss: 0.6438 - val_acc: 0.9157\n",
      "Epoch 141/170\n",
      "391/391 [==============================] - 46s - loss: 0.3163 - acc: 0.9901 - val_loss: 0.6209 - val_acc: 0.9165\n",
      "Epoch 142/170\n",
      "391/391 [==============================] - 46s - loss: 0.3082 - acc: 0.9906 - val_loss: 0.6508 - val_acc: 0.9102\n",
      "Epoch 143/170\n",
      "391/391 [==============================] - 46s - loss: 0.3038 - acc: 0.9906 - val_loss: 0.6329 - val_acc: 0.9139\n",
      "Epoch 144/170\n",
      "391/391 [==============================] - 46s - loss: 0.2965 - acc: 0.9914 - val_loss: 0.6455 - val_acc: 0.9114\n",
      "Epoch 145/170\n",
      "391/391 [==============================] - 46s - loss: 0.2947 - acc: 0.9903 - val_loss: 0.6247 - val_acc: 0.9141\n",
      "Epoch 146/170\n",
      "391/391 [==============================] - 46s - loss: 0.2874 - acc: 0.9911 - val_loss: 0.6295 - val_acc: 0.9130\n",
      "Epoch 147/170\n",
      "391/391 [==============================] - 46s - loss: 0.2845 - acc: 0.9905 - val_loss: 0.6222 - val_acc: 0.9148\n",
      "Epoch 148/170\n",
      "391/391 [==============================] - 46s - loss: 0.2780 - acc: 0.9907 - val_loss: 0.6465 - val_acc: 0.9083\n",
      "Epoch 149/170\n",
      "391/391 [==============================] - 46s - loss: 0.2767 - acc: 0.9902 - val_loss: 0.6324 - val_acc: 0.9086\n",
      "Epoch 150/170\n",
      "391/391 [==============================] - 46s - loss: 0.2705 - acc: 0.9909 - val_loss: 0.6128 - val_acc: 0.9123\n",
      "Epoch 151/170\n",
      "391/391 [==============================] - 46s - loss: 0.2638 - acc: 0.9914 - val_loss: 0.5972 - val_acc: 0.9145\n",
      "Epoch 152/170\n",
      "391/391 [==============================] - 46s - loss: 0.2594 - acc: 0.9920 - val_loss: 0.6372 - val_acc: 0.9057\n",
      "Epoch 153/170\n",
      "391/391 [==============================] - 46s - loss: 0.2578 - acc: 0.9914 - val_loss: 0.6167 - val_acc: 0.9078\n",
      "Epoch 154/170\n",
      "391/391 [==============================] - 46s - loss: 0.2545 - acc: 0.9909 - val_loss: 0.6002 - val_acc: 0.9109\n",
      "Epoch 155/170\n",
      "391/391 [==============================] - 46s - loss: 0.2486 - acc: 0.9914 - val_loss: 0.5844 - val_acc: 0.9128\n",
      "Epoch 156/170\n",
      "391/391 [==============================] - 46s - loss: 0.2452 - acc: 0.9919 - val_loss: 0.6113 - val_acc: 0.9065\n",
      "Epoch 157/170\n",
      "391/391 [==============================] - 46s - loss: 0.2412 - acc: 0.9916 - val_loss: 0.5785 - val_acc: 0.9151\n",
      "Epoch 158/170\n",
      "391/391 [==============================] - 46s - loss: 0.2371 - acc: 0.9922 - val_loss: 0.6072 - val_acc: 0.9115\n",
      "Epoch 159/170\n",
      "391/391 [==============================] - 46s - loss: 0.2401 - acc: 0.9900 - val_loss: 0.6420 - val_acc: 0.9013\n",
      "Epoch 160/170\n",
      "391/391 [==============================] - 46s - loss: 0.2368 - acc: 0.9902 - val_loss: 0.5932 - val_acc: 0.9095\n",
      "Epoch 161/170\n",
      "391/391 [==============================] - 46s - loss: 0.2341 - acc: 0.9900 - val_loss: 0.5711 - val_acc: 0.9102\n",
      "Epoch 162/170\n",
      "391/391 [==============================] - 46s - loss: 0.2278 - acc: 0.9908 - val_loss: 0.6049 - val_acc: 0.9042\n",
      "Epoch 163/170\n",
      "391/391 [==============================] - 46s - loss: 0.2269 - acc: 0.9907 - val_loss: 0.5716 - val_acc: 0.9111\n",
      "Epoch 164/170\n",
      "391/391 [==============================] - 46s - loss: 0.2238 - acc: 0.9911 - val_loss: 0.6103 - val_acc: 0.8998\n",
      "Epoch 165/170\n",
      "391/391 [==============================] - 46s - loss: 0.2233 - acc: 0.9898 - val_loss: 0.5775 - val_acc: 0.9070\n",
      "Epoch 166/170\n",
      "391/391 [==============================] - 46s - loss: 0.2215 - acc: 0.9903 - val_loss: 0.5826 - val_acc: 0.9059\n",
      "Epoch 167/170\n",
      "391/391 [==============================] - 46s - loss: 0.2164 - acc: 0.9907 - val_loss: 0.5751 - val_acc: 0.9098\n",
      "Epoch 168/170\n",
      "391/391 [==============================] - 46s - loss: 0.2127 - acc: 0.9913 - val_loss: 0.5643 - val_acc: 0.9147\n",
      "Epoch 169/170\n",
      "391/391 [==============================] - 46s - loss: 0.2126 - acc: 0.9903 - val_loss: 0.6312 - val_acc: 0.8973\n",
      "Epoch 170/170\n",
      "391/391 [==============================] - 46s - loss: 0.2121 - acc: 0.9902 - val_loss: 0.5702 - val_acc: 0.9060\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n",
    "                    steps_per_epoch=iterations,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=cbks,\n",
    "                    validation_data=(x_test, y_test))\n",
    "model.save('vgg19_retrain.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
