{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Summarization of Medical Articles\n",
    "### Author: Abhijit V Thatte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution:__ We will use TextRank for automatic summarization of medical articles. NIH's (National Institues for Health) PubMed repository consists of links to hundreds of thousands of medical articles. We will use articles relevant to various types of cancer. We will use the abstract of each article as the \"ground truth\". We will apply the TextRank algorithm to only the body of the PubMed article without the abstract to generate an extractive summary. We will use a Java based implementation of ROUGE software to evaluate the precision, recall and F1 score of extractive summary with respect to the ground truth.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 1: Import required modules__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import networkx as nx\n",
    "import re\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2: Generate a list of documents__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urls = []\n",
    "\n",
    "#https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1994795/\n",
    "urls.append('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=1994795')\n",
    "\n",
    "#https://www.ncbi.nlm.nih.gov/pmc/articles/PMC314300/\n",
    "urls.append('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=314300')\n",
    "\n",
    "#https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4383356/\n",
    "urls.append('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=4383356')\n",
    "\n",
    "#https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4596899/\n",
    "urls.append('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=4596899')\n",
    "\n",
    "#https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4303126/\n",
    "urls.append('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=4303126')\n",
    "\n",
    "#https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4637461/\n",
    "urls.append('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=4637461')\n",
    "\n",
    "#https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4690355/\n",
    "urls.append('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=4690355')\n",
    "\n",
    "#https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3505152/\n",
    "urls.append('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=3505152')\n",
    "\n",
    "#https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3976810/\n",
    "urls.append('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=3976810')\n",
    "\n",
    "#https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4061037/\n",
    "urls.append('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=4061037')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 3: Preprocess the documents__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing documents. This may take few minutes ...\n",
      "Preprocessing document 1 ...\n",
      "Preprocessing document 2 ...\n",
      "Preprocessing document 3 ...\n",
      "Preprocessing document 4 ...\n",
      "Preprocessing document 5 ...\n",
      "Preprocessing document 6 ...\n",
      "Preprocessing document 7 ...\n",
      "Preprocessing document 8 ...\n",
      "Preprocessing document 9 ...\n",
      "Preprocessing document 10 ...\n",
      "All documents preprocessed successfully.\n",
      "We have 10 documents with 10 abstracts and 10 texts.\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "abstracts = []\n",
    "texts = []\n",
    "\n",
    "print 'Preprocessing documents. This may take few minutes ...'\n",
    "for i, url in enumerate(urls):\n",
    "    print 'Preprocessing document %d ...' % (i+1)\n",
    "    # Download the document\n",
    "    my_url = urllib2.urlopen(url)\n",
    "    raw_doc = BeautifulSoup(my_url.read(), 'xml')\n",
    "    documents.append(raw_doc)\n",
    "\n",
    "    # Extract the cleaned abstract\n",
    "    raw_abstract = raw_doc.abstract\n",
    "    my_abstract = re.sub(r'<\\/?\\w+>', r' ', str(raw_abstract)) # remove xml tags\n",
    "    abstracts.append(my_abstract)\n",
    "\n",
    "    # Extract the cleaned text\n",
    "    text = raw_doc.body\n",
    "    text = re.sub(r'\\\\n', r' ', str(text)) # remove newline characters\n",
    "    text = re.sub(r'<[^>]+>', r' ', str(text)) # remove xml tags\n",
    "    text = re.sub(r'\\[[^\\[^\\]]+\\]', r' ', str(text)) # remove references\n",
    "    text = re.sub(r'\\[', r' ', str(text)) # remove any remaining [\n",
    "    text = re.sub(r'\\]', r' ', str(text)) # remove any remaining ]\n",
    "    text = re.sub(r'[\\s]{2,}', r' ', str(text)) # remove more than a single blank space\n",
    "    text = re.sub(r'\\.\\s+,\\s+\\S', r' ', str(text)) # remove , after a period\n",
    "\n",
    "    text = text.decode('utf-8')\n",
    "    texts.append(text)\n",
    "\n",
    "print 'All documents preprocessed successfully.'\n",
    "print 'We have %d documents with %d abstracts and %d texts.' % (len(documents), len(abstracts), len(texts))\n",
    "assert len(documents) == len(abstracts)\n",
    "assert len(documents) == len(texts)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Step 4: Split the documents into sentences__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punkttokenizer = PunktSentenceTokenizer()\n",
    "text_sentences = []\n",
    "\n",
    "for text in texts:\n",
    "    sentences =  []\n",
    "    seen = set()\n",
    "    for sentence in punkttokenizer.tokenize(text):\n",
    "        sentences.append(sentence)\n",
    "    text_sentences.append(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 5: Count the term frequency for sentences__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating sentence simiarities. This may take few minutes ...\n",
      "Calculating sentence simiarities of document 1 ...\n",
      "Calculating sentence simiarities of document 2 ...\n",
      "Calculating sentence simiarities of document 3 ...\n",
      "Calculating sentence simiarities of document 4 ...\n",
      "Calculating sentence simiarities of document 5 ...\n",
      "Calculating sentence simiarities of document 6 ...\n",
      "Calculating sentence simiarities of document 7 ...\n",
      "Calculating sentence simiarities of document 8 ...\n",
      "Calculating sentence simiarities of document 9 ...\n",
      "Calculating sentence simiarities of document 10 ...\n",
      "All documents processed successfully.\n",
      "We have 10 documents with 10 tf_matrices 10 tfidf_matrices and 10 cosine_similarity_matrices.\n"
     ]
    }
   ],
   "source": [
    "tf_matrices = []\n",
    "tfidf_matrices = []\n",
    "cosine_similarity_matrices = []\n",
    "\n",
    "print 'Calculating sentence simiarities. This may take few minutes ...'\n",
    "for i, sentences in enumerate(text_sentences):\n",
    "    print 'Calculating sentence simiarities of document %d ...' % (i+1)\n",
    "    tf_matrix = CountVectorizer().fit_transform(sentences)\n",
    "    tf_matrices.append(tf_matrix)\n",
    "    \n",
    "    tfidf_matrix = TfidfTransformer().fit_transform(tf_matrix)\n",
    "    tfidf_matrices.append(tfidf_matrix)\n",
    "    \n",
    "    cosine_similarity_matrix = tfidf_matrix * tfidf_matrix.T\n",
    "    cosine_similarity_matrices.append(cosine_similarity_matrix)\n",
    "\n",
    "print 'All documents processed successfully.'\n",
    "print 'We have %d documents with %d tf_matrices %d tfidf_matrices and %d cosine_similarity_matrices.' \\\n",
    "        % (len(documents), len(tf_matrices), len(tfidf_matrices), len(cosine_similarity_matrices))\n",
    "assert len(documents) == len(tf_matrices)\n",
    "assert len(documents) == len(tfidf_matrices)\n",
    "assert len(documents) == len(cosine_similarity_matrices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 6: Calculate TextRank__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating TextRanks. This may take few minutes ...\n",
      "Calculating TextRanks of document 1 ...\n",
      "Calculating TextRanks of document 2 ...\n",
      "Calculating TextRanks of document 3 ...\n",
      "Calculating TextRanks of document 4 ...\n",
      "Calculating TextRanks of document 5 ...\n",
      "Calculating TextRanks of document 6 ...\n",
      "Calculating TextRanks of document 7 ...\n",
      "Calculating TextRanks of document 8 ...\n",
      "Calculating TextRanks of document 9 ...\n",
      "Calculating TextRanks of document 10 ...\n",
      "All documents processed successfully.\n",
      "We have 10 documents with 10 similarity_graphs 10 graph_ranks and 10 highest_ranks.\n"
     ]
    }
   ],
   "source": [
    "similarity_graphs = []\n",
    "graph_ranks = []\n",
    "highest_ranks = []\n",
    "lowest_ranks = []\n",
    "\n",
    "print 'Calculating TextRanks. This may take few minutes ...'\n",
    "for i, cosine_similarity_matrix in enumerate(cosine_similarity_matrices):\n",
    "    print 'Calculating TextRanks of document %d ...' % (i+1)\n",
    "    similarity_graph = nx.from_scipy_sparse_matrix(cosine_similarity_matrix)\n",
    "    similarity_graphs.append(similarity_graph)\n",
    "    \n",
    "    ranks = nx.pagerank(similarity_graph)\n",
    "    graph_ranks.append(ranks)\n",
    "    \n",
    "    highest = sorted(((ranks[j],s) for j,s in enumerate(text_sentences[i])), reverse=True)\n",
    "    highest_ranks.append(highest)\n",
    "    \n",
    "    lowest = sorted(((ranks[j],s) for j,s in enumerate(text_sentences[i])), reverse=False)\n",
    "    lowest_ranks.append(lowest)\n",
    "    \n",
    "print 'All documents processed successfully.'\n",
    "print 'We have %d documents with %d similarity_graphs %d graph_ranks and %d highest_ranks.' \\\n",
    "        % (len(documents), len(similarity_graphs), len(graph_ranks), len(highest_ranks))\n",
    "assert len(documents) == len(similarity_graphs)\n",
    "assert len(documents) == len(graph_ranks)\n",
    "assert len(documents) == len(highest_ranks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 7: Save extractive summaries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving extractive summaries. This may take a few minutes ...\n",
      "Writing extractive summary for document 1 ...\n",
      "Writing extractive summary for document 2 ...\n",
      "Writing extractive summary for document 3 ...\n",
      "Writing extractive summary for document 4 ...\n",
      "Writing extractive summary for document 5 ...\n",
      "Writing extractive summary for document 6 ...\n",
      "Writing extractive summary for document 7 ...\n",
      "Writing extractive summary for document 8 ...\n",
      "Writing extractive summary for document 9 ...\n",
      "Writing extractive summary for document 10 ...\n",
      "All documents processed successfully.\n"
     ]
    }
   ],
   "source": [
    "print 'Saving extractive summaries. This may take a few minutes ...'\n",
    "for i, highest in enumerate(highest_ranks):\n",
    "    print 'Writing extractive summary for document %d ...' % (i+1)\n",
    "    out_file = '\\\\TextRank\\\\system\\\\article%d_system1.txt' % (i+1)\n",
    "    with open(out_file, 'w') as f:\n",
    "        for i in range(5):\n",
    "            f.write((highest[i][1] + '\\n').encode('utf-8'))\n",
    "print 'All documents processed successfully.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Step 8: Save ground truths.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving ground truths. This may take a few minutes ...\n",
      "Writing ground truth for document 1 ...\n",
      "Writing ground truth for document 2 ...\n",
      "Writing ground truth for document 3 ...\n",
      "Writing ground truth for document 4 ...\n",
      "Writing ground truth for document 5 ...\n",
      "Writing ground truth for document 6 ...\n",
      "Writing ground truth for document 7 ...\n",
      "Writing ground truth for document 8 ...\n",
      "Writing ground truth for document 9 ...\n",
      "Writing ground truth for document 10 ...\n",
      "All documents processed successfully.\n"
     ]
    }
   ],
   "source": [
    "print 'Saving ground truths. This may take a few minutes ...'\n",
    "for i, abstract in enumerate(abstracts):\n",
    "    print 'Writing ground truth for document %d ...' % (i+1)\n",
    "    out_file = '\\\\TextRank\\\\reference\\\\article%d_reference1.txt' % (i+1)\n",
    "    with open(out_file, 'w') as f:\n",
    "        f.write(abstract.strip() + '\\n')\n",
    "print 'All documents processed successfully.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 9: Calculate ROUGE score__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\ROUGE\n",
      "0    [main] INFO  com.rxnlp.tools.rouge.SettingsUtil  - Using rouge.properties file specified as 'rouge.properties'\n",
      "\n",
      "========Results=======\n",
      "\n",
      "ROUGE-1+Stemming\tARTICLE6\tSYSTEM1.TXT\tAverage_R:0.45902\tAverage_P:0.55263\tAverage_F:0.50149\tNum Reference Summaries:1\n",
      "ROUGE-1+Stemming\tARTICLE7\tSYSTEM1.TXT\tAverage_R:0.45977\tAverage_P:0.66946\tAverage_F:0.54514\tNum Reference Summaries:1\n",
      "ROUGE-1+Stemming\tARTICLE8\tSYSTEM1.TXT\tAverage_R:0.29877\tAverage_P:0.54751\tAverage_F:0.38658\tNum Reference Summaries:1\n",
      "ROUGE-1+Stemming\tARTICLE9\tSYSTEM1.TXT\tAverage_R:0.33146\tAverage_P:0.35542\tAverage_F:0.34302\tNum Reference Summaries:1\n",
      "ROUGE-1+Stemming\tARTICLE1\tSYSTEM1.TXT\tAverage_R:0.51389\tAverage_P:0.39362\tAverage_F:0.44578\tNum Reference Summaries:1\n",
      "ROUGE-1+Stemming\tARTICLE10\tSYSTEM1.TXT\tAverage_R:0.36593\tAverage_P:0.58000\tAverage_F:0.44874\tNum Reference Summaries:1\n",
      "ROUGE-1+Stemming\tARTICLE2\tSYSTEM1.TXT\tAverage_R:0.47895\tAverage_P:0.48404\tAverage_F:0.48148\tNum Reference Summaries:1\n",
      "ROUGE-1+Stemming\tARTICLE3\tSYSTEM1.TXT\tAverage_R:0.44697\tAverage_P:0.44697\tAverage_F:0.44697\tNum Reference Summaries:1\n",
      "ROUGE-1+Stemming\tARTICLE4\tSYSTEM1.TXT\tAverage_R:0.60982\tAverage_P:0.54128\tAverage_F:0.57351\tNum Reference Summaries:1\n",
      "ROUGE-1+Stemming\tARTICLE5\tSYSTEM1.TXT\tAverage_R:0.45559\tAverage_P:0.57194\tAverage_F:0.50718\tNum Reference Summaries:1\n",
      "\n",
      "======Results End======\n",
      "\n",
      "110  [main] INFO  com.rxnlp.tools.rouge.ROUGECalculator  - Results written to results.csv\n",
      "\n",
      "\n",
      "Please find results file in: results.csv\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\ROUGE\n",
    "!java -jar rouge2.0_0.2.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ROUGE-Type  Task Name  System Name  Avg_Recall  Avg_Precision  \\\n",
      "8  ROUGE-1+Stemming   ARTICLE4  SYSTEM1.TXT     0.60982        0.54128   \n",
      "1  ROUGE-1+Stemming   ARTICLE7  SYSTEM1.TXT     0.45977        0.66946   \n",
      "9  ROUGE-1+Stemming   ARTICLE5  SYSTEM1.TXT     0.45559        0.57194   \n",
      "0  ROUGE-1+Stemming   ARTICLE6  SYSTEM1.TXT     0.45902        0.55263   \n",
      "6  ROUGE-1+Stemming   ARTICLE2  SYSTEM1.TXT     0.47895        0.48404   \n",
      "5  ROUGE-1+Stemming  ARTICLE10  SYSTEM1.TXT     0.36593        0.58000   \n",
      "7  ROUGE-1+Stemming   ARTICLE3  SYSTEM1.TXT     0.44697        0.44697   \n",
      "4  ROUGE-1+Stemming   ARTICLE1  SYSTEM1.TXT     0.51389        0.39362   \n",
      "2  ROUGE-1+Stemming   ARTICLE8  SYSTEM1.TXT     0.29877        0.54751   \n",
      "3  ROUGE-1+Stemming   ARTICLE9  SYSTEM1.TXT     0.33146        0.35542   \n",
      "\n",
      "   Avg_F-Score  Num Reference Summaries  \n",
      "8      0.57351                        1  \n",
      "1      0.54514                        1  \n",
      "9      0.50718                        1  \n",
      "0      0.50149                        1  \n",
      "6      0.48148                        1  \n",
      "5      0.44874                        1  \n",
      "7      0.44697                        1  \n",
      "4      0.44578                        1  \n",
      "2      0.38658                        1  \n",
      "3      0.34302                        1  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('results.csv')\n",
    "print df.sort_values('Avg_F-Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
