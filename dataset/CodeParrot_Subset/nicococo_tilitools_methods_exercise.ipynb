{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Exercise: _Anomaly Detection Methods_\n",
    "***\n",
    "\n",
    "In this exercise, we want to compare existing anomaly detection methods on a variety of real-world multi-variate benchmark datasets. Moreover, we will implement a recent method that is both simple and elegant as well as powerful and fast.\n",
    "\n",
    "1. LODA: Leight-weight Online Detector of Anomalies\n",
    "    - Implement LODA (paper is provided) using the pyOD framework\n",
    "    - Evaluate against competitors\n",
    "2. Unsupervised vs Supervised Outlier Detection\n",
    "    - Implement an regression and classification method (sklearn-based)\n",
    "    - Evaluate and discuss the results on various datasets\n",
    "3. High-dimensional Outlier Detection\n",
    "    - Read the provided material\n",
    "    - Check empirically if the concentration of distances condition is fulfilled\n",
    "\n",
    "\n",
    "### Datasets\n",
    "\n",
    "The datasets used here are available at the Outlier Detection DataSets (ODDS) libraryand can be downloaded from http://odds.cs.stonybrook.edu. Most of the datasets are processed datasets from other sources (e.g. mnist) to meet anomaly detection benchmark standards, e.g. labeled anomalies rather than classes. Further information for each dataset is available at http://odds.cs.stonybrook.edu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import scipy.spatial.distance as dist\n",
    "\n",
    "from pyod.models.base import BaseDetector\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.cblof import CBLOF\n",
    "from pyod.models.feature_bagging import FeatureBagging\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.mcd import MCD\n",
    "from pyod.models.pca import PCA\n",
    "\n",
    "from pyod.utils.utility import standardizer\n",
    "from pyod.utils.utility import precision_n_scores\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LODA: Leight-weight Online Detector of Anomalies\n",
    "\n",
    "In this first round, we will implement a recent, popular anomaly detector based on the pyOD framework\n",
    "(https://github.com/yzhao062/pyod). You should have received a copy of the paper [1] by now and the task is, \n",
    "to read the paper carefully and implement the proposed method into the LODA class skeleton below.\n",
    "\n",
    "[1] T. Pevny, “Loda: Lightweight on-line detector of anomalies,” Mach. Learn., vol. 102, pp. 275–304, 2016.\n",
    "\n",
    "\n",
    "Some hints:\n",
    "\n",
    "- The cumulative sum of a histogram needs to sum to one. However, unless unity width bins are used (which is not the case) and density=True, this will not happen:\n",
    "\n",
    "    _If density=True, the result is the value of the probability density function at the bin, normalized such that the integral over the range is 1. Note that the sum of the histogram values will not be equal to 1 unless bins of unity width are chosen._\n",
    "    \n",
    "    \n",
    "- -log(x) will produce 'inf' values and hence, pseudo-counts are needed before normalizing the histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LODA(BaseDetector):\n",
    "\n",
    "    def __init__(self, contamination=0.1, n_bins=10, n_random_cuts=100, **kwargs):\n",
    "        super(LODA, self).__init__(contamination=contamination)\n",
    "        self.n_bins = n_bins\n",
    "        self.n_random_cuts = n_random_cuts\n",
    "        self.weights = np.ones(n_random_cuts, dtype=np.float) / n_random_cuts\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # TODO\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        pred_scores = np.zeros([X.shape[0], 1])\n",
    "        # TODO\n",
    "        return pred_scores.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data file and read X and y\n",
    "mat_file_list = ['arrhythmia.mat',\n",
    "                 'cardio.mat',\n",
    "                 'glass.mat',\n",
    "                 'ionosphere.mat',\n",
    "                 'letter.mat',\n",
    "                 'lympho.mat',\n",
    "                 'mnist.mat',\n",
    "                 'pendigits.mat',\n",
    "                 'pima.mat',\n",
    "                 'satimage-2.mat',\n",
    "                 'vertebral.mat',\n",
    "                 'vowels.mat',\n",
    "                 'wbc.mat']\n",
    "\n",
    "# mat_file_list = ['ionosphere.mat','arrhythmia.mat']\n",
    "\n",
    "\n",
    "# Define nine outlier detection tools to be compared\n",
    "random_state = np.random.RandomState(42)\n",
    "\n",
    "df_columns = ['Data', '#Samples', '# Dimensions', 'Outlier Perc',\n",
    "              'ABOD', 'CBLOF', 'FB', 'HBOS', 'IForest', 'KNN', 'LOF', 'MCD', 'PCA', 'LODA']\n",
    "roc_df = pd.DataFrame(columns=df_columns)\n",
    "prn_df = pd.DataFrame(columns=df_columns)\n",
    "time_df = pd.DataFrame(columns=df_columns)\n",
    "\n",
    "for mat_file in mat_file_list:\n",
    "    print(\"\\n... Processing\", mat_file, '...')\n",
    "    mat = loadmat(os.path.join('../data', mat_file))\n",
    "\n",
    "    X = mat['X']\n",
    "    y = mat['y'].ravel()\n",
    "    outliers_fraction = np.count_nonzero(y) / len(y)\n",
    "    outliers_percentage = round(outliers_fraction * 100, ndigits=4)\n",
    "\n",
    "    # construct containers for saving results\n",
    "    roc_list = [mat_file[:-4], X.shape[0], X.shape[1], outliers_percentage]\n",
    "    prn_list = [mat_file[:-4], X.shape[0], X.shape[1], outliers_percentage]\n",
    "    time_list = [mat_file[:-4], X.shape[0], X.shape[1], outliers_percentage]\n",
    "\n",
    "    # 60% data for training and 40% for testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=random_state)\n",
    "\n",
    "    # standardizing data for processing\n",
    "    X_train_norm, X_test_norm = standardizer(X_train, X_test)\n",
    "\n",
    "    classifiers = {\n",
    "        'Angle-based Outlier Detector (ABOD)': ABOD(contamination=outliers_fraction),\n",
    "        'Cluster-based Local Outlier Factor': CBLOF(contamination=outliers_fraction, check_estimator=False, random_state=random_state),\n",
    "        'Feature Bagging': FeatureBagging(contamination=outliers_fraction, check_estimator=False, random_state=random_state),\n",
    "        'Histogram-base Outlier Detection (HBOS)': HBOS(contamination=outliers_fraction),\n",
    "        'Isolation Forest': IForest(contamination=outliers_fraction, random_state=random_state),\n",
    "        'K Nearest Neighbors (KNN)': KNN(contamination=outliers_fraction),\n",
    "        'Local Outlier Factor (LOF)': LOF(contamination=outliers_fraction),\n",
    "        'Minimum Covariance Determinant (MCD)': MCD(contamination=outliers_fraction, random_state=random_state),\n",
    "        'Principal Component Analysis (PCA)': PCA(contamination=outliers_fraction, random_state=random_state),\n",
    "        'Lightweight on-line detector of anomalies (LODA)': LODA(contamination=outliers_fraction, random_state=random_state),\n",
    "    }\n",
    "\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        t0 = time()\n",
    "        clf.fit(X_train_norm)\n",
    "        test_scores = clf.decision_function(X_test_norm)\n",
    "        t1 = time()\n",
    "        duration = round(t1 - t0, ndigits=4)\n",
    "        time_list.append(duration)\n",
    "\n",
    "        roc = round(roc_auc_score(y_test, test_scores), ndigits=4)\n",
    "        prn = round(precision_n_scores(y_test, test_scores), ndigits=4)\n",
    "\n",
    "        print('{clf_name} ROC:{roc}, precision @ rank n:{prn}, execution time: {duration}s'.format(\n",
    "            clf_name=clf_name, roc=roc, prn=prn, duration=duration))\n",
    "\n",
    "        roc_list.append(roc)\n",
    "        prn_list.append(prn)\n",
    "\n",
    "    temp_df = pd.DataFrame(time_list).transpose()\n",
    "    temp_df.columns = df_columns\n",
    "    time_df = pd.concat([time_df, temp_df], axis=0)\n",
    "\n",
    "    temp_df = pd.DataFrame(roc_list).transpose()\n",
    "    temp_df.columns = df_columns\n",
    "    roc_df = pd.concat([roc_df, temp_df], axis=0)\n",
    "\n",
    "    temp_df = pd.DataFrame(prn_list).transpose()\n",
    "    temp_df.columns = df_columns\n",
    "    prn_df = pd.concat([prn_df, temp_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ROC Performance')\n",
    "roc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Runtime Performance')\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Unsupervised vs Supervised Outlier Detection\n",
    "\n",
    "In this section, we will examine the difference of unsupervised and supervised method and see what happens \n",
    "if we provide a growing number of labeled examples. Therefore, you have to implement/use two methods of your choice (from sklearn): one regression method and one classifier. Caveat here is that we will not do any kind of model selection after all, the goal is to study the general pattern and not tweak the last percentages of accuracy out of each model. I would advice to use _Ridge_ and _SVC_ from sklearn. \n",
    "\n",
    "Questions:\n",
    "- Check what happens when you use the 'predict' function of the classifier rather than the \"decision_function'\n",
    "- Check the three datasets and discuss the results. When does unsupervised learning works best?\n",
    "- Supervised does not seem too bad, right? Why is it still not a bad idea to apply unsupervised methods?\n",
    "- What would be the best of two worlds, here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_regression(X_train_norm, y_train, X_test_norm, y_test):\n",
    "    # TODO\n",
    "    return -1\n",
    "\n",
    "def train_and_predict_classifier(X_train_norm, y_train, X_test_norm, y_test):\n",
    "    # TODO\n",
    "    return -1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file_list = ['mnist.mat', 'ionosphere.mat', 'satimage-2.mat']\n",
    "\n",
    "reps = 5\n",
    "\n",
    "fig = plt.figure(figsize=(14, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "for f in range(len(mat_file_list)):\n",
    "    mat = loadmat('../data/'+mat_file_list[f])\n",
    "    print('File: ', mat_file_list[f])\n",
    "\n",
    "    X = mat['X']\n",
    "    y = mat['y'].ravel()\n",
    "\n",
    "    # 60% data for training and 40% for testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=random_state)\n",
    "    samples = np.linspace(1, y_train.size, 10, dtype=np.int)\n",
    "\n",
    "    # standardizing data for processing\n",
    "    X_train_norm, X_test_norm = standardizer(X_train, X_test)\n",
    "\n",
    "    rocs = np.zeros((3, len(samples), reps))\n",
    "    for r in range(reps):\n",
    "        for i in range(len(samples)):\n",
    "            inds = np.random.permutation(y_train.size)[:samples[i]]\n",
    "            rocs[0, i, r] = train_and_predict_classifier(X_train_norm[inds, :], y_train[inds], X_test_norm, y_test)\n",
    "            rocs[1, i, r] = train_and_predict_regression(X_train_norm[inds, :], y_train[inds], X_test_norm, y_test)\n",
    "            loda = LODA(contamination=outliers_fraction, random_state=random_state, n_random_cuts=250, n_bins=40)\n",
    "            loda.fit(X_train_norm)\n",
    "            test_scores = loda.decision_function(X_test_norm)\n",
    "            rocs[2, i, r] = round(roc_auc_score(y_test, test_scores), ndigits=4)\n",
    "\n",
    "    res = np.mean(rocs, axis=2)\n",
    "    std = np.std(rocs, axis=2)\n",
    "\n",
    "    plt.subplot(2, len(mat_file_list), f+1)\n",
    "    plt.errorbar(samples, res[0, :], std[0, :], fmt='.-r', elinewidth=1., linewidth=2.)\n",
    "    plt.errorbar(samples, res[1, :], std[1, :], fmt='.-g', elinewidth=1., linewidth=2.)\n",
    "    plt.errorbar(samples, res[2, :], std[2, :], fmt='.-b', elinewidth=1., linewidth=2.)\n",
    "    plt.legend(['Supervised Classifier', 'Supervised Regression', 'Unsupervised Loda'], fontsize=10)\n",
    "    plt.xticks(samples, (samples/np.max(samples)*100.0).astype(np.int) )\n",
    "    plt.xlabel('Percentage of labeled samples', fontsize=14)\n",
    "    plt.ylabel('Performance [in AUC]', fontsize=14)\n",
    "    plt.title(mat_file_list[f], fontsize=14)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(2, len(mat_file_list), f+1+len(mat_file_list))\n",
    "    plt.errorbar(samples, res[0, :], std[0, :], fmt='.-r', elinewidth=1., linewidth=2.)\n",
    "    plt.errorbar(samples, res[1, :], std[1, :], fmt='.-g', elinewidth=1., linewidth=2.)\n",
    "    plt.errorbar(samples, res[2, :], std[2, :], fmt='.-b', elinewidth=1., linewidth=2.)\n",
    "    plt.legend(['Supervised Classifier', 'Supervised Regression', 'Unsupervised Loda'], fontsize=10)\n",
    "    plt.xticks(samples, (samples/np.max(samples)*100.0).astype(np.int) )\n",
    "    plt.xlabel('Percentage of labeled samples [log]', fontsize=14)\n",
    "    plt.ylabel('Performance [in AUC]', fontsize=14)\n",
    "    plt.semilogx()\n",
    "    plt.grid()\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. High-dimensional Outlier Detection\n",
    "\n",
    "This part is all about the paper by Beyer et al. [1] and, i.e. Theorem 1 that\n",
    "formalized the problem of nearest neighbor based outlier detection in high-dimensional \n",
    "outlier detection settings. Some notation and inspiration also comes from the very good tutorial of \n",
    "Zimek et al. [2] and their corresponding survey paper [3].\n",
    "\n",
    "##### Glossary\n",
    "- $\\mathbf{D}^{dims}$ : distances between samples from the data distribution and samples from the query distribution, both have dimensionality _dims_ (vector)\n",
    "- $D_{max} = \\max_i \\mathbf{D}^{dims}_i$ : maximum distance in the set of all distances (scalar, similar with $D_{min}$)\n",
    "- $\\mathbf{X}^{dims}$ : sample from the data distribution having dimensionality _dims_\n",
    "- $P$ : is a probability\n",
    "- $\\mathbf{E[X]}$ : is the expectation of X over some probability\n",
    "- $var(X)$: equal to $\\mathbf{E}[(X-\\mathbf{E}[X])^2] = \\mathbf{E}[X^2] - \\mathbf{E}[X]^2$\n",
    "\n",
    "##### References\n",
    "[1] K. Beyer, J. Goldstein, R. Ramakrishnan, and U. Shaft, “When Is ‘Nearest Neighbor’ Meaningful?,” in International conference on database theory (ICDT), 1999, pp. 217–235.\n",
    "\n",
    "[2] A. Zimek, E. Schubert, and H.-P. Kriegel, “A survey on unsupervised outlier detection in high-dimensional numerical data,” Statistical Analysis and Data Mining, vol. 5, no. 5. pp. 363–387, 2012.\n",
    "\n",
    "[3] A. Zimek, E. Schubert, and H.-P. Kriegel, “Tutorial on Outlier Detection for High-dimensional Data,” 2013.\n",
    "\n",
    "##### Side note\n",
    "Normalization of distances is done by multiplying $1/\\sqrt{dims}$ (for $\\ell^p$-norms $1/\\sqrt[p]{dims}$) since the maximum distance in the unit cube is $\\sqrt{dims} = \\sqrt{\\sum_d 1^2}$ for euclidean metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Empirical verification\n",
    "\n",
    "The main insight of Theorem 1 is that as the dimensionality of data sets increases, the distance between \n",
    "some query point and to its nearest neighbor $D_{min}$ and its furthest neighbor $D_{max}$ will converge to the same value. That means $lim_{dims \\rightarrow \\infty}\\; P\\left( \\frac{D_{max}}{D_{min}} - 1\\leq \\epsilon \\right) = 1$ for any $\\epsilon > 0$.\n",
    "\n",
    "Lets check empirically in a standard (toy) setting if that is correct. Therefore, we are generating some Gaussian random data points (various cluster with random means and variance) with increasing dimensionality and uniformly distributed query points. Further, we will check for 3 distinct $\\ell^p$-norm induced distance metrics (manhattan, euclidean and max norm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMS = np.logspace(0, 4, 17, dtype=np.int)\n",
    "REPS = 3\n",
    "CLUSTER = 3\n",
    "res = np.zeros((REPS, len(DIMS), 3))\n",
    "extremas = np.zeros((REPS, len(DIMS), 3))\n",
    "for r in range(REPS):\n",
    "    for d in range(len(DIMS)):\n",
    "        X_data = np.empty((DIMS[d], 0))\n",
    "        for c in range(CLUSTER):\n",
    "            rndm_var = np.random.rand()\n",
    "            rndm_mean = 10.*np.random.randn()\n",
    "            X_data = np.append(X_data, rndm_var*np.random.randn(DIMS[d], np.int(1000/CLUSTER)) + rndm_mean, axis=1)\n",
    "        X_query = (np.random.rand(DIMS[d], 10)-0.5) * 2. * 2 * np.max(np.abs(X_data))\n",
    "        # ok, lets measure the distance gap for various metrics\n",
    "        D = dist.cdist(X_data.T, X_query.T, metric='minkowski', p=1)\n",
    "        res[r, d, 0] = np.max(D)/np.min(D)\n",
    "        D = dist.cdist(X_data.T, X_query.T, metric='minkowski', p=2)\n",
    "        res[r, d, 1] = np.max(D)/np.min(D)\n",
    "        # ..and store the euclidean min, max, mean values\n",
    "        extremas[r, d, 0] = np.min(D) * 1./np.sqrt(DIMS[d])\n",
    "        extremas[r, d, 1] = np.max(D) * 1./np.sqrt(DIMS[d])\n",
    "        extremas[r, d, 2] = np.mean(D) * 1./np.sqrt(DIMS[d])\n",
    "        D = dist.cdist(X_data.T, X_query.T, metric='chebyshev')\n",
    "        res[r, d, 2] = np.max(D)/np.min(D)\n",
    "\n",
    "plt.figure(figsize=(16, 6), dpi= 80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.errorbar(DIMS, np.mean(res[:,:,0], axis=0), np.std(res[:,:,0], axis=0), alpha=0.5, linewidth=4, elinewidth=1)\n",
    "plt.errorbar(DIMS, np.mean(res[:,:,1], axis=0), np.std(res[:,:,1], axis=0), alpha=0.5, linewidth=4, elinewidth=1)\n",
    "plt.errorbar(DIMS, np.mean(res[:,:,2], axis=0), np.std(res[:,:,2], axis=0), alpha=0.5, linewidth=4, elinewidth=1)\n",
    "plt.loglog()\n",
    "plt.grid()\n",
    "plt.xticks([1, 10, 100, 1000, 10000], [1, 10, 100, 1000, 10000], fontsize=14)\n",
    "plt.yticks([0.1, 1, 10, 100, 1000, 10000, 100000], [0.1, 1, 10, 100, 1000, 10000, 100000], fontsize=14)\n",
    "plt.xlabel('Number of dimensions', fontsize=18)\n",
    "plt.ylabel('Distance gap $\\epsilon + 1$', fontsize=18)\n",
    "plt.legend(['$\\ell^1$', '$\\ell^2$', '$\\ell^\\infty$'], fontsize=16)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.errorbar(DIMS, np.mean(extremas[:,:,0], axis=0), np.std(extremas[:,:,0], axis=0), fmt='--g', alpha=0.5, linewidth=2, elinewidth=1)\n",
    "plt.errorbar(DIMS, np.mean(extremas[:,:,1], axis=0), np.std(extremas[:,:,1], axis=0), fmt='--b', alpha=0.5, linewidth=2, elinewidth=1)\n",
    "plt.errorbar(DIMS, np.mean(extremas[:,:,2], axis=0), np.std(extremas[:,:,2], axis=0), fmt='-r', alpha=0.5, linewidth=4, elinewidth=1)\n",
    "plt.loglog()\n",
    "plt.grid()\n",
    "plt.xticks([1, 10, 100, 1000, 10000], [1, 10, 100, 1000, 10000], fontsize=14)\n",
    "plt.yticks([0.1, 1, 10, 100], [0.1, 1, 10, 100], fontsize=14)\n",
    "plt.xlabel('Number of dimensions', fontsize=18)\n",
    "plt.ylabel('$\\ell^2$-norm induced metric (normalized)', fontsize=18)\n",
    "plt.legend(['Minimum', 'Maximum', 'Mean'], loc=4, fontsize=16) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether or not distances concentrate depends on the following condition:\n",
    "\n",
    "Assume that $lim_{dims \\rightarrow \\infty}\\; var \\left( \\frac{\\Vert \\mathbf{X}^{dims}\\Vert}{\\mathbf{E}[\\Vert\\mathbf{X}^{dims}\\Vert]} \\right) = 0$, distances will concentrate.\n",
    "\n",
    "Your task is to verify empirically this condition on a number of datasets. \n",
    "When does it not hold? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_condition_empirically(X, n_reps, n_dims):\n",
    "    # TODO\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file_list = ['gaussian', 'mnist.mat', 'wbc.mat', 'ionosphere.mat', 'satimage-2.mat']\n",
    "\n",
    "n_reps = 10\n",
    "n_dims = 30\n",
    "\n",
    "res = np.zeros((len(mat_file_list), n_dims))\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4), dpi=80, facecolor='w', edgecolor='k')\n",
    "for f in range(len(mat_file_list)):\n",
    "    X = np.random.randn(1000, 100)\n",
    "    if not mat_file_list[f] == 'gaussian':\n",
    "        mat = loadmat('../data/'+mat_file_list[f])\n",
    "        print('File: ', mat_file_list[f])\n",
    "        X = mat['X']\n",
    "    X, _ = standardizer(X, X)\n",
    "    res[f, :] = check_condition_empirically(X, n_reps, n_dims)\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(np.arange(n_dims), res[f, :])\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(np.arange(n_dims), res[f, :])\n",
    "\n",
    "    mat_file_list[f] += ' ({0})'.format(X.shape[1])\n",
    "    \n",
    "plt.subplot(1, 2, 1)\n",
    "plt.legend(mat_file_list, fontsize=14, loc=1)\n",
    "plt.semilogy()\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.legend(mat_file_list, fontsize=14, loc=1)\n",
    "plt.loglog()\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
