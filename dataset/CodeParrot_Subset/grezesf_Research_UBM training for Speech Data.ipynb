{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['find_phone_index', 'load_phone_file']\n"
     ]
    }
   ],
   "source": [
    "# import custom functions\n",
    "import sys\n",
    "# path to libraries\n",
    "# currently in ../scripts-lib/\n",
    "tool_path = os.path.abspath('../scripts-lib')\n",
    "\n",
    "if tool_path not in sys.path:\n",
    "    sys.path.append(tool_path)\n",
    "import lib_phones as lph\n",
    "\n",
    "# print the loaded functions\n",
    "print dir(lph)[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 ['aa', 'ae', 'ah', 'ao', 'aw', 'ax', 'ax-h', 'axr', 'ay', 'b', 'bcl', 'ch', 'd', 'dcl', 'dh', 'dx', 'eh', 'el', 'em', 'en', 'eng', 'epi', 'er', 'ey', 'f', 'g', 'gcl', 'h#', 'hh', 'hv', 'ih', 'ix', 'iy', 'jh', 'k', 'kcl', 'l', 'm', 'n', 'ng', 'nx', 'ow', 'oy', 'p', 'pau', 'pcl', 'q', 'r', 's', 'sh', 't', 'tcl', 'th', 'uh', 'uw', 'ux', 'v', 'w', 'y', 'z', 'zh']\n"
     ]
    }
   ],
   "source": [
    "# load phone list\n",
    "phone_path = os.path.abspath('../datasets/TIMIT-MFCCs/TIMIT_phone_list.txt')\n",
    "phone_list = lph.load_phone_file(phone_path)\n",
    "print len(phone_list), phone_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working in path : C:\\Users\\FG\\Desktop\\PhD\\Research\\Reservoirs\\datasets\\TIMIT-MFCCs\\dev\n",
      "working on: si1027.mfcc.csv\n",
      "working on: si1105.mfcc.csv\n",
      "working on: si1657.mfcc.csv\n",
      "working on: si1735.mfcc.csv\n",
      "working on: si475.mfcc.csv\n",
      "working on: si648.mfcc.csv\n",
      "working on: sx115.mfcc.csv\n",
      "working on: sx127.mfcc.csv\n",
      "working on: sx205.mfcc.csv\n",
      "working on: sx217.mfcc.csv\n",
      "working on: sx25.mfcc.csv\n",
      "working on: sx295.mfcc.csv\n",
      "working on: sx307.mfcc.csv\n",
      "working on: sx37.mfcc.csv\n",
      "working on: sx385.mfcc.csv\n",
      "working on: sx397.mfcc.csv\n"
     ]
    }
   ],
   "source": [
    "#load mfccs into sklearn observations, each frame is an obs\n",
    "\n",
    "train_TIMIT_dir = os.path.abspath('../datasets/TIMIT-MFCCs/dev')\n",
    "\n",
    "train_obs = []\n",
    "train_obs_labels = []\n",
    "\n",
    "# walk the directories\n",
    "for (path, dirs, files) in os.walk(train_TIMIT_dir):\n",
    "    print \"working in path : \" + path\n",
    "\n",
    "    for file in files:\n",
    "        # skip the SA files\n",
    "        #dev, only work on file si1573.mfcc.csv     \"si1573\" in file and\n",
    "        if \".mfcc\" in file  and \"sa\" not in file:\n",
    "            #check if corresponding .phn file exists\n",
    "            if not os.path.exists(path + \"/\" + file[:-8] + \"phn\"):\n",
    "                print path + \"/\" + file[:-8] + \"phn\"\n",
    "                print \"corresponding .phn file does not exist!\"\n",
    "            else:\n",
    "                \n",
    "                print \"working on: \" + file\n",
    "#                 print \"from path : \" + path\n",
    "\n",
    "                # open the files\n",
    "                mfcc_file = open(path + \"/\" + file)\n",
    "                phn_file = open(path + \"/\" + file[:-8] + \"phn\")\n",
    "\n",
    "                # extract phone times\n",
    "                phone_times = []\n",
    "                for phn_line in phn_file:\n",
    "                    phone_times.append(phn_line.split())\n",
    "                # transpose for easier use\n",
    "                phone_times = map(list, zip(*phone_times))\n",
    "\n",
    "                # skip mfcc_file header\n",
    "                next(mfcc_file)\n",
    "\n",
    "                # reset frame count\n",
    "                frame_cnt = 0\n",
    "\n",
    "                # for each line of mfcc_file\n",
    "                for mfcc_line in mfcc_file:\n",
    "\n",
    "                    # increment frame count\n",
    "                    frame_cnt += 1 \n",
    "\n",
    "                    # print \"frame line #:\", frame_cnt \n",
    "\n",
    "                    # frame start time in seconds\n",
    "                    start_t = mfcc_line.split(\";\")[1]\n",
    "\n",
    "                    # create frame (skiping first 2 values, frame_index and frame_time)\n",
    "                    frame = map( float,  mfcc_line.split(\";\")[2:])\n",
    "                    # print numpy.shape(frame)\n",
    "                    # print frame\n",
    "\n",
    "                    # find correspond phoneme and index in the list\n",
    "                    phn_index = lph.find_phone_index(start_t, phone_times, phone_list)\n",
    "\n",
    "                    # add to instances\n",
    "                    train_obs.append(frame)\n",
    "                    train_obs_labels.append(phone_list[phn_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4266, 39)\n",
      "(4266,)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(train_obs)\n",
    "print np.shape(train_obs_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-24.63259, -12.55975, -0.4234125, -22.09616, -11.36787, 15.35193, 0.2450585, -18.58464, -28.29842, -5.80051, 2.788154, 4.604296, -17.4054, -0.275135, 1.378781, -1.950969, 3.052481, 3.25547, 1.203516, -0.7550904, -0.3006264, 2.328182, 3.589715, 2.428725, 4.183867, -0.1700937, -0.4764511, 0.5360677, 0.24275, 0.9877825, 0.0331907, -0.5391095, 0.6034231, 1.644038, 1.840576, 0.2822124, -0.1602798, -0.7427405, -0.02820093]\n",
      "['h#', 'h#', 'h#', 'h#', 'h#', 'h#', 'h#', 'h#', 'h#', 'h#', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'iy', 'iy', 'iy', 'iy', 'iy', 'iy', 'iy', 'iy', 'iy', 'v', 'v', 'v', 'v', 'ih', 'ih', 'ih', 'ih', 'ih', 'ih', 'ih', 'ih', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'eh', 'eh', 'eh', 'eh', 'eh', 'eh', 'eh', 'eh', 'eh', 'eh', 'eh', 'eh', 'eh', 'eh', 'eh', 'eh', 'eh', 'eh', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'n', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'ix', 'ix', 'ix', 'ix', 'ix', 'f']\n"
     ]
    }
   ],
   "source": [
    "print train_obs[0]\n",
    "print train_obs_labels[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create gmm\n",
    "num_components = 10\n",
    "num_iter=2\n",
    "g = mixture.GMM(n_components= num_components, n_iter=num_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GMM(covariance_type='diag', init_params='wmc', min_covar=0.001,\n",
       "  n_components=10, n_init=1, n_iter=2, params='wmc', random_state=None,\n",
       "  thresh=None, tol=0.001)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the GMM to the observations\n",
    "g.fit(train_obs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pred = g.predict(train_obs)\n",
    "# print train_obs_labels\n",
    "# print pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-104.66\n",
      "87.85\n",
      "[-102. -114. -104. -102. -100.]\n"
     ]
    }
   ],
   "source": [
    "print np.round(g.score(train_obs).mean(), 2)\n",
    "print np.round(g.score(train_obs).var(), 2)\n",
    "print np.round(g.score(train_obs)[0:5] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 components 2 iterations in  1.50999999046 for 4266 observations.\n"
     ]
    }
   ],
   "source": [
    "# refit, time \n",
    "t0 = time.time()\n",
    "g.fit(train_obs)\n",
    "t1 = time.time()\n",
    "print num_components, \"components\", num_iter, \"iterations in \", t1-t0, \"for\", len(train_obs_labels), \"observations.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-104.65\n",
      "87.9\n",
      "[-102. -114. -104. -103. -100.]\n"
     ]
    }
   ],
   "source": [
    "print np.round(g.score(train_obs).mean(), 2)\n",
    "print np.round(g.score(train_obs).var(), 2)\n",
    "print np.round(g.score(train_obs)[0:5] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refitting does work as expected. (does not reset gmm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved gmm in  C:\\Users\\FG\\Desktop\\PhD\\Research\\Reservoirs\\datasets\\TIMIT Pickled Data\\TIMIT_ubm_gmm_10.pckl\n"
     ]
    }
   ],
   "source": [
    "#save gmm in pickled form\n",
    "import cPickle as pickle\n",
    "\n",
    "# name and location to save in\n",
    "pickle_name = \"TIMIT_ubm_gmm_\" + str(num_components) + \".pckl\"\n",
    "pickle_dir = os.path.abspath('../datasets/TIMIT Pickled Data')\n",
    "\n",
    "if not os.path.isdir(pickle_dir):\n",
    "    os.makedirs(pickle_dir)\n",
    "    \n",
    "pickle.dump( g, open( pickle_dir + os.sep + pickle_name, \"wb\") )\n",
    "print \"saved gmm in \", pickle_dir + '\\\\' + pickle_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded gmm from  C:\\Users\\FG\\Desktop\\PhD\\Research\\Reservoirs\\datasets\\TIMIT Pickled Data\\TIMIT_ubm_gmm_10.pckl\n"
     ]
    }
   ],
   "source": [
    "# reload pickled file for testing\n",
    "g2 = pickle.load( open(pickle_dir + os.sep + pickle_name, \"rb\") )\n",
    "print \"loaded gmm from \", pickle_dir + os.sep + pickle_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
