{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "yDtkhWqDLPAq"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3GLuRaTlLPAv"
   },
   "source": [
    "Boilerplate for graph visualization\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "0Vnl3jP_LPAx"
   },
   "outputs": [],
   "source": [
    "# This is for graph visualization.\n",
    "\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5r42SomULPA2"
   },
   "source": [
    "Load the data\n",
    "---\n",
    "Run 00_download_data.ipynb if you haven't already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "OzoRsG-5LPA3",
    "outputId": "ef95b4a0-cef9-4671-fa63-14c2791d855c"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/'\n",
    "data_filename = os.path.join(DATA_DIR, \"zoo.npz\")\n",
    "data = np.load(open(data_filename))\n",
    "\n",
    "train_data = data['arr_0']\n",
    "train_labels = data['arr_1']\n",
    "test_data = data['arr_2']\n",
    "test_labels = data['arr_3']\n",
    "del data\n",
    "print(\"Data shapes: \", test_data.shape, test_labels.shape, train_data.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a simple classifier with low-level TF Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8292,
     "status": "error",
     "timestamp": 1497369977531,
     "user": {
      "displayName": "Christian Buck",
      "photoUrl": "//lh5.googleusercontent.com/-i04j1OZ3aUs/AAAAAAAAAAI/AAAAAAAAABc/Dx1Bhh2XWUE/s50-c-k-no/photo.jpg",
      "userId": "108154180342320802225"
     },
     "user_tz": -120
    },
    "id": "odSuMrBULPA9",
    "outputId": "6182b273-3fb2-486e-dbf8-7147c7de99f0"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "input_dimension = train_data.shape[1]   # 784 = 28*28 pixels\n",
    "output_dimension = train_labels.shape[1]  # 10 classes\n",
    "\n",
    "batch_size = 32\n",
    "hidden1_units = 128\n",
    "\n",
    "data_batch =  tf.placeholder(\"float\", shape=[None, input_dimension], name=\"data\")\n",
    "label_batch = tf.placeholder(\"float\", shape=[None, output_dimension], name=\"labels\")\n",
    "\n",
    "weights_1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [input_dimension, hidden1_units], \n",
    "        stddev=1.0 / np.sqrt(float(input_dimension))),\n",
    "    name='weights_1')\n",
    "\n",
    "# Task: Add Bias to first layer\n",
    "# Task: Use Cross-Entropy instead of Squared Loss\n",
    "\n",
    "# SOLUTION: Create biases variable.\n",
    "biases_1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [hidden1_units], \n",
    "        stddev=1.0 / np.sqrt(float(hidden1_units))),\n",
    "    name='biases_1')\n",
    "\n",
    "weights_2 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [hidden1_units, output_dimension], \n",
    "        stddev=1.0 / np.sqrt(float(hidden1_units))),\n",
    "    name='weights_2')\n",
    "\n",
    "# SOLUTION: Add the bias term to the first layer\n",
    "wx_b = tf.add(tf.matmul(data_batch, weights_1), biases_1)\n",
    "hidden_activations = tf.nn.relu(wx_b)\n",
    "output_activations = tf.nn.tanh(tf.matmul(hidden_activations, weights_2))\n",
    "\n",
    "# SOLUTION: Replace the l2 loss with softmax cross entropy.\n",
    "with tf.name_scope(\"loss\"):\n",
    "    # loss = tf.nn.l2_loss(label_batch - output_activations)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=label_batch, \n",
    "            logits=output_activations))\n",
    "\n",
    "show_graph(tf.get_default_graph().as_graph_def())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BnAyjAXnLPBB"
   },
   "source": [
    "We can run this graph by feeding in batches of examples using a feed_dict. The keys of the feed_dict are placeholders we've defined previously.\n",
    "The first argument of session.run is the tensor that we're computing. Only parts of the graph required to produce this value will be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "GDn0ixriLPBC",
    "outputId": "43d24a35-8774-47bc-ff45-9ee2ae36125e"
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "  \n",
    "    random_indices = np.random.permutation(train_data.shape[0])\n",
    "    for i in range(1000):\n",
    "        batch_start_idx = (i % (train_data.shape[0] // batch_size)) * batch_size\n",
    "        batch_indices = random_indices[batch_start_idx:batch_start_idx + batch_size]\n",
    "        batch_loss = sess.run(\n",
    "            loss, \n",
    "            feed_dict = {\n",
    "                data_batch : train_data[batch_indices,:],\n",
    "                label_batch : train_labels[batch_indices,:]\n",
    "            })\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\"Loss at iteration {}: {}\".format(i+1, batch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Je87CgiLPBF"
   },
   "source": [
    "No learning yet but we get the losses per batch.\n",
    "We need to add an optimizer to the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {}
     ]
    },
    "colab_type": "code",
    "id": "IYoyWqrgLPBG",
    "outputId": "16393ac8-48b4-4bd3-8627-458ccbba7be8"
   },
   "outputs": [],
   "source": [
    "# Task: Replace GradientDescentOptimizer with AdagradOptimizer and a 0.1 learning rate.\n",
    "# learning_rate = 0.005\n",
    "# updates = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# SOLUTION: Replace GradientDescentOptimizer\n",
    "learning_rate = 0.1\n",
    "updates = tf.train.AdagradOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    random_indices = np.random.permutation(train_data.shape[0])\n",
    "    n_epochs = 10  # how often do to go through the training data\n",
    "    max_steps = train_data.shape[0]*n_epochs // batch_size\n",
    "    for i in range(max_steps):\n",
    "        batch_start_idx = (i % (train_data.shape[0] // batch_size)) * batch_size\n",
    "        batch_indices = random_indices[batch_start_idx:batch_start_idx+batch_size]\n",
    "        batch_loss, _ = sess.run(\n",
    "            [loss, updates], \n",
    "            feed_dict = {\n",
    "                data_batch : train_data[batch_indices,:],\n",
    "                label_batch : train_labels[batch_indices,:]\n",
    "            })\n",
    "\n",
    "        if i % 200 == 0 or i == max_steps - 1:\n",
    "            random_indices = np.random.permutation(train_data.shape[0])\n",
    "            print(\"Batch-Loss at iteration {}: {}\".format(i, batch_loss))\n",
    "\n",
    "            test_predictions = sess.run(\n",
    "                output_activations, \n",
    "                feed_dict = {\n",
    "                    data_batch : test_data,\n",
    "                    label_batch : test_labels\n",
    "                })\n",
    "            wins = np.argmax(test_predictions, axis=1) == np.argmax(test_labels, axis=1)\n",
    "            print(\"Accuracy on test: {}%\".format(100*np.mean(wins)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VaEx4NK_LPBJ"
   },
   "source": [
    "Loss going down, Accuracy going up! \\o/\n",
    "\n",
    "Notice how batch loss differs between batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AlR_4etuLPBK"
   },
   "source": [
    "# Model wrapped in a custom estimator\n",
    "In TensorFlow, we can make it easier to experiment with different models when we separately define a model_fn and an input_fn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {},
      {},
      {},
      {},
      {}
     ]
    },
    "colab_type": "code",
    "id": "2_sBrEKhLPBL",
    "outputId": "fc5468ca-7e24-4a65-b6c5-aaf028f8a627"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Model parameters.\n",
    "batch_size = 32\n",
    "hidden1_units = 128\n",
    "learning_rate = 0.005\n",
    "input_dimension = train_data.shape[1]   # 784 = 28*28 pixels\n",
    "output_dimension = train_labels.shape[1]  # 6 classes\n",
    "n_epochs = 10  # how often do to go through the training data\n",
    "\n",
    "\n",
    "def input_fn(data, labels):\n",
    "    input_images = tf.constant(data, shape=data.shape, verify_shape=True, dtype=tf.float32)\n",
    "    input_labels = tf.constant(labels, shape=labels.shape, verify_shape=True, dtype=tf.float32)\n",
    "    image, label = tf.train.slice_input_producer(\n",
    "        [input_images, input_labels],\n",
    "        num_epochs=n_epochs)\n",
    "    dataset_dict = dict(images=image, labels=label)\n",
    "    batch_dict = tf.train.batch(\n",
    "        dataset_dict, batch_size, allow_smaller_final_batch=True)\n",
    "    batch_labels = batch_dict.pop('labels')\n",
    "    return batch_dict, batch_labels\n",
    "\n",
    "\n",
    "def model_fn(features, targets, mode, params):\n",
    "    # 1. Configure the model via TensorFlow operations (same as above)\n",
    "    weights_1 = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                    [input_dimension, hidden1_units],\n",
    "                    stddev=1.0 / np.sqrt(float(input_dimension))))\n",
    "    weights_2 = tf.Variable(\n",
    "            tf.truncated_normal(\n",
    "                    [hidden1_units, output_dimension],\n",
    "                    stddev=1.0 / np.sqrt(float(hidden1_units))))\n",
    "    hidden_activations = tf.nn.relu(tf.matmul(features['images'], weights_1))\n",
    "    output_activations = tf.matmul(hidden_activations, weights_2)\n",
    "    \n",
    "    # 2. Define the loss function for training/evaluation\n",
    "    loss = tf.reduce_mean(tf.nn.l2_loss(targets - output_activations))\n",
    "    \n",
    "    # 3. Define the training operation/optimizer\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=loss,\n",
    "            global_step=tf.contrib.framework.get_global_step(),\n",
    "            learning_rate=learning_rate,\n",
    "            optimizer=\"SGD\")\n",
    "    \n",
    "    # 4. Generate predictions\n",
    "    predictions_dict = {\n",
    "        \"classes\":       tf.argmax(input=output_activations, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(output_activations, name=\"softmax_tensor\"), \n",
    "        \"logits\":        output_activations,\n",
    "    }\n",
    "    \n",
    "    # Optional: Define eval metric ops; here we add an accuracy metric.\n",
    "    is_correct = tf.equal(tf.argmax(input=targets, axis=1),\n",
    "                                                tf.argmax(input=output_activations, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "    eval_metric_ops = { \"accuracy\":  accuracy}\n",
    "\n",
    "    # 5. Return predictions/loss/train_op/eval_metric_ops in ModelFnOps object\n",
    "    return tf.contrib.learn.ModelFnOps(\n",
    "            mode=mode,\n",
    "            predictions=predictions_dict,\n",
    "            loss=loss,\n",
    "            train_op=train_op,\n",
    "            eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "\n",
    "custom_model = tf.contrib.learn.Estimator(model_fn=model_fn)\n",
    "\n",
    "# Train and evaluate the model.\n",
    "def evaluate_model(model, input_fn):\n",
    "    for i in range(6):\n",
    "        max_steps = train_data.shape[0]*n_epochs // batch_size\n",
    "        model.fit(input_fn=lambda: input_fn(train_data, train_labels), steps=max_steps)\n",
    "        print(model.evaluate(input_fn=lambda: input_fn(test_data, test_labels),\n",
    "                                                 steps=150))\n",
    "\n",
    "\n",
    "evaluate_model(custom_model, input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "5bn1rtOnLPBP"
   },
   "source": [
    "# Custom model, simplified with tf.layers\n",
    "Instead of doing the matrix multiplications and everything ourselves, we can use tf.layers to simplify the definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Model parameters.\n",
    "batch_size = 32\n",
    "hidden1_units = 128\n",
    "learning_rate = 0.005\n",
    "input_dimension = train_data.shape[1]   # 784 = 28*28 pixels\n",
    "output_dimension = train_labels.shape[1]  # 6 classes\n",
    "\n",
    "def layers_custom_model_fn(features, targets, mode, params):\n",
    "    # 1. Configure the model via TensorFlow operations (using tf.layers). Note how\n",
    "    #    much simpler this is compared to defining the weight matrices and matrix\n",
    "    #    multiplications by hand.\n",
    "    hidden_layer = tf.layers.dense(inputs=features['images'], units=hidden1_units, activation=tf.nn.relu)\n",
    "    output_layer = tf.layers.dense(inputs=hidden_layer, units=output_dimension, activation=tf.nn.relu)\n",
    "    \n",
    "    # 2. Define the loss function for training/evaluation\n",
    "    loss = tf.losses.mean_squared_error(labels=targets, predictions=output_layer)\n",
    "    \n",
    "    # 3. Define the training operation/optimizer\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=loss,\n",
    "            global_step=tf.contrib.framework.get_global_step(),\n",
    "            learning_rate=learning_rate,\n",
    "            optimizer=\"SGD\")\n",
    "    \n",
    "    # 4. Generate predictions\n",
    "    predictions_dict = {\n",
    "        \"classes\":       tf.argmax(input=output_layer, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(output_layer, name=\"softmax_tensor\"), \n",
    "        \"logits\":        output_layer,\n",
    "    }\n",
    "    \n",
    "    # Define eval metric ops; we can also use a pre-defined function here.\n",
    "    accuracy = tf.metrics.accuracy(\n",
    "        labels=tf.argmax(input=targets, axis=1),\n",
    "        predictions=tf.argmax(input=output_layer, axis=1))\n",
    "    eval_metric_ops = {\"accuracy\":  accuracy}\n",
    "\n",
    "    # 5. Return predictions/loss/train_op/eval_metric_ops in ModelFnOps object\n",
    "    return tf.contrib.learn.ModelFnOps(\n",
    "        mode=mode,\n",
    "       predictions=predictions_dict,\n",
    "       loss=loss,\n",
    "       train_op=train_op,\n",
    "       eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "\n",
    "layers_custom_model = tf.contrib.learn.Estimator(\n",
    "        model_fn=layers_custom_model_fn)\n",
    "\n",
    "# Train and evaluate the model.\n",
    "evaluate_model(layers_custom_model, input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model using canned estimators\n",
    "Instead of defining our own DNN classifier, TensorFlow supplies a number of *canned* estimators that can save a lot of work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Model parameters.\n",
    "hidden1_units = 128\n",
    "learning_rate = 0.005\n",
    "input_dimension = train_data.shape[1]   # 784 = 28*28 pixels\n",
    "output_dimension = train_labels.shape[1]  # 6 classes\n",
    "\n",
    "# Our model can be defined using just three simple lines...\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "images_column = tf.contrib.layers.real_valued_column(\"images\")\n",
    "# Task: Use the DNNClassifier Estimator to create the model in 1 line.\n",
    "# SOLUTION: DNNClassifier can be used to efficiently (in lines of code) create the model.\n",
    "canned_model = tf.contrib.learn.DNNClassifier(\n",
    "    feature_columns=[images_column],\n",
    "    hidden_units=[hidden1_units],\n",
    "    n_classes=output_dimension,\n",
    "    activation_fn=tf.nn.relu,\n",
    "    optimizer=optimizer)\n",
    "\n",
    "# Potential exercises: play with model parameters, e.g. add dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to change the input_fn so that it returns integers representing the classes instead of one-hot vectors.\n",
    "def class_input_fn(data, labels):\n",
    "    input_images = tf.constant(\n",
    "        data, shape=data.shape, verify_shape=True, dtype=tf.float32)\n",
    "    # The next two lines are different.\n",
    "    class_labels = np.argmax(labels, axis=1)\n",
    "    input_labels = tf.constant(\n",
    "        class_labels, shape=class_labels.shape, verify_shape=True, dtype=tf.int32)\n",
    "    image, label = tf.train.slice_input_producer(\n",
    "        [input_images, input_labels], num_epochs=n_epochs)\n",
    "    dataset_dict = dict(images=image, labels=label)\n",
    "    batch_dict = tf.train.batch(\n",
    "        dataset_dict, batch_size, allow_smaller_final_batch=True)\n",
    "    batch_labels = batch_dict.pop('labels')\n",
    "    return batch_dict, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the model.\n",
    "evaluate_model(canned_model, class_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "input_dimension = train_data.shape[1]   # 784 = 28*28 pixels\n",
    "output_dimension = train_labels.shape[1]  # 6 classes\n",
    "batch_size = 32\n",
    "\n",
    "data_batch =  tf.placeholder(\"float\", shape=[None, input_dimension])\n",
    "label_batch = tf.placeholder(\"float\", shape=[None, output_dimension])\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Task: convert the batch_size x num_pixels (784) input to batch_size, height (28), width(28), channels\n",
    "# SOLUTION: reshape the input. We only have a single color channel.\n",
    "image_batch = tf.reshape(data_batch, [-1, 28, 28, 1])\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(image_batch, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 48])\n",
    "b_conv2 = bias_variable([48])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 48, 256])\n",
    "b_fc1 = bias_variable([256])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*48])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# Task: add dropout to fully connected layer. Add a variable to turn dropout off in eval.\n",
    "# SOLUTION: add placeholder variable to deactivate dropout (keep_prob=1.0) in eval.\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "# SOLUTION: add dropout to fully connected layer.\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([256, output_dimension])\n",
    "b_fc2 = bias_variable([output_dimension])\n",
    "\n",
    "output_activations = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "loss = tf.reduce_mean(\n",
    "           tf.nn.softmax_cross_entropy_with_logits(labels=label_batch, \n",
    "                                                   logits=output_activations))\n",
    "\n",
    "# Solution: Switch from GradientDescentOptimizer to AdamOptimizer\n",
    "# learning_rate = 0.001\n",
    "# updates = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "learning_rate = 0.001\n",
    "updates = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    random_indices = np.random.permutation(train_data.shape[0])\n",
    "    n_epochs = 5  # how often to go through the training data\n",
    "    max_steps = train_data.shape[0]*n_epochs // batch_size\n",
    "    for i in range(max_steps):\n",
    "        batch_start_idx = (i % (train_data.shape[0] // batch_size)) * batch_size\n",
    "        batch_indices = random_indices[batch_start_idx:batch_start_idx+batch_size]\n",
    "        batch_loss, _ = sess.run(\n",
    "            [loss, updates], \n",
    "            feed_dict = {\n",
    "                data_batch : train_data[batch_indices,:],\n",
    "                label_batch : train_labels[batch_indices,:],\n",
    "                # SOLUTION: Dropout active during training\n",
    "                keep_prob : 0.5})\n",
    "        if i % 100 == 0 or i == max_steps - 1:\n",
    "            random_indices = np.random.permutation(train_data.shape[0])\n",
    "            print(\"Batch-Loss at iteration {}/{}: {}\".format(i, max_steps-1, batch_loss))\n",
    "    \n",
    "            test_predictions = sess.run(\n",
    "                output_activations,\n",
    "                feed_dict = {\n",
    "                    data_batch : test_data,\n",
    "                    label_batch : test_labels,\n",
    "                    # SOLUTION: No dropout during eval\n",
    "                    keep_prob : 1.0\n",
    "                })\n",
    "            wins = np.argmax(test_predictions, axis=1) == np.argmax(test_labels, axis=1)\n",
    "            print(\"Accuracy on test: {}%\".format(100*np.mean(wins)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "02_Quickdraw_FFNN.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
