{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run magic.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Rule\n",
    "考慮 $F = f(\\mathbf{a},\\mathbf{g}(\\mathbf{b},\\mathbf{h}(\\mathbf{c}, \\mathbf{i}))$\n",
    "\n",
    "$\\mathbf{a},\\mathbf{b},\\mathbf{c},$  代表著權重 , $\\mathbf{i}$ 是輸入\n",
    "\n",
    "站在 \\mathbf{g}  的角度，為了要更新權重，我們想算\n",
    "### $\\frac{\\partial F}{\\partial b_i}$\n",
    "\n",
    "\n",
    "我們需要什麼？ 由 chain rule 得知\n",
    "### $\\frac{\\partial F}{\\partial b_i} = \n",
    "\\sum_j \\frac{\\partial F}{\\partial g_j}\\frac{\\partial g_j}{\\partial b_i}$\n",
    "或者寫成 Jabobian 的形式\n",
    "### $\\frac{\\partial F}{\\partial \\mathbf{b}} = \n",
    "\\frac{\\partial F}{\\partial \\mathbf{g}} \\frac{\\partial \\mathbf{g}}{\\partial \\mathbf{b}}$\n",
    "\n",
    "所以我們希望前面能傳給我們 $\\frac{\\partial F}{\\partial \\mathbf{g}}$\n",
    "\n",
    "\n",
    "將心比心，因為 $\\mathbf{h}$ 也要算 $\\frac{\\partial F}{\\partial \\mathbf{c}}$， 所以我們還要負責傳 $\\frac{\\partial F}{\\partial \\mathbf{h}}$ 給他。 而因為 \n",
    "### $\\frac{\\partial F}{\\partial \\mathbf{h}}=\n",
    "\\frac{\\partial F}{\\partial \\mathbf{g}} \\frac{\\partial \\mathbf{g}}{\\partial \\mathbf{h}}$\n",
    "\n",
    "所以 $\\mathbf{g}$ 中間真正需要負責計算的東西就是 $\\frac{\\partial \\mathbf{g}}{\\partial \\mathbf{h}}$ 和 $\\frac{\\partial \\mathbf{g}}{\\partial \\mathbf{b}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 誤差函數\n",
    "我們的誤差函數還是 Cross entropy， \n",
    "假設輸入值 $x$ 對應到的真實類別是 $y$, 那我們定義誤差函數\n",
    "\n",
    "## $ loss = -\\log(q_y)=- \\log(Predict(Y=y|x)) $\n",
    "\n",
    "\n",
    "或比較一般的\n",
    "\n",
    "## $ loss = - p \\cdot \\log q  $\n",
    "\n",
    "其中 $ p_i = \\Pr(Y=i|x) $ 代表真實發生的機率\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以一層 hidden layer  的 feedforward neural network 來看\n",
    "## $ L= loss = -p \\cdot \\log \\sigma(C(f(Ax+b))+d) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由於\n",
    "### $-\\log \\sigma (Z) = 1 \\log (\\sum e^{Z_j})-Z$\n",
    "### $\\frac{\\partial -\\log \\sigma (Z)}{\\partial Z} = 1 \\sigma(Z)^T - \\delta$\n",
    "let $U = f(Ax+b) $,   $Z=CU+d$\n",
    "\n",
    "### $ \\frac{\\partial L}{\\partial d} = \\frac{\\partial L}{\\partial Z} \\frac{\\partial CU+d}{\\partial d} \n",
    "= \\frac{\\partial L}{\\partial Z}\n",
    "= p^T (1 \\sigma(Z)^T - \\delta)\n",
    "=  \\sigma(Z)^T - p^T\n",
    "=  \\sigma(CU+d)^T - p^T\n",
    "$\n",
    "\n",
    "### $ \\frac{\\partial L}{\\partial C_{i,j} } \n",
    "= \\frac{\\partial L}{\\partial Z} \\frac{\\partial CU+d}{\\partial C_{i,j}} \n",
    "= (p^T (1 \\sigma(Z)^T - \\delta))_i U_j \n",
    "=  (\\sigma(Z) - p)_i   U_j\n",
    "$\n",
    "所以\n",
    "\n",
    "### $ \\frac{\\partial L}{\\partial C } \n",
    "=  (\\sigma(Z) - p)   U^T\n",
    "$\n",
    "\n",
    "到目前為止，都跟原來 softmax 的結果一樣。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "繼續計算 A, b 的偏微分\n",
    "### $ \\frac{\\partial L}{\\partial U } \n",
    "= \\frac{\\partial L}{\\partial Z} \\frac{\\partial CU+d}{\\partial U} \n",
    "= (p^T (1 \\sigma(Z)^T - \\delta)) C\n",
    "=  (\\sigma(Z) - p)^T C\n",
    "$\n",
    "\n",
    "$ \\frac{\\partial U_k}{\\partial b_i} \n",
    "= \\frac{\\partial f(A_kx+b_k)}{\\partial b_i}\n",
    "= \\delta_{k,i} f'(Ax+b)_i $\n",
    "\n",
    "$  \\frac{\\partial L}{\\partial b_i } \n",
    "=  ((\\sigma(Z) - p)^T C)_i f'(Ax+b)_i$\n",
    "\n",
    "$  \\frac{\\partial L}{\\partial A_{i,j} } \n",
    "=  ((\\sigma(Z) - p)^T C)_i f'(Ax+b)_i x_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 任務：先暴力的利用上面直接微分好的式子來試試看\n",
    "* 把之前的 softmax, relu, sigmoid 都拿回來看看\n",
    "* 計算 relu 和 sigmoid 的微分\n",
    "* 來試試看 mod 3 問題\n",
    "* 隨機設定 A,b,C,d (可以嘗試不同的隱藏層維度)\n",
    "* 看看 loss\n",
    "* 設定一個 x\n",
    "* 計算 gradient\n",
    "* 扣掉 gradient\n",
    "* 看看 loss 是否有減少？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 參考範例， 各種函數、微分\n",
    "%run -i solutions/ff_funcs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 參考範例， 計算 loss\n",
    "%run -i solutions/ff_compute_loss2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial L}{\\partial d} =  \\sigma(CU+d)^T - p^T$\n",
    "\n",
    "$ \\frac{\\partial L}{\\partial C } =  (\\sigma(Z) - p)   U^T$\n",
    "\n",
    "$  \\frac{\\partial L}{\\partial b_i } \n",
    "=  ((\\sigma(Z) - p)^T C)_i f'(Ax+b)_i$\n",
    "\n",
    "$  \\frac{\\partial L}{\\partial A_{i,j} } \n",
    "=  ((\\sigma(Z) - p)^T C)_i f'(Ax+b)_i x_j$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 計算 gradient\n",
    "%run -i solutions/ff_compute_gradient.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 更新權重，計算新的 loss\n",
    "%run -i solutions/ff_update.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "練習：隨機訓練 20000 次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 參考範例\n",
    "%run -i solutions/ff_train_mod3.py\n",
    "plt.plot(L_history);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 訓練結果測試\n",
    "for i in range(16):\n",
    "    x = Vector(i%2, (i>>1)%2, (i>>2)%2, (i>>3)%2)\n",
    "    y = i%3\n",
    "    U = relu(A@x+b)\n",
    "    q = softmax(C@U+d)\n",
    "    print(q.argmax(), y)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習：井字棋的判定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def truth(x):\n",
    "    x = x.reshape(3,3)\n",
    "    return int(x.all(axis=0).any() or\n",
    "            x.all(axis=1).any() or\n",
    "            x.diagonal().all() or\n",
    "            x[::-1].diagonal().all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run -i solutions/ff_train_ttt.py\n",
    "plt.plot(accuracy_history);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
