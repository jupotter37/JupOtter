{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark version of wordcount examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the pyspark environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "findspark.init('/home/ubuntu/shortcourse/spark-1.5.1-bin-hadoop2.6')\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"test\").setMaster(\"local[2]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your HDFS is still on and the input files (the three books) are still in the input folder.\n",
    "\n",
    "Create the input RDD from the files on the HDFS (hdfs://localhost:54310/user/ubuntu/input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30213"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile('hdfs://localhost:54310/user/ubuntu/input')\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the counting, by flatMap, map, and reduceByKey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "counts = lines.flatMap(lambda x: x.split()).map(lambda x: (x, 1)).reduceByKey(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the top 10 frequently used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'the', 11273),\n",
       " (u'to', 7594),\n",
       " (u'of', 6978),\n",
       " (u'and', 6887),\n",
       " (u'a', 5182),\n",
       " (u'I', 4533),\n",
       " (u'in', 3916),\n",
       " (u'was', 3484),\n",
       " (u'that', 3204),\n",
       " (u'her', 2428)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.takeOrdered(10, lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern Matching WordCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the pattern file into a set. (file: /home/ubuntu/shortcourse/notes/scripts/wordcount2/wc2-pattern.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = set()\n",
    "f = open('/home/ubuntu/shortcourse/notes/scripts/wordcount2/wc2-pattern.txt')\n",
    "for line in f:\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        pattern.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the counting, by flatMap, filter, map, and reduceByKey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = lines.flatMap(lambda x: x.split()).filter(lambda x: x in pattern).map(lambda x: (x, 1)).reduceByKey(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect and show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'and', 6887),\n",
       " (u'his', 2356),\n",
       " (u'that', 3204),\n",
       " (u'I', 4533),\n",
       " (u'of', 6978),\n",
       " (u'had', 2107),\n",
       " (u'in', 3916),\n",
       " (u'not', 2076),\n",
       " (u'was', 3484),\n",
       " (u'to', 7594),\n",
       " (u'the', 11273),\n",
       " (u'with', 2097),\n",
       " (u'her', 2428),\n",
       " (u'a', 5182),\n",
       " (u'be', 1975),\n",
       " (u'it', 2284),\n",
       " (u'as', 2141),\n",
       " (u'she', 2095),\n",
       " (u'you', 2317),\n",
       " (u'he', 2148)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stop the spark context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
