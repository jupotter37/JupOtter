{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping\n",
    "========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X = \\theta + \\epsilon$$\n",
    "\n",
    "where \n",
    " * $\\theta \\in \\mathbb{R}$ and \n",
    " * $\\epsilon $ is the error with CDF $F_\\epsilon$\n",
    "    * $\\mathbf{E}[\\epsilon] = 0$\n",
    "    * density of $\\epsilon$ is symmetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall: \n",
    "\n",
    " * if $\\epsilon\\sim \\mathcal{N}$ (normal), then the **\"best\"** estimator for $\\theta$ is sample mean $\\bar{X}$.\n",
    "\n",
    " * However, if $\\epsilon$ is not normal, this may not be the case.\n",
    " \n",
    "For example, if $f_\\epsilon(x)=1/2 e^{-|x|}$ (\"double xponential\"), theb MLE is sample median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is important because if $F_\\epsilon$ is not known, then it's hard to choose a good estimator.\n",
    "\n",
    "  * The biggest problem with not knowing the $F_\\epsilon$ is its tails might be very heavy.\n",
    "  \n",
    "    (heavy tail refers to the speed at which the density goes to zero when its input goes to infinity)\n",
    "    \n",
    "  * This is a problem when the number of data points $n$ is not very large. Indeed, tjhen the **CLT** is **NOT** a good approximation, therefore the  $\\bar{X}$ is **NOT** approximately normal, and the MLE for $\\theta$ is **NOT** $\\approx \\bar{X}$\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** what to do when $F_\\epsilon$ is not known and $n$ is not large? ($n=50$ is to low)\n",
    "\n",
    " * **Answer:** resample from the sample a number of times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition:\n",
    "\n",
    "A Bootstrap sample $X^*_i: i=1...n$ is a set of $n$ variables picked independently from the data points $X_i: i=1..n$\n",
    "\n",
    "(noice that the size of sample is the sample as the original)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDEA: **trimmed mean** $\\bar{X}_\\beta$ where $\\beta\\in(0,1)$:\n",
    "  * throw away a proportion $\\beta$ of data points from the left & right tail of your data\n",
    "  \n",
    "  * **This certainly avoids outliers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDEA: Let $\\bar{\\epsilon}_\\beta = \\bar{X}_\\beta - \\theta$\n",
    "\n",
    "Consider the 0.025 & 0.975 quantiles $c_1$ and $c_2$ for $F_\\epsilon$.\n",
    "\n",
    "Then, we would be able to say $[\\bar{X}_\\beta-c_2, \\bar{X}_\\beta-c_1]$ is a $95\\%$ confidence interval for $\\theta$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:** Since we don't have $c_1$ and $c_2$, let us estimate them.\n",
    "\n",
    " * Most obvious idea is to order the $n$ data points, and pick the ones corresponding to quantil $2.5\\%$ and $97.5\\%$. This is a bad idea, because not enough data. (becuase those quantiles will be extremely sensitive)\n",
    " \n",
    "**Solution:** let us use a large number $B$ of bootstrap samples:\n",
    " \n",
    "  * these are $X^*_{ij}$ where $i=1,..,n$ and $j=1,..,B$\n",
    "  \n",
    "  * then for each $j$ we compute the sample quantiles\n",
    "  \n",
    "  * then, we average these sample quantiles over $j=1,..,B$\n",
    "  \n",
    "  * the resulting estimates are $\\hat{C}_1$ and $\\hat{C}_2$; we cold call them bootstapped sample quantiles.\n",
    "  \n",
    "  * finally, the bootstrapped $95\\%$ confidence interval for $\\theta$ is \n",
    "  \n",
    "  $$\\left[\\bar{X}_\\beta - \\hat{C}_2, \\bar{X}_\\beta - \\hat{C}_1\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark:\n",
    "\n",
    "The textbook by stapelton has an excellent treatment of the extension of this idea to the case of linear regression with non-normal errors,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark: \n",
    "\n",
    "The only good (not hard) alternative to the bootstrap for cofidence intervals is the ordinary $t$ method.\n",
    "\n",
    " * **when tails are fat, this leads to big mistakes**\n",
    " \n",
    " Using t-tables to build confidecen intervals when $n$ is not large, this only works well for **NORMAL ERRORS**\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
