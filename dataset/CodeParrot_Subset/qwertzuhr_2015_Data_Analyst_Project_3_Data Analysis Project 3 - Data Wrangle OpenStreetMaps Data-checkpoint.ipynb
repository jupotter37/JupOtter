{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analyst Project 3\n",
    "Data Wrangle (Retrieve, Analyze and Clean) OpenStreetMaps Data from the City of Dresden\n",
    "\n",
    "_by Benjamin SÃ¶llner, benjamin.soellner@gmail.com_\n",
    "\n",
    "_based on the Udacity.com Data Wrangling With MongoDB_\n",
    "\n",
    "\n",
    "<img src=\"city_dresden_json.png\" alt=\"The city of Dresden as a JSON object illustration\" width=\"400\" height=\"312\" style=\"display: inline; margin: 6pt;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Abstract\n",
    "This paper describes describes the process of downloading, analyzing and cleaning of an OpenStreet Map data set of my former home town as a student: [Dresden](https://en.wikipedia.org/wiki/Dresden), a state capital in eastern Germany, [a baroque town beautifully located on the board of the river Elbe](https://www.google.de/search?q=dresden&newwindow=1&es_sm=122&source=lnms&tbm=isch&sa=X&ved=0CAkQ_AUoA2oVChMIrc3d_cT3xwIVh2fbCh3Y6QYu&biw=1920&bih=969) and town home to a high-tech conglomerate from the micro-electronics sector called [Silicon Saxony](https://en.wikipedia.org/wiki/Silicon_Saxony).\n",
    "\n",
    "In this paper, first, the pipeline (and python script) to perform retrieval, analysis and cleaning of the data is introduced (chapters [Approach](#Approach)) and results of the analysis stage are presented (chapter [Overview of the Data](#Overview-of-the-Data)). During the analysis, interesting facts of Dresden are uncovered, like the most popular religion, sport, beer, cuisine or leisure activity.\n",
    "\n",
    "For the cleaning stage (chapter [Problems Encountered in the Map](#Problems-Encountered-in-the-Map-/-Data-Quality)), canonicalizing phone numbers present in the data set and unifying cuisine classifications where the challenge of choice. Some other cleaning techniques like cleaning street names and post codes where tried, but proved not fruitful. The paper is finally concluded with some further ideas for data set cleaning (chapter [Other Ideas about the Data Set](#Other-Ideas-about-the-Data-Set))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##The Approach\n",
    "I implemented retrieving / storing / analysing and cleaning in a python script. The script can be used like this:\n",
    "```\n",
    "# python project.py\n",
    "Usage:\n",
    "  python project.py -d    Download & unpack bz2 file to OSM file (experimental)\n",
    "  python project.py -p    Process OSM file and write JSON file\n",
    "  python project.py -w    Write JSON file to MongoDB\n",
    "  python project.py -z    Download and install the zipcode helpers\"\n",
    "  python project.py -f    Audit format / structure of data\n",
    "  python project.py -s    Audit statistics of data\n",
    "  python project.py -q    Audit quality of data\n",
    "  python project.py -Z    Audit quality of data: Zipcodes - (see -z option)\n",
    "  python project.py -c    Clean data in MongoDB\n",
    "  python project.py -C    Clean data debug mode - don't actually write to DB\n",
    "```\n",
    "\n",
    "Different options can be combined, so ``` python project.py -dpwfsqc ``` will do the whole round trip. During the process, I re-used most of the code and data format developed during the \"Data Wrangling With MongoDB\" Udacity course. For example, the data format used for storing the data (```-p``` and ```-w``` option) is completely based on [Lesson 6](https://www.udacity.com/course/viewer#!/c-ud032/l-768058569)  - with some fine-tuning.\n",
    "\n",
    "Some output of the script is shown on the terminal, some is written to local files. If a file is written, this is indicated in the terminal output. A sample of the script's terminal output is included in the ```output_*.txt``` files included in the submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Format\n",
    "**Try it out:** Use ```python project.py -f``` to obtain the data for this chapter. This is a long-running process which might take a few hours to complete! There is an output file written to [```Project/data/audit_format_map.csv```](Project/data/audit_format_map.csv) which can be beautified into an [Excel spreadsheet](Project/data/audit_format_map.xlsx).\n",
    "\n",
    "![A picture of the excel spreadsheet audit_format.xlsx](audit_format.png)\n",
    "\n",
    "First, the data format was audited, which consisted of going through all the documents and aggregating the occurence of any attributes and the prevalence of their types (```str```ing, ```int```eger, ```float``` and ```other```). For this, batches of 1000 documents each are retrieved from the collection and each combed through by the python code while a Python Dataframe keeps track of the counters. Since there are 1,360,000 elements, this process takes many hours; an alternative would be to run the query natively in JavaScript code on the MongoDB shell or to issue the command as a BSON command.\n",
    "\n",
    "The overview of the format showed no obvious big problems with the data at first glance but provided some valuable insights:\n",
    "\n",
    "* One area of improvement could be the phone number, which is scattered across multiple data fields (```address:phone```, ```phone``` and ```phone_mobile```) and was identified as a potential candidate for cleaning (see [Auditing Phone Numbers](#Auditing-Phone-Numbers) and [Cleaning Phone Numbers](#Cleaning-Phone-Numbers)).\n",
    "* Some values are present in the dataset as sometimes ```str```ing, othertimes numeric: The XML parsing process takes care that each value is, whenever parsable, stored as integer or float. For attributes like street numbers, mixed occurences may be in the data set.\n",
    "* This automatic parsing of ```int``` or ```float``` turned out to be not always useful: a problem are leading zeros which in certain cases hold semantics. For german phone numbers, a leading zero signifies the start of an area code (```0```) or the start of a country code (```00```). For german postcodes, a leading zero in a postcode represents the german state of Saxony. As an outcome of this insight, I changed the parsing routine of the XML data to only parse values as numeric, if they do not contain a leading zero (```not s.startswith(\"0\")```)\n",
    "* I checked some of the lesser-common values for sanity. E.g., there is a parameter ```dogshit``` which appears three times. As it turns out, this is not a prank of some map editors, who document dog feces they find in the area, but an indication about whether a public trash can contains a dispenser of plastic bags for relevant situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try it out:** Use ```python project.py -s``` to obtain the data for this chapter. See Sample Output in file [```Project/output_project.py_-s.txt```](Project/output_project.py_-s.txt).\n",
    "\n",
    "A couple of basic MongoDB queries were run to explore the data set based on the knowledge of its format from the previous chapter. The queries produce mostly rankings of values for certain data fields. Some of them are subsequently also visualized in a ggplot graph (png file) relying on the skill set gained in [Udacity's Intro to Data Science course, Lesson 4: Data Visualization](https://www.udacity.com/course/viewer#!/c-ud359/l-692548568) while not too much effort was put in making the graphs look particularily beautiful. The graphs are located in ```Project/data/stats_*.png```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Filesize, Number of Nodes and Ways\n",
    "The total file size of the OSM export is 281.778.428 Bytes, there are 208813 nodes and 1146807 ways in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'way', u'count': 209356}, {u'_id': u'node', u'count': 1148321}]\n"
     ]
    }
   ],
   "source": [
    "from Project.notebook_stub import project_coll\n",
    "import pprint\n",
    "\n",
    "# Query used - see function Project.audit_stats_map.stats_general\n",
    "pipeline = [\n",
    "        {\"$group\": {\"_id\": \"$type\", \"count\": {\"$sum\": 1}}},\n",
    "        {\"$match\": {\"_id\": {\"$in\": [\"node\", \"way\"]}}}\n",
    "    ]\n",
    "l = list(project_coll.aggregate(pipeline))\n",
    "pprint.pprint(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users Involved\n",
    "There were about 1634 users involved in creating the data set, the top 10 of all users accounts for 40% of the created data. There is no direct evidence from the user name that any of them are bot-like users. This could be determined by further research. Many users (over 60%) have made less than 10 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1634 users were involved:\n",
      "[{u'_id': u'Wolle DD', u'count': 145807},\n",
      " {u'_id': u'NESDD', u'count': 79159},\n",
      " {u'_id': u'Thomas8122', u'count': 75621},\n",
      " {u'_id': u'stw1701', u'count': 57403},\n",
      " '...',\n",
      " {u'_id': u'chkr', u'count': 1},\n",
      " {u'_id': u'The King', u'count': 1},\n",
      " {u'_id': u'mowsw', u'count': 1},\n",
      " {u'_id': u'kicherschleife', u'count': 1},\n",
      " {u'_id': u'choess', u'count': 1}]\n"
     ]
    }
   ],
   "source": [
    "from Project.notebook_stub import project_coll\n",
    "import pprint\n",
    "\n",
    "# Query used - see function: Project.audit_stats_map.stats_users(...):\n",
    "pipeline = [\n",
    "        {\"$match\": {\"created.user\": {\"$exists\": True}}},\n",
    "        {\"$group\": {\"_id\": \"$created.user\", \"count\": {\"$sum\": 1}}},\n",
    "        {\"$sort\": {\"count\": -1}}\n",
    "    ]\n",
    "l = list(project_coll.aggregate(pipeline))\n",
    "print str(len(l)) + \" users were involved:\"\n",
    "pprint.pprint(l[1:5]+[\"...\"]+l[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Types of Amenities\n",
    "The attribute ```amenity``` inspired me to do further research in which kind of buildings / objects / facilities are stored in the Open Street Map data in larger quantities in order to do more detailed research on those objects. Especially [Restaurants](#Cuisines-in-Restaurants), [Pubs](#Beers-in-Pubs) and [Churches / Places of Worship](#Religions-in-Places-of-Worship) were investigated further (as can be seen below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'parking', u'count': 2689},\n",
      " {u'_id': u'bicycle_parking', u'count': 959},\n",
      " {u'_id': u'recycling', u'count': 892},\n",
      " {u'_id': u'post_box', u'count': 793},\n",
      " {u'_id': u'restaurant', u'count': 684},\n",
      " {u'_id': u'vending_machine', u'count': 616},\n",
      " {u'_id': u'waste_basket', u'count': 536},\n",
      " {u'_id': u'fast_food', u'count': 330},\n",
      " {u'_id': u'telephone', u'count': 329},\n",
      " '...']\n"
     ]
    }
   ],
   "source": [
    "from Project.notebook_stub import project_coll\n",
    "import pprint\n",
    "\n",
    "# Query used - see function: Project.audit_stats_map.stats_amenities(...):\n",
    "pipeline = [\n",
    "        {\"$match\": {\"amenity\": {\"$exists\": True}}},\n",
    "        {\"$group\": {\"_id\": \"$amenity\", \"count\": {\"$sum\": 1}}},\n",
    "        {\"$sort\": {\"count\": -1}}\n",
    "    ]\n",
    "l = list(project_coll.aggregate(pipeline))\n",
    "pprint.pprint(l[1:10]+['...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Popular Leisure Activities\n",
    "The attribute ```leisure``` shows the types of leisure activities one can do in Dresden and inspired me to invesigate more on [popular sports in the city](#Popular-Sports) (```leisure```=```sports_center``` or ```leisure```=```stadium```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'pitch', u'count': 575},\n",
      " {u'_id': u'park', u'count': 431},\n",
      " {u'_id': u'sports_centre', u'count': 193},\n",
      " {u'_id': u'garden', u'count': 190},\n",
      " {u'_id': u'swimming_pool', u'count': 108},\n",
      " {u'_id': u'track', u'count': 48},\n",
      " {u'_id': u'dance', u'count': 23},\n",
      " {u'_id': u'water_park', u'count': 22},\n",
      " {u'_id': u'stadium', u'count': 19},\n",
      " '...']\n"
     ]
    }
   ],
   "source": [
    "from Project.notebook_stub import project_coll\n",
    "import pprint\n",
    "\n",
    "# Query used - see function: Project.audit_stats_map.stats_amenities(...):\n",
    "pipeline = [\n",
    "        {\"$match\": {\"leisure\": {\"$exists\": True}}},\n",
    "        {\"$group\": {\"_id\": \"$leisure\", \"count\": {\"$sum\": 1}}},\n",
    "        {\"$sort\": {\"count\": -1}}\n",
    "    ]\n",
    "l = list(project_coll.aggregate(pipeline))\n",
    "pprint.pprint(l[1:10]+['...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Religions in Places of Worship\n",
    "Grouping and sorting by the occurences of the ```religion``` attribute for all ```amenities``` classified as ```place_of_worship``` or ```community_center``` gives us an indication, how prevalent religions are in our city: obviously, ```christian``` is the most prevalent here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'christian', u'count': 140},\n",
      " {u'_id': u'muslim', u'count': 3},\n",
      " {u'_id': u'buddhist', u'count': 2},\n",
      " {u'_id': None, u'count': 2},\n",
      " {u'_id': u'jewish', u'count': 2},\n",
      " {u'_id': u'multifaith', u'count': 1}]\n"
     ]
    }
   ],
   "source": [
    "from Project.notebook_stub import project_coll\n",
    "import pprint\n",
    "\n",
    "# Query used - see function: Project.audit_stats_map.stats_religions(...):\n",
    "pipeline = [\n",
    "        {\"$match\": {\"amenity\":{\"$in\": [\"place_of_worship\",\"community_center\"]}}},\n",
    "        {\"$group\": {\"_id\": \"$religion\", \"count\": {\"$sum\": 1}}},\n",
    "        {\"$sort\": {\"count\": -1}}\n",
    "    ]\n",
    "l = list(project_coll.aggregate(pipeline))\n",
    "pprint.pprint(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Cuisines in Restaurants\n",
    "We can list the types of ```cuisines``` in restaurants (elements with attribute ```amenity``` matching ```restaurant```) and sort them in decending order. We can notice certain inconsistencies or overlaps in the classifications of this data: e.g., a ```kebab``` cuisine may very well be also classified as an ```arab``` cuisine or may, in fact a sub- or super-classification of this cuisine. One could, e.g., eliminate or cluster together especially occurences of cuisines which are less common, but Without having a formal taxonomy of all cuisines, I decided that is probably best to leave the data as-is in order to not sacrifice preciseness for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'german', u'count': 66},\n",
      " {u'_id': u'regional', u'count': 56},\n",
      " {u'_id': u'italian', u'count': 52},\n",
      " {u'_id': u'greek', u'count': 28},\n",
      " {u'_id': u'asian', u'count': 28},\n",
      " {u'_id': u'pizza', u'count': 17},\n",
      " {u'_id': u'chinese', u'count': 12},\n",
      " {u'_id': u'international', u'count': 11},\n",
      " {u'_id': u'indian', u'count': 11},\n",
      " '...']\n"
     ]
    }
   ],
   "source": [
    "from Project.notebook_stub import project_coll\n",
    "import pprint\n",
    "\n",
    "# Query used - see function: Project.audit_stats_map.stats_cuisines(...):\n",
    "pipeline = [\n",
    "        {\"$match\": {\"amenity\": \"restaurant\"}},\n",
    "        {\"$group\": {\"_id\": \"$cuisine\", \"count\": {\"$sum\": 1}}},\n",
    "        {\"$sort\": {\"count\": -1}}\n",
    "    ]\n",
    "l = list(project_coll.aggregate(pipeline))\n",
    "pprint.pprint(l[1:10]+['...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Beers in Pubs\n",
    "Germans do love their beers and the dataset shows that certain ```pub```s, ```restaurant```s or ```bar```s are sponsored by certain beer brands (often advertised on the pubs entrance). We can analyze the prevalence of beer brands by grouping and sorting by occurence of the attribute ```brewery``` for all the ```amenities``` classified as respective establishment. Most popular are [```Radeberger```](https://en.wikipedia.org/wiki/Radeberger_Brewery), a [very popular](https://www.youtube.com/watch?v=QJ9-euumMzQ) local beer, [```FeldschlÃ¶sschen```](https://en.wikipedia.org/wiki/Feldschl%C3%B6sschen), a swiss beer and ```Dresdner Felsenkeller```, a very local and niche-sort-of beer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': None, u'count': 867},\n",
      " {u'_id': u'Radeberger', u'count': 10},\n",
      " {u'_id': u'Feldschl\\xf6\\xdfchen', u'count': 3},\n",
      " {u'_id': u'Dresdner Felsenkeller', u'count': 3},\n",
      " {u'_id': u'Warsteiner', u'count': 2},\n",
      " {u'_id': u'Rechenberger', u'count': 2},\n",
      " {u'_id': u'Feldschl\\xf6\\xdfchen;Schwarzer Steiger', u'count': 2},\n",
      " {u'_id': u'Einsiedler', u'count': 1},\n",
      " {u'_id': u'Eibauer', u'count': 1},\n",
      " {u'_id': u'Freiberger', u'count': 1},\n",
      " {u'_id': u'Freiberger;Jever;Astra;Lech;Tyskie;B\\xf6hmisch Brauhaus',\n",
      "  u'count': 1},\n",
      " {u'_id': u'Kulmbacher', u'count': 1},\n",
      " {u'_id': u\"Neustadt Helles;Lenin's Hanf\", u'count': 1},\n",
      " {u'_id': u'Feldschl\\xf6\\xdfchen;Paulaner;Schwarzer Steiger', u'count': 1},\n",
      " {u'_id': u'Radeberger;Eibauer', u'count': 1}]\n"
     ]
    }
   ],
   "source": [
    "from Project.notebook_stub import project_coll\n",
    "import pprint\n",
    "\n",
    "# Query used - see function: Project.audit_stats_map.stats_beers(...):\n",
    "pipeline = [\n",
    "        {\"$match\": {\"amenity\": {\"$in\":[\"pub\",\"bar\",\"restaurant\"]}}},\n",
    "        {\"$group\": {\"_id\": \"$brewery\", \"count\": {\"$sum\": 1}}},\n",
    "        {\"$sort\": {\"count\": -1}}\n",
    "    ]\n",
    "l = list(project_coll.aggregate(pipeline))\n",
    "pprint.pprint(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Popular Sports\n",
    "To investigate, which sports are popular, we can group and sort by the (occurence of the) ```sport``` attribute for all elements classified as ```sports_centre``` or ```stadium``` in their ```leisure``` attribute. Unsurprisingly for a german city, we notice that ```9pin``` (bowling) and ```soccer``` are the most popular sports, followed by ```climbing```, an activity very much enjoyed by people in Dresden, presumably because of the close-by sand-stone mountains of the national park [SÃ¤chsische Schweiz](http://www.saechsische-schweiz.de/en/saxon-switzerland.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'multi', u'count': 55},\n",
      " {u'_id': u'9pin', u'count': 5},\n",
      " {u'_id': u'soccer', u'count': 5},\n",
      " {u'_id': u'climbing', u'count': 4},\n",
      " '...']\n"
     ]
    }
   ],
   "source": [
    "from Project.notebook_stub import project_coll\n",
    "import pprint\n",
    "\n",
    "# Query used - see function: Project.audit_stats_map.stats_sports(...):\n",
    "pipeline = [\n",
    "        {\"$match\": {\"leisure\": {\"$in\": [\"sports_centre\",\"stadium\"]}}},\n",
    "        {\"$group\": {\"_id\": \"$sport\", \"count\": {\"$sum\": 1}}},\n",
    "        {\"$sort\": {\"count\": -1}}\n",
    "    ]\n",
    "l = list(project_coll.aggregate(pipeline))\n",
    "pprint.pprint(l[1:5]+['...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Where to Dance in Dresden\n",
    "I am a passionate social dancer, so a list of dance schools in Dresden should not be abscent from this investigation. We can quickly grab all elements which have the ```leisure``` attribute set to ```dancing```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Tanzschule Graf',\n",
      " u'Tanzschule Weise',\n",
      " u'tres tangos',\n",
      " u'Tango im Salon',\n",
      " u'Studio24',\n",
      " u'La Academia Tango',\n",
      " u'Tanzschule Herrmann-Nebl',\n",
      " u'Tanzstudio Sandana',\n",
      " u'TSC Casino Dresden e.V.',\n",
      " '...']\n"
     ]
    }
   ],
   "source": [
    "from Project.notebook_stub import project_coll\n",
    "import pprint\n",
    "\n",
    "# Query used - see function: Project.audit_stats_map.stats_dances(...):\n",
    "l = list(project_coll.distinct(\"name\", {\"leisure\": \"dance\"}))\n",
    "pprint.pprint(l[1:10]+['...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Problems Encountered in the Map / Data Quality\n",
    "**Try it out**: Use ```python project.py -q``` to obtain the data from this chapter. See Sample Output in file [```Project/output_project.py_-q.txt```](Project/data/audit_buildings.csv). The script also writes a CSV file to [```Project/data/audit_buildings.csv```](Project/data/audit_buildings.csv), which is also beautified into a [Excel File](Project/data/audit_buildings.xlsx)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Leading Zeros\n",
    "As already discussed, during the parsing stage, we are using an optimistic approach of parsing any numerical value as integer or float, if it is parsable as such. However, we noticed that we should _not_ do this, if leading zeros are present as those hold semantics for phone numbers and zip codes. Otherwise, this cleaning approach gives us a much smaller representation of the data in MongoDB and in-memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Normalizing / Cleaning Cuisines\n",
    "As hinted in section [Cuisines in Restaurant](#Cuisines-in-Restaurants), classification of cuisines is inconsistent. There are two problems with this value:\n",
    "\n",
    "* There are multiple values separated by ';' which makes the parameter hard to parse. We overcome this by creating a parameter ```cuisineTag``` which stores the cuisine classifications as an array:\n",
    "\n",
    "  ```python\n",
    "  db.eval('''db.osmnodes.find({\n",
    "                        \"cuisine\": {\"$exists\": true},\n",
    "                        \"amenity\": \"restaurant\"\n",
    "                    }).snapshot().forEach(function(val, idx) {\n",
    "                        val.cuisineTags = val.cuisine.split(';');\n",
    "                        db.osmnodes.save(val)\n",
    "                    })\n",
    "             ''')\n",
    "   ```\n",
    "   \n",
    "* Some values are inconsistently used; therefore, we unify them with a mapping table and a subsequent MongoDB update:\n",
    "\n",
    "  ```python\n",
    "  cuisines_synonyms = {\n",
    "      'german': ['regional', 'schnitzel', 'buschenschank'],\n",
    "      'portuguese': ['Portugiesisches_Restaurant_&_Weinbar'],\n",
    "      'italian': ['pizza', 'pasta'],\n",
    "      'mediterranean': ['fish', 'seafood'],\n",
    "      'japanese': ['sushi'],\n",
    "      'turkish': ['kebab'],\n",
    "      'american': ['steak_house']\n",
    "    }\n",
    "  # not mapped:\n",
    "  # greek, asian, chinese, indian, international, vietnamese, thai, spanish, arabic\n",
    "  # sudanese, russian, korean, hungarian, syrian, vegan, soup, croatian, african\n",
    "  # balkan, mexican, french, cuban, lebanese\n",
    "  \n",
    "  for target in cuisines_synonyms:\n",
    "    db.osmnodes.update( {\n",
    "        \"cuisine\": {\"$exists\": True},\n",
    "        \"amenity\": \"restaurant\",\n",
    "        \"cuisineTags\": {\"$in\": cuisines_synonyms[target]}\n",
    "      }, {\n",
    "        \"$pullAll\": { \"cusineTags\": cuisines_synonyms[target] },\n",
    "        \"$addToSet\": { \"cuisineTags\": [ target ] }\n",
    "      }, multi=False )\n",
    "  ```\n",
    "  \n",
    "This allows us to convert a restaurant with the MongoDB representation\n",
    "\n",
    "```{..., \"cuisine\": \"pizza;kebab\", ...}``` \n",
    "\n",
    "to the alternative representation\n",
    "\n",
    "```{..., \"cuisine\": \"pizza;kebab\", \"cuisineTag\": [\"italian\", \"turkish\"], ...}```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auditing Phone Numbers\n",
    "Phone re scattered over different attributes (```address.phone```, ```phone``` and ```mobile_phone```) and come in different styles of formating (like ```+49 351 123 45``` vs. ```0049-351-12345```). First, we retrieve a list of all phone numbers. With the goal in mind to later store the normalized phone number back into the attribute ```phone```, this value has to be read first, and only if it is empty, ```mobile_phone``` or ```address.phone``` should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Project.notebook_stub import project_coll\n",
    "\n",
    "# Query used - see function: Project.audit_quality_map.audit_phone_numbers(...):\n",
    "pipeline = [\n",
    "        {\"$match\": {\"$or\": [\n",
    "                {\"phone\": {\"$exists\": True}},\n",
    "                {\"mobile_phone\": {\"$exists\": True}},\n",
    "                {\"address.phone\": {\"$exists\": True}}\n",
    "            ]}},\n",
    "        {\"$project\": {\n",
    "                \"_id\": 1,\n",
    "                \"phone\": {\"$ifNull\": [\"$phone\", {\"$ifNull\": [\"$mobile_phone\", \"$address.phone\"]}]}\n",
    "            }}\n",
    "    ]\n",
    "l = project_coll.aggregate(pipeline)\n",
    "\n",
    "# Output too long... See the file Project/output_project.py_-q.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Cleaning Phone Numbers\n",
    "**Try it out:** Use ```python project.py -C``` to clean in debug mode. See Sample Output in file ```Project/output_project.py_-C.txt```. The script also writes a CSV file to [```Project/data/clean_phones.csv```](Project/data/clean_phones.csv), which is also beautified into a [Excel File](Project/data/clean_phones.xlsx).\n",
    "\n",
    "Cleaning the phone numbers involves:\n",
    "* unifying the different phone attributes (```phone```, ```address.phone``` and ```mobile_phone```) - this is already taken care by extracting the phone numbers during the audit stage\n",
    "* if possible, canonicalizing the phone number notations by parsing them using a regular expression:\n",
    "\n",
    "```python\n",
    "phone_regex = re.compile(ur'^(\\(?([\\+|\\*]|00) *(?P<country>[1-9][0-9]*)\\)?)?' +  # country code\n",
    "                         ur'[ \\/\\-\\.]*\\(?0?\\)?[ \\/\\-\\.]*' +  # separator\n",
    "                         ur'(\\(0?(?P<area1>[1-9][0-9 ]*)\\)|0?(?P<area2>[1-9][0-9]*))?' +  # area code\n",
    "                         ur'[ \\/\\-\\.]*' +  # separator\n",
    "                         ur'(?P<number>([0-9]+ *[\\/\\-.]? *)*)$', # number\n",
    "                         re.UNICODE)\n",
    "```\n",
    "\n",
    "The regular expression is resilient to various separators (\"```/```\", \"```-```\", \" \", \"```(0)```\") and bracket notation of phone numbers. It is not resilient for some unicode characters or written lists of phone numbers which are designed to be interpreted by humans (using separators like \"```,```\", \"```/-```\" or \"```oder```\" lit. or). During the cleaning stage, an output is written which phone numbers could not be parsed. This contains only a tiny fraction of phone numbers (9 or 0.5%) which would be easily cleanable by hand.\n",
    "\n",
    "```\n",
    "The following objects couldn't be parsed:\n",
    "                                                 normalized\n",
    "55f57294b1c8a72c34523897           +49 35207 81429 or 81469\n",
    "55f57299b1c8a72c345272cd  +49 351 8386837, +49 176 67032256\n",
    "55f572c2b1c8a72c34546689                      0351 4810426\n",
    "55f572c3b1c8a72c34546829         +49 351 8902284 or 2525375\n",
    "55f572fdb1c8a72c34574963   +49 351 4706625, +49 351 0350602\n",
    "55f573bdb1c8a72c3460bdb3                +49 351 87?44?44?00\n",
    "55f573bdb1c8a72c3460c066         0162 2648953, 0162 2439168\n",
    "55f573edb1c8a72c346304b1           03512038973, 03512015831\n",
    "55f5740eb1c8a72c34649008                0351 4455193 / -118\n",
    "```\n",
    "\n",
    "If the phone number was parsable, the country code, area code and rest of the phone number are separated and subsequently strung together to a canonical form. The data to be transformed is stored into a Pandas Dataframe. By using the option ```-C``` instead of ```-c``` the execution of the transformation can be surpressed and the Dataframe instead be written to a [CSV file](Project/data/clean_phones.csv) which might be further beautified into an [Excel File](Project/data/clean_phones.xlsx) in order to test or debug the transformation before writing it to the database with the ```-c``` option.\n",
    "\n",
    "![A screenshot of the Excel file](clean_phones.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Auditing Street Names (Spoiler Alert: No Cleaning Necessary)\n",
    "Auditing the map's street names analogous to how it was done in the Data Wrangling course was done as follows: Check, whether 'weird' street names occur, which do not end on a suffix like ```street``` (in German ```-straÃe``` or ```StraÃe```, depending on whether it is a compound word or not). It is assumed that then, they would most likely end in an abbreviation like ```str.```. For this we use a regular expression querying all streets <u>not</u> ending with a particular suffix like ```[Ss]traÃe``` (street), ```[Ww]eg``` (way) etc. This is accomplished by a chain of [\"negative lookbehind\"](http://www.regular-expressions.info/lookaround.html) expressions (```(?<!...)```) which must all in sequence evaluate to \"true\" in order to flag a street name as non-conforming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Project.notebook_stub import project_coll\n",
    "\n",
    "# Query used - see function: Project.audit_quality_map.audit_streets(...):\n",
    "expectedStreetPattern = \\\n",
    "    u\"^.*(?<![Ss]tra\\u00dfe)(?<![Ww]eg)(?<![Aa]llee)(?<![Rr]ing)(?<![Bb]erg)\" + \\\n",
    "    u\"(?<![Pp]ark)(?<![Hh]\\u00f6he)(?<![Pp]latz)(?<![Bb]r\\u00fccke)(?<![Gg]rund)$\"\n",
    "l = list(project_coll.distinct(\"name\", {\n",
    "                    \"type\": \"way\",\n",
    "                    \"name\": {\"$regex\": expectedStreetPattern}\n",
    "                }))\n",
    "# Output too long... See the file Project/output_project.py_-q.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skimming through the list, it was noticable that the nature of the german language (and how in Germany streetnames work) results in the fact, that there are many small places without a suffix like \"street\" but \"their own thing\" (like ```Am Hang``` lit. 'At The Slope', ```Beerenhut``` lit. 'Berry Hat', ```Im Grunde``` lit. 'In The Ground'). The street names can therefore not be processed just by looking at the suffixes - I tried something different..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Cross Auditing Street Names with Street Addresses (Spoiler Alert: No Cleaning Necessary)\n",
    "I did not want to trust the street names of the data set fully yet. Next, I tried figuring out if street names of buildings were consistent with street names of objects in close proximity. Therefore, a JavaScript query is run directly on the database server returning all ```buildings``` with the objects nearby having an ```address.street``` parameter. This should allow us to cross-audit if objects in close proximity do have the same street names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Project.notebook_stub import project_db\n",
    "\n",
    "# Query used - see function: Project.audit_quality_map.audit_buildings(...):\n",
    "buildings_with_streets = project_db.eval('''\n",
    "            db.osmnodes.ensureIndex({pos:\"2dsphere\"});\n",
    "            result = [];\n",
    "            db.osmnodes.find(\n",
    "                    {\"building\": {\"$exists\": true}, \"address.street\": {\"$exists\": true}, \"pos\": {\"$exists\": true}},\n",
    "                    {\"address.street\": \"\", \"pos\": \"\"}\n",
    "                ).forEach(function(val, idx) {\n",
    "                    val.nearby = db.osmnodes.distinct(\"address.street\",\n",
    "                            {\"_id\": {\"$ne\": val._id}, \"pos\": {\"$near\": {\"$geometry\": {\"type\": \"Point\", \"coordinates\": val.pos}, \"$maxDistance\": 50, \"$minDistance\": 0}}}\n",
    "                        );\n",
    "                    result.push(val);\n",
    "                })\n",
    "            return result;\n",
    "        ''')\n",
    "\n",
    "# Output too long... See the file Project/output_project.py_-q.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting objects are then iterated through and the best and worst fitting nearby street name are identified each using the [Levenshtein distance](http://stackoverflow.com/questions/18134437/where-can-the-documentation-for-python-levenshtein-be-found-online). For each object, a row is created in a DataFrame which is subsequently exported to a csv file [Project/data/audit_buildings.csv](Project/data/audit_buildings.csv) that was manually beautified into an [Excel File](Project/data/audit_buildings.xlsx). \n",
    "\n",
    "![Screenshot of the Excel File](audit_buildings.png)\n",
    "\n",
    "As can be seen, street names of nearby objects mostly match those of the building itself (Levenshtein distance is zero). If they deviate greatly, they are totally different street names in the same area and not just \"typos\" or non-conforming abbreviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Auditing Zip Codes (Spoiler Alert: No Cleaning Necessary)\n",
    "**Try it out:** Use ```python project.py -Z``` which runs the auditing script for zipcodes. See Sample Output in file ```Project/output_project.py_-Z.txt```. To be able to run this script correctly, the [zipcode data from Geonames.org](http://download.geonames.org/export/zip/) needs to be downloaded  and installed first using the ```-z``` option (see output in ```Project/output_project.py_-Z.txt``).\n",
    "\n",
    "This part of the auditing process makes use of an additional at [Geonames.org](http://download.geonames.org/export/zip/) to resolve and audit the zip codes in the data set. During the \"installation process\" (option ```-z```) the zipcode data (provided as a tab-separated file) is downloaded and, line-by-line, stored to a (separate) MongoDB collection. However, we are only interested \"zipcode\" (2) and  \"place\" (3)\n",
    "\n",
    "During the auditing stage (option ```-Z```) we first get a list of all used zipcode using the following query:\n",
    "\n",
    "```python\n",
    "pipeline = [\n",
    "            { \"$match\": {\"address.postcode\": {\"$exists\": 1}} },\n",
    "            { \"$group\": {\"_id\": \"$address.postcode\", \"count\": {\"$sum\": 1}} },\n",
    "            { \"$sort\": {\"count\": 1} }\n",
    "        ]\n",
    "```\n",
    "\n",
    "The zipcodes are then all looked up in the zipcode collection using the ```$in```-operator. The data obtained is joined back into the original result.\n",
    "\n",
    "```python\n",
    "zipcodeObjects = zipcodeColl.find( {\"zipcode\": {\"$in\": [z[\"_id\"] for z in zipcodeList]}} )\n",
    "```\n",
    "\n",
    "The following output shows that there the lesser used zipcodes are from the Dresden metropolitan area, not Dresden itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'01847', u'count': 2, 'place': u'Lohmen'},\n",
      " {u'_id': u'01896', u'count': 17, 'place': u'Pulsnitz'},\n",
      " {u'_id': u'01665', u'count': 18, 'place': u'Diera-Zehren'},\n",
      " {u'_id': u'01689', u'count': 63, 'place': u'Weinb\\xf6hla'},\n",
      " {u'_id': u'01833', u'count': 93, 'place': u'Stolpen'},\n",
      " {u'_id': u'01477', u'count': 134, 'place': u'Arnsdorf'},\n",
      " {u'_id': u'01737', u'count': 165, 'place': u'Tharandt'},\n",
      " {u'_id': u'01471', u'count': 325, 'place': u'Radeburg'},\n",
      " {u'_id': u'01796', u'count': 516, 'place': u'Pirna'},\n",
      " '...']\n"
     ]
    }
   ],
   "source": [
    "from Project.audit_zipcode_map import audit_zipcode_map\n",
    "from Project.notebook_stub import project_server, project_port\n",
    "import pprint\n",
    "\n",
    "zipcodeJoined = audit_zipcode_map(project_server, project_port, quiet=True)\n",
    "pprint.pprint(zipcodeJoined[1:10]+['...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Other Ideas about the Data Set\n",
    "\n",
    "### Auditing City Names for Correctness and Completeness\n",
    "The Geonames.org data could help us to validate the entered city names or add them, where missing. One could compare the ```address.city``` attribute of any OSM element with all of the 4 hierarchically names of the Geonames.org document which belongs to the zipcode referred by ```address.postcode```.\n",
    "\n",
    "* If no ```address.city``` is present in the OSM element at all, the lowestmost value in the Geonames.org hieararchy could be added and the data therefore enhanced.\n",
    "* If the value of the OSM element does not match any name of the Geonames.org data, the element could be flagged for manual processing.\n",
    "\n",
    "**Cost**: Relatively easily implementable, however, out of scope for this project. We should, however, strive for implementing the related query in native BSON code in order to not hit the database with every zipcode-to-Geonames-element mapping request for each OSM element.\n",
    "\n",
    "**Benefit**: Potentially high, depending on how many cities are not entered at all (quick win) or entered correctly (some additional manual work required).\n",
    "\n",
    "### Cuisine Taxonomy\n",
    "\n",
    "The taxonomy of cuisines could be further formalized to contain super- and subsets of cuisines (e.g. each \"italian\" cuisine is also an \"international\" cuisine). With domain knowledge, coarsly classified restaurants could potentially also be sub-classified.\n",
    "\n",
    "**Cost**: High, The creation of a proper cuisine-taxonomy would require substantial knowledge of the subject matter of cuisines and the subtle differences in culinary art. Also, rather than a tree-based classification, some \"fusion\" kitchens might overlap:  any simplification or unification we carry out here comes at the cost of sacrificing detail.\n",
    "\n",
    "**Benefit**: Medium-high in certain use cases, higher number of restaurants with a certain classification lets us better find the restaurant of our taste and compare various restaurants with each other. \n",
    "\n",
    "### Other Open Questions\n",
    "Overall, the data set of Dresden is pretty neat and tidy. Compared to other, huger cities (e.g., in India) I might have had an easier job. Further open questions or ideas (out of scope for this report) include:\n",
    "\n",
    "* The users might be analyzed further: Why are so many nodes (many thousands) created by so few users? Are bots at work? Why are so many users only contributing with very few edits? (Maybe gamification - leaderboards for who has the most edits during the recent week - would provide help.)\n",
    "* One could audit for completeness by parsing several sample websites for addresses and trying to retrieve those addresses in the Open Street Map data.\n",
    "* One could feed the normalized phone data back into Open Street Map by either using a Web Service or using the XML format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##References\n",
    "- [Project Rubric](https://docs.google.com/document/d/1TpfNxDzUjhibq9Qb8cOQHtlvZUelft-W0fb7pCTTyYE/pub)\n",
    "- [Sample Project](https://docs.google.com/document/d/1F0Vs14oNEs2idFJR3C_OPxwS6L0HPliOii-QpbmrMo4/pub)\n",
    "- [Lesson & Problem Set 6 of Udacities Data Wrangling with OpenDB Class](https://www.udacity.com/course/viewer#!/c-ud032/l-768058569)\n",
    "- [Project Evaluation & Submission](https://www.udacity.com/course/viewer#!/c-nd002/l-3168208620/m-3189488621)\n",
    "- [Python CSV Reader Documentation](https://docs.python.org/2/library/csv.html)\n",
    "- [Python ElementTree Documentation](https://docs.python.org/2/library/xml.etree.elementtree.html)\n",
    "- [MongoDB Aggregation Framework Operators](http://docs.mongodb.org/manual/reference/operator/aggregation/project/#pipe._S_project)\n",
    "- [MongoDB: Indexes](http://docs.mongodb.org/manual/indexes/)\n",
    "- [Regex Lookarounds](http://www.regular-expressions.info/lookaround.html)\n",
    "- [MongoDB University](https://university.mongodb.com/)\n",
    "- [BZip2 Module](https://docs.python.org/2/library/bz2.html)\n",
    "- [MapZen Metro Extracts](https://mapzen.com/data/metro-extracts)\n",
    "- [MongoDB Extended JSON](http://docs.mongodb.org/master/reference/mongodb-extended-json/)\n",
    "- [Retrieving URLs](http://stackoverflow.com/questions/22676/how-do-i-download-a-file-over-http-using-python)\n",
    "- [Using the Levenshtein distance](http://stackoverflow.com/questions/18134437/where-can-the-documentation-for-python-levenshtein-be-found-online)\n",
    "- [Zipcode Helper from Geonames.org](http://download.geonames.org/export/zip/) ([cc-by](http://www.geonames.org/export/))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
