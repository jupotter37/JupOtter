{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset split of AotM-2011/30Music playlists for playlist augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from scipy.sparse import lil_matrix, issparse, hstack, vstack\n",
    "from collections import Counter\n",
    "import gensim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_settings0 = np.seterr(all='raise')\n",
    "RAND_SEED = 0\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['aotm2011', '30music']\n",
    "ffeature = 'data/msd/song2feature.pkl.gz'\n",
    "fgenre = 'data/msd/song2genre.pkl.gz'\n",
    "fsong2artist = 'data/msd/song2artist.pkl.gz'\n",
    "audio_feature_indices = [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 185, 186, 187, 198, 199, 200, 201]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dix = 0\n",
    "dataset_name = datasets[dix]\n",
    "data_dir = 'data/%s' % dataset_name\n",
    "print(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load playlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load playlists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fplaylist = os.path.join(data_dir, '%s-playlist.pkl.gz' % dataset_name)\n",
    "_all_playlists = pkl.load(gzip.open(fplaylist, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _all_playlists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_playlists = []\n",
    "\n",
    "if type(_all_playlists[0][1]) == tuple:\n",
    "    for pl, u in _all_playlists:\n",
    "        user = '%s_%s' % (u[0], u[1])  # user string\n",
    "        all_playlists.append((pl, user))\n",
    "else:\n",
    "    all_playlists = _all_playlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_playlists = dict()\n",
    "# for pl, u in all_playlists:\n",
    "#     try:\n",
    "#         user_playlists[u].append(pl)\n",
    "#     except KeyError:\n",
    "#         user_playlists[u] = [pl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_playlists = []\n",
    "# for u in user_playlists:\n",
    "#     if len(user_playlists[u]) > 4:\n",
    "#         all_playlists += [(pl, u) for pl in user_playlists[u]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_users = sorted(set({user for _, user in all_playlists}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#user    : {:,}'.format(len(all_users)))\n",
    "print('#playlist: {:,}'.format(len(all_playlists)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_lengths = [len(pl) for pl, _ in all_playlists]\n",
    "plt.hist(pl_lengths, bins=100)\n",
    "print('Average playlist length: %.1f' % np.mean(pl_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check duplicated songs in the same playlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:,} | {:,}'.format(np.sum(pl_lengths), np.sum([len(set(pl)) for pl, _ in all_playlists])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load song features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load `song_id` --> `feature array` mapping: map a song to the audio features of one of its corresponding tracks in MSD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_song2feature = pkl.load(gzip.open(ffeature, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2feature = dict()\n",
    "\n",
    "for sid in sorted(_song2feature):\n",
    "    song2feature[sid] = _song2feature[sid][audio_feature_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Song genres from [MSD Allmusic Genre Dataset (Top MAGD)](http://www.ifs.tuwien.ac.at/mir/msd/TopMAGD.html) and [tagtraum](http://www.tagtraum.com/msd_genre_datasets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2genre = pkl.load(gzip.open(fgenre, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Song collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_all_songs = sorted([(sid, int(song2feature[sid][-1])) for sid in {s for pl, _ in all_playlists for s in pl}], \n",
    "                   key=lambda x: (x[1], x[0]))\n",
    "print('{:,}'.format(len(_all_songs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomise the order of song with the same age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_age_dict = dict()\n",
    "\n",
    "for sid, age in _all_songs:\n",
    "    age = int(age)\n",
    "    try:\n",
    "        song_age_dict[age].append(sid)\n",
    "    except KeyError:\n",
    "        song_age_dict[age] = [sid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_songs = []\n",
    "\n",
    "np.random.seed(RAND_SEED)\n",
    "for age in sorted(song_age_dict.keys()):\n",
    "    all_songs += [(sid, age) for sid in np.random.permutation(song_age_dict[age])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(all_songs, gzip.open(os.path.join(data_dir, 'setting2/all_songs.pkl.gz'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all songs have genre info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#songs missing genre: {:,}'.format(len(all_songs) - np.sum([sid in song2genre for (sid, _) in all_songs])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Song popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2index = {sid: ix for ix, (sid, _) in enumerate(all_songs)}\n",
    "song_pl_mat = lil_matrix((len(all_songs), len(all_playlists)), dtype=np.int8)\n",
    "for j in range(len(all_playlists)):\n",
    "    pl = all_playlists[j][0]\n",
    "    ind = [song2index[sid] for sid in pl]\n",
    "    song_pl_mat[ind, j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_pop = song_pl_mat.tocsc().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pop = np.max(song_pop)\n",
    "max_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2pop = {sid: song_pop[song2index[sid], 0] for (sid, _) in all_songs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(song2pop, gzip.open(os.path.join(data_dir, 'setting2/song2pop.pkl.gz'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deal with one outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# song_pop1 = song_pop.copy()\n",
    "# maxix = np.argmax(song_pop)\n",
    "# song_pop1[maxix] = 0\n",
    "# clipped_max_pop = np.max(song_pop1) + 10   # second_max_pop + 10\n",
    "# if max_pop - clipped_max_pop > 500:\n",
    "#     song_pop1[maxix] = clipped_max_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create song-playlist matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Songs as rows, playlists as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataset(playlists, song2feature, song2genre, song2artist, artist2vec, \n",
    "                train_song_set, dev_song_set=[], test_song_set=[], song2pop_train=None):\n",
    "    \"\"\"\n",
    "    Create labelled dataset: rows are songs, columns are users.\n",
    "    \n",
    "    Input:\n",
    "        - playlists: a set of playlists\n",
    "        - train_song_set: a list of songIDs in training set\n",
    "        - dev_song_set: a list of songIDs in dev set\n",
    "        - test_song_set: a list of songIDs in test set\n",
    "        - song2feature: dictionary that maps songIDs to features from MSD\n",
    "        - song2genre: dictionary that maps songIDs to genre\n",
    "        - song2pop_train: a dictionary that maps songIDs to its popularity\n",
    "    Output:\n",
    "        - (Feature, Label) pair (X, Y)\n",
    "          X: #songs by #features\n",
    "          Y: #songs by #users\n",
    "    \"\"\" \n",
    "    song_set = train_song_set + dev_song_set + test_song_set\n",
    "    N = len(song_set)\n",
    "    K = len(playlists)\n",
    "    \n",
    "    genre_set = sorted({v for v in song2genre.values()})\n",
    "    genre2index = {genre: ix for ix, genre in enumerate(genre_set)}\n",
    "    \n",
    "    def onehot_genre(songID):\n",
    "        \"\"\"\n",
    "        One-hot encoding of genres.\n",
    "        Data imputation: \n",
    "            - mean imputation (default)\n",
    "            - one extra entry for songs without genre info\n",
    "            - sampling from the distribution of genre popularity\n",
    "        \"\"\"\n",
    "        num = len(genre_set) # + 1\n",
    "        vec = np.zeros(num, dtype=np.float)\n",
    "        if songID in song2genre:\n",
    "            genre_ix = genre2index[song2genre[songID]]\n",
    "            vec[genre_ix] = 1\n",
    "        else:\n",
    "            vec[:] = np.nan\n",
    "            #vec[-1] = 1\n",
    "        return vec\n",
    "    \n",
    "    def song_artist_feature(songID):\n",
    "        \"\"\"\n",
    "        Return the artist feature for a given song\n",
    "        \"\"\"\n",
    "        if songID in song2artist:\n",
    "            aid = song2artist[songID]\n",
    "            return artist2vec[aid]\n",
    "        else:\n",
    "            return artist2vec['$UNK$']\n",
    "    \n",
    "    X = np.array([np.concatenate([song2feature[sid], song_artist_feature(sid), onehot_genre(sid)], axis=-1) \\\n",
    "                  for sid in song_set])\n",
    "    Y = lil_matrix((N, K), dtype=np.bool)\n",
    "    \n",
    "    song2index = {sid: ix for ix, sid in enumerate(song_set)}\n",
    "    for k in range(K):\n",
    "        pl = playlists[k]\n",
    "        indices = [song2index[sid] for sid in pl if sid in song2index]\n",
    "        Y[indices, k] = True\n",
    "        \n",
    "    # genre imputation\n",
    "    genre_ix_start = -len(genre_set)\n",
    "    genre_nan = np.isnan(X[:, genre_ix_start:])\n",
    "    genre_mean = np.nansum(X[:, genre_ix_start:], axis=0) / (X.shape[0] - np.sum(genre_nan, axis=0))\n",
    "    #print(np.nansum(X[:, genre_ix_start:], axis=0))\n",
    "    #print(genre_set)\n",
    "    #print(genre_mean)\n",
    "    for j in range(len(genre_set)):\n",
    "        X[genre_nan[:,j], j+genre_ix_start] = genre_mean[j]\n",
    "        \n",
    "    # normalise the sum of all genres per song to 1\n",
    "    # X[:, -len(genre_set):] /= X[:, -len(genre_set):].sum(axis=1).reshape(-1, 1)  \n",
    "    # NOTE: this is not necessary, as the imputed values are guaranteed to be normalised (sum to 1) \n",
    "    # due to the above method to compute mean genres.\n",
    "    \n",
    "    # the log of song popularity\n",
    "    if song2pop_train is not None:\n",
    "        # for sid in song_set: \n",
    "        #     assert sid in song2pop_train  # trust the input\n",
    "        logsongpop = np.log2([song2pop_train[sid]+1 for sid in song_set])  # deal with 0 popularity\n",
    "        X = np.hstack([X, logsongpop.reshape(-1, 1)])\n",
    "\n",
    "    #return X, Y\n",
    "    Y = Y.tocsr()\n",
    "    \n",
    "    train_ix = [song2index[sid] for sid in train_song_set]\n",
    "    X_train = X[train_ix, :]\n",
    "    Y_train = Y[train_ix, :]\n",
    "    \n",
    "    dev_ix = [song2index[sid] for sid in dev_song_set]\n",
    "    X_dev = X[dev_ix, :]\n",
    "    Y_dev = Y[dev_ix, :]\n",
    "    \n",
    "    test_ix = [song2index[sid] for sid in test_song_set]\n",
    "    X_test = X[test_ix, :]\n",
    "    Y_test = Y[test_ix, :]\n",
    "    \n",
    "    if len(dev_song_set) > 0:\n",
    "        if len(test_song_set) > 0:\n",
    "            return X_train, Y_train.tocsc(), X_dev, Y_dev.tocsc(), X_test, Y_test.tocsc()\n",
    "        else:\n",
    "            return X_train, Y_train.tocsc(), X_dev, Y_dev.tocsc()\n",
    "    else:\n",
    "        if len(test_song_set) > 0:\n",
    "            return X_train, Y_train.tocsc(), X_test, Y_test.tocsc()\n",
    "        else:\n",
    "            return X_train, Y_train.tocsc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split playlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split playlists such that every song in test set is also in training set.  \n",
    "~~Split playlists (60/10/30 train/dev/test split) uniformly at random.~~  \n",
    "~~Split each user's playlists (60/20/20 train/dev/test split) uniformly at random if the user has $5$ or more playlists.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_playlists = []\n",
    "dev_playlists   = []\n",
    "test_playlists  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_pl_indices = []\n",
    "other_pl_indices = []\n",
    "\n",
    "for i in range(len(all_playlists)):\n",
    "    pl = all_playlists[i][0]\n",
    "    if np.all(np.asarray([song2pop[sid] for sid in pl]) >= 5):\n",
    "        candidate_pl_indices.append(i)\n",
    "    else:\n",
    "        other_pl_indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%d + %d = %d | %d' % (len(candidate_pl_indices), len(other_pl_indices), \\\n",
    "                             len(candidate_pl_indices) + len(other_pl_indices), len(all_playlists)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_ratio = 0.05\n",
    "test_ratio = 0.2\n",
    "npl_dev  = int(dev_ratio  * len(all_playlists))\n",
    "npl_test = int(test_ratio * len(all_playlists))\n",
    "np.random.seed(RAND_SEED)\n",
    "pl_indices = np.random.permutation(candidate_pl_indices)\n",
    "\n",
    "test_ix = pl_indices[:npl_test]\n",
    "test_playlists = [all_playlists[ix] for ix in test_ix]\n",
    "\n",
    "dev_ix = pl_indices[npl_test:npl_test + npl_dev]\n",
    "dev_playlists = [all_playlists[ix] for ix in dev_ix]\n",
    "\n",
    "train_ix = pl_indices[npl_test + npl_dev:].tolist() + other_pl_indices\n",
    "train_playlists = [all_playlists[ix] for ix in train_ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every song in test set should also be in training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('#Songs in train + dev set: %d, #Songs total: %d' % \\\n",
    "      (len(set([sid for pl, _ in train_playlists + dev_playlists for sid in pl])), len(all_songs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:30s} {:,}'.format('#playlist in training set:', len(train_playlists)))\n",
    "print('{:30s} {:,}'.format('#playlist in dev set:', len(dev_playlists)))\n",
    "print('{:30s} {:,}'.format('#playlist in test set:', len(test_playlists)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_playlists) + len(dev_playlists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_playlists = dict()\n",
    "# for pl, u in all_playlists:\n",
    "#     try: \n",
    "#         user_playlists[u].append(pl)\n",
    "#     except KeyError:\n",
    "#         user_playlists[u] = [pl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "# npl_all = np.sum([len(user_playlists[u]) for u in user_playlists])\n",
    "# print('{:30s} {:,}'.format('#users:', len(user_playlists)))\n",
    "# print('{:30s} {:,}'.format('#playlists:', npl_all))\n",
    "# print('{:30s} {:.2f}'.format('Average #playlists per user:', npl_all / len(user_playlists)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(RAND_SEED)\n",
    "# for u in user_playlists:\n",
    "#     playlists_u = [(pl, u) for pl in user_playlists[u]]\n",
    "#     if len(user_playlists[u]) < 5:\n",
    "#         train_playlists += playlists_u\n",
    "#     else:\n",
    "#         npl_test = int(test_ratio * len(user_playlists[u]))\n",
    "#         npl_dev  = int(dev_ratio * len(user_playlists[u]))\n",
    "#         pl_indices = np.random.permutation(len(user_playlists[u]))\n",
    "#         test_playlists  += playlists_u[:npl_test]\n",
    "#         dev_playlists   += playlists_u[npl_test:npl_test + npl_dev]\n",
    "#         train_playlists += playlists_u[npl_test + npl_dev:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmax = np.max([len(pl) for (pl, _) in all_playlists]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(111)\n",
    "ax.hist([len(pl) for (pl, _) in train_playlists], bins=100)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(0, xmax)\n",
    "ax.set_title('Histogram of playlist length in TRAINING set')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(111)\n",
    "ax.hist([len(pl) for (pl, _) in dev_playlists], bins=100)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(0, xmax)\n",
    "ax.set_title('Histogram of playlist length in DEV set')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(111)\n",
    "ax.hist([len(pl) for (pl, _) in test_playlists], bins=100)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(0, xmax)\n",
    "ax.set_title('Histogram of playlist length in TEST set')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn artist features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2artist = pkl.load(gzip.open(fsong2artist, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_playlist = []\n",
    "\n",
    "for pl, _ in train_playlists + dev_playlists:\n",
    "    pl_artists = [song2artist[sid] if sid in song2artist else '$UNK$' for sid in pl]\n",
    "    artist_playlist.append(pl_artists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fartist2vec_bin = os.path.join(data_dir, 'setting2/artist2vec.bin')\n",
    "if os.path.exists(fartist2vec_bin):\n",
    "    artist2vec = gensim.models.KeyedVectors.load_word2vec_format(fartist2vec_bin, binary=True)\n",
    "else:\n",
    "    artist2vec_model = gensim.models.Word2Vec(sentences=artist_playlist, size=50, seed=RAND_SEED, \n",
    "                                              window=10, iter=10, min_count=1)\n",
    "    artist2vec_model.wv.save_word2vec_format(fartist2vec_bin, binary=True)\n",
    "    artist2vec = artist2vec_model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hold a subset of songs in dev/test playlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the first $K=1,2,3,4$ songs for playlist in dev and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SEED_K = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_playlists_obs   = []\n",
    "dev_playlists_held  = []\n",
    "test_playlists_obs  = []\n",
    "test_playlists_held = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pl, _ in dev_playlists:\n",
    "    npl = len(pl)\n",
    "    k = N_SEED_K\n",
    "    dev_playlists_obs.append(pl[:k])\n",
    "    dev_playlists_held.append(pl[k:])\n",
    "for pl, _ in test_playlists:\n",
    "    npl = len(pl)\n",
    "    k = N_SEED_K\n",
    "    test_playlists_obs.append(pl[:k])\n",
    "    test_playlists_held.append(pl[k:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix in range(len(dev_playlists)):\n",
    "    assert np.all(dev_playlists[ix][0]  == dev_playlists_obs[ix]  + dev_playlists_held[ix])\n",
    "for ix in range(len(test_playlists)):\n",
    "    assert np.all(test_playlists[ix][0] == test_playlists_obs[ix] + test_playlists_held[ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DEV  obs: {:,} | DEV  held: {:,} \\nTEST obs: {:,} | TEST held: {:,}'.format(\n",
    "    np.sum([len(ppl) for ppl in dev_playlists_obs]),  np.sum([len(ppl) for ppl in dev_playlists_held]),\n",
    "    np.sum([len(ppl) for ppl in test_playlists_obs]), np.sum([len(ppl) for ppl in test_playlists_held])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2pop_train = song2pop.copy()\n",
    "song2pop_trndev = song2pop.copy()\n",
    "for ppl in dev_playlists_held:\n",
    "    for sid in ppl:\n",
    "        song2pop_train[sid] -= 1\n",
    "for ppl in test_playlists_held:\n",
    "    for sid in ppl:\n",
    "        song2pop_train[sid] -= 1\n",
    "        song2pop_trndev[sid] -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hold a subset of songs in a subset of playlists, use all songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir2 = os.path.join(data_dir, 'setting2')\n",
    "fpl2     = os.path.join(pkl_dir2, 'playlists_train_dev_test_s2_%d.pkl.gz' % N_SEED_K)\n",
    "fy2      = os.path.join(pkl_dir2, 'Y_%d.pkl.gz' % N_SEED_K)\n",
    "fxtrain2 = os.path.join(pkl_dir2, 'X_train_%d.pkl.gz' % N_SEED_K)\n",
    "fytrain2 = os.path.join(pkl_dir2, 'Y_train_%d.pkl.gz' % N_SEED_K)\n",
    "fxtrndev2 = os.path.join(pkl_dir2, 'X_trndev_%d.pkl.gz' % N_SEED_K)\n",
    "fytrndev2 = os.path.join(pkl_dir2, 'Y_trndev_%d.pkl.gz' % N_SEED_K)\n",
    "fydev2   = os.path.join(pkl_dir2, 'PU_dev_%d.pkl.gz' % N_SEED_K)\n",
    "fytest2  = os.path.join(pkl_dir2, 'PU_test_%d.pkl.gz' % N_SEED_K)\n",
    "fclique21 = os.path.join(pkl_dir2, 'cliques_trndev_%d.pkl.gz' % N_SEED_K)\n",
    "fclique22 = os.path.join(pkl_dir2, 'cliques_all_%d.pkl.gz' % N_SEED_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = gen_dataset(playlists = [t[0] for t in train_playlists + dev_playlists + test_playlists],\n",
    "                   song2feature = song2feature, song2genre = song2genre, \n",
    "                   song2artist = song2artist, artist2vec = artist2vec, \n",
    "                   train_song_set = [t[0] for t in all_songs], song2pop_train=song2pop_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X\n",
    "assert X.shape[0] == len(song2pop_trndev)\n",
    "X_trndev = X_train.copy()\n",
    "X_trndev[:, -1] = np.log([song2pop_trndev[sid]+1 for sid, _ in all_songs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_cols  = np.arange(len(train_playlists), len(train_playlists) + len(dev_playlists))\n",
    "test_cols = np.arange(len(train_playlists) + len(dev_playlists), Y.shape[1])\n",
    "assert len(dev_cols)  == len(dev_playlists)  == len(dev_playlists_obs)\n",
    "assert len(test_cols) == len(test_playlists) == len(test_playlists_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump({'train_playlists': train_playlists, 'dev_playlists': dev_playlists, 'test_playlists': test_playlists,\n",
    "          'dev_playlists_obs': dev_playlists_obs, 'dev_playlists_held': dev_playlists_held,\n",
    "          'test_playlists_obs': test_playlists_obs, 'test_playlists_held': test_playlists_held},\n",
    "          gzip.open(fpl2, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song2index = {sid: ix for ix, sid in enumerate([t[0] for t in all_songs])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use dedicated sparse matrices to reprsent what entries are observed in dev and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = Y[:, :len(train_playlists)].tocsc()\n",
    "Y_trndev = Y[:, :len(train_playlists) + len(dev_playlists)].tocsc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PU_dev  = lil_matrix((len(all_songs), len(dev_playlists)),  dtype=np.bool)\n",
    "PU_test = lil_matrix((len(all_songs), len(test_playlists)), dtype=np.bool)\n",
    "\n",
    "num_known_dev = 0\n",
    "for j in range(len(dev_playlists)):\n",
    "    if (j+1) % 1000 == 0:\n",
    "        sys.stdout.write('\\r%d / %d' % (j+1, len(dev_playlists))); sys.stdout.flush()\n",
    "    rows = [song2index[sid] for sid in dev_playlists_obs[j]]\n",
    "    PU_dev[rows, j] = True\n",
    "    num_known_dev += len(rows)\n",
    "\n",
    "num_known_test = 0\n",
    "for j in range(len(test_playlists)):\n",
    "    if (j+1) % 1000 == 0:\n",
    "        sys.stdout.write('\\r%d / %d' % (j+1, len(test_playlists))); sys.stdout.flush()\n",
    "    rows = [song2index[sid] for sid in test_playlists_obs[j]]\n",
    "    PU_test[rows, j] = True\n",
    "    num_known_test += len(rows)\n",
    "\n",
    "PU_dev  = PU_dev.tocsr()\n",
    "PU_test = PU_test.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#unknown entries in DEV  set: {:15,d} | {:15,d} \\n#unknown entries in TEST set: {:15,d} | {:15,d}'.format(\n",
    "    np.prod(PU_dev.shape)  - PU_dev.sum(),  len(dev_playlists)  * len(all_songs) - num_known_dev,\n",
    "    np.prod(PU_test.shape) - PU_test.sum(), len(test_playlists) * len(all_songs) - num_known_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mean = np.mean(X_train, axis=0).reshape((1, -1))\n",
    "X_train_std = np.std(X_train, axis=0).reshape((1, -1)) + 10 ** (-6)\n",
    "X_train -= X_train_mean\n",
    "X_train /= X_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trndev_mean = np.mean(X_trndev, axis=0).reshape((1, -1))\n",
    "X_trndev_std = np.std(X_trndev, axis=0).reshape((1, -1)) + 10 ** (-6)\n",
    "X_trndev -= X_trndev_mean\n",
    "X_trndev /= X_trndev_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.mean(X_train, axis=0)))\n",
    "print(np.mean( np.std(X_train, axis=0)) - 1)\n",
    "print(np.mean(np.mean(X_trndev, axis=0)))\n",
    "print(np.mean( np.std(X_trndev, axis=0)) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All   : %s' % str(Y.shape))\n",
    "print('Train : %s, %s' % (X_train.shape, Y_train.shape))\n",
    "print('Dev   : %s' % str(PU_dev.shape))\n",
    "print('Trndev: %s, %s' % (X_trndev.shape, Y_trndev.shape))\n",
    "print('Test  : %s' % str(PU_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(X_train,  gzip.open(fxtrain2, 'wb'))\n",
    "pkl.dump(Y_train,  gzip.open(fytrain2, 'wb'))\n",
    "pkl.dump(Y,        gzip.open(fy2, 'wb'))\n",
    "pkl.dump(X_trndev, gzip.open(fxtrndev2, 'wb'))\n",
    "pkl.dump(Y_trndev, gzip.open(fytrndev2, 'wb'))\n",
    "pkl.dump(PU_dev,   gzip.open(fydev2, 'wb'))\n",
    "pkl.dump(PU_test,  gzip.open(fytest2, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the adjacent matrix of playlists (nodes) for *setting II*, playlists of the same user form a *clique*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cliques in train + dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_users = [u for (_, u) in train_playlists + dev_playlists]\n",
    "cliques_trndev = []\n",
    "for u in sorted(set(pl_users)):\n",
    "    clique = np.where(u == np.array(pl_users, dtype=np.object))[0]\n",
    "    #if len(clique) > 1:\n",
    "    cliques_trndev.append(clique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(cliques_trndev, gzip.open(fclique21, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clqsize = [len(clq) for clq in cliques_trndev]\n",
    "print(np.min(clqsize), np.max(clqsize), len(clqsize), np.sum(clqsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(np.arange(Y_trndev.shape[1]) == np.asarray(sorted([k for clq in cliques_trndev for k in clq])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cliques in train + dev + test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_users = [u for (_, u) in train_playlists + dev_playlists + test_playlists]\n",
    "clique_all = []\n",
    "for u in sorted(set(pl_users)):\n",
    "    clique = np.where(u == np.array(pl_users, dtype=np.object))[0]\n",
    "    #if len(clique) > 1:\n",
    "    clique_all.append(clique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(clique_all, gzip.open(fclique22, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clqsize = [len(clq) for clq in clique_all]\n",
    "print(np.min(clqsize), np.max(clqsize), len(clqsize), np.sum(clqsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(np.arange(Y.shape[1]) == np.asarray(sorted([k for clq in clique_all for k in clq])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
