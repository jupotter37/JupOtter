{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Deep LSTM RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "mx.random.seed(1)\n",
    "# ctx = mx.gpu(0)\n",
    "ctx = mx.cpu(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "# import mpld3\n",
    "sns.set_style('whitegrid')\n",
    "#sns.set_context('notebook')\n",
    "sns.set_context('poster')\n",
    "# Make inline plots vector graphics instead of raster graphics\n",
    "from IPython.display import set_matplotlib_formats\n",
    "#set_matplotlib_formats('pdf', 'svg')\n",
    "set_matplotlib_formats('pdf', 'png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 100 + 1  # needs to be at least the seq_length for training + 1 because of the time shift between inputs and labels\n",
    "NUM_SAMPLES_TRAINING = 5000 + 1\n",
    "NUM_SAMPLES_TESTING = 100 + 1\n",
    "CREATE_DATA_SETS = False  # True if you don't have the data files or re-create them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: \"Some time-series\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gimme_one_random_number():\n",
    "    return nd.random_uniform(low=0, high=1, shape=(1,1)).asnumpy()[0][0]\n",
    "\n",
    "def create_one_time_series(seq_length=10):\n",
    "  freq = (gimme_one_random_number()*0.5) + 0.1  # 0.1 to 0.6\n",
    "  ampl = gimme_one_random_number() + 0.5  # 0.5 to 1.5\n",
    "  x = np.sin(np.arange(0, seq_length) * freq) * ampl\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_batch_time_series(seq_length=10, num_samples=4):\n",
    "    column_labels = ['t'+str(i) for i in range(0, seq_length)]\n",
    "    df = pd.DataFrame(create_one_time_series(seq_length=seq_length)).transpose()\n",
    "    df.columns = column_labels\n",
    "    df.index = ['s'+str(0)]\n",
    "    for i in range(1, num_samples):\n",
    "        more_df = pd.DataFrame(create_one_time_series(seq_length=seq_length)).transpose()\n",
    "        more_df.columns = column_labels\n",
    "        more_df.index = ['s'+str(i)]\n",
    "        df = pd.concat([df, more_df], axis=0)\n",
    "    return df # returns a dataframe of shape (num_samples, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create some time-series\n",
    "# uncomment below to force predictible random numbers\n",
    "# mx.random.seed(1)\n",
    "if CREATE_DATA_SETS:\n",
    "    data_train = create_batch_time_series(seq_length=SEQ_LENGTH, num_samples=NUM_SAMPLES_TRAINING)  \n",
    "    data_test = create_batch_time_series(seq_length=SEQ_LENGTH, num_samples=NUM_SAMPLES_TESTING)\n",
    "    # Write data to csv\n",
    "    data_train.to_csv(\"../data/timeseries/train.csv\")\n",
    "    data_test.to_csv(\"../data/timeseries/test.csv\")\n",
    "else: \n",
    "    data_train = pd.read_csv(\"../data/timeseries/train.csv\", index_col=0)\n",
    "    data_test = pd.read_csv(\"../data/timeseries/test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the data real quick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# num_sampling_points = min(SEQ_LENGTH, 400)\n",
    "# (data_train.sample(4).transpose().iloc[range(0, SEQ_LENGTH, SEQ_LENGTH//num_sampling_points)]).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(data_train.loc[:,data_train.columns[:-1]]) # inputs\n",
    "# print(data_train.loc[:,data_train.columns[1:]])  # outputs (i.e. inputs shift by +1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "batch_size_test = 1\n",
    "seq_length = 16\n",
    "\n",
    "num_batches_train = data_train.shape[0] // batch_size\n",
    "num_batches_test = data_test.shape[0] // batch_size_test\n",
    "\n",
    "num_features = 1  #  we do 1D time series for now, this is like vocab_size = 1 for characters\n",
    "\n",
    "# inputs are from t0 to t_seq_length - 1. because the last point is kept for the output (\"label\") of the penultimate point \n",
    "data_train_inputs = data_train.loc[:,data_train.columns[:-1]]\n",
    "data_train_labels = data_train.loc[:,data_train.columns[1:]]\n",
    "data_test_inputs = data_test.loc[:,data_test.columns[:-1]]\n",
    "data_test_labels = data_test.loc[:,data_test.columns[1:]]\n",
    "\n",
    "train_data_inputs = nd.array(data_train_inputs.values).reshape((num_batches_train, batch_size, seq_length, num_features))\n",
    "train_data_labels = nd.array(data_train_labels.values).reshape((num_batches_train, batch_size, seq_length, num_features))\n",
    "test_data_inputs = nd.array(data_test_inputs.values).reshape((num_batches_test, batch_size_test, seq_length, num_features))\n",
    "test_data_labels = nd.array(data_test_labels.values).reshape((num_batches_test, batch_size_test, seq_length, num_features))\n",
    "\n",
    "train_data_inputs = nd.swapaxes(train_data_inputs, 1, 2)\n",
    "train_data_labels = nd.swapaxes(train_data_labels, 1, 2)\n",
    "test_data_inputs = nd.swapaxes(test_data_inputs, 1, 2)\n",
    "test_data_labels = nd.swapaxes(test_data_labels, 1, 2)\n",
    "\n",
    "\n",
    "print('num_samples_training={0} | num_batches_train={1} | batch_size={2} | seq_length={3}'.format(NUM_SAMPLES_TRAINING, num_batches_train, batch_size, seq_length))\n",
    "print('train_data_inputs shape: ', train_data_inputs.shape)\n",
    "print('train_data_labels shape: ', train_data_labels.shape)\n",
    "# print(data_train_inputs.values)\n",
    "# print(train_data_inputs[0]) # see what one batch looks like\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long short-term memory (LSTM) RNNs\n",
    "\n",
    "An LSTM block has mechanisms to enable \"memorizing\" information for an extended number of time steps. We use the LSTM block with the following transformations that map inputs to outputs across blocks at consecutive layers and consecutive time steps: $\\newcommand{\\xb}{\\mathbf{x}} \\newcommand{\\RR}{\\mathbb{R}}$\n",
    "\n",
    "$$g_t = \\text{tanh}(X_t W_{xg} + h_{t-1} W_{hg} + b_g),$$\n",
    "\n",
    "$$i_t = \\sigma(X_t W_{xi} + h_{t-1} W_{hi} + b_i),$$\n",
    "\n",
    "$$f_t = \\sigma(X_t W_{xf} + h_{t-1} W_{hf} + b_f),$$\n",
    "\n",
    "$$o_t = \\sigma(X_t W_{xo} + h_{t-1} W_{ho} + b_o),$$\n",
    "\n",
    "$$c_t = f_t \\odot c_{t-1} + i_t \\odot g_t,$$\n",
    "\n",
    "$$h_t = o_t \\odot \\text{tanh}(c_t),$$\n",
    "\n",
    "where $\\odot$ is an element-wise multiplication operator, and\n",
    "for all $\\xb = [x_1, x_2, \\ldots, x_k]^\\top \\in \\RR^k$ the two activation functions:\n",
    "\n",
    "$$\\sigma(\\xb) = \\left[\\frac{1}{1+\\exp(-x_1)}, \\ldots, \\frac{1}{1+\\exp(-x_k)}]\\right]^\\top,$$\n",
    "\n",
    "$$\\text{tanh}(\\xb) = \\left[\\frac{1-\\exp(-2x_1)}{1+\\exp(-2x_1)},  \\ldots, \\frac{1-\\exp(-2x_k)}{1+\\exp(-2x_k)}\\right]^\\top.$$\n",
    "\n",
    "In the transformations above, the memory cell $c_t$ stores the \"long-term\" memory in the vector form.\n",
    "In other words, the information accumulatively captured and  encoded  until time step $t$ is stored in $c_t$ and is only passed along the same layer over different time steps.\n",
    "\n",
    "Given the inputs $c_t$ and $h_t$, the input gate $i_t$ and forget gate $f_t$ will help the memory cell to decide how to overwrite or keep the memory information. The output gate $o_t$ further lets the LSTM block decide how to retrieve the memory information to generate the current state $h_t$ that is passed to both the next layer of the current time step and the next time step of the current layer. Such decisions are made using the hidden-layer parameters $W$ and $b$ with different subscripts: these parameters will be inferred during the training phase by ``gluon``.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allocate parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_inputs = num_features  # for a 1D time series, this is just a scalar equal to 1.0\n",
    "num_outputs = num_features  # same comment\n",
    "num_hidden_units = [8, 8]  # num of hidden units in each hidden LSTM layer\n",
    "num_hidden_layers = len(num_hidden_units)  # num of hidden LSTM layers\n",
    "num_units_layers = [num_features] + num_hidden_units\n",
    "\n",
    "########################\n",
    "#  Weights connecting the inputs to the hidden layer\n",
    "########################\n",
    "Wxg, Wxi, Wxf, Wxo, Whg, Whi, Whf, Who, bg, bi, bf, bo = {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {} \n",
    "for i_layer in range(1, num_hidden_layers+1):\n",
    "    num_inputs = num_units_layers[i_layer-1]\n",
    "    num_hidden_units = num_units_layers[i_layer]\n",
    "    Wxg[i_layer] = nd.random_normal(shape=(num_inputs,num_hidden_units), ctx=ctx) * .01\n",
    "    Wxi[i_layer] = nd.random_normal(shape=(num_inputs,num_hidden_units), ctx=ctx) * .01\n",
    "    Wxf[i_layer] = nd.random_normal(shape=(num_inputs,num_hidden_units), ctx=ctx) * .01\n",
    "    Wxo[i_layer] = nd.random_normal(shape=(num_inputs,num_hidden_units), ctx=ctx) * .01\n",
    "\n",
    "    ########################\n",
    "    #  Recurrent weights connecting the hidden layer across time steps\n",
    "    ########################\n",
    "    Whg[i_layer] = nd.random_normal(shape=(num_hidden_units, num_hidden_units), ctx=ctx) * .01\n",
    "    Whi[i_layer] = nd.random_normal(shape=(num_hidden_units, num_hidden_units), ctx=ctx) * .01\n",
    "    Whf[i_layer] = nd.random_normal(shape=(num_hidden_units, num_hidden_units), ctx=ctx) * .01\n",
    "    Who[i_layer] = nd.random_normal(shape=(num_hidden_units, num_hidden_units), ctx=ctx) * .01\n",
    "\n",
    "    ########################\n",
    "    #  Bias vector for hidden layer\n",
    "    ########################\n",
    "    bg[i_layer] = nd.random_normal(shape=num_hidden_units, ctx=ctx) * .01\n",
    "    bi[i_layer] = nd.random_normal(shape=num_hidden_units, ctx=ctx) * .01\n",
    "    bf[i_layer] = nd.random_normal(shape=num_hidden_units, ctx=ctx) * .01\n",
    "    bo[i_layer] = nd.random_normal(shape=num_hidden_units, ctx=ctx) * .01\n",
    "\n",
    "########################\n",
    "# Weights to the output nodes\n",
    "########################\n",
    "Why = nd.random_normal(shape=(num_units_layers[-1], num_outputs), ctx=ctx) * .01\n",
    "by = nd.random_normal(shape=num_outputs, ctx=ctx) * .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = []\n",
    "for i_layer in range(1, num_hidden_layers+1):\n",
    "    params += [Wxg[i_layer], Wxi[i_layer], Wxf[i_layer], Wxo[i_layer], Whg[i_layer], Whi[i_layer], Whf[i_layer], Who[i_layer], bg[i_layer], bi[i_layer], bf[i_layer], bo[i_layer]]\n",
    "\n",
    "params += [Why, by]  # add the output layer\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(y_linear, temperature=1.0):\n",
    "    lin = (y_linear-nd.max(y_linear)) / temperature\n",
    "    exp = nd.exp(lin)\n",
    "    partition = nd.sum(exp, axis=0, exclude=True).reshape((-1,1))\n",
    "    return exp / partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy(yhat, y):\n",
    "    return - nd.mean(nd.sum(y * nd.log(yhat), axis=0, exclude=True))\n",
    "\n",
    "def rmse(yhat, y):\n",
    "    return nd.mean(nd.sqrt(nd.sum(nd.power(y - yhat, 2), axis=0, exclude=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging the loss over the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_ce_loss(outputs, labels):\n",
    "    assert(len(outputs) == len(labels))\n",
    "    total_loss = 0.\n",
    "    for (output, label) in zip(outputs,labels):\n",
    "        total_loss = total_loss + cross_entropy(output, label)\n",
    "    return total_loss / len(outputs)\n",
    "\n",
    "def average_rmse_loss(outputs, labels):\n",
    "    assert(len(outputs) == len(labels))\n",
    "    total_loss = 0.\n",
    "    for (output, label) in zip(outputs,labels):\n",
    "        total_loss = total_loss + rmse(output, label)\n",
    "    return total_loss / len(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(params, learning_rate):\n",
    "    for param in params:\n",
    "#         print('grrrrr: ', param.grad)\n",
    "        param[:] = param - learning_rate * param.grad\n",
    "\n",
    "def adam(params, learning_rate, M , R, index_adam_call, beta1, beta2, eps):\n",
    "    k = -1\n",
    "    for param in params:\n",
    "        k += 1\n",
    "        M[k] = beta1 * M[k] + (1. - beta1) * param.grad\n",
    "        R[k] = beta2 * R[k] + (1. - beta2) * (param.grad)**2\n",
    "        # bias correction since we initilized M & R to zeros, they're biased toward zero on the first few iterations\n",
    "        m_k_hat = M[k] / (1. - beta1**(index_adam_call))\n",
    "        r_k_hat = R[k] / (1. - beta2**(index_adam_call))\n",
    "        if((np.isnan(M[k].asnumpy())).any() or (np.isnan(R[k].asnumpy())).any()):\n",
    "#             print('GRRRRRR ', M, K)\n",
    "            stop()\n",
    "#         print('grrrrr: ', param.grad)\n",
    "        param[:] = param - learning_rate * m_k_hat / (nd.sqrt(r_k_hat) + eps)\n",
    "#     print('m_k_hat r_k_hat', m_k_hat, r_k_hat)\n",
    "    return params, M, R\n",
    "\n",
    "# def adam(params, learning_rate, M, R, index_iteration, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "#     for k, param in enumerate(params):\n",
    "#         if k==0:\n",
    "#             print('batch_iteration {}: {}'.format(index_iteration, param))\n",
    "#         M[k] = beta1 * M[k] + (1. - beta1) * param.grad\n",
    "#         R[k] = beta2 * R[k] + (1. - beta2) * (param.grad)**2\n",
    "\n",
    "#         m_k_hat = M[k] / (1. - beta1**(index_iteration))\n",
    "#         r_k_hat = R[k] / (1. - beta2**(index_iteration))\n",
    "\n",
    "#         param[:] = param - learning_rate * m_k_hat / (nd.sqrt(r_k_hat) + eps)\n",
    "# #         print(beta1, beta2, M, R)\n",
    "#         if k==0:\n",
    "#             print('batch_iteration {}: {}'.format(index_iteration, param.grad))\n",
    "    \n",
    "#     for k, param in enumerate(params):\n",
    "#         print('batch_iteration {}: {}'.format(index_iteration, param))\n",
    "\n",
    "#     return M, R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def single_lstm_unit_calcs(X, c, Wxg, h, Whg, bg, Wxi, Whi, bi, Wxf, Whf, bf, Wxo, Who, bo):\n",
    "    g = nd.tanh(nd.dot(X, Wxg) + nd.dot(h, Whg) + bg)\n",
    "    i = nd.sigmoid(nd.dot(X, Wxi) + nd.dot(h, Whi) + bi)\n",
    "    f = nd.sigmoid(nd.dot(X, Wxf) + nd.dot(h, Whf) + bf)\n",
    "    o = nd.sigmoid(nd.dot(X, Wxo) + nd.dot(h, Who) + bo)\n",
    "    #######################\n",
    "    c = f * c + i * g\n",
    "    h = o * nd.tanh(c)\n",
    "    return c, h\n",
    "\n",
    "def deep_lstm_rnn(inputs, h, c, temperature=1.0):\n",
    "    \"\"\"\n",
    "        h: dict of nd.arrays, each key is the index of a hidden layer (from 1 to whatever). \n",
    "        Index 0, if any, is the input layer\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    # inputs is one BATCH of sequences so its shape is number_of_seq, seq_length, features_dim \n",
    "    # (latter is 1 for a time series, vocab_size for a character, n for a n different times series)\n",
    "    for X in inputs:\n",
    "        # X is batch of one time stamp. E.g. if each batch has 37 sequences, then the first value of X will be a set of the 37 first values of each of the 37 sequences \n",
    "        # that means each iteration on X corresponds to one time stamp, but it is done in batches of different sequences\n",
    "        h[0] = X # the first hidden layer takes the input X as input \n",
    "        for i_layer in range(1, num_hidden_layers+1):\n",
    "            # lstm units now have the 2 following inputs: \n",
    "            # i) h_t from the previous layer (equivalent to the input X for a non-deep lstm net), \n",
    "            # ii) h_t-1 from the current layer (same as for non-deep lstm nets)\n",
    "            c[i_layer], h[i_layer] = single_lstm_unit_calcs(h[i_layer-1], c[i_layer], Wxg[i_layer], h[i_layer], Whg[i_layer], bg[i_layer], Wxi[i_layer], Whi[i_layer], bi[i_layer], Wxf[i_layer], Whf[i_layer], bf[i_layer], Wxo[i_layer], Who[i_layer], bo[i_layer])\n",
    "        yhat_linear = nd.dot(h[num_hidden_layers], Why) + by\n",
    "        # yhat is a batch of several values of the same time stamp\n",
    "        # this is basically the prediction of the sequence, which overlaps most of the input sequence, plus one point (character or value)\n",
    "#         yhat = softmax(yhat_linear, temperature=temperature)\n",
    "#         yhat = nd.sigmoid(yhat_linear)\n",
    "#         yhat = nd.tanh(yhat_linear)\n",
    "        yhat = yhat_linear # we cant use a 1.0-bounded activation function since amplitudes can be greater than 1.0\n",
    "        outputs.append(yhat) # outputs has same shape as inputs, i.e. a list of batches of data points.\n",
    "#     print('some shapes... yhat outputs', yhat.shape, len(outputs) )\n",
    "    return (outputs, h, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_prediction(one_input_seq, one_label_seq, temperature=1.0):\n",
    "    #####################################\n",
    "    # Set the initial state of the hidden representation ($h_0$) to the zero vector\n",
    "    #####################################  # some better initialization needed??\n",
    "    h, c = {}, {}\n",
    "    for i_layer in range(1, num_hidden_layers+1):\n",
    "        h[i_layer] = nd.zeros(shape=(batch_size_test, num_units_layers[i_layer]), ctx=ctx)\n",
    "        c[i_layer] = nd.zeros(shape=(batch_size_test, num_units_layers[i_layer]), ctx=ctx)\n",
    "    \n",
    "    outputs, h, c = deep_lstm_rnn(one_input_seq, h, c, temperature=temperature)\n",
    "    loss = rmse(outputs[-1][0], one_label_seq)\n",
    "    return outputs[-1][0].asnumpy()[-1], one_label_seq.asnumpy()[-1], loss.asnumpy()[-1], outputs, one_label_seq\n",
    "\n",
    "def check_prediction(index):\n",
    "    o, label, loss, outputs, labels = test_prediction(test_data_inputs[index], test_data_labels[index], temperature=1.0)\n",
    "    prediction = round(o, 3)\n",
    "    true_label = round(label, 3)\n",
    "    outputs = [float(i.asnumpy().flatten()) for i in outputs]\n",
    "    true_labels = list(test_data_labels[index].asnumpy().flatten())\n",
    "    # print(outputs, '\\n----\\n', true_labels)\n",
    "    df = pd.DataFrame([outputs, true_labels]).transpose()\n",
    "    df.columns = ['predicted', 'true']\n",
    "    # print(df)\n",
    "    rel_error = round(100. * (prediction / true_label - 1.0), 2)\n",
    "#     print('\\nprediction = {0} | actual_value = {1} | rel_error = {2}'.format(prediction, true_label, rel_error))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 48  # at some point, some nans appear in M, R matrices of Adam. TODO investigate why\n",
    "moving_loss = 0.\n",
    "learning_rate = 0.001  # 0.1 works for a [8, 8] after about 70 epochs of 32-sized batches\n",
    "\n",
    "# Adam Optimizer stuff\n",
    "beta1 = .9\n",
    "beta2 = .999\n",
    "index_adam_call = 0\n",
    "# M & R arrays to keep track of momenta in adam optimizer. params is a list that contains all ndarrays of parameters\n",
    "M = {k: nd.zeros_like(v) for k, v in enumerate(params)}\n",
    "R = {k: nd.zeros_like(v) for k, v in enumerate(params)}\n",
    "\n",
    "df_moving_loss = pd.DataFrame(columns=['Loss', 'Error'])\n",
    "df_moving_loss.index.name = 'Epoch'\n",
    "\n",
    "# needed to update plots on the fly\n",
    "%matplotlib notebook\n",
    "fig, axes_fig1 = plt.subplots(1,1, figsize=(6,3))\n",
    "fig2, axes_fig2 = plt.subplots(1,1, figsize=(6,3))\n",
    "\n",
    "for e in range(epochs):\n",
    "    ############################\n",
    "    # Attenuate the learning rate by a factor of 2 every 100 epochs\n",
    "    ############################\n",
    "    if ((e+1) % 80 == 0):\n",
    "        learning_rate = learning_rate / 2.0  # TODO check if its ok to adjust learning_rate when using Adam Optimizer\n",
    "    h, c = {}, {}\n",
    "    for i_layer in range(1, num_hidden_layers+1):\n",
    "        h[i_layer] = nd.zeros(shape=(batch_size, num_units_layers[i_layer]), ctx=ctx)\n",
    "        c[i_layer] = nd.zeros(shape=(batch_size, num_units_layers[i_layer]), ctx=ctx)\n",
    "\n",
    "    for i in range(num_batches_train):\n",
    "        data_one_hot = train_data_inputs[i]\n",
    "        label_one_hot = train_data_labels[i]\n",
    "        with autograd.record():\n",
    "            outputs, h, c = deep_lstm_rnn(data_one_hot, h, c)\n",
    "            loss = average_rmse_loss(outputs, label_one_hot)\n",
    "            loss.backward()\n",
    "#         SGD(params, learning_rate)\n",
    "        index_adam_call += 1  # needed for bias correction in Adam optimizer\n",
    "        params, M, R = adam(params, learning_rate, M, R, index_adam_call, beta1, beta2, 1e-8)\n",
    "        \n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        if (i == 0) and (e == 0):\n",
    "            moving_loss = nd.mean(loss).asscalar()\n",
    "        else:\n",
    "            moving_loss = .99 * moving_loss + .01 * nd.mean(loss).asscalar()\n",
    "        df_moving_loss.loc[e] = round(moving_loss, 4)\n",
    "\n",
    "    ############################\n",
    "    #  Predictions and plots\n",
    "    ############################\n",
    "    data_prediction_df = check_prediction(index=e)\n",
    "    axes_fig1.clear()\n",
    "    data_prediction_df.plot(ax=axes_fig1)\n",
    "    fig.canvas.draw()\n",
    "    prediction = round(data_prediction_df.tail(1)['predicted'].values.flatten()[-1], 3)\n",
    "    true_label = round(data_prediction_df.tail(1)['true'].values.flatten()[-1], 3)\n",
    "    rel_error = round(100. * np.abs(prediction / true_label - 1.0), 2)\n",
    "    print(\"Epoch = {0} | Loss = {1} | Prediction = {2} True = {3} Error = {4}\".format(e, moving_loss, prediction, true_label, rel_error ))\n",
    "    axes_fig2.clear()\n",
    "    if e == 0:\n",
    "        moving_rel_error = rel_error\n",
    "    else:\n",
    "        moving_rel_error = .9 * moving_rel_error + .1 * rel_error\n",
    "\n",
    "    df_moving_loss.loc[e, ['Error']] = moving_rel_error\n",
    "    axes_loss_plot = df_moving_loss.plot(ax=axes_fig2, secondary_y='Loss', color=['r','b'])\n",
    "    axes_loss_plot.right_ax.grid(False)\n",
    "#     axes_loss_plot.right_ax.set_yscale('log')\n",
    "    fig2.canvas.draw()\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
