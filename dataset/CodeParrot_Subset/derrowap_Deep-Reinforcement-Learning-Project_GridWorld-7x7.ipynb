{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import itertools\n",
    "import os\n",
    "from IPython import display\n",
    "import time\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gameOb():\n",
    "    def __init__(self,coordinates,size,intensity,channel,reward,name):\n",
    "        self.x = coordinates[0]\n",
    "        self.y = coordinates[1]\n",
    "        self.size = size\n",
    "        self.intensity = intensity\n",
    "        self.channel = channel\n",
    "        self.reward = reward\n",
    "        self.name = name\n",
    "        \n",
    "class gameEnv():\n",
    "    def __init__(self,partial,size):\n",
    "        self.sizeX = size\n",
    "        self.sizeY = size\n",
    "        self.actions = 4\n",
    "        self.objects = []\n",
    "        self.partial = partial\n",
    "        self.a = self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.objects = []\n",
    "        hero = gameOb(self.newPosition(),1,1,2,None,'hero')\n",
    "        self.objects.append(hero)\n",
    "        bug = gameOb(self.newPosition(),1,1,1,10,'goal')\n",
    "        self.objects.append(bug)\n",
    "        hole = gameOb(self.newPosition(),1,1,0,-2,'fire')\n",
    "        self.objects.append(hole)\n",
    "        hole2 = gameOb(self.newPosition(),1,1,0,-2,'fire')\n",
    "        self.objects.append(hole2)\n",
    "        self.a = self.renderEnv()\n",
    "        return self.a\n",
    "\n",
    "    def moveChar(self,direction):\n",
    "        # 0 - up, 1 - down, 2 - left, 3 - right\n",
    "        hero = self.objects[0]\n",
    "        heroX = hero.x\n",
    "        heroY = hero.y\n",
    "        penalize = 0.00\n",
    "        if direction == 0 and hero.y >= 1:\n",
    "            hero.y -= 1\n",
    "        if direction == 1 and hero.y <= self.sizeY-2:\n",
    "            hero.y += 1\n",
    "        if direction == 2 and hero.x >= 1:\n",
    "            hero.x -= 1\n",
    "        if direction == 3 and hero.x <= self.sizeX-2:\n",
    "            hero.x += 1     \n",
    "        if hero.x == heroX and hero.y == heroY:\n",
    "            penalize = 1\n",
    "        self.objects[0] = hero\n",
    "        return penalize\n",
    "    \n",
    "    def newPosition(self):\n",
    "        iterables = [ range(self.sizeX), range(self.sizeY)]\n",
    "        points = []\n",
    "        for t in itertools.product(*iterables):\n",
    "            points.append(t)\n",
    "        currentPositions = []\n",
    "        for objectA in self.objects:\n",
    "            if (objectA.x,objectA.y) not in currentPositions:\n",
    "                currentPositions.append((objectA.x,objectA.y))\n",
    "        for pos in currentPositions:\n",
    "            points.remove(pos)\n",
    "        location = np.random.choice(range(len(points)),replace=False)\n",
    "        return points[location]\n",
    "\n",
    "    def checkGoal(self):\n",
    "        others = []\n",
    "        for obj in self.objects:\n",
    "            if obj.name == 'hero':\n",
    "                hero = obj\n",
    "            else:\n",
    "                others.append(obj)\n",
    "        ended = False\n",
    "        for other in others:\n",
    "            if hero.x == other.x and hero.y == other.y:\n",
    "                self.objects.remove(other)\n",
    "                if other.reward == 1:\n",
    "                    self.objects.append(gameOb(self.newPosition(),1,1,1,10,'goal'))\n",
    "                else: \n",
    "                    self.objects.append(gameOb(self.newPosition(),1,1,0,-2,'fire'))\n",
    "                return other.reward,True\n",
    "        if ended == False:\n",
    "            return 0.0,False\n",
    "\n",
    "    def renderEnv(self):\n",
    "        #a = np.zeros([self.sizeY,self.sizeX,3])\n",
    "        self.a = np.ones([self.sizeY+2,self.sizeX+2,3])\n",
    "        self.a[1:-1,1:-1,:] = 0\n",
    "        hero = None\n",
    "        for item in self.objects:\n",
    "            self.a[item.y+1:item.y+item.size+1,item.x+1:item.x+item.size+1,item.channel] = item.intensity\n",
    "            if item.name == 'hero':\n",
    "                hero = item\n",
    "        if self.partial == True:\n",
    "            self.a = self.a[hero.y:hero.y+3,hero.x:hero.x+3,:]\n",
    "        b = scipy.misc.imresize(self.a[:,:,0],[84,84,1],interp='nearest')\n",
    "        c = scipy.misc.imresize(self.a[:,:,1],[84,84,1],interp='nearest')\n",
    "        d = scipy.misc.imresize(self.a[:,:,2],[84,84,1],interp='nearest')\n",
    "        self.a = np.stack([b,c,d],axis=2)\n",
    "        return self.a\n",
    "\n",
    "    def step(self,action):\n",
    "        penalty = self.moveChar(action)\n",
    "        reward,done = self.checkGoal()\n",
    "        if(penalty != 0):\n",
    "            done = True\n",
    "        state = self.renderEnv()\n",
    "        return state,(reward-penalty),done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = gameEnv(partial=False,size=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size,prev_states):\n",
    "#         self.image_to_resize = tf.placeholder(shape=[1,84,84,3], dtype=tf.float32,name='image_to_resize')\n",
    "#         self.y = tf.identity(self.image_to_resize)\n",
    "#         self.resized_image = tf.image.resize_images(self.y, 84, 84)\n",
    "        \n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        self.imageIn = tf.placeholder(shape=[None,84,84,3],dtype=tf.float32)\n",
    "        self.conv1 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.imageIn,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', biases_initializer=None)\n",
    "        self.conv2 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        self.conv4 = tf.contrib.layers.convolution2d( \\\n",
    "            inputs=self.conv3,num_outputs=512,kernel_size=[7,7],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        \n",
    "        #We take the output from the final convolutional layer and split it into separate advantage and value streams.\n",
    "        self.streamAC,self.streamVC = tf.split(3,2,self.conv4)\n",
    "        self.streamA = tf.contrib.layers.flatten(self.streamAC)\n",
    "        self.streamV = tf.contrib.layers.flatten(self.streamVC)\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size//2,env.actions]))\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size//2,1]))\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        #Then combine them together to get our final Q-values.\n",
    "        self.Qout = self.Value + tf.sub(self.Advantage,tf.reduce_mean(self.Advantage,reduction_indices=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,env.actions,dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.mul(self.Qout, self.actions_onehot), reduction_indices=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    \"\"\"Used to store experiences and samples randomly to train the network.\"\"\"\n",
    "    def __init__(self, buffer_size=50000):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        \n",
    "    def add(self, states, actions, rewards, dones):\n",
    "        if len(self.actions) == self.buffer_size:\n",
    "            self.states = self.states[1:]\n",
    "            self.actions = self.actions[1:]\n",
    "            self.rewards = self.rewards[1:]\n",
    "            self.dones = self.dones[1:]\n",
    "\n",
    "        self.states.append(states)\n",
    "        self.actions.append(actions)\n",
    "        self.rewards.append(rewards)\n",
    "        self.dones.append(dones)\n",
    "        \n",
    "    def sample(self, size, previous_states):\n",
    "        samples = np.random.permutation(len(self.actions)-(previous_states-1)) + (previous_states-1)\n",
    "\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        states_ = []\n",
    "        dones = []\n",
    "        for i in samples[:size]:\n",
    "            temp = []\n",
    "            for j in range(previous_states):\n",
    "                temp.append(self.states[i - previous_states + j + 1])\n",
    "            states.append(np.dstack(temp))\n",
    "            actions.append(self.actions[i])\n",
    "            rewards.append(self.rewards[i])\n",
    "            states_.append(self.states[i+1])\n",
    "            dones.append(self.dones[i])\n",
    "                \n",
    "        return states, actions, rewards, states_, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 #How many experiences to use for each training step.\n",
    "update_freq = 8 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "anneling_steps = 75000 #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 100003 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 20000 #How many steps of random actions before training begins.\n",
    "pre_train_steps_from_Q = False #If true, initialize buffer with steps from Q instead of random actions\n",
    "max_epLength = 20 #The max allowed length of our episode.\n",
    "load_model = False #Whether to load a saved model.\n",
    "path = \"./dqn/save_data/gridWorld/seven_by_seven/\" #The path to save our model to.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.0001 #Rate to update target network toward primary network\n",
    "previous_states=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distance(x1, y1, x2, y2):\n",
    "    return math.sqrt((x2-x1)**2+(y2-y1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size,previous_states)\n",
    "targetQN = Qnetwork(h_size,previous_states)\n",
    "t_start = time.time()\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/anneling_steps\n",
    "\n",
    "#arrays to save\n",
    "eps_arr = []\n",
    "time_arr = []\n",
    "err_arr = []\n",
    "annel_arr = []\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "step_list = []\n",
    "reward_list = []\n",
    "total_steps = 0\n",
    "hero_x = 0\n",
    "hero_y = 0\n",
    "least_distance = 100\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        load = './dqn/awjuliani/algorithm_train1/model-70000.cptk'\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess,load)\n",
    "    sess.run(init)\n",
    "    updateTarget(targetOps,sess) #Set the target network to be equal to the primary network.\n",
    "    for i in range(1, num_episodes):\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "#         s = np.reshape(s, [-1, 84, 84, 3])\n",
    "        if(i==1):\n",
    "            myBuffer.states.append(s)\n",
    "        d = False\n",
    "        reward_sum = 0\n",
    "        step = 0\n",
    "        #The Q-Network\n",
    "        while step < max_epLength: #If the agent takes longer than 200 moves to reach either of the blocks, end the trial.\n",
    "            step+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:       \n",
    "                goal_x = 0\n",
    "                goal_y = 0\n",
    "                for obj in env.objects:\n",
    "                    if(obj.name == 'hero'):\n",
    "                        hero_x = obj.x\n",
    "                        hero_y = obj.y\n",
    "                    if(obj.name == 'goal'):\n",
    "                        dist = distance(hero_x,hero_y,obj.x,obj.y)\n",
    "                        if(dist<least_distance):\n",
    "                            lesast_distance = dist\n",
    "                            goal_x = obj.x\n",
    "                            goal_y = obj.y\n",
    "\n",
    "                            x_dir = hero_x - goal_x\n",
    "                            y_dir = hero_y - goal_y\n",
    "\n",
    "                            if(y_dir > 0):\n",
    "                                a = 0\n",
    "                            elif(y_dir < 0):\n",
    "                                a = 1\n",
    "                            elif(x_dir > 0):\n",
    "                                a = 2\n",
    "                            elif(x_dir < 0):\n",
    "                                a = 3\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict,feed_dict={mainQN.imageIn:[s]})[0]\n",
    "            s1,r,d = env.step(a)\n",
    "#             s1 = np.reshape(s1, [-1, 84, 84, 3])\n",
    "            total_steps += 1\n",
    "            myBuffer.add(s1,a,r,d) #Save the experience to our episode buffer.\n",
    "\n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "\n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    states, actions, rewards, state_, done = myBuffer.sample(batch_size,previous_states) #Get a random batch of experiences.\n",
    "                    #Below we perform the Double-DQN update to the target Q-values\n",
    "                    Q1 = sess.run(mainQN.predict,feed_dict={mainQN.imageIn:state_})\n",
    "                    Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.imageIn:state_})\n",
    "                    end_multiplier = -(np.array(done) - 1)\n",
    "                    doubleQ = Q2[range(batch_size),Q1]\n",
    "                    targetQ = rewards + (y*doubleQ * end_multiplier)\n",
    "                    #Update the network with our target values.\n",
    "                    _ = sess.run(mainQN.updateModel, \\\n",
    "                        feed_dict={mainQN.imageIn:states,mainQN.targetQ:targetQ, mainQN.actions:actions})\n",
    "\n",
    "                    updateTarget(targetOps,sess) #Set the target network to be equal to the primary network.\n",
    "            reward_sum += r\n",
    "            s = s1\n",
    "\n",
    "            if d == True:\n",
    "\n",
    "                break\n",
    "\n",
    "        #Get all experiences from this episode and discount their rewards. \n",
    "        step_list.append(step)\n",
    "        reward_list.append(reward_sum)\n",
    "        #Periodically save the model. \n",
    "        if i % 1000 == 0:\n",
    "            print(str(i), \" -- avg steps \",np.mean(step_list[-1000:]), \" -- %complete \", (len([k for k in reward_list[-1000:] if k > 0]) / len(reward_list[-1000:])), ', anneling -- ', e)\n",
    "            eps_arr.append(i)\n",
    "            time_arr.append(time.time() - t_start)\n",
    "            err_arr.append((len([k for k in reward_list[-100:] if k > 0]) / len(reward_list[-100:])))\n",
    "            annel_arr.append(e)\n",
    "        if i % 10000 == 0:\n",
    "            saver.save(sess,path+'model-'+str(i)+'.cptk')\n",
    "            np.savez(path+'data' + str(i)+ '.npz', episode=eps_arr, time=time_arr, error = err_arr, anneling = annel_arr)\n",
    "            print(\"Saved Model @ \", path+'model-'+str(i)+'.cptk')\n",
    "            \n",
    "    saver.save(sess,path+'model-'+str(i)+'.cptk')\n",
    "    np.savez(path+'data' + str(i)+ '.npz', episode=eps_arr, time=time_arr, error = err_arr, anneling = annel_arr)\n",
    "print(\"Percent of succesful episodes: \" + str(sum(reward_list)/num_episodes) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directions = ['up', 'down', 'left', 'right']\n",
    "arr = []\n",
    "\n",
    "mainQN = Qnetwork(h_size, previous_states)\n",
    "targetQN = Qnetwork(h_size, previous_states)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    load = './dqn/save_data/gridWorld/one_go1/model-50000.cptk'\n",
    "    saver.restore(sess, load)\n",
    "    for i in range(20):\n",
    "        complete = False\n",
    "        iter = 0\n",
    "        env = gameEnv(partial=False,size=5)\n",
    "        s = env.reset()\n",
    "        plt.imshow(env.a)\n",
    "        r_all = 0\n",
    "        while(complete == False):\n",
    "            \n",
    "            action = sess.run(mainQN.predict, feed_dict={mainQN.imageIn:[s]})\n",
    "            s, reward, done = env.step(action)\n",
    "            plt.imshow(env.a)\n",
    "#             display.clear_output(wait=True)\n",
    "#             display.display(plt.gcf())\n",
    "            r_all += reward\n",
    "            if(iter > 20):\n",
    "                done = True\n",
    "            iter += 1\n",
    "            if(done):\n",
    "                arr.append(r_all)\n",
    "                complete = True\n",
    "                print(iter)\n",
    "                print(\"Complete\")\n",
    "                print(\"reward: \", r_all)\n",
    "#             time.sleep(.5)\n",
    "            plt.show()\n",
    "\n",
    "(len([k for k in arr[-100:] if k > 0]) / len(arr[-100:]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
