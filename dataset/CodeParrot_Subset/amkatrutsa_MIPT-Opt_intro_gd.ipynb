{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "196b8a50-3d29-45c3-82b9-f4a09b49491d"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Введение в численные методы оптимизации (Ю. Е. Нестеров Введение в выпуклую оптимизацию, гл. 1 $\\S$ 1.1)\n",
    "\n",
    " 1. Обзор материала весеннего семестра\n",
    " 2. Постановка задачи\n",
    " 3. Общая схема решения\n",
    " 4. Сравнение методов оптимизации\n",
    " 5. Методы одномерной минимизации\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2a573842-172b-4931-b0dd-9c9d3c47a450"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Обзор материала весеннего семестра\n",
    "\n",
    "Также на [странице курса](https://github.com/amkatrutsa/MIPT-Opt/blob/master/Spring2021/README.md).\n",
    "\n",
    "1. Методы решения задач **безусловной** оптимизации\n",
    "    - Одномерная минимизация (**уже сегодня!**)\n",
    "    - Градиентный спуск и способы его ускорения\n",
    "    - Метод Ньютона\n",
    "    - Квазиньютоновские методы\n",
    "    - Метод сопряжённых градиентов  \n",
    "    - Решение задачи наименьших квадратов\n",
    "2. Методы решения задач **условной** оптимизации\n",
    "    - Методы проекции градиента и условного градиента\n",
    "    - Проксимальные методы\n",
    "    - Методы штрафных и барьерных функций\n",
    "    - Метод модифицированой функции Лагранжа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Постановка задачи\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "& \\min_{x \\in S} f_0(x)\\\\\n",
    "\\text{s.t. } & f_j(x) = 0, \\; j = 1,\\ldots,m\\\\\n",
    "& g_k(x) \\leq 0, \\; k = 1,\\ldots,p\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "где $S \\subseteq \\mathbb{R}^n$, $f_j: S \\rightarrow \\mathbb{R}, \\; j = 0,\\ldots,m$, $g_k: S \\rightarrow \\mathbb{R}, \\; k=1,\\ldots,p$\n",
    "\n",
    "Все функции как минимум непрерывны. \n",
    "\n",
    "Важный факт</span>: задачи **нелинейной** оптимизации \n",
    "\n",
    "в их самой общей форме являются **численно неразрешимыми**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Аналитические результаты\n",
    "- Необходимое условие первого порядка: \n",
    "\n",
    "если $x^*$ точка локального минимума дифференцируемой функции $f(x)$, тогда \n",
    "\n",
    "$$\n",
    "f'(x^*) = 0\n",
    "$$\n",
    "\n",
    "- Необходимое условие второго порядка \n",
    "\n",
    "если $x^*$ точка локального минимума дважды дифференцируемой функции $f(x)$, тогда \n",
    "\n",
    "$$\n",
    "f'(x^*) = 0 \\quad \\text{и} \\quad f''(x^*) \\succeq 0\n",
    "$$\n",
    "\n",
    "- Достаточное условие: \n",
    "\n",
    "пусть $f(x)$ дважды дифференцируемая функция, и пусть точка $x^*$ удовлетворяет условиям\n",
    "\n",
    "$$\n",
    "f'(x^*) = 0 \\quad f''(x^*) \\succ 0,\n",
    "$$\n",
    "\n",
    "тогда $x^*$ является точкой строго локального минимума функции $f(x)$.\n",
    "\n",
    "**Замечание**: убедитесь, что Вы понимаете, как доказывать эти\n",
    "\n",
    "результаты!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Особенности численного решения\n",
    "\n",
    "1. Точно решить задачу принципиально невозможно из-за погрешности машинной арифметики\n",
    "2. Необходимо задать критерий обнаружения решения\n",
    "3. Необходимо определить, какую информацию о задаче использовать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Общая итеративная схема\n",
    "\n",
    "Дано: начальное приближение $x$, требуемая точность $\\varepsilon$.\n",
    "\n",
    "```python\n",
    "def GeneralScheme(x, epsilon):\n",
    "    \n",
    "    while StopCriterion(x) > epsilon:\n",
    "        \n",
    "        OracleResponse = RequestOracle(x)\n",
    "        \n",
    "        UpdateInformation(I, x, OracleResponse)\n",
    "        \n",
    "        x = NextPoint(I, x)\n",
    "        \n",
    "    return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Вопросы\n",
    "1. Какие критерии остановки могут быть?\n",
    "2. Что такое оракул и зачем он нужен?\n",
    "3. Что такое информационная модель?\n",
    "4. Как вычисляется новая точка?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Критерии остановки\n",
    "1. Сходимость по аргументу: \n",
    "$$\n",
    "\\| x_k - x^*  \\|_2 < \\varepsilon\n",
    "$$ \n",
    "2. Сходимость по функции: \n",
    "$$\n",
    "\\| f_k - f^* \\|_2 < \\varepsilon\n",
    "$$ \n",
    "3. Выполнение необходимого условия \n",
    "$$\n",
    "\\| f'(x_k) \\|_2 < \\varepsilon\n",
    "$$\n",
    "\n",
    "Но ведь $x^*$ неизвестна!\n",
    "\n",
    "Тогда\n",
    "\n",
    "\\begin{align*}\n",
    "& \\|x_{k+1} - x_k \\| = \\|x_{k+1} - x_k + x^* - x^* \\| \\leq \\\\\n",
    "& \\|x_{k+1} - x^* \\| + \\| x_k - x^* \\| \\leq 2\\varepsilon\n",
    "\\end{align*}\n",
    "\n",
    "Аналогично для сходимости по функции, \n",
    "\n",
    "однако иногда можно оценить $f^*$! \n",
    "\n",
    "**Замечание**: лучше использовать относительные изменения \n",
    "\n",
    "этих величин! \n",
    "\n",
    "Например $\\dfrac{\\|x_{k+1} - x_k \\|_2}{\\| x_k \\|_2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Что такое оракул?\n",
    "**Определение**: оракулом называют некоторое абстрактное \n",
    "\n",
    "устройство, которое отвечает на последовательные вопросы \n",
    "\n",
    "метода\n",
    "\n",
    "Аналогия из ООП: \n",
    "\n",
    "- оракул - это виртуальный метод базового класса\n",
    "- каждая задача - производный класс\n",
    "- оракул определяется для каждой задачи отдельно согласно общему определению в базовом классе\n",
    "\n",
    "**Концепция чёрного ящика**\n",
    "1. Единственной информацией, получаемой в ходе работы итеративного метода, являются ответы оракула\n",
    "2. Ответы оракула являются *локальными*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Информация о задаче\n",
    "1. Каждый ответ оракула даёт **локальную** информацию о поведении функции в точке\n",
    "2. Агрегируя все полученные ответы оракула, обновляем информацию о **глобальном** виде целевой функции:\n",
    "    - кривизна\n",
    "    - направление убывания\n",
    "    - etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Вычисление следующей точки\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_{k} + \\alpha_k h_k\n",
    "$$\n",
    "\n",
    "- **Линейный поиск**: фиксируется направление $h_k$ и производится поиск по этому направлению \"оптимального\" значения  $\\alpha_k$\n",
    "- **Метод доверительных областей**: фиксируется допустимый размер *области* по некоторой норме $\\| \\cdot  \\| \\leq \\alpha$ и *модель* целевой функции, которая хорошо её аппроксимирует в выбранной области. \n",
    "    \n",
    "    Далее производится поиск направления $h_k$, минимизирующего модель целевой функции и не выводящего точку $x_k + h_k$ за пределы доверительной области"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Вопросы\n",
    "1. Как выбрать $\\alpha_k$?\n",
    "2. Как выбрать $h_k$?\n",
    "3. Как выбрать модель?\n",
    "4. Как выбрать область?\n",
    "5. Как выбрать размер области? \n",
    "\n",
    "<span style=\"color:red\">\n",
    "    В курсе рассматривается только линейный поиск!</span> \n",
    "    \n",
    "Однако несколько раз копцепция метода доверительных областей \n",
    "\n",
    "будет использована."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Как сравнивать методы оптимизации?\n",
    "Для заданного класса задач сравнивают следующие величины:\n",
    "1. Сложность\n",
    "    - аналитическая: число обращений к оракулу для решения задачи с точностью $\\varepsilon$\n",
    "    - арифметическая: общее число всех вычислений, необходимых для решения задачи с точностью $\\varepsilon$\n",
    "2. Скорость сходимости\n",
    "3. Эксперименты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Скорости сходимости \n",
    "\n",
    "_1._ Сублинейная\n",
    "\n",
    "$$\n",
    "\\| x_{k+1} - x^* \\|_2 \\leq C k^{\\alpha},\n",
    "$$\n",
    "\n",
    "где $\\alpha < 0$ и $ 0 < C < \\infty$\n",
    "\n",
    "_2._ Линейная (геометрическая прогрессия)\n",
    "\n",
    "$$\n",
    "\\| x_{k+1} - x^* \\|_2 \\leq Cq^k, \n",
    "$$\n",
    "\n",
    "где $q \\in (0, 1)$ и $ 0 < C < \\infty$\n",
    "\n",
    "_3._ Сверхлинейная \n",
    "\n",
    "$$\n",
    "\\| x_{k+1} - x^* \\|_2 \\leq Cq^{k^p}, \n",
    "$$\n",
    "\n",
    "где $q \\in (0, 1)$, $ 0 < C < \\infty$ и $p > 1$\n",
    "\n",
    "_4._ Квадратичная\n",
    "\n",
    "$$\n",
    "\\| x_{k+1} - x^* \\|_2 \\leq C\\| x_k - x^* \\|^2_2, \\qquad \\text{или} \\qquad \\| x_{k+1} - x^* \\|_2 \\leq C q^{2^k}\n",
    "$$\n",
    "\n",
    "где $q \\in (0, 1)$ и $ 0 < C < \\infty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "USE_COLAB = False\n",
    "if not USE_COLAB:\n",
    "    plt.rc(\"text\", usetex=True)\n",
    "\n",
    "import numpy as np\n",
    "C = 10\n",
    "alpha = -0.5\n",
    "q = 0.9\n",
    "num_iter = 50\n",
    "sublinear = np.array([C * k**alpha for k in range(1, num_iter + 1)])\n",
    "linear = np.array([C * q**k for k in range(1, num_iter + 1)])\n",
    "superlinear = np.array([C * q**(k**2) for k in range(1, num_iter + 1)])\n",
    "quadratic = np.array([C * q**(2**k) for k in range(1, num_iter + 1)])\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.semilogy(np.arange(1, num_iter+1), sublinear, \n",
    "             label=r\"Sublinear, $\\alpha = -0.5$\", linewidth=5)\n",
    "# plt.semilogy(np.arange(1, num_iter+1), superlinear, linewidth=5,\n",
    "#              label=r\"Superlinear, $q = 0.5, p=2$\")\n",
    "plt.semilogy(np.arange(1, num_iter+1), linear, \n",
    "             label=r\"Linear, $q = 0.5$\", linewidth=5)\n",
    "# plt.semilogy(np.arange(1, num_iter+1), quadratic, \n",
    "#              label=r\"Quadratic, $q = 0.5$\", linewidth=5)\n",
    "plt.xlabel(\"Number of iterations, $k$\", fontsize=28)\n",
    "plt.ylabel(\"Error rate upper bound\", fontsize=28)\n",
    "plt.legend(loc=\"best\", fontsize=26)\n",
    "plt.xticks(fontsize = 28)\n",
    "_ = plt.yticks(fontsize = 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Значение теорем сходимости (Б.Т. Поляк Введение в оптимизацию, гл. 1, $\\S$ 6)\n",
    "1. Что дают теоремы сходимости\n",
    "     - класс задач, для которых можно рассчитывать на применимость метода (важно не завышать условия!)\n",
    "         - выпуклость\n",
    "         - гладкость\n",
    "     - качественное поведение метода\n",
    "         - существенно ли начальное приближение\n",
    "         - по какому функционалу есть сходимость\n",
    "     - оценку скорости сходимости\n",
    "         - теоретическая оценка поведения метода без проведения экспериментов\n",
    "         - определение факторов, которые влияют на сходимость (обусловленность, размерность, etc)\n",
    "         - иногда заранее можно выбрать число итераций для достижения заданной точности \n",
    "\n",
    "2. Что **НЕ** дают теоремы сходимости\n",
    "    - сходимость метода **ничего не говорит** о целесообразности его применения\n",
    "    - оценки сходимости зависят от неизвестных констант - неконструктивный характер\n",
    "    - учёт ошибок округления и точности решения вспомогательных задач\n",
    "    \n",
    "**Мораль**: нужно проявлять разумную осторожность \n",
    "\n",
    "и здравый смысл!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Классификация задач\n",
    "1. Безусловная оптимизация\n",
    "    - целевая функция липшицева\n",
    "    - градиент целевой функции липшицев\n",
    "2. Условная оптимизация\n",
    "    - многогранник\n",
    "    - множество простой структуры\n",
    "    - общего вида"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Классификация методов\n",
    "\n",
    "### Какой размер истории нужно хранить для обновления?\n",
    "\n",
    "1. Одношаговые методы \n",
    "\n",
    "$$\n",
    "x_{k+1} = \\Phi(x_k)\n",
    "$$\n",
    "\n",
    "2. Многошаговые методы\n",
    "\n",
    "$$\n",
    "x_{k+1} = \\Phi(x_k, x_{k-1}, ...)\n",
    "$$\n",
    "\n",
    "### Какой порядок поизводных нужно вычислить?\n",
    "1. Методы нулевого порядка: оракул возвращает только значение функции $f(x)$\n",
    "\n",
    "2. Методы первого порядка: оракул возвращает значение функции $f(x)$ и её градиент $f'(x)$\n",
    "\n",
    "3. Методы второго порядка: оракул возвращает значение функции $f(x)$, её градиент $f'(x)$ и гессиан $f''(x)$.\n",
    "\n",
    "**Q**: существуют ли методы более высокого порядка?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**А**: [Implementable tensor methods in unconstrained convex optimization](https://link.springer.com/content/pdf/10.1007/s10107-019-01449-1.pdf) by Y. Nesterov, 2019 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Одномерная минимизация\n",
    "**Определение**. Функция $f(x)$ называется унимодальной на $[a, b]$, если существует такая точка $x^* \\in [a, b]$, что \n",
    "- $f(x_1) > f(x_2)$ для любых $a \\leq x_1 < x_2 < x^*$, \n",
    "\n",
    "и \n",
    "- $f(x_1) < f(x_2)$ для любых $x^* < x_1 < x_2 \\leq b$.\n",
    "\n",
    "**Вопрос**: какая геометрия унимодальных функций?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Метод дихотомии\n",
    "\n",
    "Идея из информатики первого семестра: \n",
    "\n",
    "делим отрезок $[a,b]$ на две равные части \n",
    "\n",
    "пока не найдём минимум унимодальной функции.\n",
    "\n",
    "- $N$ - число вычислений функции $f$\n",
    "- $K = \\frac{N - 1}{2}$ - число итераций\n",
    "\n",
    "Тогда\n",
    "\n",
    "$$\n",
    "|x_{K+1} - x^*| \\leq \\frac{b_{K+1} - a_{K+1}}{2} = \\left( \\frac{1}{2} \\right)^{\\frac{N-1}{2}} (b - a) \\approx 0.5^{K} (b - a) \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def binary_search(f, a, b, epsilon, callback=None):\n",
    "    c = (a + b) / 2.0\n",
    "    while abs(b - a) > epsilon:\n",
    "#         Check left subsegment\n",
    "        y = (a + c) / 2.0\n",
    "        if f(y) <= f(c):\n",
    "            b = c\n",
    "            c = y\n",
    "        else:\n",
    "#         Check right subsegment\n",
    "            z = (b + c) / 2.0\n",
    "            if f(c) <= f(z):\n",
    "                a = y\n",
    "                b = z\n",
    "            else:\n",
    "                a = c\n",
    "                c = z\n",
    "        if callback is not None:\n",
    "            callback(a, b)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_callback(a, b, left_bound, right_bound, approximation):\n",
    "    left_bound.append(a)\n",
    "    right_bound.append(b)\n",
    "    approximation.append((a + b) / 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "left_boud_bs = []\n",
    "right_bound_bs = []\n",
    "approximation_bs = []\n",
    "\n",
    "callback_bs = lambda a, b: my_callback(a, b, \n",
    "            left_boud_bs, right_bound_bs, approximation_bs)\n",
    "\n",
    "# Target unimodal function on given segment\n",
    "f = lambda x: (x - 2) * x * (x + 2)**2 # np.power(x+2, 2)\n",
    "# f = lambda x: -np.sin(x)\n",
    "x_true = -2\n",
    "# x_true = np.pi / 2.0\n",
    "a = -3\n",
    "b = -1.5\n",
    "epsilon = 1e-8\n",
    "x_opt = binary_search(f, a, b, epsilon, callback_bs)\n",
    "print(np.abs(x_opt - x_true))\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(np.linspace(a,b), f(np.linspace(a,b)))\n",
    "plt.title(\"Objective function\", fontsize=28)\n",
    "plt.xticks(fontsize = 28)\n",
    "_ = plt.yticks(fontsize = 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Метод золотого сечения\n",
    "Идея: \n",
    "\n",
    "делить отрезок $[a,b]$ не на две равные насти, \n",
    "\n",
    "а в пропорции \"золотого сечения\".\n",
    "\n",
    "Оценим скорость сходимости аналогично методу дихотомии:\n",
    "\n",
    "$$\n",
    "|x_{K+1} - x^*| \\leq b_{K+1} - a_{K+1} = \\left( \\frac{1}{\\tau} \\right)^{N-1} (b - a) \\approx 0.618^K(b-a),\n",
    "$$\n",
    "где $\\tau = \\frac{\\sqrt{5} + 1}{2}$.\n",
    "\n",
    "- Константа геометрической прогрессии **больше**, чем у метода дихотомии\n",
    "- Количество вызовов функции **меньше**, чем у метода дихотомии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def golden_search(f, a, b, tol=1e-5, callback=None):\n",
    "    tau = (np.sqrt(5) + 1) / 2.0\n",
    "    y = a + (b - a) / tau**2\n",
    "    z = a + (b - a) / tau\n",
    "    while b - a > tol:\n",
    "        if f(y) <= f(z):\n",
    "            b = z\n",
    "            z = y\n",
    "            y = a + (b - a) / tau**2\n",
    "        else:\n",
    "            a = y\n",
    "            y = z\n",
    "            z = a + (b - a) / tau\n",
    "        if callback is not None:\n",
    "            callback(a, b)\n",
    "    return (a + b) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "left_boud_gs = []\n",
    "right_bound_gs = []\n",
    "approximation_gs = []\n",
    "\n",
    "cb_gs = lambda a, b: my_callback(a, b, left_boud_gs, right_bound_gs, approximation_gs)\n",
    "x_gs = golden_search(f, a, b, epsilon, cb_gs)\n",
    "\n",
    "print(f(x_opt))\n",
    "print(f(x_gs))\n",
    "print(np.abs(x_opt - x_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Сравнение методов одномерной минимизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.semilogy(np.arange(1, len(approximation_bs) + 1), np.abs(x_true - np.array(approximation_bs, dtype=np.float64)), label=\"Binary search\")\n",
    "plt.semilogy(np.arange(1, len(approximation_gs) + 1), np.abs(x_true - np.array(approximation_gs, dtype=np.float64)), label=\"Golden search\")\n",
    "plt.xlabel(r\"Number of iterations, $k$\", fontsize=26)\n",
    "plt.ylabel(\"Error rate upper bound\", fontsize=26)\n",
    "plt.legend(loc=\"best\", fontsize=26)\n",
    "plt.xticks(fontsize = 26)\n",
    "_ = plt.yticks(fontsize = 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%timeit binary_search(f, a, b, epsilon)\n",
    "%timeit golden_search(f, a, b, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Пример иного поведения методов\n",
    "\n",
    "$$\n",
    "f(x) = \\sin(\\sin(\\sin(\\sqrt{x}))), \\; x \\in [2, 60]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f = lambda x: np.sin(np.sin(np.sin(np.sqrt(x))))\n",
    "x_true = (3 * np.pi / 2)**2\n",
    "a = 2\n",
    "b = 60\n",
    "epsilon = 1e-8\n",
    "plt.plot(np.linspace(a,b), f(np.linspace(a,b)))\n",
    "plt.xticks(fontsize = 28)\n",
    "_ = plt.yticks(fontsize = 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Сравнение скорости сходимости и времени работы методов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Метод дихотомии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "left_boud_bs = []\n",
    "right_bound_bs = []\n",
    "approximation_bs = []\n",
    "\n",
    "callback_bs = lambda a, b: my_callback(a, b, \n",
    "            left_boud_bs, right_bound_bs, approximation_bs)\n",
    "\n",
    "x_opt = binary_search(f, a, b, epsilon, callback_bs)\n",
    "print(np.abs(x_opt - x_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Метод золотого сечения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "left_boud_gs = []\n",
    "right_bound_gs = []\n",
    "approximation_gs = []\n",
    "\n",
    "cb_gs = lambda a, b: my_callback(a, b, left_boud_gs, right_bound_gs, approximation_gs)\n",
    "x_gs = golden_search(f, a, b, epsilon, cb_gs)\n",
    "\n",
    "print(np.abs(x_opt - x_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Сходимость"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.semilogy(np.abs(x_true - np.array(approximation_bs, dtype=np.float64)), label=\"Binary\")\n",
    "plt.semilogy(np.abs(x_true - np.array(approximation_gs, dtype=np.float64)), label=\"Golden\")\n",
    "plt.legend(fontsize=28)\n",
    "plt.xticks(fontsize=28)\n",
    "_ = plt.yticks(fontsize=28)\n",
    "plt.xlabel(r\"Number of iterations, $k$\", fontsize=26)\n",
    "plt.ylabel(\"Error rate upper bound\", fontsize=26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Время работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%timeit binary_search(f, a, b, epsilon)\n",
    "%timeit golden_search(f, a, b, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Резюме\n",
    "1. Введение в численные методы оптимизации\n",
    "2. Общая схема работы метода\n",
    "3. Способы сравнения методов оптимизации\n",
    "4. Зоопарк задач и методов\n",
    "5. Одномерная минимизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Методы спуска. Градиентный спуск и его ускоренные модификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Что такое методы спуска?\n",
    "\n",
    "Последовательность $x_k$ генерируется по правилу\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k + \\alpha_k h_k\n",
    "$$\n",
    "\n",
    "так что\n",
    "\n",
    "$$\n",
    "f(x_{k+1}) < f(x_k)\n",
    "$$\n",
    "\n",
    "Направление $h_k$ называется *направлением убывания*.\n",
    "\n",
    "**Замечание**: существуют методы, которые не требуют монотонного убывания функции от итерации к итерации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "def DescentMethod(f, x0, epsilon, **kwargs):\n",
    "    \n",
    "    x = x0\n",
    "    \n",
    "    while StopCriterion(x, f, **kwargs) > epsilon:\n",
    "        \n",
    "        h = ComputeDescentDirection(x, f, **kwargs)\n",
    "        \n",
    "        alpha = SelectStepSize(x, h, f, **kwargs)\n",
    "        \n",
    "        x = x + alpha * h\n",
    "        \n",
    "    return x\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Способ 1: направление убывания\n",
    "Рассмотрим линейную аппроксимацию дифференцируемой функции $f$ вдоль некоторого направления убывания $h, \\|h\\|_2 = 1$:\n",
    "\n",
    "$$\n",
    "f(x + \\alpha h) = f(x) + \\alpha \\langle f'(x), h \\rangle + o(\\alpha)\n",
    "$$\n",
    "\n",
    "Из условия убывания\n",
    "\n",
    "$$\n",
    "f(x) + \\alpha \\langle f'(x), h \\rangle + o(\\alpha) < f(x)\n",
    "$$\n",
    "\n",
    "и переходя к пределу при $\\alpha \\rightarrow 0$:\n",
    "\n",
    "$$\n",
    "\\langle f'(x), h \\rangle \\leq 0\n",
    "$$\n",
    "\n",
    "Также из неравенства Коши-Буняковского-Шварца\n",
    "\n",
    "$$\n",
    "\\langle f'(x), h \\rangle \\geq -\\| f'(x) \\|_2 \\| h \\|_2 = -\\| f'(x) \\|_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Таким образом, направление антиградиента \n",
    "\n",
    "$$\n",
    "h = -\\dfrac{f'(x)}{\\|f'(x)\\|_2}\n",
    "$$\n",
    "\n",
    "даёт направление **наискорейшего локального** убывания функции$~f$.\n",
    "\n",
    "В итоге метод имеет вид\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k - \\alpha f'(x_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Способ 2: схема Эйлера решения ОДУ\n",
    "\n",
    "Рассмотрим обыкновенное диференциальное уравнение вида:\n",
    "\n",
    "$$\n",
    "\\frac{dx}{dt} = -f'(x(t))\n",
    "$$\n",
    "\n",
    "и дискретизуем его на равномерной сетке с шагом $\\alpha$:\n",
    "\n",
    "$$\n",
    "\\frac{x_{k+1} - x_k}{\\alpha} = -f'(x_k),\n",
    "$$\n",
    "\n",
    "где $x_k \\equiv x(t_k)$ и $\\alpha = t_{k+1} - t_k$ - шаг сетки.\n",
    "\n",
    "Отсюда получаем выражение для $x_{k+1}$\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k - \\alpha f'(x_k),\n",
    "$$\n",
    "\n",
    "которое в точности совпадает с выражением для градиентного спуска.\n",
    "\n",
    "Такая схема называется явной или прямой схемой Эйлера.\n",
    "\n",
    "**Q:** какая схема называется неявной или обратной?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Способ 3: минимизация квадратичной оценки сверху \n",
    "#### (А. В. Гасников \"Метод универсального градиентного спуска\" https://arxiv.org/abs/1711.00394)\n",
    "\n",
    "Глобальная оценка сверху на функцию $f$ в точке $x_k$:\n",
    "\n",
    "$$\n",
    "f(y) \\leq f(x_k) + \\langle f'(x_k), y - x_k \\rangle + \\frac{L}{2} \\|y - x_k \\|_2^2 = g(y), \n",
    "$$\n",
    "\n",
    "где $\\lambda_{\\max}(f''(x)) \\leq L$ для всех допустимых $x$.\n",
    "\n",
    "Справа &mdash; квадратичная форма, точка минимума которой имеет аналитическое выражение:\n",
    "\n",
    "\\begin{align*}\n",
    "& g'(y^*) = 0 \\\\\n",
    "& f'(x_k) + L (y^* - x_k) = 0 \\\\\n",
    "& y^* = x_k - \\frac{1}{L}f'(x_k) = x_{k+1}\n",
    "\\end{align*}\n",
    "\n",
    "Этот способ позволяет оценить значение шага как $\\frac{1}{L}$. Однако часто константа $L$ неизвестна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Итого: метод градиентного спуска &mdash; дёшево и сердито\n",
    "\n",
    "```python\n",
    "def GradientDescentMethod(f, x0, epsilon, **kwargs):\n",
    "    \n",
    "    x = x0\n",
    "    \n",
    "    while StopCriterion(x, f, **kwargs) > epsilon:\n",
    "        \n",
    "        h = ComputeGradient(x, f, **kwargs)\n",
    "        \n",
    "        alpha = SelectStepSize(x, h, f, **kwargs)\n",
    "        \n",
    "        x = x - alpha * h\n",
    "        \n",
    "    return x\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Как выбрать шаг $\\alpha_k$? (J. Nocedal, S. Wright Numerical Optimization, $\\S$ 3.1.)\n",
    "\n",
    "Список подходов:\n",
    "- Постоянный шаг \n",
    "\n",
    "$$\n",
    "\\alpha_k = \\overline{\\alpha}\n",
    "$$\n",
    "\n",
    "- Априорно заданная последовательность, например\n",
    "\n",
    "$$\n",
    "\\alpha_k = \\dfrac{\\overline{\\alpha}}{\\sqrt{k+1}}\n",
    "$$\n",
    "\n",
    "- Наискорейший спуск\n",
    "\n",
    "$$\n",
    "\\alpha_k = \\arg\\min_{\\alpha \\geq 0} f(x_k - \\alpha f'(x_k))\n",
    "$$\n",
    "\n",
    "- Требование **достаточного** убывания, требование **существенного** убывания и условие кривизны: для некоторых $\\beta_1, \\beta_2$, таких что $0 < \\beta_1 < \\beta_2 < 1$ найти $x_{k+1}$ такую что\n",
    "\n",
    "    - Достаточное убывание: $f(x_{k+1}) \\leq f(x_k) + \\beta_1 \\alpha_k \\langle f'(x_k), h_k \\rangle$ или\n",
    "    $ f(x_k) - f(x_{k+1}) \\geq \\beta_1 \\alpha_k \\langle f'(x_k), h_k \\rangle\n",
    "    $\n",
    "    - Существенное убывание: $f(x_{k+1}) \\geq f(x_k) + \\beta_2 \\alpha_k \\langle f'(x_k), h_k \\rangle$ или\n",
    "    $\n",
    "    f(x_k) - f(x_{k+1}) \\leq \\beta_2 \\alpha_k \\langle f'(x_k), h_k \\rangle\n",
    "    $\n",
    "    - Условие кривизны: $\\langle f'(x_{k+1}), h_k \\rangle \\geq \\beta_2 \\langle f'(x_k), h_k \\rangle$\n",
    "\n",
    "Обычно коэффициенты выбирают так: $\\beta_1 \\in (0, 0.3)$, а $\\beta_2 \\in (0.9, 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Анализ и мотивация подходов к выбору шага $\\alpha_k$\n",
    "- Постоянный шаг: самое простое и неэффективное решение\n",
    "- Априорно заданная последовательность: немногим лучше постоянного шага\n",
    "- Наискорейший спуск: самое лучшее решение, но применимо только если вспомогательная задача решается аналитически или ооооооочень быстро. <br></br>\n",
    "То есть почти всегда неприменимо :)\n",
    "- Требование достаточного убывания, требование существенного убывания и условие кривизны:\n",
    "    - требование достаточного убывания гарантирует, что функция в точке $x_{k+1}$ не превосходит линейной аппроксимации с коэффициентом наклона $\\beta_1$\n",
    "    - требование существенного убывания гарантирует, что функция в точке $x_{k+1}$ убывает не меньше, чем линейная аппроксимация c коэффициентом наклона $\\beta_2$\n",
    "    - условие кривизны гарантирует, что угол наклона касательной в точке $x_{k+1}$ не меньше, чем угол наклона касательной в точке $x_k$, <br></br>\n",
    "умноженный на $\\beta_2$ \n",
    "\n",
    "Требование существенного убывания и условие кривизны обеспечивают убывание функции по выбранному направлению $h_k$. Обычно выбирают одно из них."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Альтернативные названия\n",
    "- Требование достаточного убывания $\\equiv$ правило Армихо\n",
    "- Требование достаточного убывания + условие кривизны $\\equiv$ правило Вольфа\n",
    "- Требование достаточного убывания + требование существенного убывания $\\equiv$ правило Гольдштейна"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Зачем нужно условие существенного убывания?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc(\"text\", usetex=True)\n",
    "import ipywidgets as ipywidg\n",
    "import numpy as np\n",
    "import liboptpy.unconstr_solvers as methods\n",
    "import liboptpy.step_size as ss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "f = lambda x: np.power(x, 2)\n",
    "gradf = lambda x: 2 * x\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "\n",
    "\n",
    "def update(x0, step):\n",
    "    gd = methods.fo.GradientDescent(f, gradf, ss.ConstantStepSize(step))\n",
    "    _ = gd.solve(np.array([x0]), max_iter=10)\n",
    "    x_hist = gd.get_convergence()\n",
    "    x = np.linspace(-5, 5)\n",
    "    ax.clear()\n",
    "    ax.plot(x, f(x), color=\"r\", label=\"$f(x) = x^2$\")\n",
    "    y_hist = np.array([f(x) for x in x_hist])\n",
    "    x_hist = np.array(x_hist)\n",
    "    plt.quiver(x_hist[:-1], y_hist[:-1], x_hist[1:]-x_hist[:-1], y_hist[1:]-y_hist[:-1], \n",
    "               scale_units='xy', angles='xy', scale=1, width=0.005, color=\"green\", label=\"Descent path\")\n",
    "    ax.legend()\n",
    "    fig.canvas.draw()\n",
    "\n",
    "step_slider = ipywidg.FloatSlider(value=0.8, min=0, max=1.2, step=0.1, description=\"Step\")\n",
    "x0_slider = ipywidg.FloatSlider(value=1.5, min=-4, max=4, step=0.1, description=\"Initial point\")\n",
    "_ = ipywidg.interact(update, x0=x0_slider, step=step_slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def plot_alpha(f, grad, x, h, alphas, beta1, beta2):\n",
    "    df = np.zeros_like(alphas)\n",
    "    for i, alpha in enumerate(alphas):\n",
    "        df[i] = f(x + alpha * h)\n",
    "    upper_bound = f(x) + beta1 * alphas * grad(x) * h\n",
    "    lower_bound = f(x) + beta2 * alphas * grad(x) * h\n",
    "    plt.plot(alphas, df, label=r\"$f(x + \\alpha h)$\")\n",
    "    plt.plot(alphas, upper_bound, label=\"Upper bound\")\n",
    "    plt.plot(alphas, lower_bound, label=\"Lower bound\")\n",
    "    plt.xlabel(r\"$\\alpha$\", fontsize=18)\n",
    "    plt.legend(loc=\"best\", fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "f = lambda x: x**2\n",
    "grad = lambda x: 2 * x\n",
    "beta1 = 0.1\n",
    "beta2 = 0.9\n",
    "x0 = 0.5\n",
    "plot_alpha(f, grad, x0, -grad(x0), np.linspace(1e-3, 1.01, 10), beta1, beta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $f(x) = x\\log x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x_range = np.linspace(1e-10, 4)\n",
    "plt.plot(x_range, x_range * np.log(x_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x0 = 1\n",
    "f = lambda x: x * np.log(x)\n",
    "grad = lambda x: np.log(x) + 1\n",
    "beta1 = 0.3\n",
    "beta2 = 0.7\n",
    "plot_alpha(f, grad, x0, -grad(x0), np.linspace(1e-3, 0.9, 10), beta1, beta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backtracking \n",
    "\n",
    "```python\n",
    "def SelectStepSize(x, f, h, rho, alpha0, beta1, beta2):\n",
    "    \n",
    "    # 0 < rho < 1\n",
    "    \n",
    "    # alpha0 - initial guess of step size\n",
    "    \n",
    "    # beta1 and beta2 - constants from conditions\n",
    "    \n",
    "    alpha = alpha0\n",
    "    \n",
    "    # Check violating sufficient decrease and curvature conditions\n",
    "    \n",
    "    while (f(x - alpha * h) >= f(x) + beta1 * alpha grad_f(x_k).dot(h)) and \n",
    "    \n",
    "          (grad_f(x - alpha * h).dot(h) <= beta2 * grad_f(x_k).dot(h)):\n",
    "            \n",
    "        alpha *= rho\n",
    "        \n",
    "    return alpha\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Теоремы сходимости (Б.Т. Поляк Введение в оптимизацию, гл. 1, $\\S$ 4;  гл. 3, $\\S$ 1; Ю.Е. Нестеров Введение в выпуклую оптимизацию, $\\S$ 2.2)\n",
    "От общего к частному:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Теорема 1.** \n",
    "Пусть \n",
    "\n",
    "- $f(x)$ дифференцируема на $\\mathbb{R}^n$, \n",
    "- градиент $f(x)$ удовлетворяет условию Липшица с константой $L$\n",
    "- $f(x)$ ограничена снизу\n",
    "- $\\alpha = const$ и $0 < \\alpha < \\frac{2}{L}$\n",
    "\n",
    "Тогда для градиентного метода выполнено:\n",
    "\n",
    "$$\n",
    "\\lim\\limits_{k \\to \\infty} f'(x_k) = 0,\n",
    "$$\n",
    "\n",
    "а функция монотонно убывает $f(x_{k+1}) < f(x_k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Теорема 2.** Пусть\n",
    "- $f(x)$ дифференцируема на $\\mathbb{R}^n$\n",
    "- $f(x)$ выпукла \n",
    "- $f'(x)$ удовлетворяет условию Липшица с константой $L$\n",
    "- $\\alpha = \\dfrac{1}{L}$\n",
    "\n",
    "Тогда \n",
    "\n",
    "$$\n",
    "f(x_k) - f^* \\leq \\dfrac{2L \\| x_0 - x^*\\|^2_2}{k+4}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Теорема 3.**\n",
    "Пусть \n",
    "- $f(x)$ дважды дифференцируема и $\\mu\\mathbf{I} \\preceq f''(x) \\preceq L\\mathbf{I}$ для всех $x$\n",
    "- $\\alpha = const$ и $0 < \\alpha < \\frac{2}{L}$\n",
    "\n",
    "Тогда \n",
    "\n",
    "$$\n",
    "\\| x_k - x^*\\|_2 \\leq \\|x_0 - x^*\\|_2 q^k, \\qquad q = \\max(|1 - \\alpha l|, |1 - \\alpha L|) < 1\n",
    "$$\n",
    "\n",
    "и минимальное $q^* = \\dfrac{L - \\mu}{L + \\mu}$ при $\\alpha^* = \\dfrac{2}{L + \\mu}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### От чего зависит $q^*$ и как это использовать?\n",
    "Из Теоремы 3 имеем \n",
    "\n",
    "$$\n",
    "q^* = \\dfrac{L - \\mu}{L + \\mu} = \\dfrac{L/\\mu - 1}{L/\\mu + 1} = \\dfrac{M - 1}{M + 1},\n",
    "$$\n",
    "\n",
    "где $M$ - оценка числа обусловленности $f''(x)$.\n",
    "\n",
    "**Вопрос**: что такое число обусловленности матрицы?\n",
    "\n",
    "- При $M \\gg 1$, $q^* \\to 1 \\Rightarrow$ оооочень **медленная** сходимости градиентного метода. Например при $M = 100$: $q^* \\approx 0.98 $\n",
    "- При $M \\simeq 1$, $q^* \\to 0 \\Rightarrow$ **ускорение** сходимости градиентного метода. Например при $M = 4$: $q^* = 0.6 $\n",
    "\n",
    "**Вопрос**: какая геометрия у этого требования?\n",
    "\n",
    "**Мораль**: необходимо сделать оценку $M$ как можно ближе к 1!\n",
    "\n",
    "О том, как это сделать, Вам будет предложено подумать в домашнем задании :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Вычислительный аспект и эксперименты\n",
    "1. Для каждого шага метода нужно хранить только текущую точку и вектор градиента: $O(n)$ памяти\n",
    "2. Поиск $\\alpha_k$:\n",
    "\n",
    "    - дан априори\n",
    "    - ищется из аналитического решения задачи наискорейшего спуска\n",
    "    - заканчивается за конечное число шагов\n",
    "3. Для каждого шага метода нужно вычислять линейную комбинацию векторов: $O(n)$ вычислений + высокопроизводительные реализации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pеализация градиентного спуска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def GradientDescent(f, gradf, x0, epsilon, num_iter, line_search, \n",
    "                    disp=False, callback=None, **kwargs):\n",
    "    x = x0.copy()\n",
    "    iteration = 0\n",
    "    opt_arg = {\"f\": f, \"grad_f\": gradf}\n",
    "    for key in kwargs:\n",
    "        opt_arg[key] = kwargs[key]\n",
    "    while True:\n",
    "        gradient = gradf(x)\n",
    "        alpha = line_search(x, -gradient, **opt_arg)\n",
    "        x = x - alpha * gradient\n",
    "        if callback is not None:\n",
    "            callback(x)\n",
    "        iteration += 1\n",
    "        if disp:\n",
    "            print(\"Current function val =\", f(x))\n",
    "            print(\"Current gradient norm = \", np.linalg.norm(gradf(x)))\n",
    "        if np.linalg.norm(gradf(x)) < epsilon:\n",
    "            break\n",
    "        if iteration >= num_iter:\n",
    "            break\n",
    "    res = {\"x\": x, \"num_iter\": iteration, \"tol\": np.linalg.norm(gradf(x))}\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Выбор шага\n",
    "\n",
    "Реализации различных способов выбора шага приведены [тут](https://github.com/amkatrutsa/liboptpy/blob/master/step_size.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Зависимость от обусловленности матрицы $f''(x)$\n",
    "Рассмотрим задачу \n",
    "$$\n",
    "\\min f(x),\n",
    "$$ \n",
    "где\n",
    "$$ f(x) = x^{\\top}Ax, \\; A = \\begin{bmatrix} 1 & 0\\\\ 0 & \\gamma \\end{bmatrix} $$\n",
    "\n",
    "$$\n",
    "f'(x) = 2Ax\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def my_f(x, A):\n",
    "    return 0.5 * x.dot(A.dot(x))\n",
    "\n",
    "def my_gradf(x, A):\n",
    "    return A.dot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.rc(\"text\", usetex=True)\n",
    "\n",
    "gammas = [0.1, 0.5, 1, 2, 3, 4, 5, 10, 20, 50, 100, 1000, 5000, 10000]\n",
    "# gammas = [1]\n",
    "num_iter_converg = []\n",
    "for g in gammas:\n",
    "    A = np.array([[1, 0], \n",
    "                  [0, g]], dtype=np.float64)\n",
    "    f = lambda x: my_f(x, A)\n",
    "    gradf = lambda x: my_gradf(x, A)\n",
    "#     x0 = np.random.rand(A.shape[0])\n",
    "#     x0 = np.sort(x0)\n",
    "#     x0 = x0[::-1]\n",
    "    x0 = np.array([g, 1], dtype=np.float64)\n",
    "#     print x0[1] / x0[0]\n",
    "    gd = methods.fo.GradientDescent(f, gradf, ss.ExactLineSearch4Quad(A))\n",
    "    x = gd.solve(x0, tol=1e-7, max_iter=100)\n",
    "    num_iter_converg.append(len(gd.get_convergence()))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.loglog(gammas, num_iter_converg)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.yticks(fontsize = 20)\n",
    "plt.xlabel(r\"$\\gamma$\", fontsize=20)\n",
    "plt.ylabel(r\"Number of iterations with $\\varepsilon = 10^{-7}$\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- При неудачном начальном приближении сходимость для плохо обусловенной задачи очень медленная\n",
    "- При случайном начальном приближении сходимость может быть гораздо быстрее теоретических оценок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Эксперимент на многомерной задаче\n",
    "Пусть $A \\in \\mathbb{R}^{m \\times n}$. Рассмотрим систему линейных неравенств: $Ax \\leq 1$ при условии $|x_i| \\leq 1$ для всех $i$.\n",
    "\n",
    "**Определение.** Аналитическим центром системы неравенств $Ax \\leq 1$ при условии $|x_i| \\leq 1$ является решение задачи\n",
    "$$\n",
    "f(x) = - \\sum_{i=1}^m \\log(1 - a_i^{\\top}x) - \\sum_{i = 1}^n \\log (1 - x^2_i) \\to \\min_x\n",
    "$$\n",
    "$$\n",
    "f'(x) - ?\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Точное решение с помощью CVXPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 1000\n",
    "m = 2000\n",
    "A = np.random.rand(n, m)\n",
    "\n",
    "x = cvx.Variable(n)\n",
    "\n",
    "obj = cvx.Minimize(cvx.sum(-cvx.log(1 - A.T * x)) - \n",
    "                   cvx.sum(cvx.log(1 - cvx.square(x))))\n",
    "prob = cvx.Problem(obj)\n",
    "prob.solve(solver=\"SCS\", verbose=True)\n",
    "x = x.value\n",
    "print(\"Optimal value =\", prob.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Решение с помощью градиентного спуска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import cvxpy as cvx\n",
    "print(cvx.installed_solvers())\n",
    "\n",
    "# !pip install jax \n",
    "# !pip install jaxlib\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "# from jax.config import config\n",
    "# config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "A = jnp.array(A)\n",
    "print(A.dtype)\n",
    "x0 = jnp.zeros(n)\n",
    "f = lambda x: -jnp.sum(jnp.log(1 - A.T@x)) - jnp.sum(jnp.log(1 - x*x))\n",
    "grad_f = lambda x: jnp.sum(A @ (jnp.diagflat(1 / (1 - A.T @ x))), \\\n",
    "                          axis=1) + 2 * x / (1 - jnp.power(x, 2))\n",
    "grad_f_jax = jax.grad(f)\n",
    "print(jnp.linalg.norm(grad_f(x0) - grad_f_jax(x0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Подробнее про jax, его возможности и особенности можно посмотреть например [тут](https://github.com/amkatrutsa/MIPT-Opt/blob/master/Fall2020/03-MatrixCalculus/jax_autodiff_tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "gd = methods.fo.GradientDescent(f, grad_f_jax, ss.Backtracking(\"Armijo\", rho=0.5, beta=0.1, init_alpha=1.))\n",
    "x = gd.solve(x0, tol=1e-5, max_iter=100, disp=True)\n",
    "\n",
    "x_conv = gd.get_convergence()\n",
    "grad_conv = [jnp.linalg.norm(grad_f_jax(x)) for x in x_conv]\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.semilogy(grad_conv, label=r\"$\\| f'(x_k) \\|_2$\")\n",
    "plt.semilogy([np.linalg.norm(x - np.array(x_k)) for x_k in x_conv], label=r\"$\\|x_k - x^*\\|_2$\")\n",
    "plt.semilogy([np.linalg.norm(prob.value - f(np.array(x_k))) for x_k in x_conv], label=r\"$\\|f(x_k) - f^*\\|_2$\")\n",
    "plt.semilogy([np.linalg.norm(np.array(x_conv[i]) - np.array(x_conv[i+1])) for i in range(len(x_conv) - 1)], label=r\"$\\|x_k - x_{k+1}\\|_2$\")\n",
    "plt.semilogy([np.linalg.norm(f(np.array(x_conv[i])) - f(np.array(x_conv[i+1]))) for i in range(len(x_conv) - 1)], label=r\"$\\|f(x_k) - f(x_{k+1})\\|_2$\")\n",
    "plt.xlabel(r\"Number of iteration, $k$\", fontsize=20)\n",
    "plt.ylabel(r\"Convergence rate\", fontsize=20)\n",
    "plt.xticks(fontsize = 20)\n",
    "plt.yticks(fontsize = 20)\n",
    "plt.legend(loc=\"best\", fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pro & Contra\n",
    "\n",
    "Pro\n",
    "- легко реализовать\n",
    "- сходимость как минимум к стационарной точке\n",
    "- параметры при выборе шага влияют на сходимость не столь сильно\n",
    "- имеет многочисленные вариации\n",
    "\n",
    "Contra\n",
    "- линейная сходимость для сильно выпуклых функций\n",
    "- очень сильно зависит от числа обусловленности $f''(x)$, выбор начального приближения может помочь\n",
    "- не является оптимальным для выпуклых функций с липшицевым градиентом и сильновыпуклых функций (см. [ускорение Нестерова](https://blogs.princeton.edu/imabandit/2013/04/01/acceleratedgradientdescent/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Резюме\n",
    "1. Методы спуска\n",
    "2. Направление убывания\n",
    "3. Метод градиентного спуска\n",
    "4. Правила выбора шага\n",
    "5. Теоремы сходимости\n",
    "6. Эксперименты"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (cvxpy)",
   "language": "python",
   "name": "cvxpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nbpresent": {
   "slides": {
    "b738dfa0-30f4-4350-8338-c31d236608ec": {
     "id": "b738dfa0-30f4-4350-8338-c31d236608ec",
     "prev": null,
     "regions": {
      "96278b08-d857-478b-803c-5921bd08bbcd": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "196b8a50-3d29-45c3-82b9-f4a09b49491d",
        "part": "whole"
       },
       "id": "96278b08-d857-478b-803c-5921bd08bbcd"
      },
      "9b98a7cc-d9d4-4a02-ac7e-30168dcde7af": {
       "attrs": {
        "height": 0.4,
        "width": 0.8,
        "x": 0.1,
        "y": 0.5
       },
       "content": {
        "cell": "2a573842-172b-4931-b0dd-9c9d3c47a450",
        "part": "whole"
       },
       "id": "9b98a7cc-d9d4-4a02-ac7e-30168dcde7af"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
