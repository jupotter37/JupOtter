{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Ordinary Least Squares -- Part II\n",
    "## Course recap\n",
    "This lab consists in implementing the **Ordinary Least Squares** (OLS) algorithm, which is **a linear regression with a least-squares penalty**. Given a training set $ D = \\left\\{ \\left(x^{(i)}, y^{(i)}\\right), x^{(i)} \\in \\mathcal{X}, y^{(i)} \\in \\mathcal{Y}, i \\in \\{1, \\dots, n \\}  \\right\\}$, recall (from lectures 1 and 2) OLS aims at minimizing the following cost function $J$:\n",
    "$$J(\\theta) = \\dfrac{1}{2} \\sum_{i = 1}^{n} \\left( h\\left(x^{(i)}\\right) - y^{(i)} \\right)^2$$\n",
    "where \n",
    "$$h(x) = \\sum_{j = 0}^{d} \\theta_j x_j = \\theta^T x.$$\n",
    "\n",
    "For the sake of simplicity, we will be working on a small training set (the one we used in lectures 1 and 2):\n",
    "\n",
    "| living area (m$^2$) | price (1000's BGN)|\n",
    "|--------------------:|------------------:|\n",
    "|                 50  |         30        |\n",
    "|                 76  |         48        |\n",
    "|                 26  |         12        |\n",
    "|                102  |         90        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Defining the training set\n",
    "**Exercise 1**: Define variables `X` and `Y` that will contain the features $\\mathcal{X}$ and labels $\\mathcal{Y}$ of the training set.\n",
    "\n",
    "**Hint**: Do not forget the intercept!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this simple example, the dimensionality is $d = 1$ (which means 2 features: don't forget the intercept!) and the number of samples is $n = 4$.\n",
    "\n",
    "**Remark**: `1.` is written instead of `1` in order to avoid *integers operations*. For example, in some languages (including Python 2), the result of `1/2` is `0` and not `0.5`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Instead, writing `1./2` forces a *float operation* and gives `0.5` as a result, which is what we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prediction function\n",
    "**Exercise**: Define a function `predict` that takes as parameter *the feature vector* $x$ and *the model* $\\theta$ and returns the predicted label:\n",
    "$$ \\hat{y} = h(x) = \\theta^T x = \\sum_{j = 0}^d \\theta_j x_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Defining the cost function\n",
    "### Cost function on a single sample\n",
    "**Exercise**: Define a function `cost_function` that takes as parameter *the predicted label* $y$ and *the actual label* $\\hat{y}$ of a single sample and returns the value of the cost function for this pair. Recall from lectures 1 and 2 that it is given by:\n",
    "$$ \\ell \\left( y, \\hat{y} \\right) = \\dfrac{1}{2}\\left( y - \\hat{y} \\right)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Cost function on the whole training set\n",
    "We are now able to compute the cost function for a single sample. We can easily compute the cost function for the whole training set by summing the cost function values for all the samples in the training set. Recall that the total cost function is given by:\n",
    "$$J(\\theta) = \\dfrac{1}{2} \\sum_{i = 1}^{n} \\left( h\\left(x^{(i)}\\right) - y^{(i)} \\right)^2$$\n",
    "where, for all $i \\in \\{ 1, \\dots, n \\}$\n",
    "$$h\\left(x^{(i)}\\right) = \\sum_{j = 0}^{d} \\theta_j x^{(i)}_j = \\theta^T x^{(i)}$$\n",
    "is the prediction of $x$ given the model $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's now test the code written above and check the total cost function we would have when $\\theta = [0, 0]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note that this error is big, which is expectable because having $\\theta = [0, 0]$ means always predicting $\\hat{y} = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Defining the gradient of the cost function\n",
    "### Gradient on a single sample\n",
    "**Exercise**: Define a function `gradient` that implements the gradient of the cost function for a given sample $(x, y)$. Recall from the lectures 1 and 2 that the gradient is given by:\n",
    "$$\\nabla J(\\theta) = \\left[ \\dfrac{\\partial}{\\partial \\theta_1} J(\\theta), \\dots, \\dfrac{\\partial}{\\partial \\theta_d} J(\\theta) \\right]^T$$\n",
    "where, for all $j \\in \\{0, \\dots, d \\}$:\n",
    "$$ \\dfrac{\\partial}{\\partial \\theta_j} J(\\theta) = \\left( h\\left(x\\right) - y \\right) x_j. $$\n",
    "**Hint**: Recall that $d = 1$, hence the gradient is of size $2$ (one value for $j = 0$, and another one for $j = 1$). Its two values are given by:\n",
    "$$ \\dfrac{\\partial}{\\partial \\theta_0} J(\\theta) = \\left( h\\left(x\\right) - y \\right) x_0 \\quad \\text{and} \\quad \n",
    "\\dfrac{\\partial}{\\partial \\theta_1} J(\\theta) = \\left( h\\left(x\\right) - y \\right) x_1. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's try the `gradient` function on a simple example ($\\theta = [0, 0]$ on the first sample of the training set, *i.e.* $\\left(x^{(0)}, y^{(0)}\\right)$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Gradient on the whole training set\n",
    "Now we are able to compute the gradient of the cost function on a single sample, we can easily compute `gradient_total`, the gradient of the cost function on the whole training set by summing the gradients for all the samples in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's now test the code written above and check the total gradient we would have when $\\theta = [0, 0]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question**: What is the sign of the gradient values? What would it mean if we had such a gradient when applying a gradient descent?\n",
    "\n",
    "**Hint**: Recall the gradient descent update:\n",
    "$$\\theta_j := \\theta_j - \\alpha \\dfrac{\\partial}{\\partial \\theta_j} J(\\theta) \\quad \\text{for all } j \\in \\{0, \\dots, d \\}$$\n",
    "\n",
    "**Answer**: Both values are negative, which means this gradient step would increase the value of $\\theta$ due to fact we **substract** the gradient. This makes sense, because:\n",
    "- we start with $\\theta = [0, 0]$,\n",
    "- we expect $\\theta_0 > 0$ and $\\theta_1 > 0$ because otherwise we could predict a negative price.\n",
    "\n",
    "## Applying a gradient descent\n",
    "### Gradient descent step implementation\n",
    "We now have all the building blocs needed for the gradient descent algorithm, that is:\n",
    "- The loss function\n",
    "- The gradient\n",
    "Indeed, the iterative update scheme of this algorithm is given by the following formula:\n",
    "$$\\theta_j := \\theta_j - \\alpha \\dfrac{\\partial}{\\partial \\theta_j} J(\\theta)$$\n",
    "for all $j \\in \\{0, \\dots, d \\}$. Recall that $\\alpha$ is a parameter called the *learning rate* (or *step size*).\n",
    "\n",
    "**Exercise**: Define a function called `gradient_descent_step` that performs an update on theta by applying the formula above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise**: Run a few iterations manually. Play with the value of $\\alpha$ to see how it impacts the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Iterating gradient descent steps\n",
    "The `gradient_descent_step` implements a single gradient step of the gradient descent algorithm. Implement a function called `gradient_descent` that starts from a given $\\theta$ (exaple $\\theta = [0, 0]$) and applies 100 gradient descent iterations. Display the total cost function $J(\\theta)$ at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise**: Play with the code you've just run. Try different values of $\\alpha$ and see the impact it has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise**: Plot the loss over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let us see what the trained regression looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question**: Looks at the evolution of the cost function over time. What comment can you make?\n",
    "\n",
    "**Answer**: The loss function constantly drops over time with this initial value of $\\theta$ and this $\\alpha$. It ends up reaching a plateau at around 186. It seems like the algorithm has converged to the optimal value of the cost function.\n",
    "\n",
    "**Question**: What does the value `theta_trained` represent?\n",
    "\n",
    "**Answer**: Recall that the model $\\theta$ has to values, $\\theta_0$ and $\\theta_1$. Hence, with the model `theta_trained` we've learnt, price prediction would be:\n",
    "$$ \\text{price} = \\theta_0 + \\theta_1 \\times \\text{area}.$$\n",
    "\n",
    "## Batch gradient descent vs. stochastic gradient descent\n",
    "As we have seen during the lecture 1, the gradient descent methods are often split into 2 different subfamilies:\n",
    "- **Batch methods** update $\\theta$ after having computed the gradient on the whole training set\n",
    "- **Stochastic methods** update $\\theta$ after having computed the gradient on a single sample\n",
    "The gradient descent we have implemented above (`gradient_descent_step` and `gradient_descent`) corresponds to the batch version because it sums the gradient of all the samples in the training set. \n",
    "\n",
    "**Exercise**: Try to implement the stochastic version of the gradient descent algorithm. You will need to define a function `stochastic_gradient_descent_step` that compute a stochastic gradient step (on a single $(x, y)$ sample) and a function `stochastic_gradient_descent` iterates 100 stochastic gradient descent steps and returns the trained model $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise**: Apply the algorithm with the same parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise**: Plot the loss history over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise**: Plot the obtained regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question**: Compare the results obtained when solving the OLS problem with stochastic gradient descent and batch gradient descent. Are the results the same? Why?\n",
    "\n",
    "**Hint**: Plot the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
