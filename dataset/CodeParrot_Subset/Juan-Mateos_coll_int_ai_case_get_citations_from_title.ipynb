{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from alphabet_detector import AlphabetDetector\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import process as fuzzy_proc\n",
    "from fuzzywuzzy import fuzz\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ad = AlphabetDetector()\n",
    "def title_processor(title):\n",
    "    result = \"\".join([x if len(ad.detect_alphabet(x)) > 0 or x.isnumeric()\n",
    "                      else \" \" for x in title.lower()])\n",
    "    while \"  \" in result:\n",
    "        result = result.replace(\"  \",\" \")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Ocp-Apim-Subscription-Key': 'a9a9efa851b44d5bbd6c841215a99e00',\n",
    "    'Content-Type': 'application/x-www-form-urlencoded'\n",
    "}\n",
    "\n",
    "def process_titles(raw_titles):\n",
    "\n",
    "    titles = [(pid,title_processor(t)) for pid,t in raw_titles]\n",
    "\n",
    "    title_count = 800\n",
    "    title_offset = 0\n",
    "    query_count = 1000\n",
    "\n",
    "    calls = 0\n",
    "    \n",
    "    data = []\n",
    "    while title_offset < len(titles):\n",
    "\n",
    "        calls += 1\n",
    "        if calls > 10:\n",
    "            break\n",
    "        \n",
    "        last_title = title_offset+title_count\n",
    "        if last_title > len(titles):\n",
    "            last_title = None\n",
    "\n",
    "        titles_subset = titles[title_offset:last_title]\n",
    "        expr = [\"Ti='\"+t+\"'\" for _,t in titles_subset]\n",
    "        expr = ','.join(expr)\n",
    "        expr = \"expr=OR(\"+expr+\")\"\n",
    "        title_offset += title_count\n",
    "\n",
    "        query = expr+\"&count=\"+str(query_count)+\"&attributes=Id,Ti,D,AA.AuN,AA.AuId,F.FId,J.JId,AA.AfId,CC,ECC,AA.AfN,J.JN\"    \n",
    "        #print(query)\n",
    "        \n",
    "        r = requests.post('https://westus.api.cognitive.microsoft.com/academic/v1.0/evaluate', \n",
    "                          data=query.encode(\"utf-8\"), headers=headers)\n",
    "        js = r.json()\n",
    "\n",
    "        print(len(js[\"entities\"]),len(titles))\n",
    "        \n",
    "        for pid,t in titles_subset:\n",
    "            matched = False\n",
    "            for row in js[\"entities\"]:\n",
    "                if t != row[\"Ti\"]:\n",
    "                    continue\n",
    "                insts = list(set(author[\"AfN\"] for author in row[\"AA\"] if \"AfN\" in author))\n",
    "                data.append(dict(pid=pid,title=t,institutes=insts,citations=row[\"CC\"],date=row[\"D\"],matched=True))\n",
    "                matched = True\n",
    "                break\n",
    "            if not matched:\n",
    "                data.append(dict(pid=pid,title=t,matched=False))\n",
    "\n",
    "    print(\"Made\",calls,\"calls\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629 7017\n",
      "758 7017\n",
      "776 7017\n",
      "786 7017\n",
      "796 7017\n",
      "797 7017\n",
      "809 7017\n",
      "790 7017\n",
      "660 7017\n",
      "Made 9 calls\n"
     ]
    }
   ],
   "source": [
    "#raw_titles = [(1,\"Search for invisible decays of a Higgs boson using vector-boson fusion in pp collisions at s√=8 TeV with the ATLAS detector\"),\n",
    "#              (2,\"Muon-induced background to proton decay in the p→K+ν decay channel with large underground liquid argon TPC detectors\"),\n",
    "#              (3,\"personalizing search via automated analysis of interests and activities\")]\n",
    "\n",
    "df = pd.read_csv(\"/Users/hep/Downloads/ai_id_title.csv\")\n",
    "raw_titles = df[[\"id\",\"title\"]].values\n",
    "data = process_titles(raw_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7017 6408 4459 5544 4117\n"
     ]
    }
   ],
   "source": [
    "ncite = 0\n",
    "ninst = 0\n",
    "nmatch = 0 \n",
    "nboth = 0\n",
    "for row in data:\n",
    "    if not row[\"matched\"]:\n",
    "        continue\n",
    "    nmatch += 1\n",
    "    if row[\"citations\"] > 0:\n",
    "        ncite += 1\n",
    "    if len(row[\"institutes\"]) > 0:\n",
    "        ninst += 1\n",
    "    if row[\"citations\"] > 0 and len(row[\"institutes\"]) > 0:\n",
    "        nboth += 1\n",
    "print(len(data),nmatch,ncite,ninst,nboth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/Users/hep/Downloads/ai_id_title_MAK-matched.json', 'w') as fp:\n",
    "    json.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mak_df = pd.read_json('/Users/hep/Nesta/coll_int_ai_case/notebooks/MAK_disambiguate/modules/CS_STATS_id_title_tag_MAK-matched.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citations</th>\n",
       "      <th>date</th>\n",
       "      <th>institutes</th>\n",
       "      <th>matched</th>\n",
       "      <th>pid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.0</td>\n",
       "      <td>2008-11-13</td>\n",
       "      <td>[max planck society, heidelberg institute for ...</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/0811.2055v2</td>\n",
       "      <td>gpu based interactive visualization of billion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2007-01-10</td>\n",
       "      <td>[bielefeld university, washington university i...</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/0707.0808v1</td>\n",
       "      <td>the cyborg astrobiologist porting from a weara...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0</td>\n",
       "      <td>2008-09-20</td>\n",
       "      <td>[university of california berkeley, university...</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/0706.4108v1</td>\n",
       "      <td>event weighted tests for detecting periodicity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2008-11-01</td>\n",
       "      <td>[massachusetts institute of technology]</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/0706.4048v1</td>\n",
       "      <td>getting more from your multicore exploiting op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2007-01-06</td>\n",
       "      <td>[harvard university]</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/cs/0701035v1</td>\n",
       "      <td>finding astronomical communities through co re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   citations       date                                         institutes  \\\n",
       "0       12.0 2008-11-13  [max planck society, heidelberg institute for ...   \n",
       "1        3.0 2007-01-10  [bielefeld university, washington university i...   \n",
       "2       12.0 2008-09-20  [university of california berkeley, university...   \n",
       "3        2.0 2008-11-01            [massachusetts institute of technology]   \n",
       "4        0.0 2007-01-06                               [harvard university]   \n",
       "\n",
       "  matched                                pid  \\\n",
       "0    True   http://arxiv.org/abs/0811.2055v2   \n",
       "1    True   http://arxiv.org/abs/0707.0808v1   \n",
       "2    True   http://arxiv.org/abs/0706.4108v1   \n",
       "3    True   http://arxiv.org/abs/0706.4048v1   \n",
       "4    True  http://arxiv.org/abs/cs/0701035v1   \n",
       "\n",
       "                                               title  \n",
       "0  gpu based interactive visualization of billion...  \n",
       "1  the cyborg astrobiologist porting from a weara...  \n",
       "2  event weighted tests for detecting periodicity...  \n",
       "3  getting more from your multicore exploiting op...  \n",
       "4  finding astronomical communities through co re...  "
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mak_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    122198.000000\n",
       "mean         10.942757\n",
       "std          82.726236\n",
       "min           0.000000\n",
       "25%           0.000000\n",
       "50%           1.000000\n",
       "75%           7.000000\n",
       "max       15810.000000\n",
       "Name: citations, dtype: float64"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mak_df.loc[~pd.isnull(mak_df[\"citations\"]),\"citations\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>ID</th>\n",
       "      <th>alias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Australian National University</td>\n",
       "      <td>-35.277800</td>\n",
       "      <td>149.120500</td>\n",
       "      <td>grid.1001.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Monash University</td>\n",
       "      <td>-37.908300</td>\n",
       "      <td>145.138000</td>\n",
       "      <td>grid.1002.3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>University of Queensland</td>\n",
       "      <td>-27.495964</td>\n",
       "      <td>153.009627</td>\n",
       "      <td>grid.1003.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Macquarie University</td>\n",
       "      <td>-33.775259</td>\n",
       "      <td>151.112915</td>\n",
       "      <td>grid.1004.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UNSW Australia</td>\n",
       "      <td>-33.917731</td>\n",
       "      <td>151.230964</td>\n",
       "      <td>grid.1005.4</td>\n",
       "      <td>University of New South Wales</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Name        lat         lng           ID  \\\n",
       "0  Australian National University -35.277800  149.120500  grid.1001.0   \n",
       "1               Monash University -37.908300  145.138000  grid.1002.3   \n",
       "2        University of Queensland -27.495964  153.009627  grid.1003.2   \n",
       "3            Macquarie University -33.775259  151.112915  grid.1004.5   \n",
       "4                  UNSW Australia -33.917731  151.230964  grid.1005.4   \n",
       "\n",
       "                           alias  \n",
       "0                            NaN  \n",
       "1                            NaN  \n",
       "2                            NaN  \n",
       "3                            NaN  \n",
       "4  University of New South Wales  "
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_full = pd.read_csv(\"/Users/hep/Downloads/grid20170810/grid.csv\",low_memory=False)\n",
    "grid_address = pd.read_csv(\"/Users/hep/Downloads/grid20170810/full_tables/addresses.csv\",low_memory=False)\n",
    "grid_alias = pd.read_csv(\"/Users/hep/Downloads/grid20170810/full_tables/aliases.csv\",low_memory=False)\n",
    "\n",
    "grid_df = grid_full.join(grid_address.set_index(keys=[\"grid_id\"]),on=\"ID\")\n",
    "grid_df = grid_df.join(grid_alias.set_index(keys=[\"grid_id\"]),on=\"ID\")\n",
    "grid_df = grid_df[[\"Name\",\"lat\",\"lng\",\"ID\",\"alias\"]]\n",
    "grid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_______________________\n",
    "class ComboFuzzer:\n",
    "    def __init__(self,fuzzers):\n",
    "        self.fuzzers = fuzzers\n",
    "        # Define the normalisation variable in advance\n",
    "        # NB: defined as inverse for speed\n",
    "        self.norm = 1/np.sqrt(len(fuzzers))\n",
    "    \n",
    "    def combo_fuzz(self,target,candidate):\n",
    "        _score = 0\n",
    "        for _fuzz in self.fuzzers:\n",
    "            _raw_score = (_fuzz(target,candidate)/100)\n",
    "            _score += _raw_score**2\n",
    "        return np.sqrt(_score)*self.norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already got 41 matches\n"
     ]
    }
   ],
   "source": [
    "#_______________________\n",
    "class LatLonGetter:\n",
    "    def __init__(self,grid_df,scorer):\n",
    "        self.scorer = scorer\n",
    "        # Find the null aliases\n",
    "        self.df = grid_df\n",
    "        null_alias = pd.isnull(self.df.alias)\n",
    "        not_null = self.df.loc[~null_alias]\n",
    "        # Now generate the list of names + not null aliases\n",
    "        alias_names = list(not_null.alias.values)\n",
    "        std_names = list(grid_df.Name.values)\n",
    "        self.all_possible_values = std_names + alias_names\n",
    "        self.lower_possible_values = [x.lower() for x in \n",
    "                                      self.all_possible_values]\n",
    "        with open(\"fuzzy_scores.pydict\") as f:\n",
    "            self.fuzzy_matches = ast.literal_eval(f.read())\n",
    "#         self.fuzzy_matches = {\"ibm\" : (\"IBM (United States)\",1.),\n",
    "#                               \"microsoft\" : (\"Microsoft (United States)\",1.),\n",
    "#                               \"xerox\" : (\"Xerox (United States)\", 1.),\n",
    "#                               \"pricewaterhousecoopers\" : (\"PricewaterhouseCoopers (United States)\",1.),\n",
    "#                               \"university of california berkeley\": (\"University of California, Berkeley\",1.),\n",
    "#                               \"university of california santa cruz\": (\"University of California, Santa Cruz\",1.),\n",
    "#                               \"linkoping university\": (\"Linköping University\",1.),\n",
    "#                               \"nec\" : (\"NEC (United States)\",1.),\n",
    "#                               \"university of michigan\" : (\"Michigan State University\",1.),\n",
    "#                               \"google\" : (\"Google (United States)\",1.),\n",
    "#                               \"yahoo\" : (\"Yahoo (United States)\",1.),\n",
    "#                               \"at t\" : (\"AT&T (United States)\",1.),\n",
    "#                               \"at t labs\" : (\"AT&T (United States\",1.)}\n",
    "        \n",
    "    def get_latlon(self,mak_name):\n",
    "\n",
    "        # Super-fast check to see if there is an exact match\n",
    "        try:\n",
    "            idx = self.lower_possible_values.index(mak_name)\n",
    "            match = self.all_possible_values[idx]\n",
    "            score = 1.\n",
    "        # Otherwise, fuzzy match\n",
    "        except ValueError:\n",
    "            # If already done a fuzzy match for this\n",
    "            if mak_name in self.fuzzy_matches:\n",
    "                match,score = self.fuzzy_matches[mak_name]\n",
    "            # Otherwise, do the fuzzy match\n",
    "            else:\n",
    "                match,score = fuzzy_proc.extractOne(query=mak_name,\n",
    "                                                    choices=self.all_possible_values,\n",
    "                                                    scorer=self.scorer)\n",
    "                self.fuzzy_matches[mak_name] = (match,score)\n",
    "        \n",
    "        # Check whether the match was a Name or alias\n",
    "        condition = grid_df.Name == match\n",
    "        if condition.sum() == 0:\n",
    "            condition = grid_df.alias == match\n",
    "        _df = grid_df.loc[condition]\n",
    "\n",
    "        # Get the lat/lon\n",
    "        lat = _df[\"lat\"].values[0]\n",
    "        lon = _df[\"lng\"].values[0]\n",
    "        return (lat,lon,score)\n",
    "\n",
    "\n",
    "    def process_latlons(self,mak_institutes):\n",
    "        isnull = pd.isnull(mak_institutes)\n",
    "        if type(isnull) is bool:\n",
    "            if isnull:\n",
    "                return []\n",
    "        elif all(isnull):     \n",
    "            return []\n",
    "        return [self.get_latlon(mak_name) \n",
    "                for mak_name in mak_institutes]\n",
    "    \n",
    "#_______________________\n",
    "# Fuzzy combination of partial ratio and token sort ratio\n",
    "cf = ComboFuzzer([fuzz.token_sort_ratio,fuzz.partial_ratio])\n",
    "llg = LatLonGetter(grid_df=grid_df,scorer=cf.combo_fuzz)\n",
    "\n",
    "print(\"Already got\",len(llg.fuzzy_matches),\"matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing 2028\n"
     ]
    }
   ],
   "source": [
    "mak_df[\"lat_lon_score\"] = [llg.process_latlons(insts) for insts in mak_df[\"institutes\"]]\n",
    "with open(\"fuzzy_scores.pydict\",\"w\") as f:\n",
    "    print(\"writing\",len(llg.fuzzy_matches))\n",
    "    f.write(str(llg.fuzzy_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mak_df.to_json('/Users/hep/Downloads/CS_STATS_id_title_tag_MAK-matched_GRID-matched.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['citations', 'date', 'institutes', 'matched', 'pid', 'title',\n",
       "       'lat_lon_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mak_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['citations', 'date', 'institutes', 'matched', 'pid', 'title'], dtype='object')"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json('/Users/hep/Nesta/coll_int_ai_case/notebooks/MAK_disambiguate/modules/CS_STATS_id_title_tag_MAK-matched.json').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citations</th>\n",
       "      <th>date</th>\n",
       "      <th>institutes</th>\n",
       "      <th>matched</th>\n",
       "      <th>pid</th>\n",
       "      <th>title</th>\n",
       "      <th>lat_lon_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.0</td>\n",
       "      <td>2008-11-13</td>\n",
       "      <td>[max planck society, heidelberg institute for ...</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/0811.2055v2</td>\n",
       "      <td>gpu based interactive visualization of billion...</td>\n",
       "      <td>[(48.141292, 11.581925, 1.0), (49.415617, 8.73...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2007-01-10</td>\n",
       "      <td>[bielefeld university, washington university i...</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/0707.0808v1</td>\n",
       "      <td>the cyborg astrobiologist porting from a weara...</td>\n",
       "      <td>[(52.037778, 8.493056, 1.0), (38.649033, -90.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0</td>\n",
       "      <td>2008-09-20</td>\n",
       "      <td>[university of california berkeley, university...</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/0706.4108v1</td>\n",
       "      <td>event weighted tests for detecting periodicity...</td>\n",
       "      <td>[(37.872162, -122.258572, 1.0), (52.355792, 4....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2008-11-01</td>\n",
       "      <td>[massachusetts institute of technology]</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/0706.4048v1</td>\n",
       "      <td>getting more from your multicore exploiting op...</td>\n",
       "      <td>[(42.35982, -71.09211, 1.0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2007-01-06</td>\n",
       "      <td>[harvard university]</td>\n",
       "      <td>True</td>\n",
       "      <td>http://arxiv.org/abs/cs/0701035v1</td>\n",
       "      <td>finding astronomical communities through co re...</td>\n",
       "      <td>[(42.377053, -71.116657, 1.0)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   citations       date                                         institutes  \\\n",
       "0       12.0 2008-11-13  [max planck society, heidelberg institute for ...   \n",
       "1        3.0 2007-01-10  [bielefeld university, washington university i...   \n",
       "2       12.0 2008-09-20  [university of california berkeley, university...   \n",
       "3        2.0 2008-11-01            [massachusetts institute of technology]   \n",
       "4        0.0 2007-01-06                               [harvard university]   \n",
       "\n",
       "  matched                                pid  \\\n",
       "0    True   http://arxiv.org/abs/0811.2055v2   \n",
       "1    True   http://arxiv.org/abs/0707.0808v1   \n",
       "2    True   http://arxiv.org/abs/0706.4108v1   \n",
       "3    True   http://arxiv.org/abs/0706.4048v1   \n",
       "4    True  http://arxiv.org/abs/cs/0701035v1   \n",
       "\n",
       "                                               title  \\\n",
       "0  gpu based interactive visualization of billion...   \n",
       "1  the cyborg astrobiologist porting from a weara...   \n",
       "2  event weighted tests for detecting periodicity...   \n",
       "3  getting more from your multicore exploiting op...   \n",
       "4  finding astronomical communities through co re...   \n",
       "\n",
       "                                       lat_lon_score  \n",
       "0  [(48.141292, 11.581925, 1.0), (49.415617, 8.73...  \n",
       "1  [(52.037778, 8.493056, 1.0), (38.649033, -90.3...  \n",
       "2  [(37.872162, -122.258572, 1.0), (52.355792, 4....  \n",
       "3                       [(42.35982, -71.09211, 1.0)]  \n",
       "4                     [(42.377053, -71.116657, 1.0)]  "
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mak_df.loc[mak_df.matched == True].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output from MAK and GRID matching\n",
    "\n",
    "## Method\n",
    "\n",
    "### Matching to MAK\n",
    "\n",
    "MAK can be queried by concatenating OR-statements together. The number of results from a MAK query can be no larger than 1000, so we nominally query with 600 sub-queries. We use the paper title from arXiv for the matching, which are prepared by the following procedure:\n",
    "\n",
    "1. Identify any foreign characters as non-symbolic.\n",
    "2. Replace all symbolic characters with spaces.\n",
    "3. Ensure no more than one space separates characters.\n",
    "\n",
    "This procedure returns a 90% match rate, which may be missing paper where the title is different from that presented on arXiv, or where the paper has not been published in a journal. It may be possible to recuperate some of these missing 10% of papers in the future, for example by matching paper credentials, although this is currently not a limiting factor in our analysis.\n",
    "\n",
    "### Matching to GRID\n",
    "\n",
    "The GRID dataset contains institute names, and aliases (where applicable), and a corresponding geospatial coordinate (latitude and longitude). Each institute name from MAK is matched to the comprehensive list from GRID in the following manner:\n",
    "\n",
    "1. If there is an exact match amongst the institute names or aliases, then extract the coordinates of this match. Assign a \"score\" of 1 to this match (see step 3. for the definition of \"score\").\n",
    "2. Otherwise, check whether a match has previously been found. If so, extract the coordinates and score of this match.\n",
    "3. Otherwise, calculate a matching score of the MAK by convoluting the matching scores of various fuzzy-matching algorithms in the following manner:\n",
    "$$ \\frac{1}{\\sqrt{N}} \\sqrt{ \\sum_{n=0}^{N} F_{n}(w_{MAK},W_{GRID})^{2} } $$\n",
    "\n",
    "where $N$ is the number of fuzzy-matching algorithms to use, $F_{n}()$ returns a fuzzy-matching score (in the range $0 \\rightarrow 1$) from the $n^{\\text{th}}$ algorithm, $w_{MAK}$ is the name from MAK to be matched and $W_{GRID}$ is the comprensive list of institutes in the GRID data. \n",
    "\n",
    "I currently use the `token_sort_ratio` and `partial_ratio` algorithms implemented in the `fuzzywuzzy` module.\n",
    "\n",
    "## Fields\n",
    "\n",
    "| field | source | description |\n",
    "|---|---|---|\n",
    "| citations | MAK | number of citations |\n",
    "| date | MAK | date of publication |\n",
    "| matched | joel | flag indicating a successful match between arXiv and MAK |\n",
    "| pid | arXiv | arXiv publication ID, for matching back to arXiv data |\n",
    "| title | joel | the normalised publication title, used for matching to MAK |\n",
    "| institutes | MAK | list of institutes from successful matches between arXiv and MAK |\n",
    "| lat_lon_score | GRID / joel | A list of triplets, with a one-to-one correspondence with institutes. The first two fields are, respectively, latitude and longitude. The third field is the best fuzzy-matching score between GRID and MAK institutes. |\n",
    "\n",
    "It is generally recommended to only use institutes with scores of 1 of used, which is sufficient for 80% of individual institute-paper matches. Therefore the above method yields an approximate efficiency of 72%, although there are known issues with the GRID matching procedure which leads to a very small number of false matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "condition = mak_df.lat_lon_score.apply(lambda x: all(_x[2] == 1.0 for _x in x) and len(x) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74752"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(condition & (mak_df.matched == True)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122198"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mak_df.matched == True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6117285061948641"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "74752/122198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-299-46108cea6e97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmak_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlat_lon_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-299-46108cea6e97>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmak_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlat_lon_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "sum(1 for lat,lon,score in list(mak_df.lat_lon_score.values) if score == 1. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8080228249255462\n"
     ]
    }
   ],
   "source": [
    "n_good = 0\n",
    "n_total = 0\n",
    "for row in mak_df.lat_lon_score.values:\n",
    "    for lat,log,score in row:\n",
    "        n_total += 1\n",
    "        if score == 1.0:\n",
    "            n_good += 1\n",
    "print(n_good/n_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
