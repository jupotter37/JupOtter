{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Purpose\n",
    "This notebook is a testing ground for a human-robot chatbox interface. There are three primary elements to our intelligent chatbox:\n",
    " * **Suggestions** using [Bootstrap-3 Typeahead](https://github.com/bassjobsen/Bootstrap-3-Typeahead), so that the user has a better understanding of the language the robot can understand. This is shown at the end of the document.\n",
    " * **Template-matching** to allow the robot to understand non-standard sentences. We're building off Tellex, Kollar and others' work on Spatial Description Clauses [1,2] to parse the natural language into our templates. This document develops the mathematical framework.\n",
    " * **Online correction**, in which the operator tells the robot that the robot misunderstood. This work is ongoing.\n",
    "\n",
    "\n",
    "## 1.1 Template-matching Block Diagram\n",
    "In general, our goal is to find the template human sensor statement(s) that best matches some natural language input. To solve this, we have developed a workflow as shown below:\n",
    "<img src=\"Chatbox Workflow.png\" width=\"600\">\n",
    "\n",
    "## 1.2 Problem Formulation\n",
    "We identify the following probabilistic and deterministic variables to track:\n",
    " * $D$, a input document.\n",
    " * $T$, one of $\\mathcal{T}$ tokenizations of an input document.\n",
    " * $S$, a quality score associated with a tokenization. $\\mathcal{S} = \\{S_1,\\dots,S_\\mathcal{T}\\}$\n",
    " * $L$, the semantically tagged labels associated with a tokenization.\n",
    " * $TDC$, one of $\\mathcal{N}$ target description clauses associated with a tokenization.\n",
    " * $H$, a human sensor template (one of $\\mathcal{H}_T$ templates), comprised of some combination of $\\{p, c, t, g, s, a, m\\}$, where:\n",
    "   * $p$ is a positivity,\n",
    "   * $c$ is a certainty,\n",
    "   * $t$ is a target,\n",
    "   * $g$ is a grounding,\n",
    "   * $s$ is a spatial relation,\n",
    "   * $a$ is an action,\n",
    "   * $m$ is a modifier.\n",
    " * $O$, observed elements in the environment.\n",
    "\n",
    "Based on conditional independence assumptions, we can form the following Bayesian Network (BN):\n",
    "<img src=\"Chatbox PGM.png\" width=\"400\">\n",
    "\n",
    "We can simplify the joint probability distribution based on the BN:\n",
    "$$\n",
    "P(T,L,TDC,H \\vert D, \\mathcal{S}, O) = \\prod_{i=1}^\\mathcal{T}P(T_i \\vert S_i, D) P(L_i \\vert T_i) \\prod_{j=1}^\\mathcal{N} P(TDC_j \\vert L_i) P(H_j \\vert TDC, O) \n",
    "$$\n",
    "\n",
    "Our goal is to find the set of template human sensor statements $\\mathcal{H} = \\{H_1,\\dots,H_\\mathcal{N}\\}$ and tokenizations $\\mathcal{T}$ that maximize this distribution, thus:\n",
    "\n",
    "$$\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "\\argmax_{\\mathcal{H},\\mathcal{\\tilde{T}}} \\prod_{i=1}^\\mathcal{\\tilde{T}}P(T_i \\vert S_i, D) P(L_i \\vert T_i) \\prod_{j=1}^\\mathcal{N} P(TDC_j \\vert L_i) P(H_j \\vert TDC, O) \n",
    "$$\n",
    "\n",
    "In practice, this likely means we'll need to limit the number of tokenizations considered, as there are $2^{(W-1)}$ tokenizations available for each input document consisting of $W$ words (see below for a full description). \n",
    "\n",
    "Additionally, it will be important to remove 'garbage' sentences:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Sensor Statement Matching\n",
    "The following sections will describe how to compute each of the following terms:\n",
    "* $P(T_i \\vert S_i, D)$ using a tokenization and scoring algorithm; \n",
    "* $P(L_i \\vert T_i)$ using Conditional Random Fields;\n",
    "* $P(TDC_j \\vert L_i)$ using a greedy templating algorithm; \n",
    "* $P(H_j \\vert O)$ using sense2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Multi-Word Tokenization\n",
    "\n",
    "Tokenization is the process of splitting an input document into a sequence of *tokens*: individual portions of the input document. We'll then tag each token with a semantic label in the next step.\n",
    "\n",
    "Given an input document, say, \"Roy is behind the desk.\" we need to be able to figure out how to split the document the sentence into a set of tokens to be labeled. A first-cut approach would be to split the sentence based on just the words: [\"Roy\", \"is\", \"behind\", \"the\", \"desk.\"]. But, this causes issues with words that should go together for one label (i.e., one label should be assigned to \"the desk\" instead of labeling \"the\" and \"desk\" separately).\n",
    "\n",
    "We make the assumption that a document should be tokenized based on n-grams and delimiter punctuation. $n$-grams are arbitrarily grouped sequences of words, with $n$ representing the number of words grouped together (i.e. \"the desk\" is a bigram or $2$-gram). Delimiter punctuation, (',', ';', '.', '!', '?'), marks divisions between n-grams, such that no two words on either side of the punctuation can be combined into an $n$-gram. The following lists the tokenizations under these assumptions of \"Roy is behind the desk.\":\n",
    "\n",
    "|                 | Token 1                | Token 2            | Token 3         | Token 4  | Token 5 | Token 6 | Schema             |\n",
    "|-----------------|------------------------|--------------------|-----------------|----------|---------|---------|--------------------|\n",
    "| Tokenization 1  | Roy                    | is                 | behind          | the      | desk    | .       | [1, 1, 1, 1, 1, 1] |\n",
    "| Tokenization 2  | Roy is                 | behind             | the             | desk     | .       |         | [2, 1, 1, 1, 1]    |\n",
    "| Tokenization 3  | Roy is                 | behind the         | desk            | .        |         |         | [2, 2, 1, 1]       |\n",
    "| Tokenization 4  | Roy is                 | behind             | the desk        | .        |         |         | [2, 1, 2, 1]       |\n",
    "| Tokenization 5  | Roy is                 | behind the desk    | .               |          |         |         | [2, 3, 1]          |\n",
    "| Tokenization 6  | Roy is behind          | the                | desk            | .        |         |         | [3, 1, 1, 1]       |\n",
    "| Tokenization 7  | Roy is behind          | the desk           | .               |          |         |         | [3, 2, 1]          |\n",
    "| Tokenization 8  | Roy is behind the      | desk               | .               |          |         |         | [4, 1, 1]          |\n",
    "| Tokenization 9  | Roy is behind the desk | .                  |                 |          |         |         | [5, 1]          |\n",
    "| Tokenization 10 | Roy                    | is behind          | the             | desk     | .       |         | [1, 2, 1, 1, 1]    |\n",
    "| Tokenization 11 | Roy                    | is behind          | the desk        | .        |         |         | [1, 2, 2, 1]       |\n",
    "| Tokenization 12 | Roy                    | is behind the      | desk            | .        |         |         | [1, 3, 1, 1]       |\n",
    "| Tokenization 13 | Roy                    | is behind the desk | .               |          |         |         | [1, 4, 1]          |\n",
    "| Tokenization 14 | Roy                    | is                 | behind the      | desk     | .       |         | [1, 1, 2, 1, 1]    |\n",
    "| Tokenization 15 | Roy                    | is                 | behind the desk | .        |         |         | [1, 1, 3, 1]       |\n",
    "| Tokenization 16 | Roy                    | is                 | behind          | the desk | .       |         | [1, 1, 1, 2, 1]    |\n",
    "\n",
    "\n",
    "As clearly visible in the above example, a simple 5-word sentence can easily lead to a large number of possible tokenizations. Specifically, the upper bound for the total number of tokenizations is $2^{(W-1)}$, where $W$ is the number of words in the document. This is an upper bound because of the tokenization reduction effects of delimiter punctuation (i.e., \"The woman, in blue, near me.\" has only $2^1 \\times 2^1 \\times 2^1 = 8$ tokenizations, as opposed to the $2^5 =32$ tokenizations of \"The woman in blue near me.\"). \n",
    "\n",
    "\n",
    "\n",
    "### 2.1.1 Probability Measure\n",
    "\n",
    "We require the definition of $P(T_i \\vert S_i, D)$ for each $T_i \\in \\mathcal{T}$. We assume that each input document $D$ has one correct (unknown) tokenization $T^*$, and that $P(T_i \\vert S_i, D)$ represents the probability that the tokenization $T_i = T^*$. Clearly, the score metric chosen will influence this probability.\n",
    "\n",
    "### 2.1.2 Association Measures\n",
    "\n",
    "Given a corpus, we can define *association measures* between words that define how likely two words are combined to form one token. These can be applied recursively to determine multi-word tokenizations. We begin by defining a few terms (largely taken from Michelbacher 2013).\n",
    "\n",
    "A corpus is analyzed for each word pair $(U,V)$, where $U$ is the pair's first component and $V$ is the pair's second component. $u$ and $v$ are specific realizations of the random variables $U$ and $V$. For example, $\\{ (u,v) \\vert U =u \\land V\\neq v\\}$ could represent case of $(U,V) = (\\text{the}, \\text{cat})$ and $(u,v) = (\\text{the}, \\text{dog})$.\n",
    "\n",
    "*Observed frequencies* for word pairs are:\n",
    "\n",
    "\\begin{align}\n",
    "O_{11} &= \\vert \\{ (u,v) \\vert U =u \\land V=v\\} \\vert \\\\\n",
    "O_{12} &= \\vert \\{ (u,v) \\vert U =u \\land V\\neq v\\} \\vert \\\\\n",
    "O_{21} &= \\vert \\{ (u,v) \\vert U \\neq u \\land V=v\\} \\vert \\\\\n",
    "O_{22} &= \\vert \\{ (u,v) \\vert U \\neq u \\land V\\neq v\\} \\vert\n",
    "\\end{align}\n",
    "\n",
    "*Marginal frequencies* for individual words within word pairs are:\n",
    "\n",
    "\\begin{align}\n",
    "R_{1} &= \\vert \\{ (u,v) \\vert U =u\\} \\vert \\\\\n",
    "R_{2} &= \\vert \\{ (u,v) \\vert U \\neq u\\} \\vert \\\\\n",
    "C_{1} &= \\vert \\{ (u,v) \\vert V=v\\} \\vert \\\\\n",
    "C_{2} &= \\vert \\{ (u,v) \\vert V\\neq v\\} \\vert\n",
    "\\end{align}\n",
    "\n",
    "Expected frequencies, then, are:\n",
    "\n",
    "\\begin{align}\n",
    "E_{11} &= \\frac{R_1C_1}{N} \\\\\n",
    "E_{12} &= \\frac{R_1C_2}{N} \\\\\n",
    "E_{21} &= \\frac{R_2C_1}{N} \\\\\n",
    "E_{22} &= \\frac{R_2C_2}{N}\\vert\n",
    "\\end{align}\n",
    "\n",
    "#### 2.1.2.1 t-score\n",
    "\n",
    "Measure of how far (positively or negatively) the observed frequency deviates from expected frequency in multiples of sample variance:\n",
    "$$\n",
    "t =\\frac{O_{11} - E_{11}}{\\sqrt{O_{11}}}\n",
    "$$\n",
    "\n",
    "#### 2.1.2.2 z-score\n",
    "\n",
    "Measure of how far (positively or negatively) the observed frequency deviates from expected frequency in multiples of sample variance (using a different approximation for sample variance than the t-test):\n",
    "$$\n",
    "t =\\frac{O_{11} - E_{11}}{\\sqrt{E_{11}}}\n",
    "$$\n",
    "\n",
    "#### 2.1.2.3 chi-square\n",
    "\n",
    "Measure of independence between $u$ and $v$:\n",
    "$$\n",
    "\\chi^2 = \\sum_{i,j} \\frac{(O_{ij} - E_{i,j})^2}{E_{i,j}}\n",
    "$$\n",
    "\n",
    "Where $\\{i,j\\} \\in \\{\\{1,1\\},\\{1,2\\},\\{2,1\\},\\{2,2\\}\\}$.\n",
    "\n",
    "#### 2.1.2.4 log-likelihood\n",
    "\n",
    "Requires two hypotheses:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{H_0} &: P((u,v)) = P((u,v^\\prime)); v^\\prime \\neq v\\\\\n",
    "\\mathbf{H_1} &: P((u,v)) =p_1 \\neq p_2 = P((u,v^\\prime)); v^\\prime \\neq v\\\\\n",
    "\\end{align}\n",
    "\n",
    "Where $\\mathbf{H_0}$ is the hypothesis that the probability of $u$ occurring is the same regardless if the next word is $v$ or not, and $\\mathbf{H_1}$ is the probability of $u$ occurring is different depending on whether $v$ or some other word $v^\\prime$ preceeds it.\n",
    "\n",
    "The log-likelihood tests\n",
    "\n",
    "$$\n",
    "\\lambda = \\frac{\\text{max}L(H_0)}{\\text{max}L(H_1)} = \\frac{L(O_{11},C_1,p)L(O_{12},C_2,p)}{L(O_{11},C_1,p_1)L(O_{12},C_2,p_2)}\n",
    "$$\n",
    "\n",
    "Where $L(k,n,p) = p^k(1-p)^{(n-k)}$, and maximum likelihood estimates are $p = \\frac{R_1}{N}$, $p_1 = \\frac{O_{11}}{C_1}$, and $p_2 = \\frac{O_{12}}{C_2}$. $-2 \\log{\\lambda}$ is asymptotically $\\chi^2$.\n",
    "\n",
    "A simplified expression is:\n",
    "\n",
    "$$\n",
    "\\text{log-likelihood} = 2 \\sum_{i,j}O_{i,j} \\log{\\frac{O_{ij}}{E{ij}}}\n",
    "$$\n",
    "\n",
    "#### 2.1.2.5 Dice coefficient\n",
    "\n",
    "The Dice coefficient represent similarity between two sets. For sets $A$ and $B$, the coefficient is:\n",
    "\n",
    "$$\n",
    "\\frac{2 \\vert A \\cap B \\vert}{\\vert A \\vert + \\vert B \\vert}\n",
    "$$\n",
    "\n",
    "In our case:\n",
    "\n",
    "$$\n",
    "\\text{Dice} = \\frac{2O_{11}}{R_1 + C_1}\n",
    "$$\n",
    "\n",
    "#### 2.1.2.6 Pointwise Mutual Information\n",
    "\n",
    "The idea is to measure for two random variables $X$ and $Y$ how much information about $Y$ is contained in $X$ or in other words the reduction in uncertainty about $Y$ when knowing $X$. Thus,\n",
    "\n",
    "$$\n",
    "I(x,y) = \\frac{p(x,y)}{p(x)p(y)} = \\frac{O_{11}}{E_{11}}\n",
    "$$\n",
    "\n",
    "#### 2.1.2.7 Symmetrical Conditional Probability\n",
    "\n",
    "Recall: conditional probability is defined as,\n",
    "\n",
    "$$\n",
    "P(A \\vert B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "Silvia and Lopes (1999) proposed symmetrical conditional probability as,\n",
    "\n",
    "$$\n",
    "P(A \\vert B) P(B \\vert A) = \\frac{P(A \\cap B)^2}{P(a)P(B)}\n",
    "$$\n",
    "\n",
    "Which gives us:\n",
    "\n",
    "$$\n",
    "\\text{scp} = \\frac{O_{11}^2}{R_1 C_1}\n",
    "$$\n",
    "\n",
    "#### 2.1.2.8 Raw Frequency\n",
    "\n",
    "Finally, we can simply consider frequency of the observed pairs:\n",
    "\n",
    "$$\n",
    "\\text{frequency} = O_{11}\n",
    "$$\n",
    "\n",
    "### 2.1.3 Supervised Learning with Association Measures\n",
    "\n",
    "**How do we formulate this as a supervised learning problem for tokenization?**\n",
    "\n",
    "* Identify which association measures lead to the best scores to identify the true tokenizations\n",
    "* Train a classifier to use a mixture of association measures?\n",
    "\n",
    "We have an input corpus $O_k$ with words $w_c$, $c = 1, \\dots, W$ which we'd like to group into multi-word tokens $t_z$, $z = 1, \\dots, Z$ where $Z \\leq W$. Each $t_z$ contains at least one word $w_c$, and no word $w_c$ is contained in multiple tokens $t_z$.\n",
    "\n",
    "Our goal is to train a classifier to segment the input document $O_k$ into tokenizations $t_z$. To do so, we classify each word $w_c$ as belonging to the 'grouped' class $G$ or not. This process is represented below.\n",
    "\n",
    "<img src=\"mwt classification.png\" width=\"400\">\n",
    "\n",
    "Starting from the beginning of the sentence, we set the first word $w_1$ to be part of token $t_1$. We take the next word and identify if the $(w_1,w_2)$ word pair should be grouped together according to some tuned threshold $\\nu$, that is: $P_\\theta(G = True \\mid w_1, w_2) \\geq \\nu$. To we model this probability as a  $P_\\theta(G = True \\mid w_1, w_2)$\n",
    "\n",
    "$$\n",
    "J = \\frac{-1}{N} \\sum L \\cdot \\log{\\left( P_\\theta(G = True \\mid w_1, w_2) \\right)} + \\frac{\\lambda}{2} \\sum \\theta^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - s\\nabla_\\theta J = \\theta - s \\left( \\frac{-1}{N} L - P_\\theta(G = True \\mid w_1, w_2)X + \\lambda \\theta \\right)\n",
    "$$\n",
    "\n",
    "### 2.1.4 Mikolov's Association Measures\n",
    "\n",
    "If we wanted to accomodate, say, 20-word sentences, we'd need to maximize $\\mathcal{H}$ over each of those $1,048,576$ individual possible tokenizations. Instead of doing that, what we'd like to do is find a measure for the $m$ most likely tokenizations and then maximize $\\mathcal{H}$ for each of those. But, how do we find a measure for how likely each tokenization is?\n",
    "\n",
    "To explore this, we use the following cherry-picked example corpus:\n",
    "\n",
    "    The green robot is behind the couch, heading north.\n",
    "    The green robot is moving slowly.\n",
    "    The green robot is moving quickly.\n",
    "    Deckard is next to the green fern.\n",
    "\n",
    "Mikolov et al. [3] use is the following score metric:\n",
    "\n",
    "$$\n",
    "\\text{score}(w_i,w_j) = \\frac{\\text{count}(w_i,w_j) - \\delta}{\\text{count}(w_i)\\text{count}(w_j)}\n",
    "$$\n",
    "\n",
    "Where $\\delta$ is some tuned discounting coefficient. This parses the entire corpus for all unigrams and bigrams, counts them, and then returns a higher score for words that are only frequently seen in context of one-another. To identify multi-word tokens greater than two, this process is applied recursively. However, this poses several problems and is only one of several possible metrics. \n",
    "\n",
    "<!-- This metric penalizes certain tokenizations that may be common. For instance, \"the green robot\" appears three times and \"the green fern\" appears once, so their scores are, $\\frac{3}{4 \\cdot 4 \\cdot 3} = \\frac{1}{16}$ and $\\frac{1}{4 \\cdot 4 \\cdot 1} = \\frac{1}{16}$, but the phrase \"the green\" has a score of $\\frac{4}{4 \\cdot 4} = \\frac{1}{4}$. While this leads to a conservative tokenization length, in that it prefers shorter tokenizations over longer ones, there are no cases in which the longer tokenizations have a greater score than the shorter ones. For $n$-grams, we have: \n",
    "\n",
    "$$\n",
    "\\text{score}(\\mathcal{W_{1:n}}) \\leq \\text{score}(\\mathcal{W_{1:n-1}}) \\leq \\text{score}(\\mathcal{W_{1:n-2}}) \\dots \\leq \\text{score}(\\mathcal{W_{1:2}})\n",
    "$$\n",
    "\n",
    "Put another way, the only case in which a trigram has a greater score than a bigram is when none of the words of the trigram appear in another trigram, and so on for all $n$-grams. As in the case of, \"the green fern,\" trigrams may have more meaning than bigrams (\"the green\" is an incomplete tokenization). -->\n",
    "\n",
    "Most importantly, this score metric ignores possible training data – if we have proper tokenizations of words, we should use that information to modify the score. \n",
    "\n",
    "If we have access to a manually tokenized corpus, such as:\n",
    "\n",
    "    [The green robot] [is] [behind] [the couch], [heading] [north] [.]\n",
    "    [The green robot] [is] [moving] [slowly] [.]\n",
    "    [The green robot] [is] [moving] [quickly] [.]\n",
    "    [Deckard] [is] [next to] [the green fern] [.]\n",
    "    \n",
    "Then we can use this as training data for our score function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Semantic Label Tagging\n",
    "\n",
    "Conditional Random Fields (CRFs) are a classification technique postulated by Lafferty et al. [4]. They are similar to Hidden Markov Models (HMMs) and stochastic grammars (SGs), but, unlike those two generative models (i.e. concerned with joint label-observation probability), are a discriminative model (i.e. concerned with label probability conditioned on observations). SGs and HMMs can be intractable because of their need to enumerate all possible observation sequences; this is not the case for CRFs.\n",
    "\n",
    "Other discriminative models, such as Maximum Entropy Markov Models (MEMMs), however, suffer from the label-bias problem, in which the transition probabilities leaving any one state only interact with other transitions leaving that state, rather than the global transition set. This causes a bias towards states with fewer outgoing transitions (as illustrated in [4]). CRFs don't have this problem, as Lafferty et al. states: \"The critical difference between CRFs and MEMMs is that a MEMM uses per-state exponential models for the conditional probabilities of next states given the current state, while a CRF has a single exponential model for the joint probability of the entire sequence of labels given the observation sequence.\"\n",
    "\n",
    "In essence, we want to learn $p(\\mathbf{Y} \\vert \\mathbf{X})$, where $\\mathbf{Y}$ is a sequence of labels given to the sequence of observed values $\\mathbf{X}$. Once we've trained our model, we can find the $\\mathbf{Y_*}$ that maximizes $p(\\mathbf{Y_*} \\vert \\mathbf{X_*})$, where a $*$ signifies a new sequence.\n",
    "\n",
    "By the Hammersley-Clifford theorem of random fields, the join distribution over $\\mathbf{Y}$, given $\\mathbf{X}$, is:\n",
    "\n",
    "$$\n",
    "p_\\theta(\\mathbf{y} \\vert \\mathbf{x}) \\propto \\exp\\left( \n",
    "\\sum_{e \\in E,k} \\lambda_k f_k(e,\\mathbf{y}\\vert_e,\\mathbf{x}) +\n",
    "\\sum_{v \\in V,k} \\mu_k g_k(v,\\mathbf{y}\\vert_v,\\mathbf{x})\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "where $f_k(\\cdot)$ and $g_k(\\cdot)$ are *transition feature functions* and *state feature functions*, respectively, that are defined a priori, and $\\lambda_k$, $\\mu_k$ are parameters to be learned from training data.\n",
    "\n",
    "In our case, we have $P(L_i \\vert T_i) \\triangleq p_\\theta(\\mathbf{y} \\vert \\mathbf{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Template Fitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Similarity Matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "## General\n",
    "\n",
    "* Overall process: Single-word tokenization -> multi-word tokenization -> Lemmatization/POS tagging (optional)\n",
    "* Order is important! Should we check for sense matching before TDC classification, or vice-versa?\n",
    "* Requires identification method for non-measurements: i.e. \"I am blue.\" should not map to any human sensor statement:\n",
    "  * Threshhold on $P(T,L,TDC,H \\vert D, \\mathcal{S}, O)$, potentially\n",
    "* Look at precision and recall as metrics for evaluating each step of the NLP pipeline:\n",
    " * Full-sentence metrics:\n",
    "   * True positive: raw NL input **matches correct** human sensor statement\n",
    "   * False positive: raw NL input **matches incorrect** human sensor statement\n",
    "   * False negative: raw NL input **incorrectly does not match any** human sensor statement\n",
    "   * True negative: raw NL input **correctly does not match any** human sensor statement\n",
    "   * $\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$ is a measure of how many NL inputs were correctly matched\n",
    "   * $\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{TN}}$ is a measure of how many NL inputs were correctly transformed into HSSs\n",
    " * Requires having ground truth for *each* stage, not just the end\n",
    "\n",
    "## Development Process\n",
    "1. Close loop ~√\n",
    " 1. Develop chat interface in Python √\n",
    " 2. Refactor human sensor code (i.e. centralize the Human Sensor class)\n",
    "2. Identify end-to-end performance metric\n",
    " 1. Identify full human sensor codebook **[dependent on 1.2]**\n",
    " 2. Assign each input sentence to one codebook entry **[Jeremy, Sierra]**\n",
    " 3. Output proposed human sensor statement for each \n",
    " 4. Check precision/recall\n",
    "3. Identify & create missing human sensor templates\n",
    " 1. Consider N/A cases **[dependent on 2.2]**\n",
    " 2. Create low-hanging human sensor templates\n",
    "    1. Camera as grounding\n",
    "4. Identify component performance metrics\n",
    "5. Introduce clean-up and augmentation components\n",
    "6. Incorporate confidence to each step of fusion chain\n",
    "7. Consider multiple hypothesis tracking (i.e. non-monolithic framework)\n",
    "\n",
    "\n",
    "## Pre-processing\n",
    "\n",
    "* Contractions: should we change \"n't\" to \" not\" instead? I.e. \"don't\" -> \"do not\"?\n",
    "* Autocorrect: should we autocorrect transparently, behind the scenes, or neither?\n",
    "* Case-correction: should we enforce proper case (i.e. remvoe all caps)?\n",
    "* Punctuation: should we remove double punctuation, insert regular punctuation, etc.?\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "* How deep do we want tokenization to go? Set a max limit? Brute-force tokenization, POS rule-based tokenization, or a variant of Michelbacher's classification strategy? Is our domain-specific terminology too dynamic/broad to prevent phrase look-ups?\n",
    "* We *do not* care about non-compositionality (i.e. meaning cannot be inferred from components: \"Kick the bucket\" has nothing to do with footwork and pails); \"The red robot\" is a compositional phrase.\n",
    "* We *do not* care about non-modifiability (i.e. \"Kick the bucket\" cannot be modified to \"Kick the small bucket\"); \"The red robot\" can be modified to \"The fast red robot\".\n",
    "* We *do not* care about non-substitutability (i.e. \"Kick the bucket\" cannot be substituted to \"Kick the pail\"); \"The red robot\" can be substituted to to \"The red robber\".\n",
    "* In tokenizatation, we seek to capture *syntagmatic* relations between words (the relation between two words is called syntagmatic if they occur in sequence, in that words in \"the old man\" are syntagmatically related, but \"good\" and \"bad\" are paradigmatically related -- a relationship we capture at the end, with Sense Matching)\n",
    "* Can use a number of (symmetric!) association measures:\n",
    " * T-score\n",
    " * z-score\n",
    " * chi-square\n",
    " * log-likelihood\n",
    " * dice coefficient\n",
    " * pointwise mutual information\n",
    " * symmetrical conditional probability\n",
    " * raw frequency\n",
    "\n",
    "\n",
    "## Tagging\n",
    "\n",
    "* What tokens are used for CRF training? Include POS column? Lemma column? Fused-word column (e.g., \"the_red_car\")? Multiple word columns based on max tokenization (e.g., \"the\\tred\\tcar\")?\n",
    "\n",
    "\n",
    "## Template-fitting\n",
    "\n",
    "* How do we identify new templates?\n",
    "\n",
    "## Paradigmatic-matching\n",
    "* Can we provide associations to the target words (i.e., \"Roy\" is associated with \"red\")? This allows for many-to-many-to-one similarity checks, instead of many-to-one.\n",
    " * Modify configuration parameters and use evaluation metrics to determine best tunings for each knob, i.e.:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G3 Framework\n",
    "\n",
    "Their goal: \n",
    "\n",
    "$$\n",
    "%\\\\DeclareMathOperator*{\\argmax}{arg\\,max} \n",
    "\\argmax_\\Gamma p\\left(\\Phi=True \\vert \\text{command}, \\Gamma\\right)\n",
    "$$\n",
    "\n",
    "where $\\Gamma$ is the set of all groundings $\\gamma_i$ and $\\Phi$ is the set of binary correspondence variables which relate groundings to parts of natural language commands.\n",
    "\n",
    "\n",
    "They build SDCs using the Stanford NLP parser. These SDCs can be of the type:\n",
    "* **event**: An action sequence that takes/should take place (e.g. \"Move the tire pallet\")\n",
    "* **object**: A thing in the world (e.g. \"the tire pallet\")\n",
    "* **place**: A place in the world (e.g. \"Next to the tire pallet\")\n",
    "* **path**: A path or path fragment through the world (e.g. \"behind the tire pallet\")\n",
    "\n",
    "Their automatic SDC extractor takes text and generates a hierarchy of SDCs. Each $i$th SDC has a fixed set of fields: figure $\\lambda_i^f$, relation $\\lambda_i^r$, landmarks $\\lambda_i^{l1}$ and $\\lambda_i^{l2}$. They also assert that figures and landmarks are related to real-world groundings ($\\gamma_i^{f}$, $\\gamma_i^{l1}$, $\\gamma_i^{l2}$). Correspondence variables $\\phi_i$ define whether the grounding $\\gamma_i$ corresponds to $SDC_i$, though it's unclear how this works for multiple groundings (i.e., in the phrase, \"between the car and the chair\", would $\\phi_i$ be false if the true grounding for 'chair' were 'truck'?).\n",
    "\n",
    "Generalized Grounding Graphs are factor graphs that relate variables $(\\phi_i, SDC_i,\\Gamma_i)$ (i.e., the grounding, the SDC and the true-false relation between the two), through factors $\\psi_i$. The factors can take the following forms:\n",
    "\n",
    "* $\\psi(\\phi_i, \\lambda_i^f, \\gamma_i)$ for leaf SDCs\n",
    "* $\\psi(\\phi_i, \\lambda_i^r, \\gamma_i^f, \\gamma_i^{l1})$ for internal SDCs with one landmark\n",
    "* $\\psi(\\phi_i, \\lambda_i^r, \\gamma_i^f, \\gamma_i^{l1}, \\gamma_i^{l2})$ for internal SDCs with two landmarks\\\n",
    "\n",
    "Each function $\\psi$ takes the following form (supposedly taken from CRFs):\n",
    "\n",
    "$$\n",
    "\\exp{ \\left( \\sum_k \\mu_k s_k(\\phi_i, SDC_i, \\Gamma) \\right)}\n",
    "$$\n",
    "\n",
    "Where $s_k$ are some pre-defined feature functions and $\\mu_k$ are learned weights. These designed functions $s_k$ are things like $supports(\\gamma_i^f,\\gamma_i^l) \\land (\\text{\"on\"} \\in \\lambda_i^r)$, which captures both the notion of supporting and that \"on\" relates to the function $support$. They then took the Cartesian product of all base functions (the part before the $\\land$ in the prev. equation) with all the word presence functions (i.e., the part after the $\\land$ in the prev. equation). Since some features are non-binary (i.e., \"next to\"), they normalize and discretize these features.\n",
    "\n",
    "For any \n",
    "\n",
    "Their output: a set of object groundings and a sequence of actions for the agent to perform.\n",
    "\n",
    "## Main differences:\n",
    "* We don't need to define 'feature' functions; our spatial relation softmax functions (which replace their Attention Vector Sums) can be learned from data \n",
    "* We use sense2vec instead of WordNet to relate similar words\n",
    "* We assume known object labels and known locations for such objects (although this could  be extended through SLAM and object recognition)\n",
    "* They look over the *entire* space of feature functions, actions, and object assignments, they have a massive search, so they do a fixed-width best-first search (i.e. taking the 20 most promising nodes and continuing down the tree). This is a huge optimization, and were we to use this framework, we would need to define a **position** SDC that describes where a target is; this would have to be discretized and would be *very* complicated if not assumed unimodal.\n",
    "* We don't model anything that relates to actual actions (i.e., TDCs are sensory-only, SDCs are both sensory and command-based). That said, we could extend our work to include commands – \"Go to the kitchen\" would require a prior belief of where the kitchen is, as well as its shape, so that we could create a softmax function for \"inside the kitchen\". These could conceivably be obtained through a combination of SLAM and semantic mapping through object recognition and word sense matching (i.e., drive around, look at objects within a room and decide what room that is, similar to Jensfelt's work)\n",
    "* We learn four things: \n",
    "  * softmax models for arbitrary sensory information (through learning softmax model parameters, as long as they relate to pre-defined states, and we can constrain these to known geometry)\n",
    "  * multi-word tokens from text streams (through learning association measure mappings)\n",
    "  * label assignments for multi-word tokens (through learning feature function weights)\n",
    "  * word sense mapping (through learning vector representations, comes pre-learned from massive corpora)\n",
    "* They learn x things:\n",
    "  * feature function weights for 147,274 feature functions\n",
    "  * word sense mapping (through whatever WordNet does)\n",
    "* We WANT to preserve the uncertainty\n",
    "\n",
    "# Variant G3 Sensory Framework\n",
    "\n",
    "We'd build a set of Target Description Clauses (TDCs) that we'd like to reach from NL input. Instead of defining feature functions to learn, we first learn actual spatial models (that mirror these feature functions) from actual human data. After that, we specify a set of template frameworks for human sensor statements (these templates could be learned instead of designed, but that's later work). \n",
    "\n",
    "To capture spatial relations, we *do not* discretize the space, nor do we use Attention Vector Sum measures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
