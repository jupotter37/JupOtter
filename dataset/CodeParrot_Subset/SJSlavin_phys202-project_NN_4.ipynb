{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.html.widgets import interact\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "sigmoid_v = np.vectorize(sigmoid)\n",
    "\n",
    "def sigmoidprime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "sigmoidprime_v = np.vectorize(sigmoidprime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size = [64, 20, 10]\n",
    "\n",
    "weights = []\n",
    "for n in range(1, len(size)):\n",
    "    weights.append(np.random.rand(size[n-1], size[n]) * 2 - 1)\n",
    "\n",
    "biases = []\n",
    "for n in range(1, len(size)):\n",
    "    biases.append(np.random.rand(size[n]) * 2 - 1)\n",
    "\n",
    "trainingdata = digits.data[0:1200]\n",
    "traininganswers = digits.target[0:1200]\n",
    "lc = 0.02\n",
    "\n",
    "#convert the integer answers into a 10-dimension array\n",
    "traininganswervectors = np.zeros((1796,10))\n",
    "for n in range(1796):\n",
    "    traininganswervectors[n][digits.target[n]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feedforward(a, weights, biases):\n",
    "    b = []\n",
    "    #first element is inputs \"a\"\n",
    "    b.append(a)\n",
    "    for n in range(1, len(size)):\n",
    "        #all other elements depend on the number of neurons\n",
    "        b.append(np.zeros(size[n]))\n",
    "        for n2 in range(0, size[n]):\n",
    "            b[n][n2] = sigmoid_v(np.dot(weights[n-1][0:,n2], b[n-1]) + biases[n-1][n2])\n",
    "      \n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  0.,   0.,   5.,  13.,   9.,   1.,   0.,   0.,   0.,   0.,  13.,\n",
       "         15.,  10.,  15.,   5.,   0.,   0.,   3.,  15.,   2.,   0.,  11.,\n",
       "          8.,   0.,   0.,   4.,  12.,   0.,   0.,   8.,   8.,   0.,   0.,\n",
       "          5.,   8.,   0.,   0.,   9.,   8.,   0.,   0.,   4.,  11.,   0.,\n",
       "          1.,  12.,   7.,   0.,   0.,   2.,  14.,   5.,  10.,  12.,   0.,\n",
       "          0.,   0.,   0.,   6.,  13.,  10.,   0.,   0.,   0.]),\n",
       " array([  9.99999748e-01,   8.98553537e-04,   2.69396153e-23,\n",
       "          1.00000000e+00,   1.36074434e-32,   8.18070554e-23,\n",
       "          1.00000000e+00,   1.00000000e+00,   3.97928335e-20,\n",
       "          1.78323070e-18,   1.53818826e-24,   9.99994182e-01,\n",
       "          9.96131193e-01,   3.20678543e-01,   9.99949835e-01,\n",
       "          1.00000000e+00,   1.26558022e-11,   1.50519342e-03,\n",
       "          1.61018918e-13,   9.72589484e-17]),\n",
       " array([ 0.34792334,  0.26626077,  0.72389612,  0.10852067,  0.32030888,\n",
       "         0.47422726,  0.9414225 ,  0.38432016,  0.94768218,  0.46550206])]"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedforward(trainingdata[0], weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GradientDescent(inputs, answers, weights, biases, batchsize, lc, epochs):\n",
    "    for n in range(epochs):\n",
    "        #pick random locations for input/result data\n",
    "        locations = np.random.randint(0, len(inputs), batchsize)\n",
    "        minibatch = []\n",
    "        #create tuples (inputs, result) based on random locations\n",
    "        for n2 in range(batchsize):\n",
    "            minibatch.append((inputs[locations[n2]], answers[locations[n2]]))\n",
    "        for n3 in range(batchsize):\n",
    "            train(minibatch, weights, biases, lc)\n",
    "        \n",
    "        results = []\n",
    "        for n4 in range(len(trainingdata)):\n",
    "            results.append(feedforward(inputs[n4], weights, biases)[-1])\n",
    "            \n",
    "        accresult = accuracy(inputs, results, answers)\n",
    "        print(\"Epoch \", n, \" : \", accresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(minibatch, weights, biases, lc):\n",
    "    #set the nabla functions to be the functions themselves initially, same size\n",
    "    nb = [np.zeros(b.shape) for b in biases]\n",
    "    nw = [np.zeros(w.shape) for w in weights]\n",
    "    #largely taken from Michael Nielsen's implementation\n",
    "    for i, r in minibatch:\n",
    "        dnb, dnw = backprop(i, r)\n",
    "        nb = [a+b for a, b in zip(nb, dnb)]\n",
    "        nw = [a+b for a, b in zip(nw, dnw)]\n",
    "    \n",
    "    print(weights[0][0])\n",
    "    \n",
    "    #not changing?\n",
    "    weights = [w-(lc/len(minibatch))*nw_ for w, nw_ in zip(weights, nw)]\n",
    "    biases = [b-(lc/len(minibatch))*nb_ for b, nb_ in zip(biases, nb)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backprop(inputs, answers):\n",
    "    #set the nabla functions to be the functions themselves initially, same size\n",
    "    nb = [np.zeros(b.shape) for b in biases]\n",
    "    nw = [np.zeros(w.shape) for w in weights]\n",
    "    a = inputs\n",
    "    alist = [inputs]\n",
    "    zlist = []\n",
    "    #from feedforward\n",
    "    for n in range(1, len(size)):\n",
    "        #all other elements depend on the number of neurons\n",
    "        alist.append(np.zeros(size[n]))\n",
    "        zlist.append(np.zeros(size[n]))\n",
    "        for n2 in range(0, size[n]):\n",
    "            alist[n][n2] = np.dot(weights[n-1][0:,n2], alist[n-1]) + biases[n-1][n2]\n",
    "            zlist[n-1][n2] = sigmoid_v(alist[n][n2])\n",
    "    \n",
    "    delta = costderivative(alist[-1], answers) * sigmoidprime_v(zlist[-1])\n",
    "    nb[-1] = delta\n",
    "    nw[-1] = np.dot(delta, alist[-1].transpose())\n",
    "    \n",
    "    for n in range(2, len(size)):\n",
    "        delta = np.dot(weights[-n+1], delta) * sigmoidprime_v(zlist[-n])\n",
    "        nb[-n] = delta\n",
    "        nw[-n] = np.dot(delta, alist[-n].transpose())\n",
    "    \n",
    "    return (nb, nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def costderivative(output, answers):\n",
    "    return (output - answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(inputs, results, answers):\n",
    "    correct = 0\n",
    "    binresults = results\n",
    "    for n in range(0, len(results)):\n",
    "        #converts the output into a binary y/n for each digit\n",
    "        for n2 in range(len(results[n])):\n",
    "            if results[n][n2] == np.amax(results[n]):\n",
    "                binresults[n][n2] = 1\n",
    "            else:\n",
    "                binresults[n][n2] = 0\n",
    "        \n",
    "        if np.array_equal(answers[n], binresults[n]):\n",
    "            correct += 1\n",
    "    return correct / len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingdata = digits.data[0:100]\n",
    "traininganswers = digits.target[0:100]\n",
    "\n",
    "traininganswervectors = np.zeros((100,10))\n",
    "for n in range(100):\n",
    "    traininganswervectors[n][digits.target[n]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "Epoch  0  :  0.09\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "Epoch  1  :  0.09\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "Epoch  2  :  0.09\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "Epoch  3  :  0.09\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "Epoch  4  :  0.09\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "Epoch  5  :  0.09\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "Epoch  6  :  0.09\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "Epoch  7  :  0.09\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "Epoch  8  :  0.09\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "[ 0.87299774 -0.25864325 -0.55462275 -0.09420273 -0.84490824  0.6782452\n",
      "  0.05929519 -0.66297401  0.99509137  0.31726547  0.13995765  0.77248254\n",
      " -0.38440848 -0.27380042 -0.54618856  0.26302363 -0.3260963  -0.11028518\n",
      "  0.36211248  0.75764216]\n",
      "Epoch  9  :  0.09\n"
     ]
    }
   ],
   "source": [
    "GradientDescent(trainingdata, traininganswervectors, weights, biases, 5, 0.1, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
