{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPI and cluster computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is large-scale and cluster computing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MPI: the message passing interface (`mpi4py` and ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GPU: graphics processing-based parallelism (`pyopencl` and `pycuda`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cloud computing (`cloud`, `mrjob`, and Apache's `mesos`/`spark`/`hadoop`/`zookeeper`/...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll focus on `mpi4py`, as it's probably the most stable and active of the python MPI modules, and generally provides the most in terms of classic scalability to instutional class resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting started with `mpi4py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Typically not an `easy_install` -- thanks MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Getting started: say `\"hello\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hellompi.py\n"
     ]
    }
   ],
   "source": [
    "%%file hellompi.py\n",
    "\"\"\"\n",
    "Parallel Hello World\n",
    "\"\"\"\n",
    "\n",
    "from mpi4py import MPI\n",
    "import sys\n",
    "\n",
    "size = MPI.COMM_WORLD.Get_size()\n",
    "rank = MPI.COMM_WORLD.Get_rank()\n",
    "name = MPI.Get_processor_name()\n",
    "\n",
    "sys.stdout.write(\n",
    "    \"Hello, World! I am process %d of %d on %s.\\n\" \n",
    "    % (rank, size, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Executes with `mpiexec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World! I am process 2 of 4 on hilbert.local.\r\n",
      "Hello, World! I am process 1 of 4 on hilbert.local.\r\n",
      "Hello, World! I am process 0 of 4 on hilbert.local.\r\n",
      "Hello, World! I am process 3 of 4 on hilbert.local.\r\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -n 4 python2.7 hellompi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Coding for multiple \"personalities\" (nodes, actually)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point to point communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mpipt2pt.py\n"
     ]
    }
   ],
   "source": [
    "%%file mpipt2pt.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank, size = comm.Get_rank(), comm.Get_size()\n",
    "\n",
    "if rank == 0:\n",
    "    data = range(10)\n",
    "    more = range(0,20,2)\n",
    "    print 'rank %i sends data:' % rank, data\n",
    "    comm.send(data, dest=1, tag=1337)\n",
    "    print 'rank %i sends data:' % rank, more\n",
    "    comm.send(more, dest=2 ,tag=1456)\n",
    "elif rank == 1:\n",
    "    data = comm.recv(source=0, tag=1337)\n",
    "    print 'rank %i got data:' % rank, data\n",
    "elif rank == 2:\n",
    "    more = comm.recv(source=0, tag=1456)\n",
    "    print 'rank %i got data:' % rank, more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank 0 sends data: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\n",
      "rank 0 sends data: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\r\n",
      "rank 2 got data: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\r\n",
      "rank 1 got data: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\r\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -n 4 python2.7 mpipt2pt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mpipt2pt2.py\n"
     ]
    }
   ],
   "source": [
    "%%file mpipt2pt2.py\n",
    "'''nonblocking communication\n",
    "'''\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank, size = comm.Get_rank(), comm.Get_size()\n",
    "\n",
    "pair = {0:1, 1:0} # rank 0 sends to 1 and vice versa\n",
    "sendbuf = np.zeros(5) + rank\n",
    "recvbuf = np.empty_like(sendbuf)\n",
    "\n",
    "print 'rank %i sends data:' % rank, sendbuf\n",
    "sreq = comm.Isend(sendbuf, dest=pair[rank], tag=1337)\n",
    "rreq = comm.Irecv(recvbuf, source=pair[rank], tag=1337)\n",
    "\n",
    "# rreq.Wait(); sreq.Wait()\n",
    "MPI.Request.Waitall([rreq, sreq])\n",
    "if rank == 1: \n",
    "    time.sleep(0.001) # delay slightly for better printing\n",
    "print 'rank %i got data:' % rank, recvbuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank 1 sends data: [ 1.  1.  1.  1.  1.]\r\n",
      "rank 0 sends data: [ 0.  0.  0.  0.  0.]\r\n",
      "rank 0 got data: [ 1.  1.  1.  1.  1.]\r\n",
      "rank 1 got data: [ 0.  0.  0.  0.  0.]\r\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -n 2 python2.7 mpipt2pt2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collective communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mpiscattered.py\n"
     ]
    }
   ],
   "source": [
    "%%file mpiscattered.py\n",
    "'''mpi scatter\n",
    "'''\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank, size = comm.Get_rank(), comm.Get_size()\n",
    "\n",
    "if rank == 0:\n",
    "    data = np.arange(10)\n",
    "    print 'rank %i has data' % rank, data\n",
    "    data_split_list = np.array_split(data, size)\n",
    "else:\n",
    "    data_split_list = None\n",
    "data_split = comm.scatter(data_split_list, root=0)\n",
    "\n",
    "# some delays for printing purposes\n",
    "if rank == 1:\n",
    "    time.sleep(0.001)\n",
    "elif rank == 2:\n",
    "    time.sleep(0.002)\n",
    "print 'rank %i got data' % rank, data_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank 0 has data [0 1 2 3 4 5 6 7 8 9]\r\n",
      "rank 0 got data [0 1 2 3]\r\n",
      "rank 1 got data [4 5 6]\r\n",
      "rank 2 got data [7 8 9]\r\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -n 3 python2.7 mpiscattered.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mpibroadcasted.py\n"
     ]
    }
   ],
   "source": [
    "%%file mpibroadcasted.py\n",
    "'''mpi broadcast\n",
    "'''\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank, size = comm.Get_rank(), comm.Get_size()\n",
    "\n",
    "N = 10.\n",
    "data = np.arange(N) if rank == 0 else np.zeros(N)\n",
    "if rank == 1:\n",
    "    time.sleep(0.001)\n",
    "elif rank == 2:\n",
    "    time.sleep(0.002)\n",
    "print 'rank %i has data' % rank, data\n",
    "\n",
    "comm.Bcast(data, root=0)\n",
    "if rank == 1:\n",
    "    time.sleep(0.001)\n",
    "elif rank == 2:\n",
    "    time.sleep(0.002)\n",
    "print 'rank %i got data' % rank, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank 1 has data [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n",
      "rank 2 has data [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\r\n",
      "rank 0 has data [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9.]\r\n",
      "rank 0 got data [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9.]\r\n",
      "rank 1 got data [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9.]\r\n",
      "rank 2 got data [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9.]\r\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -n 3 python2.7 mpibroadcasted.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not covered: shared memory and shared objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "\n",
    "try:\n",
    "    import dill\n",
    "    MPI._p_pickle.dumps = dill.dumps\n",
    "    MPI._p_pickle.loads = dill.loads\n",
    "except ImportError, AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with cluster schedulers, the `JOB ` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing jobscript.sh\n"
     ]
    }
   ],
   "source": [
    "%%file jobscript.sh\n",
    "#!/bin/sh\n",
    "#PBS -l nodes=1:ppn=4\n",
    "#PBS -l walltime=00:03:00\n",
    "cd ${PBS_O_WORKDIR} || exit 2\n",
    "mpiexec -np 4 python hellompi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond `mpi4py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The task `Pool`: `pyina` and `emcee.utils`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pyinapool.py\n"
     ]
    }
   ],
   "source": [
    "%%file pyinapool.py\n",
    "\n",
    "def test_pool(obj):\n",
    "    from pyina.launchers import Mpi\n",
    "    x = range(6)\n",
    "    p = Mpi(8)\n",
    "    \n",
    "    # worker pool strategy + dill\n",
    "    p.scatter = False\n",
    "    print p.map(obj, x)\n",
    "    \n",
    "    # worker pool strategy + dill.source \n",
    "    p.source = True\n",
    "    print p.map(obj, x)\n",
    "    \n",
    "    # scatter-gather strategy + dill.source\n",
    "    p.scatter = True\n",
    "    print p.map(obj, x)\n",
    "    \n",
    "    # scatter-gather strategy + dill\n",
    "    p.source = False\n",
    "    print p.map(obj, x)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    from math import sin\n",
    "    f = lambda x:x+1\n",
    "    def g(x):\n",
    "        return x+2\n",
    "\n",
    "    for func in [g, f, abs, sin]:\n",
    "        test_pool(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7]\n",
      "[2, 3, 4, 5, 6, 7]\n",
      "[2, 3, 4, 5, 6, 7]\n",
      "[2, 3, 4, 5, 6, 7]\n",
      "[1, 2, 3, 4, 5, 6]\n",
      "[1, 2, 3, 4, 5, 6]\n",
      "[1, 2, 3, 4, 5, 6]\n",
      "[1, 2, 3, 4, 5, 6]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "[0, 1, 2, 3, 4, 5]\n",
      "[0.0, 0.8414709848078965, 0.9092974268256817, 0.1411200080598672, -0.7568024953079282, -0.9589242746631385]\n",
      "[0.0, 0.8414709848078965, 0.9092974268256817, 0.1411200080598672, -0.7568024953079282, -0.9589242746631385]\n",
      "[0.0, 0.8414709848078965, 0.9092974268256817, 0.1411200080598672, -0.7568024953079282, -0.9589242746631385]\n",
      "[0.0, 0.8414709848078965, 0.9092974268256817, 0.1411200080598672, -0.7568024953079282, -0.9589242746631385]\n"
     ]
    }
   ],
   "source": [
    "!python2.7 pyinapool.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For emcee, see: http://dan.iel.fm/emcee/current/user/advanced/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Loooooong import times: `MPI_import`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Interactive MPI: `pyina` and `IPython.parallel`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ ipcluster mpiexec -n 16 --mpi=mpi4py"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from IPython.kernel import client\n",
    "mec = client.MultiEngineClient()\n",
    "mec.activate()\n",
    "\n",
    "%px from mpi4py import MPI\n",
    "%px print(MPI.Get_processor_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Working with schedulers directly: `pyina` and `ipython-cluster-helper`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pyina.launchers import SerialMapper\n",
    "from pyina.schedulers import Torque\n",
    "from pyina.mpi import _save, _debug\n",
    "\n",
    "def host(id):\n",
    "    import socket\n",
    "    return \"Rank: %d -- %s\" % (id, socket.gethostname())\n",
    "\n",
    "print \"Submit a non-parallel job to torque in the 'productionQ' queue...\"\n",
    "print \"Using 5 items over 1 nodes and the default mapping strategy\"\n",
    "\n",
    "torque = Torque(queue='productionQ', timelimit='20:00:00', workdir='.')\n",
    "pool = SerialMapper(scheduler=torque) \n",
    "res = pool.map(host, range(5))\n",
    "\n",
    "print pool\n",
    "print '\\n'.join(res)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pyina.launchers import Mpi\n",
    "from pyina.schedulers import Torque\n",
    "from pyina.mpi import _save, _debug\n",
    "\n",
    "def host(id):\n",
    "    import socket\n",
    "    return \"Rank: %d -- %s\" % (id, socket.gethostname())\n",
    "\n",
    "print \"Submit an mpi job to torque in the 'productionQ' queue...\"\n",
    "print \"Using 15 items over 5 nodes and the scatter-gather strategy\"\n",
    "\n",
    "torque = Torque('5:ppn=2', queue='productionQ', timelimit='20:00:00', workdir='.')\n",
    "pool = Mpi(scheduler=torque, scatter=True)\n",
    "res = pool.map(host, range(15))\n",
    "\n",
    "print pool\n",
    "print '\\n'.join(res)\n",
    "\n",
    "print \"hello from master\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Issue: conforming to the `multiprocessing` interface: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are in the tall weeds here... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other end of the spectrum is [high-performance parallel](Untitled6.ipynb) instead of large-scale parallel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
