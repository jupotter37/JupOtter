{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-05T21:18:23.076458Z",
     "start_time": "2017-05-05T21:18:23.069128Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#version vx3\n",
    "\n",
    "## new to this version\n",
    "# optimize lr\n",
    "\n",
    "## already incode\n",
    "# selected features 11\n",
    "# STD>3 train entry removed\n",
    "# keras.layers.merge.Concatenate(axis=-1)\n",
    "\n",
    "## already tried\n",
    "# \n",
    "\n",
    "## other options\n",
    "# optimizers, batch size, \n",
    "# dropout layer, batchnorm, dense (layers/nuron per) numbers\n",
    "\n",
    "## future\n",
    "# remove column with <.2 variance\n",
    "# try individual feature then try combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-05T21:18:23.875544Z",
     "start_time": "2017-05-05T21:18:23.868917Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#0.40094 <- \n",
    "#0.39138 <- # real,elim std>3, # 12-300-30-10-1 # test_vx1 #epoch 60/200 0s - loss: 0.3677 - val_loss: 0.2493\n",
    "#0.40566 <- # real,elim std>3, # 12-300-30-10-1 # test_v1_ext #epoch 72/200 0s - loss: 0.5543 - val_loss: 0.2496\n",
    "#0.48500 <- elim std>3, 12-300-30-10-1\n",
    "#0.52512 <- Epoch 11/200  16s - loss: 0.4993 - val_loss: 0.3509\n",
    "#0.38798 <- 12-300-30-5-1\n",
    "#0.41785<- 9col grp1 loss: 0.3123 - acc: 0.0000e+00 - val_loss: 0.2602\n",
    "#0.43247 <-3col \n",
    "#0.53085 <- adadelta\n",
    "#0.50647 <-decay\n",
    "#0.48978 <-rmsle\n",
    "\n",
    "#task\n",
    "# feature extraction\n",
    "# feature selection\n",
    "# optimizaer selection , lr , decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-05T21:18:27.633249Z",
     "start_time": "2017-05-05T21:18:25.108180Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20170505-211827\n"
     ]
    }
   ],
   "source": [
    "### importing libraries\n",
    "%matplotlib inline\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, Convolution1D\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD,RMSprop\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Concatenate,Merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "#mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "print(timestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-05T21:18:27.647452Z",
     "start_time": "2017-05-05T21:18:27.643526Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df_train = pd.read_csv(\"../input/train.csv\", parse_dates=['timestamp'])\n",
    "    df_test = pd.read_csv(\"../input/test.csv\", parse_dates=['timestamp'])\n",
    "    df_macro = pd.read_csv(\"../input/macro.csv\", parse_dates=['timestamp'])\n",
    "    return [df_train,df_test,df_macro]\n",
    "#df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-05T21:18:28.487346Z",
     "start_time": "2017-05-05T21:18:28.483813Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print( df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-05T21:18:29.656605Z",
     "start_time": "2017-05-05T21:18:29.306375Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9711c9ffc3dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgr1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgr1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "#selected features\n",
    "\n",
    "#gr1=[\"full_sq\", \"life_sq\", \"floor\", \"max_floor\", \"material\", \n",
    "#\"build_year\",\"num_room\", \"kitch_sq\",\"state\",\"radiation_km\",\n",
    "#\"green_zone_km\",\"railroad_km\", \"public_transport_station_km\",\"metro_min_avto\" ] \n",
    "#\"kindergarden_km\" \n",
    "#\"sub_area\" = object, str\n",
    "\n",
    "#gr1=[\"full_sq\"] #elbo 10 epo  26/200 0s - loss: 0.3472 - val_loss: 0.2767\n",
    "#gr1=[\"life_sq\"] #     20 epo 163/200 0s - loss: 0.4144 - val_loss: 0.3150\n",
    "#gr1=[\"floor\"]   #     55 epo  83/200 0s - loss: 0.4261 - val_loss: 0.3554\n",
    "\n",
    "\n",
    "#gr1 = list(set(gr1))\n",
    "#print(len(gr1))\n",
    "#for c in gr1:\n",
    "#    print (c+\"\\t\"+str(df_train[c].dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-05T21:18:54.775895Z",
     "start_time": "2017-05-05T21:18:54.765433Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This section added:  drop crazy data point\n",
    "def drop_crazy_data_point(clist,df_train):\n",
    "    '''drop_crazy_data_point, STD>3\n",
    "    clist = columnlist to use\n",
    "    df_train = pandas dataframe\n",
    "    '''\n",
    "    \n",
    "    #clist = gr1#['life_sq','floor']\n",
    "    for cname in clist:\n",
    "        df_train.drop(df_train[ np.abs((df_train[cname]-df_train[cname].mean())/df_train[cname].std())>3].index, inplace=True)\n",
    "    print('shape after drop_crazy_data_point:')\n",
    "    print( df_train.shape)\n",
    "    return df_train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-05T21:18:56.333201Z",
     "start_time": "2017-05-05T21:18:56.287526Z"
    },
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def take_ytrain_testid_traincnt(df_train,df_test):\n",
    "    y_train = df_train['price_doc'].values\n",
    "    id_test = df_test['id']\n",
    "\n",
    "    df_train.drop(['id', 'price_doc'], axis=1, inplace=True)\n",
    "    df_test.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "    num_train = len(df_train)\n",
    "    df_all = pd.concat([df_train, df_test])\n",
    "    df_all = df_all.join(df_macro, on='timestamp', rsuffix='_macro')\n",
    "    print(df_all.shape)\n",
    "    \n",
    "\n",
    "    # Add month-year\n",
    "    month_year = (df_all.timestamp.dt.month + df_all.timestamp.dt.year * 100)\n",
    "    month_year_cnt_map = month_year.value_counts().to_dict()\n",
    "    df_all['month_year_cnt'] = month_year.map(month_year_cnt_map)\n",
    "\n",
    "    # Add week-year count\n",
    "    week_year = (df_all.timestamp.dt.weekofyear + df_all.timestamp.dt.year * 100)\n",
    "    week_year_cnt_map = week_year.value_counts().to_dict()\n",
    "    df_all['week_year_cnt'] = week_year.map(week_year_cnt_map)\n",
    "\n",
    "    # Add month and day-of-week\n",
    "    df_all['month'] = df_all.timestamp.dt.month\n",
    "    df_all['dow'] = df_all.timestamp.dt.dayofweek\n",
    "\n",
    "    # Other feature engineering\n",
    "    #df_all['rel_floor'] = df_all['floor'] / df_all['max_floor'].astype(float)\n",
    "    #df_all['rel_kitch_sq'] = df_all['kitch_sq'] / df_all['full_sq'].astype(float)\n",
    "\n",
    "    # Remove timestamp column (may overfit the model in train)\n",
    "    df_all.drop(['timestamp', 'timestamp_macro'], axis=1, inplace=True)\n",
    "\n",
    "    return [y_train,id_test,num_train,df_all]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-05T21:18:57.758652Z",
     "start_time": "2017-05-05T21:18:57.696349Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def factor(df_all,num_train,gr1,gr2):\n",
    "    factorize = lambda t: pd.factorize(t[1])[0]\n",
    "\n",
    "    df_obj = df_all.select_dtypes(include=['object'])\n",
    "\n",
    "    X_all = np.c_[\n",
    "        df_all.select_dtypes(exclude=['object']).values,\n",
    "        np.array(list(map(factorize, df_obj.iteritems()))).T\n",
    "    ]\n",
    "    print(X_all.shape)\n",
    "\n",
    "    X_train = X_all[:num_train]\n",
    "    X_test = X_all[num_train:]\n",
    "\n",
    "    # Deal with categorical values\n",
    "    df_numeric = df_all.select_dtypes(exclude=['object'])\n",
    "    df_obj = df_all.select_dtypes(include=['object']).copy()\n",
    "\n",
    "    for c in df_obj:\n",
    "        df_obj[c] = pd.factorize(df_obj[c])[0]\n",
    "\n",
    "    df_values = pd.concat([df_numeric, df_obj], axis=1)\n",
    "\n",
    "    df_values=df_values.fillna(df_values.mean())\n",
    "    df_values=df_values.dropna(axis=\"columns\", how='all')\n",
    "\n",
    "    #df_values.drop(['area_m','ID_metro'], axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "    df_values1=df_values[gr1]\n",
    "    df_values2=df_values[gr2]\n",
    "    \n",
    "    #df_values.mean()\n",
    "    #df_values.shape\n",
    "    \n",
    "    # Convert to numpy values\n",
    "    X_all1 = df_values1.values\n",
    "    X_all2 = df_values2.values\n",
    "    #print(X_all1.shape)\n",
    "\n",
    "    X_train1 = X_all1[:num_train]\n",
    "    X_train2 = X_all2[:num_train]\n",
    "    \n",
    "    X_test1 = X_all1[num_train:]\n",
    "    X_test2 = X_all2[num_train:]\n",
    "\n",
    "    #df_columns = df_values.columns.tolist\n",
    "    #df_columns\n",
    "\n",
    "    return [X_train1,X_train2,X_test1,X_test2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-05T21:18:59.235972Z",
     "start_time": "2017-05-05T21:18:59.231593Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for c in df_values.columns:\n",
    "#    if df_values[c].dtype == 'object':\n",
    "#        lbl = preprocessing.LabelEncoder()\n",
    "#        lbl.fit(list(df_values[c].values)) \n",
    "#        df_values[c] = lbl.transform(list(df_values[c].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-05T21:18:59.995918Z",
     "start_time": "2017-05-05T21:18:59.993521Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#c = df_values.columns[4]\n",
    "#df_values[c]\n",
    "#c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-05T21:19:00.672944Z",
     "start_time": "2017-05-05T21:19:00.669354Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_all.tofile(\"x_all.csv\",format=\"%s\",sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-05T21:19:01.309313Z",
     "start_time": "2017-05-05T21:19:01.305639Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_valuesclean=df_values.dropna(axis=\"columns\", how='any')\n",
    "#df_valuesclean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-05T21:19:11.590062Z",
     "start_time": "2017-05-05T21:19:11.548377Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.merge import Concatenate\n",
    "# define base model\n",
    "def baseline_model(df_values1_cols,df_values2_cols):\n",
    "    \n",
    "    model1 = Sequential()\n",
    "    model1.add(Dense(30, input_dim=df_values1_cols, activation='relu'))\n",
    "    #model1.add(BatchNormalization())\n",
    "    #model1.add(Dropout(0.5))\n",
    "    #model1.add(Dense(60,activation=\"relu\"))\n",
    "    #model1.add(Dropout(0.25))\n",
    "    model1.add(Dense(15,activation=\"relu\"))\n",
    "    #model1.add(Dropout(0.25))\n",
    "    \n",
    "    model2 = Sequential()\n",
    "    model2.add(Dense(30, input_dim=df_values2_cols, activation='relu'))\n",
    "    #model1.add(BatchNormalization())\n",
    "    #model2.add(Dropout(0.5))\n",
    "    #model2.add(Dense(60,activation=\"relu\"))\n",
    "    #model2.add(Dropout(0.5))\n",
    "    model2.add(Dense(15,activation=\"relu\"))\n",
    "    #model2.add(Dropout(0.25))\n",
    "    \n",
    "    model = Sequential()\n",
    "    #model.add(Concatenate([model1, model2],input_shape=(120,1)))\n",
    "    model.add(Merge([model1, model2], mode='concat'))\n",
    "    model.add(Dense(100,activation=\"relu\"))\n",
    "    #model.add(Dropout(0.25))\n",
    "    model.add(Dense(30,activation=\"relu\"))\n",
    "    model.add(Dense(1,activation=\"relu\"))\n",
    "    # Compile model\n",
    "    #sgd=SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     RMSProp optimizer.\n",
    "\n",
    "# It is recommended to leave the parameters of this optimizer at their default values (except the learning rate, which can be freely tuned).\n",
    "\n",
    "# This optimizer is usually a good choice for recurrent neural networks.\n",
    "\n",
    "# Arguments\n",
    "\n",
    "# lr: float >= 0. Learning rate.\n",
    "# rho: float >= 0.\n",
    "# epsilon: float >= 0. Fuzz factor.\n",
    "# decay: float >= 0. Learning rate decay over each update\n",
    "    \n",
    "    #default optimizer\n",
    "    #model.compile(loss='mean_squared_logarithmic_error', \\\n",
    "    #              optimizer=RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0))\n",
    "    \n",
    "    #lowering lr\n",
    "    model.compile(loss='mean_squared_logarithmic_error', \\\n",
    "                  optimizer=RMSprop(lr=0.00001, rho=0.9, epsilon=1e-08, decay=0.0))\n",
    "    \n",
    "    #my model\n",
    "    #model.compile(loss='mean_squared_logarithmic_error', optimizer=RMSprop(decay=0.0001))\n",
    "        #Adadelta\n",
    "        #sgd\n",
    "    return [model1,model2,model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-05T21:06:01.686174Z",
     "start_time": "2017-05-05T21:06:00.379348Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[df_train,df_test,df_macro]=load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-05T21:06:01.988976Z",
     "start_time": "2017-05-05T21:06:01.900523Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 30)                480       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 15)                465       \n",
      "=================================================================\n",
      "Total params: 945\n",
      "Trainable params: 945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 30)                480       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 15)                465       \n",
      "=================================================================\n",
      "Total params: 945\n",
      "Trainable params: 945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_3 (Merge)              (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 100)               3100      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 30)                3030      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 8,051\n",
      "Trainable params: 8,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minesh/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:25: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "[model1,model2,model]=baseline_model(15,15)\n",
    "model1.summary()\n",
    "model2.summary()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-05T21:34:47.630759Z",
     "start_time": "2017-05-05T21:34:47.625299Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainKeras(model,X_train1,X_train2,y_train,epo):\n",
    "#model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])\n",
    "    print (\"^^^INFO: Fit Model^^^\")\n",
    "#X_train = X_train.reshape(X_train.shape[0],244,1)\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=50, verbose=2)\n",
    "    ]\n",
    "\n",
    "    history = model.fit(x=[X_train1,X_train2],y= y_train, epochs=epo, batch_size=780, validation_split=0.3, verbose=2)#,callbacks=callbacks) #verbose=2 )#\n",
    "    return [model,history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-05T21:30:28.429886Z",
     "start_time": "2017-05-05T21:30:28.197307Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQkFERCIKBRERiSgUREQk8v8B3A++lrzvCYQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7eff4c53b668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list all data in history\n",
    "'''\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "# axes = plt.gca()\n",
    "# axes.set_xlim([0,120])\n",
    "# axes.set_ylim([90,100])\n",
    "#plt.savefig('acc.png')  # save the figure to file\n",
    "plt.show()\n",
    "#plt.close()\n",
    "'''\n",
    "\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "#plt.savefig('loss.png')\n",
    "#plt.show()\n",
    "\n",
    "plt.ylim([0, \\\n",
    "          min(history.history['loss'])+min(history.history['loss'])/2])\n",
    "plt.show()\n",
    "\n",
    "#plt.fig()\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-04T22:42:51.225714Z",
     "start_time": "2017-05-04T22:42:51.184883Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-4788bc045bd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# summarize history for loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6c3b2c44e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "with PdfPages('multipage_pdf.pdf') as pdf:\n",
    "    for i in range(0,10):\n",
    "    # summarize history for loss\n",
    "        plt.figure()\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "        plt.ylim([0, 2])\n",
    "        #plt.show()\n",
    "        pdf.savefig()  # saves the current figure into a pdf page\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-05T21:31:10.503507Z",
     "start_time": "2017-05-05T21:31:10.313678Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiJSFAWGiIgURYEhIiJF+f/R7kMdpa0UVwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7eff4c53c2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.ylim([0, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-04T22:42:56.880710Z",
     "start_time": "2017-05-04T22:42:56.876740Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filePath=\"myfile.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-05T21:19:36.478009Z",
     "start_time": "2017-05-05T21:19:36.460288Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def writeToFile(history,filePath,columns):\n",
    "    fh=open(filePath,'a')\n",
    "    loss=history.history['loss']\n",
    "    loss_diff=np.diff(loss)\n",
    "    val_loss=history.history['val_loss']\n",
    "    val_loss_diff=np.diff(val_loss)\n",
    "\n",
    "    loss_Elbow=min(list((x for x in range(0,len(loss_diff.tolist())) if loss_diff.tolist()[x] > -0.01)))+1\n",
    "    val_loss_Elbow=min(list((x for x in range(0,len(val_loss_diff.tolist())) if val_loss_diff.tolist()[x] > -0.01)))+1\n",
    "\n",
    "    print(','.join(columns),\"\\tlE\",loss_Elbow,\"\\tvlE\",val_loss_Elbow,\"\\ttepo\",len(history.history['loss']),end='',file=fh)\n",
    "    print(\"\\tloss\\t{0:0.5f}\".format(history.history['loss'][-1]),\"\\tval_loss\\t{0:0.5f}\".format(history.history['val_loss'][-1]),file=fh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-04T22:43:03.008451Z",
     "start_time": "2017-05-04T22:43:02.986797Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-04bf8de6f0a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwriteToFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilePath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "writeToFile(history,filePath,columns=['1','2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-04T22:43:07.208900Z",
     "start_time": "2017-05-04T22:43:07.204533Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#print(\"loss\\t{0:0.5f}\".format(history.history['loss'][-1]),\"\\tval_loss\\t{0:0.5f}\".format(history.history['val_loss'][-1]))\n",
    "\n",
    "#print(\"%0.3f \\tval_loss\\t %0.5f \" % history.history['loss'][-1],history.history['val_loss'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-05-05T12:38:31.406Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------> ['num_room', 'life_sq', 'metro_min_walk', 'nuclear_reactor_km', 'ttk_km', 'zd_vokzaly_avto_km', 'sadovoe_km', 'bulvar_ring_km', 'kremlin_km', 'stadium_km', 'basketball_km', 'ID_railroad_station_walk', 'build_count_slag', 'build_count_panel', 'prom_part_3000', 'build_count_frame', 'build_count_before_1920', 'indust_part', 'raion_build_count_with_material_info', 'build_count_1971-1995', 'green_zone_km', 'railroad_km', 'public_transport_station_km', 'metro_min_avto', 'full_sq', 'life_sq', 'floor', 'max_floor', 'material', 'build_year', 'num_room', 'kitch_sq', 'state', 'radiation_km', 'life_sq', 'big_church_km', 'metro_km_walk', 'public_healthcare_km', 'kremlin_km', 'fitness_km', 'market_shop_km', 'basketball_km', 'zd_vokzaly_avto_km', 'green_part_1500']\n",
      "=> loading data:\n",
      "=> drop_crazy_data_point:\n",
      "shape after drop_crazy_data_point:\n",
      "(21836, 292)\n",
      "=> prepare data:\n",
      "(29498, 390)\n",
      "=> handle factors\n",
      "(29498, 392)\n",
      "=> model arch\n",
      "=> train\n",
      "^^^INFO: Fit Model^^^\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minesh/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:25: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15285 samples, validate on 6551 samples\n",
      "Epoch 1/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 2/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 3/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 4/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 5/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 6/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 7/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 8/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 9/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 10/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 11/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 12/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 13/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 14/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 15/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 16/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 17/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 18/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 19/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 20/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 21/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 22/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 23/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 24/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 25/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 26/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 27/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 28/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 29/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 30/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 31/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 32/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 33/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 34/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 35/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 36/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 37/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 38/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 39/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 40/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 41/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 42/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 43/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 44/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 45/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 46/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 47/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 48/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 49/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 50/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 51/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 52/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 53/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 54/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 55/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 56/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 57/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 58/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 59/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 60/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 61/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 62/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 63/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 64/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 65/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 66/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 67/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 68/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 69/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 70/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 71/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 72/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 73/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 74/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 75/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 76/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 77/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 78/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 79/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 80/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 81/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 82/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 83/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 84/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 85/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 86/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 87/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 88/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 89/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 90/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 91/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 92/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 93/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 94/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 95/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 96/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 97/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 98/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 99/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 100/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 101/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 102/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 103/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 104/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 105/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 106/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 107/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 108/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 109/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 110/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 111/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 112/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 113/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 114/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 115/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 116/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 117/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 118/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 119/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 120/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 121/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 122/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 123/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 124/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 125/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 126/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 127/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 128/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 129/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 130/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 131/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 132/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 133/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 134/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 135/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 136/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 137/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 138/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 139/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 140/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 141/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 142/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 143/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 144/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 145/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 146/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 147/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 148/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 149/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 150/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 151/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 152/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 153/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 154/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 155/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 156/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 157/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 158/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 159/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 160/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 161/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 162/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 163/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 164/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 165/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 166/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 167/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 168/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 169/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 170/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 171/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 172/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 173/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 174/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 175/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 176/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 177/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 178/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 179/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 180/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 181/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 182/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 183/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 184/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 185/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 186/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 187/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 188/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 189/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 190/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 191/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 192/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 193/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 194/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 195/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 196/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 197/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 198/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 199/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 200/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 201/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 202/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 203/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 204/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 205/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 206/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 207/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 208/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 209/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 210/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 211/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 212/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 213/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 214/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 215/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 216/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 217/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 218/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 219/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 220/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 221/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 222/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 223/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 224/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 225/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 226/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 227/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 228/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 229/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 230/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 231/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 232/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 233/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 234/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 235/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 236/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 237/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 238/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 239/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 240/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 241/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 242/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 243/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 244/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 245/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 246/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 247/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 248/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 249/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 250/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 251/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 252/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 253/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 254/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 255/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 256/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 257/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 258/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 259/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 260/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 261/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 262/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 263/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 264/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 265/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 266/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 267/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 268/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 269/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 270/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 271/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 272/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 273/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 274/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 275/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 276/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 277/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 278/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 279/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 280/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 281/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 282/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 283/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 284/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 285/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 286/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 287/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 288/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 289/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 290/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 291/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 292/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 293/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 294/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 295/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 296/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 297/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 298/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 299/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 300/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 301/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 302/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 303/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 304/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 305/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 306/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 307/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 308/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 309/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 310/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 311/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 312/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 313/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 314/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 315/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 316/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 317/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 318/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 319/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 320/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 321/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 322/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 323/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 324/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 325/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 326/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 327/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 328/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 329/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 330/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 331/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 332/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 333/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 334/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 335/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 336/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 337/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 338/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 339/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 340/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 341/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 342/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 343/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 344/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 345/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 346/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 347/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 348/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 349/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 350/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 351/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 352/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 353/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 354/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 355/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 356/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 357/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 358/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 359/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 360/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 361/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 362/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 363/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 364/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 365/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 366/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 367/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 368/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 369/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 370/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 371/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 372/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 373/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 374/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 375/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 376/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 377/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 378/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 379/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 380/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 381/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 382/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 383/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 384/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 385/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 386/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 387/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 388/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 389/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 390/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 391/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 392/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 393/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 394/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 395/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 396/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 397/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 398/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 399/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 400/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 401/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 402/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 403/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 404/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 405/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 406/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 407/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 408/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 409/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 410/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 411/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 412/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 413/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 414/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 415/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 416/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 417/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 418/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 419/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 420/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 421/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 422/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 423/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 424/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 425/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 426/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 427/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 428/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 429/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 430/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 431/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 432/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 433/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 434/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 435/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 436/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 437/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 438/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 439/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 440/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 441/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 442/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 443/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 444/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 445/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 446/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 447/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 448/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 449/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 450/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 451/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 452/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 453/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 454/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 455/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 456/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 457/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 458/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 459/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 460/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 461/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 462/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 463/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 464/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 465/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 466/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 467/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 468/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 469/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 470/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 471/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 472/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 473/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 474/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 475/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 476/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 477/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 478/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 479/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 480/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 481/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 482/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 483/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 484/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 485/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 486/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 487/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 488/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 489/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 490/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 491/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 492/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 493/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 494/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 495/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 496/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 497/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 498/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 499/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 500/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 501/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 502/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 503/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 504/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 505/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 506/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 507/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 508/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 509/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 510/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 511/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 512/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 513/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 514/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 515/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 516/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 517/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 518/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 519/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 520/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 521/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 522/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 523/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 524/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 525/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 526/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 527/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 528/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 529/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 530/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 531/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 532/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 533/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 534/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 535/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 536/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 537/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 538/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 539/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 540/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 541/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 542/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 543/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 544/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 545/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 546/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 547/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 548/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 549/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 550/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 551/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 552/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 553/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 554/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 555/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 556/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 557/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 558/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 559/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 560/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 561/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 562/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 563/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 564/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 565/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 566/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 567/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 568/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 569/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 570/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 571/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 572/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 573/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 574/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 575/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 576/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 577/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 578/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 579/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 580/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 581/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 582/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 583/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 584/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 585/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 586/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 587/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 588/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 589/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 590/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 591/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 592/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 593/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 594/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 595/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 596/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 597/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 598/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 599/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 600/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 601/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 602/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 603/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 604/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 605/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 606/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 607/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 608/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 609/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 610/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 611/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 612/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 613/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 614/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 615/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 616/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 617/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 618/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 619/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 620/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 621/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 622/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 623/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 624/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 625/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 626/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 627/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 628/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 629/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 630/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 631/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 632/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 633/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 634/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 635/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 636/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 637/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 638/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 639/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 640/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 641/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 642/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 643/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 644/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 645/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 646/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 647/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 648/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 649/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 650/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 651/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 652/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 653/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 654/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 655/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 656/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 657/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 658/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 659/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 660/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 661/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 662/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 663/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 664/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 665/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 666/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 667/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 668/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 669/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 670/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 671/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 672/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 673/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 674/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 675/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 676/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 677/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 678/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 679/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 680/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 681/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 682/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 683/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 684/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 685/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 686/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 687/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 688/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 689/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 690/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 691/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 692/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 693/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 694/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 695/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 696/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 697/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 698/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 699/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 700/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 701/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 702/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 703/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 704/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 705/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 706/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 707/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 708/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 709/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 710/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 711/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 712/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 713/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 714/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 715/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 716/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 717/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 718/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 719/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 720/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 721/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 722/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 723/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 724/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 725/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 726/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 727/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 728/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 729/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 730/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 731/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 732/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 733/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 734/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 735/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 736/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 737/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 738/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 739/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 740/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 741/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 742/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 743/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 744/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 745/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 746/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 747/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 748/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 749/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 750/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 751/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 752/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 753/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 754/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 755/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 756/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 757/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 758/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 759/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 760/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 761/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 762/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 763/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 764/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 765/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 766/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 767/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 768/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 769/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 770/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 771/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 772/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 773/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 774/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 775/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 776/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 777/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 778/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 779/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 780/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 781/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 782/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 783/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 784/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 785/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 786/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 787/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 788/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 789/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 790/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 791/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 792/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 793/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 794/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 795/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 796/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 797/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 798/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 799/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 800/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 801/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 802/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 803/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 804/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 805/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 806/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 807/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 808/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 809/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 810/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 811/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 812/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 813/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 814/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 815/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 816/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 817/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 818/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 819/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 820/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 821/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 822/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 823/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 824/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 825/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 826/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 827/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 828/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 829/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 830/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 831/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 832/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 833/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 834/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 835/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 836/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 837/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 838/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 839/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 840/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 841/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 842/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 843/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 844/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 845/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 846/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 847/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 848/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 849/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 850/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 851/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 852/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 853/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 854/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 855/7000\n",
      "0s - loss: 243.4148 - val_loss: 248.3149\n",
      "Epoch 856/7000\n"
     ]
    }
   ],
   "source": [
    "#looping for features\n",
    "gr4=[\"full_sq\", \"life_sq\", \"floor\", \"max_floor\", \"material\", \n",
    "\"build_year\",\"num_room\", \"kitch_sq\",\"state\",\"radiation_km\"]\n",
    "gr3 =[\"green_zone_km\",\"railroad_km\", \"public_transport_station_km\",\"metro_min_avto\" ] \n",
    "\n",
    "gr1=[\"num_room\", \"life_sq\", \"metro_min_walk\", \"nuclear_reactor_km\", \"ttk_km\", \"zd_vokzaly_avto_km\", \"sadovoe_km\", \"bulvar_ring_km\", \"kremlin_km\", \"stadium_km\"]\n",
    "gr2=[\"basketball_km\", \"ID_railroad_station_walk\", \"build_count_slag\", \"build_count_panel\", \"prom_part_3000\", \"build_count_frame\",\n",
    "     \"build_count_before_1920\", \"indust_part\", \"raion_build_count_with_material_info\",\n",
    "     \"build_count_1971-1995\"]\n",
    "\n",
    "gr1=gr1+gr2+gr3+gr4\n",
    "gr2=[\"life_sq\", \"big_church_km\", \"metro_km_walk\", \"public_healthcare_km\", \"kremlin_km\",\n",
    "     \"fitness_km\", \"market_shop_km\", \"basketball_km\", \"zd_vokzaly_avto_km\", \n",
    "     \"green_part_1500\"]\n",
    "\n",
    "clist=gr1+gr2\n",
    "print(\"--------->\",clist)\n",
    "print(\"=> loading data:\")\n",
    "[df_train,df_test,df_macro]=load_data()\n",
    "print(\"=> drop_crazy_data_point:\")\n",
    "df_train=drop_crazy_data_point(clist,df_train)\n",
    "print(\"=> prepare data:\")\n",
    "[y_train,id_test,num_train,df_all]=take_ytrain_testid_traincnt(df_train,df_test)\n",
    "print('=> handle factors')\n",
    "[X_train1,X_train2,X_test1,X_test2]=factor(df_all,num_train,gr1,gr2)\n",
    "print('=> model arch')\n",
    "[model1,model2,model]=baseline_model(X_train1.shape[1],X_train2.shape[1])\n",
    "print('=> train')\n",
    "[model,history]=trainKeras(model,X_train1,X_train2,y_train,epo=7000)\n",
    "print('=>print to file')\n",
    "writeToFile(history,filePath=\"try1_concat_lr.txt\",columns=clist)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-04T22:58:32.320089Z",
     "start_time": "2017-05-04T22:58:32.314386Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 43.        ,  27.        ,   4.        , ...,   1.30515949,\n",
       "          0.27498514,   2.5902411 ],\n",
       "       [ 34.        ,  19.        ,   3.        , ...,   0.69453573,\n",
       "          0.06526334,   0.93669973],\n",
       "       [ 43.        ,  29.        ,   2.        , ...,   0.70069112,\n",
       "          0.32875604,   2.1209989 ],\n",
       "       ..., \n",
       "       [ 45.        ,  32.99267568,  10.        , ...,   1.72722333,\n",
       "          0.63001355,   2.15279162],\n",
       "       [ 64.        ,  32.        ,   5.        , ...,   4.89804719,\n",
       "          0.26152751,   3.37781368],\n",
       "       [ 43.        ,  28.        ,   1.        , ...,   0.7349489 ,\n",
       "          0.25015115,   0.58463601]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-03T01:53:15.725201Z",
     "start_time": "2017-05-03T01:53:15.721230Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_test = X_test.reshape(X_test.shape[0],244,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-05T21:31:29.537437Z",
     "start_time": "2017-05-05T21:31:29.353190Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test=model.predict([X_test1,X_test2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-05T21:31:33.708778Z",
     "start_time": "2017-05-05T21:31:33.702379Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7662,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-05T21:32:00.604353Z",
     "start_time": "2017-05-05T21:32:00.564063Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame({'id': id_test, 'price_doc': y_test[:,0]})\n",
    "\n",
    "df_sub.to_csv('sub 2017-04-30_rmsle_vx3_lr_4000epo.csv', index=False) \n",
    "\n",
    "# 0.49433\n",
    "# Epoch 4000/4000\n",
    "# 0s - loss: 0.3124 - val_loss: 0.3100\n",
    "\n",
    "\n",
    "#0.40908\n",
    "#120/120\n",
    "#0s - loss: 0.2793 - val_loss: 0.2215\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submitreal 12 elimstd>3 #0.40566"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "notify_time": "5",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 516,
   "position": {
    "height": "538px",
    "left": "1166px",
    "right": "20px",
    "top": "120px",
    "width": "335px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
