{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!rm -rf data/\n",
    "!unzip data.zip -d data\n",
    "from datascience import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to reproduce several findings from Ted Underwood and Jordan Sellers's article \"How Quickly Do Literary Standards Change?\" (draft (2015), forthcoming in <i>Modern Language Quarterly</i>). See especially Fig 1 and reported classifier accuracy (p 8).\n",
    "\n",
    "Underwood and Sellers have made their corpus of poems and their code available here: https://github.com/tedunderwood/paceofchange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Underwood and Sellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_tb = Table.read_table('data/poemeta.csv', keep_default_na=False)\n",
    "metadata_tb.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see above a `recept` column that has their reception status. We want to look at `reviewed` and `random`, just like Underwood and Sellers did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reception_mask = (metadata_tb['recept']=='reviewed') + (metadata_tb['recept']=='random')\n",
    "clf_tb = metadata_tb.where(reception_mask)\n",
    "clf_tb.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we've successfully subsetted our data!\n",
    "\n",
    "No we're going to import Term Frequencies by Text. This script is specifically tailored to the format of \n",
    "textual data made available from [Hathi Trust](https://www.hathitrust.org/). This consists of a series of spreadsheets, each containing book-level term frequencies.\n",
    "\n",
    "Each spreadsheet will become a row in our Document-Term Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create list that will contain a series of dictionaries\n",
    "freqdict_list = []\n",
    "\n",
    "# Iterate through texts in our spreadsheet\n",
    "for _id in clf_tb['docid']:\n",
    "\n",
    "    # Each text will have its own dictionary\n",
    "    # Keys are terms and values are frequencies\n",
    "    termfreq_dict = {}\n",
    "    \n",
    "    # Open the given text's spreadsheet\n",
    "    with open('data/poems/' + _id + '.poe.tsv', encoding='utf-8') as f:\n",
    "        filelines = f.readlines()\n",
    "        \n",
    "        # Each line in the spreadsheet contains a unique term and its frequency\n",
    "        for line in filelines:\n",
    "            termfreq = line.split('\\t')\n",
    "            \n",
    "            # 'If' conditions throw out junk lines in the spreadsheet\n",
    "            if len(termfreq) > 2 or len(termfreq) > 2:\n",
    "                continue\n",
    "            term, freq = termfreq[0], int(termfreq[1])\n",
    "            if len(term)>0 and term[0].isalpha():\n",
    "                \n",
    "                # Create new entry in text's dictionary for the term\n",
    "                termfreq_dict[term] = freq\n",
    "                \n",
    "    freqdict_list.append(termfreq_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `sklearn`'s `DictVectorizer` to turn this into a DTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "dv = DictVectorizer()\n",
    "dtm = dv.fit_transform(freqdict_list)\n",
    "term_list = dv.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Feature Selection, Training, Prediction\n",
    "\n",
    "We can put this DTM into a `pandas` dataframe, very similar to what you know from `Table`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_df = pd.DataFrame(dtm.toarray(), columns = term_list)\n",
    "dtm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add in the `docid` as the index instead of just a counter for the row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtm_df.set_index(clf_tb['docid'], inplace=True)\n",
    "dtm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Inputs and Outputs\n",
    "\n",
    "Underwood and Sellers create a unique model for each author in the corpus. They set aside a given author from the training set and then use the model to predict whether she was likely to be reviewed or not. Create a list of authors and an \"empty\" array in which to record probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = list(set(clf_tb['author']))\n",
    "probabilities = np.zeros([len(clf_tb['docid'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll set up the regression model with `sklearn`.\n",
    "\n",
    "Underwood and Sellers use a regularization constant ('C') to prevent overfitting, since this is a major concern when observing thousands of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(C = 0.00007)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then write a function `set_author_aside` that sifts out the target author's works from the dataset. Recall that Underwood and Sellers trained a model for *each* author when they were taken out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_author_aside(author, tb, df):\n",
    "    '''\n",
    "    Set aside each author's texts from training set\n",
    "    '''\n",
    "    train_ids = tb.where(tb['author']!=author).column('docid')\n",
    "    test_ids = tb.where(tb['author']==author).column('docid')\n",
    "    \n",
    "    train_df_ = df.loc[train_ids]\n",
    "    test_df_ = df.loc[test_ids]\n",
    "    \n",
    "    train_targets_ = tb.where(tb['author']!=author)['recept']=='reviewed'\n",
    "    \n",
    "    return train_df_, test_df_, train_targets_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need  to get out the most common words by their document frequency for each model. The function below, `top_vocab_by_docfreq` will do this each time we loop and create a new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_vocab_by_docfreq(df, num_words):\n",
    "    '''\n",
    "    Retrieve the most common words (by document frequency) for a given model\n",
    "    '''\n",
    "    docfreq_df = df > 0\n",
    "    wordcolumn_sums = docfreq_df.sum()\n",
    "    words_by_freq = wordcolumn_sums.sort_values(ascending=False)\n",
    "    top_words = words_by_freq[:num_words]\n",
    "    top_words_list = top_words.index.tolist()\n",
    "    \n",
    "    return top_words_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to normalize the frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_model(train_df_, test_df_, vocabulary):\n",
    "    '''\n",
    "    Normalize the model's term frequencies and put them into standard units\n",
    "    '''\n",
    "    # Select columns for only the most common words\n",
    "    train_df_ = train_df_[vocabulary]\n",
    "    test_df_ = test_df_[vocabulary]\n",
    "    \n",
    "    # Normalize each value by the sum of all values in its row\n",
    "    train_df_ = train_df_.apply(lambda x: x/sum(x), axis=1)\n",
    "    test_df_ = test_df_.apply(lambda x: x/sum(x), axis=1)\n",
    "    \n",
    "    # Get mean and stdev for each column\n",
    "    train_mean = np.mean(train_df_)\n",
    "    train_std = np.std(train_df_)\n",
    "\n",
    "    # Transform each value to standard units for its column\n",
    "    train_df_ = ( train_df_ - train_mean ) / train_std\n",
    "    test_df_ = ( test_df_ - train_mean ) / train_std\n",
    "    \n",
    "    return train_df_, test_df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Prediction\n",
    "\n",
    "The cell below will build a model for ***just 1*** author, let's see how long it takes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "for author in authors[:1]:\n",
    "    \n",
    "    # Set aside each author's texts from training set\n",
    "    train_df, test_df, train_targets = set_author_aside(author, clf_tb, dtm_df)\n",
    "\n",
    "    # Retrieve the most common words (by document frequency) for a given model\n",
    "    vocab_list = top_vocab_by_docfreq(train_df, 3200)\n",
    "    \n",
    "    # Normalize the model's term frequencies and put them into standard units\n",
    "    train_df, test_df = normalize_model(train_df, test_df, vocab_list)\n",
    "    \n",
    "    # Learn the Logistic Regression over our model\n",
    "    clf.fit(train_df, train_targets)\n",
    "    \n",
    "    # Some authors have more than one text in the corpus, so we retrieve all\n",
    "    for _id in test_df.index.tolist():\n",
    "        \n",
    "        # Make prediction whether text was reviewed\n",
    "        text = test_df.loc[_id]\n",
    "        probability = clf.predict_proba([text])[0][1]\n",
    "\n",
    "        # Record predictions in same order as the metadata spreadsheet\n",
    "        _index = list(clf_tb.column('docid')).index(_id)\n",
    "        probabilities[_index] = probability\n",
    "    \n",
    "        \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how long is this going to take us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(authors) * (end-start) / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have 100 minutes!\n",
    "\n",
    "# Efficiency\n",
    "\n",
    "A lot of that time is figuring out the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(term_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in a preprocessed vocabulary file. This contains only words that will be used in classification. This list was created by simply iterating through each model and observing the words that appeared in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/preprocessed_vocab.pickle', 'rb') as f:\n",
    "    pp_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pp_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's select only columns for words in our pre-processed vocabulary. This will make our computation more efficient later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_df = dtm_df[pp_vocab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the unique IDs from our metadata to keep track of each text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dtm_df.set_index(clf_tb['docid'], inplace=True)\n",
    "dtm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DataFrame that simply lists whether a term appears in\n",
    "# each document, so that we don't have to repeat this process evey iteration\n",
    "\n",
    "term_in_doc_df = dtm_df>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "term_in_doc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-write the model-building function\n",
    "\n",
    "def set_author_aside(author, tb, dtm_df_, dfreq_df_):\n",
    "    train_ids = tb.where(tb['author']!=author).column('docid')\n",
    "    test_ids = tb.where(tb['author']==author).column('docid')\n",
    "    \n",
    "    train_df_ = dtm_df_.loc[train_ids]\n",
    "    dfreq_df_ = dfreq_df_.loc[train_ids] # Include only term_in_doc values for texts in training set\n",
    "    test_df_ = dtm_df_.loc[test_ids]\n",
    "    \n",
    "    train_targets_ = tb.where(tb['author']!=author)['recept']=='reviewed'\n",
    "    \n",
    "    return train_df_, test_df_, train_targets_, dfreq_df_\n",
    "\n",
    "\n",
    "# Re-write our vocabulary selection function\n",
    "\n",
    "def top_vocab_by_docfreq(df, num_words):\n",
    "    # Removed the test of whether a term is in a given document (i.e. df>0)\n",
    "    wordcolumn_sums = sum(df)\n",
    "    words_by_freq = wordcolumn_sums.sort_values(ascending=False)\n",
    "    top_words = words_by_freq[:num_words]\n",
    "    top_words_list = top_words.index.tolist()\n",
    "    \n",
    "    return top_words_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel Processing means running our script on multiple cores simultaneously\n",
    "# This can be used in situations where we might otherwise use a 'FOR' loop\n",
    "# (when it doesn't matter what order we go through the list of values!)\n",
    "\n",
    "clf = LogisticRegression(C = 0.00007)\n",
    "\n",
    "def master_function(author):\n",
    "    # Note: Our only input is the name of the author.\n",
    "    # Remember that we had iterated over the list of authors previously.\n",
    "    train_df, test_df, train_targets, dfreq_df = set_author_aside(author, clf_tb, dtm_df, term_in_doc_df)\n",
    "    vocab_list = top_vocab_by_docfreq(dfreq_df, 3200)\n",
    "    train_df, test_df = normalize_model(train_df, test_df, vocab_list)\n",
    "    clf.fit(train_df, train_targets)\n",
    "    \n",
    "    # Create a list of each text's probability of review AND its index in the metadata table\n",
    "    index_probability_tuples = []\n",
    "    for _id in test_df.index.tolist():\n",
    "        text = test_df.loc[_id]\n",
    "        probability = clf.predict_proba([text])[0][1]\n",
    "        _index = list(clf_tb.column('docid')).index(_id)\n",
    "        index_probability_tuples.append( (_index, probability) )\n",
    "    return index_probability_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiprocessing enables Python to parallelize\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "# Return number of cores\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, the Pool contains one worker for each core\n",
    "\n",
    "# Since we are working on a shared server, we'll set the number\n",
    "# of workers to 4.\n",
    "\n",
    "pool = multiprocessing.Pool(4, maxtasksperchild=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiently applies the master_function() to our list of authors\n",
    "# Returns a list where each entry is an item returned by the function\n",
    "\n",
    "# Timing the process again\n",
    "start = time.time()\n",
    "\n",
    "output = pool.map(master_function, authors)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case, each element in output is itself a list ('index_probability_tuples'),\n",
    "# the length of which is the number of texts by a given author. We'll flatten it for\n",
    "# ease of use.\n",
    "\n",
    "flat_output = [tup  for lst in output  for tup in lst]\n",
    "flat_output[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the indices returned with the output to arrange probabilities properly\n",
    "\n",
    "probabilities = np.zeros([len(clf_tb['docid'])])\n",
    "for tup in flat_output:\n",
    "    probabilities[tup[0]] = tup[1]\n",
    "\n",
    "clf_tb['P(reviewed)'] = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_tb.select(['docid', 'firstpub','author', 'title', 'recept', 'P(reviewed)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the probability each text was reviewed\n",
    "\n",
    "colors = ['r' if recept=='reviewed' else 'b'  for recept in clf_tb['recept']]\n",
    "\n",
    "clf_tb.scatter('firstpub', 'P(reviewed)', c=colors, fit_line=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Does the Logistic Regression Model think its likely each book was reviewed?\n",
    "predictions = probabilities>0.5\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Creates array where '1' indicates a reviewed book and '0' indicates not\n",
    "targets = clf_tb['recept']=='reviewed'\n",
    "\n",
    "print(accuracy_score(predictions, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Often we prefer to evaluate accuracy based on the F1-score, which\n",
    "# weighs the number of times we correctly predicted reviewed texts against\n",
    "# the number of times we incorrectly predicted them as 'random'.\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(f1_score(predictions, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EX. Change the regularization parameter ('C') in our Logistic Regression function.\n",
    "##     How does this change the classifier's accuracy?\n",
    "\n",
    "## EX. Reduce the size of the vocabulary used for classification. How does accuracy change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Q. Are there cases when we might not want to set the classification threshold\n",
    "##     to 50% likelihood? How certain are we that 51% is different from a 49% probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model using full set of 'reviewed' and 'random' texts\n",
    "\n",
    "# Use this to predict the probability that other prestigious texts\n",
    "# (i.e. ones that we haven't trained on) might have been reviewed\n",
    "# ...if they had been published! The new texts include, for example,\n",
    "# Emily Dickinson and Gerard Manley Hopkins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run script from scratch\n",
    "\n",
    "%pylab inline\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "from datascience import *\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "corpus_path = 'poems/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read metadata from spreadsheet\n",
    "metadata_tb = Table.read_table('poemeta.csv', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll copy just our new texts into a separate table as well, for later\n",
    "canon_tb = metadata_tb.where('recept','addcanon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Term Frequencies from files\n",
    "\n",
    "freqdict_list = []\n",
    "\n",
    "# Iterate through texts in our spreadsheet\n",
    "for _id in metadata_tb['docid']:\n",
    "\n",
    "    # Each text will have its own dictionary\n",
    "    # Keys are terms and values are frequencies\n",
    "    termfreq_dict = {}\n",
    "    \n",
    "    # Open the given text's spreadsheet\n",
    "    with open(corpus_path+_id+'.poe.tsv', encoding='utf-8') as file_in:\n",
    "        filelines = file_in.readlines()\n",
    "        \n",
    "        # Each line in the spreadsheet contains a unique term and its frequency\n",
    "        for line in filelines:\n",
    "            termfreq = line.split('\\t')\n",
    "            \n",
    "            # 'If' conditions throw out junk lines in the spreadsheet\n",
    "            if len(termfreq) > 2 or len(termfreq) > 2:\n",
    "                continue\n",
    "            term, freq = termfreq[0], int(termfreq[1])\n",
    "            if len(term)>0 and term[0].isalpha():\n",
    "                \n",
    "                # Create new entry in text's dictionary for the term\n",
    "                termfreq_dict[term] = freq\n",
    "                \n",
    "    freqdict_list.append(termfreq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Document-Term-Matrix\n",
    "\n",
    "dv = DictVectorizer()\n",
    "dtm = dv.fit_transform(freqdict_list)\n",
    "term_list = dv.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place the DTM into a Pandas DataFrame for further manipulation\n",
    "\n",
    "dtm_df = pd.DataFrame(dtm.toarray(), columns = term_list)\n",
    "dtm_df.set_index(metadata_tb['docid'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are Feature Selection functions like the ones we originally defined,\n",
    "# not their efficiency minded counterparts, since we only train once\n",
    "\n",
    "\n",
    "# Set aside each canonic texts from training set\n",
    "\n",
    "def set_canon_aside(tb, df):\n",
    "    train_ids = tb.where(tb['recept']!='addcanon').column('docid')\n",
    "    classify_ids = tb.where(tb['recept']=='addcanon').column('docid')\n",
    "    \n",
    "    train_df_ = df.loc[train_ids]\n",
    "    classify_df_ = df.loc[classify_ids]\n",
    "    \n",
    "    train_targets_ = tb.where(tb['recept']!='addcanon')['recept']=='reviewed'\n",
    "    \n",
    "    return train_df_, classify_df_, train_targets_\n",
    "\n",
    "\n",
    "# Retrieve the most common words (by document frequency) for a given model\n",
    "\n",
    "def top_vocab_by_docfreq(df, num_words):\n",
    "    docfreq_df = df > 0\n",
    "    wordcolumn_sums = sum(docfreq_df)\n",
    "    words_by_freq = wordcolumn_sums.sort_values(ascending=False)\n",
    "    top_words = words_by_freq[:num_words]\n",
    "    top_words_list = top_words.index.tolist()\n",
    "    \n",
    "    return top_words_list\n",
    "\n",
    "\n",
    "# Normalize the model's term frequencies and put them into standard units\n",
    "\n",
    "def normalize_model(train_df_, classify_df_, vocabulary):\n",
    "    # Select columns for only the most common words\n",
    "    train_df_ = train_df_[vocabulary]\n",
    "    classify_df_ = classify_df_[vocabulary]\n",
    "    \n",
    "    # Normalize each value by the sum of all values in its row\n",
    "    train_df_ = train_df_.apply(lambda x: x/sum(x), axis=1)\n",
    "    classify_df_ = classify_df_.apply(lambda x: x/sum(x), axis=1)\n",
    "    \n",
    "    # Get mean and stdev for each column\n",
    "    train_mean = np.mean(train_df_)\n",
    "    train_std = np.std(train_df_)\n",
    "\n",
    "    # Transform each value to standard units for its column\n",
    "    train_df_ = ( train_df_ - train_mean ) / train_std\n",
    "    classify_df_ = ( classify_df_ - train_mean ) / train_std\n",
    "    \n",
    "    return train_df_, classify_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our Logistic Regression Model\n",
    "\n",
    "clf = LogisticRegression(C = 0.00007)\n",
    "\n",
    "model_df, classify_df, model_targets = set_canon_aside(metadata_tb, dtm_df)\n",
    "vocab_list = top_vocab_by_docfreq(model_df, 3200)\n",
    "model_df, classify_df = normalize_model(model_df, classify_df, vocab_list)\n",
    "clf.fit(model_df, model_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict whether our new prestigious texts might have been reviewed\n",
    "\n",
    "probabilities = numpy.zeros([len(canon_tb.column('docid'))])\n",
    "for _id in classify_df.index.tolist():\n",
    "    text = classify_df.loc[_id]\n",
    "    probability = clf.predict_proba([text])[0][1]\n",
    "    \n",
    "    _index = list(canon_tb.column('docid')).index(_id)\n",
    "    probabilities[_index] = probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this probability as a new column to our table of canonic texts\n",
    "\n",
    "canon_tb['P(reviewed)'] = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "\n",
    "canon_tb.scatter('firstpub','P(reviewed)', fit_line=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Q. Two of the prestigious texts are assigned less than 50% probability\n",
    "##     that they were reviewed. How do we make sense of that?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
