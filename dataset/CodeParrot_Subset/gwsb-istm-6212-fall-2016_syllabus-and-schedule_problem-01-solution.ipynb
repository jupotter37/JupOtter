{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 01 - solution\n",
    "\n",
    "The following are my approach to solving these problems.  Note that there may be more than one approach to each, and we could even debate the exact solutions.\n",
    "\n",
    "\n",
    "## Problem 1 - Word Counts\n",
    "\n",
    "\n",
    "\n",
    "### Part A - Part A. Characters in Little Women\n",
    "\n",
    "#### How many times are each of the following characters mentioned by name in the text of Little Women: Jo, Beth, Meg, Amy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-26 13:37:51--  https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/women.txt\n",
      "Resolving raw.githubusercontent.com... 151.101.32.133\n",
      "Connecting to raw.githubusercontent.com|151.101.32.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1053440 (1.0M) [text/plain]\n",
      "Saving to: ‘women.txt’\n",
      "\n",
      "women.txt           100%[===================>]   1.00M  1.13MB/s    in 0.9s    \n",
      "\n",
      "2016-09-26 13:37:53 (1.13 MB/s) - ‘women.txt’ saved [1053440/1053440]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/women.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1355 jo\r\n",
      " 683 meg\r\n",
      " 645 amy\r\n",
      " 459 beth\r\n",
      " 163 john\r\n",
      "   6 joanna\r\n",
      "   5 bethy\r\n",
      "   4 josephine\r\n",
      "   3 jove\r\n",
      "   3 josy\r\n",
      "   2 johnson\r\n",
      "   1 megs\r\n",
      "   1 meggy\r\n",
      "   1 jouvin\r\n",
      "   1 josyphine\r\n",
      "   1 joneses\r\n",
      "   1 jones\r\n",
      "   1 joian\r\n",
      "   1 johnsonianly\r\n",
      "   1 johnsonian\r\n",
      "   1 beths\r\n"
     ]
    }
   ],
   "source": [
    "!cat women.txt | grep -oE '\\w{{2,}}' \\\n",
    "   | grep -e \"Jo\\|Beth\\|Meg\\|Amy\" \\\n",
    "   | tr '[:upper:]' '[:lower:]' \\\n",
    "   | sort | uniq -c | sort -rn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the output of the command above, by the straight wording of the question, there are exactly 1,355 mentions of Jo, 683 of Meg, 645 of Amy, and 459 of Beth in Little Women.  If we were to assume that diminutive or nickname forms might count as well, we might add mentions of \"Megs\", \"Bethy\", and \"Meggy\" to these counts, for example.  For the purposes of this solution, however, I assume that these are not required, because the text might need to be consulted directly by someone familiar with the novel to determine which nicknames are valid, which seems beyond the scope of the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B - Juliet and Romeo in Romeo and Juliet\n",
    "\n",
    "#### How many times do each of the characters Juliet and Romeo have speaking lines in Romeo and Juliet? Keep in mind that this is the text of a play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-26 13:49:06--  https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/romeo.txt\n",
      "Resolving raw.githubusercontent.com... 151.101.32.133\n",
      "Connecting to raw.githubusercontent.com|151.101.32.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 178983 (175K) [text/plain]\n",
      "Saving to: ‘romeo.txt’\n",
      "\n",
      "romeo.txt           100%[===================>] 174.79K   999KB/s    in 0.2s    \n",
      "\n",
      "2016-09-26 13:49:07 (999 KB/s) - ‘romeo.txt’ saved [178983/178983]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/romeo.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we must recall -- as the problem highlights -- that this text is that of a play.  Because of this, we cannot simply count mentions of \"Romeo,\" as we might accidentally inflate the count due to mentions of this character, for example, by other characters in their speaking lines.  Instead, we must first look for a patter that indicates Romeo's speaking lines specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg EBook of Romeo and Juliet, by William Shakespeare\r",
      "\r\n",
      "Title: Romeo and Juliet\r",
      "\r\n",
      "The Tragedy of Romeo and Juliet\r",
      "\r\n",
      "  Romeo, son to Montague.\r",
      "\r\n",
      "  Mercutio, kinsman to the Prince and friend to Romeo.\r",
      "\r\n",
      "  Benvolio, nephew to Montague, and friend to Romeo\r",
      "\r\n",
      "  Balthasar, servant to Romeo.\r",
      "\r\n",
      "  M. Wife. O, where is Romeo? Saw you him to-day?\r",
      "\r\n",
      "                       Enter Romeo.\r",
      "\r\n",
      "  Rom. Is the day so young?\r",
      "\r\n",
      "  Rom. Ay me! sad hours seem long.\r",
      "\r\n",
      "  Ben. It was. What sadness lengthens Romeo's hours?\r",
      "\r\n",
      "  Rom. Not having that which having makes them short.\r",
      "\r\n",
      "  Rom. Out-\r",
      "\r\n",
      "  Rom. Out of her favour where I am in love.\r",
      "\r\n",
      "  Rom. Alas that love, whose view is muffled still,\r",
      "\r\n",
      "  Rom. Good heart, at what?\r",
      "\r\n",
      "  Rom. Why, such is love's transgression.\r",
      "\r\n",
      "  Rom. Tut! I have lost myself; I am not here:\r",
      "\r\n",
      "    This is not Romeo, he's some other where.\r",
      "\r\n",
      "  Rom. What, shall I groan and tell thee?\r",
      "\r\n",
      "  Rom. Bid a sick man in sadness make his will.\r",
      "\r\n",
      "  Rom. A right good markman! And she's fair I love.\r",
      "\r\n",
      "  Rom. Well, in that hit you miss. She'll not be hit\r",
      "\r\n",
      "  Rom. She hath, and in that sparing makes huge waste;\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat romeo.txt | grep \"Rom\" | head -25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this brief sample, we can see title lines and metadata that include mention of Romeo, and both stage directions (\"Enter Romeo\") and spoken lines that include his name.  What stands out, though, is that lines spoken by Romeo appear to be delineated by \"Rom.\", so we can search for this specific pattern.  Let's verify that the same should hold true for mentions of Juliet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg EBook of Romeo and Juliet, by William Shakespeare\r",
      "\r\n",
      "Title: Romeo and Juliet\r",
      "\r\n",
      "The Tragedy of Romeo and Juliet\r",
      "\r\n",
      "  Peter, servant to Juliet's nurse.\r",
      "\r\n",
      "  Juliet, daughter to Capulet.\r",
      "\r\n",
      "  Nurse to Juliet.\r",
      "\r\n",
      "    God forbid! Where's this girl? What, Juliet!\r",
      "\r\n",
      "                         Enter Juliet.\r",
      "\r\n",
      "  Jul. How now? Who calls?\r",
      "\r\n",
      "  Jul. Madam, I am here.\r",
      "\r\n",
      "    Wilt thou not, Jule?' and, by my holidam,\r",
      "\r\n",
      "    I never should forget it. 'Wilt thou not, Jule?' quoth he,\r",
      "\r\n",
      "    Wilt thou not, Jule?' It stinted, and said 'Ay.'\r",
      "\r\n",
      "  Jul. And stint thou too, I pray thee, nurse, say I.\r",
      "\r\n",
      "    I came to talk of. Tell me, daughter Juliet,\r",
      "\r\n",
      "  Jul. It is an honour that I dream not of.\r",
      "\r\n",
      "  Jul. I'll look to like, if looking liking move;\r",
      "\r\n",
      "    Juliet, the County stays.\r",
      "\r\n",
      "              Juliet, Tybalt, and all the Guests\r",
      "\r\n",
      "  Jul. Good pilgrim, you do wrong your hand too much,\r",
      "\r\n",
      "  Jul. Ay, pilgrim, lips that they must use in pray'r.\r",
      "\r\n",
      "  Jul. Saints do not move, though grant for prayers' sake.\r",
      "\r\n",
      "  Jul. Then have my lips the sin that they have took.\r",
      "\r\n",
      "  Jul. You kiss by th' book.\r",
      "\r\n",
      "                              Exeunt [all but Juliet and Nurse].\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat romeo.txt | grep \"Jul\" | head -25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the pattern seems to hold for both.  I will assume that matches of the exact characters \"Rom.\" and \"Jul.\" indicate the start of a speaking line for one or the other characters, and will explicitly count only those lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 163 Rom.\r\n"
     ]
    }
   ],
   "source": [
    "!cat romeo.txt | grep -w \"Rom\\.\" \\\n",
    "   | grep -oE '\\w{{2,}}\\.' \\\n",
    "   | grep \"Rom\" \\\n",
    "   | sort | uniq -c | sort -rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 117 Jul.\r\n"
     ]
    }
   ],
   "source": [
    "!cat romeo.txt | grep -w \"Jul\\.\" \\\n",
    "   | grep -oE '\\w{{2,}}\\.' \\\n",
    "   | grep \"Jul\" \\\n",
    "   | sort | uniq -c | sort -rn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two pipelines above indicate that Romeo has 163 speaking lines, while Juliet has only 117.  To match the specific case with a trailing `.`, the first regular expressions in both above cases use the `-w` flag to denote a word match and the escape sequence `\\.` to match the literal trailing period.  The second regular expressions include this literal at the end of the match sequence as well, with the trailing literal period in `'\\w{{2,}}\\.'` requiring that the match include the period at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 - Capital Bikeshare\n",
    "\n",
    "### Part A - Station counts\n",
    "\n",
    "#### Which 10 Capital Bikeshare stations were the most popular departing stations in Q1 2016?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-26 14:02:12--  https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/2016q1.csv.zip\n",
      "Resolving raw.githubusercontent.com... 151.101.32.133\n",
      "Connecting to raw.githubusercontent.com|151.101.32.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10643003 (10M) [application/octet-stream]\n",
      "Saving to: ‘2016q1.csv.zip’\n",
      "\n",
      "2016q1.csv.zip      100%[===================>]  10.15M  1.18MB/s    in 8.8s    \n",
      "\n",
      "2016-09-26 14:02:22 (1.15 MB/s) - ‘2016q1.csv.zip’ saved [10643003/10643003]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/2016q1.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  2016q1.csv.zip\n",
      "  inflating: 2016q1.csv              \n"
     ]
    }
   ],
   "source": [
    "!unzip 2016q1.csv.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|----------------+-----------------+---------------+----------------------+--------------------------------+--------------------+---------------------------+-------------+--------------|\r\n",
      "|  Duration (ms) | Start date      | End date      | Start station number | Start station                  | End station number | End station               | Bike number | Member Type  |\r\n",
      "|----------------+-----------------+---------------+----------------------+--------------------------------+--------------------+---------------------------+-------------+--------------|\r\n",
      "|  301295        | 3/31/2016 23:59 | 4/1/2016 0:04 | 31280                | 11th & S St NW                 | 31506              | 1st & Rhode Island Ave NW | W00022      | Registered   |\r\n",
      "|  557887        | 3/31/2016 23:59 | 4/1/2016 0:08 | 31275                | New Hampshire Ave & 24th St NW | 31114              | 18th St & Wyoming Ave NW  | W01294      | Registered   |\r\n",
      "|  555944        | 3/31/2016 23:59 | 4/1/2016 0:08 | 31101                | 14th & V St NW                 | 31221              | 18th & M St NW            | W01416      | Registered   |\r\n",
      "|  766916        | 3/31/2016 23:57 | 4/1/2016 0:09 | 31226                | 34th St & Wisconsin Ave NW     | 31214              | 17th & Corcoran St NW     | W01090      | Registered   |\r\n",
      "|----------------+-----------------+---------------+----------------------+--------------------------------+--------------------+---------------------------+-------------+--------------|\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 2016q1.csv | csvlook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: Duration (ms)\r\n",
      "  2: Start date\r\n",
      "  3: End date\r\n",
      "  4: Start station number\r\n",
      "  5: Start station\r\n",
      "  6: End station number\r\n",
      "  7: End station\r\n",
      "  8: Bike number\r\n",
      "  9: Member Type\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -n 2016q1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13120 Columbus Circle / Union Station\r\n",
      "9560 Massachusetts Ave & Dupont Circle NW\r\n",
      "9388 Lincoln Memorial\r\n",
      "8138 Jefferson Dr & 14th St SW\r\n",
      "7479 Thomas Circle\r\n",
      "7401 15th & P St NW\r\n",
      "6568 14th & V St NW\r\n",
      "6491 New Hampshire Ave & T St NW\r\n",
      "5649 Eastern Market Metro / Pennsylvania Ave & 7th St SE\r\n",
      "5514 17th & Corcoran St NW\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c5 2016q1.csv | tail -n +2 | csvsort | uniq -c | sort -rn | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the above results, the top ten starting stations in this time period were led by Columbus Circle / Union Station with over 13,000 rides, followed by Dupont Circle and the Lincoln Memorial and the rest as listed.\n",
    "\n",
    "In the pipeline above, `tail -n +2` ensures we skip the header line before the sort process begins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which 10 were the most popular destination stations in Q1 2016?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13880 Columbus Circle / Union Station\r\n",
      "11183 Massachusetts Ave & Dupont Circle NW\r\n",
      "9419 Lincoln Memorial\r\n",
      "8975 Jefferson Dr & 14th St SW\r\n",
      "8092 15th & P St NW\r\n",
      "7267 14th & V St NW\r\n",
      "6997 Thomas Circle\r\n",
      "6245 New Hampshire Ave & T St NW\r\n",
      "5761 5th & K St NW\r\n",
      "5651 17th & Corcoran St NW\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c7 2016q1.csv | tail -n +2 | csvsort | uniq -c | sort -rn | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results show us very similar numbers for destination stations during the same time period, with the first four stations unchanged and led again by Union Station with over 13,000 rides.  Thomas Circle appears to be a more prominent start station than end station, as does Eastern Market, which does not even make the top ten destination stations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B - bike counts\n",
    "\n",
    "#### For the most popular departure station, which 10 bikes were used most in trips departing from there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will use `csvgrep` to select only the required stations - Union Station, in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration (ms),Start date,End date,Start station number,Start station,End station number,End station,Bike number,Member Type\r\n",
      "179833,3/31/2016 23:31,3/31/2016 23:34,31623,Columbus Circle / Union Station,31616,3rd & H St NE,W20445,Registered\r\n",
      "701676,3/31/2016 23:13,3/31/2016 23:25,31623,Columbus Circle / Union Station,31601,19th & East Capitol St SE,W21148,Registered\r\n",
      "709799,3/31/2016 23:01,3/31/2016 23:13,31623,Columbus Circle / Union Station,31611,13th & H St NE,W01261,Registered\r\n",
      "274002,3/31/2016 22:46,3/31/2016 22:50,31623,Columbus Circle / Union Station,31603,1st & M St NE,W21049,Registered\r\n",
      "533503,3/31/2016 22:38,3/31/2016 22:47,31623,Columbus Circle / Union Station,31622,13th & D St NE,W01253,Registered\r\n",
      "465837,3/31/2016 21:36,3/31/2016 21:44,31623,Columbus Circle / Union Station,31640,Maryland Ave & E St NE,W00086,Registered\r\n",
      "371081,3/31/2016 21:11,3/31/2016 21:18,31623,Columbus Circle / Union Station,31632,15th & F St NE,W00736,Registered\r\n",
      "569451,3/31/2016 21:11,3/31/2016 21:21,31623,Columbus Circle / Union Station,31628,1st & K St SE,W00945,Registered\r\n",
      "439199,3/31/2016 21:04,3/31/2016 21:11,31623,Columbus Circle / Union Station,31611,13th & H St NE,W21751,Registered\r\n"
     ]
    }
   ],
   "source": [
    "!csvgrep -c5 -m \"Columbus Circle / Union Station\" 2016q1.csv | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further limit the columns used to cut down on the data flowing through the pipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start station,Bike number\r\n",
      "Columbus Circle / Union Station,W20445\r\n",
      "Columbus Circle / Union Station,W21148\r\n",
      "Columbus Circle / Union Station,W01261\r\n",
      "Columbus Circle / Union Station,W21049\r\n",
      "Columbus Circle / Union Station,W01253\r\n",
      "Columbus Circle / Union Station,W00086\r\n",
      "Columbus Circle / Union Station,W00736\r\n",
      "Columbus Circle / Union Station,W00945\r\n",
      "Columbus Circle / Union Station,W21751\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c5,8 2016q1.csv | csvgrep -c1 -m \"Columbus Circle / Union Station\" | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  17 W22227\r\n",
      "  16 W21867\r\n",
      "  16 W21641\r\n",
      "  16 W21538\r\n",
      "  16 W21239\r\n",
      "  16 W20540\r\n",
      "  16 W00714\r\n",
      "  15 W22080\r\n",
      "  15 W21450\r\n",
      "  15 W21076\r\n",
      "  15 W00777\r\n",
      "  15 W00288\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c5,8 2016q1.csv \\\n",
    "    | csvgrep -c1 -m \"Columbus Circle / Union Station\" \\\n",
    "    | csvcut -c2 \\\n",
    "    | tail -n +2 \\\n",
    "    | sort | uniq -c | sort -rn | head -12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the most commonly used bikes in trips departing from Union Station, led by bike number W22227.  As we might expect it appears that the distribution seems rather uniform.  Note that because several bikes had exactly 15 trips starting from Union Station, the list includes the top twelve bikes, rather than the top ten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which 10 bikes were used most in trips ending at the most popular destination station?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  18 W00485\r\n",
      "  17 W22227\r\n",
      "  16 W22099\r\n",
      "  16 W22080\r\n",
      "  16 W21239\r\n",
      "  16 W21076\r\n",
      "  16 W20425\r\n",
      "  16 W00714\r\n",
      "  15 W21997\r\n",
      "  15 W21867\r\n",
      "  15 W21641\r\n",
      "  15 W21538\r\n",
      "  15 W21450\r\n",
      "  15 W20540\r\n",
      "  15 W01439\r\n"
     ]
    }
   ],
   "source": [
    "!csvcut -c7,8 2016q1.csv \\\n",
    "    | csvgrep -c1 -m \"Columbus Circle / Union Station\" \\\n",
    "    | csvcut -c2 \\\n",
    "    | tail -n +2 \\\n",
    "    | sort | uniq -c | sort -rn | head -15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the most commonly used bikes in trips arriving at Union Station, let by bike number W00485.  It is interesting to note that bike W22227, the top departing bike, is in second place, but bike W00485, the top arriving bike, does not appear in the top ten departing bikes.  In any case these also seem at first glance to be uniformly distributed.  Again, the list is expanded, this time to fifteen bikes, to account for the tie at exactly fifteen trips."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 - Filters\n",
    "\n",
    "### Part A - split and lowercase filters\n",
    "\n",
    "Write a Python filter than replaces grep -oE '\\w{2,}' to split lines of text into one word per line, and write an additional Python filter to replace tr '[:upper:]' '[:lower:]' to transform text into lower case.\n",
    "\n",
    "With your two new filters, repeat the original pipeline, and substitute your new filters as appropriate. You should obtain the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-26 14:27:23--  https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/simplefilter.py\n",
      "Resolving raw.githubusercontent.com... 151.101.32.133\n",
      "Connecting to raw.githubusercontent.com|151.101.32.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 208 [text/plain]\n",
      "Saving to: ‘simplefilter.py’\n",
      "\n",
      "simplefilter.py     100%[===================>]     208  --.-KB/s    in 0s      \n",
      "\n",
      "2016-09-26 14:27:23 (19.8 MB/s) - ‘simplefilter.py’ saved [208/208]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/simplefilter.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cp simplefilter.py split.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `split.py` is modified from the template to split lines of text into one word per line.  To demonstrate this, we can compare the original pipeline with a new pipeline with `split.py` substituting for the first `grep` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8155 and\r\n",
      "7689 the\r\n",
      "5152 to\r\n",
      "4531 a\r\n",
      "4003 i\r\n",
      "3523 of\r\n",
      "3245 her\r\n",
      "2774 it\r\n",
      "2503 in\r\n",
      "2447 you\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!cat women.txt \\\n",
    "    | grep -oE '\\w{{1,}}' \\\n",
    "    | tr '[:upper:]' '[:lower:]' \\\n",
    "    | sort \\\n",
    "    | uniq -c \\\n",
    "    | sort -rn \\\n",
    "    | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ignore the broken pipe and related errors as the output appears to be correct.\n",
    "\n",
    "Next, we repeat the pipeline with `split.py` substituted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x split.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the filter script below, the key line, #14, removes trailing newlines, splits tokens by the space (`' '`), and removes words that are not entirely alphabetical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:#!/usr/bin/env python\r\n",
      "2:\r\n",
      "3:\"\"\"\r\n",
      "4:A filter that splits incoming lines of text into one word per line.\r\n",
      "5:It eliminates all tokens that are non-alphabetic, containing numbers,\r\n",
      "6:punctuation, or whitespace.\r\n",
      "7:\"\"\" \r\n",
      "8:\r\n",
      "9:import fileinput\r\n",
      "10:\r\n",
      "11:\r\n",
      "12:def process(line):\r\n",
      "13:    \"\"\"For each line of input, split it into words.\"\"\"\r\n",
      "14:    for token in [w for w in line[:-1].split(' ') if w.isalpha()]:\r\n",
      "15:        print(token)\r\n",
      "16:\r\n",
      "17:\r\n",
      "18:for line in fileinput.input():\r\n",
      "19:    process(line)\r\n"
     ]
    }
   ],
   "source": [
    "!grep -n '' split.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7408 and\r\n",
      "6963 the\r\n",
      "4749 to\r\n",
      "4064 a\r\n",
      "3251 of\r\n",
      "2626 her\r\n",
      "2525 i\r\n",
      "2275 in\r\n",
      "2066 she\r\n",
      "2057 for\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!cat women.txt \\\n",
    "    | ./split.py \\\n",
    "    | tr '[:upper:]' '[:lower:]' \\\n",
    "    | sort \\\n",
    "    | uniq -c \\\n",
    "    | sort -rn \\\n",
    "    | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost the exact words listed appear in nearly the same order, but with lower counts for each.  We can examine the output of each command to see if there are obvious differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\r\n",
      "Project\r\n",
      "Gutenberg\r\n",
      "EBook\r\n",
      "of\r\n",
      "Little\r\n",
      "Women\r\n",
      "by\r\n",
      "Louisa\r\n",
      "May\r\n",
      "Alcott\r\n",
      "This\r\n",
      "eBook\r\n",
      "is\r\n",
      "for\r\n",
      "the\r\n",
      "use\r\n",
      "of\r\n",
      "anyone\r\n",
      "anywhere\r\n",
      "at\r\n",
      "no\r\n",
      "cost\r\n",
      "and\r\n",
      "with\r\n"
     ]
    }
   ],
   "source": [
    "!cat women.txt | grep -oE '\\w{{2,}}' | head -25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project\r\n",
      "Gutenberg\r\n",
      "EBook\r\n",
      "of\r\n",
      "Little\r\n",
      "by\r\n",
      "Louisa\r\n",
      "May\r\n",
      "This\r\n",
      "eBook\r\n",
      "is\r\n",
      "for\r\n",
      "the\r\n",
      "use\r\n",
      "of\r\n",
      "anyone\r\n",
      "anywhere\r\n",
      "at\r\n",
      "no\r\n",
      "cost\r\n",
      "and\r\n",
      "almost\r\n",
      "no\r\n",
      "restrictions\r\n",
      "You\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"./split.py\", line 19, in <module>\r\n",
      "    process(line)\r\n",
      "  File \"./split.py\", line 15, in process\r\n",
      "    print(token)\r\n",
      "BrokenPipeError: [Errno 32] Broken pipe\r\n",
      "Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='UTF-8'>\r\n",
      "BrokenPipeError: [Errno 32] Broken pipe\r\n",
      "cat: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!cat women.txt | ./split.py | head -25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see straight away on the first few lines that there is a difference.  Let's look at the text itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg EBook of Little Women, by Louisa May Alcott\r",
      "\r\n",
      "\r",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -3 women.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three obvious issues jump out.  First, the initial \"The\" is elided; it is not clear why.  Next, \"Women\" is removed, perhaps due to the trailing comma, which will cause the token to fail the `isalpha()` test.  Also, \"Alcott\" is removed, perhaps having to do with its position at the end of the line.\n",
    "\n",
    "We can update the filter to use Python's regular expression model and a similar expression, `\\w{1,}` to find all matches more intelligently.  Here the regular expression is prepared in line 13 and used in line 18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:#!/usr/bin/env python\r\n",
      "2:\r\n",
      "3:\"\"\"\r\n",
      "4:A filter that splits incoming lines of text into one word per line.\r\n",
      "5:It eliminates all tokens that are non-alphabetic, containing numbers,\r\n",
      "6:punctuation, or whitespace.\r\n",
      "7:\"\"\" \r\n",
      "8:\r\n",
      "9:import fileinput\r\n",
      "10:import re\r\n",
      "11:\r\n",
      "12:\r\n",
      "13:RE_SPLIT = re.compile('(\\w{1,})')\r\n",
      "14:\r\n",
      "15:\r\n",
      "16:def process(line):\r\n",
      "17:    \"\"\"For each line of input, split it into words.\"\"\"\r\n",
      "18:    for token in [w for w in RE_SPLIT.findall(line)]:\r\n",
      "19:        print(token)\r\n",
      "20:\r\n",
      "21:\r\n",
      "22:for line in fileinput.input():\r\n",
      "23:    process(line)\r\n"
     ]
    }
   ],
   "source": [
    "!grep -n '' split.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\r\n",
      "Project\r\n",
      "Gutenberg\r\n",
      "EBook\r\n",
      "of\r\n",
      "Little\r\n",
      "Women\r\n",
      "by\r\n",
      "Louisa\r\n",
      "May\r\n",
      "Alcott\r\n",
      "This\r\n",
      "eBook\r\n",
      "is\r\n",
      "for\r\n",
      "the\r\n",
      "use\r\n",
      "of\r\n",
      "anyone\r\n",
      "anywhere\r\n",
      "at\r\n",
      "no\r\n",
      "cost\r\n",
      "and\r\n",
      "with\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"./split.py\", line 23, in <module>\r\n",
      "    process(line)\r\n",
      "  File \"./split.py\", line 19, in process\r\n",
      "    print(token)\r\n",
      "BrokenPipeError: [Errno 32] Broken pipe\r\n",
      "Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='UTF-8'>\r\n",
      "BrokenPipeError: [Errno 32] Broken pipe\r\n",
      "cat: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!cat women.txt | ./split.py | head -25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better.  We can try the full pipeline again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8155 and\r\n",
      "7689 the\r\n",
      "5152 to\r\n",
      "4531 a\r\n",
      "4003 i\r\n",
      "3523 of\r\n",
      "3245 her\r\n",
      "2774 it\r\n",
      "2503 in\r\n",
      "2447 you\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!cat women.txt \\\n",
    "    | ./split.py \\\n",
    "    | tr '[:upper:]' '[:lower:]' \\\n",
    "    | sort \\\n",
    "    | uniq -c \\\n",
    "    | sort -rn \\\n",
    "    | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks to be an exact match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cp simplefilter.py lowercase.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filter `lowercase.py` is modified from the template to lowercase incoming lines of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x lowercase.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:#!/usr/bin/env python\r\n",
      "2:\r\n",
      "3:\"\"\"\r\n",
      "4:A filter that lower cases all incoming text.\r\n",
      "5:\"\"\"\r\n",
      "6:\r\n",
      "7:import fileinput\r\n",
      "8:\r\n",
      "9:\r\n",
      "10:def process(line):\r\n",
      "11:    \"\"\"For each line of input, lower case it and print the result.\"\"\"\r\n",
      "12:    print(line[:-1].lower())\r\n",
      "13:\r\n",
      "14:\r\n",
      "15:for line in fileinput.input():\r\n",
      "16:    process(line)\r\n"
     ]
    }
   ],
   "source": [
    "!grep -n '' lowercase.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the only line aside from the comments that changes in the above script is line #12, which adds the `lower()` to the print statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿the project gutenberg ebook of little women, by louisa may alcott\r",
      "\r\n",
      "\r",
      "\r\n",
      "this ebook is for the use of anyone anywhere at no cost and with\r",
      "\r\n",
      "almost no restrictions whatsoever.  you may copy it, give it away or\r",
      "\r\n",
      "re-use it under the terms of the project gutenberg license included\r",
      "\r\n",
      "with this ebook or online at www.gutenberg.net\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "title: little women\r",
      "\r\n",
      "\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head women.txt | ./lowercase.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks correct, so we'll first attempt to replace the original pipeline's use of `tr` with `lowercase.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8155 and\r\n",
      "7689 the\r\n",
      "5152 to\r\n",
      "4531 a\r\n",
      "4003 i\r\n",
      "3523 of\r\n",
      "3245 her\r\n",
      "2774 it\r\n",
      "2503 in\r\n",
      "2447 you\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!cat women.txt \\\n",
    "    | grep -oE '\\w{{1,}}' \\\n",
    "    | ./lowercase.py \\\n",
    "    | sort \\\n",
    "    | uniq -c \\\n",
    "    | sort -rn \\\n",
    "    | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good so far, we are seeing the exact same counts.  To address the problem's challenge, we finally replace both filters at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8155 and\r\n",
      "7689 the\r\n",
      "5152 to\r\n",
      "4531 a\r\n",
      "4003 i\r\n",
      "3523 of\r\n",
      "3245 her\r\n",
      "2774 it\r\n",
      "2503 in\r\n",
      "2447 you\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!cat women.txt \\\n",
    "    | ./split.py \\\n",
    "    | ./lowercase.py \\\n",
    "    | sort \\\n",
    "    | uniq -c \\\n",
    "    | sort -rn \\\n",
    "    | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes Problem 3 - Part A.\n",
    "\n",
    "### Part B - stop words\n",
    "\n",
    "#### Write a Python filter that removes at least ten common words of English text, commonly known as \"stop words\". Sources of English stop word lists are readily available online, or you may generate your own list from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by acquiring a common list of English stop words, gathered from the site http://www.textfixer.com/resources/common-english-words.txt as linked from the [Wikipedia page on stop words](https://en.wikipedia.org/wiki/Stop_words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-26 15:00:23--  http://www.textfixer.com/resources/common-english-words.txt\n",
      "Resolving www.textfixer.com... 216.172.105.85\n",
      "Connecting to www.textfixer.com|216.172.105.85|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 551 [text/plain]\n",
      "Saving to: ‘common-english-words.txt’\n",
      "\n",
      "common-english-word 100%[===================>]     551  --.-KB/s    in 0s      \n",
      "\n",
      "2016-09-26 15:00:23 (7.73 MB/s) - ‘common-english-words.txt’ saved [551/551]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.textfixer.com/resources/common-english-words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a,able,about,across,after,all,almost,also,am,among,an,and,any,are,as,at,be,because,been,but,by,can,cannot,could,dear,did,do,does,either,else,ever,every,for,from,get,got,had,has,have,he,her,hers,him,his,how,however,i,if,in,into,is,it,its,just,least,let,like,likely,may,me,might,most,must,my,neither,no,nor,not,of,off,often,on,only,or,other,our,own,rather,said,say,says,she,should,since,so,some,than,that,the,their,them,then,there,these,they,this,tis,to,too,twas,us,wants,was,we,were,what,when,where,which,while,who,whom,why,will,with,would,yet,you,your"
     ]
    }
   ],
   "source": [
    "!head common-english-words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we copy the template filter script as before, renaming it appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cp simplefilter.py stopwords.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x stopwords.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:#!/usr/bin/env python\r\n",
      "2:\r\n",
      "3:\"\"\"\r\n",
      "4:A filter that removes a set of stop words obtained from a local file.\r\n",
      "5:\r\n",
      "6:This filter assumes that incoming text has been prepared to arrive as\r\n",
      "7:one word per line, in lower case.\r\n",
      "8:\"\"\"\r\n",
      "9:\r\n",
      "10:import fileinput\r\n",
      "11:\r\n",
      "12:\r\n",
      "13:STOPWORDS = open('common-english-words.txt', 'r').read().strip().split(',')\r\n",
      "14:\r\n",
      "15:\r\n",
      "16:def process(line):\r\n",
      "17:    \"\"\"For each line of input, remove any matching stopwords.\"\"\"\r\n",
      "18:    # Remove trailing newline first\r\n",
      "19:    line = line[:-1]\r\n",
      "20:    if line not in STOPWORDS:\r\n",
      "21:        print(line)\r\n",
      "22:\r\n",
      "23:    \r\n",
      "24:for line in fileinput.input():\r\n",
      "25:    process(line[:-1])\r\n"
     ]
    }
   ],
   "source": [
    "!grep -n '' stopwords.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key changes in `stopwords.py` from the template are line #13, which imports the list of stopwords, and line #20, which checks whether an incoming word is in the stopword list.  Note also that in line #19 the removal of a trailing newline occurs before checking for stopwords.\n",
    "\n",
    "The assumption that incoming text will already be split into one word per line and lowercased is stated explicitly in the first comment, lines #6-7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project\r\n",
      "gutenberg\r\n",
      "ebook\r\n",
      "little\r\n",
      "women\r\n",
      "louisa\r\n",
      "alcott\r\n",
      "ebook\r\n",
      "use\r\n",
      "anyone\r\n",
      "anywhere\r\n",
      "cost\r\n",
      "restrictions\r\n",
      "whatsoever\r\n",
      "copy\r\n",
      "give\r\n",
      "away\r\n",
      "re\r\n",
      "use\r\n",
      "under\r\n",
      "terms\r\n",
      "project\r\n",
      "gutenberg\r\n",
      "license\r\n",
      "included\r\n",
      "ebook\r\n",
      "online\r\n",
      "www\r\n",
      "gutenberg\r\n",
      "net\r\n",
      "title\r\n",
      "little\r\n",
      "women\r\n"
     ]
    }
   ],
   "source": [
    "!head women.txt | ./split.py | ./lowercase.py | ./stopwords.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This appears to be correct.  Let's put it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680 t\r\n",
      "1495 s\r\n",
      "1362 jo\r\n",
      " 730 little\r\n",
      " 725 one\r\n",
      " 686 meg\r\n",
      " 658 up\r\n",
      " 652 amy\r\n",
      " 598 laurie\r\n",
      " 550 don\r\n",
      " 492 very\r\n",
      " 485 out\r\n",
      " 467 beth\r\n",
      " 462 good\r\n",
      " 399 now\r\n",
      " 398 m\r\n",
      " 393 go\r\n",
      " 380 old\r\n",
      " 377 mother\r\n",
      " 375 never\r\n",
      " 374 much\r\n",
      " 368 well\r\n",
      " 361 ll\r\n",
      " 360 see\r\n",
      " 355 over\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!cat women.txt \\\n",
    "    | ./split.py \\\n",
    "    | ./lowercase.py \\\n",
    "    | ./stopwords.py \\\n",
    "    | sort \\\n",
    "    | uniq -c \\\n",
    "    | sort -rn \\\n",
    "    | head -25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would seem to be correct - we see the names we looked for earlier appearing near the top of the list, and common stop words are indeed removed - however the list starts with odd \"words\", in \"t\", \"s\", \"m\", and \"ll\".  Is it possible that these are occurences of contractions?  We can check a few different ways.  First, let's see if our `split.py` is causing the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680 t\r\n",
      "1495 s\r\n",
      "1362 jo\r\n",
      " 730 little\r\n",
      " 725 one\r\n",
      " 686 meg\r\n",
      " 658 up\r\n",
      " 652 amy\r\n",
      " 598 laurie\r\n",
      " 550 don\r\n",
      " 492 very\r\n",
      " 485 out\r\n",
      " 467 beth\r\n",
      " 462 good\r\n",
      " 399 now\r\n",
      " 398 m\r\n",
      " 393 go\r\n",
      " 380 old\r\n",
      " 377 mother\r\n",
      " 375 never\r\n",
      " 374 much\r\n",
      " 368 well\r\n",
      " 361 ll\r\n",
      " 360 see\r\n",
      " 355 over\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "!cat women.txt \\\n",
    "    | grep -oE '\\w{{1,}}' \\\n",
    "    | ./lowercase.py \\\n",
    "    | ./stopwords.py \\\n",
    "    | sort \\\n",
    "    | uniq -c \\\n",
    "    | sort -rn \\\n",
    "    | head -25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, the results are exactly the same.  Instead, we'll need to look for occurrences of \"t\" and \"s\" by themselves.  The `--context` option to `grep` might help us here, pointing out surrounding text to search for in the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "christmas\r\n",
      "won\r\n",
      "t\r\n",
      "be\r\n",
      "christmas\r\n",
      "--\r\n",
      "--\r\n",
      "i\r\n",
      "don\r\n",
      "t\r\n",
      "think\r\n",
      "it\r\n",
      "--\r\n",
      "--\r\n",
      "we\r\n",
      "haven\r\n",
      "t\r\n",
      "got\r\n",
      "father\r\n",
      "--\r\n"
     ]
    }
   ],
   "source": [
    "!cat women.txt \\\n",
    "    | ./split.py \\\n",
    "    | ./lowercase.py \\\n",
    "    | grep --context=2 -oE '^t$' \\\n",
    "    | head -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cheerful words, but darkened again as Jo said sadly, \"We haven't got\r",
      "\r\n",
      "poplins, because we haven't got anything else?\" answered Jo with her\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!grep -i \"we haven't got\" women.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha, it does appear that the occurences of a bare \"t\" are from contractions.  Let's repeat with \"s\", which might occur in possessives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seven\r\n",
      "amy\r\n",
      "s\r\n",
      "valley\r\n",
      "of\r\n",
      "--\r\n",
      "--\r\n",
      "nineteen\r\n",
      "amy\r\n",
      "s\r\n",
      "will\r\n",
      "twenty\r\n",
      "--\r\n",
      "--\r\n",
      "three\r\n",
      "jo\r\n",
      "s\r\n",
      "journal\r\n",
      "thirty\r\n",
      "--\r\n"
     ]
    }
   ],
   "source": [
    "!cat women.txt \\\n",
    "    | ./split.py \\\n",
    "    | ./lowercase.py \\\n",
    "    | grep --context=2 -oE '^s$' \\\n",
    "    | head -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        SEVEN  AMY'S VALLEY OF HUMILIATION\r",
      "\r\n",
      "AMY'S VALLEY OF HUMILIATION\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!grep -i \"amy's valley\" women.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we have it - the counts from above were correct, and we could eliminate \"t\" and \"s\" from consideration with a `grep -v`, and we can further assume that the \"ll\" and \"m\" occurences are also from contractions, so we'll remove them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1362 jo\r\n",
      " 725 one\r\n",
      " 658 up\r\n",
      " 598 laurie\r\n",
      " 550 don\r\n",
      " 492 very\r\n",
      " 462 good\r\n",
      " 399 now\r\n",
      " 393 go\r\n",
      " 380 old\r\n",
      " 375 never\r\n",
      " 355 over\r\n",
      " 331 away\r\n",
      " 319 know\r\n",
      " 291 down\r\n",
      " 279 young\r\n",
      " 268 girls\r\n",
      " 254 face\r\n",
      " 252 asked\r\n",
      " 245 herself\r\n",
      " 242 day\r\n",
      " 240 looked\r\n",
      " 238 look\r\n",
      " 218 long\r\n",
      " 214 love\r\n"
     ]
    }
   ],
   "source": [
    "!cat women.txt \\\n",
    "    | ./split.py \\\n",
    "    | ./lowercase.py \\\n",
    "    | ./stopwords.py \\\n",
    "    | grep -v -oE '^s|t|m|ll$' \\\n",
    "    | sort \\\n",
    "    | uniq -c \\\n",
    "    | sort -rn \\\n",
    "    | head -25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a final count.  It is interesting to note that these counts of character names (Jo, Meg, etc.) are slightly different from before, perhaps due to punctuation handling, but it seems beyond the scope of the question to answer it precisely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra credit - parallel stop words\n",
    "\n",
    "### Use GNU parallel to count the 25 most common words across all the 109 texts in the zip file provided, with stop words removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-09-26 15:26:10--  https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/texts.zip\n",
      "Resolving raw.githubusercontent.com... 151.101.32.133\n",
      "Connecting to raw.githubusercontent.com|151.101.32.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12668137 (12M) [application/octet-stream]\n",
      "Saving to: ‘texts.zip’\n",
      "\n",
      "texts.zip           100%[===================>]  12.08M   894KB/s    in 11s     \n",
      "\n",
      "2016-09-26 15:26:22 (1.08 MB/s) - ‘texts.zip’ saved [12668137/12668137]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/gwsb-istm-6212-fall-2016/syllabus-and-schedule/master/projects/project-01/texts.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  texts.zip\r\n",
      "  Length     Date   Time    Name\r\n",
      " --------    ----   ----    ----\r\n",
      "    52510  04-09-05 12:40   10001.txt\r\n",
      "   306892  01-20-09 00:00   10002.txt\r\n"
     ]
    }
   ],
   "source": [
    "!unzip -l texts.zip | head -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir all-texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  texts.zip\n",
      "  inflating: all-texts/10001.txt     \n",
      "  inflating: all-texts/10002.txt     \n",
      "  inflating: all-texts/10003.txt     \n",
      "  inflating: all-texts/10004.txt     \n",
      "  inflating: all-texts/10005.txt     \n",
      "  inflating: all-texts/10006.txt     \n",
      "  inflating: all-texts/10007.txt     \n",
      "  inflating: all-texts/10008.txt     \n",
      "  inflating: all-texts/10009.txt     \n",
      "  inflating: all-texts/10010.txt     \n",
      "  inflating: all-texts/10011.txt     \n",
      "  inflating: all-texts/10012.txt     \n",
      "  inflating: all-texts/10013.txt     \n",
      "  inflating: all-texts/10014.txt     \n",
      "  inflating: all-texts/10015.txt     \n",
      "  inflating: all-texts/10016.txt     \n",
      "  inflating: all-texts/10017.txt     \n",
      "  inflating: all-texts/10018.txt     \n",
      "  inflating: all-texts/10019.txt     \n",
      "  inflating: all-texts/10020.txt     \n",
      "  inflating: all-texts/10021.txt     \n",
      "  inflating: all-texts/10023.txt     \n",
      "  inflating: all-texts/10024.txt     \n",
      "  inflating: all-texts/10025.txt     \n",
      "  inflating: all-texts/10026.txt     \n",
      "  inflating: all-texts/10027.txt     \n",
      "  inflating: all-texts/10028.txt     \n",
      "  inflating: all-texts/10029.txt     \n",
      "  inflating: all-texts/10030.txt     \n",
      "  inflating: all-texts/10031.txt     \n",
      "  inflating: all-texts/10032.txt     \n",
      "  inflating: all-texts/10033.txt     \n",
      "  inflating: all-texts/10034.txt     \n",
      "  inflating: all-texts/10035.txt     \n",
      "  inflating: all-texts/10036.txt     \n",
      "  inflating: all-texts/10037.txt     \n",
      "  inflating: all-texts/10038.txt     \n",
      "  inflating: all-texts/10039.txt     \n",
      "  inflating: all-texts/10040.txt     \n",
      "  inflating: all-texts/10041.txt     \n",
      "  inflating: all-texts/10042.txt     \n",
      "  inflating: all-texts/10043.txt     \n",
      "  inflating: all-texts/10045.txt     \n",
      "  inflating: all-texts/10046.txt     \n",
      "  inflating: all-texts/10047.txt     \n",
      "  inflating: all-texts/10048.txt     \n",
      "  inflating: all-texts/10049.txt     \n",
      "  inflating: all-texts/10050.txt     \n",
      "  inflating: all-texts/10051.txt     \n",
      "  inflating: all-texts/10052.txt     \n",
      "  inflating: all-texts/10056.txt     \n",
      "  inflating: all-texts/10059.txt     \n",
      "  inflating: all-texts/10060.txt     \n",
      "  inflating: all-texts/10062.txt     \n",
      "  inflating: all-texts/12370.txt     \n",
      "  inflating: all-texts/12372.txt     \n",
      "  inflating: all-texts/12373.txt     \n",
      "  inflating: all-texts/12374.txt     \n",
      "  inflating: all-texts/12375.txt     \n",
      "  inflating: all-texts/12376.txt     \n",
      "  inflating: all-texts/12377.txt     \n",
      "  inflating: all-texts/12378.txt     \n",
      "  inflating: all-texts/12380.txt     \n",
      "  inflating: all-texts/12381.txt     \n",
      "  inflating: all-texts/12383.txt     \n",
      "  inflating: all-texts/12384.txt     \n",
      "  inflating: all-texts/12385.txt     \n",
      "  inflating: all-texts/12386.txt     \n",
      "  inflating: all-texts/1jcfs10.txt   \n",
      "  inflating: all-texts/2babb10.txt   \n",
      "  inflating: all-texts/3babb10.txt   \n",
      "  inflating: all-texts/50bab10.txt   \n",
      "  inflating: all-texts/ajtl10.txt    \n",
      "  inflating: all-texts/allyr10.txt   \n",
      "  inflating: all-texts/alpsn10.txt   \n",
      "  inflating: all-texts/balen10.txt   \n",
      "  inflating: all-texts/baleng2.txt   \n",
      "  inflating: all-texts/batlf10.txt   \n",
      "  inflating: all-texts/bgopr10.txt   \n",
      "  inflating: all-texts/brnte10.txt   \n",
      "  inflating: all-texts/bstjg10.txt   \n",
      "  inflating: all-texts/cambp10.txt   \n",
      "  inflating: all-texts/canbe10.txt   \n",
      "  inflating: all-texts/cantp10.txt   \n",
      "  inflating: all-texts/cfrz10.txt    \n",
      "  inflating: all-texts/crsnk10.txt   \n",
      "  inflating: all-texts/esbio10.txt   \n",
      "  inflating: all-texts/grybr10.txt   \n",
      "  inflating: all-texts/mklmt10.txt   \n",
      "  inflating: all-texts/morem10.txt   \n",
      "  inflating: all-texts/mspcd10.txt   \n",
      "  inflating: all-texts/penbr10.txt   \n",
      "  inflating: all-texts/pgjr10.txt    \n",
      "  inflating: all-texts/pntvw10.txt   \n",
      "  inflating: all-texts/prcpg10.txt   \n",
      "  inflating: all-texts/prhg10.txt    \n",
      "  inflating: all-texts/prhsb10.txt   \n",
      "  inflating: all-texts/rlsl110.txt   \n",
      "  inflating: all-texts/rlsl210.txt   \n",
      "  inflating: all-texts/rmlav10.txt   \n",
      "  inflating: all-texts/sesli10.txt   \n",
      "  inflating: all-texts/svyrd10.txt   \n",
      "  inflating: all-texts/tecom10.txt   \n",
      "  inflating: all-texts/utrkj10.txt   \n",
      "  inflating: all-texts/vpasm10.txt   \n",
      "  inflating: all-texts/wldsp10.txt   \n",
      "  inflating: all-texts/wtrbs10.txt   \n",
      "  inflating: all-texts/zncli10.txt   \n"
     ]
    }
   ],
   "source": [
    "!unzip -d all-texts texts.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computers / CPU cores / Max jobs to run\n",
      "1:local / 8 / 8\n",
      "\n",
      "Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete\n",
      "ETA: 0s Left: 0 AVG: 0.04s  local:0/108/100%/0.0s \n",
      "\n",
      "real\t0m3.924s\n",
      "user\t0m23.976s\n",
      "sys\t0m1.810s\n"
     ]
    }
   ],
   "source": [
    "!time ls all-texts/*.txt \\\n",
    "    | parallel --eta -j+0 \"grep -oE '\\w{1,}' {} | tr '[:upper:]' '[:lower:]' | grep -v -oE '^s|t|m|l|ll|d$' | ./stopwords.py >> all-words.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above line, I've limited the word size to one character, removed common contractions, and piped the overall result through the new `stopwords.py` Python filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  831140 all-words.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l all-words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19589 one\r\n",
      "12235 up\r\n",
      "9508 very\r\n",
      "9503 now\r\n",
      "8539 upon\r\n",
      "7101 over\r\n",
      "6892 down\r\n",
      "6749 day\r\n",
      "6671 work\r\n",
      "6670 know\r\n",
      "6270 before\r\n",
      "6107 here\r\n",
      "5874 1\r\n",
      "5666 never\r\n",
      "5591 way\r\n",
      "5466 go\r\n",
      "4760 even\r\n",
      "4689 new\r\n",
      "4614 back\r\n",
      "4312 again\r\n",
      "4310 away\r\n",
      "4264 being\r\n",
      "3991 don\r\n",
      "3949 under\r\n",
      "3903 house\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n",
      "\r\n",
      "real\t0m8.251s\r\n",
      "user\t0m8.268s\r\n",
      "sys\t0m0.037s\r\n"
     ]
    }
   ],
   "source": [
    "!time sort all-words.txt | uniq -c | sort -rn | head -25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have the top 25 words across all 109 texts, with common English stop words removed.  It is easy to imagine that a broader set of stop words would remove many more common words like \"one\", \"up\", \"down\", and \"here\", but that is the tradeoff of choosing any single list."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python [root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
