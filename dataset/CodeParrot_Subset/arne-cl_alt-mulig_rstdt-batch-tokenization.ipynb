{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization of RST-DT files using off-the-shelf tokenizers\n",
    "\n",
    "* CoreNLP: failed\n",
    "* nltk's TreebankWordTokenizer: failed, but might be adaptable\n",
    "* let's try the preprocessing provided by the [Educational Testing Service](https://github.com/EducationalTestingService)'s [RST discourse parser](https://github.com/EducationalTestingService/discourse-parsing),  \n",
    "  cf. [rstdt-fixing-tokenization.ipynb](rstdt-fixing-tokenization.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named stanford_corenlp_pywrapper",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b2b6b3b69279>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mstanford_corenlp_pywrapper\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msockwrap\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mCORENLP_PYWRAPPER_DIR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'~/repos/stanford_corenlp_pywrapper'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m jars = (\"stanford-corenlp-full-2014-08-27/stanford-corenlp-3.4.1.jar\",\n\u001b[0;32m      5\u001b[0m         \"stanford-corenlp-full-2014-08-27/stanford-corenlp-3.4.1-models.jar\")\n",
      "\u001b[1;31mImportError\u001b[0m: No module named stanford_corenlp_pywrapper"
     ]
    }
   ],
   "source": [
    "from stanford_corenlp_pywrapper import sockwrap\n",
    "\n",
    "CORENLP_PYWRAPPER_DIR = os.path.expanduser('~/repos/stanford_corenlp_pywrapper')\n",
    "jars = (\"stanford-corenlp-full-2014-08-27/stanford-corenlp-3.4.1.jar\",\n",
    "        \"stanford-corenlp-full-2014-08-27/stanford-corenlp-3.4.1-models.jar\")\n",
    "\n",
    "p=sockwrap.SockWrap(\"pos\",\n",
    "                    corenlp_jars=[os.path.join(CORENLP_PYWRAPPER_DIR, jar) for jar in jars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import discoursegraphs as dg\n",
    "\n",
    "# a string enclosed in '_!', possibly with '<P>' before the closing '_!' \n",
    "RST_DIS_TEXT_REGEX = re.compile(\"_!(.*?)(\\<P\\>)?_!\", re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-4ec65294dfcd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcorenlp_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\"that its money would be better spent \"in areas such as research\" and development.\"\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorenlp_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentences'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokens'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "corenlp_result = p.parse_doc(\"\"\"that its money would be better spent \"in areas such as research\" and development.\"\"\")\n",
    "\n",
    "print ' '.join(tok for sent in corenlp_result['sentences'] for tok in sent['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "RSTDT_MAIN_ROOT = os.path.expanduser('~/repos/rst_discourse_treebank/data/RSTtrees-WSJ-main-1.0')\n",
    "RSTDT_TOKENIZED_ROOT = os.path.expanduser('~/repos/rst_discourse_treebank/data/RSTtrees-WSJ-main-1.0-tokenized')\n",
    "\n",
    "RSTDT_TEST_FILE = os.path.join(RSTDT_MAIN_ROOT, 'TEST', 'wsj_1306.out.dis')\n",
    "\n",
    "def tokenize_rst_file(rst_input_path, rst_output_path):\n",
    "#     edus = {}\n",
    "    with open(rst_input_path, 'r') as rstfile, codecs.open(rst_output_path, 'w', encoding='utf-8') as outfile:\n",
    "        rstfile_str = rstfile.read()\n",
    "        input_file_onset = 0\n",
    "        edu_matches = RST_DIS_TEXT_REGEX.finditer(rstfile_str)\n",
    "\n",
    "        for edu in edu_matches:\n",
    "            doc_onset = edu.start()\n",
    "            doc_offset = edu.end()\n",
    "            doc_untokenized_str = edu.groups()[0]\n",
    "            corenlp_result = p.parse_doc(doc_untokenized_str)\n",
    "            corenlp_tokenized_str = u' '.join(tok for sent in corenlp_result['sentences'] for tok in sent['tokens'])\n",
    "            outfile.write(rstfile_str[input_file_onset:doc_onset])\n",
    "            outfile.write(u'\"{}\"'.format(corenlp_tokenized_str))\n",
    "            input_file_onset = doc_offset\n",
    "        outfile.write(rstfile_str[input_file_onset:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-6d2e76672065>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#     print f.read()[325]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtokenize_rst_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRSTDT_TEST_FILE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'/tmp/1306.dis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-41190de0ed14>\u001b[0m in \u001b[0;36mtokenize_rst_file\u001b[1;34m(rst_input_path, rst_output_path)\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mdoc_offset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0medu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mdoc_untokenized_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0medu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mcorenlp_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_untokenized_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[0mcorenlp_tokenized_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mu' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtok\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorenlp_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentences'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokens'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0moutfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrstfile_str\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_file_onset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdoc_onset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "# with open(RSTDT_TEST_FILE, 'r') as f:\n",
    "#     print f.read()[325]\n",
    "\n",
    "tokenize_rst_file(RSTDT_TEST_FILE, '/tmp/1306.dis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for folder in ('TEST', 'TRAINING'):\n",
    "    for rst_fpath in glob.glob(os.path.join(RSTDT_MAIN_ROOT, folder, '*.dis')):\n",
    "        out_fpath = os.path.join(RSTDT_TOKENIZED_ROOT, folder, os.path.basename(rst_fpath))\n",
    "        out_dir, _fname = os.path.split(out_fpath)\n",
    "        dg.util.create_dir(out_dir)\n",
    "        tokenize_rst_file(rst_fpath, out_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenize using nltk.tokenize.treebank.TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['that',\n",
       " 'its',\n",
       " 'money',\n",
       " 'would',\n",
       " 'be',\n",
       " 'better',\n",
       " 'spent',\n",
       " '``',\n",
       " 'in',\n",
       " 'areas',\n",
       " 'such',\n",
       " 'as',\n",
       " 'research',\n",
       " \"''\",\n",
       " 'and',\n",
       " 'development',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(\"\"\"that its money would be better spent \"in areas such as research\" and development.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "ENDS_WITH_COMMA = re.compile('(.*),$')\n",
    "ENDS_WITH_PUNCTUATION = re.compile('(.*)(,|.|!|:|;)$')\n",
    "\n",
    "foo = \"Cummins Engine Co. , Columbus , Ind.,\"\n",
    "bar = ENDS_WITH_COMMA.sub(r'\\1 ,', foo)\n",
    "\n",
    "BRACKETS = {\n",
    "    '(': '-LRB-', # round brackets\n",
    "    ')': '-RRB-',\n",
    "    '[': '-LSB-', # square brackets\n",
    "    ']': '-RSB-',\n",
    "    '{': '-LCB-', # curly brackets\n",
    "    '}': '-RCB-'\n",
    "}\n",
    "\n",
    "def fix_tokenized_sentence(tokenized_sentence):\n",
    "    # If an EDU ends with a comma, we'll have to tokenize it,\n",
    "    # e.g. \"when it ends,\" -> \"when it ends ,\"\n",
    "    tokenized_sentence[-1] = ENDS_WITH_PUNCTUATION.sub(r'\\1 \\2', tokenized_sentence[-1])\n",
    "    for i, token in enumerate(tokenized_sentence):\n",
    "        if token in BRACKETS:\n",
    "            tokenized_sentence[i] = BRACKETS[token]\n",
    "    return tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Cummins Engine Co. , Columbus , Ind.', ',')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENDS_WITH_PUNCTUATION = re.compile('(.*)(,|\\.|!|:|;)$')\n",
    "\n",
    "ENDS_WITH_PUNCTUATION.match(foo).groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "TOKENIZER = TreebankWordTokenizer()\n",
    "\n",
    "def tokenize_rst_file_with_nltk(rst_input_path, rst_output_path, tokenizer):\n",
    "#     edus = {}\n",
    "    with open(rst_input_path, 'r') as rstfile, codecs.open(rst_output_path, 'w', encoding='utf-8') as outfile:\n",
    "        rstfile_str = rstfile.read()\n",
    "        input_file_onset = 0\n",
    "        edu_matches = RST_DIS_TEXT_REGEX.finditer(rstfile_str)\n",
    "\n",
    "        for edu in edu_matches:\n",
    "            doc_onset = edu.start()\n",
    "            doc_offset = edu.end()\n",
    "            doc_untokenized_str = edu.groups()[0]\n",
    "            untokenized_sents = sent_tokenize(doc_untokenized_str)\n",
    "            tokenized_sents = tokenizer.tokenize_sents(untokenized_sents)\n",
    "            fixed_tokenized_sents = [fix_tokenized_sentence(sent) for sent in tokenized_sents]\n",
    "            tokenized_str = u' '.join(tok for sent in fixed_tokenized_sents for tok in sent)\n",
    "\n",
    "            outfile.write(rstfile_str[input_file_onset:doc_onset])\n",
    "            outfile.write(u'\"{}\"'.format(tokenized_str))\n",
    "            input_file_onset = doc_offset\n",
    "        outfile.write(rstfile_str[input_file_onset:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.18 s, sys: 864 ms, total: 4.04 s\n",
      "Wall time: 4.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "RSTDT_NLTK_TOKENIZED_ROOT = os.path.expanduser('~/repos/rst_discourse_treebank/data/RSTtrees-WSJ-main-1.0-nltk-tokenized')\n",
    "\n",
    "for folder in ('TEST', 'TRAINING'):\n",
    "    for rst_fpath in glob.glob(os.path.join(RSTDT_MAIN_ROOT, folder, '*.dis')):\n",
    "        out_fpath = os.path.join(RSTDT_NLTK_TOKENIZED_ROOT, folder, os.path.basename(rst_fpath))\n",
    "        out_dir, _fname = os.path.split(out_fpath)\n",
    "        dg.util.create_dir(out_dir)\n",
    "        tokenize_rst_file_with_nltk(rst_fpath, out_fpath, TOKENIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['on',\n",
       " 'Monday',\n",
       " 'the',\n",
       " 'small',\n",
       " '(',\n",
       " 'investors',\n",
       " ')',\n",
       " 'are',\n",
       " 'going',\n",
       " 'to',\n",
       " 'panic',\n",
       " 'and',\n",
       " 'sell']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENIZER.tokenize(\"on Monday the small ( investors ) are going to panic and sell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank.  \n",
    "This is the method that is invoked by word_tokenize().  \n",
    "It assumes that the text has already been segmented into sentences, e.g. using sent_tokenize()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sents = sent_tokenize(\"a tree. You are a ball.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'a tree . You are a ball .'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sents = TOKENIZER.tokenize_sents(sents)\n",
    "u' '.join(tok for sent in tokenized_sents for tok in sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
