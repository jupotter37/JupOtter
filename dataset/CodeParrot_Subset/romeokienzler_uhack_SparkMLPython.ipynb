{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ibmos2spark\n",
    "\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'auth_url': 'https://identity.open.softlayer.com',\n",
    "    'project_id': '6aaf54352357483486ee2d4981f8ef15',\n",
    "    'region': 'dallas',\n",
    "    'user_id': 'b160340071b3407ca50c6b9a46b0bb25',\n",
    "    'username': 'member_b092a5c6f5c11f819059a83dfbd5d922b8a2299b',\n",
    "    'password': 'qwN4Y5EM*0KuZck['\n",
    "}\n",
    "\n",
    "configuration_name = 'os_d3bd5b94a9334de59a55a7fed2bedeaa_configs'\n",
    "bmos = ibmos2spark.bluemix(sc, credentials, configuration_name)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Please read the documentation of PySpark to learn more about the possibilities to load data files.\n",
    "# PySpark documentation: https://spark.apache.org/docs/2.0.1/api/python/pyspark.sql.html#pyspark.sql.SparkSession\n",
    "# The SparkSession object is already initalized for you.\n",
    "# The following variable contains the path to your file on your Object Storage.\n",
    "path_1 = bmos.url('dwlive', 'part-00000-ddf48c42-d9ea-4d72-aa4e-0b879587b372.snappy.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_2 = bmos.url('dwlive', 'part-00000-abf604d6-bc0a-4ea3-8ffa-7838c5912fa2.snappy.parquet')\n",
    "df_categorical = spark.read.parquet(path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_numeric = spark.read.parquet(path_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_categorical.createOrReplaceTempView(\"dfcat\")\n",
    "dfcat = spark.sql(\"select Id, L0_S22_F545 from dfcat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_numeric.createOrReplaceTempView(\"dfnum\")\n",
    "dfnum = spark.sql(\"select Id,L0_S0_F0,L0_S0_F2,L0_S0_F4,Response from dfnum\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = dfcat.join(dfnum,\"Id\")\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "df_notnull = spark.sql(\"\"\"\n",
    "select\n",
    "    Response as label,\n",
    "    case \n",
    "       when L0_S22_F545 is null then 'NA'\n",
    "       when L0_S22_F545 = '' then 'NA' \n",
    "       else L0_S22_F545 end as L0_S22_F545, \n",
    "    case\n",
    "       when L0_S0_F0 is null then 0.0 \n",
    "       else L0_S0_F0 end as L0_S0_F0, \n",
    "    case\n",
    "       when L0_S0_F2 is null then 0.0 \n",
    "       else L0_S0_F2 end as L0_S0_F2,\n",
    "    case\n",
    "       when L0_S0_F4 is null then 0.0 \n",
    "       else L0_S0_F4 end as L0_S0_F4\n",
    "from df\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer() \\\n",
    "  .setInputCol(\"L0_S22_F545\") \\\n",
    "  .setOutputCol(\"L0_S22_F545Index\")\n",
    "\n",
    "indexed = indexer.setHandleInvalid(\"skip\").fit(df_notnull).transform(df_notnull)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexed.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexed.select(\"L0_S22_F545Index\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexed.select(\"L0_S22_F545Index\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder().setInputCol(\"L0_S22_F545Index\").setOutputCol(\"L0_S22_F545Vec\")\n",
    "encoded = encoder.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded.first().L0_S22_F545Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "transformers = [\n",
    "    indexer,\n",
    "    encoder,\n",
    "    VectorAssembler()\n",
    "        .setInputCols([\"L0_S22_F545Vec\", \"L0_S0_F0\", \"L0_S0_F2\",\"L0_S0_F4\"])\n",
    "        .setOutputCol(\"features\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline().setStages(transformers).fit(df_notnull)\n",
    "\n",
    "transformed = pipeline.transform(df_notnull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformed.first().features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf =RandomForestClassifier() \\\n",
    "  .setLabelCol(\"label\") \\\n",
    "  .setFeaturesCol(\"features\")\n",
    "\n",
    "model = Pipeline().setStages([rf]).fit(transformed)\n",
    "\n",
    "result = model.transform(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pixiedust": {
     "displayParams": {
      "handlerId": "dataframe"
     }
    }
   },
   "outputs": [],
   "source": [
    "import pixiedust\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluation = evaluator.evaluate(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [3,5]) \\\n",
    "    .addGrid(rf.featureSubsetStrategy, [\"auto\",\"all\"]) \\\n",
    "    .addGrid(rf.impurity, [\"gini\",\"entropy\"]) \\\n",
    "    .addGrid(rf.maxBins, [2,5]) \\\n",
    "    .addGrid(rf.maxDepth, [3,5]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformed_sampled = transformed.sample(False,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_sampled.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(transformed)\n",
    "# this will likely take a fair amount of time because of the amount of models that we're creating and testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "/**var paramGrid = new ParamGridBuilder()\n",
    "    .addGrid(rf.numTrees, 3 :: 5 :: 10 :: 30 :: 50 :: 70 :: 100 :: 150 :: Nil)\n",
    "    .addGrid(rf.featureSubsetStrategy, \"auto\" :: \"all\" :: \"sqrt\" :: \"log2\" :: \"onethird\" :: Nil)\n",
    "    .addGrid(rf.impurity, \"gini\" :: \"entropy\" :: Nil)    \n",
    "    .addGrid(rf.maxBins, 2 :: 5 :: 10 :: 15 :: 20 :: 25 :: 30 :: Nil)\n",
    "    .addGrid(rf.maxDepth, 3 :: 5 :: 10 :: 15 :: 20 :: 25 :: 30 :: Nil)\n",
    "    .build()*/\n",
    "\n",
    "var paramGrid = new ParamGridBuilder().\n",
    "    addGrid(rf.numTrees, 3 :: 5 :: 10 :: Nil).\n",
    "    addGrid(rf.featureSubsetStrategy, \"auto\" :: \"all\" :: Nil).\n",
    "    addGrid(rf.impurity, \"gini\" :: \"entropy\" :: Nil).    \n",
    "    addGrid(rf.maxBins, 2 :: 5 :: Nil).\n",
    "    addGrid(rf.maxDepth, 3 :: 5 :: Nil).\n",
    "    build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//Model is created\n",
    "    var crossValidatorModel = crossValidator.fit(df_notnull)\n",
    "    //Model used to Predict\n",
    "    var newPredictions = crossValidatorModel.transform(df_notnull)\n",
    "\n",
    "\n",
    "    var newAucTest = evaluator.evaluate(newPredictions, evaluatorParamMap)\n",
    "    println(\"new AUC (with Cross Validation) \" + newAucTest)\n",
    "    var bestModel = crossValidatorModel.bestModel\n",
    "\n",
    "    //Understand the Model selected\n",
    "    println()\n",
    "    println(\"Parameters for Best Model:\")\n",
    "\n",
    "    var bestPipelineModel = crossValidatorModel.bestModel.asInstanceOf[PipelineModel]\n",
    "    var stages = bestPipelineModel.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.RandomForestClassificationModel\n",
    "    val rfStage = stages(stages.length-1).asInstanceOf[RandomForestClassificationModel]\n",
    "rfStage.getNumTrees\n",
    "rfStage.getFeatureSubsetStrategy\n",
    "rfStage.getImpurity\n",
    "rfStage.getMaxBins\n",
    "rfStage.getMaxDepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.1",
   "language": "python",
   "name": "python2-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
