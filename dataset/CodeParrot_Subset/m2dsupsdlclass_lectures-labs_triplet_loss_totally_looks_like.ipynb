{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Loss on Totally Looks Like dataset\n",
    "\n",
    "This notebook is inspired from [this Keras tutorial](https://keras.io/examples/vision/siamese_network/) by Hazem Essam and Santiago L. Valdarrama.\n",
    "\n",
    "The goal is to showcase the use of siamese networks and triplet loss to do representation learning using a CNN. It will also showcase data generators and data augmentation techniques.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The dataset considered is the [Totally Looks Like](https://sites.google.com/view/totally-looks-like-dataset) dataset, consisting of pairs of web curated similar looking images:\n",
    "\n",
    "Image pair 1               |  Image pair 2\n",
    ":-------------------------:|:-------------------------:\n",
    "![](https://github.com/m2dsupsdlclass/lectures-labs/raw/master/labs/09_triplet_loss/example1.jpg)  |  ![](https://github.com/m2dsupsdlclass/lectures-labs/raw/master/labs/09_triplet_loss/example2.jpg)\n",
    "\n",
    "The goal is to extract generic human perceptual representation through a CNN. The next cell downloads the dataset and unzips it (run it asap, it will download a few hundead megabytes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "from urllib.request import urlretrieve\n",
    "from pathlib import Path\n",
    "\n",
    "URL = \"https://github.com/m2dsupsdlclass/lectures-labs/releases/download/totallylookslike/dataset_totally.zip\"\n",
    "FILENAME = \"dataset_totally.zip\"\n",
    "\n",
    "if not op.exists(FILENAME):\n",
    "    print('Downloading %s to %s...' % (URL, FILENAME))\n",
    "    urlretrieve(URL, FILENAME)\n",
    "\n",
    "import zipfile\n",
    "if not op.exists(\"anchors\"):\n",
    "    print('Extracting image files...')\n",
    "    with zipfile.ZipFile(FILENAME, 'r') as zip_ref:\n",
    "        zip_ref.extractall('.')\n",
    "\n",
    "home_dir = Path(Path.home())\n",
    "anchor_images_path = Path(\"./anchors\")\n",
    "positive_images_path = Path(\"./positives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use mostly TensorFlow functions to open and process images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_image(filename, target_shape = (256, 256)):\n",
    "    \"\"\" Load the specified file as a JPEG image, preprocess it and\n",
    "    resize it to the target shape.\n",
    "    \"\"\"\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize(image, target_shape)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Careful to sort images folders so that the anchor and positive images correspond.\n",
    "anchor_images = sorted([str(anchor_images_path / f) for f in os.listdir(anchor_images_path)])\n",
    "positive_images = sorted([str(positive_images_path / f) for f in os.listdir(positive_images_path)])\n",
    "\n",
    "anchor_count = len(anchor_images)\n",
    "positive_count = len(positive_images)\n",
    "\n",
    "print(f\"number of anchors: {anchor_count}, positive: {positive_count}\")\n",
    "\n",
    "anchor_dataset_files = tf.data.Dataset.from_tensor_slices(anchor_images)\n",
    "anchor_dataset = anchor_dataset_files.map(open_image)\n",
    "positive_dataset_files = tf.data.Dataset.from_tensor_slices(positive_images)\n",
    "positive_dataset = positive_dataset_files.map(open_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def visualize(img_list):\n",
    "    \"\"\"Visualize a list of images\"\"\"\n",
    "    def show(ax, image):\n",
    "        ax.imshow(image)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 18))\n",
    "    \n",
    "    num_imgs = len(img_list)\n",
    "    \n",
    "    axs = fig.subplots(1, num_imgs)\n",
    "    for i in range(num_imgs):\n",
    "        show(axs[i], img_list[i])\n",
    "\n",
    "# display the first element of our dataset\n",
    "anc = next(iter(anchor_dataset))\n",
    "pos = next(iter(positive_dataset))\n",
    "visualize([anc, pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# data augmentations\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    # layers.RandomRotation(0.15), # you may add random rotations\n",
    "    layers.RandomCrop(224, 224)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the list of negative images, let's randomize the list of available images (anchors and positives) and concatenate them together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "rng = np.random.RandomState(seed=42)\n",
    "rng.shuffle(anchor_images)\n",
    "rng.shuffle(positive_images)\n",
    "\n",
    "negative_images = anchor_images + positive_images\n",
    "np.random.RandomState(seed=32).shuffle(negative_images)\n",
    "\n",
    "negative_dataset_files = tf.data.Dataset.from_tensor_slices(negative_images)\n",
    "negative_dataset_files = negative_dataset_files.shuffle(buffer_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final triplet dataset\n",
    "dataset = tf.data.Dataset.zip((anchor_dataset_files, positive_dataset_files, negative_dataset_files))\n",
    "dataset = dataset.shuffle(buffer_size=1024)\n",
    "\n",
    "# preprocess function\n",
    "def preprocess_triplets(anchor, positive, negative):\n",
    "    return (\n",
    "        data_augmentation(open_image(anchor)),\n",
    "        data_augmentation(open_image(positive)),\n",
    "        data_augmentation(open_image(negative)),\n",
    "    )\n",
    "\n",
    "# The map function is lazy, it is not evaluated on the spot, \n",
    "# but each time a batch is sampled.\n",
    "dataset = dataset.map(preprocess_triplets)\n",
    "\n",
    "# Let's now split our dataset in train and validation.\n",
    "train_dataset = dataset.take(round(anchor_count * 0.8))\n",
    "val_dataset = dataset.skip(round(anchor_count * 0.8))\n",
    "\n",
    "# define the batch size\n",
    "train_dataset = train_dataset.batch(32, drop_remainder=False)\n",
    "train_dataset = train_dataset.prefetch(8)\n",
    "\n",
    "val_dataset = val_dataset.batch(32, drop_remainder=False)\n",
    "val_dataset = val_dataset.prefetch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize a triplet and display its shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anc_batch, pos_batch, neg_batch = next(train_dataset.take(1).as_numpy_iterator())\n",
    "print(anc_batch.shape, pos_batch.shape, neg_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, 32)\n",
    "visualize([anc_batch[idx], pos_batch[idx], neg_batch[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Build the embedding network, starting from a resnet and adding a few layers. The output should have a dimension $d= 128$ or $d=256$. Edit the following code, and you may use the next cell to test your code.\n",
    "\n",
    "Bonus: Try to freeze the weights of the ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model, layers\n",
    "from tensorflow.keras import optimizers, losses, metrics, applications\n",
    "from tensorflow.keras.applications import resnet\n",
    "\n",
    "input_img = layers.Input((224,224,3))\n",
    "\n",
    "output = input_img # change that line and edit this code!\n",
    "\n",
    "embedding = Model(input_img, output, name=\"Embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = embedding(np.random.randn(1,224,224,3))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following can be run to get the same architecture as we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model, layers\n",
    "from tensorflow.keras import optimizers, losses, metrics, applications\n",
    "from tensorflow.keras.applications import resnet\n",
    "\n",
    "input_img = layers.Input((224,224,3))\n",
    "\n",
    "base_cnn = resnet.ResNet50(weights=\"imagenet\", input_shape=(224,224,3), include_top=False)\n",
    "resnet_output = base_cnn(input_img)\n",
    "\n",
    "flatten = layers.Flatten()(resnet_output)\n",
    "dense1 = layers.Dense(512, activation=\"relu\")(flatten)\n",
    "# The batch normalization layer enables to normalize the activations\n",
    "# over the batch\n",
    "dense1 = layers.BatchNormalization()(dense1)\n",
    "dense2 = layers.Dense(256, activation=\"relu\")(dense1)\n",
    "dense2 = layers.BatchNormalization()(dense2)\n",
    "output = layers.Dense(256)(dense2)\n",
    "\n",
    "embedding = Model(input_img, output, name=\"Embedding\")\n",
    "\n",
    "trainable = False\n",
    "for layer in base_cnn.layers:\n",
    "    if layer.name == \"conv5_block1_out\":\n",
    "        trainable = True\n",
    "    layer.trainable = trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    \"\"\" we'll need to preprocess the input before passing them\n",
    "    to the resnet for better results. This is the same preprocessing\n",
    "    that was used during the training of ResNet on ImageNet.\n",
    "    \"\"\"\n",
    "    return resnet.preprocess_input(x * 255.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Our goal is now to build the positive and negative distances from 3 inputs images: the anchor, the positive, and the negative one $‖f(A) - f(P)‖²$ $‖f(A) - f(N)‖²$. You may define a specific Layer using the [Keras subclassing API](https://keras.io/guides/making_new_layers_and_models_via_subclassing/), or any other method.\n",
    "\n",
    "You will need to run the Embedding model previously defined, don't forget to apply the preprocessing function defined above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_input = layers.Input(name=\"anchor\", shape=(224, 224, 3))\n",
    "positive_input = layers.Input(name=\"positive\", shape=(224, 224, 3))\n",
    "negative_input = layers.Input(name=\"negative\", shape=(224, 224, 3))\n",
    "\n",
    "distances = [anchor_input, positive_input] # TODO: Change this code to actually compute the distances\n",
    "\n",
    "siamese_network = Model(\n",
    "    inputs=[anchor_input, positive_input, negative_input], outputs=distances\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: run the following cell to get the exact same method as we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceLayer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
    "        return (ap_distance, an_distance)\n",
    "\n",
    "\n",
    "anchor_input = layers.Input(name=\"anchor\", shape=(224, 224, 3))\n",
    "positive_input = layers.Input(name=\"positive\", shape=(224, 224, 3))\n",
    "negative_input = layers.Input(name=\"negative\", shape=(224, 224, 3))\n",
    "\n",
    "distances = DistanceLayer()(\n",
    "    embedding(preprocess(anchor_input)),\n",
    "    embedding(preprocess(positive_input)),\n",
    "    embedding(preprocess(negative_input)),\n",
    ")\n",
    "\n",
    "siamese_network = Model(\n",
    "    inputs=[anchor_input, positive_input, negative_input], outputs=distances\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The final triplet model\n",
    "Once we are able to produce the distances, we may wrap it into a new Keras Model which includes the computation of the loss. The following implementation uses a subclassing of the Model class, redefining a few functions used internally during `model.fit`: `call`, `train_step`, `test_step` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletModel(Model):\n",
    "    \"\"\"The Final Keras Model with a custom training and testing loops.\n",
    "\n",
    "    Computes the triplet loss using the three embeddings produced by the\n",
    "    Siamese Network.\n",
    "\n",
    "    The triplet loss is defined as:\n",
    "       L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, siamese_network, margin=0.5):\n",
    "        super(TripletModel, self).__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.siamese_network(inputs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # GradientTape is a context manager that records every operation that\n",
    "        # you do inside. We are using it here to compute the loss so we can get\n",
    "        # the gradients and apply them using the optimizer specified in\n",
    "        # `compile()`.\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._compute_loss(data)\n",
    "\n",
    "        # Storing the gradients of the loss function with respect to the\n",
    "        # weights/parameters.\n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "\n",
    "        # Applying the gradients on the model using the specified optimizer\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Let's update and return the training loss metric.\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self._compute_loss(data)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        # The output of the network is a tuple containing the distances\n",
    "        # between the anchor and the positive example, and the anchor and\n",
    "        # the negative example.\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "\n",
    "        loss = ap_distance - an_distance\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker]\n",
    "\n",
    "\n",
    "siamese_model = TripletModel(siamese_network)\n",
    "siamese_model.compile(optimizer=optimizers.Adam(0.0001))\n",
    "siamese_model.fit(train_dataset, epochs=10, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.save('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to get a pretrained model\n",
    "url_pretrained = \"https://github.com/m2dsupsdlclass/lectures-labs/releases/download/totallylookslike/best_model.h5\"\n",
    "urlretrieve(url_pretrained, \"best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find most similar images in test dataset\n",
    "\n",
    "The `negative_images` list was built by concatenating all possible images, both anchors and positive. We can reuse these to form a bank of possible images to query from.\n",
    "\n",
    "We will first compute all embeddings of these images. To do so, we build a `tf.Dataset` and apply the few functions: `open_img` and `preprocess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "open_img = partial(open_image, target_shape=(224,224))\n",
    "all_img_files = tf.data.Dataset.from_tensor_slices(negative_images)\n",
    "dataset = all_img_files.map(open_img).map(preprocess).take(1024).batch(32, drop_remainder=False).prefetch(8)\n",
    "all_embeddings = loaded_model.predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build a `most_similar` function which takes an image path as input and return the `topn` most similar images through the embedding representation. It would be possible to use another metric, such as the cosine similarity here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_img = np.random.choice(negative_images)\n",
    "\n",
    "def most_similar(img, topn=5):\n",
    "    img_batch = tf.expand_dims(open_image(img, target_shape=(224, 224)), 0)\n",
    "    new_emb = loaded_model.predict(preprocess(img_batch))\n",
    "    dists = tf.sqrt(tf.reduce_sum((all_embeddings - new_emb)**2, -1)).numpy()\n",
    "    idxs = np.argsort(dists)[:topn]\n",
    "    return [(negative_images[idx], dists[idx]) for idx in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random_img)\n",
    "most_similar(random_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_img = np.random.choice(negative_images)\n",
    "visualize([open_image(im) for im, _ in most_similar(random_img)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is not a rigorous evaluation, as we are using the images from the training set for both the query and the possible images. You may try with a completely different picture!\n",
    "\n",
    "### Going further\n",
    "\n",
    "In order to improve the training efficiency, hard negative mining would be most relevant in that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
