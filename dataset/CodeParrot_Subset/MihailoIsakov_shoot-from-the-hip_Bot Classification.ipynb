{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*-coding:utf-8-*-\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "from bald_latin import remove_cyrillic_and_accents as balden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the original and stemmed comments, and their labels.\n",
    "Remove the cyrillic comments and remove accents from ć,č,ž,š,đ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LnS comments and labels\n",
      "Loaded scraped comments\n",
      "Loaded scraped nots\n"
     ]
    }
   ],
   "source": [
    "# comments collected by the \"Lovac na sendvice\" app\n",
    "def load_comments_and_labels():\n",
    "    lns_comments = balden(open('dataset/lns/lns_comments.txt', 'r').readlines())\n",
    "    lns_stemmed = open('dataset/lns/lns_comments_stemmed.txt', 'r').readlines()\n",
    "    lns_labels = open('dataset/lns/lns_labels.txt', 'r').readlines()\n",
    "    \n",
    "    # remove cyrillic and accents on stemmed comments\n",
    "    lns_stemmed, lns_labels = balden(lns_stemmed, lns_labels)\n",
    "    \n",
    "    # labels as a numpy array\n",
    "    lns_labels = np.array([int(float(x)) for x in lns_labels])\n",
    "    \n",
    "    assert len(lns_comments) == len(lns_stemmed)\n",
    "    return lns_comments, lns_stemmed, lns_labels\n",
    "   \n",
    "def load_scraped():\n",
    "    # scraped comments from Blic.rs\n",
    "    scraped_comments, scraped_stemmed = balden(\n",
    "        open('dataset/scraped/comments.txt').readlines(),\n",
    "        open('dataset/scraped/comments_stemmed.txt').readlines())\n",
    "    \n",
    "    assert len(scraped_comments) == len(scraped_stemmed)\n",
    "    return scraped_comments, scraped_stemmed\n",
    "\n",
    "def load_scraped_not_category():\n",
    "    scraped_nots_comments, scraped_nots_stemmed = balden(\n",
    "        open('dataset/scraped/slobodno_vreme.txt').readlines(),\n",
    "        open('dataset/scraped/slobodno_vreme_stemmed.txt').readlines())\n",
    "    \n",
    "    assert len(scraped_nots_comments) == len(scraped_nots_stemmed)\n",
    "    return scraped_nots_comments, scraped_nots_stemmed\n",
    "\n",
    "lns_comments, lns_stemmed, lns_labels = load_comments_and_labels()\n",
    "print \"Loaded LnS comments and labels\"\n",
    "scraped_comments, scraped_stemmed = load_scraped()\n",
    "print \"Loaded scraped comments\"\n",
    "scraped_nots_comments, scraped_nots_stemmed = load_scraped_not_category()\n",
    "print \"Loaded scraped nots\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vectorizer, add methods for testing predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def build_vectorizer():\n",
    "    croatian_stop_words = set([u\"a\",u\"ako\",u\"ali\",u\"bi\",u\"bih\",u\"bila\",u\"bili\",u\"bilo\",u\"bio\",u\"bismo\",u\"biste\",u\"biti\",u\"bumo\",u\"da\",u\"do\",u\"duž\",u\"ga\",u\"hoće\",u\"hoćemo\",u\"hoćete\",u\"hoćeš\",u\"hoću\",u\"i\",u\"iako\",u\"ih\",u\"ili\",u\"iz\",u\"ja\",u\"je\",u\"jedna\",u\"jedne\",u\"jedno\",u\"jer\",u\"jesam\",u\"jesi\",u\"jesmo\",u\"jest\",u\"jeste\",u\"jesu\",u\"jim\",u\"joj\",u\"još\",u\"ju\",u\"kada\",u\"kako\",u\"kao\",u\"koja\",u\"koje\",u\"koji\",u\"kojima\",u\"koju\",u\"kroz\",u\"li\",u\"me\",u\"mene\",u\"meni\",u\"mi\",u\"mimo\",u\"moj\",u\"moja\",u\"moje\",u\"mu\",u\"na\",u\"nad\",u\"nakon\",u\"nam\",u\"nama\",u\"nas\",u\"naš\",u\"naša\",u\"naše\",u\"našeg\",u\"ne\",u\"nego\",u\"neka\",u\"neki\",u\"nekog\",u\"neku\",u\"nema\",u\"netko\",u\"neće\",u\"nećemo\",u\"nećete\",u\"nećeš\",u\"neću\",u\"nešto\",u\"ni\",u\"nije\",u\"nikoga\",u\"nikoje\",u\"nikoju\",u\"nisam\",u\"nisi\",u\"nismo\",u\"niste\",u\"nisu\",u\"njega\",u\"njegov\",u\"njegova\",u\"njegovo\",u\"njemu\",u\"njezin\",u\"njezina\",u\"njezino\",u\"njih\",u\"njihov\",u\"njihova\",u\"njihovo\",u\"njim\",u\"njima\",u\"njoj\",u\"nju\",u\"no\",u\"o\",u\"od\",u\"odmah\",u\"on\",u\"ona\",u\"oni\",u\"ono\",u\"ova\",u\"pa\",u\"pak\",u\"po\",u\"pod\",u\"pored\",u\"prije\",u\"s\",u\"sa\",u\"sam\",u\"samo\",u\"se\",u\"sebe\",u\"sebi\",u\"si\",u\"smo\",u\"ste\",u\"su\",u\"sve\",u\"svi\",u\"svog\",u\"svoj\",u\"svoja\",u\"svoje\",u\"svom\",u\"ta\",u\"tada\",u\"taj\",u\"tako\",u\"te\",u\"tebe\",u\"tebi\",u\"ti\",u\"to\",u\"toj\",u\"tome\",u\"tu\",u\"tvoj\",u\"tvoja\",u\"tvoje\",u\"u\",u\"uz\",u\"vam\",u\"vama\",u\"vas\",u\"vaš\",u\"vaša\",u\"vaše\",u\"već\",u\"vi\",u\"vrlo\",u\"za\",u\"zar\",u\"će\",u\"ćemo\",u\"ćete\",u\"ćeš\",u\"ću\",u\"što\"])\n",
    "\n",
    "    # build tf-idf vectorizer which uses unigrams and bigrams.\n",
    "    # uses words with 2+ occurances as features\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        strip_accents=\"unicode\",\n",
    "        lowercase=True,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=10,\n",
    "        norm='l2',\n",
    "        smooth_idf=True,\n",
    "        use_idf=True,\n",
    "        stop_words=croatian_stop_words)\n",
    "    \n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once we have picked a threshold for bot classification, lets classify the scraped comments\n",
    "### With the scraped comments, add the scraped not-bots and train an LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify_bots(text_train, y_train, unlabeled_stemmed, threshold=0.83):\n",
    "    \"\"\"\n",
    "    Train the classifier on text_train and y_train,\n",
    "    and label bots in the unlabeled stemmed comments, with probability above the threshold.\n",
    "    Return indices of bot comments\n",
    "    \"\"\"\n",
    "    # build the dataset, vectorize it using TF-IDF\n",
    "    vectorizer = build_vectorizer()\n",
    "    X_train = vectorizer.fit_transform(text_train)\n",
    "    X_unlabeled = vectorizer.transform(unlabeled_stemmed)\n",
    "        \n",
    "    # create and fit the classifier\n",
    "    clf = MultinomialNB().fit(X_train, y_train)\n",
    "    \n",
    "    # predict on the unlabeled set\n",
    "    y_pred = clf.predict_proba(X_unlabeled)[:,1]\n",
    "    # select comments with very high or low probabilities\n",
    "    bot_indices = np.argwhere(y_pred > threshold)\n",
    "    \n",
    "    return bot_indices\n",
    "\n",
    "def build_large_comment_set(lns_comments, lns_labels, scraped_bots, scraped_nots, weight=0.1):\n",
    "    \"\"\"\n",
    "    Build dataset from the original unstemmed 'Lovac na Sendvice' comments, \n",
    "    newly classified bot comments, and manually tagged not comments.\n",
    "    \"\"\"\n",
    "    combined_comments = lns_comments + scraped_bots + scraped_nots\n",
    "    combined_labels = list(lns_labels) + list(np.ones(len(scraped_bots))) + list(np.zeros(len(scraped_nots)))\n",
    "    combined_labels = np.array(combined_labels)\n",
    "    # the weigths of the new samples are decreased\n",
    "    combined_weights = np.ones(len(combined_labels))\n",
    "    combined_weights[len(lns_comments):] = weight\n",
    "    \n",
    "    return combined_comments, combined_labels, combined_weights\n",
    " \n",
    "\n",
    "def comments2matrix(comments):\n",
    "    \"\"\"\n",
    "    Prepare the comments to be fed to the LSTM\n",
    "    \"\"\"\n",
    "    def remove_symbols(comments):\n",
    "        # replace characters, reduce set \n",
    "        bad_chars = ['\\n', '\\t', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
    "\n",
    "        def clean_comment(comment):\n",
    "            comment = comment.encode('ascii', errors='ignore').lower()\n",
    "            for bc in bad_chars:\n",
    "                comment = comment.replace(bc, ' ')\n",
    "            return comment\n",
    "\n",
    "        return map(clean_comment, comments)\n",
    "\n",
    "    def pad_comments(comments, size=100):\n",
    "        \"\"\"\n",
    "        Pad each comment to a *size* characters. Longer comments get cut off.\n",
    "        \"\"\"\n",
    "        def pad(comment):\n",
    "            comment = comment[:size]\n",
    "            comment = comment + \" \" * (size - len(comment)) \n",
    "            return comment\n",
    "\n",
    "        return map(pad, comments)\n",
    "\n",
    "    def one_hot(comments):\n",
    "        #char_set = list(set(\"\".join(comments)))\n",
    "        char_set = set([' ', '1', '0', '3', '2', '5', '4', '7', '6', '9', '8', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z'])\n",
    "        char_set_str = \"\".join(char_set)\n",
    "        char_set_len = len(char_set)\n",
    "\n",
    "        # TODO optimize? should use sparse matrices\n",
    "        # Currently X is < 1GB, which is kinda ok\n",
    "        X = np.zeros((len(comments), len(comments[0]), char_set_len))\n",
    "        for comment_ind, comment in enumerate(comments):\n",
    "            for char_ind, char in enumerate(comment):\n",
    "                X[comment_ind, char_ind, char_set_str.find(char)] = 1\n",
    "\n",
    "        return X \n",
    "    \n",
    "    # remove symbols, pad, and vectorize\n",
    "    return one_hot(pad_comments(remove_symbols(comments)))\n",
    "\n",
    "\n",
    "def y_one_hot(y):\n",
    "    \"\"\"\n",
    "    Two categories require two neurons in the output. Must convert y to a one-hot representation.\n",
    "    \"\"\"\n",
    "    one_hot = np.zeros((len(y), 2))\n",
    "    one_hot[np.arange(len(y)), np.round(y).astype(int)] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label a dataset from the scraped comments and train the network on it\n",
    "### For each fold, train a separate network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built the training dataset\n",
      "Classified bots using NB\n",
      "Built the training set for the LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mihailo/development/projects/botvis/.venv/lib/python2.7/site-packages/ipykernel/__main__.py:23: VisibleDeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized the training set\n"
     ]
    }
   ],
   "source": [
    "def build_training_and_test_set(lns_comments, lns_stemmed, lns_labels, unlabeled_comments, unlabeled_stemmed, scraped_nots):\n",
    "    \n",
    "    ratio = 0.8\n",
    "    # Split the dataset into k folds, each fold is a test set for one iteration\n",
    "    #train = range(int(len(lns_comments) * ratio))\n",
    "    #test = range(int(len(lns_comments) * ratio), len(lns_comments))\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    train, test = train_test_split(range(len(lns_comments)))\n",
    "    \n",
    "    text_train_stemmed = [lns_stemmed[x] for x in train]\n",
    "    text_train_comments = [lns_comments[x] for x in train]\n",
    "    text_test_stemmed  = [lns_stemmed[x] for x in test]\n",
    "    text_test_comments  = [lns_comments[x] for x in test]\n",
    "    y_train = [lns_labels[x] for x in train]\n",
    "    y_test  = [lns_labels[x] for x in test]\n",
    "        \n",
    "    print(\"Built the training dataset\")\n",
    "        \n",
    "    bot_indices = classify_bots(text_train_stemmed, y_train, unlabeled_stemmed)\n",
    "    print(\"Classified bots using NB\")\n",
    "    \n",
    "    # using the NB predictions, pull the bots from the original comments\n",
    "    classified_bots = [unlabeled_comments[x] for x in bot_indices]\n",
    "    # and take the same number of comments from the not categories\n",
    "    classified_nots = scraped_nots[:len(classified_bots)]\n",
    "        \n",
    "    # build a large dataset from LnS comments and NB labeled comments. NB comments have lower weigths.\n",
    "    comments_large, y_large, weigths_large = build_large_comment_set(\n",
    "        text_train_comments, \n",
    "        y_train, \n",
    "        classified_bots, \n",
    "        classified_nots) \n",
    "    \n",
    "    # shuffle text_train_comments and y_train\n",
    "    from sklearn.utils import shuffle\n",
    "    comments_large, y_large = shuffle(comments_large, y_large)\n",
    "    \n",
    "    y_large = y_one_hot(y_large)\n",
    "    y_test = y_one_hot(y_test)\n",
    "    print(\"Built the training set for the LSTM\")\n",
    "\n",
    "    X_large = comments2matrix(comments_large)\n",
    "    X_test = comments2matrix(text_test_comments)\n",
    "    print(\"Vectorized the training set\")\n",
    "    \n",
    "    return X_large, y_large, X_test, y_test\n",
    "\n",
    "    \n",
    "max_scraped = 3*10**6  # can't wait\n",
    "unlabeled_comments = scraped_comments[:max_scraped]\n",
    "unlabeled_stemmed = scraped_stemmed[:max_scraped]\n",
    "\n",
    "X_train, y_train, X_test, y_test = build_training_and_test_set(lns_comments, lns_stemmed, lns_labels, unlabeled_comments, unlabeled_stemmed, scraped_nots_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built the net\n"
     ]
    }
   ],
   "source": [
    "def build_net(shape, nb_filter=64, pool_length=2, lstm_output_size=128):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout, Embedding, LSTM, GRU, Input, merge, Activation,Convolution1D, MaxPooling1D, Flatten, Convolution2D\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Convolution1D(input_shape=(shape[1], shape[2]),\n",
    "                            nb_filter=16, filter_length=11, border_mode='valid', activation='relu', subsample_length=1))\n",
    "    model.add(Convolution1D(nb_filter=16, filter_length=11, border_mode='valid', activation='relu', subsample_length=1))\n",
    "    model.add(MaxPooling1D(pool_length=pool_length))\n",
    "    model.add(Convolution1D(nb_filter=32, filter_length=9, border_mode='valid', activation='relu', subsample_length=1))\n",
    "    model.add(Convolution1D(nb_filter=32, filter_length=9, border_mode='valid', activation='relu', subsample_length=1))\n",
    "    model.add(MaxPooling1D(pool_length=pool_length))\n",
    "    model.add(Convolution1D(nb_filter=64, filter_length=7, border_mode='valid', activation='relu', subsample_length=1))\n",
    "    model.add(Convolution1D(nb_filter=64, filter_length=7, border_mode='valid', activation='relu', subsample_length=1))\n",
    "    model.add(MaxPooling1D(pool_length=pool_length))\n",
    "    #model.add(Convolution1D(nb_filter=16, filter_length=5, border_mode='valid', activation='relu', subsample_length=1))\n",
    "    #model.add(Convolution1D(nb_filter=16, filter_length=5, border_mode='valid', activation='relu', subsample_length=1))\n",
    "    #model.add(MaxPooling1D(pool_length=pool_length))\n",
    "    #model.add(Convolution1D(nb_filter=16, filter_length=3, border_mode='valid', activation='relu', subsample_length=1))\n",
    "    #model.add(MaxPooling1D(pool_length=pool_length))\n",
    "    #model.add(Flatten())\n",
    "    #model.add(LSTM(lstm_output_size, return_sequences=True))\n",
    "    #model.add(LSTM(64, input_shape=(shape[1], shape[0]), dropout_U=0.5, dropout_W=0.3, return_sequences=True))\n",
    "    \n",
    "    model.add(LSTM(64, dropout_U=0.5, dropout_W=0.3, return_sequences=True))\n",
    "    #model.add(GRU(64, return_sequences=True, activation='softsign', input_shape=(shape[1], shape[2])))\n",
    "    #model.add(Flatten())\n",
    "    #model.add(Dropout(0.2))\n",
    "    #model.add(GRU(10, return_sequences=False, activation='softsign'))\n",
    "    model.add(LSTM(64, dropout_U=0.3, dropout_W=0.3, return_sequences=False))\n",
    "    #model.add(GRU(32, return_sequences=False))\n",
    "    model.add(Dense(20))\n",
    "    model.add(Dense(2))\n",
    "    # TODO replace with softmax\n",
    "    model.add(Activation('softmax'))\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    from keras.optimizers import SGD\n",
    "    #sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    #model.compile(sgd, 'binary_crossentropy', metrics=['accuracy'])\n",
    "    model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "net = build_net(X_train.shape)\n",
    "print(\"Built the net\")\n",
    "\n",
    "epoch = 100\n",
    "batch_size = 64\n",
    "max_samples = 20000\n",
    "\n",
    "for ep in range(epoch):\n",
    "    net.fit(X_train[:max_samples], y_train[:max_samples],\n",
    "        batch_size=batch_size,\n",
    "        nb_epoch=1,\n",
    "        validation_data=[X_test, y_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
