{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TO RE-RUN\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from time import time\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit, cross_val_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV, ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from imblearn.over_sampling import SMOTE,ADASYN, RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "from operator import truediv\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "from pylab import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "plt.style.use('classic')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, \"../src/\")\n",
    "from TypeFeatImputer import TypeFeatImputer\n",
    "from UnivCombineFilter import UnivCombineFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(typeEncounter, typeDiagnosis, typeDataFeatures):\n",
    "    if typeDataFeatures == \"minimum\":\n",
    "        df_all=pd.read_pickle(os.path.join('resources','clean_data_' + typeEncounter + \"_\" +  typeDiagnosis+ '_hyp_1.pkl'))\n",
    "    if typeDataFeatures == \"extended\":\n",
    "        df_all=pd.read_pickle(os.path.join('resources','clean_data_' + typeEncounter + \"_\" +  typeDiagnosis+ '_hyp_1_' + typeDataFeatures + '.pkl'))\n",
    "\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns(df_all, typeDiagnosis):\n",
    "    if typeDiagnosis == \"diag_1\":\n",
    "        colsDiseases = [u'Diabetis_1', u'Circulatory_1', u'Digestive_1', u'Genitourinary_1', u'Poisoning_1', u'Muscoskeletal_1',\n",
    "               u'Neoplasms_1', u'Respiratory_1']\n",
    "\n",
    "    if typeDiagnosis == \"diag_3\":\n",
    "        colsDiseases = [u'Diabetis_3', u'Circulatory_3', u'Digestive_3', u'Genitourinary_3', u'Poisoning_3', u'Muscoskeletal_3',\n",
    "               u'Neoplasms_3', u'Respiratory_3']\n",
    "    \n",
    "    colsNonDiseases = [c for c in df_all.columns if c not in colsDiseases]\n",
    "    \n",
    "    return colsDiseases, colsNonDiseases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data_by_class(df_all, typeHypothesis):\n",
    "    \n",
    "    # Readmitted none vs readmitted\n",
    "    if typeHypothesis == \"all_readmisssion_vs_none\":\n",
    "        df_all[\"readmitted\"][df_all[\"readmitted\"].values > 0] = 1\n",
    "\n",
    "    # Readmitted none vs early readmitted            \n",
    "    if typeHypothesis == \"early_readmission_vs_none\":\n",
    "        df_all= df_all[df_all[\"readmitted\"].isin([0,1])]\n",
    "        \n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute partition (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_partition(df_all, ts_thr=0.30):\n",
    "    y = df_all.readmitted\n",
    "    y = y.values\n",
    "\n",
    "    X = df_all.iloc[:,:-1].values\n",
    "    sss = StratifiedShuffleSplit(y, 1, test_size=ts_thr, random_state=32) #random_state=42\n",
    "    for train_index, test_index in sss:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_partition(X_train, y_train, tr_thr=0.10):\n",
    "    X_train_aux = []\n",
    "    y_train_aux = []\n",
    "    sss = StratifiedShuffleSplit(y_train, 1, test_size=1-tr_thr, random_state=32) #random_state=42\n",
    "    for train_index, test_index in sss:\n",
    "        X_train_aux = X_train[train_index]\n",
    "        y_train_aux = y_train[train_index]\n",
    "\n",
    "    return X_train_aux, y_train_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_pipelines(catCols,reducedCols, fs_methods, sm_types, cls_methods, lms):\n",
    "    basePipeline = Pipeline([\n",
    "            (\"Imputer\", TypeFeatImputer(catCols, reducedCols)),\n",
    "            (\"Scaler\", StandardScaler()),\n",
    "            (\"Variance\", VarianceThreshold(threshold=0.0))\n",
    "        ])\n",
    "\n",
    "    pipeline = [] \n",
    "\n",
    "    for fs_method in fs_methods:\n",
    "        for sm_type in sm_types:\n",
    "            for cls_method in cls_methods:\n",
    "                for lm in lms:\n",
    "                    if not (fs_method == \"rfe_rf_fs\" and cls_method == \"rf\"):\n",
    "                        params = {}   \n",
    "                        pipe = Pipeline(list(basePipeline.steps))\n",
    "\n",
    "                        if fs_method == \"combine_fs\":\n",
    "                            pipe.steps.insert(1,(fs_method, UnivCombineFilter(catCols,np.array(reducedCols))))\n",
    "                            params.update({fs_method + '__percentile':[5,10,20,30,40,50]})\n",
    "\n",
    "                        if fs_method == \"rfe_rf_fs\":\n",
    "                            pipe.steps.append((fs_method, RFE(estimator=RandomForestClassifier(class_weight='balanced',\n",
    "                                                                                               n_estimators=100,\n",
    "                                                                                               random_state=33))))\n",
    "                            params.update({fs_method + '__step':[0.1]})\n",
    "                            params.update({fs_method + '__n_features_to_select':[\n",
    "                                                            int(len(reducedCols)*0.4), \n",
    "                                                            int(len(reducedCols)*0.6), \n",
    "                                                            int(len(reducedCols)*0.8)]})\n",
    "\n",
    "                        if fs_method == 'lasso_fs':\n",
    "                            pipe.steps.append((fs_method, SelectFromModel(\n",
    "                                        LogisticRegression(n_jobs=-1, penalty=\"l1\", dual=False, random_state=42))))\n",
    "                            params.update({fs_method + '__estimator__C': [0.001,0.01,0.1,1]})\n",
    "\n",
    "                        #Add classifiers\n",
    "                        if cls_method == \"knn\":\n",
    "                            pipe.steps.append((cls_method, KNeighborsClassifier()))\n",
    "                            params.update({'knn__n_neighbors':[1,3,5,7,9,11], 'knn__weights':['uniform', 'distance']})\n",
    "\n",
    "                        if cls_method == \"logReg\":\n",
    "                            pipe.steps.append((cls_method, LogisticRegression(random_state=42)))\n",
    "                            params.update({'logReg__C': [0.00001,0.0001,0.0005,0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,15,30]})\n",
    "                            params.update({'logReg__class_weight': [None, 'balanced']})\n",
    "                            params.update({'logReg__penalty': ['l1', 'l2']})\n",
    "\n",
    "                        if cls_method == \"svmRBF\":\n",
    "                            pipe.steps.append((cls_method, SVC(random_state=42,probability=True)))\n",
    "                            params.update({'svmRBF__C': [0.01,0.1,0.5,1,5,10,30,50,100], \n",
    "                             'svmRBF__gamma' : [0.0001,0.001,0.01, 0.1,1,5]})\n",
    "                            params.update({'svmRBF__class_weight': [None, 'balanced']})\n",
    "\n",
    "                        if cls_method == \"rf\":\n",
    "                            pipe.steps.append((cls_method, RandomForestClassifier(n_jobs=-1,random_state=42)))\n",
    "                            params.update({'rf__n_estimators': [100,150,200,250,500], \n",
    "                                           'rf__criterion': ['entropy','gini'],\n",
    "                                           'rf__max_depth' : [None,4,6]})\n",
    "                            params.update({'rf__class_weight': [None, 'balanced']})\n",
    "\n",
    "                        if cls_method == \"nb\":\n",
    "                             pipe.steps.append((cls_method, GaussianNB()))\n",
    "                             params.update({})\n",
    "                                               \n",
    "                        if cls_method == \"nn\":\n",
    "                            pipe.steps.append((cls_method, MLPClassifier(\n",
    "                                        activation='logistic',\n",
    "                                        solver='lbfgs', \n",
    "                                        hidden_layer_sizes=(5, 2), \n",
    "                                        random_state=13)))\n",
    "                            params.update({\n",
    "                                    'nn__alpha': [1e-5,0.00001,0.0001,0.001,0.01,0.1,1,3,5,10],\n",
    "                                    'nn__hidden_layer_sizes':[(30,),(50,),(70,),(100,),(150,),\n",
    "                                                              (30,30),(50,50),(70,70),(100,100),\n",
    "                                                              (30,30,30),(50,50,50),(70,70,70)\n",
    "                                                             ]\n",
    "                                          })\n",
    "\n",
    "                        #Add sampling\n",
    "                        pipe_imb = make_pipeline(*[p[1] for p in pipe.steps])\n",
    "                        stps = len(pipe_imb.steps)        \n",
    "                        for s in range(stps):\n",
    "                            pipe_imb.steps.remove(pipe_imb.steps[0])\n",
    "                        for s in range(stps):\n",
    "                            pipe_imb.steps.append(pipe.steps[s])\n",
    "\n",
    "                        if sm_type == \"after\":                    \n",
    "                            pipe_imb.steps.insert(stps - 1, \n",
    "                                                  (sm_method, SMOTE(ratio='auto', kind='regular', random_state=32)))\n",
    "                            params.update({sm_method + \"__k_neighbors\":[3,4,5]})\n",
    "\n",
    "\n",
    "                        pipeline.append([fs_method,sm_type,cls_method,lm,pipe_imb,params])\n",
    "    pipelines = pd.DataFrame(pipeline, columns=[\"fs\",\"sm\",\"cls\",\"metric\",\"pipe\",\"pipe_params\"])\n",
    "    pipelines.sort_values(\"fs\", inplace=True)\n",
    "    print pipelines.shape\n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute type fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_type_features(df_all):\n",
    "    if typeDataFeatures == \"minimum\":\n",
    "        numCols = ['time_in_hospital']\n",
    "\n",
    "    if typeDataFeatures == \"extended\":\n",
    "        numCols = ['time_in_hospital','num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', \n",
    "                'number_emergency', 'number_inpatient', 'number_diagnoses']\n",
    "\n",
    "    catCols = []\n",
    "    cols = df_all.columns\n",
    "    reducedCols = cols[:-1]\n",
    "\n",
    "    for i in range(len(cols)-1):\n",
    "        if cols[i] not in numCols:\n",
    "            catCols.append(1)\n",
    "        else:\n",
    "            catCols.append(0)\n",
    "    catCols = np.array(catCols)\n",
    "    \n",
    "    return catCols, reducedCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_diseases(typeDisease):\n",
    "    if typeDisease == \"subset\":\n",
    "        return [\"subset\"]\n",
    "    else:\n",
    "        if typeDisease in colsDiseases:\n",
    "            return [typeDisease]\n",
    "        else:\n",
    "            return colsDiseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_data_by_diseases(df_all, disease, typeDataExperiment):\n",
    "    if disease == \"subset\":\n",
    "        df_all_filtered = df_all.copy()\n",
    "    else:\n",
    "        cols_filtered = colsNonDiseases[:]\n",
    "        cols_filtered.insert(-1, disease)\n",
    "        df_all_filtered = df_all[cols_filtered].copy()    \n",
    "    \n",
    "    if typeDataExperiment == \"disease\" and disease != \"subset\":\n",
    "        df_all_filtered = df_all_filtered[df_all_filtered[disease] == 1]\n",
    "        df_all_filtered = df_all_filtered[[c for c in df_all_filtered.columns if c != disease]]\n",
    "    \n",
    "    return df_all_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "typeEncounter = \"first\" # ['first','last']\n",
    "typeHypothesis = \"early_readmission_vs_none\" # ['all_readmisssion_vs_none','early_readmission_vs_none']\n",
    "typeDataFeatures = \"extended\" # [\"minimum,\"extended']\n",
    "    #Extended -> Subset of columns\n",
    "    #Minimum -> minimum set of columns \n",
    "typeDisease = \"any\" # [\"subset\",\"any\",[\"Respiratory\",...]]\n",
    "    #subset -> Return subset of predefined disease features\n",
    "    #any -> Return all disease features    \n",
    "    #disase -> Return diases feature\n",
    "typeDataExperiment = \"disease\" #[\"all\", \"disease\"] \n",
    "    #all -> Include all diagnosis as columns\n",
    "    #disease -> Remove diagnosis as column and keep only rows with diagnosis == 1       \n",
    "typeDiagnosis = \"diag_1\"  #[\"diag_1\", \"diag_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbose = True\n",
    "cv_thr = 0.3\n",
    "cv_folds = 5\n",
    "\n",
    "tr_thrs = [0.20,0.40,0.60] # 1.0\n",
    "ts_thr = 0.30\n",
    "\n",
    "fs_methods = [\"none\"] #[\"none\",\"combine_fs\",\"lasso_fs\",\"rfe_rf_fs\"]\n",
    "cls_methods = [\"logReg\",\"nb\",\"rf\"] #[\"rf\",\"svmRBF\",\"logReg\",\"knn\",\"nn\"]\n",
    "lms = [\"roc_auc\",\"recall\",\"f1_weighted\"] #[\"f1_weighted\",\"precision_weighted\"]\n",
    "sm_types = [\"none\"] #[\"none\",\"after\"]\n",
    "sm_method = \"sm_smote\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71518, 30)\n",
      "Index([u'diss_1', u'adm_src_ref', u'adm_src_em', u'race_AfricanAmerican',\n",
      "       u'race_Caucasian', u'race_Other', u'medSpec_cardio',\n",
      "       u'medSpec_Family/GeneralPractice', u'medSpec_InternalMedicine',\n",
      "       u'medSpec_surgery', u'age_cat', u'Diabetis_1', u'Circulatory_1',\n",
      "       u'Digestive_1', u'Genitourinary_1', u'Poisoning_1', u'Muscoskeletal_1',\n",
      "       u'Neoplasms_1', u'Respiratory_1', u'HbA1c', u'Change',\n",
      "       u'time_in_hospital', u'num_lab_procedures', u'num_procedures',\n",
      "       u'num_medications', u'number_outpatient', u'number_emergency',\n",
      "       u'number_inpatient', u'number_diagnoses', u'readmitted'],\n",
      "      dtype='object')\n",
      "0    54374\n",
      "2    13920\n",
      "1     3224\n",
      "Name: readmitted, dtype: int64\n",
      "0   0.76\n",
      "2   0.19\n",
      "1   0.05\n",
      "Name: readmitted, dtype: float64\n",
      "[0 1] 54374 3224\n",
      "[u'Diabetis_1', u'Circulatory_1', u'Digestive_1', u'Genitourinary_1', u'Poisoning_1', u'Muscoskeletal_1', u'Neoplasms_1', u'Respiratory_1']\n",
      "['diss_1', 'adm_src_ref', 'adm_src_em', 'race_AfricanAmerican', 'race_Caucasian', 'race_Other', 'medSpec_cardio', 'medSpec_Family/GeneralPractice', 'medSpec_InternalMedicine', 'medSpec_surgery', 'age_cat', 'HbA1c', 'Change', 'time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', 'number_emergency', 'number_inpatient', 'number_diagnoses', 'readmitted']\n",
      "Total data: (57598, 30)\n",
      "[u'Diabetis_1', u'Circulatory_1', u'Digestive_1', u'Genitourinary_1', u'Poisoning_1', u'Muscoskeletal_1', u'Neoplasms_1', u'Respiratory_1']\n"
     ]
    }
   ],
   "source": [
    "#Load data\n",
    "df_all = load_data(typeEncounter, typeDiagnosis, typeDataFeatures)\n",
    "print df_all.shape\n",
    "print df_all.columns\n",
    "print df_all.readmitted.value_counts()\n",
    "print df_all.readmitted.value_counts()/float(df_all.shape[0])\n",
    "\n",
    "#Filter data by class\n",
    "df_all = filter_data_by_class(df_all, typeHypothesis)\n",
    "print df_all.iloc[:,-1].sort_values().unique(), np.sum(df_all[\"readmitted\"] == 0), np.sum(df_all[\"readmitted\"] == 1)\n",
    "    \n",
    "#Get columns\n",
    "colsDiseases, colsNonDiseases = get_columns(df_all,typeDiagnosis)\n",
    "print colsDiseases\n",
    "print colsNonDiseases\n",
    "    \n",
    "#Load diseases\n",
    "diseases = get_diseases(typeDisease)\n",
    "print \"Total data:\", df_all.shape\n",
    "print diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71518, 30)\n",
      "Index([u'diss_1', u'adm_src_ref', u'adm_src_em', u'race_AfricanAmerican',\n",
      "       u'race_Caucasian', u'race_Other', u'medSpec_cardio',\n",
      "       u'medSpec_Family/GeneralPractice', u'medSpec_InternalMedicine',\n",
      "       u'medSpec_surgery', u'age_cat', u'Diabetis_1', u'Circulatory_1',\n",
      "       u'Digestive_1', u'Genitourinary_1', u'Poisoning_1', u'Muscoskeletal_1',\n",
      "       u'Neoplasms_1', u'Respiratory_1', u'HbA1c', u'Change',\n",
      "       u'time_in_hospital', u'num_lab_procedures', u'num_procedures',\n",
      "       u'num_medications', u'number_outpatient', u'number_emergency',\n",
      "       u'number_inpatient', u'number_diagnoses', u'readmitted'],\n",
      "      dtype='object')\n",
      "0    54374\n",
      "2    13920\n",
      "1     3224\n",
      "Name: readmitted, dtype: int64\n",
      "0   0.76\n",
      "2   0.19\n",
      "1   0.05\n",
      "Name: readmitted, dtype: float64\n",
      "[u'Diabetis_1', u'Circulatory_1', u'Digestive_1', u'Genitourinary_1', u'Poisoning_1', u'Muscoskeletal_1', u'Neoplasms_1', u'Respiratory_1']\n",
      "['diss_1', 'adm_src_ref', 'adm_src_em', 'race_AfricanAmerican', 'race_Caucasian', 'race_Other', 'medSpec_cardio', 'medSpec_Family/GeneralPractice', 'medSpec_InternalMedicine', 'medSpec_surgery', 'age_cat', 'HbA1c', 'Change', 'time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', 'number_emergency', 'number_inpatient', 'number_diagnoses', 'readmitted']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_diseases' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6a2e658e41cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mcolsNonDiseases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#Load diseases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdiseases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_diseases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypeDisease\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Total data:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0msubsetFeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_diseases' is not defined"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for tr_thr in tr_thrs:\n",
    "    for disease in diseases:\n",
    "        \n",
    "        df_all_filtered = filter_data_by_diseases(df_all, disease, typeDataExperiment)               \n",
    "        X_train, X_test, y_train, y_test = train_test_partition(df_all_filtered, ts_thr)\n",
    "        X_train, y_train = train_partition(X_train, y_train, tr_thr)\n",
    "\n",
    "        catCols, reducedCols = compute_type_features(df_all_filtered)\n",
    "        pipeline = create_pipeline(catCols,reducedCols, fs_methods, sm_types, cls_methods, lms)\n",
    "\n",
    "        print \"\\nDataSet:\"\n",
    "        print \"**********\"\n",
    "        print \"**********\"\n",
    "        print \"SIZE:\", tr_thr\n",
    "        print \"DISEASE:\", disease\n",
    "\n",
    "        print df_all_filtered.shape\n",
    "        print \"ALL TRAIN:\", X_train.shape\n",
    "        print \"TRAIN:\", \"[0's:\", np.sum(y_train==0), \"1's:\", np.sum(y_train==1), \"]\"\n",
    "        print \"ALL TEST:\", X_test.shape\n",
    "        print \"TEST:\", \"[0's:\", np.sum(y_test==0), \"1's:\", np.sum(y_test==1), \"]\"\n",
    "\n",
    "        for num_exp in range(pipeline.shape[0]):\n",
    "\n",
    "            # Run experiment\n",
    "            start = time.time()\n",
    "\n",
    "            #Prepare pipe_cls      \n",
    "            pipeline_cls = pipeline[\"pipe\"].iloc[num_exp]\n",
    "            pipeline_params = pipeline[\"pipe_params\"].iloc[num_exp]\n",
    "            fs = pipeline[\"fs\"].iloc[num_exp]\n",
    "            sm = pipeline[\"sm\"].iloc[num_exp]\n",
    "            cls = pipeline[\"cls\"].iloc[num_exp]\n",
    "            lm = pipeline[\"metric\"].iloc[num_exp]\n",
    "\n",
    "            print \"\\nNum experiment:\", str(num_exp), \"/\", str(pipeline.shape[0] - 1)\n",
    "            print \"****************\"\n",
    "\n",
    "            print \"FS:\",fs\n",
    "            print \"SM:\",sm\n",
    "            print \"CLS:\",cls\n",
    "            print \"METRIC:\",lm\n",
    "\n",
    "            #Prepare cv\n",
    "            cv_inner = StratifiedShuffleSplit(y_train, n_iter=cv_folds, test_size=cv_thr, random_state=24)\n",
    "            cv_outer = StratifiedShuffleSplit(y_train, n_iter=cv_folds, test_size=cv_thr, random_state=42)\n",
    "\n",
    "            #Fit pipeline with CV                        \n",
    "            grid_pipeline = GridSearchCV(pipeline_cls, param_grid=pipeline_params, verbose=verbose, \n",
    "                                         n_jobs=-1, cv=cv_inner, scoring= lm, error_score = 0) \n",
    "            grid_pipeline.fit(X_train, y_train)\n",
    "\n",
    "            # Compute pipeline evaluation with CVmetric\n",
    "            print \"\\nCV INNER metric: {}\".format(lm)\n",
    "            print \"CV INNER selected params {}\".format(grid_pipeline.best_params_.values())\n",
    "            print \"CV INNER score: {}\".format(grid_pipeline.best_score_)\n",
    "\n",
    "            cv_f1 = cross_val_score(grid_pipeline.best_estimator_, X_train, y_train, \n",
    "                                                     cv=cv_outer, scoring='f1_weighted', n_jobs=-1)\n",
    "\n",
    "            cv_prec = cross_val_score(grid_pipeline.best_estimator_, X_train, y_train, \n",
    "                                                     cv=cv_outer, scoring='precision_weighted', n_jobs=-1)\n",
    "\n",
    "            cv_rec = cross_val_score(grid_pipeline.best_estimator_, X_train, y_train, \n",
    "                                                     cv=cv_outer, scoring='recall_weighted', n_jobs=-1)\n",
    "\n",
    "            print \"\\nCV OUTER f1 score: %0.3f  (+/-%0.03f)\" % (np.mean(cv_f1), np.std(cv_f1))\n",
    "            print \"CV OUTER prec score: %0.3f  (+/-%0.03f)\" % (np.mean(cv_prec), np.std(cv_prec))\n",
    "            print \"CV OUTER rec score: %0.3f  (+/-%0.03f)\" % (np.mean(cv_rec), np.std(cv_rec))\n",
    "            print \"Selected params (bests from CV) {}\".format(grid_pipeline.best_params_.values())\n",
    "\n",
    "            # Computel Train score (with best CV params)\n",
    "            y_pred = grid_pipeline.best_estimator_.predict(X_train)\n",
    "            train_prec_scores = metrics.precision_score(y_train, y_pred, average='weighted', pos_label=None)\n",
    "            train_rec_scores = metrics.recall_score(y_train, y_pred, average='weighted', pos_label=None)    \n",
    "            train_f1_scores = metrics.f1_score(y_train, y_pred, average='weighted', pos_label=None)\n",
    "\n",
    "            print \"\\nTR F1 score:\", train_f1_scores\n",
    "            print \"TR Prec score:\", train_prec_scores\n",
    "            print \"TR Rec score:\", train_rec_scores\n",
    "\n",
    "            #Compute test score\n",
    "            y_pred = grid_pipeline.best_estimator_.predict(X_test)\n",
    "            test_f1 = metrics.f1_score(y_test, y_pred, average='weighted', pos_label=None)\n",
    "            test_prec = metrics.recall_score(y_test, y_pred, average='weighted', pos_label=None)\n",
    "            test_rec = metrics.precision_score(y_test, y_pred, average='weighted', pos_label=None)\n",
    "            test_auc = metrics.roc_auc_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "            print \"\\nTest f1: %0.3f\" % (test_f1)\n",
    "            print \"Test Precision: %0.3f\" % (test_prec)\n",
    "            print \"Test Recall: %0.3f\" % (test_rec)\n",
    "            print \"Test AUC: %0.3f\" % (test_auc)\n",
    "\n",
    "            print \"with following performance in test:\"\n",
    "            print metrics.classification_report(y_test, y_pred)\n",
    "            print metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "            end = time.time()\n",
    "            print \"\\nTotal time:\", end - start\n",
    "            results = [num_exp,\n",
    "                           disease,\n",
    "                           typeEncounter,\n",
    "                           typeHypothesis,\n",
    "                           typeDataFeatures,\n",
    "                           typeDiagnosis,\n",
    "                           tr_thr,\n",
    "                           fs,\n",
    "                           sm,\n",
    "                           cls,\n",
    "                           lm,\n",
    "                           grid_pipeline.best_params_.values(),\n",
    "                           train_f1_scores,\n",
    "                           train_prec_scores,\n",
    "                           train_rec_scores,\n",
    "                           np.mean(cv_f1), \n",
    "                           np.std(cv_f1),\n",
    "                           np.mean(cv_prec), \n",
    "                           np.std(cv_prec),\n",
    "                           np.mean(cv_rec), \n",
    "                           np.std(cv_rec),                    \n",
    "                           test_f1,\n",
    "                           test_prec,\n",
    "                           test_rec,\n",
    "                           test_auc,                    \n",
    "                           end - start,\n",
    "                           grid_pipeline.best_estimator_\n",
    "                          ]\n",
    "\n",
    "            #Save results\n",
    "            df = pd.DataFrame(np.array(results).reshape(1,27), columns=\n",
    "                      [\"exp\", \"typeDisease\",\"typeEncounter\",\"typeHypothesis\",\"typeDataFeatures\",\"typeDiagnosis\",\n",
    "                       \"size_tr\",\"fs\",\"sm\",\"cls\",\"metric\",\"params\",\n",
    "                       \"tr_f1\",\"tr_prec\",\"tr_rec\",\n",
    "                       \"cv_f1_mean\",\"cv_f1_std\",\"cv_prec_mean\",\"cv_prec_std\",\"cv_rec_mean\",\"cv_rec_std\",\n",
    "                       \"test_f1\",\"test_prec\",\"test_rec\",\"test_auc\",\n",
    "                       \"time\",\"pipeline\"])\n",
    "\n",
    "            df.to_pickle(os.path.join(\"resources\", \"results\",\n",
    "                                      'results_pipe_' + \n",
    "                                      \"test_\" + str(ts_thr) + \"_\" +\n",
    "                                      \"train_\" + str(tr_thr) + \"_\" +\n",
    "                                      str(disease) + '_' +\n",
    "                                      str(typeEncounter) + '_' +\n",
    "                                      str(typeHypothesis) + '_' +\n",
    "                                      str(typeDataFeatures) + '_' +\n",
    "                                      str(typeDataExperiment) + '_' +\n",
    "                                      str(typeDiagnosis) + '_' +\n",
    "                                      str(fs) + '_' +\n",
    "                                      str(sm) + '_' +\n",
    "                                      str(lm) + '_' +\n",
    "                                      str(cls) + '_' +\n",
    "                                      '.pkl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
