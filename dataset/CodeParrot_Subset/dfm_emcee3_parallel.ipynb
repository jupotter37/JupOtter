{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelization\n",
    "===============\n",
    "\n",
    "**emcee** supports parallelization out of the box. The algorithmic details are given in [the paper](http://arxiv.org/abs/1202.3665) but the implementation is very simple. The parallelization is applied across the walkers in the ensemble at each step and it must therefore be synchronized after each iteration. This means that you will really only benefit from this feature when your probability function is relatively expensive to compute.\n",
    "\n",
    "The recommended method is to use [IPython's parallel feature](http://ipython.org/ipython-doc/dev/parallel/) but it's possible to use other \"mappers\" like the Python standard library's `multiprocessing.Pool`. The only requirement of the mapper is that it exposes a `map` method.\n",
    "\n",
    "\n",
    "Using multiprocessing\n",
    "---------------------\n",
    "\n",
    "As mentioned above, it's possible to parallelize your model using the standard library's `multiprocessing` package. Instead, I would recommend the `pools.InterruptiblePool` that is included with **emcee** because it is a simple thin wrapper around `multiprocessing.Pool` with support for a keyboard interrupt (`^C`)... you'll thank me later! If we wanted to use this pool, the final few lines from the example on the front page would become the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import emcee3\n",
    "import numpy as np\n",
    "\n",
    "def log_prob(x):\n",
    "    return -0.5 * np.sum(x ** 2)\n",
    "\n",
    "ndim, nwalkers = 10, 100\n",
    "with emcee3.pools.InterruptiblePool() as pool:\n",
    "    ensemble = emcee3.Ensemble(log_prob, np.random.randn(nwalkers, ndim), pool=pool)\n",
    "    sampler = emcee3.Sampler()\n",
    "    sampler.run(ensemble, 1000)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. note:: Don't forget to close the pool! It is **your responsibility** as the\n",
    "          user to close the pool. Otherwise, the Python processes that get\n",
    "          initialized to run your code won't shut down until your main process\n",
    "          exits. It's not enough to ``del`` the pool, you have to close it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using MPI\n",
    "---------\n",
    "\n",
    "To distribute emcee3 across nodes on a cluster, you'll need to use MPI. This can be done with the `MPIPool` from [schwimmbad](https://github.com/adrn/schwimmbad). To use this, you'll need to install the dependency [mpi4py](http://mpi4py.readthedocs.io/). Otherwise, the code is almost the same as the multiprocessing example above â€“ the main change is the definition of the pool:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. code-block:: python\n",
    "\n",
    "    import sys\n",
    "    import schwimmbad\n",
    "\n",
    "    with schwimmbad.MPIPool() as pool:\n",
    "        if not pool.is_master():\n",
    "            pool.wait()\n",
    "            sys.exit(0)\n",
    "\n",
    "        ensemble = emcee3.Ensemble(log_prob, np.random.randn(nwalkers, ndim), pool=pool)\n",
    "        sampler = emcee3.Sampler()\n",
    "        sampler.run(ensemble, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `if not pool.is_master()` block is crucial otherwise the code will hang at the end of execution. To run this code, you would execute something like the following: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. code-block:: bash\n",
    "\n",
    "    mpiexec -np 16 name_of_file.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ipyparallel\n",
    "-----------------\n",
    "\n",
    "[ipyparallel](https://ipyparallel.readthedocs.io) is a\n",
    "flexible and powerful framework for running distributed computation in Python.\n",
    "It works on a single machine with multiple cores in the same way as it does on\n",
    "a huge compute cluster and in both cases it is very efficient!\n",
    "\n",
    "To use IPython parallel, make sure that you have a recent version of IPython\n",
    "installed ([ipyparallel docs](https://ipyparallel.readthedocs.io)) and start up the cluster\n",
    "by running:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. code-block:: bash\n",
    "\n",
    "    ipcluster start --engines=MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing emcee3 on engine(s)\n",
      "importing numpy on engine(s)\n"
     ]
    }
   ],
   "source": [
    "# Connect to the cluster.\n",
    "from ipyparallel import Client\n",
    "rc = Client()\n",
    "dv = rc.direct_view()\n",
    "\n",
    "# Run the imports on the cluster too.\n",
    "with dv.sync_imports():\n",
    "    import emcee3\n",
    "    import numpy\n",
    "\n",
    "# Define the model.\n",
    "def log_prob(x):\n",
    "    return -0.5 * numpy.sum(x ** 2)\n",
    "\n",
    "# Distribute the model to the nodes of the cluster.\n",
    "dv.push(dict(log_prob=log_prob), block=True)\n",
    "\n",
    "# Set up the ensemble with the IPython \"DirectView\" as the pool.\n",
    "ndim, nwalkers = 10, 100\n",
    "ensemble = emcee3.Ensemble(log_prob, numpy.random.randn(nwalkers, ndim), pool=dv)\n",
    "\n",
    "# Run the sampler in the same way as usual.\n",
    "sampler = emcee3.Sampler()\n",
    "ensemble = sampler.run(ensemble, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a significant overhead incurred when using any of these\n",
    "parallelization methods so for this simple example, the parallel version is\n",
    "actually *slower* but this effect will be quickly offset if your probability\n",
    "function is computationally expensive.\n",
    "\n",
    "One major benefit of using ipyparallel is that it can also be used\n",
    "identically on a cluster with MPI if you have a really big problem. The Python\n",
    "code would look identical and the only change that you would have to make is\n",
    "to start the cluster using:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ".. code-block:: bash\n",
    "\n",
    "    ipcluster start --engines=MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at [the documentation](http://ipython.org/ipython-doc/dev/parallel/) for more details of all of the features available in ipyparallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
