{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This example demonstraites how to convert Caffe pretrained ResNet-50 model from https://github.com/KaimingHe/deep-residual-networks (firstly described in http://arxiv.org/pdf/1512.03385v1.pdf) into Theano/Lasagne format.\n",
    "\n",
    "We will create a set of Lasagne layers corresponding to the Caffe model specification (prototxt), then copy the parameters from the caffemodel file into our model (like <a href=\"https://github.com/Lasagne/Recipes/blob/master/examples/Using%20a%20Caffe%20Pretrained%20Network%20-%20CIFAR10.ipynb\">here</a>).\n",
    "\n",
    "This notebook produce *resnet50.pkl* file, which contains dictionary with following foelds:\n",
    " * values: numpy array with parameters of the model\n",
    " * synset_words: labels of classes\n",
    " * mean_image: mean image which should be subtracted from each input image\n",
    "\n",
    "This file can be used for initialization of weights of the model created by *modelzoo/resnet50.py*.\n",
    "\n",
    "## License\n",
    "Same as in parent project https://github.com/KaimingHe/deep-residual-networks/blob/master/LICENSE\n",
    "\n",
    "# Requirements\n",
    "\n",
    "## Download the required files\n",
    "\n",
    "<a href=\"https://onedrive.live.com/?authkey=%21AAFW2-FVoxeVRck&id=4006CBB8476FF777%2117887&cid=4006CBB8476FF777\">Here</a> you can find folder with caffe/proto files, we need followings to be stored in ./:\n",
    " * *ResNet-50-deploy.prototxt* contains architecture of ResNet-50 in proto format\n",
    " * *ResNet-50-model.caffemodel* is proto serialization of model parameters\n",
    " * *ResNet_mean.binaryproto* contains mean values\n",
    " \n",
    "## Imports\n",
    "We need caffe to load weights and compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import caffe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a lot of building blocks from Lasagne to build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.utils import floatX\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import Conv2DLayer as ConvLayer # can be replaced with dnn layers\n",
    "from lasagne.layers import BatchNormLayer\n",
    "from lasagne.layers import Pool2DLayer as PoolLayer\n",
    "from lasagne.layers import NonlinearityLayer\n",
    "from lasagne.layers import ElemwiseSumLayer\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.nonlinearities import rectify, softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper modules, some of them will help us to download images and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = 8, 6\n",
    "import io\n",
    "import urllib\n",
    "import skimage.transform\n",
    "from IPython.display import Image\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Lasagne model\n",
    "\n",
    "## BatchNormalization issue in caffe\n",
    "\n",
    "Caffe doesn't have correct BN layer as described in https://arxiv.org/pdf/1502.03167.pdf:\n",
    " * it can collect datasets mean ($\\hat{\\mu}$) and variance ($\\hat{\\sigma}^2$)\n",
    " * it can't fit $\\gamma$ and $\\beta$ parameters to scale and shift standardized distribution of feature in following formula: $\\hat{x}_i = \\dfrac{x_i - \\hat{\\mu}_i}{\\sqrt{\\hat{\\sigma}_i^2 + \\epsilon}}\\cdot\\gamma + \\beta$\n",
    "\n",
    "To fix this issue, <a href=\"https://github.com/KaimingHe/deep-residual-networks\">here</a> authors use such BN layer followed by Scale layer, which can fit scale and shift parameters, but can't standardize data:\n",
    "\n",
    "<pre>\n",
    "layer {\n",
    "\tbottom: \"res2a_branch1\"\n",
    "\ttop: \"res2a_branch1\"\n",
    "\tname: \"bn2a_branch1\"\n",
    "\ttype: \"BatchNorm\"\n",
    "\tbatch_norm_param {\n",
    "\t\tuse_global_stats: true\n",
    "\t}\n",
    "}\n",
    "\n",
    "layer {\n",
    "\tbottom: \"res2a_branch1\"\n",
    "\ttop: \"res2a_branch1\"\n",
    "\tname: \"scale2a_branch1\"\n",
    "\ttype: \"Scale\"\n",
    "\tscale_param {\n",
    "\t\tbias_term: true\n",
    "\t}\n",
    "}\n",
    "</pre>\n",
    "\n",
    "In Lasagne we have correct BN layer, so we do not need use such a trick.\n",
    "\n",
    "## Replicated blocks\n",
    "\n",
    "### Simple blocks\n",
    "\n",
    "ResNet contains a lot of similar replicated blocks, lets call them *simple blocks*, which have one of two architectures:\n",
    " * Convolution $\\rightarrow$ BN $\\rightarrow$ Nonlinearity\n",
    " * Convolution $\\rightarrow$ BN\n",
    " \n",
    "http://ethereon.github.io/netscope/#/gist/2f702ea9e05900300462102a33caff9c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='images/head.png', width='40%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can increase, decrease or keep same dimensionality of data using such blocks. In ResNet-50 only several transformation are used.\n",
    "\n",
    "#### Keep shape with 1x1 convolution\n",
    "We can apply nonlinearity transformation from (None, 64, 56, 56) to (None, 64, 56, 56) if we apply simple block with following parameters (look at the origin of a network after first pool layer):\n",
    " * num_filters: same as parent has\n",
    " * filter_size: 1\n",
    " * stride: 1\n",
    " * pad: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='images/conv1x1.png', width='40%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep shape with 3x3 convolution\n",
    "Also we can apply nonlinearity transformation from (None, 64, 56, 56) to (None, 64, 56, 56) if we apply simple block with following parameters (look at the middle of any residual blocks):\n",
    " * num_filters: same as parent has\n",
    " * filter_size: 3x3\n",
    " * stride: 1\n",
    " * pad: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='images/conv3x3.png', width='40%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increase shape using number of filters\n",
    "We can nonlinearly increase shape from (None, 64, 56, 56) to (None, 256, 56, 56) if we apply simple block with following parameters (look at the last simple block of any risidual block):\n",
    " * num_filters: four times greater then parent has\n",
    " * filter_size: 1x1\n",
    " * stride: 1\n",
    " * pad: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='images/increase_fn.png', width='40%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increase shape using number of filters\n",
    "We can nonlinearly decrease shape from (None, 256, 56, 56) to (None, 64, 56, 56) if we apply simple block with following parameters (look at the first simple block of any risidual block without left branch):\n",
    " * num_filters: four times less then parent has\n",
    " * filter_size: 1x1\n",
    " * stride: 1\n",
    " * pad: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='images/decrease_fn.png', width='40%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increase shape using number of filters\n",
    "We can also nonlinearly decrease shape from (None, 256, 56, 56) to (None, 128, 28, 28) if we apply simple block with following parameters (look at the first simple block of any risidual block with left branch):\n",
    " * num_filters: two times less then parent has\n",
    " * filter_size: 1x1\n",
    " * stride: 2\n",
    " * pad: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='images/decrease_fnstride.png', width='40%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following function creates simple block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_simple_block(incoming_layer, names,\n",
    "                       num_filters, filter_size, stride, pad, \n",
    "                       use_bias=False, nonlin=rectify):\n",
    "    \"\"\"Creates stacked Lasagne layers ConvLayer -> BN -> (ReLu)\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    incoming_layer : instance of Lasagne layer\n",
    "        Parent layer\n",
    "    \n",
    "    names : list of string\n",
    "        Names of the layers in block\n",
    "        \n",
    "    num_filters : int\n",
    "        Number of filters in convolution layer\n",
    "        \n",
    "    filter_size : int\n",
    "        Size of filters in convolution layer\n",
    "        \n",
    "    stride : int\n",
    "        Stride of convolution layer\n",
    "        \n",
    "    pad : int\n",
    "        Padding of convolution layer\n",
    "        \n",
    "    use_bias : bool\n",
    "        Whether to use bias in conlovution layer\n",
    "        \n",
    "    nonlin : function\n",
    "        Nonlinearity type of Nonlinearity layer\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple: (net, last_layer_name)\n",
    "        net : dict\n",
    "            Dictionary with stacked layers\n",
    "        last_layer_name : string\n",
    "            Last layer name\n",
    "    \"\"\"\n",
    "    net = []\n",
    "    net.append((\n",
    "            names[0], \n",
    "            ConvLayer(incoming_layer, num_filters, filter_size, pad, stride, \n",
    "                      flip_filters=False, nonlinearity=None) if use_bias \n",
    "            else ConvLayer(incoming_layer, num_filters, filter_size, stride, pad, b=None, \n",
    "                           flip_filters=False, nonlinearity=None)\n",
    "        ))\n",
    "    \n",
    "    net.append((\n",
    "            names[1], \n",
    "            BatchNormLayer(net[-1][1])\n",
    "        ))\n",
    "    if nonlin is not None:\n",
    "        net.append((\n",
    "            names[2], \n",
    "            NonlinearityLayer(net[-1][1], nonlinearity=nonlin)\n",
    "        ))\n",
    "    \n",
    "    return dict(net), net[-1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual blocks\n",
    "\n",
    "ResNet also contains several **residual blockes** built from simple blocks, each of them have two branches; left branch sometimes contains simple block, sometimes not. Each block ends with Elementwise sum layer followed by ReLu nonlinearity.  \n",
    "\n",
    "http://ethereon.github.io/netscope/#/gist/410e7e48fa1e5a368ee7bca5eb3bf0ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='images/left_branch.png', width='40%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='images/no_left_branch.png', width='40%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_block_name_pattern = ['res%s_branch%i%s', 'bn%s_branch%i%s', 'res%s_branch%i%s_relu']\n",
    "\n",
    "def build_residual_block(incoming_layer, ratio_n_filter=1.0, ratio_size=1.0, has_left_branch=False, \n",
    "                         upscale_factor=4, ix=''):\n",
    "    \"\"\"Creates two-branch residual block\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    incoming_layer : instance of Lasagne layer\n",
    "        Parent layer\n",
    "    \n",
    "    ratio_n_filter : float\n",
    "        Scale factor of filter bank at the input of residual block\n",
    "        \n",
    "    ratio_size : float\n",
    "        Scale factor of filter size\n",
    "        \n",
    "    has_left_branch : bool\n",
    "        if True, then left branch contains simple block\n",
    "        \n",
    "    upscale_factor : float\n",
    "        Scale factor of filter bank at the output of residual block\n",
    "        \n",
    "    ix : int\n",
    "        Id of residual block\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple: (net, last_layer_name)\n",
    "        net : dict\n",
    "            Dictionary with stacked layers\n",
    "        last_layer_name : string\n",
    "            Last layer name\n",
    "    \"\"\"\n",
    "    net = {}\n",
    "    \n",
    "    # right branch\n",
    "    net_tmp, last_layer_name = build_simple_block(\n",
    "        incoming_layer, map(lambda s: s % (ix, 2, 'a'), simple_block_name_pattern),\n",
    "        int(lasagne.layers.get_output_shape(incoming_layer)[1]*ratio_n_filter), 1, int(1.0/ratio_size), 0)\n",
    "    net.update(net_tmp)\n",
    "    \n",
    "    net_tmp, last_layer_name = build_simple_block(\n",
    "        net[last_layer_name], map(lambda s: s % (ix, 2, 'b'), simple_block_name_pattern),\n",
    "        lasagne.layers.get_output_shape(net[last_layer_name])[1], 3, 1, 1)\n",
    "    net.update(net_tmp)\n",
    "    \n",
    "    net_tmp, last_layer_name = build_simple_block(\n",
    "        net[last_layer_name], map(lambda s: s % (ix, 2, 'c'), simple_block_name_pattern),\n",
    "        lasagne.layers.get_output_shape(net[last_layer_name])[1]*upscale_factor, 1, 1, 0,\n",
    "        nonlin=None)\n",
    "    net.update(net_tmp)\n",
    "    \n",
    "    right_tail = net[last_layer_name]\n",
    "    left_tail = incoming_layer\n",
    "    \n",
    "    # left branch\n",
    "    if has_left_branch:\n",
    "        net_tmp, last_layer_name = build_simple_block(\n",
    "            incoming_layer, map(lambda s: s % (ix, 1, ''), simple_block_name_pattern),\n",
    "            int(lasagne.layers.get_output_shape(incoming_layer)[1]*4*ratio_n_filter), 1, int(1.0/ratio_size), 0,\n",
    "            nonlin=None)\n",
    "        net.update(net_tmp)\n",
    "        left_tail = net[last_layer_name]\n",
    "        \n",
    "    net['res%s' % ix] = ElemwiseSumLayer([left_tail, right_tail], coeffs=1)\n",
    "    net['res%s_relu' % ix] = NonlinearityLayer(net['res%s' % ix], nonlinearity=rectify)\n",
    "    \n",
    "    return net, 'res%s_relu' % ix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gathering everighting together\n",
    "\n",
    "Create head of the network (everithing before first residual block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = {}\n",
    "net['input'] = InputLayer((None, 3, 224, 224))\n",
    "sub_net, parent_layer_name = build_simple_block(\n",
    "    net['input'], ['conv1', 'bn_conv1', 'conv1_relu'],\n",
    "    64, 7, 3, 2, use_bias=True)\n",
    "net.update(sub_net)\n",
    "net['pool1'] = PoolLayer(net[parent_layer_name], pool_size=3, stride=2, pad=0, mode='max', ignore_border=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create four groups of residual blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "block_size = list('abc')\n",
    "parent_layer_name = 'pool1'\n",
    "for c in block_size:\n",
    "    if c == 'a':\n",
    "        sub_net, parent_layer_name = build_residual_block(net[parent_layer_name], 1, 1, True, 4, ix='2%s' % c)\n",
    "    else:\n",
    "        sub_net, parent_layer_name = build_residual_block(net[parent_layer_name], 1.0/4, 1, False, 4, ix='2%s' % c)\n",
    "    net.update(sub_net)\n",
    "    \n",
    "block_size = list('abcd')\n",
    "for c in block_size:\n",
    "    if c == 'a':\n",
    "        sub_net, parent_layer_name = build_residual_block(net[parent_layer_name], 1.0/2, 1.0/2, True, 4, ix='3%s' % c)\n",
    "    else:\n",
    "        sub_net, parent_layer_name = build_residual_block(net[parent_layer_name], 1.0/4, 1, False, 4, ix='3%s' % c)\n",
    "    net.update(sub_net)\n",
    "    \n",
    "block_size = list('abcdef')\n",
    "for c in block_size:\n",
    "    if c == 'a':\n",
    "        sub_net, parent_layer_name = build_residual_block(net[parent_layer_name], 1.0/2, 1.0/2, True, 4, ix='4%s' % c)\n",
    "    else:\n",
    "        sub_net, parent_layer_name = build_residual_block(net[parent_layer_name], 1.0/4, 1, False, 4, ix='4%s' % c)\n",
    "    net.update(sub_net)\n",
    "    \n",
    "block_size = list('abc')\n",
    "for c in block_size:\n",
    "    if c == 'a':\n",
    "        sub_net, parent_layer_name = build_residual_block(net[parent_layer_name], 1.0/2, 1.0/2, True, 4, ix='5%s' % c)\n",
    "    else:\n",
    "        sub_net, parent_layer_name = build_residual_block(net[parent_layer_name], 1.0/4, 1, False, 4, ix='5%s' % c)\n",
    "    net.update(sub_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Create tail of the network (everighting after last resudual block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net['pool5'] = PoolLayer(net[parent_layer_name], pool_size=7, stride=1, pad=0, \n",
    "                         mode='average_exc_pad', ignore_border=False)\n",
    "net['fc1000'] = DenseLayer(net['pool5'], num_units=1000, nonlinearity=None)\n",
    "net['prob'] = NonlinearityLayer(net['fc1000'], nonlinearity=softmax)\n",
    "\n",
    "print 'Total number of layers:', len(lasagne.layers.get_all_layers(net['prob']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Transfer weights from caffe to lasagne\n",
    "\n",
    "## Load pretrained caffe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net_caffe = caffe.Net('./ResNet-50-deploy.prototxt', './ResNet-50-model.caffemodel', caffe.TEST)\n",
    "layers_caffe = dict(zip(list(net_caffe._layer_names), net_caffe.layers))\n",
    "print 'Number of layers: %i' % len(layers_caffe.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy weights\n",
    "\n",
    "There is one more issue with BN layer: caffa stores variance $\\sigma^2$, but lasagne stores inverted standard deviation $\\dfrac{1}{\\sigma}$, so we need make simple transfommation to handle it.\n",
    "\n",
    "Other issue reffers to weights ofthe dense layer, in caffe it is transposed, we should handle it too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name, layer in net.items():    \n",
    "    if name not in layers_caffe:\n",
    "        print name, type(layer).__name__\n",
    "        continue\n",
    "    if isinstance(layer, BatchNormLayer):\n",
    "        layer_bn_caffe = layers_caffe[name]\n",
    "        layer_scale_caffe = layers_caffe['scale' + name[2:]]\n",
    "        layer.gamma.set_value(layer_scale_caffe.blobs[0].data)\n",
    "        layer.beta.set_value(layer_scale_caffe.blobs[1].data)\n",
    "        layer.mean.set_value(layer_bn_caffe.blobs[0].data)\n",
    "        layer.inv_std.set_value(1/np.sqrt(layer_bn_caffe.blobs[1].data) + 1e-4)\n",
    "        continue\n",
    "    if isinstance(layer, DenseLayer):\n",
    "        layer.W.set_value(layers_caffe[name].blobs[0].data.T)\n",
    "        layer.b.set_value(layers_caffe[name].blobs[1].data)\n",
    "        continue\n",
    "    if len(layers_caffe[name].blobs) > 0:\n",
    "        layer.W.set_value(layers_caffe[name].blobs[0].data)\n",
    "    if len(layers_caffe[name].blobs) > 1:\n",
    "        layer.b.set_value(layers_caffe[name].blobs[1].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "Read ImageNet synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./imagenet_classes.txt', 'r') as f:\n",
    "    classes = map(lambda s: s.strip(), f.readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Download some image urls for recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = urllib.urlopen('http://www.image-net.org/challenges/LSVRC/2012/ori_urls/indexval.html').read()\n",
    "image_urls = index.split('<br>')\n",
    "np.random.seed(23)\n",
    "np.random.shuffle(image_urls)\n",
    "image_urls = image_urls[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load mean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blob = caffe.proto.caffe_pb2.BlobProto()\n",
    "data = open('./ResNet_mean.binaryproto', 'rb').read()\n",
    "blob.ParseFromString(data)\n",
    "mean_values = np.array(caffe.io.blobproto_to_array(blob))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Image loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prep_image(url, fname=None):\n",
    "    if fname is None:\n",
    "        ext = url.split('.')[-1]\n",
    "        im = plt.imread(io.BytesIO(urllib.urlopen(url).read()), ext)\n",
    "    else:\n",
    "        ext = fname.split('.')[-1]\n",
    "        im = plt.imread(fname, ext)\n",
    "    h, w, _ = im.shape\n",
    "    if h < w:\n",
    "        im = skimage.transform.resize(im, (256, w*256/h), preserve_range=True)\n",
    "    else:\n",
    "        im = skimage.transform.resize(im, (h*256/w, 256), preserve_range=True)\n",
    "    h, w, _ = im.shape\n",
    "    im = im[h//2-112:h//2+112, w//2-112:w//2+112]\n",
    "    rawim = np.copy(im).astype('uint8')\n",
    "    im = np.swapaxes(np.swapaxes(im, 1, 2), 0, 1)\n",
    "    im = im[::-1, :, :]\n",
    "    im = im - mean_values\n",
    "    return rawim, floatX(im[np.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Lets take five images and compare prediction of Lasagne with Caffe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 5\n",
    "m = 5\n",
    "i = 0\n",
    "for url in image_urls:\n",
    "    print url\n",
    "    try:\n",
    "        rawim, im = prep_image(url)\n",
    "    except:\n",
    "        print 'Failed to download'\n",
    "        continue\n",
    "\n",
    "    prob_lasangne = np.array(lasagne.layers.get_output(net['prob'], im, deterministic=True).eval())[0]\n",
    "    prob_caffe = net_caffe.forward_all(data=im)['prob'][0]\n",
    "\n",
    "    \n",
    "    print 'Lasagne:'\n",
    "    res = sorted(zip(classes, prob_lasangne), key=lambda t: t[1], reverse=True)[:n]\n",
    "    for c, p in res:\n",
    "        print '  ', c, p\n",
    "        \n",
    "    print 'Caffe:'\n",
    "    res = sorted(zip(classes, prob_caffe), key=lambda t: t[1], reverse=True)[:n]\n",
    "    for c, p in res:\n",
    "        print '  ', c, p\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(rawim.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    i += 1\n",
    "    if i == m:\n",
    "        break\n",
    "    \n",
    "    print '\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = {\n",
    "    'values': lasagne.layers.get_all_param_values(net['prob']),\n",
    "    'synset_words': classes,\n",
    "    'mean_image': mean_values\n",
    "}\n",
    "\n",
    "pickle.dump(model, open('./resnet50.pkl', 'wb'), protocol=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
