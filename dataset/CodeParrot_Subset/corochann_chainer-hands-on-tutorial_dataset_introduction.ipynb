{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Dataset module introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial setup following http://docs.chainer.org/en/stable/tutorial/basic.html\n",
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions\n",
    "import chainer.dataset\n",
    "import chainer.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in dataset modules\n",
    "\n",
    "Some dataset format is already implemented in `chainer.datasets`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TupleDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type: <class 'chainer.datasets.tuple_dataset.TupleDataset'>, len: 10\n"
     ]
    }
   ],
   "source": [
    "from chainer.datasets import TupleDataset\n",
    "\n",
    "x = np.arange(10)\n",
    "t = x * x\n",
    "\n",
    "data = TupleDataset(x, t)\n",
    "\n",
    "print('data type: {}, len: {}'.format(type(data), len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TupleDataset' object has no attribute 'shape'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c2d494b81f2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# Unlike numpy, it does not have shape property.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'TupleDataset' object has no attribute 'shape'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Unlike numpy, it does not have shape property.\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`i`-th data can be accessed by `data[i]`\n",
    "\n",
    "which is a tuple of format ($x_i$, $t_i$, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get forth data -> x=3, t=9\n",
    "data[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slice accessing\n",
    "\n",
    "When TupleDataset is accessed by slice indexing, e.g. `data[i:j]`, returned value is __list of tuple__\n",
    "$[(x_i, t_i), ..., (x_{j-1}, t_{j-1})]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (1, 1), (2, 4), (3, 9)]\nexamples type: <class 'list'>, len: 4\n"
     ]
    }
   ],
   "source": [
    "# Get 1st, 2nd, 3rd data at the same time.\n",
    "examples = data[0:4]\n",
    "\n",
    "print(examples)\n",
    "print('examples type: {}, len: {}'\n",
    "      .format(type(examples), len(examples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert examples into minibatch format, you can use `concat_examples` function in `chainer.dataset`.\n",
    "\n",
    "Its return value is in format `([x_array], [t array], ...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_minibatch = [0 1 2 3], type: <class 'numpy.ndarray'>, shape: (4,)\nt_minibatch = [0 1 4 9], type: <class 'numpy.ndarray'>, shape: (4,)\n"
     ]
    }
   ],
   "source": [
    "from chainer.dataset import concat_examples\n",
    "\n",
    "data_minibatch = concat_examples(examples)\n",
    "\n",
    "#print(data_minibatch)\n",
    "#print('data_minibatch type: {}, len: {}'\n",
    "#      .format(type(data_minibatch), len(data_minibatch)))\n",
    "\n",
    "x_minibatch, t_minibatch = data_minibatch\n",
    "# Now it is array format, which has shape\n",
    "print('x_minibatch = {}, type: {}, shape: {}'.format(x_minibatch, type(x_minibatch), x_minibatch.shape))\n",
    "print('t_minibatch = {}, type: {}, shape: {}'.format(t_minibatch, type(t_minibatch), t_minibatch.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DictDataset\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data type: <class 'chainer.datasets.dict_dataset.DictDataset'>, len: 10\n"
     ]
    }
   ],
   "source": [
    "from chainer.datasets import DictDataset\n",
    "\n",
    "x = np.arange(10)\n",
    "t = x * x\n",
    "\n",
    "# To construct `DictDataset`, you can specify each key-value pair by passing \"key=value\" in kwargs.\n",
    "data = DictDataset(x=x, t=t)\n",
    "\n",
    "print('data type: {}, len: {}'.format(type(data), len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t': 4, 'x': 2}\nexamples type: <class 'dict'>, len: 2\nx: 2, t: 4\n"
     ]
    }
   ],
   "source": [
    "# Get 3rd data at the same time.\n",
    "example = data[2]\n",
    "          \n",
    "print(example)\n",
    "print('examples type: {}, len: {}'\n",
    "      .format(type(example), len(example)))\n",
    "\n",
    "# You can access each value via key\n",
    "print('x: {}, t: {}'.format(example['x'], example['t']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImageDataset\n",
    "\n",
    "This is util class for image dataset.\n",
    "\n",
    "If the number of dataset becomes very big (for example ImageNet dataset), \n",
    "it is not practical to load all the images into memory unlike CIFAR-10 or CIFAR-100.\n",
    "\n",
    "In this case, `ImageDataset` class can be used to open image from storage everytime of minibatch creation.\n",
    "\n",
    "[Note] `ImageDataset` will download only the images, if you need another label information \n",
    "(for example if you are working with image classification task) use `LabeledImageDataset` instead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to create a text file which contains the list of image paths to use `ImageDataset`.\n",
    "See `data/images.dat` for how the paths text file look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_dataset type: <class 'chainer.datasets.image_dataset.ImageDataset'>, len: 10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from chainer.datasets import ImageDataset\n",
    "\n",
    "# print('Current direcotory: ', os.path.abspath(os.curdir))\n",
    "\n",
    "filepath = './data/images.dat'\n",
    "image_dataset = ImageDataset(filepath, root='./data/images')\n",
    "\n",
    "print('image_dataset type: {}, len: {}'.format(type(image_dataset), len(image_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created the `image_dataset` above, however, images are not expanded into memory yet.\n",
    "\n",
    "Image data will be loaded into memory from storage every time when you access via index, for efficient memory use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img <class 'numpy.ndarray'> (3, 426, 640)\n"
     ]
    }
   ],
   "source": [
    "# Access i-th image by image_dataset[i].\n",
    "# image data is loaded here. for only 0-th image.\n",
    "img = image_dataset[0]\n",
    "\n",
    "# img is numpy array, already aligned as (channels, height, width), \n",
    "# which is the standard shape format to feed into convolutional layer.\n",
    "print('img', type(img), img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LabeledImageDataset\n",
    "\n",
    "This is util class for image dataset.\n",
    "\n",
    "It is similar to `ImageDataset` to allow load the image file from storage into memory at runtime of training.\n",
    "The difference is that it contains label information, which is usually used for image classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to create a text file which contains the list of image paths and labels to use `LabeledImageDataset`.\n",
    "See `data/images_labels.dat` for how the text file look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled_image_dataset type: <class 'chainer.datasets.image_dataset.LabeledImageDataset'>, len: 10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from chainer.datasets import LabeledImageDataset\n",
    "\n",
    "# print('Current direcotory: ', os.path.abspath(os.curdir))\n",
    "\n",
    "filepath = './data/images_labels.dat'\n",
    "labeled_image_dataset = LabeledImageDataset(filepath, root='./data/images')\n",
    "\n",
    "print('labeled_image_dataset type: {}, len: {}'.format(type(labeled_image_dataset), len(labeled_image_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created the `labeled_image_dataset` above, however, images are not expanded into memory yet.\n",
    "\n",
    "Image data will be loaded into memory from storage every time when you access via index, for efficient memory use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img <class 'numpy.ndarray'> (3, 426, 640)\nlabel <class 'numpy.ndarray'> 0\n"
     ]
    }
   ],
   "source": [
    "# Access i-th image and label by image_dataset[i].\n",
    "# image data is loaded here. for only 0-th image.\n",
    "img, label = labeled_image_dataset[0]\n",
    "\n",
    "print('img', type(img), img.shape)\n",
    "print('label', type(label), label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SubDataset\n",
    "\n",
    "TBD\n",
    "\n",
    "It can be used for cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "split_dataset_n_random() missing 2 required positional arguments: 'dataset' and 'n'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ef1a22e41dd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_dataset_n_random\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: split_dataset_n_random() missing 2 required positional arguments: 'dataset' and 'n'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "datasets.split_dataset_n_random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement your own custom dataset\n",
    "\n",
    "You can define your own dataset by implementing a sub class of `DatasetMixin` in `chainer.dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DatasetMixin\n",
    "\n",
    "If you want to define custom dataset, `DatasetMixin` provides the base function to make compatible with other dataset format.\n",
    "\n",
    "Another important usage for `DatasetMixin` is to __preprocess__ the input data, including __data augmentation__.\n",
    "\n",
    "To implement subclass of `DatasetMixin`, you usually need to implement these 3 functions.\n",
    " - Override `__init__(self, *args)` function: It is not compulsary but\n",
    " - Override `__len__(self)` function        : Iterator need to know the length of this dataset to understand the end of epoch.\n",
    " - Override `get_examples(self, i)` function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer.dataset import DatasetMixin\n",
    "\n",
    "\n",
    "print_debug = True\n",
    "class SimpleDataset(DatasetMixin):\n",
    "    def __init__(self, values):\n",
    "        self.values = values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.values)\n",
    "\n",
    "    def get_example(self, i):\n",
    "        if print_debug: \n",
    "            print('get_example, i = {}'.format(i))\n",
    "        return self.values[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important function in `DatasetMixin` is `get_examples(self, i)` function. \n",
    "This function is called when they access data[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_data = SimpleDataset([0, 1, 4, 9, 16, 25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_example, i = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_example(self, i) is called when data is accessed by data[i]\n",
    "simple_data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_example, i = 1\nget_example, i = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 4]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data can be accessed using slice indexing as well\n",
    "\n",
    "simple_data[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important point is that `get_example` function is called every time when the data is accessed by [] indexing.\n",
    "\n",
    "Thus you may put random value generation for data augmentation code in get_example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from chainer.dataset import DatasetMixin\n",
    "\n",
    "print_debug = False\n",
    "\n",
    "\n",
    "def calc(x):\n",
    "    return x * x\n",
    "\n",
    "\n",
    "class SquareNoiseDataset(DatasetMixin):\n",
    "    def __init__(self, values):\n",
    "        self.values = values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.values)\n",
    "\n",
    "    def get_example(self, i):\n",
    "        if print_debug: \n",
    "            print('get_example, i = {}'.format(i))\n",
    "        x = self.values[i]\n",
    "        t = calc(x) \n",
    "        t_noise = t + np.random.normal(0, 0.1)\n",
    "        return x, t_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_noise_data = SquareNoiseDataset(np.arange(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below `SimpleNoiseDataset` adds small Gaussian noise to the original value,\n",
    "and every time the value is accessed, `get_example` is called and differenct noise is added even if you access to the data with same index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing square_noise_data[3]\n1st:  (3, 8.9710277341058227)\n2nd:  (3, 9.0598517818294599)\n3rd:  (3, 9.070345838019648)\n"
     ]
    }
   ],
   "source": [
    "# Accessing to the same index, but the value is different!\n",
    "print('Accessing square_noise_data[3]', )\n",
    "print('1st: ', square_noise_data[3])\n",
    "print('2nd: ', square_noise_data[3])\n",
    "print('3rd: ', square_noise_data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing square_noise_data[0:4]\n1st:  [(0, -0.14427626899774257), (1, 0.85360656988561656), (2, 3.9732713008069145), (3, 8.9809134295500979)]\n2nd:  [(0, -0.078466401141966013), (1, 0.85183819235205771), (2, 3.9409961378011142), (3, 9.0302699062379599)]\n3rd:  [(0, 0.071952579879583839), (1, 1.025589783563474), (2, 4.10475859520119), (3, 9.0260985190124767)]\n"
     ]
    }
   ],
   "source": [
    "# Same applies for slice index accessing.\n",
    "print('Accessing square_noise_data[0:4]')\n",
    "print('1st: ', square_noise_data[0:4])\n",
    "print('2nd: ', square_noise_data[0:4])\n",
    "print('3rd: ', square_noise_data[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert examples into minibatch format, you can use `concat_examples` function in `chainer.dataset` in the sameway explained at TupleDataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples = [(0, -0.011602612124293674), (1, 0.99882934589681571), (2, 3.9934740900218269), (3, 9.1771593657407067)]\nx_minibatch = [0 1 2 3], type: <class 'numpy.ndarray'>, shape: (4,)\nt_minibatch = [-0.01160261  0.99882935  3.99347409  9.17715937], type: <class 'numpy.ndarray'>, shape: (4,)\n"
     ]
    }
   ],
   "source": [
    "from chainer.dataset import concat_examples\n",
    "\n",
    "examples = square_noise_data[0:4]\n",
    "print('examples = {}'.format(examples))\n",
    "data_minibatch = concat_examples(examples)\n",
    "\n",
    "x_minibatch, t_minibatch = data_minibatch\n",
    "# Now it is array format, which has shape\n",
    "print('x_minibatch = {}, type: {}, shape: {}'.format(x_minibatch, type(x_minibatch), x_minibatch.shape))\n",
    "print('t_minibatch = {}, type: {}, shape: {}'.format(t_minibatch, type(t_minibatch), t_minibatch.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformDataset\n",
    "\n",
    "Transform dataset can be used to create/modify dataset from existing dataset.\n",
    "New (modified) dataset can be created by `TransformDataset(original_data, transform_function)`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a concrete example to create new dataset from original tuple dataset by adding a small noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer.datasets import TransformDataset\n",
    "\n",
    "x = np.arange(10)\n",
    "t = x * x - x\n",
    "\n",
    "original_dataset = TupleDataset(x, t)\n",
    "\n",
    "def transform_function(in_data):\n",
    "    x_i, t_i = in_data\n",
    "    new_t_i = t_i + np.random.normal(0, 0.1)\n",
    "    return x_i, new_t_i\n",
    "\n",
    "transformed_dataset = TransformDataset(original_dataset, transform_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (1, 0), (2, 2)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_dataset[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, -0.045087054953009055),\n (1, -0.042453714041671031),\n (2, 1.9586232300993316)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now Gaussian noise is added (in transform_function) to the original_dataset.\n",
    "transformed_dataset[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}