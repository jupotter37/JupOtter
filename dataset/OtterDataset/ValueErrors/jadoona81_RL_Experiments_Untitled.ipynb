{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f562ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MK19823\\.conda\\envs\\PythonTensor\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\MK19823\\.conda\\envs\\PythonTensor\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\MK19823\\.conda\\envs\\PythonTensor\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\MK19823\\.conda\\envs\\PythonTensor\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\MK19823\\.conda\\envs\\PythonTensor\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\MK19823\\.conda\\envs\\PythonTensor\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\MK19823\\.conda\\envs\\PythonTensor\\lib\\site-packages\\stable_baselines\\__init__.py:33: UserWarning: stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\n",
      "  \"stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\MK19823\\\\.conda\\\\envs\\\\PythonTensor\\\\lib\\\\site-packages\\\\ipykernel_launcher.py', '-f', 'C:\\\\Users\\\\MK19823\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-494b37cc-f897-4277-a093-f14c572dae8c.json']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5b772fed77b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;31m#arguments       minEpisodes      approach    areaValsIndex      numTargets      stepsLimitPerEpisode   train/test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m         \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-5b772fed77b6>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[0mstateRep\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'TargetCov'\u001b[0m\u001b[1;31m#'CurrSeen' #'TargetCov CurrSeen#@@@@@@@@*@*@*@*@*@*@*@*@*@*@*@*@ ((((( CHANGE ))))) @*@*@*@*@*@*@*@*@*@*@*@*@@@@@@@@@\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m     \u001b[0mminEpisodes\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m     \u001b[0mMobilityModel\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#\"ManhattanPython\" #RPGM #\"TM\" #StepsMatlab #Manhatten\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m     \u001b[0mareaValsIndex\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#0-4 new vals, 5 old vals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '-f'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Jun 18 21:15:19 2020\n",
    "\n",
    "@author: ebaccourepbesaid\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "from stable_baselines.deepq.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines import DQN\n",
    "\n",
    "#from Env import NetworkEnv\n",
    "from OneDroneMobileTargetsEnv import OneDroneMobileTargetsEnv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "import drone\n",
    "import baselinePolicy\n",
    "import math\n",
    "from Benchmark.TSP_greedy import TSP_greedy\n",
    "from MobilityPatterns.GenerateMobilityData import generateMobilityData\n",
    "\n",
    "def runTrainedModel(env, modelName, targetsDatFolder, mobilityModel):\n",
    "\n",
    "    tRandomLocs= False\n",
    "    \n",
    "    if(len(env.targets)==0):\n",
    "        env.createTargets(False)\n",
    "\n",
    "    \n",
    "    model = DQN.load(\"PKL_Files/\"+modelName+\"_\"+str(env.stepsLimit)+\".pkl\")\n",
    "    model.exploration_fraction=0\n",
    "    env.testing=True\n",
    "    env.testFilesFolder=targetsDatFolder\n",
    "   # env.stepsLimit= 5*env.stepsLimit\n",
    "    obs = env.reset()\n",
    "    print('done reset')\n",
    "    done= False\n",
    "    totalReward=0\n",
    "    states=[]\n",
    "    \n",
    "    while not done:\n",
    "        print('obs'+str(obs))\n",
    "        states.append(obs)\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, rewards, done, info = env.step(action)\n",
    "        totalReward+=rewards\n",
    "        print('action'+ str(action))\n",
    "\n",
    "        #env.render()\n",
    "    print(env.route)\n",
    "    print(totalReward)\n",
    "    print(env.timeStep)\n",
    "    \n",
    "    return[totalReward, env.timeStep]\n",
    "\n",
    "    \n",
    "def runDRL(total_timesteps, env, droneAgent, modelName, mobilityModel, stateRep):\n",
    "    env.testing=False\n",
    "    env.trainingStarted=True\n",
    "    tRandomLocs= True\n",
    "    env.createTargets(True)\n",
    "\n",
    "    with open(\"TargetsInitLocFiles/TargetsIniLocs_\"+modelName+\"_\"+str(env.stepsLimit)+\".dat\", \"w\") as expfile:\n",
    "\n",
    "        if(mobilityModel == \"RPGM\"):\n",
    "            for i in range(env.numTargets):\n",
    "                expfile.write(str(env.RPGM.NodeGroups[i]) +'\\t')\n",
    "            expfile.write('\\n')\n",
    "                \n",
    "            for g in range(env.RPGM.numGroups):\n",
    "                expfile.write(str(g)+'\\t'+str(env.RPGM.GroupsInitLocX[g])+'\\t'+str(env.RPGM.GroupsInitLocY[g])+'\\n')\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            for t in env.targets:\n",
    "                if(mobilityModel == \"ManhattanPython\"):\n",
    "                    mnObj= t.ManMobObject\n",
    "                    expfile.write(str(t.ID)+'\\t'+str(t.initialLocation[0])+'\\t'+str(t.initialLocation[1])+'\\t'+str(mnObj.direction)+'\\t'+str(mnObj.line_manhattan)+'\\t'+str(mnObj.column_manhattan)+'\\t'+str(mnObj.speed)+'\\n')\n",
    "    \n",
    "                elif(mobilityModel == \"TM\"):\n",
    "                    transitionMatrix= t.M\n",
    "                    expfile.write(str(t.ID)+'\\t'+str(t.initialLocation[0])+'\\t'+str(t.initialLocation[1]))\n",
    "                    \n",
    "                    for c in transitionMatrix:\n",
    "                        neighbors= transitionMatrix[c]\n",
    "                        expfile.write('\\t'+str(c)+','+str(len(neighbors)))\n",
    "                        for n in neighbors:\n",
    "                            expfile.write(','+str(n)+':'+str(neighbors[n]))\n",
    "                    expfile.write('\\n')\n",
    "                            \n",
    "                elif(mobilityModel == \"ShowUpPattern\"):\n",
    "\n",
    "                    expfile.write(str(t.ID)+'\\t'+str(t.initialLocation[0])+'\\t'+str(t.initialLocation[1]))\n",
    "                    cells= t.subcells\n",
    "                    prs= t.showUpPrs\n",
    "                    \n",
    "                    expfile.write('\\t'+str(len(cells)))\n",
    "                    for i in range(len(cells)):\n",
    "                        expfile.write('\\t'+str(cells[i])+','+str(prs[i]))\n",
    "                        \n",
    "                    expfile.write('\\n')\n",
    "\n",
    "                    #folderName= 'MobilityPatterns/'+mobilityModel+'/A'+str(int(env.areaDimLimit))+'_N'+str(env.numTargets)\n",
    "                    \n",
    "                    # with open(folderName+\"/Node\"+str(t.ID)+\".dat\", \"w\") as targetfile:\n",
    "                    #     targetfile.write('TS'+'\\t'+'X'+'\\t'+'Y'+'\\n')\n",
    "                            \n",
    "                    #     for i in range(env.stepsLimit+1):\n",
    "                    #         cellID= t.MobPerTimeStep[i]\n",
    "                    #         currentCell= env.gridlowCorners[cellID]\n",
    "                    #         targetfile.write(str(i)+'\\t'+str(currentCell[0])+'\\t'+str(currentCell[1])+'\\n')\n",
    "\n",
    "                \n",
    "                else:\n",
    "                    expfile.write(str(t.ID)+'\\t'+str(t.initialLocation[0])+'\\t'+str(t.initialLocation[1])+'\\n')\n",
    "                \n",
    "        ###################### DQL #########################\n",
    "\t#env = SubprocVecEnv([lambda:  NetworkEnv() for i in range(1)])\n",
    "    model = DQN(MlpPolicy, env, verbose=0)#, exploration_fraction=0.005)#,cliprange_vf=-1)\n",
    "    print(model.exploration_fraction)\n",
    "    model.learn(total_timesteps) #2261475#3000000\n",
    "    model.save(\"PKL_Files/\"+modelName+\"_\"+str(env.stepsLimit)+\".pkl\")\n",
    "    \n",
    "            \n",
    "    \n",
    "def runBaseline(env, droneAgent, targetsDatFolder):\n",
    "    tRandomLocs=False\n",
    "    if(len(env.targets)==0):\n",
    "        env.createTargets(tRandomLocs)\n",
    "\n",
    "    env.testing=True\n",
    "    env.testFilesFolder=targetsDatFolder\n",
    "\n",
    "   # env.stepsLimit= 5*env.stepsLimit\n",
    "    env.reset()\n",
    "\n",
    "    [steps, finalReward, energy, numTargets]=baselinePolicy.runBaselinePolicy(env, droneAgent)\n",
    "\n",
    "    print(finalReward)\n",
    "    print(steps)\n",
    "    print(numTargets)\n",
    "    \n",
    "    return[finalReward, steps, energy, numTargets]\n",
    "\n",
    "    \n",
    "def runBenchmark(env, targetsDatFolder, reward_scale, MobilityModel, horizontalEnergyPerMeter_J):\n",
    "    \n",
    "    env.reset()\n",
    "    \n",
    "    maxTimeSteps= env.stepsLimit\n",
    "    \n",
    "    print(env.numTargets)\n",
    "    \n",
    "    tsp= TSP_greedy(env.numTargets, maxTimeSteps, len(env.gridlowCorners), env.numCellsPerSide, env.gridlowCorners, env.cellSideSize, targetsDatFolder, reward_scale, MobilityModel, horizontalEnergyPerMeter_J)\n",
    "    tsp.runOptimization()\n",
    "\n",
    "    # for t in tsp.nodes:\n",
    "    #     print(tsp.nodes[t].mobilityCells)\n",
    "    #     print(tsp.nodes[t].mobilityRoute)\n",
    "    energy= tsp.totalDist * horizontalEnergyPerMeter_J\n",
    "\n",
    "    print(tsp.route)\n",
    "    print(tsp.reward)\n",
    "    print(tsp.totalSteps)\n",
    "    \n",
    "    return [tsp.reward, tsp.totalSteps, energy, len(tsp.coveredTargets)]\n",
    "\n",
    "        \n",
    "\n",
    "def train(minEpisodes, stepsLimitPerEpisode, areaSize, numTargets, coverageMinorSemiAxis,  coverageMajorSemiAxis, numCells, cellSideSize, altitude, cameraAngle, modelName, MobilityModel, reward_scale, learning_rate, gamma, targetsDatFolder, stateRep):\n",
    "    \n",
    "    testing= False\n",
    "    env = OneDroneMobileTargetsEnv(stepsLimitPerEpisode, areaSize, numTargets, coverageMinorSemiAxis*2, numCells, cellSideSize,  MobilityModel, reward_scale, testing, targetsDatFolder, stateRep) \n",
    "\n",
    "    total_timesteps= minEpisodes*env.stepsLimit\n",
    "    \n",
    "    startLocsForTargets={}\n",
    "    for t in env.targets:\n",
    "        startLocsForTargets[t]= t.initialLocation()\n",
    "\n",
    "    droneAgent= drone.drone(MobilityModel, areaSize, coverageMinorSemiAxis,  coverageMajorSemiAxis, altitude, cameraAngle, learning_rate, gamma, env.gridlowCorners, \n",
    "               env.cellSideSize, env.targets, \n",
    "               actions=list(range(env.n_actions)))\n",
    "    \n",
    "    env.placeDrone(droneAgent)\n",
    "\n",
    "    #######################################\n",
    "    runDRL(total_timesteps, env, droneAgent, modelName, MobilityModel, stateRep) #set Testing False\n",
    "    \n",
    "    \n",
    "def test(areaSize, numTargets, coverageMinorSemiAxis,  coverageMajorSemiAxis, numCells, cellSideSize, modelName, MobilityModel, reward_scale, learning_rate, gamma, targetsDatFolder, squareSideLen, stateRep):\n",
    "\n",
    "    testing= True\n",
    "    env = OneDroneMobileTargetsEnv(areaSize, numTargets, coverageMinorSemiAxis*2, numCells, cellSideSize, MobilityModel, reward_scale, testing, targetsDatFolder, stateRep) \n",
    "\n",
    "    startLocsForTargets={}\n",
    "    for t in env.targets:\n",
    "        startLocsForTargets[t]= t.initialLocation()\n",
    "\n",
    "    droneAgent= drone.drone(areaSize, learning_rate, gamma, env.gridlowCorners, \n",
    "               env.cellSideSize, env.targets, \n",
    "               actions=list(range(env.n_actions)))\n",
    "    \n",
    "    env.placeDrone(droneAgent)\n",
    "\n",
    "    #generateMobilityData(env.stepsLimit*5, numTargets, squareSideLen, squareSideLen, env.cellSideSize, MobilityModel, modelName, False)\n",
    "    #runBaseline(env, droneAgent, targetsDatFolder)\n",
    "\n",
    "    runTrainedModel(env, modelName, targetsDatFolder, MobilityModel)\n",
    "\n",
    "    #runBenchmark(env, targetsDatFolder, reward_scale)\n",
    "\n",
    "\n",
    "\n",
    "def runExperiment(stepsLimitPerEpisode, areaSize, numTargets, coverageMinorSemiAxis, coverageMajorSemiAxis, numCells, cellSideSize, altitude, cameraAngle, modelName, MobilityModel, reward_scale, learning_rate, gamma, targetsDatFolder, areaSquareSideLen, stateRep):\n",
    "\n",
    "    numRuns= 50\n",
    "    DRL= True\n",
    "    Bench= True\n",
    "    Base= True\n",
    "\n",
    "    testing= True\n",
    "\n",
    "    rBM_List=[]\n",
    "    rDRL_List=[]\n",
    "    rBL_List=[]\n",
    "    \n",
    "    sBM_List=[]\n",
    "    sDRL_List=[]\n",
    "    sBL_List=[]\n",
    "    \n",
    "    eBM_List=[]\n",
    "    eDRL_List=[]\n",
    "    eBL_List=[]\n",
    "    \n",
    "    tBM_List=[]\n",
    "    tDRL_List=[]\n",
    "    tBL_List=[]\n",
    "    \n",
    "    with open(\"ExperimentsFiles/Experiment_\"+modelName+\"_.dat\", \"w\") as expfile:\n",
    "        expfile.write('Steps_BM'+'\\t'+'Steps_DRL'+'\\t'+'Steps_BL'+'\\t'+\n",
    "                      'Reward_BM'+'\\t'+'Reward_DRL'+'\\t'+'Reward_BL'+'\\t'+\n",
    "                      'Energy_BM'+'\\t'+'Energy_DRL'+'\\t'+'Energy_BL'+'\\t'+\n",
    "                      'Targets_BM'+'\\t'+'Targets_DRL'+'\\t'+'Targets_BL'+'\\n')\n",
    "    \n",
    "    #        expfile.write('Steps_BM'+'\\t'+'Reward_BM'+'\\t'+#'ener_BM'+'\\t'+'NumCovTargets_BL'+'\\t'+\n",
    "    #                      'Steps_DRL'+'\\t'+'Reward_DRL'+'\\t'+#'ener_DRL'+'\\t'+'NumCovTargets_DRL'+'\\t'+\n",
    "    #                      'Steps_BL'+'\\t'+'Reward_BL'+'\\n') #'\\t'+'ener_BL'+'\\t'+'NumCovTargets_BL'+'\\n')\n",
    "\n",
    "    for i in range(numRuns):\n",
    "\n",
    "        with open(\"ExperimentsFiles/Experiment_\"+modelName+\"_.dat\", \"a\") as expfile:\n",
    "\n",
    "            env = OneDroneMobileTargetsEnv(stepsLimitPerEpisode, areaSize, numTargets, coverageMinorSemiAxis*2, numCells, cellSideSize,  MobilityModel, reward_scale, testing, targetsDatFolder, stateRep) \n",
    "\n",
    "            startLocsForTargets={}\n",
    "            for t in env.targets:\n",
    "                startLocsForTargets[t]= t.initialLocation()\n",
    "\n",
    "            droneAgent= drone.drone(MobilityModel, areaSize, coverageMinorSemiAxis,  coverageMajorSemiAxis, altitude, cameraAngle, learning_rate, gamma, env.gridlowCorners, \n",
    "                           env.cellSideSize, env.targets, \n",
    "                           actions=list(range(env.n_actions)))\n",
    "\n",
    "            env.placeDrone(droneAgent)\n",
    "                \n",
    "#################\n",
    "            #if(not MobilityModel=='ShowUpPattern'):\n",
    "            generateMobilityData(env.gridlowCorners, env.stepsLimit, numTargets, areaSquareSideLen, areaSquareSideLen, env.cellSideSize, MobilityModel, modelName, False, env.timeStepsScale, env.numCellsPerSide)\n",
    "                \n",
    "            if(Bench):\n",
    "                [rBM, sBM, eBM, tBM]= runBenchmark(env, targetsDatFolder, reward_scale, MobilityModel, droneAgent.horizontalEnergyPerMeter_J)\n",
    "            else:\n",
    "                [rBM, sBM, eBM, tBM] = [0,0,0,0]\n",
    "                \n",
    "            rBM_List.append(rBM)\n",
    "            sBM_List.append(sBM)\n",
    "            eBM_List.append(eBM)\n",
    "            tBM_List.append(tBM)\n",
    "\n",
    "##############\n",
    "            \n",
    "            if(not Bench and DRL):\n",
    "                env = OneDroneMobileTargetsEnv(stepsLimitPerEpisode, areaSize, numTargets, coverageMinorSemiAxis*2, numCells, cellSideSize,  MobilityModel, reward_scale, testing, targetsDatFolder, stateRep) \n",
    "    \n",
    "                startLocsForTargets={}\n",
    "                for t in env.targets:\n",
    "                    startLocsForTargets[t]= t.initialLocation()\n",
    "                \n",
    "                droneAgent= drone.drone(MobilityModel, areaSize,coverageMinorSemiAxis,  coverageMajorSemiAxis, altitude, cameraAngle, learning_rate, gamma, env.gridlowCorners, \n",
    "                                env.cellSideSize, env.targets, \n",
    "                                actions=list(range(env.n_actions)))\n",
    "                    \n",
    "                env.placeDrone(droneAgent)\n",
    "            \n",
    "            if(DRL):\n",
    "                [rDRL, sDRL]= runTrainedModel(env, modelName, targetsDatFolder, MobilityModel)\n",
    "                eDRL= droneAgent.totalEnergy\n",
    "                tDRL= len(droneAgent.coveredTargets)\n",
    "            else:\n",
    "                [rDRL, sDRL, eDRL, rDRL]= [0,0, 0,0]\n",
    "            \n",
    "            rDRL_List.append(rDRL)\n",
    "            sDRL_List.append(sDRL)\n",
    "            eDRL_List.append(eDRL)\n",
    "            tDRL_List.append(tDRL)\n",
    "\n",
    "# ##############\n",
    "\n",
    "            if(not Bench and Base):\n",
    "                env = OneDroneMobileTargetsEnv(stepsLimitPerEpisode, areaSize, numTargets, coverageMinorSemiAxis*2, numCells, cellSideSize,  MobilityModel, reward_scale, testing, targetsDatFolder, stateRep) \n",
    "    \n",
    "                startLocsForTargets={}\n",
    "                for t in env.targets:\n",
    "                    startLocsForTargets[t]= t.initialLocation()\n",
    "                \n",
    "                droneAgent= drone.drone(MobilityModel, areaSize, coverageMinorSemiAxis,  coverageMajorSemiAxis, altitude, cameraAngle, learning_rate, gamma, env.gridlowCorners, \n",
    "                                env.cellSideSize, env.targets, \n",
    "                                actions=list(range(env.n_actions)))\n",
    "                    \n",
    "                env.placeDrone(droneAgent)\n",
    "\n",
    "            if(Base):\n",
    "                [rBL, sBL, eBL, tBL] = runBaseline(env, droneAgent, targetsDatFolder)\n",
    "            else:\n",
    "                [rBL, sBL, eBL, tBL] = [0,0,0,0]\n",
    "\n",
    "            rBL_List.append(rBL)\n",
    "            sBL_List.append(sBL)\n",
    "            eBL_List.append(eBL)\n",
    "            tBL_List.append(tBL)\n",
    "            \n",
    "# #############\n",
    "            \n",
    "            print('Trained Model')\n",
    "            print(rDRL)\n",
    "            print(sDRL)\n",
    "            print(tDRL)\n",
    "            \n",
    "            print('Benchmark')\n",
    "            print(rBM)\n",
    "            print(sBM)\n",
    "            print(tBM)\n",
    "            # expfile.write(str(sBM)+'\\t'+str(rBM)+'\\t'+#'ener_BM'+'\\t'+'NumCovTargets_BL'+'\\t'+\n",
    "            #                  str(sDRL)+'\\t'+str(rDRL)+'\\t'+#'ener_DRL'+'\\t'+'NumCovTargets_DRL'+'\\t'+\n",
    "            #                  str(sBL)+'\\t'+str(rBL)+'\\n') #'\\t'+'ener_BL'+'\\t'+'NumCovTargets_BL'+'\\n')\n",
    "            \n",
    "            expfile.write(str(sBM)+'\\t'+str(sDRL)+'\\t'+str(sBL)+'\\t'+\n",
    "                           str(rBM)+'\\t'+str(rDRL)+'\\t'+str(rBL)+'\\t'+\n",
    "                           str(eBM)+'\\t'+str(eDRL)+'\\t'+str(eBL)+'\\t'+\n",
    "                           str(tBM)+'\\t'+str(tDRL)+'\\t'+str(tBL)+'\\n')\n",
    "            \n",
    "    return [eBM_List, eDRL_List, eBL_List, rBM_List, rDRL_List, rBL_List, sBM_List, sDRL_List, sBL_List]\n",
    "\n",
    "\n",
    "\n",
    "def runExperiments(variance, stepsLimitPerEpisode, areaSize, numTargets, coverageMinorSemiAxis, coverageMajorSemiAxis, numCells, cellSideSize, altitude, cameraAngle, modelName, MobilityModel, reward_scale, learning_rate, gamma, targetsDatFolder, squareSideLen, stateRep):\n",
    "    \n",
    "\n",
    "    \n",
    "    targetsVals=[1, 5, 10, 30, 50]\n",
    "    \n",
    "    areaSideLenList=[999, 3000, 4998, 7000, 10000]\n",
    "    numCellsList= [9, 25, 36, 49, 64]\n",
    "    cellSideLenList= [333, 600, 833, 1000, 1250]\n",
    "    altitudesForAreas= [40, 70, 100, 120, 150]\n",
    "    cameraAngle= 55\n",
    "    semiMinorAxisList= [235.46655813512035, 424.26406871192853, 589.0199487283942, 707.1067811865476, 883.8834764831845]\n",
    "    semiMajorAxisList= [357.7172565323477, 645.6331250169183, 894.8621579652718, 1074.2898002735851, 1342.8622503419815]\n",
    "\n",
    "\n",
    "    mobilityModelVals= ['TM', 'ManhattanPython', 'RPGM']\n",
    "    \n",
    "    if(variance == 'targets'):\n",
    "        varList= targetsVals\n",
    "    elif(variance == 'cells'):\n",
    "        varList = numCellsList\n",
    "    else:\n",
    "        varList = mobilityModelVals\n",
    "\n",
    "    \n",
    "    titlePrefix=\"Title\"\n",
    "    filePrefix=variance+\"Variation_\"\n",
    "    \n",
    "    with open(filePrefix+\"ener.dat\", \"w\") as enerfile:\n",
    "          enerfile.write(titlePrefix+\n",
    "                      '\\t'+'BM'+'\\t'+'BM_min'+'\\t'+'BM_max'+\n",
    "                      '\\t'+'DRL'+'\\t'+'DRL_min'+'\\t'+'DRL_max'+\n",
    "                      '\\t'+'BL'+'\\t'+'BL_min'+'\\t'+'BL_max'+\n",
    "                      '\\n')\n",
    "    \n",
    "    with open(filePrefix+\"reward.dat\", \"w\") as rewardfile:\n",
    "          rewardfile.write(titlePrefix+\n",
    "                      '\\t'+'BM'+'\\t'+'BM_min'+'\\t'+'BM_max'+\n",
    "                      '\\t'+'DRL'+'\\t'+'DRL_min'+'\\t'+'DRL_max'+\n",
    "                      '\\t'+'BL'+'\\t'+'BL_min'+'\\t'+'BL_max'+\n",
    "                      '\\n')\n",
    "    \n",
    "    with open(filePrefix+\"steps.dat\", \"w\") as stepsfile:\n",
    "          stepsfile.write(titlePrefix+\n",
    "                      '\\t'+'BM'+'\\t'+'BM_min'+'\\t'+'BM_max'+\n",
    "                      '\\t'+'DRL'+'\\t'+'DRL_min'+'\\t'+'DRL_max'+\n",
    "                      '\\t'+'BL'+'\\t'+'BL_min'+'\\t'+'BL_max'+\n",
    "                      '\\n')\n",
    "\n",
    "\n",
    "    areaValsIndex= 2\n",
    "    targetsNominal= 10\n",
    "    mobilityNominal= 'ManhattanPython'\n",
    "\n",
    "    numTargets= targetsNominal\n",
    "    MobilityModel= mobilityNominal\n",
    "    \n",
    "        \n",
    "    for val in varList: #targetsVariations: #areaSizeVariations: coverageVariations\n",
    "        if(variance == 'targets'):\n",
    "            numTargets= val\n",
    "        elif(variance == 'cells'):\n",
    "            areaValsIndex= numCellsList.index(val)\n",
    "            numCells= val\n",
    "        else:\n",
    "            MobilityModel= val\n",
    "\n",
    "        coverageMajorSemiAxis= semiMajorAxisList[areaValsIndex] #3.5355339059327\n",
    "        coverageMinorSemiAxis= semiMinorAxisList[areaValsIndex] # 2.83 #2.53553390593275\n",
    "        areaSquareSideLen= areaSideLenList[areaValsIndex] #36  #>> #numCells  #2*numCells  #1.5*numCells\n",
    "        numCells= numCellsList[areaValsIndex] #6\n",
    "        cellSideSize = cellSideLenList[areaValsIndex]  #4\n",
    "        altitude= altitudesForAreas[areaValsIndex] \n",
    "\n",
    "        [eBM_List, eDRL_List, eBL_List, rBM_List, rDRL_List, rBL_List, sBM_List, sDRL_List, sBL_List]= runExperiment(stepsLimitPerEpisode, areaSize, numTargets, coverageMinorSemiAxis, coverageMajorSemiAxis, numCells, cellSideSize, altitude, cameraAngle, modelName, MobilityModel, reward_scale, learning_rate, gamma, targetsDatFolder, squareSideLen)\n",
    "        \n",
    "        with open(filePrefix+\"ener.dat\", \"a\") as enerfile:\n",
    "            enerfile.write(titlePrefix+\n",
    "                      '\\t'+str(sum(eBM_List)/len(eBM_List))+'\\t'+str(min(eBM_List))+'\\t'+str(max(eBM_List))+\n",
    "                      '\\t'+str(sum(eDRL_List)/len(eDRL_List))+'\\t'+str(min(eDRL_List))+'\\t'+str(max(eDRL_List))+\n",
    "                      '\\t'+str(sum(eBL_List)/len(eBL_List))+'\\t'+str(min(eBL_List))+'\\t'+str(max(eBL_List))+\n",
    "                      '\\n')\n",
    "    \n",
    "        with open(filePrefix+\"reward.dat\", \"a\") as rewardfile:\n",
    "            rewardfile.write(titlePrefix+\n",
    "                      '\\t'+str(sum(rBM_List)/len(rBM_List))+'\\t'+str(min(rBM_List))+'\\t'+str(max(rBM_List))+\n",
    "                      '\\t'+str(sum(rDRL_List)/len(rDRL_List))+'\\t'+str(min(rDRL_List))+'\\t'+str(max(rDRL_List))+\n",
    "                      '\\t'+str(sum(rBL_List)/len(rBL_List))+'\\t'+str(min(rBL_List))+'\\t'+str(max(rBL_List))+\n",
    "                      '\\n')\n",
    "    \n",
    "        with open(filePrefix+\"steps.dat\", \"a\") as stepsfile:\n",
    "            stepsfile.write(titlePrefix+\n",
    "                      '\\t'+str(sum(sBM_List)/len(sBM_List))+'\\t'+str(min(sBM_List))+'\\t'+str(max(sBM_List))+\n",
    "                      '\\t'+str(sum(sDRL_List)/len(sDRL_List))+'\\t'+str(min(sDRL_List))+'\\t'+str(max(sDRL_List))+\n",
    "                      '\\t'+str(sum(sBL_List)/len(sBL_List))+'\\t'+str(min(sBL_List))+'\\t'+str(max(sBL_List))+\n",
    "                      '\\n')\n",
    "\n",
    "def TrainExperiment(variance, areaSize, numTargets, coverageMinorSemiAxis, coverageMajorSemiAxis, numCells, cellSideSize, altitude, cameraAngle, modelName, MobilityModel, reward_scale, learning_rate, gamma, targetsDatFolder, squareSideLen, stateRep):\n",
    "    \n",
    "    targetsVals=[1, 5, 10, 30, 50]\n",
    "    \n",
    "    areaSideLenList=[999, 3000, 4998, 7000, 10000]\n",
    "    numCellsList= [9, 25, 36, 49, 64]\n",
    "    cellSideLenList= [333, 600, 833, 1000, 1250]\n",
    "    altitudesForAreas= [40, 70, 100, 120, 150]\n",
    "    cameraAngle= 55\n",
    "    semiMinorAxisList= [235.46655813512035, 424.26406871192853, 589.0199487283942, 707.1067811865476, 883.8834764831845]\n",
    "    semiMajorAxisList= [357.7172565323477, 645.6331250169183, 894.8621579652718, 1074.2898002735851, 1342.8622503419815]\n",
    "\n",
    "    mobilityModelVals= ['TM', 'ManhattanPython', 'RPGM']\n",
    "    \n",
    "    if(variance == 'targets'):\n",
    "        varList= targetsVals\n",
    "    elif(variance == 'cells'):\n",
    "        varList = numCellsList\n",
    "    else:\n",
    "        varList = mobilityModelVals\n",
    "\n",
    "    areaValsIndex= 2\n",
    "    targetsNominal= 10\n",
    "    mobilityNominal= 'ManhattanPython'\n",
    "\n",
    "    numTargets= targetsNominal\n",
    "    MobilityModel= mobilityNominal\n",
    "    \n",
    "        \n",
    "    for val in varList: #targetsVariations: #areaSizeVariations: coverageVariations\n",
    "        if(variance == 'targets'):\n",
    "            numTargets= val\n",
    "        elif(variance == 'cells'):\n",
    "            areaValsIndex= numCellsList.index(val)\n",
    "            numCells= val\n",
    "        else:\n",
    "            MobilityModel= val\n",
    "\n",
    "        coverageMajorSemiAxis= semiMajorAxisList[areaValsIndex] #3.5355339059327\n",
    "        coverageMinorSemiAxis= semiMinorAxisList[areaValsIndex] # 2.83 #2.53553390593275\n",
    "        areaSquareSideLen= areaSideLenList[areaValsIndex] #36  #>> #numCells  #2*numCells  #1.5*numCells\n",
    "        numCells= numCellsList[areaValsIndex] #6\n",
    "        cellSideSize = cellSideLenList[areaValsIndex]  #4\n",
    "        altitude= altitudesForAreas[areaValsIndex] \n",
    "    \n",
    "        reward_scale= 10\n",
    "        gamma = 0.95    # discount rate\n",
    "        learning_rate = 0.001\n",
    "        areaSize= np.power(areaSquareSideLen, 2) # keep area size as multiple of 4    \n",
    "        targetsDatFolder='MobilityPatterns/'+MobilityModel+'/A'+str(areaSquareSideLen)+'_N'+ str(numTargets)\n",
    "        modelName= \"A\"+str(areaSquareSideLen)+\"_T\"+str(numTargets)+\"_\"+MobilityModel+\"_\"+stateRep\n",
    "\n",
    "        train(areaSize, numTargets, coverageMinorSemiAxis, coverageMajorSemiAxis, numCells, cellSideSize, altitude, cameraAngle, modelName, MobilityModel, reward_scale, learning_rate, gamma, targetsDatFolder)\n",
    "\n",
    "def testGeneratedData(stepsLimitPerEpisode, areaSize, numTargets, coverageMinorSemiAxis, coverageMajorSemiAxis, numCells, cellSideSize, altitude, cameraAngle, modelName, MobilityModel, reward_scale, learning_rate, gamma, targetsDatFolder, areaSquareSideLen, stateRep):\n",
    "\n",
    "    testing = True\n",
    "    env = OneDroneMobileTargetsEnv(stepsLimitPerEpisode, areaSize, numTargets, coverageMinorSemiAxis*2, numCells, cellSideSize,  MobilityModel, reward_scale, testing, targetsDatFolder, stateRep) \n",
    "\n",
    "    startLocsForTargets={}\n",
    "    for t in env.targets:\n",
    "        startLocsForTargets[t]= t.initialLocation()\n",
    "\n",
    "    droneAgent= drone.drone(MobilityModel, areaSize, coverageMinorSemiAxis,  coverageMajorSemiAxis, altitude, cameraAngle, learning_rate, gamma, env.gridlowCorners, \n",
    "                           env.cellSideSize, env.targets, \n",
    "                           actions=list(range(env.n_actions)))\n",
    "\n",
    "    env.placeDrone(droneAgent)\n",
    "    \n",
    "    print('cellsPerSide: '+ str(env.numCellsPerSide))\n",
    "    print('timeStepsScale: '+ str(env.timeStepsScale))\n",
    "    print('cellSideSize:'+ str(env.cellSideSize))\n",
    "                \n",
    "    generateMobilityData(env.gridlowCorners, env.stepsLimit, numTargets, areaSquareSideLen, areaSquareSideLen, env.cellSideSize, MobilityModel, modelName, False, env.timeStepsScale, env.numCellsPerSide)\n",
    "\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    stateRep= 'TargetCov'#'CurrSeen' #'TargetCov CurrSeen#@@@@@@@@*@*@*@*@*@*@*@*@*@*@*@*@ ((((( CHANGE ))))) @*@*@*@*@*@*@*@*@*@*@*@*@@@@@@@@@\n",
    "    print(args)\n",
    "    minEpisodes= int(args[1])\n",
    "    MobilityModel= args[2] #\"ManhattanPython\" #RPGM #\"TM\" #StepsMatlab #Manhatten\n",
    "    areaValsIndex= int(args[3]) #0-4 new vals, 5 old vals\n",
    "    numTargets= int(args[4]) #[1, 5, 10, 30, 50]\n",
    "\n",
    "    areaSideLenList=[999, 3000, 4998, 7000, 10000, 24, 12, 28]\n",
    "    numCellsList= [9, 25, 36, 49, 64, 36, 9, 49]\n",
    "    cellSideLenList= [333, 600, 833, 1000, 1250, 4, 4, 4]\n",
    "    altitudesForAreas= [40, 70, 100, 120, 150, 20, 20, 20]\n",
    "    cameraAngle= 55\n",
    "    semiMinorAxisList= [235.46655813512035, 424.26406871192853, 589.0199487283942, 707.1067811865476, 883.8834764831845, 2.83, 2.83, 2.83]\n",
    "    semiMajorAxisList= [357.7172565323477, 645.6331250169183, 894.8621579652718, 1074.2898002735851, 1342.8622503419815, 3.5355339059327, 3.5355339059327, 3.5355339059327]\n",
    "\n",
    "    coverageMajorSemiAxis= semiMajorAxisList[areaValsIndex] #3.5355339059327\n",
    "    coverageMinorSemiAxis= semiMinorAxisList[areaValsIndex] # 2.83 \n",
    "    areaSquareSideLen= areaSideLenList[areaValsIndex] #36  #>> #numCells  #2*numCells  #1.5*numCells\n",
    "    numCells= numCellsList[areaValsIndex] #6\n",
    "    cellSideSize = cellSideLenList[areaValsIndex]  #4\n",
    "    altitude= altitudesForAreas[areaValsIndex] #20\n",
    "\n",
    "    stepsLimitPerEpisode= math.ceil(float(args[5])*numCells)\n",
    "\n",
    "\n",
    "    reward_scale= 10 \n",
    "    gamma = 0.95    # discount rate\n",
    "    learning_rate = 0.001\n",
    "    areaSize= np.power(areaSquareSideLen, 2) # keep area size as multiple of 4    \n",
    "    targetsDatFolder='MobilityPatterns/'+MobilityModel+'/A'+str(areaSquareSideLen)+'_N'+ str(numTargets)\n",
    "    modelName= \"A\"+str(areaSquareSideLen)+\"_T\"+str(numTargets)+\"_\"+MobilityModel+\"_\"+stateRep\n",
    "\n",
    "    if(args[6] == \"train\"):\n",
    "        train(minEpisodes, stepsLimitPerEpisode, areaSize, numTargets, coverageMinorSemiAxis, coverageMajorSemiAxis, numCells, cellSideSize, altitude, cameraAngle, modelName, MobilityModel, reward_scale, learning_rate, gamma, targetsDatFolder, stateRep)\n",
    "    else:\n",
    "        #testGeneratedData(stepsLimitPerEpisode, areaSize, numTargets, coverageMinorSemiAxis, coverageMajorSemiAxis, numCells, cellSideSize, altitude, cameraAngle, modelName, MobilityModel, reward_scale, learning_rate, gamma, targetsDatFolder, areaSquareSideLen)\n",
    "\n",
    "        runExperiment(stepsLimitPerEpisode, areaSize, numTargets, coverageMinorSemiAxis,  coverageMajorSemiAxis, numCells, cellSideSize,altitude, cameraAngle, modelName, MobilityModel, reward_scale, learning_rate, gamma, targetsDatFolder, areaSquareSideLen, stateRep)\n",
    "\n",
    "    #test(areaSize, numTargets, coverageMinorSemiAxis, modelName, MobilityModel, reward_scale, learning_rate, gamma, targetsDatFolder, squareSideLen)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #arguments       minEpisodes      approach    areaValsIndex      numTargets      stepsLimitPerEpisode   train/test\n",
    "\tmain(sys.argv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7295683d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\mk19823\\.conda\\envs\\pythontensor\\lib\\site-packages (1.19.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb87cb05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
