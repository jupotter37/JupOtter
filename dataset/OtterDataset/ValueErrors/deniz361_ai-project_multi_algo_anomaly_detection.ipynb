{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "967578a1-87cc-4eec-930c-f742c75104e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved supervised_dataset_chunk_part_1.csv\n",
      "Saved supervised_dataset_chunk_part_2.csv\n",
      "Saved supervised_dataset_chunk_part_3.csv\n",
      "Saved supervised_dataset_chunk_part_4.csv\n",
      "Saved supervised_dataset_chunk_part_5.csv\n",
      "Saved supervised_dataset_chunk_part_6.csv\n",
      "Saved supervised_dataset_chunk_part_7.csv\n",
      "Saved supervised_dataset_chunk_part_8.csv\n",
      "Saved supervised_dataset_chunk_part_9.csv\n",
      "Saved supervised_dataset_chunk_part_10.csv\n",
      "Saved supervised_dataset_chunk_part_11.csv\n",
      "Saved supervised_dataset_chunk_part_12.csv\n",
      "Saved supervised_dataset_chunk_part_13.csv\n",
      "Saved supervised_dataset_chunk_part_14.csv\n",
      "Saved supervised_dataset_chunk_part_15.csv\n",
      "Saved supervised_dataset_chunk_part_16.csv\n",
      "Saved supervised_dataset_chunk_part_17.csv\n",
      "Saved supervised_dataset_chunk_part_18.csv\n",
      "Saved supervised_dataset_chunk_part_19.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def split_csv(file_path, chunk_size, output_prefix):\n",
    "    # Read the large CSV file in chunks\n",
    "    chunk_iter = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "    \n",
    "    # Process each chunk and save it as a new CSV file\n",
    "    for i, chunk in enumerate(chunk_iter):\n",
    "        output_file = f\"{output_prefix}_part_{i+1}.csv\"\n",
    "        chunk.to_csv(output_file, index=False)\n",
    "        print(f\"Saved {output_file}\")\n",
    "\n",
    "# Define the parameters\n",
    "file_path = 'supervised_dataset.csv'\n",
    "chunk_size = 10000  # Adjust the chunk size as needed\n",
    "output_prefix = 'supervised_dataset_chunk'\n",
    "\n",
    "# Split the CSV file into smaller chunks\n",
    "split_csv(file_path, chunk_size, output_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9486255-6dde-4c18-9060-36d260c602cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4189c0e7-16a6-43d2-9bb8-3078c516e3af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37aa3fb7-e6b3-4c9b-8b4d-382b627aecaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 278    1]\n",
      " [   3 1718]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       279\n",
      "           1       1.00      1.00      1.00      1721\n",
      "\n",
      "    accuracy                           1.00      2000\n",
      "   macro avg       0.99      1.00      1.00      2000\n",
      "weighted avg       1.00      1.00      1.00      2000\n",
      "\n",
      "\n",
      "Accuracy Score:\n",
      "0.998\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Plant_2100'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Plant_2100'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 69\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col_index \u001b[38;5;129;01min\u001b[39;00m top_5_columns:\n\u001b[1;32m     68\u001b[0m     column_name \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcolumns[col_index]\n\u001b[0;32m---> 69\u001b[0m     column_data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdropna()  \u001b[38;5;66;03m# Drop NaN values from the column\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m column_data\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     71\u001b[0m         plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/pandas/core/frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Plant_2100'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "file_path = 'supervised_dataset_chunk_part_1.csv'\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(file_path, low_memory=False)\n",
    "data.head()\n",
    "\n",
    "# Define the categorical and numerical columns\n",
    "categorical_columns = [\"Material number\", \"Supplier\", \"Contract\", \"Contract Position\", \"Procurement type\", \n",
    "                       \"Special procurement type\", \"Dispatcher\", \"Buyer\", \"Purchasing group\", \n",
    "                       \"Purchasing lot size\", \"Calendar\", \"Plant\", \"Plant information record\", \n",
    "                       \"Information record number\", \"Information record type\",  \"Product group\",\n",
    "                       \"Base unit\"]\n",
    "numerical_columns = [\"Fulfillment time\", \"Fixed contract 1\", \"Fixed contract 2\", \"Total quantity\", \"Total value\", \n",
    "                     \"Price unit\", \"Plant processing time\", \"Material master time\"]\n",
    "\n",
    "# Convert all categorical columns to strings to handle mixed types\n",
    "for col in categorical_columns:\n",
    "    data[col] = data[col].astype(str)\n",
    "\n",
    "# Handling missing values by replacing them with the median for numerical columns\n",
    "for col in numerical_columns:\n",
    "    if data[col].isna().any():\n",
    "        data[col].fillna(data[col].median(), inplace=True)\n",
    "\n",
    "# One-hot encoding for categorical columns\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoded_categorical_data = encoder.fit_transform(data[categorical_columns])\n",
    "encoded_categorical_df = pd.DataFrame(encoded_categorical_data, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "# Combine numerical and encoded categorical data\n",
    "data_combined = pd.concat([data[numerical_columns], encoded_categorical_df], axis=1)\n",
    "\n",
    "# Define the features (X) and the target (y)\n",
    "X = data_combined\n",
    "y = data['anomaly']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train XGBoost model for anomaly detection\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict anomaly labels using XGBoost\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nAccuracy Score:\")\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Find the top 5 columns with the most importance according to XGBoost\n",
    "top_5_columns = xgb_model.feature_importances_.argsort()[-5:][::-1]\n",
    "\n",
    "# Plot the distributions of the top 5 most important columns\n",
    "for col_index in top_5_columns:\n",
    "    column_name = X.columns[col_index]\n",
    "    column_data = data[column_name].dropna()  # Drop NaN values from the column\n",
    "    if not column_data.empty:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(column_data, bins=50, label=f'Distribution of {column_name}')\n",
    "        plt.title(f'Distribution of {column_name}')\n",
    "        plt.xlabel(column_name)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'{column_name}_distribution.png')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No data available for plotting {column_name}.\")\n",
    "\n",
    "# Save the detected anomaly column in the original DataFrame\n",
    "data['detected_anomaly'] = xgb_model.predict(X)\n",
    "\n",
    "# Save the original DataFrame with detected anomalies to a new CSV file\n",
    "original_data_with_anomalies = data[['detected_anomaly'] + numerical_columns + list(encoded_categorical_df.columns)]  # Include numerical and encoded categorical columns\n",
    "original_data_with_anomalies.to_csv('supervised_dataset_chunk_part_1_with_anomalies_xgboost.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac76c0a-1584-4076-b3de-905a0fb4dc6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47c553d2-58f8-48ad-981d-4403714d61b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n+---------------+--------+----------+\n| Column        | Found  | Expected |\n+---------------+--------+----------+\n| Product group | object | int64    |\n+---------------+--------+----------+\n\nThe following columns also raised exceptions on conversion:\n\n- Product group\n  ValueError(\"invalid literal for int() with base 10: 'LEEB'\")\n\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\ndtype={'Product group': 'object'}\n\nto the call to `read_csv`/`read_table`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupervised_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m data \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupervised_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m data \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mfrom_pandas(\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m, npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Viewing initial data types\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mdtypes)\n",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/pandas/core/frame.py:830\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, abc\u001b[38;5;241m.\u001b[39mSequence):\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    829\u001b[0m         \u001b[38;5;66;03m# GH#44616 big perf improvement for e.g. pytorch tensor\u001b[39;00m\n\u001b[0;32m--> 830\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    832\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/dask_expr/_collection.py:443\u001b[0m, in \u001b[0;36mFrameBase.__array__\u001b[0;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/dask_expr/_collection.py:476\u001b[0m, in \u001b[0;36mFrameBase.compute\u001b[0;34m(self, fuse, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mrepartition(npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    475\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39moptimize(fuse\u001b[38;5;241m=\u001b[39mfuse)\n\u001b[0;32m--> 476\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDaskMethodsMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/dask/base.py:375\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    352\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/dask/base.py:661\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 661\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/dask/dataframe/io/csv.py:142\u001b[0m, in \u001b[0;36mCSVFunctionWrapper.__call__\u001b[0;34m(self, part)\u001b[0m\n\u001b[1;32m    139\u001b[0m         rest_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m columns\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Call `pandas_read_text`\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpandas_read_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrest_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m project_after_read:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns]\n",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/dask/dataframe/io/csv.py:197\u001b[0m, in \u001b[0;36mpandas_read_text\u001b[0;34m(reader, b, header, kwargs, dtypes, columns, write_header, enforce, path)\u001b[0m\n\u001b[1;32m    195\u001b[0m df \u001b[38;5;241m=\u001b[39m reader(bio, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtypes:\n\u001b[0;32m--> 197\u001b[0m     \u001b[43mcoerce_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m enforce \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m(columns)):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns do not match\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mcolumns, columns)\n",
      "File \u001b[0;32m~/.conda/envs/ai/lib/python3.12/site-packages/dask/dataframe/io/csv.py:298\u001b[0m, in \u001b[0;36mcoerce_dtypes\u001b[0;34m(df, dtypes)\u001b[0m\n\u001b[1;32m    294\u001b[0m rule \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m61\u001b[39m)\n\u001b[1;32m    295\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatched dtypes found in `pd.read_csv`/`pd.read_table`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    296\u001b[0m     rule\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, [dtype_msg, date_msg]))\n\u001b[1;32m    297\u001b[0m )\n\u001b[0;32m--> 298\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n+---------------+--------+----------+\n| Column        | Found  | Expected |\n+---------------+--------+----------+\n| Product group | object | int64    |\n+---------------+--------+----------+\n\nThe following columns also raised exceptions on conversion:\n\n- Product group\n  ValueError(\"invalid literal for int() with base 10: 'LEEB'\")\n\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\ndtype={'Product group': 'object'}\n\nto the call to `read_csv`/`read_table`."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import dask.dataframe as dd\n",
    "\n",
    "file_path = 'supervised_dataset.csv'\n",
    "data = dd.read_csv('supervised_dataset.csv')\n",
    "\n",
    "\n",
    "data = dd.from_pandas(pd.DataFrame(data), npartitions=1)\n",
    "\n",
    "# Viewing initial data types\n",
    "print(data.dtypes)\n",
    "\n",
    "# Changing the data type of 'col1' to int32\n",
    "data = data.astype({'Product group': 'object'})\n",
    "\n",
    "# Define the categorical and numerical columns\n",
    "categorical_columns = [\"Material number\", \"Supplier\", \"Contract\", \"Contract Position\", \"Procurement type\", \n",
    "                       \"Special procurement type\", \"Dispatcher\", \"Buyer\", \"Purchasing group\", \n",
    "                       \"Purchasing lot size\", \"Calendar\", \"Plant\", \"Plant information record\", \n",
    "                       \"Information record number\", \"Information record type\",  \"Product group\",\n",
    "                       \"Base unit\"]\n",
    "numerical_columns = [\"Fulfillment time\", \"Fixed contract 1\", \"Fixed contract 2\", \"Total quantity\", \"Total value\", \n",
    "                     \"Price unit\", \"Plant processing time\", \"Material master time\"]\n",
    "\n",
    "# Convert all categorical columns to strings to handle mixed types\n",
    "for col in categorical_columns:\n",
    "    data[col] = data[col].astype(str)\n",
    "\n",
    "# Handling missing values by replacing them with the median for numerical columns\n",
    "for col in numerical_columns:\n",
    "    if data[col].isna().any():\n",
    "        data[col] = data[col].fillna(data[col].median())\n",
    "\n",
    "# One-hot encoding for categorical columns\n",
    "encoder = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
    "encoded_categorical_data = encoder.fit_transform(data[categorical_columns])\n",
    "\n",
    "# Convert encoded categorical data to DataFrame\n",
    "encoded_categorical_df = pd.DataFrame(encoded_categorical_data, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "# Combine numerical and encoded categorical data\n",
    "data_combined = dd.concat([data[numerical_columns], encoded_categorical_df], axis=1)\n",
    "\n",
    "# Define the target (y)\n",
    "y = data['anomaly']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_combined.compute(), y.compute(), test_size=0.2, random_state=42)\n",
    "\n",
    "# Train XGBoost model for anomaly detection\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict anomaly labels using XGBoost\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nAccuracy Score:\")\n",
    "print(accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4e6f69-11ca-4957-bdb8-d5097ea63e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c61df7-cd9e-4b6d-aa28-6be898372bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee1e0f-09c1-4d7a-ad36-35e6604aab9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4b4fac-e6ae-4748-ad52-5cb26e2b8e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144179d1-eb63-4b43-880a-2e2ad1f5fd56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501c1bc1-69dc-4bef-b37f-1a095c6ccdcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc23561-fb52-4467-8c99-fb6afd8c9ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dabbaa3-a22c-44fe-a6e5-902b338e726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee759bcf-2569-428b-84cf-3c1d29f3271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Stammdaten.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b020ebb-37ec-416f-b52d-8844970a05ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"Material number\", \"Supplier\", \"Contract\", \"Contract Position\", \"Procurement type\", \n",
    "                                    \"Special procurement type\", \"Dispatcher\", \"Buyer\", \"Purchasing group\", \n",
    "                                    \"Purchasing lot size\", \"Calendar\", \"Plant\", \"Plant information record\", \n",
    "                                    \"Information record number\", \"Information record type\",  \"Product group\",\n",
    "                                    \"Base unit\"]\n",
    "numerical_columns = [\"Fulfillment time\", \"Fixed contract 1\", \"Fixed contract 2\", \"Total quantity\", \"Total value\", \n",
    "                                  \"Price unit\", \"Plant processing time\", \"Material master time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e33efe-6467-4592-b5bb-4ac7bfd976a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "\n",
    "class Data_Preprocessing():\n",
    "    def __init__(self, file_path) -> None:\n",
    "        self.data = pd.read_csv(file_path, low_memory=False, nrows=3000)\n",
    "        \n",
    "        self.rename_to_english()\n",
    "        \n",
    "        print(self.data.columns)\n",
    "        \n",
    "        # Specify categorical and numerical numbers manually\n",
    "        self.categorical_columns = [\"Material number\", \"Supplier\", \"Contract\", \"Contract Position\", \"Procurement type\", \n",
    "                                    \"Special procurement type\", \"Dispatcher\", \"Buyer\", \"Purchasing group\", \n",
    "                                    \"Purchasing lot size\", \"Calendar\", \"Plant\", \"Plant information record\", \n",
    "                                    \"Information record number\", \"Information record type\",  \"Product group\",\n",
    "                                    \"Base unit\"]\n",
    "        self.numerical_columns = [\"Fulfillment time\", \"Fixed contract 1\", \"Fixed contract 2\", \"Total quantity\", \"Total value\", \n",
    "                                  \"Price unit\", \"Plant processing time\", \"Material master time\"]\n",
    "        \n",
    "        # Initialize MinMaxScaler\n",
    "        self.scaler = MinMaxScaler()\n",
    "    \n",
    "    def rename_to_english(self):\n",
    "        self.data.rename(columns={\"Materialnummer\": \"Material number\", \"Lieferant OB\": \"Supplier\", \"Vertrag OB\": \"Contract\", \n",
    "                                 \"Vertragsposition OB\": \"Contract Position\", \"Planlieferzeit Vertrag\": \"Fulfillment time\", \n",
    "                                 \"Vertrag Fix1\": \"Fixed contract 1\", \"Vertrag_Fix2\": \"Fixed contract 2\", \"Beschaffungsart\": \n",
    "                                 \"Procurement type\", \"Sonderbeschaffungsart\": \"Special procurement type\", \"Disponent\":\n",
    "                                 \"Dispatcher\", \"Einkäufer\": \"Buyer\", \"DispoGruppe\": \"Purchasing group\", \"Dispolosgröße\": \n",
    "                                 \"Purchasing lot size\", \"Gesamtbestand\": \"Total quantity\", \"Gesamtwert\": \"Total value\",\n",
    "                                 \"Preiseinheit\": \"Price unit\", \"Kalender\": \"Calendar\", \"Werk OB\": \"Plant\", \"Werk Infosatz\":\n",
    "                                 \"Plant information record\", \"Infosatznummer\": \"Information record number\", \"Infosatztyp\":\n",
    "                                 \"Information record type\", \"WE-Bearbeitungszeit\": \"Plant processing time\", \"Planlieferzeit Mat-Stamm\":\n",
    "                                 \"Material master time\", \"Warengruppe\": \"Product group\", \"Basiseinheit\": \"Base unit\"}, inplace=True)\n",
    "\n",
    "    def normalize_data(self, numerical_data):\n",
    "        # Fit and transform the data\n",
    "        normalized_data = self.scaler.fit_transform(numerical_data)\n",
    "\n",
    "        return normalized_data\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        self.data[self.categorical_columns] = self.data[self.categorical_columns].astype('category')\n",
    "        self.data[self.numerical_columns] = self.data[self.numerical_columns].astype('int64')\n",
    "\n",
    "\n",
    "        # If Delivery time is 0, the value is missing\n",
    "        self.data[\"Fulfillment time\"] = self.data[\"Fulfillment time\"].replace(0, np.nan)\n",
    "        self.data[\"Material master time\"] = self.data[\"Material master time\"].replace(0, np.nan)\n",
    "\n",
    "        # If processing time is 0, the value is missing\n",
    "        self.data[\"Plant processing time\"] = self.data[\"Plant processing time\"].replace(0, np.nan)\n",
    "\n",
    "        # If total quantity is 0, the value is missing\n",
    "        self.data[\"Total quantity\"] = self.data[\"Total quantity\"].replace(0, np.nan)\n",
    "\n",
    "        # If total value is 0, the toal value is not known or missing\n",
    "        self.data[\"Total value\"] = self.data[\"Total value\"].replace(0, np.nan)\n",
    "\n",
    "        self.data[\"Fixed contract 1\"] = self.data[\"Fixed contract 1\"].replace(0, np.nan)\n",
    "        self.data[\"Fixed contract 2\"] = self.data[\"Fixed contract 2\"].replace(0, np.nan)\n",
    "\n",
    "\n",
    "        return self.data\n",
    "    \n",
    "\n",
    "    def preprocess_dbscan(self, data):\n",
    "        numerical_columns = [\"Fulfillment time\", \"Fixed contract 1\"]\n",
    "        data = data[numerical_columns]\n",
    "\n",
    "        # Remove rows with NaN values\n",
    "        data_without_nan = data.dropna(axis=0)\n",
    "\n",
    "        return data_without_nan\n",
    "    \n",
    "    def preprocess_data_kmean(self):\n",
    "        \"\"\"\n",
    "        Preprocess the data by imputing missing values, performing one-hot encoding for categorical variables,\n",
    "        and performing feature normalization.\n",
    "\n",
    "        Parameters:\n",
    "        - data: pandas DataFrame containing the dataset\n",
    "\n",
    "        Returns:\n",
    "        - processed_data: pandas DataFrame containing imputed missing values, one-hot encoded features, and normalized features\n",
    "        \"\"\"\n",
    "        # Separate numeric and categorical columns\n",
    "        # numeric_cols = data.select_dtypes(include=np.number).columns\n",
    "        # categorical_cols = data.select_dtypes(include='object').columns\n",
    "\n",
    "        #data = data[numerical_columns]\n",
    "\n",
    "        self.data=self.preprocess_data()\n",
    "\n",
    "        not_scaled_data = self.data.copy()\n",
    "\n",
    "        # categorical_cols = [\"Materialnummer\", \"Lieferant OB\", \"Vertragsposition OB\", \"Beschaffungsart\", \"Disponent\", \"Einkäufer\", \"Dispolosgröße\", \"Werk OB\", \"Warengruppe\", \"Basiseinheit\"]\n",
    "        # numeric_cols = [\"Planlieferzeit Vertrag\", \"Vertrag Fix1\", \"Vertrag_Fix2\", \"Gesamtbestand\", \"Gesamtwert\", \"Preiseinheit\", \"WE-Bearbeitungszeit\", \"Planlieferzeit Mat-Stamm\"]\n",
    "        \n",
    "        # Impute missing values using mean imputation for numeric columns\n",
    "        # imputer = SimpleImputer(strategy='mean')\n",
    "        # data_numeric_imputed = pd.DataFrame(imputer.fit_transform(self.data[self.numerical_columns]), columns=self.numerical_columns)\n",
    "\n",
    "        # One-hot encode categorical variables\n",
    "        if len(self.categorical_columns) > 0:\n",
    "            encoder = OneHotEncoder(drop='first')\n",
    "            data_encoded = encoder.fit_transform(self.data[self.categorical_columns])\n",
    "            column_names = encoder.get_feature_names_out(self.categorical_columns)\n",
    "            data_imputed_encoded = pd.DataFrame(data_encoded.toarray(), columns=column_names)\n",
    "        else:\n",
    "            data_imputed_encoded = pd.DataFrame()\n",
    "\n",
    "        # Combine numeric and encoded categorical columns\n",
    "        processed_data = pd.concat([self.data[self.numerical_columns], data_imputed_encoded], axis=1)\n",
    "\n",
    "\n",
    "        # Normalize features\n",
    "        scaler = StandardScaler()\n",
    "        processed_data = pd.DataFrame(scaler.fit_transform(processed_data), columns=processed_data.columns)\n",
    "\n",
    "\n",
    "        return processed_data, not_scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3f4531-264e-4c12-b497-becf87004383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from gensim.models import Word2Vec\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "class Data_Preprocessing:\n",
    "    def __init__(self, file_path) -> None:\n",
    "        self.data = pd.read_csv(file_path, low_memory=False, nrows=3000)\n",
    "        \n",
    "        self.rename_to_english()\n",
    "        \n",
    "        print(self.data.columns)\n",
    "        \n",
    "        # Specify categorical and numerical numbers manually\n",
    "        self.categorical_columns = [\"Material number\", \"Supplier\", \"Contract\", \"Contract Position\", \"Procurement type\", \n",
    "                                    \"Special procurement type\", \"Dispatcher\", \"Buyer\", \"Purchasing group\", \n",
    "                                    \"Purchasing lot size\", \"Calendar\", \"Plant\", \"Plant information record\", \n",
    "                                    \"Information record number\", \"Information record type\",  \"Product group\",\n",
    "                                    \"Base unit\"]\n",
    "        self.numerical_columns = [\"Fulfillment time\", \"Fixed contract 1\", \"Fixed contract 2\", \"Total quantity\", \"Total value\", \n",
    "                                  \"Price unit\", \"Plant processing time\", \"Material master time\"]\n",
    "        \n",
    "        # Initialize MinMaxScaler\n",
    "        self.scaler = MinMaxScaler()\n",
    "    \n",
    "    def rename_to_english(self):\n",
    "        self.data.rename(columns={\"Materialnummer\": \"Material number\", \"Lieferant OB\": \"Supplier\", \"Vertrag OB\": \"Contract\", \n",
    "                                 \"Vertragsposition OB\": \"Contract Position\", \"Planlieferzeit Vertrag\": \"Fulfillment time\", \n",
    "                                 \"Vertrag Fix1\": \"Fixed contract 1\", \"Vertrag_Fix2\": \"Fixed contract 2\", \"Beschaffungsart\": \n",
    "                                 \"Procurement type\", \"Sonderbeschaffungsart\": \"Special procurement type\", \"Disponent\":\n",
    "                                 \"Dispatcher\", \"Einkäufer\": \"Buyer\", \"DispoGruppe\": \"Purchasing group\", \"Dispolosgröße\": \n",
    "                                 \"Purchasing lot size\", \"Gesamtbestand\": \"Total quantity\", \"Gesamtwert\": \"Total value\",\n",
    "                                 \"Preiseinheit\": \"Price unit\", \"Kalender\": \"Calendar\", \"Werk OB\": \"Plant\", \"Werk Infosatz\":\n",
    "                                 \"Plant information record\", \"Infosatznummer\": \"Information record number\", \"Infosatztyp\":\n",
    "                                 \"Information record type\", \"WE-Bearbeitungszeit\": \"Plant processing time\", \"Planlieferzeit Mat-Stamm\":\n",
    "                                 \"Material master time\", \"Warengruppe\": \"Product group\", \"Basiseinheit\": \"Base unit\"}, inplace=True)\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        self.data[self.categorical_columns] = self.data[self.categorical_columns].astype('category')\n",
    "        self.data[self.numerical_columns] = self.data[self.numerical_columns].astype('int64')\n",
    "\n",
    "\n",
    "        # If Delivery time is 0, the value is missing\n",
    "        self.data[\"Fulfillment time\"] = self.data[\"Fulfillment time\"].replace(0, np.nan)\n",
    "        self.data[\"Material master time\"] = self.data[\"Material master time\"].replace(0, np.nan)\n",
    "\n",
    "        # If processing time is 0, the value is missing\n",
    "        self.data[\"Plant processing time\"] = self.data[\"Plant processing time\"].replace(0, np.nan)\n",
    "\n",
    "        # If total quantity is 0, the value is missing\n",
    "        self.data[\"Total quantity\"] = self.data[\"Total quantity\"].replace(0, np.nan)\n",
    "\n",
    "        # If total value is 0, the toal value is not known or missing\n",
    "        self.data[\"Total value\"] = self.data[\"Total value\"].replace(0, np.nan)\n",
    "\n",
    "        self.data[\"Fixed contract 1\"] = self.data[\"Fixed contract 1\"].replace(0, np.nan)\n",
    "        self.data[\"Fixed contract 2\"] = self.data[\"Fixed contract 2\"].replace(0, np.nan)\n",
    "\n",
    "\n",
    "        return self.data\n",
    "\n",
    "    def preprocess_data_kmean(self):\n",
    "        self.data = self.preprocess_data()\n",
    "        not_scaled_data = self.data.copy()\n",
    "\n",
    "        if len(self.categorical_columns) > 0:\n",
    "            encoder = OneHotEncoder(drop='first')\n",
    "            data_encoded = encoder.fit_transform(self.data[self.categorical_columns])\n",
    "            column_names = encoder.get_feature_names_out(self.categorical_columns)\n",
    "            data_imputed_encoded = pd.DataFrame(data_encoded.toarray(), columns=column_names)\n",
    "        else:\n",
    "            data_imputed_encoded = pd.DataFrame()\n",
    "\n",
    "        processed_data = pd.concat([self.data[self.numerical_columns], data_imputed_encoded], axis=1)\n",
    "        scaler = StandardScaler()\n",
    "        processed_data = pd.DataFrame(scaler.fit_transform(processed_data), columns=processed_data.columns)\n",
    "\n",
    "        return processed_data, not_scaled_data\n",
    "\n",
    "file_path = 'Stammdaten.csv'\n",
    "\n",
    "Data = Data_Preprocessing(file_path=file_path)\n",
    "data, not_processed_data = Data.preprocess_data_kmean()\n",
    "\n",
    "# Handling missing values by replacing them with the median of each column\n",
    "for col in Data.numerical_columns:\n",
    "    if data[col].isna().any():\n",
    "        data[col].fillna(data[col].median(), inplace=True)\n",
    "\n",
    "# Applying Word2Vec for categorical columns\n",
    "categorical_data = not_processed_data[Data.categorical_columns].astype(str).values.tolist()\n",
    "w2v_model = Word2Vec(sentences=categorical_data, vector_size=100, window=5, min_count=1, workers=4, seed=42)\n",
    "word_vectors = w2v_model.wv\n",
    "\n",
    "# Create embeddings for each categorical feature\n",
    "for col in Data.categorical_columns:\n",
    "    not_processed_data[col + '_embedding'] = not_processed_data[col].apply(lambda x: word_vectors[x])\n",
    "\n",
    "# Combine all embeddings into a single feature set\n",
    "embedding_features = np.array(not_processed_data[[col + '_embedding' for col in Data.categorical_columns]].values.tolist())\n",
    "embedding_features = embedding_features.reshape(len(not_processed_data), -1)\n",
    "\n",
    "# Combine numerical and embedding features\n",
    "X = np.hstack((not_processed_data[Data.numerical_columns].values, embedding_features))\n",
    "\n",
    "# Applying Z-score for anomaly detection in numeric columns\n",
    "for col in Data.numerical_columns:\n",
    "    data[col + '_z_score'] = np.abs(stats.zscore(data[col]))\n",
    "    data[col + '_outlier'] = 0\n",
    "    data.loc[data[col + '_z_score'] > 3, col + '_outlier'] = 1\n",
    "\n",
    "data['anomaly_label'] = data[[col + '_outlier' for col in Data.numerical_columns]].max(axis=1)\n",
    "\n",
    "# Train XGBoost model for anomaly detection\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "xgb_model.fit(X, data['anomaly_label'])\n",
    "\n",
    "# Predict anomaly labels using XGBoost\n",
    "data['xgb_anomaly'] = xgb_model.predict(X)\n",
    "\n",
    "# Save the detected anomaly column in the original DataFrame\n",
    "original_data = pd.read_csv(file_path)\n",
    "original_data['detected_anomaly'] = data['xgb_anomaly']\n",
    "\n",
    "# Save the original DataFrame with detected anomalies to a new CSV file\n",
    "original_data.to_csv('updated_with_anomalies_xgboost.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f63696-7a2f-405f-ba05-253ce23a20b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6286a3ce-92f4-49eb-a32e-0178213d3915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c550138d-d493-4f04-b765-08361fa07aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"Material number\", \"Supplier\", \"Contract\", \"Contract Position\", \"Procurement type\", \n",
    "                                    \"Special procurement type\", \"Dispatcher\", \"Buyer\", \"Purchasing group\", \n",
    "                                    \"Purchasing lot size\", \"Calendar\", \"Plant\", \"Plant information record\", \n",
    "                                    \"Information record number\", \"Information record type\",  \"Product group\",\n",
    "                                    \"Base unit\"]\n",
    "numerical_columns = [\"Fulfillment time\", \"Fixed contract 1\", \"Fixed contract 2\", \"Total quantity\", \"Total value\", \n",
    "                                  \"Price unit\", \"Plant processing time\", \"Material master time\"]\n",
    "        \n",
    "\n",
    "\n",
    "file_path = 'supervised_dataset.csv'\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Handling missing values by replacing them with the median of each column\n",
    "for col in numerical_columns:\n",
    "    if data[col].isna().any():\n",
    "        data[col].fillna(data[col].median(), inplace=True)\n",
    "\n",
    "# Applying Z-score for anomaly detection in numeric columns\n",
    "for col in numerical_columns:\n",
    "    data[col + '_z_score'] = np.abs(stats.zscore(data[col]))\n",
    "    data[col + '_outlier'] = 0\n",
    "    data.loc[data[col + '_z_score'] > 3, col + '_outlier'] = 1  # Any Z-score > 3 is considered an outlier\n",
    "\n",
    "# Combine all outlier flags to a single anomaly label\n",
    "data['anomaly_label'] = data[[col + '_outlier' for col in numerical_columns]].max(axis=1)\n",
    "\n",
    "# Train XGBoost model for anomaly detection\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "xgb_model.fit(data[numerical_columns], data['anomaly_label'])\n",
    "\n",
    "# Predict anomaly labels using XGBoost\n",
    "data['xgb_anomaly'] = xgb_model.predict(data[numerical_columns])\n",
    "\n",
    "# Find the top 5 columns with the most anomalies predicted by XGBoost\n",
    "top_5_columns = xgb_model.feature_importances_.argsort()[-5:][::-1]\n",
    "\n",
    "# Plot the distributions of the top 5 most anomalous columns\n",
    "for col_index in top_5_columns:\n",
    "    column_name = numerical_columns[col_index]\n",
    "    column_data = data[column_name].dropna()  # Drop NaN values from the column\n",
    "    if not column_data.empty:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(column_data, bins=50, label=f'Distribution of {column_name}')\n",
    "        plt.title(f'Distribution of {column_name}')\n",
    "        plt.xlabel(column_name)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'{column_name}_distribution.png')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No data available for plotting {column_name}.\")\n",
    "\n",
    "\n",
    "# Save the detected anomaly column in the original DataFrame\n",
    "data['detected_anomaly'] = np.where(data['anomaly_label'] == 1, 1, 0)\n",
    "\n",
    "# Save the original DataFrame with detected anomalies to a new CSV file\n",
    "original_data_with_anomalies = data[['detected_anomaly'] + numerical_columns]  # Include only numerical columns\n",
    "original_data_with_anomalies.to_csv('supervised_dataset_with_anomalies_xgboost.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc88fd0-dcf7-44b1-a626-7634bc3678ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e69c4b9-9978-4a86-a5f7-a3a833a567cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac1edc-260e-48d4-8c6e-c33a319dc425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25f9e28-b6cd-4413-af2d-e09cc71cc2e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470498b-6893-4c83-a57d-8352a49ef44e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191709a0-e303-4b78-b7f2-cf490c446eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3665c8b4-6475-4ad1-983f-386257562282",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"Material number\", \"Supplier\", \"Contract\", \"Contract Position\", \"Procurement type\", \n",
    "                                    \"Special procurement type\", \"Dispatcher\", \"Buyer\", \"Purchasing group\", \n",
    "                                    \"Purchasing lot size\", \"Calendar\", \"Plant\", \"Plant information record\", \n",
    "                                    \"Information record number\", \"Information record type\",  \"Product group\",\n",
    "                                    \"Base unit\"]\n",
    "numerical_columns = [\"Fulfillment time\", \"Fixed contract 1\", \"Fixed contract 2\", \"Total quantity\", \"Total value\", \n",
    "                                  \"Price unit\", \"Plant processing time\", \"Material master time\"]\n",
    "        \n",
    "\n",
    "\n",
    "file_path = 'Stammdaten.csv'\n",
    "\n",
    "Data=Data_Preprocessing(file_path=file_path)\n",
    "data, not_processed_data= Data.preprocess_data_kmean()\n",
    "print(data)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Encode categorical variables\n",
    "#categorical_columns = data.select_dtypes(include=['object']).columns\n",
    "#data[categorical_columns] = data[categorical_columns].apply(lambda col: pd.factorize(col)[0])\n",
    "\n",
    "\n",
    "# Handling missing values by replacing them with the median of each column\n",
    "# for col in numerical_columns:\n",
    "#     if data[col].isna().any():\n",
    "#         data[col].fillna(data[col].median(), inplace=True)\n",
    "\n",
    "# Applying Z-score for anomaly detection in numeric columns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Assuming 'data' is your DataFrame containing numerical features\n",
    "\n",
    "# Calculate mean and standard deviation for each numerical column\n",
    "mean_values = data[numerical_columns].mean()\n",
    "std_dev_values = data[numerical_columns].std()\n",
    "\n",
    "# Calculate Z-scores for each numerical column\n",
    "z_scores = (data[numerical_columns] - mean_values) / std_dev_values\n",
    "\n",
    "# Optionally, you can add the Z-scores as new columns to the existing DataFrame\n",
    "for col in numerical_columns:\n",
    "    data[f'{col}_z_score'] = z_scores[col]\n",
    "\n",
    "\n",
    "# Create a DataFrame to store the outlier flags\n",
    "outlier_flags = pd.DataFrame()\n",
    "\n",
    "# Apply outlier detection threshold (e.g., Z-score > 3) to identify outliers\n",
    "for col in numerical_columns:\n",
    "    outlier_flags[col + '_outlier'] = (z_scores[col] > 3).astype(int)\n",
    "\n",
    "# Combine all outlier flags to a single anomaly label\n",
    "data['anomaly_label'] = outlier_flags.max(axis=1)\n",
    "\n",
    "# No need to drop outlier flags columns since they haven't been added yet\n",
    "# Concatenate the anomaly label column with the input data\n",
    "data = pd.concat([data, outlier_flags], axis=1)\n",
    "\n",
    "\n",
    "# Combine all outlier flags to a single anomaly label\n",
    "data['anomaly_label'] = data[[col + '_outlier' for col in numerical_columns]].max(axis=1)\n",
    "\n",
    "\n",
    "features = data.drop(columns=[col + '_z_score' for col in numerical_columns])\n",
    "# Remove columns related to anomaly detection from the input data\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'data' is your DataFrame containing numerical features and 'anomaly_label' is the binary target variable\n",
    "\n",
    "# Split data into features and target variable\n",
    "X = data.drop(columns=['anomaly_label'])\n",
    "y = data['anomaly_label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "xgb_model.fit(X, y)\n",
    "\n",
    "# Predict anomalies on the testing data\n",
    "y_pred_proba = xgb_model.predict_proba(X)[:, 1]  # Probability of being an outlier\n",
    "y_pred = xgb_model.predict(X)  # Binary prediction (0 or 1)\n",
    "\n",
    "# You can now use y_pred_proba or y_pred for further analysis or evaluation\n",
    "\n",
    "\n",
    "# You can now use y_pred_proba or y_pred for further analysis or evaluation\n",
    "\n",
    "data_without_nan = data.dropna(subset=['Total quantity_z_score'])\n",
    "\n",
    "print(data['anomaly_label'])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to create scatter plots for anomalies\n",
    "def visualize_anomalies(data, anomaly_labels):\n",
    "    # Scatter plot for \"Total quantity\" column\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(data['Total quantity'], data['Total value'], c=anomaly_labels, cmap='coolwarm')\n",
    "    plt.xlabel('Total quantity')\n",
    "    plt.ylabel('Total quantity')\n",
    "    plt.title('Anomalies in Total Quantity')\n",
    "    plt.colorbar(label='Anomaly Label')\n",
    "    plt.show()\n",
    "\n",
    "# Assuming you have already predicted anomaly labels for the dataset\n",
    "# Replace 'anomaly_labels' with your actual anomaly labels\n",
    "visualize_anomalies(data, y_pred)\n",
    "\n",
    "# Visualize statistical outliers in one of the numeric columns\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(data_without_nan['Total quantity_z_score'],bins=50,  label='Z-scores')\n",
    "plt.axvline(3, color='red', linestyle='dashed', linewidth=2, label='Outlier Threshold')\n",
    "plt.title('Histogram of Z-scores for Total Inventory')\n",
    "plt.xlabel('Z-score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Define a function to create scatter plots for anomalies\n",
    "# def visualize_anomalies2(data, anomaly_labels):\n",
    "#     # Pairplot for all numerical features\n",
    "#     sns.pairplot(data, hue=da'anomaly_labels', palette={0: 'blue', 1: 'red'})\n",
    "#     plt.title('Scatter Plot of Numerical Features with Anomalies')\n",
    "#     plt.show()\n",
    "\n",
    "# # Visualize anomalies\n",
    "# visualize_anomalies2(data[numerical_columns], y_pred)\n",
    "\n",
    "\n",
    "# Visualize statistical outliers in one of the numeric columns\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(data_without_nan['Total quantity_z_score'],  label='Z-scores')\n",
    "plt.axvline(3, color='red', linestyle='dashed', linewidth=2, label='Outlier Threshold')\n",
    "plt.title('Histogram of Z-scores for Total Inventory')\n",
    "plt.xlabel('Z-score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Find the top 5 columns with the most anomalies predicted by XGBoost\n",
    "top_5_columns = xgb_model.feature_importances_.argsort()[-5:][::-1]\n",
    "\n",
    "# Plot the distributions of the top 5 most anomalous columns\n",
    "for col_index in top_5_columns:\n",
    "    column_name = numerical_columns[col_index]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(data[column_name+'_z_score'], bins=50, label=f'Distribution of Z Score in {column_name}')\n",
    "    plt.title(f'Distribution of {column_name}')\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{column_name}_distribution.png')\n",
    "    plt.show()\n",
    "# Save the updated dataset\n",
    "data.to_csv('updated_with_anomalies_xgboost_stammdaten.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd52438-bfca-4037-92d0-127387521c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and preprocess your data\n",
    "file_path = 'Stammdaten.csv'\n",
    "# Assuming the Data_Preprocessing class is defined elsewhere that you can import\n",
    "Data = Data_Preprocessing(file_path=file_path)\n",
    "data, not_processed_data = Data.preprocess_data_kmean()\n",
    "\n",
    "# Define numerical columns\n",
    "numerical_columns = [\"Fulfillment time\", \"Fixed contract 1\", \"Fixed contract 2\", \"Total quantity\", \"Total value\", \n",
    "                                  \"Price unit\", \"Plant processing time\", \"Material master time\"]\n",
    "\n",
    "# Handling missing values by replacing them with the median of each column\n",
    "for col in numerical_columns:\n",
    "    if data[col].isna().any():\n",
    "        data[col].fillna(data[col].median(), inplace=True)\n",
    "\n",
    "# Calculate mean and standard deviation for each numerical column\n",
    "mean_values = data[numerical_columns].mean()\n",
    "std_dev_values = data[numerical_columns].std()\n",
    "\n",
    "# Calculate Z-scores for each numerical column\n",
    "z_scores = (data[numerical_columns] - mean_values) / std_dev_values\n",
    "\n",
    "# Optionally, you can add the Z-scores as new columns to the existing DataFrame\n",
    "for col in numerical_columns:\n",
    "    data[f'{col}_z_score'] = z_scores[col]\n",
    "\n",
    "# Create an outlier flag for each Z-score\n",
    "for col in numerical_columns:\n",
    "    data[col + '_outlier'] = (np.abs(data[f'{col}_z_score']) > 3).astype(int)\n",
    "\n",
    "# Combine all outlier flags to a single anomaly label\n",
    "data['anomaly_label'] = data[[col + '_outlier' for col in numerical_columns]].max(axis=1)\n",
    "\n",
    "# Split data into features and target variable for model fitting\n",
    "X = data[numerical_columns]  # Use only the original numerical features for model training\n",
    "y = data['anomaly_label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict anomalies on the testing data\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Find the top 5 columns with the most anomalies predicted by XGBoost\n",
    "top_5_columns = xgb_model.feature_importances_.argsort()[-5:][::-1]\n",
    "\n",
    "# Plot the distributions of the top 5 most anomalous columns\n",
    "for col_index in top_5_columns:\n",
    "    column_name = numerical_columns[col_index]\n",
    "    # Check if the column contains only NaN values\n",
    "    if data[column_name].isna().all():\n",
    "        print(f\"Skipping plot for {column_name} as it contains only NaN values.\")\n",
    "        continue\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    try:\n",
    "        plt.hist(data[column_name].dropna(), bins=50, label=f'Distribution of {column_name}')  # Ensure to drop NaNs for plotting\n",
    "        plt.title(f'Distribution of {column_name}')\n",
    "        plt.xlabel(column_name)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'{column_name}_distribution.png')\n",
    "        plt.show()\n",
    "    except ValueError as e:\n",
    "        print(f\"Failed to plot {column_name} due to an error: {e}\")\n",
    "\n",
    "\n",
    "# Save the updated dataset\n",
    "data.to_csv('updated_with_anomalies_xgboost_stammdaten.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f334eed6-d9f3-4900-971b-8083eb1c38cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scatter of the top 5 most anomalous columns with anomalies highlighted\n",
    "for col_index in top_5_columns:\n",
    "    column_name = numerical_columns[col_index]\n",
    "    \n",
    "    if data[column_name].isna().all():\n",
    "        print(f\"Skipping plot for {column_name} as it contains only NaN values.\")\n",
    "        continue\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    normal_data = data[data['anomaly_label'] == 0]\n",
    "    anomalies = data[data['anomaly_label'] == 1]\n",
    "\n",
    "    if normal_data.empty or anomalies.empty:\n",
    "        print(f\"No sufficient data to plot for {column_name}.\")\n",
    "        continue\n",
    "\n",
    "    plt.scatter(normal_data[column_name], normal_data[column_name], color='blue', label='Normal', alpha=0.5, edgecolors='w')\n",
    "    plt.scatter(anomalies[column_name], anomalies[column_name], color='red', label='Anomaly', alpha=0.5, edgecolors='w')\n",
    "    \n",
    "    plt.title(f'Scatter Plot for {column_name}')\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel(column_name)  # Adjust as needed\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{column_name}_anomalies_scatter.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adbb056-499c-42c8-bae1-cce2b7d3902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distributions of the top 5 most anomalous columns with anomalies highlighted\n",
    "for col_index in top_5_columns:\n",
    "    column_name = numerical_columns[col_index]\n",
    "    \n",
    "    # Check if the column contains only NaN values\n",
    "    if data[column_name].isna().all():\n",
    "        print(f\"Skipping plot for {column_name} as it contains only NaN values.\")\n",
    "        continue\n",
    "\n",
    "    # Prepare the figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    normal_data = data[data[anomaly_column] == 0]\n",
    "    anomalies = data[data[anomaly_column] == 1]\n",
    "\n",
    "    # Check that there are normal and anomalous points to plot\n",
    "    if normal_data.empty or anomalies.empty:\n",
    "        print(f\"No sufficient data to plot for {column_name}.\")\n",
    "        continue\n",
    "\n",
    "    # Plot normal points\n",
    "    plt.scatter(normal_data[column_name], normal_data[column_name], color='blue', label='Normal', alpha=0.6, edgecolors='w')\n",
    "\n",
    "    # Plot anomalies\n",
    "    plt.scatter(anomalies[column_name], anomalies[column_name], color='red', label='Anomaly', alpha=0.6, edgecolors='w')\n",
    "\n",
    "    # Additional plot formatting\n",
    "    plt.title(f'Scatter Plot for {column_name}')\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel(column_name)  # Typically you might plot against another feature or itself with jitter\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{column_name}_anomalies_scatter.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da333bb-50a0-41be-bebc-255d77735fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a0cf67-6168-4c25-a299-2139d2c1ef25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d431e0e2-2684-48c5-8b2d-8d9e0ef9a5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0daf39-7f61-49f2-9abf-3670b0258f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0ac6e4-2afc-43af-a7ef-268a4e2d68ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afed8b59-4771-4a82-af37-3e4fe1f47e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb392e0-cc4a-4522-aa60-ead78aefa9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b503f96f-9fda-4c63-afbb-19188d2df8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5deb95-ea74-44d0-abd2-8d064a65e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Handling missing values by replacing them with the median of each column\n",
    "for col in numerical_columns:\n",
    "    if data[col].isna().any():\n",
    "        data[col].fillna(data[col].median(), inplace=True)\n",
    "\n",
    "# Applying Z-score for anomaly detection in numeric columns\n",
    "for col in numerical_columns:\n",
    "    data[col + '_z_score'] = np.abs(stats.zscore(data[col]))\n",
    "    data[col + '_outlier'] = 0\n",
    "    data.loc[data[col + '_z_score'] > 3, col + '_outlier'] = 1  # Any Z-score > 3 is considered an outlier\n",
    "\n",
    "# Combine all outlier flags to a single anomaly label\n",
    "data['anomaly_label'] = data[[col + '_outlier' for col in numerical_columns]].max(axis=1)\n",
    "\n",
    "# Train Isolation Forest model for anomaly detection\n",
    "if_model = IsolationForest(random_state=42)\n",
    "if_model.fit(data[numerical_columns])\n",
    "\n",
    "# Predict anomaly labels using Isolation Forest\n",
    "data['if_anomaly'] = if_model.predict(data[numerical_columns])\n",
    "data['if_anomaly'] = np.where(data['if_anomaly'] == -1, 1, 0)  # Convert -1 to 1 for anomaly, 1 to 0 for normal\n",
    "\n",
    "# Find the top 5 columns with the most anomalies predicted by Isolation Forest\n",
    "top_5_columns = np.argsort(np.sum(np.abs(if_model.decision_function(data[numerical_columns]))))\n",
    "\n",
    "# Plot the distributions of the top 5 most anomalous columns\n",
    "for col_index in top_5_columns[:5]:\n",
    "    column_name = numerical_columns[col_index]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(data[column_name], bins=50, label=f'Distribution of {column_name}')\n",
    "    plt.title(f'Distribution of {column_name}')\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{column_name}_distribution.png')\n",
    "    plt.show()\n",
    "\n",
    "# Save the updated dataset\n",
    "data.to_csv('updated_with_anomalies_iforest_stammdaten.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0169d3d3-e7a4-4f75-867e-bf2397715dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots for detected anomalies\n",
    "plt.figure(figsize=(15, 10))\n",
    "for col_index in range(len(numerical_columns)):\n",
    "    column_name = numerical_columns[col_index]\n",
    "    # Plot anomalies detected by XGBoost\n",
    "    plt.scatter(data[column_name][data['if_anomaly'] == 1], data[column_name][data['if_anomaly'] == 1], c='red', label='Anomaly')\n",
    "    # Plot normal data points\n",
    "    plt.scatter(data[column_name][data['if_anomaly'] == 0], data[column_name][data['if_anomaly'] == 0], c='blue', label='Normal')\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f'Scatter Plot of {column_name}')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{column_name}_scatter.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8f2494-4c48-472b-89b1-b004b5342a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ratios of detected anomalies on each column\n",
    "anomaly_ratios = (data.filter(regex='_outlier$').sum() / len(data)).sort_values(ascending=False)\n",
    "\n",
    "# Identify the most bizarre values of each column\n",
    "most_bizarre_values = {}\n",
    "for col in numerical_columns:\n",
    "    # Calculate Z-score for each column\n",
    "    z_scores = np.abs(stats.zscore(data[col]))\n",
    "    # Identify the most bizarre value\n",
    "    most_bizarre_value = data.loc[np.argmax(z_scores), col]\n",
    "    most_bizarre_values[col] = most_bizarre_value\n",
    "\n",
    "# Write the report to a text file\n",
    "with open(\"anomaly_report_iforest.txt\", \"w\") as f:\n",
    "    f.write(\"Anomaly Ratios:\\n\")\n",
    "    for col, ratio in anomaly_ratios.items():\n",
    "        f.write(f\"{col}: {ratio:.2f}\\n\")\n",
    "    f.write(\"\\nMost Bizarre Values:\\n\")\n",
    "    for col, value in most_bizarre_values.items():\n",
    "        f.write(f\"{col}: {value}\\n\")\n",
    "        f.write(f\"Description: This value might occur due to ...\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a571b85-0acd-4570-a179-53da77c4fbc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b98b3a-267f-40a3-8be0-e393cea63c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4009e258-994e-401f-a47a-4140639e4184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with NaN values\n",
    "data = df.dropna(axis=1)\n",
    "    \n",
    "categorical_columns = [\"Materialnummer\", \"Lieferant OB\", \"Vertragsposition OB\", \"Beschaffungsart\", \"Disponent\", \"Einkäufer\", \"Dispolosgröße\", \"Werk OB\", \"Warengruppe\", \"Basiseinheit\"]\n",
    "numerical_columns = [\"Planlieferzeit Vertrag\", \"Vertrag Fix1\", \"Vertrag_Fix2\", \"Gesamtbestand\", \"Gesamtwert\", \"Preiseinheit\", \"WE-Bearbeitungszeit\", \"Planlieferzeit Mat-Stamm\"]\n",
    "    \n",
    "data[categorical_columns] = data[categorical_columns].astype('category')\n",
    "data[numerical_columns] = data[numerical_columns].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb31e83f-b3e6-4183-86be-975b47c5df3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4575813-1c7a-4b52-9c5f-98052a9da1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Handling missing values by replacing them with the median of each column\n",
    "for col in numerical_columns:\n",
    "    if data[col].isna().any():\n",
    "        data[col].fillna(data[col].median(), inplace=True)\n",
    "\n",
    "# Applying Z-score for anomaly detection in numeric columns\n",
    "for col in numerical_columns:\n",
    "    data[col + '_z_score'] = np.abs(stats.zscore(data[col]))\n",
    "    data[col + '_outlier'] = 0\n",
    "    data.loc[data[col + '_z_score'] > 3, col + '_outlier'] = 1  # Any Z-score > 3 is considered an outlier\n",
    "\n",
    "# Combine all outlier flags to a single anomaly label\n",
    "data['anomaly_label'] = data[[col + '_outlier' for col in numerical_columns]].max(axis=1)\n",
    "\n",
    "# Train XGBoost model for anomaly detection\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "xgb_model.fit(data[numerical_columns], data['anomaly_label'])\n",
    "\n",
    "# Predict anomaly labels using XGBoost\n",
    "data['xgb_anomaly'] = xgb_model.predict(data[numerical_columns])\n",
    "\n",
    "# Find the top 5 columns with the most anomalies predicted by XGBoost\n",
    "top_5_columns = xgb_model.feature_importances_.argsort()[-5:][::-1]\n",
    "\n",
    "# Plot the distributions of the top 5 most anomalous columns\n",
    "for col_index in top_5_columns:\n",
    "    column_name = numerical_columns[col_index]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(data[column_name], bins=50, label=f'Distribution of {column_name}')\n",
    "    plt.title(f'Distribution of {column_name}')\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{column_name}_distribution.png')\n",
    "    plt.show()\n",
    "\n",
    "# Save the updated dataset\n",
    "data.to_csv('updated_with_anomalies_xgboost_stammdaten.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f6a4ca-cd3b-499b-b089-bb3c82419cef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb24c3a-d432-419b-acf0-a8f63ec22eff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccdab6e-09b7-4d6e-a99c-a7c22a2515b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots for detected anomalies\n",
    "plt.figure(figsize=(15, 10))\n",
    "for col_index in range(len(numerical_columns)):\n",
    "    column_name = numerical_columns[col_index]\n",
    "    # Plot anomalies detected by XGBoost\n",
    "    plt.scatter(data[column_name][data['xgb_anomaly'] == 1], data[column_name][data['xgb_anomaly'] == 1], c='red', label='Anomaly')\n",
    "    # Plot normal data points\n",
    "    plt.scatter(data[column_name][data['xgb_anomaly'] == 0], data[column_name][data['xgb_anomaly'] == 0], c='blue', label='Normal')\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f'Scatter Plot of {column_name}')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{column_name}_scatter.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df44583b-9c92-48ed-bd97-a45f0ec9af88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ratios of detected anomalies on each column\n",
    "anomaly_ratios = (data.filter(regex='_outlier$').sum() / len(data)).sort_values(ascending=False)\n",
    "\n",
    "# Identify the most bizarre values of each column\n",
    "most_bizarre_values = {}\n",
    "for col in numerical_columns:\n",
    "    # Calculate Z-score for each column\n",
    "    z_scores = np.abs(stats.zscore(data[col]))\n",
    "    # Identify the most bizarre value\n",
    "    most_bizarre_value = data.loc[np.argmax(z_scores), col]\n",
    "    most_bizarre_values[col] = most_bizarre_value\n",
    "\n",
    "# Write the report to a text file\n",
    "with open(\"anomaly_report.txt\", \"w\") as f:\n",
    "    f.write(\"Anomaly Ratios:\\n\")\n",
    "    for col, ratio in anomaly_ratios.items():\n",
    "        f.write(f\"{col}: {ratio:.2f}\\n\")\n",
    "    f.write(\"\\nMost Bizarre Values:\\n\")\n",
    "    for col, value in most_bizarre_values.items():\n",
    "        f.write(f\"{col}: {value}\\n\")\n",
    "        f.write(f\"Description: This value might occur due to ...\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778afd8b-88a1-4692-bcc7-8fad9012d34d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3aabb2-66f3-4280-9e74-077ff886284a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e977e3cc-42ab-4ece-baf2-211d2ce3e15b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
