{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efb297e3-f0b3-4b42-9d4c-02c4bc606807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/dgvai/miniconda3/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dgvai/miniconda3/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dgvai/miniconda3/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dgvai/miniconda3/lib/python3.12/site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dgvai/miniconda3/lib/python3.12/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: bs4 in /home/dgvai/miniconda3/lib/python3.12/site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/dgvai/miniconda3/lib/python3.12/site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/dgvai/miniconda3/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Requirement already satisfied: emoji in /home/dgvai/miniconda3/lib/python3.12/site-packages (2.14.0)\n",
      "Requirement already satisfied: python-docx in /home/dgvai/miniconda3/lib/python3.12/site-packages (1.1.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /home/dgvai/miniconda3/lib/python3.12/site-packages (from python-docx) (5.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /home/dgvai/miniconda3/lib/python3.12/site-packages (from python-docx) (4.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install bs4\n",
    "!pip install emoji\n",
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c1b4620-bab8-4bcd-b950-0fea09495867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "import json\n",
    "import emoji\n",
    "from collections import deque\n",
    "from urllib.parse import urljoin\n",
    "from docx import Document\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36ef21ba-af0b-4a22-8781-a643498938b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAIN = \"https://webhopper-client.vercel.app\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31dbcf14-ccc2-4695-a2b3-acba50da8853",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebHopperCrawler:\n",
    "    def __init__(self, url):\n",
    "        self.domain = url\n",
    "        self.paragraphs = {}\n",
    "        self.visited = set()\n",
    "\n",
    "    def paragraph_extractor(self, soup):\n",
    "        \"\"\"extract title and paragraphs from the page\"\"\"\n",
    "        titleSoup = soup.find(id='title')\n",
    "        textSoup = soup.find(id='text')\n",
    "        \n",
    "        title = titleSoup.get_text() if titleSoup else None\n",
    "        text = textSoup.get_text() if textSoup else None\n",
    "    \n",
    "        if title in self.paragraphs:\n",
    "            self.paragraphs[title].append(text)\n",
    "        else:\n",
    "            self.paragraphs[title] = [text]\n",
    "        return title\n",
    "\n",
    "    def DFS_crawler(self, url):\n",
    "        \"\"\"Depth First Search Algorithm\"\"\"\n",
    "        if url in self.visited:\n",
    "            return\n",
    "        self.visited.add(url)\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        title = self.paragraph_extractor(soup)\n",
    "        print(emoji.emojize(\"✅ crawling completed: \"), title, url)\n",
    "        # print(title, end=\" → \")\n",
    "        \n",
    "        links = soup.find_all('a', href=True)\n",
    "        for link in links:\n",
    "            next_url = f\"{DOMAIN}{link.get('href')}\"\n",
    "            self.DFS_crawler(next_url)\n",
    "\n",
    "    def BFS_crawler(self, url):\n",
    "        \"\"\"Breadth First Search Algorithm\"\"\"\n",
    "        queue = deque([url])\n",
    "        \n",
    "        while queue:\n",
    "            url = queue.popleft()\n",
    "            if url in self.visited:\n",
    "                continue\n",
    "            self.visited.add(url)\n",
    "            \n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            title = self.paragraph_extractor(soup)\n",
    "            print(emoji.emojize(\"✅ crawling completed: \"), title, url)\n",
    "            # print(title, end=\" → \")\n",
    "    \n",
    "            links = soup.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                next_url = f\"{DOMAIN}{link.get('href')}\"\n",
    "                if next_url not in self.visited:\n",
    "                    queue.append(next_url)\n",
    "\n",
    "    def document_generator(self, algoname):\n",
    "        \"\"\"Human readable document generator\"\"\"\n",
    "        doc = Document()\n",
    "        for i, title in enumerate(self.paragraphs):\n",
    "            para = self.paragraphs[title][0]\n",
    "            doc.add_heading(title)\n",
    "            doc.add_paragraph(para)\n",
    "        doc.save(f\"webhopper_{algoname}.docx\")\n",
    "        # print(json.dumps(self.paragraphs, indent=4))\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"reset the state of the class\"\"\"\n",
    "        self.paragraphs = {}\n",
    "        self.visited = set()\n",
    "\n",
    "    def execute(self):\n",
    "        \"\"\"execute the program step by step\"\"\"\n",
    "        print(\"Starting webhopper...\")\n",
    "       \n",
    "        print(\"Depth First Search (DFS)\")\n",
    "        start_time = time.time()\n",
    "        self.DFS_crawler(self.domain)\n",
    "        print(\"DFS Completed, Saving document...\")\n",
    "        self.document_generator(\"DFS\")\n",
    "        end_time = time.time()\n",
    "        print(f\"Time elasped: {end_time - start_time}s\")\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "        print(\"Breadth First Search (BFS)\")\n",
    "        start_time = time.time()\n",
    "        self.BFS_crawler(self.domain)\n",
    "        print(\"BFS Completed, Saving document...\")\n",
    "        self.document_generator(\"BFS\")\n",
    "        end_time = time.time()\n",
    "        print(f\"Time elasped: {end_time - start_time}s\")\n",
    "        \n",
    "        print(\"Webhopper Completed!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "717274d1-ddac-409e-89ff-b7a118b8c41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting webhopper...\n",
      "Depth First Search (DFS)\n",
      "✅ crawling completed:  WebHopper | Articles https://webhopper-client.vercel.app\n",
      "✅ crawling completed:  Data Structure https://webhopper-client.vercel.app/articles/data-structures\n",
      "✅ crawling completed:  Computer Science https://webhopper-client.vercel.app/articles/computer-science\n",
      "✅ crawling completed:  Computation https://webhopper-client.vercel.app/articles/computation\n",
      "✅ crawling completed:  Algorithm https://webhopper-client.vercel.app/articles/algorithm\n",
      "✅ crawling completed:  Computer https://webhopper-client.vercel.app/articles/computer\n",
      "✅ crawling completed:  Information https://webhopper-client.vercel.app/articles/information\n",
      "✅ crawling completed:  Decision Making https://webhopper-client.vercel.app/articles/decision-making\n",
      "✅ crawling completed:  Automation https://webhopper-client.vercel.app/articles/automation\n",
      "✅ crawling completed:  Human Intervention https://webhopper-client.vercel.app/articles/human-intervention\n",
      "✅ crawling completed:  Function https://webhopper-client.vercel.app/articles/function\n",
      "✅ crawling completed:  Algebraic Structure https://webhopper-client.vercel.app/articles/algebraic-structure\n",
      "✅ crawling completed:  Data Structure Types https://webhopper-client.vercel.app/articles/data-structures/types\n",
      "✅ crawling completed:  Array https://webhopper-client.vercel.app/articles/array\n",
      "✅ crawling completed:  Linked List https://webhopper-client.vercel.app/articles/linked-list\n",
      "✅ crawling completed:  Hash Table https://webhopper-client.vercel.app/articles/record\n",
      "✅ crawling completed:  Hash Tables https://webhopper-client.vercel.app/articles/hash-table\n",
      "✅ crawling completed:  Graph https://webhopper-client.vercel.app/articles/graphs\n",
      "✅ crawling completed:  Stack-Queue https://webhopper-client.vercel.app/articles/stack-queue\n",
      "✅ crawling completed:  Tree https://webhopper-client.vercel.app/articles/trees\n",
      "DFS Completed, Saving document...\n",
      "Time elasped: 3.856663703918457s\n",
      "Breadth First Search (BFS)\n",
      "✅ crawling completed:  WebHopper | Articles https://webhopper-client.vercel.app\n",
      "✅ crawling completed:  Data Structure https://webhopper-client.vercel.app/articles/data-structures\n",
      "✅ crawling completed:  Computer Science https://webhopper-client.vercel.app/articles/computer-science\n",
      "✅ crawling completed:  Function https://webhopper-client.vercel.app/articles/function\n",
      "✅ crawling completed:  Algebraic Structure https://webhopper-client.vercel.app/articles/algebraic-structure\n",
      "✅ crawling completed:  Data Structure Types https://webhopper-client.vercel.app/articles/data-structures/types\n",
      "✅ crawling completed:  Computation https://webhopper-client.vercel.app/articles/computation\n",
      "✅ crawling completed:  Information https://webhopper-client.vercel.app/articles/information\n",
      "✅ crawling completed:  Automation https://webhopper-client.vercel.app/articles/automation\n",
      "✅ crawling completed:  Array https://webhopper-client.vercel.app/articles/array\n",
      "✅ crawling completed:  Linked List https://webhopper-client.vercel.app/articles/linked-list\n",
      "✅ crawling completed:  Hash Table https://webhopper-client.vercel.app/articles/record\n",
      "✅ crawling completed:  Hash Tables https://webhopper-client.vercel.app/articles/hash-table\n",
      "✅ crawling completed:  Graph https://webhopper-client.vercel.app/articles/graphs\n",
      "✅ crawling completed:  Stack-Queue https://webhopper-client.vercel.app/articles/stack-queue\n",
      "✅ crawling completed:  Tree https://webhopper-client.vercel.app/articles/trees\n",
      "✅ crawling completed:  Algorithm https://webhopper-client.vercel.app/articles/algorithm\n",
      "✅ crawling completed:  Computer https://webhopper-client.vercel.app/articles/computer\n",
      "✅ crawling completed:  Decision Making https://webhopper-client.vercel.app/articles/decision-making\n",
      "✅ crawling completed:  Human Intervention https://webhopper-client.vercel.app/articles/human-intervention\n",
      "BFS Completed, Saving document...\n",
      "Time elasped: 3.925551176071167s\n",
      "Webhopper Completed!\n"
     ]
    }
   ],
   "source": [
    "webhopper = WebHopperCrawler(url=DOMAIN)\n",
    "webhopper.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "552781b9-5a2f-43ca-a28c-7d4a875a4ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebHopperSearch:\n",
    "    def __init__(self, start_url, goal_keyword, max_depth=5):\n",
    "        self.start_url = start_url\n",
    "        self.goal_keyword = goal_keyword\n",
    "        self.max_depth = max_depth\n",
    "        self.visited = set()\n",
    "\n",
    "    def heuristic(self, url):\n",
    "        \"\"\"Heuristic function: prioritize URLs with the goal keyword.\"\"\"\n",
    "        return -url.count(self.goal_keyword)\n",
    "\n",
    "    def fetch_links(self, url):\n",
    "        \"\"\"Fetch all hyperlinks from a given URL.\"\"\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            links = set(\n",
    "                urljoin(url, a['href'])\n",
    "                for a in soup.find_all('a', href=True)\n",
    "                if urljoin(url, a['href']).startswith(self.start_url)\n",
    "            )\n",
    "            return links\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch links from {url}: {e}\")\n",
    "            return set()\n",
    "\n",
    "    def find_match(self, url):\n",
    "        \"\"\"find the goal from the page\"\"\"\n",
    "        response = requests.get(url, timeout=5)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        titleSoup = soup.find(id='title')\n",
    "        textSoup = soup.find(id='text')\n",
    "        \n",
    "        title = titleSoup.get_text().lower() if titleSoup else None\n",
    "        text = textSoup.get_text().lower() if textSoup else None\n",
    "\n",
    "        if not title or not text: \n",
    "            return False\n",
    "\n",
    "        if(title.find(self.goal_keyword.lower()) >= 0 or text.find(self.goal_keyword.lower()) >= 0):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def crawl(self):\n",
    "        \"\"\"A* algorithm for crawling and searching.\"\"\"\n",
    "        open_set = [(self.start_url, 0)]  # (URL, cost so far)\n",
    "        g_scores = {self.start_url: 0}\n",
    "\n",
    "        while open_set:\n",
    "            # Sort open_set by total cost (g + h)\n",
    "            open_set.sort(key=lambda x: g_scores[x[0]] + self.heuristic(x[0]))\n",
    "            current_url, current_cost = open_set.pop(0)\n",
    "\n",
    "            # Mark as visited\n",
    "            if current_url in self.visited:\n",
    "                continue\n",
    "            self.visited.add(current_url)\n",
    "            print(f\"Visiting: {current_url}\")\n",
    "\n",
    "            # Check if goal is found\n",
    "            if self.find_match(current_url):\n",
    "                print(f\"Goal found: {current_url}\")\n",
    "                return\n",
    "\n",
    "            # Fetch and process neighbors\n",
    "            if current_cost < self.max_depth:\n",
    "                for neighbor in self.fetch_links(current_url):\n",
    "                    tentative_g_score = current_cost + 1\n",
    "                    if neighbor not in g_scores or tentative_g_score < g_scores[neighbor]:\n",
    "                        g_scores[neighbor] = tentative_g_score\n",
    "                        open_set.append((neighbor, tentative_g_score))\n",
    "\n",
    "        print(\"Goal not found within max depth.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39d6ba2c-a9cb-4744-b5cc-26668b49461e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visiting: https://webhopper-client.vercel.app\n",
      "Visiting: https://webhopper-client.vercel.app/articles/data-structures\n",
      "Visiting: https://webhopper-client.vercel.app/articles/data-structures/types\n",
      "Visiting: https://webhopper-client.vercel.app/articles/algebraic-structure\n",
      "Visiting: https://webhopper-client.vercel.app/articles/function\n",
      "Visiting: https://webhopper-client.vercel.app/articles/computer-science\n",
      "Visiting: https://webhopper-client.vercel.app/articles/array\n",
      "Visiting: https://webhopper-client.vercel.app/articles/hash-table\n",
      "Visiting: https://webhopper-client.vercel.app/articles/trees\n",
      "Visiting: https://webhopper-client.vercel.app/articles/graphs\n",
      "Visiting: https://webhopper-client.vercel.app/articles/record\n",
      "Visiting: https://webhopper-client.vercel.app/articles/linked-list\n",
      "Visiting: https://webhopper-client.vercel.app/articles/stack-queue\n",
      "Visiting: https://webhopper-client.vercel.app/articles/information\n",
      "Visiting: https://webhopper-client.vercel.app/articles/automation\n",
      "Goal found: https://webhopper-client.vercel.app/articles/automation\n"
     ]
    }
   ],
   "source": [
    "goal_keyword = \"human intervention\"\n",
    "webhopper_search = WebHopperSearch(DOMAIN, goal_keyword)\n",
    "webhopper_search.crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2947b081-9ef7-4f5c-b20e-6b271da82a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
