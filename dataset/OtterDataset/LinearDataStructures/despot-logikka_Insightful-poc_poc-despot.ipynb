{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"Using CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic approach - up to 10k elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Connect to Elasticsearch\n",
    "client = Elasticsearch(\n",
    "    \"http://192.168.5.188:9200/\", \n",
    "    basic_auth=(\"despot.markovic\", \"KJmhyg7cPxfVG84UTuE\") \n",
    ")\n",
    "\n",
    "# Define the query to match documents where 'site' is an empty string\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"term\": {\n",
    "            \"site\": \"\" \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Execute the search\n",
    "response = client.search(\n",
    "    index=\"insightful-fragments-v8\",\n",
    "    size=10000,  # Adjust the size as needed to pull all documents (consider using scroll for larger datasets)\n",
    "    body=query  # Use 'body' to pass the query\n",
    ")\n",
    "\n",
    "# Extract the documents where 'site' is an empty string\n",
    "documents_with_empty_site = [doc['_source'] for doc in response['hits']['hits']]\n",
    "\n",
    "# Print or process the results\n",
    "for doc in documents_with_empty_site:\n",
    "    print(doc['app'])  # Print the app name or process as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex compositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Connect to Elasticsearch\n",
    "client = Elasticsearch(\n",
    "    \"http://192.168.5.188:9200/\", \n",
    "    basic_auth=(\"despot.markovic\", \"KJmhyg7cPxfVG84UTuE\") \n",
    ")\n",
    "\n",
    "# Define the aggregation query to get unique 'app' values where 'site' is not empty\n",
    "query = {\n",
    "    \"size\": 0,\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\"exists\": {\"field\": \"site\"}},\n",
    "                {\"wildcard\": {\"site\": \"*?\"}} \n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"aggs\": {\n",
    "        \"unique_apps\": {\n",
    "            \"terms\": {\n",
    "                \"field\": \"app.keyword\",\n",
    "                \"size\": 10000\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Execute the search with aggregation\n",
    "response = client.search(index=\"insightful-fragments-v8\", body=query)\n",
    "\n",
    "# Extract the unique apps from the aggregation results\n",
    "unique_apps = [bucket['key'] for bucket in response['aggregations']['unique_apps']['buckets']]\n",
    "\n",
    "# Print or process the unique apps\n",
    "for app in unique_apps:\n",
    "    print(app)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Connect to Elasticsearch\n",
    "client = Elasticsearch(\n",
    "    \"http://192.168.5.188:9200/\", \n",
    "    basic_auth=(\"despot.markovic\", \"KJmhyg7cPxfVG84UTuE\") \n",
    ")\n",
    "\n",
    "# Initialize an empty list to store all unique apps\n",
    "unique_apps = []\n",
    "after_key = None  # For pagination\n",
    "\n",
    "# Loop through composite aggregation to collect all unique 'app' values\n",
    "while True:\n",
    "    # Define the composite aggregation query with pagination\n",
    "    query = {\n",
    "        \"size\": 0,\n",
    "        \"aggs\": {\n",
    "            \"unique_apps\": {\n",
    "                \"composite\": {\n",
    "                    \"sources\": [\n",
    "                        {\"app\": {\"terms\": {\"field\": \"app.keyword\"}}}\n",
    "                    ],\n",
    "                    \"size\": 10000  # Adjust size per page as needed\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Include 'after' key only if we have a valid after_key value\n",
    "    if after_key:\n",
    "        query[\"aggs\"][\"unique_apps\"][\"composite\"][\"after\"] = after_key\n",
    "\n",
    "    # Execute the search with aggregation\n",
    "    response = client.search(index=\"insightful-fragments-v8\", body=query)\n",
    "\n",
    "    # Extract the unique apps from the aggregation results\n",
    "    buckets = response['aggregations']['unique_apps']['buckets']\n",
    "    unique_apps.extend([bucket['key']['app'] for bucket in buckets])\n",
    "\n",
    "    # Check if there is more data to fetch; if not, break the loop\n",
    "    if 'after_key' in response['aggregations']['unique_apps']:\n",
    "        after_key = response['aggregations']['unique_apps']['after_key']\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Print or process the unique apps\n",
    "for app in unique_apps:\n",
    "    print(app)\n",
    "\n",
    "# Optionally, remove duplicates if necessary\n",
    "unique_apps = list(set(unique_apps))  # Converts to a set and back to list for deduplication if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Connect to Elasticsearch\n",
    "client = Elasticsearch(\n",
    "    \"http://192.168.5.188:9200/\", \n",
    "    basic_auth=(\"despot.markovic\", \"KJmhyg7cPxfVG84UTuE\") \n",
    ")\n",
    "\n",
    "# Initialize an empty list to store all unique apps\n",
    "unique_apps = []\n",
    "after_key = None  # For pagination\n",
    "\n",
    "# Loop through composite aggregation to collect all unique 'app' values\n",
    "while True:\n",
    "    # Define the composite aggregation query with pagination\n",
    "    query = {\n",
    "        \"size\": 0,\n",
    "        \"aggs\": {\n",
    "            \"unique_sites\": {\n",
    "                \"composite\": {\n",
    "                    \"sources\": [\n",
    "                        {\"site\": {\"terms\": {\"field\": \"site.keyword\"}}}\n",
    "                    ],\n",
    "                    \"size\": 10000  # Adjust size per page as needed\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Include 'after' key only if we have a valid after_key value\n",
    "    if after_key:\n",
    "        query[\"aggs\"][\"unique_sites\"][\"composite\"][\"after\"] = after_key\n",
    "\n",
    "    # Execute the search with aggregation\n",
    "    response = client.search(index=\"insightful-fragments-v8\", body=query)\n",
    "\n",
    "    # Extract the unique apps from the aggregation results\n",
    "    buckets = response['aggregations']['unique_sites']['buckets']\n",
    "    unique_apps.extend([bucket['key']['site'] for bucket in buckets])\n",
    "\n",
    "    # Check if there is more data to fetch; if not, break the loop\n",
    "    if 'after_key' in response['aggregations']['unique_sites']:\n",
    "        after_key = response['aggregations']['unique_sites']['after_key']\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Optionally, remove duplicates if necessary\n",
    "unique_apps = list(set(unique_apps))  # Converts to a set and back to list for deduplication if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the list to a DataFrame with a column name \"browsers\"\n",
    "df = pd.DataFrame(unique_apps, columns=[\"apps\"])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"apps.csv\", index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrolling approach - All data pulling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import time\n",
    "\n",
    "# Connect to the Elasticsearch cluster\n",
    "client = Elasticsearch(\n",
    "    \"http://192.168.5.188:9200/\", \n",
    "    basic_auth=(\"despot.markovic\", \"KJmhyg7cPxfVG84UTuE\")\n",
    ")\n",
    "\n",
    "# To store the pulled data\n",
    "all_documents = []\n",
    "\n",
    "# Start the initial search request with scroll\n",
    "scroll_size = 10000\n",
    "scroll_time = '2m'\n",
    "iteration = 0\n",
    "total_docs = 0\n",
    "batch_size = 20\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Initial search query with scroll\n",
    "response = client.search(\n",
    "    index=\"insightful-fragments-v8\",\n",
    "    size=scroll_size,\n",
    "    scroll=scroll_time,\n",
    "    query={\"match_all\": {}}\n",
    ")\n",
    "\n",
    "# Extract the first scroll ID and documents\n",
    "scroll_id = response['_scroll_id']\n",
    "hits = response['hits']['hits']\n",
    "total_docs += len(hits)\n",
    "\n",
    "# Add the first batch of documents to the list\n",
    "all_documents.extend([doc['_source'] for doc in hits])\n",
    "\n",
    "# Continue scrolling to pull all data\n",
    "while len(hits) > 0:\n",
    "    iteration += 1\n",
    "    # Fetch the next batch of results using the scroll ID\n",
    "    response = client.scroll(scroll_id=scroll_id, scroll=scroll_time)\n",
    "    \n",
    "    # Update scroll ID and retrieve the next batch of hits\n",
    "    scroll_id = response['_scroll_id']\n",
    "    hits = response['hits']['hits']\n",
    "    total_docs += len(hits)\n",
    "    \n",
    "    # Add the new batch of documents to the list\n",
    "    all_documents.extend([doc['_source'] for doc in hits])\n",
    "    \n",
    "    # Print progress every 20 iterations\n",
    "    if iteration % batch_size == 0:\n",
    "        time_spent = time.time() - start_time\n",
    "        print(f\"Iteration: {iteration}, Total documents pulled: {total_docs}\")\n",
    "        print(f\"Time spent for last 20 iterations: {time_spent:.2f} seconds\")\n",
    "        # Reset the timer after each 20 iterations\n",
    "        start_time = time.time()\n",
    "\n",
    "# Once all data is retrieved, clear the scroll context\n",
    "client.clear_scroll(scroll_id=scroll_id)\n",
    "\n",
    "print(f\"All documents pulled. Total: {total_docs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrolling approach - pulling by sorted employeeId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import time\n",
    "\n",
    "# Connect to the Elasticsearch cluster\n",
    "client = Elasticsearch(\n",
    "    \"http://192.168.5.188:9200/\", \n",
    "    basic_auth=(\"despot.markovic\", \"KJmhyg7cPxfVG84UTuE\")\n",
    ")\n",
    "\n",
    "# To store the pulled data\n",
    "all_documents = []\n",
    "\n",
    "# Start the initial search request with scroll\n",
    "scroll_size = 10000\n",
    "scroll_time = '2m'\n",
    "iteration = 0\n",
    "total_docs = 0\n",
    "batch_size = 20\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Set the threshold for the number of unique employeeIds to pull data for\n",
    "max_employee_ids_to_pull = 150 \n",
    "\n",
    "# Track the current employeeId to ensure we finish pulling all documents for that employeeId\n",
    "current_employee_id = None\n",
    "unique_employee_ids = set()\n",
    "\n",
    "# Initial search query with scroll, sorted by employeeId\n",
    "response = client.search(\n",
    "    index=\"insightful-fragments-v8\",\n",
    "    size=scroll_size,\n",
    "    scroll=scroll_time,\n",
    "    query={\"match_all\": {}},\n",
    "    sort=[{\"employeeId\": {\"order\": \"asc\"}}]\n",
    ")\n",
    "\n",
    "# Extract the first scroll ID and documents\n",
    "scroll_id = response['_scroll_id']\n",
    "hits = response['hits']['hits']\n",
    "\n",
    "# Continue scrolling to pull all data until we've pulled for the desired number of unique employeeIds\n",
    "while len(hits) > 0:\n",
    "    iteration += 1\n",
    "    for hit in hits:\n",
    "        employee_id = hit['_source'].get('employeeId')\n",
    "\n",
    "        # If we haven't reached the max_employee_ids_to_pull, add documents\n",
    "        if len(unique_employee_ids) < max_employee_ids_to_pull or (current_employee_id == employee_id):\n",
    "            # If this is a new employeeId, add it to the set\n",
    "            if employee_id not in unique_employee_ids:\n",
    "                unique_employee_ids.add(employee_id)\n",
    "                current_employee_id = employee_id\n",
    "            \n",
    "            # Add document to the list\n",
    "            all_documents.append(hit['_source'])\n",
    "            total_docs += 1\n",
    "        else:\n",
    "            break \n",
    "\n",
    "    # Fetch the next batch of results using the scroll ID\n",
    "    response = client.scroll(scroll_id=scroll_id, scroll=scroll_time)\n",
    "    \n",
    "    # Update scroll ID and retrieve the next batch of hits\n",
    "    scroll_id = response['_scroll_id']\n",
    "    hits = response['hits']['hits']\n",
    "\n",
    "    # Print progress every 20 iterations\n",
    "    if iteration % batch_size == 0:\n",
    "        time_spent = time.time() - start_time\n",
    "        print(f\"Iteration: {iteration}, Total documents pulled: {total_docs}\")\n",
    "        print(f\"Time spent for last 20 iterations: {time_spent:.2f} seconds\")\n",
    "        print(f\"Unique employeeIds processed: {len(unique_employee_ids)}\")\n",
    "        # Reset the timer after each 20 iterations\n",
    "        start_time = time.time()\n",
    "\n",
    "    # If we've collected data for the desired number of unique employeeIds, stop the loop\n",
    "    if len(unique_employee_ids) >= max_employee_ids_to_pull:\n",
    "        break\n",
    "\n",
    "# Once all data is retrieved or the target number of employeeIds is met, clear the scroll context\n",
    "client.clear_scroll(scroll_id=scroll_id)\n",
    "\n",
    "print(f\"All documents pulled. Total: {total_docs}\")\n",
    "print(f\"Total unique employeeIds pulled: {len(unique_employee_ids)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling one whole specific team from db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import time\n",
    "\n",
    "# Connect to the Elasticsearch cluster\n",
    "client = Elasticsearch(\n",
    "    \"http://192.168.5.188:9200/\", \n",
    "    basic_auth=(\"despot.markovic\", \"KJmhyg7cPxfVG84UTuE\")\n",
    ")\n",
    "\n",
    "# To store the pulled data\n",
    "all_documents = []\n",
    "\n",
    "# Start the initial search request with scroll\n",
    "scroll_size = 10000\n",
    "scroll_time = '2m'\n",
    "iteration = 0\n",
    "total_docs = 0\n",
    "batch_size = 20\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Specify the teamId to filter data\n",
    "target_team_id = \"wxqdcesicguzlsh\"\n",
    "\n",
    "# Initial search query with scroll, filtering by teamId\n",
    "response = client.search(\n",
    "    index=\"insightful-fragments-v8\",\n",
    "    size=scroll_size,\n",
    "    scroll=scroll_time,\n",
    "    query={\n",
    "        \"term\": {\n",
    "            \"teamId.keyword\": target_team_id\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract the first scroll ID and documents\n",
    "scroll_id = response['_scroll_id']\n",
    "hits = response['hits']['hits']\n",
    "\n",
    "# Continue scrolling to pull all data for the specified teamId\n",
    "while len(hits) > 0:\n",
    "    iteration += 1\n",
    "    for hit in hits:\n",
    "        # Add document to the list\n",
    "        all_documents.append(hit['_source'])\n",
    "        total_docs += 1\n",
    "\n",
    "    # Fetch the next batch of results using the scroll ID\n",
    "    response = client.scroll(scroll_id=scroll_id, scroll=scroll_time)\n",
    "    \n",
    "    # Update scroll ID and retrieve the next batch of hits\n",
    "    scroll_id = response['_scroll_id']\n",
    "    hits = response['hits']['hits']\n",
    "\n",
    "    # Print progress every 20 iterations\n",
    "    if iteration % batch_size == 0:\n",
    "        time_spent = time.time() - start_time\n",
    "        print(f\"Iteration: {iteration}, Total documents pulled: {total_docs}\")\n",
    "        print(f\"Time spent for last {batch_size} iterations: {time_spent:.2f} seconds\")\n",
    "        # Reset the timer after each batch\n",
    "        start_time = time.time()\n",
    "\n",
    "# Once all data is retrieved, clear the scroll context\n",
    "client.clear_scroll(scroll_id=scroll_id)\n",
    "\n",
    "print(f\"All documents pulled. Total: {total_docs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "client = Elasticsearch(\n",
    "    \"http://192.168.5.188:9200/\", \n",
    "    basic_auth=(\"despot.markovic\", \"KJmhyg7cPxfVG84UTuE\")\n",
    ")\n",
    "\n",
    "index_name = \"insightful-fragments-v8\"\n",
    "mapping = client.indices.get_mapping(index=index_name)\n",
    "print(mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.search(\n",
    "    index=\"insightful-fragments-v8\",\n",
    "    query={\n",
    "        \"match_all\": {}\n",
    "    },\n",
    "    size=5\n",
    ")\n",
    "\n",
    "for hit in response['hits']['hits']:\n",
    "    print(hit['_source'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import time\n",
    "\n",
    "# Connect to the Elasticsearch cluster\n",
    "client = Elasticsearch(\n",
    "    \"http://192.168.5.188:9200/\", \n",
    "    basic_auth=(\"despot.markovic\", \"KJmhyg7cPxfVG84UTuE\")\n",
    ")\n",
    "\n",
    "# Store pulled documents\n",
    "all_documents = []\n",
    "\n",
    "# Scroll parameters\n",
    "scroll_size = 10000\n",
    "scroll_time = '2m'\n",
    "\n",
    "# Specify the teamId to filter data\n",
    "target_team_id = \"wxqdcesicguzlsh\"\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Initial search query with scroll\n",
    "response = client.search(\n",
    "    index=\"insightful-fragments-v8\",\n",
    "    size=scroll_size,\n",
    "    scroll=scroll_time,\n",
    "    query={\n",
    "        \"term\": {\n",
    "            \"teamId\": target_team_id\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Extract the first scroll ID and documents\n",
    "scroll_id = response['_scroll_id']\n",
    "hits = response['hits']['hits']\n",
    "\n",
    "# Continue scrolling until no more hits\n",
    "total_docs = 0\n",
    "iteration = 0\n",
    "batch_size = 20\n",
    "\n",
    "while len(hits) > 0:\n",
    "    iteration += 1\n",
    "    for hit in hits:\n",
    "        # Append each document to the list\n",
    "        all_documents.append(hit['_source'])\n",
    "        total_docs += 1\n",
    "\n",
    "    # Fetch the next batch of results using the scroll ID\n",
    "    response = client.scroll(scroll_id=scroll_id, scroll=scroll_time)\n",
    "    scroll_id = response['_scroll_id']\n",
    "    hits = response['hits']['hits']\n",
    "\n",
    "    # Log progress every 20 iterations\n",
    "    if iteration % batch_size == 0:\n",
    "        time_spent = time.time() - start_time\n",
    "        print(f\"Iteration: {iteration}, Total documents pulled: {total_docs}\")\n",
    "        print(f\"Time spent for last {batch_size} iterations: {time_spent:.2f} seconds\")\n",
    "        # Reset the timer\n",
    "        start_time = time.time()\n",
    "\n",
    "# Clear scroll context\n",
    "client.clear_scroll(scroll_id=scroll_id)\n",
    "\n",
    "# Final output\n",
    "print(f\"All documents pulled. Total: {total_docs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert list of dicts to pd df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the list of dicts into a DataFrame\n",
    "df = pd.DataFrame(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of unique values (employeeIds)\n",
    "print(f\"Number of unique employees: {len(df.employeeId.unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to save to csv file\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"./wxqdcesicguzlsh_test_dataset.csv\")\n",
    "\n",
    "with open(path, 'w') as fp:\n",
    "    df.to_csv(path, sep=';', encoding='utf-8', errors='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Despot Markovic\\AppData\\Local\\Temp\\ipykernel_26320\\3370170282.py:8: DtypeWarning: Columns (5,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(fp, sep=';')\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Testing load csv file:\n",
    "path = Path(\"data/150_users_dataset.csv\")\n",
    "\n",
    "with open(path, 'r', encoding='utf-8') as fp:\n",
    "    df = pd.read_csv(fp, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading mappings for sites/apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          aa2c26a1-09dc-4f59-9e56-62e808e53a57\n",
       "1          8f4a1817-0ad9-484e-a079-b87f99e3d6e5\n",
       "2          e302822a-d3c5-44a3-ba1e-a6fc1035ebef\n",
       "3          a8a89171-49aa-4ca9-b7cc-20637b610673\n",
       "4          3f921b66-3efa-4818-ba49-aa690ef00d93\n",
       "                           ...                 \n",
       "9439823    07832fa2-29c4-4544-a37c-49858a253f71\n",
       "9439824    28652618-c97a-4180-8185-fbf5d41fd1a9\n",
       "9439825    628240e0-6533-429e-a5eb-091e27e185ff\n",
       "9439826    d9ec5deb-f44d-44e0-b12d-443f4be48a56\n",
       "9439827    71934f5e-64cb-43e9-a967-e8502bf1663d\n",
       "Name: id, Length: 9439828, dtype: object"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Testing load csv file:\n",
    "path_apps = Path(\"mappings/app_mappings_2st_round.csv\")\n",
    "path_sites = Path(\"mappings/site_mappings_3rd_round_.csv\")\n",
    "path_browsers = Path(\"mappings/browsers.csv\")\n",
    "exclude_mappings = Path(\"mappings/exclude_mappings.json\")\n",
    "\n",
    "with open(path_apps, 'r', encoding='utf-8') as fp:\n",
    "    mappings_apps = pd.read_csv(fp)\n",
    "\n",
    "with open(path_sites, 'r', encoding='utf-8') as fp:\n",
    "    mappings_sites = pd.read_csv(fp)\n",
    "\n",
    "with open(path_browsers, 'r', encoding='utf-8') as fp:\n",
    "    browsers = pd.read_csv(fp, sep=';')\n",
    "\n",
    "with open(exclude_mappings, 'r', encoding='utf-8') as fp:\n",
    "    exclude_mappings = json.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings_apps['app_mapping_v2'] = mappings_apps['app_mapping_v2'] + \"-Local\"\n",
    "\n",
    "# Drop rows where 'site' is NaN\n",
    "mappings_sites = mappings_sites.dropna(subset=['site'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing / agregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employeeId</th>\n",
       "      <th>app</th>\n",
       "      <th>site</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>mouseClicks</th>\n",
       "      <th>keystrokes</th>\n",
       "      <th>mic</th>\n",
       "      <th>mouseScroll</th>\n",
       "      <th>camera</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50099</th>\n",
       "      <td>w--4rnszliaxy12</td>\n",
       "      <td>Workpuls</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-09-02 09:23:12.586</td>\n",
       "      <td>2024-09-02 09:23:12.839</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50100</th>\n",
       "      <td>w--4rnszliaxy12</td>\n",
       "      <td>Workpuls</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-09-02 09:23:14.200</td>\n",
       "      <td>2024-09-02 09:23:18.272</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50101</th>\n",
       "      <td>w--4rnszliaxy12</td>\n",
       "      <td>Windows Explorer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-09-02 09:23:18.272</td>\n",
       "      <td>2024-09-02 09:23:20.063</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50102</th>\n",
       "      <td>w--4rnszliaxy12</td>\n",
       "      <td>Private Links</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-09-02 09:23:20.063</td>\n",
       "      <td>2024-09-02 09:23:26.523</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50103</th>\n",
       "      <td>w--4rnszliaxy12</td>\n",
       "      <td>Google Chrome</td>\n",
       "      <td>docs.google.com</td>\n",
       "      <td>2024-09-02 09:23:26.523</td>\n",
       "      <td>2024-09-02 09:23:26.878</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            employeeId               app             site  \\\n",
       "50099  w--4rnszliaxy12          Workpuls              NaN   \n",
       "50100  w--4rnszliaxy12          Workpuls              NaN   \n",
       "50101  w--4rnszliaxy12  Windows Explorer              NaN   \n",
       "50102  w--4rnszliaxy12     Private Links              NaN   \n",
       "50103  w--4rnszliaxy12     Google Chrome  docs.google.com   \n",
       "\n",
       "                   start_time                end_time  mouseClicks  \\\n",
       "50099 2024-09-02 09:23:12.586 2024-09-02 09:23:12.839            0   \n",
       "50100 2024-09-02 09:23:14.200 2024-09-02 09:23:18.272            1   \n",
       "50101 2024-09-02 09:23:18.272 2024-09-02 09:23:20.063            0   \n",
       "50102 2024-09-02 09:23:20.063 2024-09-02 09:23:26.523            3   \n",
       "50103 2024-09-02 09:23:26.523 2024-09-02 09:23:26.878            1   \n",
       "\n",
       "       keystrokes    mic  mouseScroll camera  \n",
       "50099           0  False          0.0  False  \n",
       "50100           0  False          0.0  False  \n",
       "50101           0  False          0.0  False  \n",
       "50102           0  False          0.0  False  \n",
       "50103           0  False          0.0  False  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "unique_apps = browsers.browsers.unique()\n",
    "\n",
    "# 1. Adding 'Concentration Lost' where app is inactive\n",
    "df.loc[df['active'] == False, 'app'] = 'Concentration Lost'\n",
    "\n",
    "# 2. Convert 'start' and 'end' timestamps to datetime and sort the DataFrame\n",
    "df['start_time'] = pd.to_datetime(df['start'], unit='ms')\n",
    "df['end_time'] = pd.to_datetime(df['end'], unit='ms')\n",
    "df = df.sort_values(by=['employeeId', 'start_time'])\n",
    "\n",
    "# 3. Rewrite 'app' as 'Private Links' where 'app' is in unique_apps and 'site' is NaN\n",
    "df.loc[df['app'].isin(unique_apps) & df['site'].isna(), 'app'] = 'Private Links'\n",
    "\n",
    "# Keep only necessary columns\n",
    "df = df[['employeeId', 'app', 'site', 'start_time', 'end_time', 'mouseClicks', 'keystrokes', 'mic', 'mouseScroll', 'camera']]\n",
    "\n",
    "\n",
    "# Display the processed DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mappings sites/apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Ensure excluded sites map to themselves in mappings_sites\n",
    "mappings_sites.loc[mappings_sites['site'].isin(exclude_mappings['sites']), 'site_mapping'] = mappings_sites['site']\n",
    "\n",
    "# Map site values in df using mappings_sites\n",
    "df = df.merge(mappings_sites[['site', 'site_mapping']], on='site', how='left')\n",
    "# Replace app values in df with site_mapping where mapping exists\n",
    "df['app'] = df['site_mapping'].combine_first(df['app'])\n",
    "# Drop the temporary site_mapping column\n",
    "df.drop(columns=['site_mapping'], inplace=True)\n",
    "\n",
    "# Step 2: Ensure excluded apps map to themselves in mappings_apps\n",
    "mappings_apps.loc[mappings_apps['app'].isin(exclude_mappings['apps']), 'app_mapping_v2'] = mappings_apps['app']\n",
    "\n",
    "# Map app values in df using mappings_apps\n",
    "df = df.merge(mappings_apps[['app', 'app_mapping_v2']], on='app', how='left')\n",
    "# Replace app values in df with app_mapping_v2 where mapping exists\n",
    "df['app'] = df['app_mapping_v2'].combine_first(df['app'])\n",
    "# Drop the temporary app_mapping_v2 column\n",
    "df.drop(columns=['app_mapping_v2'], inplace=True)\n",
    "\n",
    "# # Keep only necessary columns in the final DataFrame\n",
    "# df = df[['employeeId', 'app', 'start_time', 'end_time']]\n",
    "\n",
    "# Replace multiple spaces with a single underscore in the 'app' column\n",
    "df['app'] = df['app'].str.replace(r'\\s+', '_', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by 'employeeId' and 'start_time' to ensure chronological order\n",
    "df = df.sort_values(by=['employeeId', 'start_time']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ProcessingPipe/data/processed_data_v2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sort by 'employeeId' and 'start_time' to ensure chronological order\n",
    "df = df.sort_values(by=['employeeId', 'start_time']).reset_index(drop=True)\n",
    "\n",
    "# Identify where a new group should start\n",
    "df['new_group'] = (\n",
    "    (df['employeeId'] != df['employeeId'].shift()) |\n",
    "    (df['app'] != df['app'].shift()) |        \n",
    "    (df['start_time'] != df['end_time'].shift()) \n",
    ")\n",
    "\n",
    "# Create a cumulative sum to assign group IDs\n",
    "df['group_id'] = df['new_group'].cumsum()\n",
    "\n",
    "# Group by 'employeeId' and 'group_id' and aggregate\n",
    "df_merged = df.groupby(['employeeId', 'group_id'], as_index=False).agg({\n",
    "    'start_time': 'first',\n",
    "    'end_time': 'last',\n",
    "    'app': 'first'\n",
    "})\n",
    "\n",
    "# Drop the 'group_id' and 'new_group' columns if not needed\n",
    "df_merged = df_merged.drop(columns=['group_id'])\n",
    "\n",
    "# Sort the merged DataFrame by 'start_time' to maintain chronological order\n",
    "df_merged = df_merged.sort_values(by=['employeeId', 'start_time']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete this all vvvvvvv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employeeId</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>app</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>w--4rnszliaxy12</td>\n",
       "      <td>2024-09-02 09:23:12.586</td>\n",
       "      <td>2024-09-02 09:23:12.839</td>\n",
       "      <td>Workpuls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>w--4rnszliaxy12</td>\n",
       "      <td>2024-09-02 09:23:14.200</td>\n",
       "      <td>2024-09-02 09:23:18.272</td>\n",
       "      <td>Workpuls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>w--4rnszliaxy12</td>\n",
       "      <td>2024-09-02 09:23:18.272</td>\n",
       "      <td>2024-09-02 09:23:20.063</td>\n",
       "      <td>Windows_Explorer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>w--4rnszliaxy12</td>\n",
       "      <td>2024-09-02 09:23:20.063</td>\n",
       "      <td>2024-09-02 09:23:26.523</td>\n",
       "      <td>Private_Links</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>w--4rnszliaxy12</td>\n",
       "      <td>2024-09-02 09:23:26.523</td>\n",
       "      <td>2024-09-02 09:23:27.746</td>\n",
       "      <td>docs.google.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        employeeId              start_time                end_time  \\\n",
       "0  w--4rnszliaxy12 2024-09-02 09:23:12.586 2024-09-02 09:23:12.839   \n",
       "1  w--4rnszliaxy12 2024-09-02 09:23:14.200 2024-09-02 09:23:18.272   \n",
       "2  w--4rnszliaxy12 2024-09-02 09:23:18.272 2024-09-02 09:23:20.063   \n",
       "3  w--4rnszliaxy12 2024-09-02 09:23:20.063 2024-09-02 09:23:26.523   \n",
       "4  w--4rnszliaxy12 2024-09-02 09:23:26.523 2024-09-02 09:23:27.746   \n",
       "\n",
       "                app  \n",
       "0          Workpuls  \n",
       "1          Workpuls  \n",
       "2  Windows_Explorer  \n",
       "3     Private_Links  \n",
       "4   docs.google.com  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"pipeline/data/processed_data.csv\")\n",
    "df1 = df1.iloc[:50, :]\n",
    "df1.to_csv(\"data/day_point_dataset_50_rows_aditional_features.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employeeId</th>\n",
       "      <th>app</th>\n",
       "      <th>app_durations</th>\n",
       "      <th>app_start_times</th>\n",
       "      <th>app_end_times</th>\n",
       "      <th>mouseClicks</th>\n",
       "      <th>keystrokes</th>\n",
       "      <th>mic</th>\n",
       "      <th>mouseScroll</th>\n",
       "      <th>camera</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>hours_until_next_workday</th>\n",
       "      <th>workday_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>w--4rnszliaxy12_1</td>\n",
       "      <td>['Workpuls', 'Windows_Explorer', 'Private_Link...</td>\n",
       "      <td>[0.09476666666666667, 0.029849999999999998, 0....</td>\n",
       "      <td>[Timestamp('2024-09-02 09:23:12.586000'), Time...</td>\n",
       "      <td>[Timestamp('2024-09-02 09:23:18.272000'), Time...</td>\n",
       "      <td>[1, 0, 3, 1, 2, 0, 2, 1, 1, 2, 0, 6, 1, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>2024-09-02 09:23:12.586</td>\n",
       "      <td>2024-09-02 20:01:35.212</td>\n",
       "      <td>255.667794</td>\n",
       "      <td>638.377100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>w--4rnszliaxy12_10</td>\n",
       "      <td>['Workpuls', 'discord.com', 'docs.google.com',...</td>\n",
       "      <td>[0.07228333333333332, 0.27903333333333336, 0.1...</td>\n",
       "      <td>[Timestamp('2024-09-13 11:41:39.272000'), Time...</td>\n",
       "      <td>[Timestamp('2024-09-13 11:41:43.609000'), Time...</td>\n",
       "      <td>[1, 4, 1, 11, 1, 2, 3, 1, 1, 1, 2, 7, 1, 2, 0,...</td>\n",
       "      <td>[0, 8, 0, 2, 0, 0, 131, 0, 0, 0, 0, 226, 0, 67...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[0.0, 16.0, 0.0, 133.0, 22.0, 12.0, 3.0, 0.0, ...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>2024-09-13 11:41:39.272</td>\n",
       "      <td>2024-09-13 20:03:15.818</td>\n",
       "      <td>15.699346</td>\n",
       "      <td>501.609100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>w--4rnszliaxy12_11</td>\n",
       "      <td>['Workpuls', 'Private_Links', 'docs.google.com...</td>\n",
       "      <td>[0.11863333333333334, 0.15068333333333334, 0.0...</td>\n",
       "      <td>[Timestamp('2024-09-14 11:45:13.464000'), Time...</td>\n",
       "      <td>[Timestamp('2024-09-14 11:45:20.582000'), Time...</td>\n",
       "      <td>[1, 6, 0, 1, 1, 1, 7, 6, 1, 1, 3, 3, 2, 5, 1, ...</td>\n",
       "      <td>[0, 0, 0, 2, 0, 0, 1, 3, 0, 0, 4, 2, 0, 0, 0, ...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[16.0, 0.0, 0.0, 0.0, 0.0, 0.0, 18.0, 13.0, 1....</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>2024-09-14 11:45:13.464</td>\n",
       "      <td>2024-09-14 20:01:58.878</td>\n",
       "      <td>15.652016</td>\n",
       "      <td>496.756900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>w--4rnszliaxy12_12</td>\n",
       "      <td>['Workpuls', 'discord.com', 'docs.google.com',...</td>\n",
       "      <td>[0.06655, 0.26875, 0.06731666666666666, 0.7145...</td>\n",
       "      <td>[Timestamp('2024-09-15 11:41:06.134000'), Time...</td>\n",
       "      <td>[Timestamp('2024-09-15 11:41:10.127000'), Time...</td>\n",
       "      <td>[1, 6, 1, 6, 1, 1, 0, 0, 1, 1, 1, 17, 1, 3, 1,...</td>\n",
       "      <td>[0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, ...</td>\n",
       "      <td>[False, False, True, True, True, True, True, T...</td>\n",
       "      <td>[0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>2024-09-15 11:41:06.134</td>\n",
       "      <td>2024-09-15 20:01:12.075</td>\n",
       "      <td>13.594557</td>\n",
       "      <td>500.099017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>w--4rnszliaxy12_13</td>\n",
       "      <td>['Workpuls', 'Private_Links', 'discord.com', '...</td>\n",
       "      <td>[0.08503333333333334, 0.12776666666666667, 0.9...</td>\n",
       "      <td>[Timestamp('2024-09-16 09:36:52.481000'), Time...</td>\n",
       "      <td>[Timestamp('2024-09-16 09:36:57.583000'), Time...</td>\n",
       "      <td>[1, 4, 9, 4, 18, 5, 1, 2, 1, 1, 1, 1, 4, 2, 3,...</td>\n",
       "      <td>[0, 0, 63, 0, 6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[0.0, 0.0, 6.0, 0.0, 15.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>2024-09-16 09:36:52.481</td>\n",
       "      <td>2024-09-16 20:15:06.288</td>\n",
       "      <td>15.811850</td>\n",
       "      <td>638.230117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           employeeId                                                app  \\\n",
       "0   w--4rnszliaxy12_1  ['Workpuls', 'Windows_Explorer', 'Private_Link...   \n",
       "1  w--4rnszliaxy12_10  ['Workpuls', 'discord.com', 'docs.google.com',...   \n",
       "2  w--4rnszliaxy12_11  ['Workpuls', 'Private_Links', 'docs.google.com...   \n",
       "3  w--4rnszliaxy12_12  ['Workpuls', 'discord.com', 'docs.google.com',...   \n",
       "4  w--4rnszliaxy12_13  ['Workpuls', 'Private_Links', 'discord.com', '...   \n",
       "\n",
       "                                       app_durations  \\\n",
       "0  [0.09476666666666667, 0.029849999999999998, 0....   \n",
       "1  [0.07228333333333332, 0.27903333333333336, 0.1...   \n",
       "2  [0.11863333333333334, 0.15068333333333334, 0.0...   \n",
       "3  [0.06655, 0.26875, 0.06731666666666666, 0.7145...   \n",
       "4  [0.08503333333333334, 0.12776666666666667, 0.9...   \n",
       "\n",
       "                                     app_start_times  \\\n",
       "0  [Timestamp('2024-09-02 09:23:12.586000'), Time...   \n",
       "1  [Timestamp('2024-09-13 11:41:39.272000'), Time...   \n",
       "2  [Timestamp('2024-09-14 11:45:13.464000'), Time...   \n",
       "3  [Timestamp('2024-09-15 11:41:06.134000'), Time...   \n",
       "4  [Timestamp('2024-09-16 09:36:52.481000'), Time...   \n",
       "\n",
       "                                       app_end_times  \\\n",
       "0  [Timestamp('2024-09-02 09:23:18.272000'), Time...   \n",
       "1  [Timestamp('2024-09-13 11:41:43.609000'), Time...   \n",
       "2  [Timestamp('2024-09-14 11:45:20.582000'), Time...   \n",
       "3  [Timestamp('2024-09-15 11:41:10.127000'), Time...   \n",
       "4  [Timestamp('2024-09-16 09:36:57.583000'), Time...   \n",
       "\n",
       "                                         mouseClicks  \\\n",
       "0  [1, 0, 3, 1, 2, 0, 2, 1, 1, 2, 0, 6, 1, 1, 0, ...   \n",
       "1  [1, 4, 1, 11, 1, 2, 3, 1, 1, 1, 2, 7, 1, 2, 0,...   \n",
       "2  [1, 6, 0, 1, 1, 1, 7, 6, 1, 1, 3, 3, 2, 5, 1, ...   \n",
       "3  [1, 6, 1, 6, 1, 1, 0, 0, 1, 1, 1, 17, 1, 3, 1,...   \n",
       "4  [1, 4, 9, 4, 18, 5, 1, 2, 1, 1, 1, 1, 4, 2, 3,...   \n",
       "\n",
       "                                          keystrokes  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "1  [0, 8, 0, 2, 0, 0, 131, 0, 0, 0, 0, 226, 0, 67...   \n",
       "2  [0, 0, 0, 2, 0, 0, 1, 3, 0, 0, 4, 2, 0, 0, 0, ...   \n",
       "3  [0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, ...   \n",
       "4  [0, 0, 63, 0, 6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "\n",
       "                                                 mic  \\\n",
       "0  [False, False, False, False, False, False, Fal...   \n",
       "1  [False, False, False, False, False, False, Fal...   \n",
       "2  [False, False, False, False, False, False, Fal...   \n",
       "3  [False, False, True, True, True, True, True, T...   \n",
       "4  [False, False, False, False, False, False, Fal...   \n",
       "\n",
       "                                         mouseScroll  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 16.0, 0.0, 133.0, 22.0, 12.0, 3.0, 0.0, ...   \n",
       "2  [16.0, 0.0, 0.0, 0.0, 0.0, 0.0, 18.0, 13.0, 1....   \n",
       "3  [0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 6.0, 0.0, 15.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                              camera               start_time  \\\n",
       "0  [False, False, False, False, False, False, Fal...  2024-09-02 09:23:12.586   \n",
       "1  [False, False, False, False, False, False, Fal...  2024-09-13 11:41:39.272   \n",
       "2  [False, False, False, False, False, False, Fal...  2024-09-14 11:45:13.464   \n",
       "3  [False, False, False, False, False, False, Fal...  2024-09-15 11:41:06.134   \n",
       "4  [False, False, False, False, False, False, Fal...  2024-09-16 09:36:52.481   \n",
       "\n",
       "                  end_time  hours_until_next_workday  workday_duration  \n",
       "0  2024-09-02 20:01:35.212                255.667794        638.377100  \n",
       "1  2024-09-13 20:03:15.818                 15.699346        501.609100  \n",
       "2  2024-09-14 20:01:58.878                 15.652016        496.756900  \n",
       "3  2024-09-15 20:01:12.075                 13.594557        500.099017  \n",
       "4  2024-09-16 20:15:06.288                 15.811850        638.230117  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employeeId                   object\n",
       "app                          object\n",
       "app_durations                object\n",
       "app_start_times              object\n",
       "app_end_times                object\n",
       "start_time                   object\n",
       "end_time                     object\n",
       "hours_until_next_workday    float64\n",
       "workday_duration            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['employeeId', 'app', 'app_durations', 'app_start_times',\n",
       "       'app_end_times', 'start_time', 'end_time', 'hours_until_next_workday',\n",
       "       'workday_duration'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        employeeId app             site              start_time  \\\n",
      "0  w--4rnszliaxy12  []              NaN 2024-09-02 09:23:12.586   \n",
      "1  w--4rnszliaxy12  []              NaN 2024-09-02 09:23:14.200   \n",
      "2  w--4rnszliaxy12  []              NaN 2024-09-02 09:23:18.272   \n",
      "3  w--4rnszliaxy12  []              NaN 2024-09-02 09:23:20.063   \n",
      "4  w--4rnszliaxy12  []  docs.google.com 2024-09-02 09:23:26.523   \n",
      "\n",
      "                 end_time  mouseClicks  keystrokes    mic  mouseScroll camera  \\\n",
      "0 2024-09-02 09:23:12.839            0           0  False          0.0  False   \n",
      "1 2024-09-02 09:23:18.272            1           0  False          0.0  False   \n",
      "2 2024-09-02 09:23:20.063            0           0  False          0.0  False   \n",
      "3 2024-09-02 09:23:26.523            3           0  False          0.0  False   \n",
      "4 2024-09-02 09:23:26.878            1           0  False          0.0  False   \n",
      "\n",
      "   new_group  group_id  \n",
      "0       True         1  \n",
      "1       True         2  \n",
      "2       True         3  \n",
      "3       True         4  \n",
      "4       True         5  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "# Sample DataFrame (assuming you already have it loaded as `df`)\n",
    "# Converting columns to proper list formats\n",
    "\n",
    "def safe_convert_to_list(val):\n",
    "    try:\n",
    "        return literal_eval(val) if pd.notna(val) else []\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "\n",
    "# Apply conversion to the columns that need to be lists\n",
    "df3['app'] = df3['app'].apply(safe_convert_to_list)\n",
    "df3['app_durations'] = df3['app_durations'].apply(safe_convert_to_list)\n",
    "df3['app_start_times'] = df3['app_start_times'].apply(safe_convert_to_list)\n",
    "df3['app_end_times'] = df3['app_end_times'].apply(safe_convert_to_list)\n",
    "\n",
    "# Verify conversion\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employeeId</th>\n",
       "      <th>app</th>\n",
       "      <th>app_durations</th>\n",
       "      <th>app_start_times</th>\n",
       "      <th>app_end_times</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>hours_until_next_workday</th>\n",
       "      <th>workday_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>w--4rnszliaxy12_1</td>\n",
       "      <td>[Workpuls, Windows_Explorer, Private_Links, do...</td>\n",
       "      <td>[0.09476666666666667, 0.029849999999999998, 0....</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2024-09-02 09:23:12.586</td>\n",
       "      <td>2024-09-02 20:01:35.212</td>\n",
       "      <td>255.667794</td>\n",
       "      <td>638.377100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>w--4rnszliaxy12_10</td>\n",
       "      <td>[Workpuls, discord.com, docs.google.com, Misce...</td>\n",
       "      <td>[0.07228333333333332, 0.27903333333333336, 0.1...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2024-09-13 11:41:39.272</td>\n",
       "      <td>2024-09-13 20:03:15.818</td>\n",
       "      <td>15.699346</td>\n",
       "      <td>501.609100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>w--4rnszliaxy12_11</td>\n",
       "      <td>[Workpuls, Private_Links, docs.google.com, Pri...</td>\n",
       "      <td>[0.11863333333333334, 0.15068333333333334, 0.0...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2024-09-14 11:45:13.464</td>\n",
       "      <td>2024-09-14 20:01:58.878</td>\n",
       "      <td>15.652016</td>\n",
       "      <td>496.756900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>w--4rnszliaxy12_12</td>\n",
       "      <td>[Workpuls, discord.com, docs.google.com, Marke...</td>\n",
       "      <td>[0.06655, 0.26875, 0.06731666666666666, 0.7145...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2024-09-15 11:41:06.134</td>\n",
       "      <td>2024-09-15 20:01:12.075</td>\n",
       "      <td>13.594557</td>\n",
       "      <td>500.099017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>w--4rnszliaxy12_13</td>\n",
       "      <td>[Workpuls, Private_Links, discord.com, Private...</td>\n",
       "      <td>[0.08503333333333334, 0.12776666666666667, 0.9...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2024-09-16 09:36:52.481</td>\n",
       "      <td>2024-09-16 20:15:06.288</td>\n",
       "      <td>15.811850</td>\n",
       "      <td>638.230117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           employeeId                                                app  \\\n",
       "0   w--4rnszliaxy12_1  [Workpuls, Windows_Explorer, Private_Links, do...   \n",
       "1  w--4rnszliaxy12_10  [Workpuls, discord.com, docs.google.com, Misce...   \n",
       "2  w--4rnszliaxy12_11  [Workpuls, Private_Links, docs.google.com, Pri...   \n",
       "3  w--4rnszliaxy12_12  [Workpuls, discord.com, docs.google.com, Marke...   \n",
       "4  w--4rnszliaxy12_13  [Workpuls, Private_Links, discord.com, Private...   \n",
       "\n",
       "                                       app_durations app_start_times  \\\n",
       "0  [0.09476666666666667, 0.029849999999999998, 0....              []   \n",
       "1  [0.07228333333333332, 0.27903333333333336, 0.1...              []   \n",
       "2  [0.11863333333333334, 0.15068333333333334, 0.0...              []   \n",
       "3  [0.06655, 0.26875, 0.06731666666666666, 0.7145...              []   \n",
       "4  [0.08503333333333334, 0.12776666666666667, 0.9...              []   \n",
       "\n",
       "  app_end_times               start_time                 end_time  \\\n",
       "0            []  2024-09-02 09:23:12.586  2024-09-02 20:01:35.212   \n",
       "1            []  2024-09-13 11:41:39.272  2024-09-13 20:03:15.818   \n",
       "2            []  2024-09-14 11:45:13.464  2024-09-14 20:01:58.878   \n",
       "3            []  2024-09-15 11:41:06.134  2024-09-15 20:01:12.075   \n",
       "4            []  2024-09-16 09:36:52.481  2024-09-16 20:15:06.288   \n",
       "\n",
       "   hours_until_next_workday  workday_duration  \n",
       "0                255.667794        638.377100  \n",
       "1                 15.699346        501.609100  \n",
       "2                 15.652016        496.756900  \n",
       "3                 13.594557        500.099017  \n",
       "4                 15.811850        638.230117  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ^^^^^^^^^^^^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating working day function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "def create_working_day(df, max_workday_gap=timedelta(hours=2)):\n",
    "    result = []\n",
    "    \n",
    "    for employee_id, group in df.groupby('employeeId'):\n",
    "        # Sort the group by 'start_time' to process logs in chronological order\n",
    "        group = group.sort_values('start_time').reset_index(drop=True)\n",
    "        \n",
    "        daily_apps = []\n",
    "        daily_durations = []\n",
    "        daily_app_start_times = []\n",
    "        daily_app_end_times = []\n",
    "        day_counter = 1\n",
    "        last_end = None\n",
    "        start_time_of_workday = None\n",
    "\n",
    "        for idx, row in group.iterrows():\n",
    "            app_name = row['app']\n",
    "            start_time = row['start_time']\n",
    "            end_time = row['end_time']\n",
    "\n",
    "            if last_end is None:\n",
    "                # Initialize start_time_of_workday and last_end\n",
    "                start_time_of_workday = start_time\n",
    "                last_end = end_time\n",
    "\n",
    "                daily_apps.append(app_name)\n",
    "                daily_durations.append((end_time - start_time).total_seconds() / 60)\n",
    "                daily_app_start_times.append(start_time)\n",
    "                daily_app_end_times.append(end_time)\n",
    "                continue\n",
    "\n",
    "            gap = start_time - last_end\n",
    "\n",
    "            # If the gap is larger than or equal to max_workday_gap, start a new workday\n",
    "            if gap >= max_workday_gap:\n",
    "                # Append the current workday to result\n",
    "                result.append({\n",
    "                    'employeeId': f'{employee_id}_{day_counter}',\n",
    "                    'app': daily_apps,\n",
    "                    'app_durations': daily_durations,\n",
    "                    'app_start_times': daily_app_start_times,\n",
    "                    'app_end_times': daily_app_end_times,\n",
    "                    'start_time': start_time_of_workday,\n",
    "                    'end_time': last_end\n",
    "                })\n",
    "                \n",
    "                day_counter += 1\n",
    "                daily_apps = []\n",
    "                daily_durations = []\n",
    "                daily_app_start_times = []\n",
    "                daily_app_end_times = []\n",
    "                start_time_of_workday = start_time\n",
    "                last_end = end_time\n",
    "\n",
    "                # Start the new workday with the current app\n",
    "                daily_apps.append(app_name)\n",
    "                daily_durations.append((end_time - start_time).total_seconds() / 60)\n",
    "                daily_app_start_times.append(start_time)\n",
    "                daily_app_end_times.append(end_time)\n",
    "                continue\n",
    "\n",
    "            # If the gap is 20 seconds or less, label as \"Log Lost/Software Bug\"\n",
    "            if timedelta(seconds=0) < gap <= timedelta(seconds=20):\n",
    "                daily_apps.append('Log Lost/Software Bug')\n",
    "                daily_durations.append(gap.total_seconds() / 60)\n",
    "                daily_app_start_times.append(last_end)\n",
    "                daily_app_end_times.append(start_time)\n",
    "            \n",
    "            # If the gap is larger than 20 seconds and less than max_workday_gap, label as \"Pause\"\n",
    "            elif timedelta(seconds=20) < gap < max_workday_gap:\n",
    "                daily_apps.append('Pause')\n",
    "                daily_durations.append(gap.total_seconds() / 60)\n",
    "                daily_app_start_times.append(last_end)\n",
    "                daily_app_end_times.append(start_time)\n",
    "\n",
    "            # Add the current app duration\n",
    "            daily_apps.append(app_name)\n",
    "            daily_durations.append((end_time - start_time).total_seconds() / 60)\n",
    "            daily_app_start_times.append(start_time)\n",
    "            daily_app_end_times.append(end_time)\n",
    "\n",
    "            # Update last_end to the current app's end time\n",
    "            last_end = end_time\n",
    "\n",
    "        # Append any remaining apps and durations for the final day\n",
    "        if daily_apps:\n",
    "            result.append({\n",
    "                'employeeId': f'{employee_id}_{day_counter}',\n",
    "                'app': daily_apps,\n",
    "                'app_durations': daily_durations,\n",
    "                'app_start_times': daily_app_start_times,\n",
    "                'app_end_times': daily_app_end_times,\n",
    "                'start_time': start_time_of_workday,\n",
    "                'end_time': last_end\n",
    "            })\n",
    "\n",
    "    # Create a DataFrame from the result list\n",
    "    result_df = pd.DataFrame(result)\n",
    "    \n",
    "    return result_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = create_working_day(df_merged, max_workday_gap=timedelta(hours=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aditional processings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_log_lost_and_same_apps(df):\n",
    "    \"\"\"\n",
    "    This function processes the DataFrame per employee per day.\n",
    "    For each row (which represents a workday for an employee), it processes the lists in 'app',\n",
    "    'app_durations', 'app_start_times', 'app_end_times', and performs the following:\n",
    "    - Merges any 'Log Lost/Software Bug' entries with the previous app:\n",
    "        - The 'Log Lost/Software Bug' entry is removed.\n",
    "        - The previous app's end time and duration are extended to include the 'Log Lost/Software Bug' period.\n",
    "    - Merges any two apps that are the same and next to each other, and the first app's end time is the same as the second app's start time:\n",
    "        - The two app entries are merged into one.\n",
    "        - The duration is summed.\n",
    "        - The start time is the start time of the first app.\n",
    "        - The end time is the end time of the second app.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    # Create a list to store processed rows\n",
    "    processed_rows = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        apps = row['app']\n",
    "        durations = row['app_durations']\n",
    "        app_starts = row['app_start_times']\n",
    "        app_ends = row['app_end_times']\n",
    "        employeeId = row['employeeId']\n",
    "        day_start = row['start_time']\n",
    "        day_end = row['end_time']\n",
    "        \n",
    "        # Initialize lists for processed data\n",
    "        new_apps = []\n",
    "        new_durations = []\n",
    "        new_app_starts = []\n",
    "        new_app_ends = []\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(apps):\n",
    "            app_name = apps[i]\n",
    "            duration = durations[i]\n",
    "            start_time = app_starts[i]\n",
    "            end_time = app_ends[i]\n",
    "            \n",
    "            # If the app is 'Log Lost/Software Bug'\n",
    "            if app_name == 'Log Lost/Software Bug':\n",
    "                # Merge with previous app\n",
    "                if len(new_apps) > 0:\n",
    "                    # Extend the previous app's end time and duration\n",
    "                    new_durations[-1] += duration\n",
    "                    new_app_ends[-1] = end_time\n",
    "                else:\n",
    "                    # If there is no previous app, skip it\n",
    "                    pass\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            # If current app is same as previous and end time of previous is same as start time\n",
    "            if (len(new_apps) > 0 and\n",
    "                app_name == new_apps[-1] and\n",
    "                new_app_ends[-1] == start_time):\n",
    "                # Merge with previous app\n",
    "                new_durations[-1] += duration\n",
    "                new_app_ends[-1] = end_time\n",
    "            else:\n",
    "                # Add new app\n",
    "                new_apps.append(app_name)\n",
    "                new_durations.append(duration)\n",
    "                new_app_starts.append(start_time)\n",
    "                new_app_ends.append(end_time)\n",
    "            i += 1\n",
    "        \n",
    "        # Create a new row with the processed data\n",
    "        new_row = {\n",
    "            'employeeId': employeeId,\n",
    "            'app': new_apps,\n",
    "            'app_durations': new_durations,\n",
    "            'app_start_times': new_app_starts,\n",
    "            'app_end_times': new_app_ends,\n",
    "            'start_time': day_start,\n",
    "            'end_time': day_end\n",
    "        }\n",
    "        processed_rows.append(new_row)\n",
    "    \n",
    "    # Create a new DataFrame\n",
    "    df_processed = pd.DataFrame(processed_rows)\n",
    "    return df_processed\n",
    "\n",
    "def delete_working_days(df):\n",
    "    \"\"\"\n",
    "    This function removes all the working days (rows) where 'start_time' is between\n",
    "    5th of September 2024 and 13th of September 2024.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure 'start_time' is in datetime format\n",
    "    df['start_time'] = pd.to_datetime(df['start_time'])\n",
    "\n",
    "    # Define the date range\n",
    "    start_date = pd.to_datetime('2024-09-05')\n",
    "    end_date = pd.to_datetime('2024-09-13')\n",
    "\n",
    "    # Filter out the rows within the specified date range\n",
    "    df_filtered = df[~((df['start_time'] >= start_date) & (df['start_time'] <= end_date))]\n",
    "\n",
    "    # Reset index if necessary\n",
    "    df_filtered = df_filtered.reset_index(drop=True)\n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, merge 'Log Lost/Software Bug' entries and consecutive same apps\n",
    "df_final = merge_log_lost_and_same_apps(result_df)\n",
    "\n",
    "# Then, delete the specified working days\n",
    "df_final = delete_working_days(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding info in how much does the next day starts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              employeeId              start_time                end_time  \\\n",
      "0      w--4rnszliaxy12_1 2024-09-02 09:23:12.586 2024-09-02 12:20:17.098   \n",
      "1      w--4rnszliaxy12_2 2024-09-02 13:35:03.404 2024-09-02 20:01:35.212   \n",
      "2     w--4rnszliaxy12_10 2024-09-13 11:41:39.272 2024-09-13 20:03:15.818   \n",
      "3     w--4rnszliaxy12_11 2024-09-14 11:45:13.464 2024-09-14 20:01:58.878   \n",
      "4     w--4rnszliaxy12_12 2024-09-15 11:41:06.134 2024-09-15 20:01:12.075   \n",
      "...                  ...                     ...                     ...   \n",
      "7317  w1rgkwz_w-eoxs0_15 2024-10-15 22:46:29.786 2024-10-16 00:45:00.000   \n",
      "7318  w1rgkwz_w-eoxs0_16 2024-10-16 12:27:49.192 2024-10-16 17:00:00.000   \n",
      "7319  w1rgkwz_w-eoxs0_17 2024-10-16 18:35:27.107 2024-10-16 22:15:00.000   \n",
      "7320  w1rgkwz_w-eoxs0_18 2024-10-17 12:21:55.256 2024-10-17 23:30:00.000   \n",
      "7321  w1rgkwz_w-eoxs0_19 2024-10-18 12:10:34.193 2024-10-18 22:48:16.256   \n",
      "\n",
      "      workday_duration  hours_until_next_workday  \n",
      "0           177.075200                  1.246196  \n",
      "1           386.530133                255.667794  \n",
      "2           501.609100                 15.699346  \n",
      "3           496.756900                 15.652016  \n",
      "4           500.099017                 13.594557  \n",
      "...                ...                       ...  \n",
      "7317        118.503567                 11.713664  \n",
      "7318        272.180133                  1.590863  \n",
      "7319        219.548217                 14.115349  \n",
      "7320        668.079067                 12.676165  \n",
      "7321        637.701050                 -1.000000  \n",
      "\n",
      "[7322 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert 'start_time' and 'end_time' to datetime objects\n",
    "df_final['start_time'] = pd.to_datetime(df_final['start_time'])\n",
    "df_final['end_time'] = pd.to_datetime(df_final['end_time'])\n",
    "\n",
    "# Extract base employee ID\n",
    "df_final['base_employeeId'] = df_final['employeeId'].str.extract(r'^(.*)_\\d+$')\n",
    "df_final['base_employeeId'] = df_final['base_employeeId'].fillna(df_final['employeeId'])\n",
    "\n",
    "# Sort the DataFrame\n",
    "df_final = df_final.sort_values(by=['base_employeeId', 'start_time']).reset_index(drop=True)\n",
    "\n",
    "# Calculate 'next_start_time'\n",
    "df_final['next_start_time'] = df_final.groupby('base_employeeId')['start_time'].shift(-1)\n",
    "\n",
    "# Calculate 'hours_until_next_workday'\n",
    "df_final['hours_until_next_workday'] = (df_final['next_start_time'] - df_final['end_time']).dt.total_seconds() / 3600\n",
    "df_final['hours_until_next_workday'] = df_final['hours_until_next_workday'].fillna(-1)\n",
    "\n",
    "# Calculate 'workday_duration' in minutes\n",
    "df_final['workday_duration'] = (df_final['end_time'] - df_final['start_time']).dt.total_seconds() / 60\n",
    "\n",
    "# Drop temporary columns if not needed\n",
    "df_final = df_final.drop(columns=['next_start_time', 'base_employeeId'])\n",
    "\n",
    "# Verify the updated DataFrame\n",
    "print(df_final[['employeeId', 'start_time', 'end_time', 'workday_duration', 'hours_until_next_workday']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling situations where len of day is 45 min or less and merging some splitting inside one working day - 1st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_workdays(df):\n",
    "    # Ensure 'start_time' and 'end_time' are datetime objects\n",
    "    df['start_time'] = pd.to_datetime(df['start_time'])\n",
    "    df['end_time'] = pd.to_datetime(df['end_time'])\n",
    "\n",
    "    # Extract base employee ID (e.g., 'emp_1' from 'emp_1_30')\n",
    "    df['base_employeeId'] = df['employeeId'].str.extract(r'^(.*)_\\d+$')\n",
    "    df['base_employeeId'] = df['base_employeeId'].fillna(df['employeeId'])\n",
    "\n",
    "    # Sort the DataFrame by 'base_employeeId' and 'start_time'\n",
    "    df = df.sort_values(by=['base_employeeId', 'start_time']).reset_index(drop=True)\n",
    "\n",
    "    # Delete all rows where 'workday_duration' < 45 minutes\n",
    "    df = df[df['workday_duration'] >= 45].reset_index(drop=True)\n",
    "\n",
    "    # Recalculate 'hours_until_next_workday' after deletion\n",
    "    def recalculate_hours_until_next_workday(emp_df):\n",
    "        emp_df = emp_df.sort_values('start_time').reset_index(drop=True)\n",
    "        emp_df['next_start_time'] = emp_df['start_time'].shift(-1)\n",
    "        emp_df['hours_until_next_workday'] = (emp_df['next_start_time'] - emp_df['end_time']).dt.total_seconds() / 3600\n",
    "        emp_df['hours_until_next_workday'] = emp_df['hours_until_next_workday'].fillna(-1)\n",
    "        return emp_df.drop(columns=['next_start_time'])\n",
    "\n",
    "    # Reset index to ensure unique indices before grouping\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Add a temporary unique identifier for each row\n",
    "    df['unique_id'] = df.index\n",
    "\n",
    "    # Apply the function and reset the index to avoid duplicate labels\n",
    "    df = df.groupby('base_employeeId', group_keys=False).apply(recalculate_hours_until_next_workday).reset_index(drop=True)\n",
    "\n",
    "    # Now, for each employee, merge adjacent workdays where 'hours_until_next_workday' < 3 hours\n",
    "    indices_to_drop = set()\n",
    "    for employee_id in df['base_employeeId'].unique():\n",
    "        emp_df = df[df['base_employeeId'] == employee_id].sort_values('start_time').reset_index(drop=True)\n",
    "        idx = 0\n",
    "        while idx < len(emp_df) - 1:\n",
    "            current_row = emp_df.loc[idx]\n",
    "            next_row = emp_df.loc[idx + 1]\n",
    "            current_unique_id = current_row['unique_id']\n",
    "            next_unique_id = next_row['unique_id']\n",
    "\n",
    "            # Calculate actual gap between current end_time and next start_time\n",
    "            actual_gap_hours = (next_row['start_time'] - current_row['end_time']).total_seconds() / 3600\n",
    "\n",
    "            # Continue to merge while actual gap is less than 3 hours\n",
    "            if 0 <= actual_gap_hours < 3:\n",
    "                merge_unique_ids = [current_unique_id, next_unique_id]\n",
    "                total_pause_duration = actual_gap_hours * 60  # Convert to minutes\n",
    "\n",
    "                # Merge the workdays\n",
    "                first_unique_id = merge_unique_ids[0]\n",
    "                first_row_index = df.index[df['unique_id'] == first_unique_id][0]\n",
    "\n",
    "                # Initialize merged lists with the first workday's data\n",
    "                first_row = df.loc[df['unique_id'] == first_unique_id].iloc[0]\n",
    "                merged_app = first_row['app'].copy()\n",
    "                merged_app_durations = first_row['app_durations'].copy()\n",
    "                merged_app_start_times = first_row['app_start_times'].copy()\n",
    "                merged_app_end_times = first_row['app_end_times'].copy()\n",
    "\n",
    "                # Loop through the rest of the workdays to merge\n",
    "                for i in range(1, len(merge_unique_ids)):\n",
    "                    uid_prev = merge_unique_ids[i - 1]\n",
    "                    uid_curr = merge_unique_ids[i]\n",
    "\n",
    "                    # Insert 'Pause' between workdays\n",
    "                    merged_app.append('Pause')\n",
    "\n",
    "                    # Retrieve times correctly\n",
    "                    pause_start_time = df.loc[df['unique_id'] == uid_prev, 'end_time'].iloc[0]\n",
    "                    pause_end_time = df.loc[df['unique_id'] == uid_curr, 'start_time'].iloc[0]\n",
    "\n",
    "                    # Calculate pause duration\n",
    "                    pause_duration_minutes = (pause_end_time - pause_start_time).total_seconds() / 60\n",
    "                    merged_app_durations.append(pause_duration_minutes)\n",
    "                    merged_app_start_times.append(pause_start_time)\n",
    "                    merged_app_end_times.append(pause_end_time)\n",
    "\n",
    "                    # Append the lists from the current workday\n",
    "                    row_curr = df.loc[df['unique_id'] == uid_curr].iloc[0]\n",
    "                    merged_app.extend(row_curr['app'])\n",
    "                    merged_app_durations.extend(row_curr['app_durations'])\n",
    "                    merged_app_start_times.extend(row_curr['app_start_times'])\n",
    "                    merged_app_end_times.extend(row_curr['app_end_times'])\n",
    "\n",
    "                # Update 'end_time', 'workday_duration', 'hours_until_next_workday'\n",
    "                last_unique_id = merge_unique_ids[-1]\n",
    "                last_row_index = df.index[df['unique_id'] == last_unique_id][0]\n",
    "                df.at[first_row_index, 'end_time'] = df.at[last_row_index, 'end_time']\n",
    "                total_workday_duration = df[df['unique_id'].isin(merge_unique_ids)]['workday_duration'].sum()\n",
    "                total_workday_duration += total_pause_duration\n",
    "                df.at[first_row_index, 'workday_duration'] = total_workday_duration\n",
    "                df.at[first_row_index, 'hours_until_next_workday'] = df.at[last_row_index, 'hours_until_next_workday']\n",
    "\n",
    "                # Update the merged lists\n",
    "                df.at[first_row_index, 'app'] = merged_app\n",
    "                df.at[first_row_index, 'app_durations'] = merged_app_durations\n",
    "                df.at[first_row_index, 'app_start_times'] = merged_app_start_times\n",
    "                df.at[first_row_index, 'app_end_times'] = merged_app_end_times\n",
    "\n",
    "                # Mark other workdays for dropping\n",
    "                indices_to_drop.update(merge_unique_ids[1:])\n",
    "\n",
    "                # Update emp_df to reflect changes\n",
    "                emp_df.loc[idx, 'end_time'] = df.at[first_row_index, 'end_time']\n",
    "                emp_df.loc[idx, 'workday_duration'] = df.at[first_row_index, 'workday_duration']\n",
    "                emp_df.loc[idx, 'hours_until_next_workday'] = df.at[first_row_index, 'hours_until_next_workday']\n",
    "                emp_df = emp_df.drop(idx + 1).reset_index(drop=True)\n",
    "\n",
    "                # Do not increment idx to check for further merges\n",
    "            else:\n",
    "                idx += 1  # Move to the next workday\n",
    "\n",
    "        # Recalculate 'hours_until_next_workday' after merging\n",
    "        emp_df = emp_df.sort_values('start_time').reset_index(drop=True)\n",
    "        emp_df['next_start_time'] = emp_df['start_time'].shift(-1)\n",
    "        emp_df['hours_until_next_workday'] = (emp_df['next_start_time'] - emp_df['end_time']).dt.total_seconds() / 3600\n",
    "        emp_df['hours_until_next_workday'] = emp_df['hours_until_next_workday'].fillna(-1)\n",
    "\n",
    "        # Update the main df with recalculated 'hours_until_next_workday'\n",
    "        for idx2, row in emp_df.iterrows():\n",
    "            df_index = df.index[df['unique_id'] == row['unique_id']][0]\n",
    "            df.at[df_index, 'hours_until_next_workday'] = row['hours_until_next_workday']\n",
    "\n",
    "    # Drop the rows marked for dropping\n",
    "    df = df[~df['unique_id'].isin(indices_to_drop)].reset_index(drop=True)\n",
    "\n",
    "    # Drop the temporary columns as they're no longer needed\n",
    "    df = df.drop(columns=['base_employeeId', 'unique_id'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Despot Markovic\\AppData\\Local\\Temp\\ipykernel_26320\\4086624341.py:33: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('base_employeeId', group_keys=False).apply(recalculate_hours_until_next_workday).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "df_processed = process_workdays(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 2. Loop through the dataset and check the lengths of the list columns\n",
    "list_columns = ['app', 'app_durations', 'app_start_times', 'app_end_times']\n",
    "for index, row in df_processed.iterrows():\n",
    "    lengths = [len(row[col]) if isinstance(row[col], list) else 0 for col in list_columns]\n",
    "    if len(set(lengths)) != 1:\n",
    "        print(f\"Row {index} has inconsistent list lengths:\")\n",
    "        for col, length in zip(list_columns, lengths):\n",
    "            print(f\" - {col}: {length}\")\n",
    "        print(\"Row data:\")\n",
    "        print(row)\n",
    "        print(\"\\n\")\n",
    "\n",
    "# 3. Plot the histogram of 'workday_duration' column\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_processed['workday_duration'].hist(bins=50, edgecolor='black')\n",
    "plt.xlabel('Workday Duration (minutes)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Workday Duration')\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df_processed is your DataFrame\n",
    "\n",
    "# Filter the data to include only values between 0 and 15 hours\n",
    "filtered_data = df_processed[df_processed['hours_until_next_workday'].between(0, 15)]\n",
    "\n",
    "# Define the bins from 0 to 15 hours with 100 bins\n",
    "bins = np.linspace(0, 15, 101)  # 101 edges for 100 bins\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "filtered_data['hours_until_next_workday'].hist(bins=bins, edgecolor='black')\n",
    "plt.xlabel('Hours Until Next Workday (Hours)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Hours Until Next Workday (0 to 15 hours)')\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_final[df_final.employeeId == 'w--4rnszliaxy12_13'].app.values)\n",
    "print(df_final[df_final.employeeId == 'w--4rnszliaxy12_13'].app_durations.values)\n",
    "print(df_final[df_final.employeeId == 'w--4rnszliaxy12_13'].app_start_times.values)\n",
    "print(df_final[df_final.employeeId == 'w--4rnszliaxy12_13'].app_end_times.values)\n",
    "print('-------------------------------------------------------------------------------------------------------------------------')\n",
    "print(df_final[df_final.employeeId == 'w--4rnszliaxy12_14'].app.values)\n",
    "print(df_final[df_final.employeeId == 'w--4rnszliaxy12_14'].app_durations.values)\n",
    "print(df_final[df_final.employeeId == 'w--4rnszliaxy12_14'].app_start_times.values)\n",
    "print(df_final[df_final.employeeId == 'w--4rnszliaxy12_14'].app_end_times.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_processed[df_processed.employeeId == 'w--4rnszliaxy12_13'].app.values)\n",
    "print(df_processed[df_processed.employeeId == 'w--4rnszliaxy12_13'].app_durations.values)\n",
    "print(df_processed[df_processed.employeeId == 'w--4rnszliaxy12_13'].app_start_times.values)\n",
    "print(df_processed[df_processed.employeeId == 'w--4rnszliaxy12_13'].app_end_times.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_processed = len(df_processed[df_processed.employeeId == 'w--4rnszliaxy12_20'].app.values[0])\n",
    "len_processed2 = len(df_processed[df_processed.employeeId == 'w--4rnszliaxy12_20'].app_durations.values[0])\n",
    "len_processed3 = len(df_processed[df_processed.employeeId == 'w--4rnszliaxy12_20'].app_start_times.values[0])\n",
    "len_processed4 = len(df_processed[df_processed.employeeId == 'w--4rnszliaxy12_20'].app_end_times.values[0])\n",
    "\n",
    "print(len_processed)\n",
    "print(len_processed2)\n",
    "print(len_processed3)\n",
    "print(len_processed4)\n",
    "\n",
    "len_final = len(df_final[df_final.employeeId == 'w--4rnszliaxy12_20'].app.values[0])\n",
    "len_final2 = len(df_final[df_final.employeeId == 'w--4rnszliaxy12_20'].app_durations.values[0])\n",
    "len_final3 = len(df_final[df_final.employeeId == 'w--4rnszliaxy12_20'].app_start_times.values[0])\n",
    "len_final4 = len(df_final[df_final.employeeId == 'w--4rnszliaxy12_20'].app_end_times.values[0])\n",
    "\n",
    "print(len_final)\n",
    "print(len_final2)\n",
    "print(len_final3)\n",
    "print(len_final4)\n",
    "\n",
    "len_final = len(df_final[df_final.employeeId == 'w--4rnszliaxy12_21'].app.values[0])\n",
    "len_final2 = len(df_final[df_final.employeeId == 'w--4rnszliaxy12_21'].app_durations.values[0])\n",
    "len_final3 = len(df_final[df_final.employeeId == 'w--4rnszliaxy12_21'].app_start_times.values[0])\n",
    "len_final4 = len(df_final[df_final.employeeId == 'w--4rnszliaxy12_21'].app_end_times.values[0])\n",
    "\n",
    "print(len_final)\n",
    "print(len_final2)\n",
    "print(len_final3)\n",
    "print(len_final4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df_final' is your DataFrame\n",
    "\n",
    "# Specify the employeeId\n",
    "employee_id = 'w--4rnszliaxy12_20'\n",
    "\n",
    "# Get the row corresponding to the employee\n",
    "employee_row = df_processed[df_processed['employeeId'] == employee_id]\n",
    "\n",
    "# Check if the employee exists in the DataFrame\n",
    "if employee_row.empty:\n",
    "    print(f\"Employee with ID '{employee_id}' not found in the DataFrame.\")\n",
    "else:\n",
    "    # Extract the 'app' list\n",
    "    app_list = employee_row['app'].values[0]\n",
    "    \n",
    "    # Find the indexes where 'app' is 'Pause'\n",
    "    pause_indexes = [index for index, value in enumerate(app_list) if value == 'Pause']\n",
    "    \n",
    "    # Extract the 'app_durations' list\n",
    "    app_durations_list = employee_row['app_durations'].values[0]\n",
    "    app_start_list = employee_row['app_start_times'].values[0]\n",
    "    app_end_list = employee_row['app_end_times'].values[0]\n",
    "    \n",
    "    # Retrieve durations at the pause indexes\n",
    "    pause_durations = [app_durations_list[index] for index in pause_indexes]\n",
    "    pause_start = [app_start_list[index] for index in pause_indexes]\n",
    "    pause_end = [app_end_list[index] for index in pause_indexes]\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Indexes of 'Pause' in 'app' list: {pause_indexes}\")\n",
    "    print(f\"Corresponding durations in 'app_durations': {pause_durations}\")\n",
    "    print(f\"Corresponding durations in 'app_durations': {pause_start}\")\n",
    "    print(f\"Corresponding durations in 'app_durations': {pause_end}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.to_csv(\"day_point_dataset_testset.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding similar apps based on the usage patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model on the app column\n",
    "import pandas as pd\n",
    "import ast\n",
    "from gensim.models import Word2Vec\n",
    "from pathlib import Path\n",
    "\n",
    "df_processed = pd.read_csv(\"day_point_dataset_v5.csv\", sep=';')\n",
    "df_processed['app'] = df_processed['app'].apply(ast.literal_eval)\n",
    "\n",
    "# Convert the 'apps' column to a list of lists for Word2Vec\n",
    "app_sequences = df_processed['app'].tolist()\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=app_sequences,\n",
    "    vector_size=100,\n",
    "    window=4,\n",
    "    min_count=5,\n",
    "    sg=1,\n",
    "    workers=4,\n",
    "    epochs=20 \n",
    ")\n",
    "\n",
    "# Save the trained model for later use\n",
    "model.save(\"apps_word2vec_v3.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_vector = model.wv['app.slack.com']\n",
    "print(\"Vector for Visual_Studio_Code:\", app_vector)\n",
    " \n",
    "# Find similar apps to 'VSCode'\n",
    "similar_apps = model.wv.most_similar('app.slack.com', topn=5)\n",
    "print(\"Apps similar to Visual_Studio_Code:\", similar_apps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference and clusterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec.load(\"apps_word2vec_v2.model\")\n",
    "\n",
    "# Retrieve training parameters\n",
    "vector_size = model.vector_size\n",
    "window = model.window\n",
    "min_count = model.min_count\n",
    "sg = model.sg\n",
    "epochs = model.epochs\n",
    "workers = model.workers\n",
    "sample = model.sample\n",
    "negative = model.negative\n",
    "alpha = model.alpha\n",
    "\n",
    "# Print the training parameters\n",
    "print(f\"Vector Size (embedding dimension): {vector_size}\")\n",
    "print(f\"Window Size (context): {window}\")\n",
    "print(f\"Min Count (frequency threshold): {min_count}\")\n",
    "print(f\"Skip-Gram (sg=1) or CBOW (sg=0): {sg}\")\n",
    "print(f\"Epochs: {epochs}\")\n",
    "print(f\"Workers (threads): {workers}\")\n",
    "print(f\"Sample (subsampling rate): {sample}\")\n",
    "print(f\"Negative Sampling (negative): {negative}\")\n",
    "print(f\"Initial Learning Rate (alpha): {alpha}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract embeddings for all apps\n",
    "app_vectors = model.wv.vectors\n",
    "\n",
    "# Run KMeans with varying numbers of clusters and calculate WCSS\n",
    "wcss = []\n",
    "for i in range(1, 100):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "    kmeans.fit(app_vectors)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot WCSS to observe the \"elbow\"\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, 100), wcss, marker='o', linestyle='-')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Perform hierarchical clustering using linkage\n",
    "Z = linkage(model.wv.vectors, method='average', metric='cosine')  # Average linkage with cosine distance\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(12, 8))\n",
    "dendrogram(Z, truncate_mode='level', p=10, labels=model.wv.index_to_key)  # Use p to limit levels shown\n",
    "plt.title(\"Dendrogram\")\n",
    "plt.xlabel(\"App Names\")\n",
    "plt.ylabel(\"Cosine Distance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "# Load or define your data\n",
    "data = model.wv.vectors  # Assuming data is from a Word2Vec model\n",
    "\n",
    "# Store the best parameters for each method\n",
    "best_k = None\n",
    "best_k_silhouette = -1\n",
    "best_threshold = None\n",
    "best_threshold_silhouette = -1\n",
    "\n",
    "# --- Finding optimal k for KMeans ---\n",
    "print(\"Evaluating KMeans clustering...\")\n",
    "for k in range(2, 301):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    score = silhouette_score(data, labels, metric='cosine')\n",
    "    \n",
    "    if score > best_k_silhouette:\n",
    "        best_k_silhouette = score\n",
    "        best_k = k\n",
    "\n",
    "    if k % 50 == 0:  # Print progress every 50 steps\n",
    "        print(f\"KMeans - Tested k={k}, Silhouette Score={score:.4f}\")\n",
    "\n",
    "print(f\"Best KMeans clustering: k={best_k} with Silhouette Score={best_k_silhouette:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aglomerative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "# Generate linkage matrix using cosine distance\n",
    "Z = linkage(data, method='average', metric='cosine')\n",
    "\n",
    "best_threshold = None\n",
    "best_silhouette = -1\n",
    "best_n_clusters = 0\n",
    "\n",
    "print(\"Evaluating Hierarchical clustering with custom distance thresholds...\")\n",
    "# Loop over thresholds and use fcluster to cut the dendrogram\n",
    "for threshold in np.arange(0.1, 1.1, 0.01):  # Adjust step size as needed\n",
    "    labels = fcluster(Z, t=threshold, criterion='distance')\n",
    "    \n",
    "    n_clusters = len(set(labels))\n",
    "    print(f\"Threshold {threshold:.2f} resulted in {n_clusters} clusters.\")\n",
    "    \n",
    "    # Ensure valid number of clusters\n",
    "    if n_clusters < 2 or n_clusters >= len(data) * 0.9:\n",
    "        continue\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    score = silhouette_score(data, labels, metric='cosine')\n",
    "    \n",
    "    if score > best_silhouette:\n",
    "        best_silhouette = score\n",
    "        best_threshold = threshold\n",
    "        best_n_clusters = n_clusters\n",
    "\n",
    "    print(f\"Threshold={threshold:.2f}, Silhouette Score={score:.4f}\")\n",
    "\n",
    "print(f\"Best threshold: {best_threshold}, Number of clusters: {best_n_clusters}, Best Silhouette Score: {best_silhouette:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "import json\n",
    "\n",
    "# Assuming `data` contains your app embedding vectors\n",
    "# Step 1: Generate the linkage matrix using hierarchical clustering\n",
    "Z = linkage(data, method='average', metric='cosine')\n",
    "\n",
    "# Step 2: Define the best threshold based on previous analysis\n",
    "threshold = best_threshold  # Replace with the best threshold from your analysis\n",
    "\n",
    "# Step 3: Generate labels based on the threshold\n",
    "labels = fcluster(Z, t=threshold, criterion='distance')\n",
    "\n",
    "# Step 4: Create a dictionary to store clusters\n",
    "clusters = {}\n",
    "app_names = model.wv.index_to_key  # Assuming these are the app names corresponding to embeddings\n",
    "\n",
    "# Step 5: Populate the dictionary with cluster labels\n",
    "for app, label in zip(app_names, labels):\n",
    "    cluster_key = f\"cluster_{label}\"\n",
    "    if cluster_key not in clusters:\n",
    "        clusters[cluster_key] = []\n",
    "    clusters[cluster_key].append(app)\n",
    "\n",
    "# Print and inspect the clusters (optional)\n",
    "print(clusters)\n",
    "\n",
    "# Save clusters to JSON\n",
    "with open(\"app_clusters.json\", \"w\") as json_file:\n",
    "    json.dump(clusters, json_file, indent=4)\n",
    "\n",
    "print(\"Clusters saved to app_clusters.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "# Extract the embeddings\n",
    "app_vectors = model.wv.vectors\n",
    "\n",
    "# Perform t-SNE for dimensionality reduction to 2D\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "app_vectors_2d = tsne.fit_transform(app_vectors)\n",
    "\n",
    "# Convert labels to a numpy array for indexing\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Plot each cluster in a different color\n",
    "plt.figure(figsize=(12, 8))\n",
    "unique_labels = set(labels)\n",
    "\n",
    "for label in unique_labels:\n",
    "    indices = np.where(labels == label)\n",
    "    cluster_points = app_vectors_2d[indices]\n",
    "    \n",
    "    # Plot the points for each cluster\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f\"Cluster {label}\", alpha=0.6)\n",
    "\n",
    "# Optional: Annotate points with app names\n",
    "for i, app_name in enumerate(app_names):\n",
    "    plt.annotate(app_name, (app_vectors_2d[i, 0], app_vectors_2d[i, 1]), fontsize=6, alpha=0.5)\n",
    "\n",
    "plt.title(\"App Clusters Visualization\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting all embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all words and their embeddings\n",
    "words = model.wv.index_to_key  # List of all words in the vocabulary\n",
    "embeddings = {word: model.wv[word] for word in words}  # Dictionary of word: embedding\n",
    "\n",
    "# Convert embeddings to a DataFrame\n",
    "embeddings_df = pd.DataFrame.from_dict(embeddings, orient='index')\n",
    "\n",
    "# Export to CSV\n",
    "embeddings_df.to_csv(\"all_word_embeddings.csv\", index_label=\"word\", sep=';')\n",
    "\n",
    "# (Optional) Export to JSON\n",
    "embeddings_df.to_json(\"all_word_embeddings.json\", orient=\"index\")\n",
    "\n",
    "print(\"All embeddings have been saved to 'all_word_embeddings.csv' and 'all_word_embeddings.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sliced = df\n",
    "\n",
    "df_sliced['start_time'] = pd.to_datetime(df_sliced['start'], unit='ms')\n",
    "df_sliced['end_time'] = pd.to_datetime(df_sliced['end'], unit='ms')\n",
    "\n",
    "df_sliced = (\n",
    "    df_sliced\n",
    "    .sort_values(by=['employeeId', 'start_time'], ascending=[True, True])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_sliced['duration'] = (df_sliced['end_time'] - df_sliced['start_time']).dt.total_seconds()\n",
    "\n",
    "df_sliced['start_date'] = df_sliced['start_time'].astype(str).apply(lambda x: x.split(' ')[0])\n",
    "df_sliced['end_date'] = df_sliced['end_time'].astype(str).apply(lambda x: x.split(' ')[0])\n",
    "\n",
    "display(df_sliced.head())\n",
    "print(f'Shape: {df_sliced.shape}.\\n')\n",
    "\n",
    "display(\n",
    "    df_sliced\n",
    "    .start_date\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    "    .reset_index()\n",
    "    .assign(proportion=lambda x: ((x['count'] / x['count'].sum())*100).round(2).astype('str') + ' %')\n",
    "    .assign(cum_count=lambda x: x['count'].cumsum())\n",
    "    .assign(cum_proportion=lambda x: ((x['cum_count'] / x['count'].sum())*100).round(2).astype(str) + ' %')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sliced = df\n",
    "\n",
    "df_sliced['start_time'] = pd.to_datetime(df_sliced['start'], unit='ms')\n",
    "df_sliced['end_time'] = pd.to_datetime(df_sliced['end'], unit='ms')\n",
    "\n",
    "df_sliced['start_date'] = df_sliced['start_time'].astype(str).apply(lambda x: x.split(' ')[0])\n",
    "df_sliced['end_date'] = df_sliced['end_time'].astype(str).apply(lambda x: x.split(' ')[0])\n",
    "\n",
    "df_sliced['duration'] = (df_sliced['end_time'] - df_sliced['start_time']).dt.total_seconds()\n",
    "\n",
    "employee_date_groupby = df_sliced.groupby(by=['employeeId', 'start_date'])\n",
    "\n",
    "df_employee_date = (\n",
    "    employee_date_groupby['start_time']\n",
    "    .min()\n",
    "    .reset_index()\n",
    "    .merge(right=(\n",
    "                    employee_date_groupby['end_time']\n",
    "                    .max()\n",
    "                    .reset_index()\n",
    "                 ),\n",
    "           how='inner',\n",
    "           on=['employeeId', 'start_date'])\n",
    "    .sort_values(by=['employeeId', 'start_date'], ascending=True)\n",
    "    .assign(start_minus_end=lambda x: (x['end_time'] - x['start_time']).astype(str).apply(lambda x: x.split(' ')[2]))\n",
    "    .rename(columns={'start_minus_end':'start_time - end_time'})\n",
    "    .merge(right=(\n",
    "                    employee_date_groupby['app']\n",
    "                    .count()\n",
    "                    .reset_index()\n",
    "                    .rename(columns={'app':'app_count'})\n",
    "                 ),\n",
    "           how='inner',\n",
    "           on=['employeeId', 'start_date'])\n",
    "    .merge(right=(\n",
    "                    employee_date_groupby['app']\n",
    "                    .nunique()\n",
    "                    .reset_index()\n",
    "                    .rename(columns={'app':'app_count_unique'})\n",
    "                 ),\n",
    "           how='inner',\n",
    "           on=['employeeId', 'start_date'])\n",
    "    .merge(right=(\n",
    "                    employee_date_groupby['duration']\n",
    "                    .min()\n",
    "                    .reset_index()\n",
    "                    .round(3)\n",
    "                    .rename(columns={'duration':'min_app_duration (s)'})\n",
    "                 ),\n",
    "           how='inner',\n",
    "           on=['employeeId', 'start_date'])\n",
    "    .merge(right=(\n",
    "                    employee_date_groupby['duration']\n",
    "                    .max()\n",
    "                    .reset_index()\n",
    "                    .round(3)\n",
    "                    .rename(columns={'duration':'max_app_duration (s)'})\n",
    "                 ),\n",
    "           how='inner',\n",
    "           on=['employeeId', 'start_date'])\n",
    "    .merge(right=(\n",
    "                    employee_date_groupby['duration']\n",
    "                    .mean()\n",
    "                    .reset_index()\n",
    "                    .round(3)\n",
    "                    .rename(columns={'duration':'average_app_duration (s)'})\n",
    "                 ),\n",
    "           how='inner',\n",
    "           on=['employeeId', 'start_date'])\n",
    "    .merge(right=(\n",
    "                    employee_date_groupby['duration']\n",
    "                    .sum()\n",
    "                    .round(3)\n",
    "                    .reset_index()\n",
    "                    .rename(columns={'duration':'total_duration (s)'})\n",
    "                 ),\n",
    "           how='inner',\n",
    "           on=['employeeId', 'start_date'])\n",
    "    .set_index(['employeeId', 'start_date'])\n",
    ")\n",
    "\n",
    "display(df_employee_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_employee = df_employee_date[df_employee_date['total_duration (s)'] < 10000.0].index.get_level_values(0).nunique()\n",
    "unique_date = df_employee_date[df_employee_date['total_duration (s)'] < 10000.0].index.get_level_values(1).nunique()\n",
    "\n",
    "unique_employee_all = df_employee_date.index.get_level_values(0).nunique()\n",
    "unique_date_all = df_employee_date.index.get_level_values(1).nunique()\n",
    "\n",
    "print(f\"Unique dates filtered: {unique_date}\")\n",
    "print(f\"Unique dates all: {unique_date_all}\\n\\n\")\n",
    "\n",
    "print(f\"Unique employee filtered: {unique_employee}\")\n",
    "print(f\"Unique employee all: {unique_employee_all}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "# convert start and end from unix to datetime \n",
    "df['start'] = pd.to_datetime(df['start'], unit='ms')\n",
    "df['end'] = pd.to_datetime(df['end'], unit='ms')\n",
    "\n",
    "df['transaction_lasted'] = (df['end'] - df['start']).dt.seconds\n",
    "\n",
    "# reorder columns\n",
    "columns_order = [\n",
    "    'id',\n",
    "    'teamId',\n",
    "    'employeeId',\n",
    "    'start',\n",
    "    'end',\n",
    "    'transaction_lasted',\n",
    "    'app',\n",
    "    'appId',\n",
    "    'appFileName',\n",
    "    'site',\n",
    "    'categoryId',\n",
    "    'redacted_url',\n",
    "    'redacted_title',\n",
    "    'keystrokes',\n",
    "    'mouseScroll',\n",
    "    'mouseClicks',\n",
    "    'mic',\n",
    "    'camera',\n",
    "    'active',\n",
    "    'productivity',\n",
    "    'failure',\n",
    "    'os'\n",
    "]\n",
    "\n",
    "# rename columns & sort values at the employee level\n",
    "df = (\n",
    "    df[columns_order]\n",
    "    .rename(columns={'start':'start_datetime', 'end': 'end_datetime'})\n",
    "    .sort_values(by=['employeeId', 'start_datetime'], ascending=[True, True])\n",
    ")\n",
    "\n",
    "display(df.tail())\n",
    "print(f'Shape of the data: {df.shape}\\n')\n",
    "print(f'Unique values of Id column: {df.id.nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI API mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Set your API key here\n",
    "openai.api_key = \"your-api-key-here\"\n",
    "\n",
    "# Function to categorize a list of sites in batches of 100\n",
    "def categorize_sites(df, site_column, categories, model=\"gpt-4-mini\", batch_size=100):\n",
    "    df['site_mapped'] = None  # Initialize the new column for mapped categories\n",
    "    \n",
    "    # Define the prompt template with placeholders\n",
    "    prompt_template = (\n",
    "        \"Categorize each of the following sites into one of these categories: \"\n",
    "        f\"{', '.join(categories)}. If a site doesnt clearly fit, use your best judgment.\\n\\n\"\n",
    "    )\n",
    "\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df.iloc[i:i + batch_size]\n",
    "        site_list = batch[site_column].tolist()\n",
    "        \n",
    "        # Prepare the batch prompt with site indices\n",
    "        sites_prompt = \"\\n\".join([f\"{j+1}. Site: {site}\" for j, site in enumerate(site_list)])\n",
    "        prompt = prompt_template + sites_prompt + \"\\n\\nReturn the categories as a list.\"\n",
    "        \n",
    "        try:\n",
    "            response = openai.Completion.create(\n",
    "                model=model,\n",
    "                prompt=prompt,\n",
    "                max_tokens=500,\n",
    "                temperature=0.3,\n",
    "                n=1,\n",
    "                stop=None\n",
    "            )\n",
    "            # Parse response, keeping list order aligned with input sites\n",
    "            categories = response.choices[0].text.strip().splitlines()\n",
    "            categories = [cat.strip() for cat in categories if cat]  # Clean and keep non-empty lines\n",
    "            \n",
    "            # Assign each mapped category back to the respective site in the DataFrame\n",
    "            for idx, category in zip(batch.index, categories):\n",
    "                df.at[idx, 'site_mapped'] = category\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch starting at index {i}: {e}\")\n",
    "            # Assign \"Error\" in case of failure for each site in the batch\n",
    "            for idx in batch.index:\n",
    "                df.at[idx, 'site_mapped'] = \"Error\"\n",
    "        \n",
    "        # Optional delay to avoid hitting rate limits\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "df = pd.DataFrame({'site': [\"example1.com\", \"example2.com\", \"financeportal.com\", ...]})\n",
    "categories = [\"Finance\", \"Communication\", \"Social Media\", \"News\", \"Entertainment\", ...]\n",
    "\n",
    "df = categorize_sites(df, 'site', categories)\n",
    "print(df[['site', 'site_mapped']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def categorize_sites(df, site_column, categories, model=\"gpt-4-mini\", batch_size=100, retries=3):\n",
    "    df['site_mapped'] = None\n",
    "\n",
    "    prompt_template = (\n",
    "        \"Categorize each of the following sites into one of these categories: \"\n",
    "        f\"{', '.join(categories)}. If a site doesnt clearly fit, use your best judgment.\\n\\n\"\n",
    "    )\n",
    "\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df.iloc[i:i + batch_size]\n",
    "        site_list = batch[site_column].tolist()\n",
    "        \n",
    "        sites_prompt = \"\\n\".join([f\"{j+1}. Site: {site}\" for j, site in enumerate(site_list)])\n",
    "        prompt = prompt_template + sites_prompt + \"\\n\\nReturn the categories as a list.\"\n",
    "\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = openai.Completion.create(\n",
    "                    model=model,\n",
    "                    prompt=prompt,\n",
    "                    max_tokens=600,\n",
    "                    temperature=0.2,\n",
    "                    n=1,\n",
    "                    stop=None\n",
    "                )\n",
    "                \n",
    "                categories_response = response.choices[0].text.strip().splitlines()\n",
    "                categories_response = [cat.strip() for cat in categories_response if cat]\n",
    "                \n",
    "                if len(categories_response) == batch_size:\n",
    "                    for idx, category in zip(batch.index, categories_response):\n",
    "                        df.at[idx, 'site_mapped'] = category\n",
    "                    break \n",
    "\n",
    "                else:\n",
    "                    print(f\"Incomplete response for batch starting at index {i}; retrying missing entries...\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch starting at index {i}, attempt {attempt + 1}: {e}\")\n",
    "                time.sleep(1)\n",
    "\n",
    "        missing_indices = batch[df['site_mapped'].isnull()].index\n",
    "        \n",
    "        for idx in missing_indices:\n",
    "            site = df.at[idx, site_column]\n",
    "            single_prompt = prompt_template + f\"1. Site: {site}\\n\\nReturn the category.\"\n",
    "\n",
    "            for attempt in range(retries):\n",
    "                try:\n",
    "                    response = openai.Completion.create(\n",
    "                        model=model,\n",
    "                        prompt=single_prompt,\n",
    "                        max_tokens=10,\n",
    "                        temperature=0.3,\n",
    "                        n=1,\n",
    "                        stop=None\n",
    "                    )\n",
    "                    category = response.choices[0].text.strip()\n",
    "                    df.at[idx, 'site_mapped'] = category\n",
    "                    break  \n",
    "                except Exception as e:\n",
    "                    print(f\"Error retrying site {site} at index {idx}, attempt {attempt + 1}: {e}\")\n",
    "                    time.sleep(1)\n",
    "                    if attempt == retries - 1:\n",
    "                        df.at[idx, 'site_mapped'] = \"Error\" \n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = categorize_sites(df, 'site', categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import csv\n",
    "\n",
    "# Set your API key\n",
    "openai.api_key = \"YOUR_API_KEY\"\n",
    "\n",
    "# Define the list of site strings you want to categorize\n",
    "sites = [\n",
    "    \"google.mail.com\", \"mail.yahoo.com\", \"linkedin.com\",\n",
    "    # Add your 250,000 site strings here\n",
    "]\n",
    "\n",
    "# Define your categories for classification\n",
    "categories = [\"Finance\", \"Communication\", \"Social Media\", \"E-commerce\", \"Education\", \"News\"]\n",
    "prompt_template = \"\"\"\n",
    "Classify the following site into one of the categories: {categories}.\n",
    "Site: {site}\n",
    "Category:\n",
    "\"\"\"\n",
    "\n",
    "# Helper function to format the prompt\n",
    "def format_prompt(site):\n",
    "    return prompt_template.format(categories=\", \".join(categories), site=site)\n",
    "\n",
    "# Divide sites into batches for the Batch API\n",
    "batch_size = 100 \n",
    "batched_sites = [sites[i:i + batch_size] for i in range(0, len(sites), batch_size)]\n",
    "\n",
    "# Process each batch\n",
    "results = []\n",
    "for batch in batched_sites:\n",
    "    # Format the prompts for the entire batch\n",
    "    batch_prompts = [format_prompt(site) for site in batch]\n",
    "\n",
    "    try:\n",
    "        # Send batch request to OpenAI API\n",
    "        response = openai.Completion.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            prompt=batch_prompts,\n",
    "            max_tokens=15,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        \n",
    "        # Collect responses, handling cases with missing or empty predictions\n",
    "        for i, choice in enumerate(response.get(\"choices\", [])):\n",
    "            if 'text' in choice and choice['text'].strip():\n",
    "                # Valid prediction found\n",
    "                results.append((batch[i], choice['text'].strip()))\n",
    "            else:\n",
    "                # Handle missing prediction (assign 'Unknown' or other placeholder)\n",
    "                results.append((batch[i], \"Unknown\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle API call errors by logging and assigning 'Error' as the prediction\n",
    "        print(f\"Error processing batch: {e}\")\n",
    "        for site in batch:\n",
    "            results.append((site, \"Error\"))\n",
    "\n",
    "# Print the results or save to a file\n",
    "for site, category in results:\n",
    "    print(f\"Site: {site} - Category: {category}\")\n",
    "\n",
    "# Optionally save to a CSV\n",
    "with open('categorized_sites.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Site\", \"Category\"])\n",
    "    writer.writerows(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from IPython.display import Image, display\n",
    "\n",
    "dataset_path1 = Path(\"./mappings/site_mappings_2nd_round.csv\")\n",
    "dataset_path2 = Path(\"./mappings/unique_apps.csv\")\n",
    "\n",
    "df_mappings = pd.read_csv(dataset_path1)\n",
    "df_apps = pd.read_csv(dataset_path2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mappings = df_mappings.site_mapping_v3.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list into a more readable format\n",
    "categories_list = \"\\n\".join(\", \".join(all_mappings[i:i+4]) for i in range(0, len(all_mappings), 4))\n",
    "\n",
    "\n",
    "categorize_system_prompt = f\"\"\"\n",
    "Your goal is to categorize apps based on their general purpose and usage. You will be provided with an app name, and you will output a JSON object containing the following information:\n",
    "\n",
    "{{\n",
    "    \"Category\": \"string\" // The primary category of the app  based on its general purpose and usage\n",
    "}}\n",
    "\n",
    "The Category must be chosen exclusively from the following predefined options:\n",
    "{categories_list}\n",
    "\n",
    "Only select a single category from this list that most accurately reflects the apps main purpose and typical usage. Do not create any new categories outside of those provided.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def get_categories(website_name):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.1,\n",
    "        response_format={ \n",
    "            \"type\": \"json_object\"\n",
    "        },\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": categorize_system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": website_name\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an array of json tasks\n",
    "\n",
    "df2_poc = df2[:1000].copy()\n",
    "\n",
    "tasks = []\n",
    "\n",
    "for index, row in df2_poc.iterrows():\n",
    "    \n",
    "    website_name = row['site']\n",
    "    \n",
    "    task = {\n",
    "        \"custom_id\": f\"task-{index}\",\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            # This is what you would have in your Chat Completions API call\n",
    "            \"model\": \"gpt-4o-mini\",\n",
    "            \"temperature\": 0.1,\n",
    "            \"response_format\": { \n",
    "                \"type\": \"json_object\"\n",
    "            },\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": categorize_system_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": website_name\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    tasks.append(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the file\n",
    "\n",
    "file_name = \"batch_sites.jsonl\"\n",
    "\n",
    "with open(file_name, 'w') as file:\n",
    "    for obj in tasks:\n",
    "        file.write(json.dumps(obj) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_file = client.files.create(\n",
    "  file=open(file_name, \"rb\"),\n",
    "  purpose=\"batch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating .jsonl files in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "dataset_path1 = Path(\"./mappings/site_mappings_2nd_round.csv\")\n",
    "dataset_path2 = Path(\"./mappings/unique_apps.csv\")\n",
    "\n",
    "df_mappings = pd.read_csv(dataset_path1)\n",
    "df_apps = pd.read_csv(dataset_path2)\n",
    "\n",
    "all_mappings = df_mappings.site_mapping_v3.unique()\n",
    "\n",
    "categories_list = \"\\n\".join(\", \".join(all_mappings[i:i+4]) for i in range(0, len(all_mappings), 4))\n",
    "\n",
    "\n",
    "categorize_system_prompt = f\"\"\"\n",
    "Your goal is to categorize a list of apps based on their general purpose and usage. You will be provided with a list of app names, and you will output a JSON object containing a single key, \"Category\", with its value as a list of categories. Each category in the list should correspond to an app in the input list, in the same order.\n",
    "\n",
    "Each input app should be mapped to one of the predefined categories below:\n",
    "{categories_list}\n",
    "\n",
    "The JSON object should have the format:\n",
    "{{\n",
    "    \"Category\": [category_1, category_2, ...]\n",
    "}}\n",
    "\n",
    "The \"Category\" list should contain a single category for each app, corresponding to the input order, and only use categories from the provided list. Do not create any new categories outside of those given.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define paths and parameters\n",
    "folder_path = './batch_files_apps'\n",
    "batch_size = 1\n",
    "batch_file_name = 'batch_sites'\n",
    "\n",
    "# Check and create the folder if it does not exist\n",
    "try:\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    print(f\"Folder '{folder_path}' {'created' if not os.path.exists(folder_path) else 'already exists'}.\")\n",
    "except OSError as e:\n",
    "    print(f\"Error creating folder '{folder_path}': {e}\")\n",
    "    exit()\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    data = pd.read_csv('./unique_sites.csv')  # Adjust the file path as necessary\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Data file not found: {e}\")\n",
    "    exit()\n",
    "except pd.errors.EmptyDataError as e:\n",
    "    print(f\"Data file is empty: {e}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data file: {e}\")\n",
    "    exit()\n",
    "\n",
    "data = data.loc[:20, :]\n",
    "\n",
    "# Compute number of files needed\n",
    "num_files = (len(data) + batch_size - 1) // batch_size\n",
    "\n",
    "# Process and write data in batches\n",
    "for num_file in range(num_files):\n",
    "    start_idx = num_file * batch_size\n",
    "    end_idx = min(start_idx + batch_size, len(data))\n",
    "    data_chunk = data.iloc[start_idx:end_idx]\n",
    "    \n",
    "    output_file = os.path.join(folder_path, f'{batch_file_name}_part{num_file}.jsonl')\n",
    "\n",
    "    try:\n",
    "        # Write each batch to the file\n",
    "        with open(output_file, 'w') as file:\n",
    "            payload = {\n",
    "                \"custom_id\": f\"task-{num_file}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": \"gpt-4o-mini\",\n",
    "                    \"temperature\": 0.1,\n",
    "                    \"response_format\": { \n",
    "                        \"type\": \"json_object\"\n",
    "                    },\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": categorize_system_prompt\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [row['site'] for _, row in data_chunk.iterrows()] \n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "            }\n",
    "                \n",
    "            file.write(json.dumps(payload) + '\\n')\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing to file '{output_file}': {e}\")\n",
    "        continue  # Skip to the next file in case of an error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing jsonl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set OpenAI API key in the environment\n",
    "OPENAI_API_KEY = \"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\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "client = OpenAI()\n",
    "\n",
    "# Directory containing batch input files\n",
    "batch_directory = './batch_files'\n",
    "batch_files = []\n",
    "\n",
    "# Create file objects for the batch job\n",
    "for filename in os.listdir(batch_directory):\n",
    "    file_path = os.path.join(batch_directory, filename)\n",
    "    batch_files.append(client.files.create(\n",
    "        file=open(file_path, \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    ))\n",
    "\n",
    "# Get the IDs of the uploaded batch files\n",
    "batch_file_ids = [batch_file.id for batch_file in batch_files]\n",
    "\n",
    "# Create batch jobs for each file\n",
    "batch_jobs = []\n",
    "for index, file_id in enumerate(batch_file_ids):\n",
    "    batch_jobs.append(client.batches.create(\n",
    "        input_file_id=file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "    ))\n",
    "\n",
    "# Print the details of the created batch jobs\n",
    "for batch_job in batch_jobs:\n",
    "    print(batch_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Extract job IDs from the list of job creations\n",
    "job_ids = [job.id for job in batch_jobs]\n",
    "\n",
    "error_detected = False\n",
    "completed_jobs = set()\n",
    "\n",
    "while True:\n",
    "    for job_id in job_ids:\n",
    "        job_info = client.batches.retrieve(job_id)\n",
    "        \n",
    "        if job_info.status == \"failed\":\n",
    "            # Stop loop if any job has failed\n",
    "            print(f\"Job {job_id} failed with error: {job_info.errors}\")\n",
    "            error_detected = True\n",
    "            break\n",
    "        \n",
    "        elif job_info.status == \"in_progress\":\n",
    "            print(f\"Job {job_id} is in progress, {job_info.request_counts.completed}/{job_info.request_counts.total} requests completed\")\n",
    "        \n",
    "        elif job_info.status == \"finalizing\":\n",
    "            print(f\"Job {job_id} is finalizing, waiting for the output file ID\")\n",
    "        \n",
    "        elif job_info.status == \"completed\":\n",
    "            print(f\"Job {job_id} has completed\")\n",
    "            completed_jobs.add(job_id)\n",
    "        \n",
    "        else:\n",
    "            print(f\"Job {job_id} is in status: {job_info.status}\")\n",
    "    \n",
    "    # Exit loop if any job failed or all jobs are completed\n",
    "    if error_detected or len(completed_jobs) == len(job_ids):\n",
    "        break\n",
    "    \n",
    "    time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract output file IDs for each job\n",
    "output_file_ids = [client.batches.retrieve(job_id).output_file_id for job_id in job_ids]\n",
    "\n",
    "# Read the content of the output files\n",
    "output_files_content = [client.files.content(file_id).text for file_id in output_file_ids]\n",
    "\n",
    "# Extract custom ID and embedding from each file\n",
    "category_data = []\n",
    "for content in output_files_content:\n",
    "    for line in content.split('\\n')[:-1]:  # Avoid processing the last empty line\n",
    "        parsed_data = json.loads(line)\n",
    "        custom_id = parsed_data.get('custom_id')\n",
    "        print(f\"custom_id: {custom_id}\")\n",
    "        print(f\"Response: {parsed_data['response']['body']}\")\n",
    "        break\n",
    "        embedding = parsed_data['response']['body']['data'][0]['Category']\n",
    "        embedding_data.append([custom_id, embedding])\n",
    "\n",
    "# Convert the list of embeddings to a DataFrame\n",
    "embedding_df = pd.DataFrame(embedding_data, columns=['custom_id', 'embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.batches.retrieve('batch_672255351ad08190949714ce7380f4d4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index()\n",
    "data = data.rename(columns={'index':'id'})\n",
    "# we extract the id from the custom_id we created to merge with the original data file\n",
    "embedding_results['id'] = embedding_results['custom_id'].apply(lambda x: int(x.split('custom_id_')[1]))\n",
    "data_with_embedding = df.merge(embedding_results[['id','embedding']], on='id', how='left') \n",
    "data_with_embedding.to_csv('./data/data_with_embedding.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from IPython.display import Image, display\n",
    "\n",
    "client = openai.OpenAI(api_key=\"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\")\n",
    "\n",
    "dataset_path1 = Path(\"./mappings/site_mappings_2nd_round.csv\")\n",
    "# dataset_path2 = Path(\"./mappings/unique_apps.csv\")\n",
    "\n",
    "df_mappings = pd.read_csv(dataset_path1)\n",
    "# df_apps = pd.read_csv(dataset_path2)\n",
    "\n",
    "# df_apps = df_apps.loc[:, 'app']\n",
    "\n",
    "all_mappings = df_mappings.site_mapping_v3.unique()\n",
    "\n",
    "categories_list = \", \".join(all_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_apps.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorize_system_prompt = f'''\n",
    "Your goal is to categorize a list of websites based on their general purpose and usage.\n",
    "You will be provided with a single string containing websites names separated by commas.\n",
    "You will output a JSON object containing a single key, \"Category\", with its value as a list of categories.\n",
    "Each category in the list should correspond to an website in the input string, in the same order.\n",
    "\n",
    "It is crucial that every website in the input string is assigned a category in the output list. Do not skip any websites, and ensure the output list has a one-to-one mapping for each website provided.\n",
    "\n",
    "Each input website should be mapped to one of the predefined categories below:\n",
    "{categories_list}\n",
    "\n",
    "The JSON object should have the format:\n",
    "\n",
    "{{\n",
    "    \"Category\": [category_1, category_2, ...] // Array of categories corresponding to each website\n",
    "}}\n",
    "\n",
    "The \"Category\" list should contain a single category for each website in the input string, based on the order provided, and only use categories from the given list.\n",
    "Do not create any new categories outside of those specified.\n",
    "'''\n",
    "\n",
    "df = df_missed_sites.copy()\n",
    "\n",
    "tasks = []\n",
    "\n",
    "# Iterate over the DataFrame in steps of 10\n",
    "for i in range(0, len(df), 2):\n",
    "\n",
    "    apps = ', '.join(df.iloc[i:i+2, 0])\n",
    "    apps = apps.strip()\n",
    " \n",
    "    # Create the task with the concatenated descriptions\n",
    "    task = {\n",
    "        \"custom_id\": f\"task-{i // 2}\",\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-4o-mini\",\n",
    "            \"temperature\": 0.1,\n",
    "            \"response_format\": { \n",
    "                \"type\": \"json_object\"\n",
    "            },\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": categorize_system_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": apps\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    tasks.append(task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Number of files to create\n",
    "num_files = 1\n",
    "\n",
    "# Splitting `tasks` list into chunks for each file\n",
    "chunk_size = len(tasks) // num_files\n",
    "chunks = [tasks[i * chunk_size: (i + 1) * chunk_size] for i in range(num_files - 1)]\n",
    "chunks.append(tasks[(num_files - 1) * chunk_size:])  # Add remaining tasks to the last chunk\n",
    "\n",
    "# Writing each chunk to a separate JSONL file\n",
    "for i, chunk in enumerate(chunks, start=1):\n",
    "    file_name = f\"batch_files_2/batch_sites{i}.jsonl\"\n",
    "    with open(file_name, 'w') as file:\n",
    "        for obj in chunk:\n",
    "            file.write(json.dumps(obj) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the file\n",
    "\n",
    "file_name = \"batch_files_sites/missed_vatch_sites.jsonl\"\n",
    "\n",
    "with open(file_name, 'w') as file:\n",
    "    for obj in tasks:\n",
    "        file.write(json.dumps(obj) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing multiple jsonl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# Directory containing batch input files\n",
    "batch_directory = './batch_files_2'\n",
    "batch_files = []\n",
    "\n",
    "# Create file objects for the batch job\n",
    "for filename in os.listdir(batch_directory):\n",
    "    file_path = os.path.join(batch_directory, filename)\n",
    "    batch_files.append(client.files.create(\n",
    "        file=open(file_path, \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    ))\n",
    "\n",
    "# Get the IDs of the uploaded batch files\n",
    "batch_file_ids = [batch_file.id for batch_file in batch_files]\n",
    "\n",
    "# Create batch jobs for each file\n",
    "batch_jobs = []\n",
    "for index, file_id in enumerate(batch_file_ids):\n",
    "    batch_jobs.append(client.batches.create(\n",
    "        input_file_id=file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\"\n",
    "    ))\n",
    "\n",
    "# Print the details of the created batch jobs\n",
    "for batch_job in batch_jobs:\n",
    "    print(batch_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Extract job IDs from the list of job creations\n",
    "job_ids = [job.id for job in batch_jobs]\n",
    "\n",
    "error_detected = False\n",
    "completed_jobs = set()\n",
    "\n",
    "while True:\n",
    "    for job_id in job_ids:\n",
    "        job_info = client.batches.retrieve(job_id)\n",
    "        \n",
    "        if job_info.status == \"failed\":\n",
    "            # Stop loop if any job has failed\n",
    "            print(f\"Job {job_id} failed with error: {job_info.errors}\")\n",
    "            error_detected = True\n",
    "            break\n",
    "        \n",
    "        elif job_info.status == \"in_progress\":\n",
    "            print(f\"Job {job_id} is in progress, {job_info.request_counts.completed}/{job_info.request_counts.total} requests completed\")\n",
    "        \n",
    "        elif job_info.status == \"finalizing\":\n",
    "            print(f\"Job {job_id} is finalizing, waiting for the output file ID\")\n",
    "        \n",
    "        elif job_info.status == \"completed\":\n",
    "            print(f\"Job {job_id} has completed\")\n",
    "            completed_jobs.add(job_id)\n",
    "        \n",
    "        else:\n",
    "            print(f\"Job {job_id} is in status: {job_info.status}\")\n",
    "    \n",
    "    # Exit loop if any job failed or all jobs are completed\n",
    "    if error_detected or len(completed_jobs) == len(job_ids):\n",
    "        break\n",
    "    \n",
    "    time.sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract output file IDs for each job\n",
    "output_file_ids = [client.batches.retrieve(job_id).output_file_id for job_id in job_ids]\n",
    "\n",
    "# Read the content of the output files\n",
    "output_files_content = [client.files.content(file_id).text for file_id in output_file_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"./batch_files_2/sites_result_\"\n",
    "\n",
    "for i, file in enumerate(output_files_content):\n",
    "    file_name = folder_name + f'{i+1}.jsonl'\n",
    "    print(file)\n",
    "    with open(file_name, 'w') as fp:\n",
    "        fp.write(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing 1 jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_file = client.files.create(\n",
    "  file=open(file_name, \"rb\"),\n",
    "  purpose=\"batch\"\n",
    ") \n",
    "\n",
    "batch_job = client.batches.create(\n",
    "  input_file_id=batch_file.id,\n",
    "  endpoint=\"/v1/chat/completions\",\n",
    "  completion_window=\"24h\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_job = client.batches.retrieve(batch_job.id)\n",
    "print(batch_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file_id = batch_job.output_file_id\n",
    "result = client.files.content(result_file_id).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file_name = \"batch_files_sites/missed_batch_sites_result.jsonl\"\n",
    "\n",
    "with open(result_file_name, 'wb') as file:\n",
    "    file.write(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Loading data from saved file\n",
    "results = []\n",
    "folder_name = \"batch_files_sites/missed_batch_sites_result.jsonl\"\n",
    "\n",
    "for i in range(1):\n",
    "    result_file_name = folder_name\n",
    "    with open(result_file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            # Parsing the JSON string into a dict and appending to the list of results\n",
    "            json_object = json.loads(line.strip())\n",
    "            results.append(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_sites = 2\n",
    "\n",
    "\n",
    "df_missed_sites[\"site_mapping\"] = np.nan\n",
    "\n",
    "# Populate the 'mapping' column\n",
    "for i, idx in enumerate(good_index):\n",
    "    start = idx * num_sites\n",
    "    end = (idx + 1) * num_sites\n",
    "    category = json.loads(results[idx]['response']['body']['choices'][0]['message']['content'])['Category']\n",
    "    df_missed_sites.iloc[start:end, 1] = category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missed_sites.site_mapping.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_mappingd_new = set(df_missed_sites.site_mapping.unique())\n",
    "unique_mappingd_old = set(mappings_sites.site_mapping_v3.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_mappingd_new - unique_mappingd_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = list(unique_mappingd_new - unique_mappingd_old)\n",
    "filtered_missed = df_missed_sites[df_missed_sites.site_mapping.isin(difference)]\n",
    "filtered_missed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_category_mapping = {\n",
    "    'sttheresahospital.com.ng': 'Health & Wellness',\n",
    "    'socialnowa.io': 'Social Networking',\n",
    "    'compress-pdf.emapnet.com': 'Utilities & Tools',\n",
    "    'demakufu.co.ke': 'Property & Real Estate',\n",
    "    'colombiasmartfit.com.co': 'Health & Wellness',\n",
    "    'holycodedoo.applytojob.com': 'Recruitment',\n",
    "    'arcamax.com': 'News',\n",
    "    'cdn.subsplash.com': 'Hosting & Domains',\n",
    "    'hauzisha.co.ke': 'Property & Real Estate',\n",
    "    'leapscholar.com': 'Education & Learning',\n",
    "    'shouldiremoveit.com': 'Utilities & Tools',\n",
    "    'localhost:42815': 'Local Development',\n",
    "    'straightsecurity.com': 'Cybersecurity',\n",
    "    'career.peek-cloppenburg.com': 'Recruitment',\n",
    "    'trackstar.co.uk': 'Analytics',\n",
    "    'engineeringblog.yelp.com': 'Blogging',\n",
    "    'dashboard.payixy.net': 'Finance',\n",
    "    'publicwin.ro': 'Gaming',\n",
    "    'account.aliyun.com': 'Cloud Services',\n",
    "    'dianesoldit.sites.cbmoxi.com': 'Property & Real Estate',\n",
    "    'gospelreformation.net': 'Nonprofit & Community',\n",
    "    'startupv.io': 'Business',\n",
    "    'pb.olx.com.br': 'E-commerce',\n",
    "    'myteleflora.com': 'E-commerce',\n",
    "    'giddyupgreenclean.com': 'Service & Support',\n",
    "    'eatntrack.ro': 'Health & Wellness',\n",
    "    'allenwooooo.github.io': 'Development',\n",
    "    'localhost:57013': 'Local Development',\n",
    "    'freepressokc.com': 'News',\n",
    "    'wesley.hu': 'Personal Blog',\n",
    "    'csharpplayersguide.com': 'Education & Learning',\n",
    "    'f-movies.cfd': 'Video Entertainment',\n",
    "    'kompare.hr': 'Finance',\n",
    "    'nrplearningplatform.com': 'Education & Learning',\n",
    "    'irentlist.website': 'Property & Real Estate',\n",
    "    'enjoy-web-establishments-oyezvwfyo-fatcatcoders.vercel.app': 'Website Management',\n",
    "    'threadreaderapp.com': 'Blogging',\n",
    "    'metropestcontroldfw.com': 'Service & Support',\n",
    "    'ecsbluediamondfm-my.sharepoint.com': 'Documents & Collaboration',\n",
    "    '7f1a-35-231-149-40.ngrok-free.app': 'Local Development',\n",
    "    'suricata.io': 'Cybersecurity',\n",
    "    'dallasfortworthtermitepestcontrol.com': 'Service & Support',\n",
    "    'srv.concrete.co.ke:8090': 'Construction',\n",
    "    'tradefeeds.com': 'Finance',\n",
    "    'info.shopify-desk.com': 'E-commerce',\n",
    "    'hostkey.com': 'Hosting & Domains',\n",
    "    'hbogoasia.ph': 'Video Entertainment',\n",
    "    'californ.zip': 'Miscellaneous',\n",
    "    'depot.dev': 'Development',\n",
    "    'thriveinfocus.com': 'Health & Wellness',\n",
    "    'theblankapp.com': 'Utilities & Tools',\n",
    "    'promptitude.io': 'AI Assistance',\n",
    "    'hotelmemories.hu': 'Hospitality',\n",
    "    'madhaafaanrealestate.com': 'Property & Real Estate',\n",
    "    'rcsprouljr.com': 'Blogging',\n",
    "    'techwelkin.com': 'Education & Learning',\n",
    "    'vpn-panel.top-chrome': 'Cybersecurity',\n",
    "    'ackstmarkswestlands.or.ke': 'Nonprofit & Community',\n",
    "    'tbctopc.com': 'Education & Learning',\n",
    "    'online.vitalsource.com': 'Education & Learning',\n",
    "    'links.rocketlawyer.com': 'Legal & Compliance',\n",
    "    'dcaa902f7ee868f656.gradio.live': 'Development',\n",
    "    'outlook.ca': 'Mail',\n",
    "    'mi-home.pl': 'E-commerce',\n",
    "    'datacater.io': 'Data & Analytics',\n",
    "    'odpc.go.ke': 'Public Sector & Politics',\n",
    "    'onehourairftworth.com': 'Service & Support'\n",
    "}\n",
    "\n",
    "# Update `site_mapping` in `df_missed_sites` only if the `site` exists in `site_category_mapping`\n",
    "df_missed_sites['site_mapping'] = df_missed_sites['site'].apply(\n",
    "    lambda x: site_category_mapping[x] if x in site_category_mapping else None\n",
    ").combine_first(df_missed_sites['site_mapping'])\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_missed_sites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mappings_sites.site_mapping_v3.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Rename `site_mapping_v3` to `site_mapping` in `mapping_sites`\n",
    "mapping_sites = mappings_sites.rename(columns={'site_mapping_v3': 'site_mapping'})\n",
    "\n",
    "# Concatenate `df_missed_sites` and `mapping_sites`\n",
    "combined_df = pd.concat([df_missed_sites, mapping_sites], ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('mappings/site_mappings_2nd_round_v3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_apps.app_mapping.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = app_categories = {\n",
    "    'Microsoft Clock': 'Time Tracking',\n",
    "    'GoToMeeting': 'Video Conferencing',\n",
    "    'Search and Cortana application': 'Search Engine',\n",
    "    'ScreenRec': 'Utilities & Tools',\n",
    "    'ProcenatRadnika': 'Human Resources',\n",
    "    'Microsoft 365 and Office': 'Documents & Collaboration',\n",
    "    'Elegir una aplicacin': 'Localization & Translation',\n",
    "    'Microsoft Phone Link': 'Remote Access',\n",
    "    'copyq': 'Utilities & Tools',\n",
    "    ' ': 'Localization & Translation',\n",
    "    'Adobe After Effects CS6': 'Creative & Design',\n",
    "    'MouseJiggler': 'Utilities & Tools',\n",
    "    'SEO SpyGlass': 'SEO & Analytics',\n",
    "    'Wacom Technology, Corp. Control Panel': 'Creative & Design',\n",
    "    'clumsy.exe': 'Utilities & Tools',\n",
    "    'CMFDeckDesign': 'Creative & Design',\n",
    "    'Command | Update Windows Universal Application, 5.4.0, A00': 'Operating System',\n",
    "    'DNSFilter Agent TrayIcon': 'Cybersecurity',\n",
    "    'Izaberite aplikaciju': 'Localization & Translation',\n",
    "    'Java Platform SE 8 U361': 'Development',\n",
    "    'LocalSend': 'File Sharing',\n",
    "    'Real HEIC to JPG Converter': 'Utilities & Tools',\n",
    "    'Robot Structural Analysis Professional Preview': 'Engineering',\n",
    "    'SAND/SCALE *** Structural ANalysis and Design *** Structural CALculations Ensemble ***': 'Engineering',\n",
    "    'Command | Update Application, 5.4.0, A00': 'Operating System',\n",
    "    'ControlPanel': 'Operating System',\n",
    "    'Dependencies': 'Development',\n",
    "    'ProSeriesLauncher': 'Business Software',\n",
    "    'Proton VPN': 'Cybersecurity',\n",
    "    'Realtek High-Definition Audio Driver, 6.0.1.6111, A06': 'Audio & Video',\n",
    "    'USBPcap': 'Utilities & Tools',\n",
    "    'United Parcel Service, Inc. PatchUPS': 'Logistics & Transportation',\n",
    "    'Windows-Problemberichterstattung': 'Operating System',\n",
    "    'Bir uygulama sein': 'Localization & Translation',\n",
    "    'Builder': 'Development',\n",
    "    'Cambiar la configuracin del equipo': 'Operating System'\n",
    "}\n",
    "res2 = app_categories = {\n",
    "    'Harver System Checker': 'IT Services',\n",
    "    'Intel Rapid Storage Technology Driver and Application, 19.5.1.1040, A02': 'Operating System',\n",
    "    'Intel(R) Network Configuration Services': 'Networking',\n",
    "    'Fill App(Windows)': 'Utilities & Tools',\n",
    "    'Firmware Downloader and ZBI Key Manager Application': 'Operating System',\n",
    "    'GitLab.UI': 'Development',\n",
    "    'VoiceTyper': 'AI Assistance',\n",
    "    'VoicemeeterProSetup': 'Audio & Video',\n",
    "    'Wacom Technology, Corp. Deployer/Undeployer': 'Creative & Design',\n",
    "    'Install ClickUp': 'Project Management',\n",
    "    'Intel BE200/AX411/AX211/AX210/AX201/AX200/9560/9462/9260 Wi-Fi UWD Driver, 23.60.1.2, A45': 'Networking',\n",
    "    'Intel Graphics Properties': 'Operating System',\n",
    "    'Intel HID Event Filter Driver, 2.2.2.9, A21': 'Operating System',\n",
    "    'Intel Management Engine Interface (MEI) Driver': 'Operating System',\n",
    "    'Jet Reports Services': 'Business Software',\n",
    "    'TimerMS': 'Time Tracking',\n",
    "    'Toshiba MQ04ABF100 Hard Drive Firmware Update, 10.0003, A00': 'Operating System',\n",
    "    'Traduo do jogo: Chrono Trigger': 'Localization & Translation',\n",
    "    'Intel 3165/7265/8260/8265 Wi-Fi UWD Driver, 22.130.0.5, A17': 'Networking',\n",
    "    'Intel(R) Arc(TM) Control': 'Operating System',\n",
    "    'Keyboard Layout': 'Operating System',\n",
    "    'Command Deploy Driver Pack for Latitude 5410, 1.0, A01': 'Operating System',\n",
    "    'CoreChip driver install': 'Operating System',\n",
    "    'CortexTools3': 'Development',\n",
    "    'Dell Optimizer Application, 4.1.353.0, A00': 'IT Services',\n",
    "    'DiskGenius': 'Data Recovery',\n",
    "    'DisplayLink Core Software v10.1.2875.0': 'Utilities & Tools',\n",
    "    'Install Notion Calendar': 'Calendar & Scheduling',\n",
    "    'Insyde H2OFFT': 'Operating System',\n",
    "    'Intel BE202/BE200/AX411/AX211/AX210/AX201/AX200/9560/9462/9260 Wi-Fi UWD Driver, 23.60.1.2, A57': 'Networking',\n",
    "    'M-TAG': 'IT Services',\n",
    "    'MAMP & MAMP PRO 4.2.0': 'Local Development',\n",
    "    'MDSuDS': 'Engineering',\n",
    "    'VisualSFM': 'Creative & Design',\n",
    "    'WPS Office': 'Documents & Collaboration',\n",
    "    'Waves MaxxAudio Pro Application, 1.1.131.0, A06': 'Audio & Video',\n",
    "    'Cinnamon-display-changes-dialog': 'Operating System',\n",
    "    'Cirrus Logic High Definition Audio Driver, 10.0.6.18, A08': 'Audio & Video',\n",
    "    'Cisco Secure Endpoint': 'Cybersecurity',\n",
    "    'Dell Firmware Update': 'Operating System',\n",
    "    'Dell On-Screen Display Application, 1.0.1.0, A00': 'Operating System',\n",
    "    'Dell SupportAssist OS Recovery Plugin for Dell Update': 'Operating System',\n",
    "    'FontDownloadConfirmationTest': 'Printing & Publishing',\n",
    "    'Foxit PDF Editor Printer: Reliable, Affordable, Efficient': 'Document Management',\n",
    "    'FurMark2 x64': 'Testing',\n",
    "    'Intel High-Definition (HD) Graphics Driver': 'Operating System',\n",
    "    'Intel Integrated Sensor Solution Driver, 3.10.100.4478, A02': 'Operating System',\n",
    "    'Intel Management Engine Interface Driver, 2345.5.42.0, A04': 'Operating System',\n",
    "    'Intel Serial IO Driver, 30.100.2020.7, A01': 'Operating System',\n",
    "    'Intel UHD/Iris Xe Graphics Driver and Intel Graphics Command Center Application, 31.0.101.5333, A16': 'Operating System',\n",
    "    'Intel UHD/Iris Xe Graphics Driver, 31.0.101.5333, A16': 'Operating System',\n",
    "    'Intel UHD/Iris Xe/Iris Plus Graphics Driver, 31.0.101.5333, A10': 'Operating System',\n",
    "    'Intel Driver & Support Assistant': 'IT Services',\n",
    "    'IoT Remote': 'Remote Access',\n",
    "    'Isoimagewriter': 'Utilities & Tools',\n",
    "    'Java Platform SE 8 U411': 'Development',\n",
    "    'KDI OM3PGP4 PCIe NVMe Solid State Drive Firmware Update, 4130.0004, A00': 'Operating System',\n",
    "    'OS Recovery Tool, 2.4.0.7813, A00': 'Data Recovery',\n",
    "    'OTPKEY.Authenticator': 'Cybersecurity',\n",
    "    'Office': 'Documents & Collaboration',\n",
    "    'Optimizer Application, 4.2.3.0, A00': 'Utilities & Tools',\n",
    "    'PC PRINT': 'Printing & Publishing',\n",
    "    'PCmoverPopup': 'Utilities & Tools',\n",
    "    'Power Manager Service, 3.16.0, A00': 'Utilities & Tools',\n",
    "    'PowerShell 7.4.3.0-x64': 'Development',\n",
    "    'PowerToys.MonacoPreviewHandler': 'Utilities & Tools',\n",
    "    'Razer.Synapse3.Installer': 'Gaming',\n",
    "    'Realtek Card Reader': 'Utilities & Tools',\n",
    "    'Realtek High Definition Audio Driver, 6.0.8934.1, A17': 'Audio & Video',\n",
    "    'Realtek High Definition Audio Driver, 6.0.9517.1, A83': 'Audio & Video',\n",
    "    'Realtek IR Camera Driver, 10.0.15063.20012, A10': 'Utilities & Tools',\n",
    "    'Realtek RTL8821CE/RTL8822CE Wi-Fi and Bluetooth Driver, 2024.10.139.3, A13': 'Networking',\n",
    "    'Realtek RTL8821CE/RTL8822CE Wi-Fi and Bluetooth Driver, 2024.10.139.3, A15': 'Networking',\n",
    "    'Realtek USB Audio DCH Driver, 6.3.9600.2370, A22': 'Audio & Video',\n",
    "    'Reason Safer Web': 'Cybersecurity',\n",
    "    'Samsung PM9A1 Solid State Drive Firmware Update, 3631.0229, A00': 'Operating System',\n",
    "    'Samsung Printer Experience': 'Printing & Publishing',\n",
    "    'ScanSnap Folder': 'Document Management',\n",
    "    'Thunking WIA APIS from 32 to 64 Process': 'Utilities & Tools',\n",
    "    'TicketingTray': 'Customer Support',\n",
    "    'Touchpad Firmware Update Utility, 1160.4171.51, A02': 'Operating System',\n",
    "    'ReSharper Visual Studio Marketplace Installer': 'Development',\n",
    "    'Realtek High Definition Audio Driver, 6.0.9147.1, A19': 'Audio & Video',\n",
    "    'Realtek High Definition Audio Driver, 6.0.9175.1, A27': 'Audio & Video',\n",
    "    'TeamSpeak 3 Client Error Reporter': 'Communication',\n",
    "    'TechPowerUp GPU-Z': 'Analytics',\n",
    "    'Tekla Portal Frame Designer and Tekla Connection Designer 24': 'Engineering'\n",
    "}\n",
    "\n",
    "# Concatenate dictionaries\n",
    "combined_dict = {**res1, **res2}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(list(combined_dict.items()), columns=['app', 'app_mapings'])\n",
    "\n",
    "len(df.app_mapings.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_apps.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_apps with df\n",
    "df_apps = df_apps.merge(df[['app', 'app_mapings']], on='app', how='left', suffixes=('', '_new'))\n",
    "\n",
    "# Check if 'app_mapings_new' was created during the merge\n",
    "if 'app_mapings_new' in df_apps.columns:\n",
    "    # Use assignment instead of inplace=True to avoid warnings\n",
    "    df_apps['app_mapping'] = df_apps['app_mapping'].fillna(df_apps['app_mapings_new'])\n",
    "    \n",
    "    # Drop the temporary 'app_mapings_new' column\n",
    "    df_apps.drop(columns=['app_mapings_new'], inplace=True)\n",
    "else:\n",
    "    print(\"Column 'app_mapings_new' was not created. Please check if 'app' values in both dataframes match correctly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Merge `df_apps` and `df` to check mappings, with troubleshooting output\n",
    "merged = df_apps.merge(df[['app', 'app_mapings']], on='app', how='left', suffixes=('_apps', '_df'))\n",
    "\n",
    "# Display columns in merged DataFrame to verify suffixes\n",
    "print(\"Merged DataFrame columns:\", merged.columns)\n",
    "\n",
    "# Confirm if 'app_mapping_apps' and 'app_mapings_df' exist\n",
    "if 'app_mapping' in merged.columns and 'app_mapings' in merged.columns:\n",
    "    # Identify inconsistent mappings\n",
    "    inconsistent_apps = merged[merged['app_mapping'] != merged['app_mapings']]['app'].unique()\n",
    "    \n",
    "    if len(inconsistent_apps) > 0:\n",
    "        print(\"Inconsistent app mappings found for apps:\", inconsistent_apps)\n",
    "    else:\n",
    "        print(\"All mappings are consistent between `df_apps` and `df`.\")\n",
    "else:\n",
    "    print(\"Merged DataFrame did not create the expected columns. Please check column names.\")\n",
    "\n",
    "# Step 2: Remove rows in `df_apps` where `app` has NaN values\n",
    "df_apps = df_apps.dropna(subset=['app'])\n",
    "\n",
    "# Step 3: Fill NaN values in `app_mapping` with \"Other\"\n",
    "df_apps['app_mapping'].fillna(\"Other\", inplace=True)\n",
    "\n",
    "# Display the updated df_apps\n",
    "df_apps.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_apps.app_mapping.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"mappings/app_mappings_1st_rounds.csv\"\n",
    "\n",
    "with open(path, 'w') as fp:\n",
    "    df_apps.to_csv(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_apps with df, specifying the different column names for app_mapping\n",
    "df_apps = df_apps.merge(df[['app', 'app_mapings']], on='app', how='left', suffixes=('', '_new'))\n",
    "\n",
    "# Update app_mapping column in df_apps where it is None using values from app_mapings in df\n",
    "df_apps['app_mapping'].fillna(df_apps['app_mapings_new'], inplace=True)\n",
    "\n",
    "# Drop the extra column after filling values\n",
    "df_apps.drop(columns=['app_mapings_new'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df2 with df2_second on 'site' to get matching 'site_mapping' values\n",
    "df2 = df2.merge(df2_second[['site', 'site_mapping']], on='site', how='left', suffixes=('', '_second'))\n",
    "\n",
    "# Update site_mapping in df2 only where it exists in df2_second\n",
    "df2['site_mapping'] = df2['site_mapping'].combine_first(df2['site_mapping_second'])\n",
    "\n",
    "# Drop the extra column from the merge\n",
    "df2.drop(columns=['site_mapping_second'], inplace=True)\n",
    "\n",
    "# Display the result to verify\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparasion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 50 unique sites from df2_second\n",
    "sample_sites = df2_second['site'].drop_duplicates().sample(500, random_state=1)\n",
    "\n",
    "# Filter both df2 and df2_second with the sampled sites\n",
    "df2_sample = df2[df2['site'].isin(sample_sites)]\n",
    "df2_second_sample = df2_second[df2_second['site'].isin(sample_sites)]\n",
    "\n",
    "# Merge the two samples on 'site' to compare 'site_mapping' values side by side\n",
    "comparison_df = df2_sample[['site', 'site_mapping']].merge(\n",
    "    df2_second_sample[['site', 'site_mapping']],\n",
    "    on='site',\n",
    "    suffixes=('_df2', '_df2_second')\n",
    ")\n",
    "\n",
    "# Display the comparison\n",
    "print(comparison_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2.site_mapping.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['site_mapping'] = df2['site_mapping'].fillna(\"Other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './mappings/sites_mappings_1st_round.csv'\n",
    "\n",
    "with open(file_path, 'w') as fp:\n",
    "    df2.to_csv(fp, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_second = df2[df2.site_mapping.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "good_index = []\n",
    "for i in range(len(results)):\n",
    "    category = json.loads(results[i]['response']['body']['choices'][0]['message']['content'])['Category']\n",
    "    if len(category) == 2:\n",
    "        good_index.append(i)\n",
    "        counter += 1\n",
    "print(f\"Total count = {counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading only the first results\n",
    "for res in results[:5]:\n",
    "    # Getting index from task id\n",
    "    index = task_id.split('-')[-1]\n",
    "    result = res['response']['body']['choices'][0]['message']['content']\n",
    "    movie = df.iloc[int(index)]\n",
    "    description = movie['Overview']\n",
    "    title = movie['Series_Title']\n",
    "    print(f\"TITLE: {title}\\nOVERVIEW: {description}\\n\\nRESULT: {result}\")\n",
    "    print(\"\\n\\n----------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = json.loads(results[555]['response']['body']['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results, good_index = [], []\n",
    "for index in range(len(results)):\n",
    "    try:\n",
    "        category = json.loads(results[index]['response']['body']['choices'][0]['message']['content'])\n",
    "        all_results.append(category)\n",
    "        good_index.append(index)\n",
    "    except Exception as e:\n",
    "        print(f\"Bad Example: {task['response']['body']['choices'][0]['message']['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "num_sites = 50\n",
    "\n",
    "df2[\"site_mapping\"] = np.nan\n",
    "\n",
    "# Populate the 'mapping' column\n",
    "for i, idx in enumerate(good_index):\n",
    "    start = idx * num_sites\n",
    "    end = (idx + 1) * num_sites\n",
    "    print(len(all_results[i][\"Category\"]))\n",
    "    df2.loc[start:end-1, \"site_mapping\"] = all_results[i][\"Category\"]\n",
    "\n",
    "# Display the result\n",
    "print(df2.head(300))  # Adjust range as needed to view more rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apps analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "path_file = Path('./mappings/unique_apps.csv')\n",
    "\n",
    "with open(path_file, 'r') as fp:\n",
    "    df_apps = pd.read_csv(fp)\n",
    "\n",
    "df_apps.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_list = \", \".join(df1[\"Category\"])\n",
    "\n",
    "categorize_system_prompt = f'''\n",
    "Your goal is to categorize a list of apps based on their general purpose and usage.\n",
    "You will be provided with a single string containing app names separated by commas.\n",
    "You will output a JSON object containing a single key, \"Category\", with its value as a list of categories.\n",
    "Each category in the list should correspond to a website in the input string, in the same order.\n",
    "\n",
    "It is crucial that every website in the input string is assigned a category in the output list. Do not skip any websites, and ensure the output list has a one-to-one mapping for each website provided.\n",
    "\n",
    "Each input website should be mapped to one of the predefined categories below:\n",
    "{categories_list}\n",
    "\n",
    "The JSON object should have the format:\n",
    "\n",
    "{{\n",
    "    \"Category\": [category_1, category_2, ...] // Array of categories corresponding to each website\n",
    "}}\n",
    "\n",
    "The \"Category\" list should contain a single category for each website in the input string, based on the order provided, and only use categories from the given list.\n",
    "Do not create any new categories outside of those specified.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "path_file = Path('./data/150_users_dataset.csv')\n",
    "\n",
    "with open(path_file, 'r') as fp:\n",
    "    df_apps = pd.read_csv(fp, sep=';')\n",
    "\n",
    "df_apps.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All data preprocessing/mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "path_file = Path('./data/150_users_dataset.csv')\n",
    "\n",
    "with open(path_file, 'r') as fp:\n",
    "    df_apps = pd.read_csv(fp, sep=';')\n",
    "\n",
    "df_apps.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Linked Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleLinked:\n",
    "    def __init__(self, val, next = None, prev = None):\n",
    "        self.val = val\n",
    "        self.next = next\n",
    "        self.prev = prev\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.val)\n",
    "\n",
    "head = tail =DoubleLinked(1)\n",
    "\n",
    "def add_on_the_beggining(val, head, tail):\n",
    "    new_node = DoubleLinked(val, next=head)\n",
    "    head.prev = new_node\n",
    "    return new_node, tail\n",
    "\n",
    "def add_on_the_end(val, head, tail):\n",
    "    new_node = DoubleLinked(val, next=None, prev=tail)\n",
    "    tail.next = new_node\n",
    "    return head, new_node\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st scenario\n",
    "\n",
    "A = [-7, -4, 0, 1, 5, 9]\n",
    "\n",
    "def binary_search(arr, num):\n",
    "    N = len(arr)\n",
    "    L = 0\n",
    "    R = N - 1\n",
    "\n",
    "    while L <= R:\n",
    "\n",
    "        M = L + ((R - L) // 2)\n",
    "        if arr[M] == num:\n",
    "            return True\n",
    "        elif arr[M] > num:\n",
    "            R = M - 1\n",
    "        else:\n",
    "            L = M + 1\n",
    "    return False\n",
    "\n",
    "B = [True, True, True, True, False, False]\n",
    "\n",
    "def binary_search_2(arr, num):\n",
    "    N = len(arr)\n",
    "    L = 0\n",
    "    R = N - 1\n",
    "\n",
    "    while L < R:\n",
    "        M = L + ((R - L) // 2)\n",
    "\n",
    "        if not B[M]:\n",
    "            R = M\n",
    "        else:\n",
    "            L = M + 1\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Trees & Binary Search Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postorder traversal - recursion\n",
    "\n",
    "def post_order(node):\n",
    "    if not node:\n",
    "        return\n",
    "\n",
    "    post_order(node.left)\n",
    "    post_order(node.right)\n",
    "    print(node.val)\n",
    "\n",
    "# Postorder - Stack implementatio\n",
    "def post_order_st(node):\n",
    "    stack = [node]\n",
    "    while stack:\n",
    "        node = stack.pop()\n",
    "\n",
    "        print(node.val)\n",
    "        if node.right: stack.append(node.right)\n",
    "        if node.left: stack.append(node.left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "# BFS\n",
    "def bfs(node):\n",
    "    queue = deque()\n",
    "    queue.append(node)\n",
    "    \n",
    "    while queue:\n",
    "        node = queue.popleft()\n",
    "        queue.append(node.left)\n",
    "        queue.append(node.right)\n",
    "        print(node.val)\n",
    "       \n",
    "# Check if a value exist ina  tree (DFS)\n",
    "def search(node, target):\n",
    "    if not node:\n",
    "        return\n",
    "    if node.val == target:\n",
    "        return True\n",
    "    return search(node.left) or search(node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_env_ins",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
