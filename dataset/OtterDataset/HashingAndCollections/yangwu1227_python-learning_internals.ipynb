{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash and Equality\n",
    "\n",
    "The `hash()` built-in function works directly with built-in types and falls back to the `__hash__` method for user-defined classes. \n",
    "\n",
    "> When correctly implemented, hashing guarantees that different hash codes always imply different objects, but the **reverse is not true**: different objects don’t always have different hash codes. When different objects have the same hash code, that’s a hash collision.\n",
    "\n",
    "To be effective hash table indices, hash values should be evenly distributed across the range of integers. This is known as the uniform hashing assumption:\n",
    "\n",
    "> The hash functions that uniformly and independently distribute keys among the integer values between 0 and $M–1$. And $M$ is the size of the hash table (chosen to be prime). (Algorithms Sedgewick and Wayne)\n",
    "\n",
    "Universal hashing has a certain mathematical property that guarantees a low number of collisions in expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12391"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "195438781095727862"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Integer\n",
    "hash(12391)\n",
    "\n",
    "# Unique case: does not hash to itself\n",
    "hash(-1)\n",
    "\n",
    "hash(-3)\n",
    "\n",
    "# Long\n",
    "hash(12345678910111213141516)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "788598309151084556"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "535699010314073601"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash(12.342)\n",
    "\n",
    "hash(1.232322412312341232)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strings, Bytes, and DateTimes\n",
    "\n",
    "Starting with Python 3.3, a random salt value is included when computing hash codes for `str`, `bytes`, and `datetime` objects, which is constant within a Python process but varies between interpreter runs.\n",
    "\n",
    "```python\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def hash_it(x):\n",
    "    # Get the current time to log when the hash is calculated\n",
    "    current_time = datetime.now().strftime(\"%H:%M:%S.%f\")\n",
    "    # Retrieve the PYTHONHASHSEED environment variable if set\n",
    "    hash_seed = os.environ.get('PYTHONHASHSEED', 'not set')\n",
    "    # Return the hash along with the current time and seed information\n",
    "    return (hash(x), current_time, hash_seed)\n",
    "\n",
    "def main():\n",
    "    now_datetime = datetime.now()\n",
    "    string = \"Yang Wu\"\n",
    "    bytes_string = b\"Yang Wu\"\n",
    "    \n",
    "    for item in [now_datetime, string, bytes_string]:\n",
    "        print(f\"Hash for the same item {item} multiple times:\")\n",
    "        print({key: hash_it(item) for key in range(3)})\n",
    "        \n",
    "    return 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash for the same item 2024-05-12 18:08:04.580142 multiple times:\n",
      "{0: (-2640074976138239007, '18:08:04.580334', 'not set'), 1: (-2640074976138239007, '18:08:04.580428', 'not set'), 2: (-2640074976138239007, '18:08:04.580452', 'not set')}\n",
      "Hash for the same item Yang Wu multiple times:\n",
      "{0: (-3333417715881684127, '18:08:04.580524', 'not set'), 1: (-3333417715881684127, '18:08:04.580549', 'not set'), 2: (-3333417715881684127, '18:08:04.580566', 'not set')}\n",
      "Hash for the same item b'Yang Wu' multiple times:\n",
      "{0: (-3333417715881684127, '18:08:04.580617', 'not set'), 1: (-3333417715881684127, '18:08:04.580637', 'not set'), 2: (-3333417715881684127, '18:08:04.580656', 'not set')}\n"
     ]
    }
   ],
   "source": [
    "!python3 ../scripts/random_salt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash for the same item 2024-05-12 18:08:04.795107 multiple times:\n",
      "{0: (-4665693135182612666, '18:08:04.795189', 'not set'), 1: (-4665693135182612666, '18:08:04.795296', 'not set'), 2: (-4665693135182612666, '18:08:04.795323', 'not set')}\n",
      "Hash for the same item Yang Wu multiple times:\n",
      "{0: (-8504513589875496229, '18:08:04.795453', 'not set'), 1: (-8504513589875496229, '18:08:04.795504', 'not set'), 2: (-8504513589875496229, '18:08:04.795528', 'not set')}\n",
      "Hash for the same item b'Yang Wu' multiple times:\n",
      "{0: (-8504513589875496229, '18:08:04.795651', 'not set'), 1: (-8504513589875496229, '18:08:04.795706', 'not set'), 2: (-8504513589875496229, '18:08:04.795734', 'not set')}\n"
     ]
    }
   ],
   "source": [
    "!python3 ../scripts/random_salt.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time we run the script above, the hash values for the same `str`, `bytes`, and `datetime` objects will be different. But the hash values without each run will be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set\n",
    "\n",
    "<center>\n",
    "<img src=\"../images/set_flowchart.png\" width=\"690\" height=\"380\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Initialize the Hash Table\n",
    "\n",
    "The hash table for a `set` starts with 8 empty buckets. As elements are added, Python makes sure at least $\\frac{1}{3}$ of the buckets are empty, but resizes the hash table by doubling or quadrupling when more space is needed.\n",
    "\n",
    "### Step 1: Compute the Hash Code of the Element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-517213032146247581\n",
      "-6862824312894268680\n",
      "2067417196853314201\n",
      "-9051317402501595766\n",
      "-4608781622546238704\n"
     ]
    }
   ],
   "source": [
    "days = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"]\n",
    "\n",
    "for day in days:\n",
    "    print(hash(day))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: From this step forward we will use a fixed set of hash codes for `Mon`, `Tue`, `Wed`, `Thu`, and `Fri` to illustrate the probing process. But in reality, the hash codes are different each time a Python process is started.** \n",
    "\n",
    "The hash codes for the days of the week are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_hash_codes = {\n",
    "    \"Mon\": 4199492796428269555,\n",
    "    \"Tue\": 2414279730484651250,\n",
    "    \"Wed\": -5145319347887138165,\n",
    "    \"Thu\": -1139383146578602409,\n",
    "    \"Fri\": 7021641685991143771,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Probe Hash Table at Index Derived from Hash Code\n",
    "\n",
    "Python takes the modulus of the hash code with the table size to find a hash table index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index for Mon is 3\n",
      "Index for Tue is 2\n",
      "Index for Wed is 3\n",
      "Index for Thu is 7\n",
      "Index for Fri is 3\n"
     ]
    }
   ],
   "source": [
    "for day in fixed_hash_codes:\n",
    "    print(f\"Index for {day} is {fixed_hash_codes[day] % 8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probing consists of computing the index from the hash, then looking at the corresponding bucket in the hash table.\n",
    "\n",
    "### Step 3: Place the Element in the Empty Bucket\n",
    "\n",
    "#### Monday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hash code for 'Mon' is 4199492796428269555\n",
      "The offset is 3\n"
     ]
    }
   ],
   "source": [
    "print(\"The hash code for 'Mon' is\", fixed_hash_codes[\"Mon\"])\n",
    "\n",
    "print(\"The offset is\", fixed_hash_codes[\"Mon\"] % 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python stores the hash code of the new element, `4199492796428269555`, in the hash code field at offset `3`, and a pointer to the string object `Mon` in the element field.\n",
    "\n",
    "<center>\n",
    "<img src=\"../images/monday_set.png\" width=\"480\" height=\"380\">\n",
    "</center>\n",
    "\n",
    "#### Tuesday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hash code for 'Tue' is 2414279730484651250\n",
      "The offset is 2\n"
     ]
    }
   ],
   "source": [
    "print(\"The hash code for 'Tue' is\", fixed_hash_codes[\"Tue\"])\n",
    "\n",
    "print(\"The offset is\", fixed_hash_codes[\"Tue\"] % 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hash code and the pointer to element `Tue` are placed in the bucket at offset `2`, which is also empty:\n",
    "\n",
    "<center>\n",
    "<img src=\"../images/tuesday_set.png\" width=\"480\" height=\"380\">\n",
    "</center>\n",
    "\n",
    "Notice the order insert is not reflected in the order of the elements in the hash table.\n",
    "\n",
    "#### Wednesday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hash code for 'Wed' is -5145319347887138165\n",
      "The offset is 3\n"
     ]
    }
   ],
   "source": [
    "print(\"The hash code for 'Wed' is\", fixed_hash_codes[\"Wed\"])\n",
    "\n",
    "print(\"The offset is\", fixed_hash_codes[\"Wed\"] % 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When adding `Wed` to the set, Python \n",
    "\n",
    "1. Computes the hash `-5145319347887138165` and derives the index `3` from `hash % 8`\n",
    "\n",
    "2. Probes bucket `3` and sees that it is already taken but the hash code stored there, `4199492796428269555` is different\n",
    "\n",
    "3. Probes the next bucket and finds it empty, so `Wed` ends up at index `4`\n",
    "\n",
    "This is called an index collision.\n",
    "\n",
    "<center>\n",
    "<img src=\"../images/wednesday_set.png\" width=\"480\" height=\"380\">\n",
    "</center>\n",
    "\n",
    "#### Thursday and Friday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hash code for 'Thu' is -1139383146578602409\n",
      "The offset is 7\n",
      "The hash code for 'Fri' is 7021641685991143771\n",
      "The offset is 3\n"
     ]
    }
   ],
   "source": [
    "print(\"The hash code for 'Thu' is\", fixed_hash_codes[\"Thu\"])\n",
    "\n",
    "print(\"The offset is\", fixed_hash_codes[\"Thu\"] % 8)\n",
    "\n",
    "print(\"The hash code for 'Fri' is\", fixed_hash_codes[\"Fri\"])\n",
    "\n",
    "print(\"The offset is\", fixed_hash_codes[\"Fri\"] % 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the next element, `Thu` results in no collision, and it lands in its natural bucket, at index `7`.\n",
    "\n",
    "For `Fri`, Python:\n",
    "\n",
    "1. Probes the hash table starting at index `3`, which is taken by `Mon`, and sees that the hash codes for `Fri` and `Mon` don't match (Index Collision)\n",
    "\n",
    "2. Probes the next bucket at index `4`, which is taken by `Wed`, and sees that the hash codes for `Fri` and `Wed` don't match (Index Collision)\n",
    "\n",
    "3. Probes the next bucket at index `5`, which has `-1` in the hash code field, marking an empty bucket, so `Fri` is placed there\n",
    "\n",
    "<center>\n",
    "<img src=\"../images/thursday_friday_set.png\" width=\"480\" height=\"380\">\n",
    "</center>\n",
    "\n",
    "### Resizing the Hash Table\n",
    "\n",
    "The hash table for the set `{Mon, Tue, Wed, Thu, Fri}` has 8 buckets, but only 5 are used ($\\frac{5}{8} = 0.625$). When the capacity exceeds $\\frac{2}{3} \\approx 0.667$, Python resizes the hash table. Based\n",
    "on the following [line](https://github.com/python/cpython/blob/main/Objects/setobject.c#L199C1-L199C75) in the CPython source code, if the set needs resizing, the resizing behavior depends on the current size of the set:\n",
    "\n",
    "* If the used count is greater than 50,000, the set size is doubled (`so->used*2`).\n",
    "\n",
    "* If the used count is 50,000 or less, the set size is quadrupled (`so->used*4`).\n",
    "\n",
    "### Hash Collision\n",
    "\n",
    "It is possible that **two different objects** have the **same hash code**. This is known as a **hash collision**.\n",
    "\n",
    "This explains why hashable objects must implement both `__hash__` and `__eq__`.\n",
    "\n",
    "#### Insertion \n",
    "\n",
    "If the hash code of the new element to be inserted matches the hash code of an existing element, Python compares the new element with the existing element using the `__eq__` method:\n",
    "\n",
    "* If the element values are equal, Python does not add the new element to the set. \n",
    "\n",
    "* If the elements values are not equal, Python continues probing the hash table until it finds an empty bucket.\n",
    "\n",
    "#### Lookup\n",
    "\n",
    "When a lookup is requested `element in set` and the hash code of `element` collides with the hash codes of other elements, Python again needs to compare the element's value using the `__eq__` method:\n",
    "\n",
    "* If the equality check returns true, the element is found\n",
    "\n",
    "* If the equality check returns false, Python continues probing the hash table until it finds the element or exhausts all buckets, which means the element is not in the set.\n",
    "\n",
    "Evidently, hash collision results in additional probing for both insertion and lookup operations, degrading the performance of the hash table.\n",
    "\n",
    "### Practical Consequences\n",
    "\n",
    "1. Elements must implement `__hash__` and `__eq__` methods.\n",
    "\n",
    "2. The bucket for an element can be located directly by computing the hash code of the element and deriving an index offset, with the possible overhead of a small number of probes to find a matching element or an empty bucket.\n",
    "\n",
    "3. The most compact internal data structure for a container would be an array of pointers. Hash tables use more memory due to additional hash codes and maintaining at least $\\frac{1}{3}$ of the buckets empty to minimize collisions.\n",
    "\n",
    "4. The order of elements in a set is influenced by the insertion sequence but is neither reliable nor predictable.\n",
    "\n",
    "5. Adding elements may cause a rehash to maintain bucket availability, altering the order of existing elements so different index collisions may occur.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Probing\n",
    "\n",
    "Python uses linear probing to resolve collisions. This is used for both index collisions and hash collisions for both sets and dictionaries.\n",
    "\n",
    "> Linear probing is a simple collision resolution strategy that consists of creating **new** indices to probe in the hash table until an either an empty bucket is found or data to be searched is found.\n",
    "\n",
    "We need a better strategy than simply looking at the next bucket since that would lead to clusters of occupied buckets. That is, if each time we find a collision, we simply probe the next bucket, we would end up with a lot of contiguous filled buckets. Then, the next time we need to probe, we would have to go through all of them to find the next empty bucket or the data we are looking for.\n",
    "\n",
    "## Psuedo code for A Modified Linear Probing\n",
    "\n",
    "```python\n",
    "def index_sequence(key, mask=0b111, PERTURB_SHIFT=5): \n",
    "    perturb = hash(key)\n",
    "    i = perturb & mask\n",
    "    yield i\n",
    "    while True:\n",
    "        perturb >>= PERTURB_SHIFT\n",
    "        i = (i * 5 + perturb + 1) & mask yield i\n",
    "```\n",
    "\n",
    "#### Function Definition\n",
    "\n",
    "- `def index_sequence(key, mask=0b111, PERTURB_SHIFT=5)`: \n",
    "  - `key`: The input for which the hash index is calculated.\n",
    "  - `mask=0b111`: Default value is 7 (in binary, `0b111`). This is used to limit the range of the indices. This can be set to a different value to control the number of buckets.\n",
    "  - `PERTURB_SHIFT=5`: Default shift value used in the perturbation process. This default comes from the properties of a linear congruential generator (LCG), which is used in generating random numbers.\n",
    "\n",
    "#### Initialization\n",
    "\n",
    "- `perturb = hash(key)`: Computes the hash of the `key`, storing it in `perturb`.\n",
    "- `i = perturb & mask`: Applies a bitwise AND between `perturb` and `mask` to get the initial index. This operation ensures that the index stays within the bounds of the hash table defined by `mask`.\n",
    "\n",
    "#### Yield the First Index\n",
    "\n",
    "- `yield i`: The first computed index is yielded back to the caller. This is the initial index in the hash table to be probed.\n",
    "\n",
    "#### Infinite Loop for Index Generation\n",
    "\n",
    "- `while True`: Starts an infinite loop to generate further indices if needed.\n",
    "  - `perturb >>= PERTURB_SHIFT`: Right-shifts the `perturb` value by `PERTURB_SHIFT` positions. This operation gradually reduces `perturb`, altering its value for subsequent index calculations.\n",
    "  - `i = (i * 5 + perturb + 1) & mask`: Calculates the next index using the previous index `i`, modified `perturb`, and an additive constant `1`. The entire result is again constrained by `mask`.\n",
    "  - `yield i`: Yields the next index in the sequence back to the caller.\n",
    "\n",
    "This function can generate a sequence of indices for placing or finding an item in a hash table, particularly useful when handling collisions using open addressing. The use of both perturbation and the `mask` helps in distributing the indices more uniformly and ensuring they stay within a certain range.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dict \n",
    "\n",
    "Consider the following dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Mon': 14, 'Tue': 12, 'Wed': 14, 'Thu': 11}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swimmers = {\"Mon\": 14, \"Tue\": 12, \"Wed\": 14, \"Thu\": 11}\n",
    "\n",
    "swimmers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Dict Hash Table\n",
    "\n",
    "Prior to the introduction of compact Dict in Python 3.6, the hash table looks as follows:\n",
    "\n",
    "<center>\n",
    "<img src=\"../images/old_dict_hash_table.png\" width=\"540\" height=\"380\">\n",
    "</center>\n",
    "\n",
    "In a 64-bit Python, each bucket holds three 64-bit fields:\n",
    "\n",
    "1. The hash code of the key\n",
    "2. A pointer to the key object\n",
    "3. A pointer to the value object\n",
    "\n",
    "That is 24 bytes or 192 (1 byte = 8 bit) bits per bucket. Each bucket is a struct with the hash code of the key, a pointer to the key, and a pointer to the value.\n",
    "\n",
    "### Compact Dict\n",
    "\n",
    "As proposed by Raymon Hettinger [here](https://mail.python.org/pipermail/python-dev/2012-December/123028.html), an optimization can be made such that\n",
    "\n",
    "1. The hash code and pointers to key and value were held in an `entries` array with no empty rows\n",
    "2. The actual hash table were a sparse array with much smaller buckets, holding indexes into the `entries` array\n",
    "\n",
    "<center>\n",
    "<img src=\"../images/compact_dict_hash_table.png\" width=\"640\" height=\"280\">\n",
    "</center>\n",
    "\n",
    "In the diagram above:\n",
    "\n",
    "* Hash codes and pointers to keys and values are stored in **insertion order** in the `entries` array\n",
    "* The entry offsets derived from the hash codes are held in the `indices` sparse array with negative values for special purposes, e.g. -1 for empty and -2 for deleted\n",
    "\n",
    "Assuming a 64-bit build of CPython, the 4-item `swimmers` dictionary would take:\n",
    "\n",
    "* 192 bytes of memory in the old scheme: 24 bytes per bucket, times 8 rows \n",
    "* 104 bytes of memory using the compact scheme: 96 bytes in `entries` (`24 * 4`), plus 8 bytes for the buckets in `indices` configured as an array of `8` bytes (the default `indices` table is initially set up with 8 buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Set up `indices` \n",
    "\n",
    "* The `indices` table is initially set up as an array of signed bytes, with 8 buckets, each initialized with `-1` to signal \"empty bucket\". Up to 5 of these buckets will eventually hold indices to rows in the `entries` array, leaving $\\frac{1}{3}$ of the buckets empty before resizing.\n",
    "\n",
    "* The `entries` array hods hash code and pointers for keys and values in **insertion order*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -1, -1, -1, -1, -1, -1, -1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = [-1] * 8\n",
    "\n",
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Compute the Hash Code of the Key\n",
    "\n",
    "To add the key-value pair `('Mon', 14)` to the swimmers dictionary, Python first calls `hash('Mon')` to compute the hash code of that key.\n",
    "\n",
    "### Step 2: Probe `entries` using `indices`\n",
    "\n",
    "Python computes `hash('Mon') % len(indices)`. Offset `3` in indices holds `-1`: it’s an empty bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The derived index for 'Mon' is 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"The derived index for 'Mon' is {fixed_hash_codes['Mon'] % len(indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Place the Key-Value Pair in the `entries` Array, Update `indices`\n",
    "\n",
    "#### Monday\n",
    "\n",
    "The `entries` array is empty, so the next available offset is `0`. Python\n",
    "\n",
    "1. Places the index `0` at offset `3` in `indices`\n",
    "2. Stores the hash code of `Mon`, the pointer to the key `'Mon'`, and the pointer to the value `14` in the `entries` array at offset `0` in the `entries` array\n",
    "\n",
    "<center>\n",
    "<img src=\"../images/monday_dict.png\" width=\"660\" height=\"300\">\n",
    "</center>\n",
    "\n",
    "So `indices[3]` holds the offset of the first entry `entries[0]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -1, -1, 0, -1, -1, -1, -1]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[fixed_hash_codes[\"Mon\"] % len(indices)] = 0\n",
    "\n",
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuesday\n",
    "\n",
    "To add `(Tue, 12)` to the dictionary, Python\n",
    "\n",
    "1. Computes the hash code of `Tue`\n",
    "\n",
    "2. Compute the offset into `indices`, e.g. `hash('Tue') % len(indices) = 2`\n",
    "\n",
    "3. Checks that `indices[2]` is empty with value `-1`, so no collision\n",
    "\n",
    "4. Places the next available `entries` offset `1` at `indices[2]`\n",
    "\n",
    "5. Stores the hash code of `Tue`, the pointer to the key `'Tue'`, and the pointer to the value `12` in the `entries` array at offset `1`\n",
    "\n",
    "<center>\n",
    "<img src=\"../images/tuesday_dict.png\" width=\"660\" height=\"300\">\n",
    "</center>\n",
    "\n",
    "So `indices[2]` holds the offset of the second entry `entries[1]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -1, 1, 0, -1, -1, -1, -1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[fixed_hash_codes[\"Tue\"] % len(indices)] = 1\n",
    "\n",
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wednesday\n",
    "\n",
    "Adding `(Wed, 14)` to the dictionary, Python\n",
    "\n",
    "1. Computes the hash code of `Wed`\n",
    "\n",
    "2. Compute the offset into `indices`, e.g. `hash('Wed') % len(indices) = 3`\n",
    "\n",
    "3. Checks that `indices[3]`, which has `0`, pointing to an existing entry, so there is an index collision\n",
    "\n",
    "   1. Compares the hash code of `Wed` with the hash code of `Mon` at `entries[0]`, which don't match\n",
    "\n",
    "   2. Probes the next bucket in `indices` at offset `4`, which is empty with value `-1`\n",
    "\n",
    "4. Places the next available `entries` offset `2` at `indices[4]`\n",
    "\n",
    "5. Stores the hash code of `Wed`, the pointer to the key `'Wed'`, and the pointer to the value `14` in the `entries` array at offset `2`\n",
    "\n",
    "<center>\n",
    "<img src=\"../images/wednesday_dict.png\" width=\"660\" height=\"300\">\n",
    "</center>\n",
    "\n",
    "So `indices[4]` holds the offset of the third entry `entries[2]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -1, 1, 0, 2, -1, -1, -1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[4] = 2\n",
    "\n",
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing\n",
    "\n",
    "Based on this block of [comments](https://github.com/python/cpython/blob/main/Objects/dictobject.c#L603C1-L612C4):\n",
    "\n",
    "1. Up to Python version 3.2, the growth rate was set to `used * 4`, which means the dictionary size was quadrupled upon reaching maximum load\n",
    "\n",
    "2. In Python version 3.3.0, the growth rate was reduced to `used * 2`, effectively doubling the dictionary size\n",
    "\n",
    "3. From Python versions 3.4.0 to 3.6.0, the growth rate was adjusted to `used * 2 + capacity / 2`. This formula implies that the size increase was dependent not only on the number of used entries but also included half of the current capacity, allowing for a more gradual increase in size.\n",
    "\n",
    "4. Currently, i.e. Python $3.11$, the dictionary size increases by $\\times 3$ once the load threshold is exceeded.\n",
    "\n",
    "**Initial Conditions and Growth Trigger:**\n",
    "\n",
    "- The smallest size allocated for a dictionary is 8 elements.\n",
    "\n",
    "- Resizing occurs when the dictionary is more than two-thirds full.\n",
    "\n",
    "**Example of Growth Progression:**\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| Insertion Event | Dictionary Size Before Insert | Dictionary Size After Insert |\n",
    "|-----------------|-------------------------------|------------------------------|\n",
    "| 6th item        | 8                             | 18                           |\n",
    "| 13th item       | 18                            | 39                           |\n",
    "| ...             | 39                            | 81                           |\n",
    "| ...             | 81                            | 165                          |\n",
    "| ...             | 165                           | 333                          |\n",
    "| ...             | 333                           | 669                          |\n",
    "| ...             | 669                           | 1,341                        |\n",
    "| ...             | 1,341                         | 2,685                        |\n",
    "| ...             | 2,685                         | 5,373                        |\n",
    "| ...             | 5,373                         | 10,749                       |\n",
    "| ...             | 10,749                        | 21,501                       |\n",
    "| ...             | 21,501                        | 43,005                       |\n",
    "| And so on       | ...                           | ...                          |\n",
    "\n",
    "</div>\n",
    "\n",
    "### Hash Collision\n",
    "\n",
    "Obviously, hash collision is not ideal since the additional probing degrades the performance of the dictionary for both insertion and lookup operations.\n",
    "\n",
    "Python dictionaries store both the hash code and the pointer to the actual key object, so even if multiple keys have the same hash, Python can distinguish between them by directly comparing the keys.\n",
    "\n",
    "#### Insertion\n",
    "\n",
    "When adding a key-value pair to a dictionary and a hash collision occurs:\n",
    "\n",
    "* Python will derive the index from the hash code for the new key and probe the `indices` array to find that there is an existing offset at that index (i.e. not `-1`).\n",
    "\n",
    "* Python will compare the actual key objects to determine if the key to be inserted and the existing key are the same. If the keys are the same, there is no insertion to do since the key-value pair is already in the dictionary. The key will be updated with the new value.\n",
    "\n",
    "* If the keys don't match, Python will continue probing the sparse `indices` array until it finds an empty bucket `i` marked by `-1`; it will insert the next available offset into `entries` at `indices[i]` and store the hash code and pointers to the key and value in the `entries` array at `offset`.\n",
    "\n",
    "#### Lookup\n",
    "\n",
    "If the keys `Mon` and `Wed` have a hash collision and a lookup for `Wed` is requested, simply computing the index from the hash code will not suffice to disambiguate the keys.\n",
    "\n",
    "Python compares the requested key object to the existing key object whose hash codes collided to find the correct key-value pair for `Wed`. This additional probing is necessary to handle hash collisions, but it degrades the performance of the dictionary.\n",
    "\n",
    "### Key Sharing Dictionary\n",
    "\n",
    "Instances of user-defined classes hold their attributes in a `__dict__` attribute, which is a regular dictionary. An instance `__dict__` maps attribute names to attribute values. Most of the time, all instances have the same attributes with different values.\n",
    "\n",
    "One [optimization](https://peps.python.org/pep-0412/) proposed by Mark Shannon:\n",
    "\n",
    "1. Store each attribute hash code and pointer only once and link it to the class\n",
    "2. Store attribute values in parallel arrays of pointers attached to each instance\n",
    "\n",
    "This optimization is known as a **key-sharing dictionary**. \n",
    "\n",
    "Below is an example of split storage for the `__dict__` of a class and three instances of that class:\n",
    "\n",
    "<center>\n",
    "<img src=\"../images/key_sharing_dict.png\" width=\"660\" height=\"300\">\n",
    "</center>\n",
    "\n",
    "### Practical Consequences\n",
    "\n",
    "1. Keys must implement `__hash__` and `__eq__` methods. But values do not have to be hashable.\n",
    "\n",
    "2. Item ordering is preserved in the `entries` table, which is an implementation detail since 3.7.\n",
    "\n",
    "3. For key sharing dictionary:\n",
    "\n",
    "   - **Default dictionary layout**: combined-table is still used when creating dictionaries using `dict()`.\n",
    "\n",
    "   - **Split-table dictionary**: This is used for the `__dict__` of a class's first instance. The keys table is then cached in the class after the creation of the first instance.\n",
    "\n",
    "     - Subsequent instances only store their values, sharing the `keys` table.\n",
    "\n",
    "     - If an instance gets a new attribute not found in the shared keys table, then this instance’s `__dict__` is converted back to combined-table form.\n",
    "  \n",
    "     - If the instance is the only one in its class, `__dict__` reverts to split-table for potential future instance attribute sharing even if a new attribute is added.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Mask \n",
    "\n",
    "The mask is used to limit the range of the indices. To find the mask for a dictionary with an arbitrary number of elements, $N$:\n",
    "\n",
    "1. Find the minimum number of buckets that the dictionary must have to still be $\\frac{2}{3}$ full:\n",
    "\n",
    "    \\begin{align*}\n",
    "    N \\times (\\frac{2}{3} + 1)\n",
    "    \\end{align*}\n",
    "\n",
    "2. Find the smallest dictionary size that will hold this number of elements: (8; 18; 39; 81; 165; 333; 669; 1,341; 2,685;)\n",
    "\n",
    "3. Find the number of bits necessary to hold this number using `bin(dict_size - 1)`\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum number of buckets for 1039 elements is 1731.6666666666665\n"
     ]
    }
   ],
   "source": [
    "n = 1039\n",
    "\n",
    "min_buckets = n * (2 / 3 + 1)\n",
    "\n",
    "print(f\"The minimum number of buckets for {n} elements is {min_buckets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0b101001111100'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_size = 2685\n",
    "\n",
    "bin(dict_size - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Derive the offset for 'Yang Wu'\n",
    "hash(\"Yang Wu\") & 0b101001111100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
