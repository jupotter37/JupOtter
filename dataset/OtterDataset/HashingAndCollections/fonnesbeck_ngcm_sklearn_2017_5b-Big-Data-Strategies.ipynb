{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, you'll want to take advantage of more computing power or you may have data that either has too many features or too many samples (or too many of both) to fit in memory.\n",
    "\n",
    "Scikit-learn provides a few options for these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the scikit-learn estimators provide a `fit_partial` method. You can read samples in to memory one-at-a-time or in batches. Some pseudo-code for this may look something like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "estimator = EstimatorPipeline()\n",
    "\n",
    "with open('data') as file_handle:\n",
    "    chunk = file_handle.read(N_LINES)\n",
    "    X, y = create_matrices(chunk)\n",
    "    estimator.fit_partial(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the bag of words representation for text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "\n",
    "SVG(\"images/bag_of_words.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that this requires having a dictionary of *all* of the words that you may encounter. When the set of features is not known in advance, for example, like when working with text data, you might try to extract them by making a full pass over the data. If this isn't feasible, you might try turning to the `hashing trick`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(\"images/hashing_vectorizer.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Hash Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hash function maps from an arbitrary size to a fixed size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(\"https://upload.wikimedia.org/wikipedia/commons/7/71/Hash_table_4_1_1_0_0_0_0_LL.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has a built-in function for this called `hash`. (You will **not** see the same numbers. Hashing in recent versions of Python is randomized for security.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash('cuckoo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash('cuckoo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn provides some performant hashing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.murmurhash import murmurhash3_bytes_u32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in \"cuckoo goes the cuckoo clock\".encode(\"utf-8\").split():\n",
    "    word_hash = murmurhash3_bytes_u32(word, 0) % 2 ** 20\n",
    "    print(f\"{word} {word_hash}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a mapping from words to an index that is stateless and the dimensionality of the output space is fixed in advance. `2 ** 20` here means roughly 1M features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, wait, if you're mapping from a potentially unbounded domain to a fixed range won't you have *collisions*. In principle, yes. In practice, rarely. YMMV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Hash Tables\n",
    "\n",
    "Hash tables are a ubiquitous data structure when you start looking for them. The most ready example is a dictionary (or a set) in Python. This mapping data structure, formally an associative array, is often called a hash in other languages. The great thing about hash tables is that lookup and insertion is (almost always) $\\mathcal{O}(1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(\"https://upload.wikimedia.org/wikipedia/commons/7/7d/Hash_table_3_1_1_0_1_0_0_SP.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {\n",
    "    ['mutable', 'object']: 123\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we take advantage of hash tables for machine learning? Well, for one thing, you've been taking advantage of hash tables all along. This is precisely what the feature vectorizers we have been using are doing. Scikit-learn also provides a few, convenient objects for taking advange of hashing.\n",
    "\n",
    "`FeatureHasher` applies a function to find the column index of a feature directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "hasher = FeatureHasher(input_type='string')\n",
    "\n",
    "hasher.transform([\n",
    "    ['a', 'simple', 'man'],\n",
    "    ['pulp', 'fiction'],\n",
    "    ['pride', '&', 'prejudice']\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HashingVectorizer` is particularly convenient for working with documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "hash_vect = HashingVectorizer(n_features=12, stop_words=['a', 'the'])\n",
    "\n",
    "hash_vect.transform([\n",
    "    'The Lion King (1994)',\n",
    "    'Pulp Fiction (1994)',\n",
    "    'A Simple Man (2013)',\n",
    "    'Pride & Prejudice (2005))'\n",
    "]).A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_vect.transform(['(1994)']).A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-core or Incremental Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, there are a number of estimators (but not all) that can be fit without holding all of the data in memory. Any estimator that implements `partial_fit`. Choosing a good size for the mini-batch that balances relevancy and memory footprint could involve some tuning.\n",
    "\n",
    "Here is a list of incremental estimators for different tasks:\n",
    "\n",
    "**Classification**\n",
    "\n",
    "* sklearn.naive_bayes.MultinomialNB\n",
    "* sklearn.naive_bayes.BernoulliNB\n",
    "* sklearn.linear_model.Perceptron\n",
    "* sklearn.linear_model.SGDClassifier\n",
    "* sklearn.linear_model.PassiveAggressiveClassifier\n",
    "\n",
    "**Regression**\n",
    "\n",
    "* sklearn.linear_model.SGDRegressor\n",
    "* sklearn.linear_model.PassiveAggressiveRegressor\n",
    "\n",
    "**Clustering**\n",
    "\n",
    "* sklearn.cluster.MiniBatchKMeans\n",
    "\n",
    "**Decomposition / feature Extraction**\n",
    "\n",
    "* sklearn.decomposition.MiniBatchDictionaryLearning\n",
    "* sklearn.decomposition.IncrementalPCA\n",
    "* sklearn.decomposition.LatentDirichletAllocation\n",
    "* sklearn.cluster.MiniBatchKMeans\n",
    "\n",
    "For classification, you need to tell the estimator in advance all of the classes you will be learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following exercise is taken from Andreas Mueller's scikit-learn tutorial for the upcoming SciPy 2017 conference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run fetch_imdb.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDb dataset\n",
    "\n",
    "To illustrate the scalability issues of the vocabulary-based vectorizers, let's load a more realistic dataset for a classical text classification task: sentiment analysis on text documents. The goal is to tell apart negative from positive movie reviews from the Internet Movie Database (IMDb).\n",
    "\n",
    "We're going to work with a large subset of movie reviews from the IMDb that has been collected by Maas et al.\n",
    "\n",
    "* A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. Learning Word Vectors for Sentiment Analysis. In the proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142â€“150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics.\n",
    "\n",
    "This dataset contains 50,000 movie reviews, which were split into 25,000 training samples and 25,000 test samples. The reviews are labeled as either negative (neg) or positive (pos). Moreover, positive means that a movie received >6 stars on IMDb; negative means that a movie received <5 stars, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_path = os.path.join('..', 'data', 'IMDb', 'aclImdb', 'train')\n",
    "train_pos = os.path.join(train_path, 'pos')\n",
    "train_neg = os.path.join(train_path, 'neg')\n",
    "\n",
    "fnames = [os.path.join(train_pos, f) for f in os.listdir(train_pos)] +\\\n",
    "         [os.path.join(train_neg, f) for f in os.listdir(train_neg)]\n",
    "\n",
    "fnames[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 12,500 reviews are positive, so let's create our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.zeros((len(fnames), ), dtype=int)\n",
    "y_train[:12500] = 1\n",
    "np.bincount(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train this model, we'll used the `SGDClassifier` with a logistic cost function. SGD stands for stochastic gradient descent, and it is a workhorse method for out-of-core learning that learns the weight vectors sample by sample.\n",
    "\n",
    "Using the below defaults, we are going to train the classifier on 25,000 random documents (with replacement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "def batch_train(clf, fnames, labels, iterations=25, batchsize=1000, random_seed=1):\n",
    "    \n",
    "    vec = HashingVectorizer(encoding='latin-1')\n",
    "    \n",
    "    # create an index for all the reviews used below\n",
    "    idx = np.arange(labels.shape[0])\n",
    "    \n",
    "    # performs a copy of our estimator\n",
    "    c_clf = clone(clf)\n",
    "    \n",
    "    rng = np.random.RandomState(seed=random_seed)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # choose 1000 random documents to train on\n",
    "        rnd_idx = rng.choice(idx, size=batchsize)\n",
    "        \n",
    "        documents = []\n",
    "        for i in rnd_idx:\n",
    "            with open(fnames[i], 'r') as f:\n",
    "                documents.append(f.read())\n",
    "                \n",
    "        # get the labels for these documents\n",
    "        batch_labels = labels[rnd_idx]\n",
    "                \n",
    "        # use our HashingVectorizer to transform the documents\n",
    "        X_batch = vec.transform(documents)\n",
    "        \n",
    "        c_clf.partial_fit(X=X_batch, \n",
    "                          y=batch_labels, \n",
    "                          classes=[0, 1])\n",
    "      \n",
    "    return c_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = SGDClassifier(loss='log', random_state=1)\n",
    "\n",
    "sgd = batch_train(clf=sgd, \n",
    "                  fnames=fnames, \n",
    "                  labels=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate how well we did on our holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "test_path = os.path.join('..', 'data', 'IMDb', 'aclImdb', 'test')\n",
    "\n",
    "test = load_files(container_path=(test_path),\n",
    "                  categories=['pos', 'neg'])\n",
    "\n",
    "docs_test, y_test = test['data'][12500:], test['target'][12500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = HashingVectorizer(encoding='latin-1')\n",
    "sgd.score(vec.transform(docs_test), y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
