{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Guides\n",
    "[How to Hire a Great Python Developer](https://www.toptal.com/python#hiring-guide)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms & Theory\n",
    "[Define precision and recall.](#5)<br>\n",
    "[What is a Fourier transform?](#11)<br>\n",
    "[Why is “Naive” Bayes naive?](#7)<br>\n",
    "[How is a decision tree pruned?](#16)<br>\n",
    "[Explain how a ROC curve works.](#4)<br>\n",
    "[What is bagging in machine learning?](#52)<br>\n",
    "[What is the F1 score? How would you use it?](#18)<br>\n",
    "[What is the “kernel trick” and how is it useful?](#25)<br>\n",
    "[How would you handle an imbalanced dataset?](#19)<br>\n",
    "[How is KNN different from k-means clustering?](#3)<br>\n",
    "[What is the trade-off between bias and variance?](#1)<br>\n",
    "[When should you use classification over regression?](#20)<br>\n",
    "[How would you evaluate a logistic regression model?](#24)<br>\n",
    "[How do you ensure you’re not overfitting with a model?](#22)<br>\n",
    "[What is the difference between Type I and Type II error?](#10)<br>\n",
    "[Explain the difference between L1 and L2 regularization.](#8)<br>\n",
    "[What is the difference between probability and likelihood?](#12)<br>\n",
    "[What are the disadvantages of a random forest algorithm?](#72)<br>\n",
    "[Name an example where ensemble techniques might be useful.](#21)<br>\n",
    "[For what kind of classification problems is SVM a bad approach?](#63)<br>\n",
    "[What is the difference between a generative and discriminative model?](#14)<br>\n",
    "[What cross-validation technique would you use on a time series dataset?](#15)<br>\n",
    "[What is Bayes’ Theorem? How is it useful in a machine learning context?](#6)<br>\n",
    "[Which is more important to you– model accuracy, or model performance?](#17)<br>\n",
    "[What is the difference between supervised and unsupervised machine learning?](#2)<br>\n",
    "[What is your favorite algorithm, and can you explain it to me in less than a minute?](#9)<br>\n",
    "[What is deep learning and how does it contrast with other machine learning algorithms?](#13)<br>\n",
    "[What evaluation approaches would you work to gauge the effectiveness of a machine learning model?](#23)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Company & Industry\n",
    "[What do you think of our current data process?](#34)<br>\n",
    "[How can we use your machine learning skills to generate revenue?](#33)<br>\n",
    "[How would you implement a recommendation system for our company’s users?](#32)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General\n",
    "[What is \"data leakage\" in data science?](#60)<br>\n",
    "[Where do you usually source datasets?](#39)<br>\n",
    "[What are the last machine learning papers you have read?](#35)<br>\n",
    "[Which fallacies do machine learning engineers fall prey to?](#65)<br>\n",
    "[What are your favorite use cases of machine learning models?](#37)<br>\n",
    "[What does the word \"embedding\" mean in the context of Machine Learning?](#70)<br>\n",
    "[Why is the frequentist approach to machine learning more successful than the Bayesian approach?](#74)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning\n",
    "[How does batch normalization help?](#75)<br>\n",
    "[What is an intuitive explanation of stochastic gradient descent?](#67)<br>\n",
    "[Why do we not initialize the weights of a neural network to zero?](#68)<br>\n",
    "[What are the major critisms against deep learning in the NLP field?](#66)<br>\n",
    "[What is the best method for determining the ideal depth for a neural network?](#59)<br>\n",
    "[What makes the ReLU function more appropriate in some cases over a Sigmoid activation function?](#62)<br>\n",
    "[Why is backpropagation such a great deal and got a special name? Is it not just the natural way to think how to calculate the gradients to perform gradient descent?](#64)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Methods\n",
    "[What is a confidence interval in statistics?](#71)<br>\n",
    "[Logistic Regression: Why sigmoid function?](#55)<br>\n",
    "[What is the meaning of \"central\" in \"Central Limit Theorem\"?](#58)<br>\n",
    "[What is the difference between logit and logistic regression?](#69)<br>\n",
    "[What is the importance of nonparametric modeling in statistics?](#73)<br>\n",
    "[What are control variables and how do I use them in regression analysis?](#57)<br>\n",
    "[What are some general ways to improve multiple linear regression models?](#54)<br>\n",
    "[What is the difference between a parametric model and a non-parametric model?](#76)<br>\n",
    "[How do we know whether we use t-test, ANOVA, chi-square, correlation, or regression analysis?](#53)<br>\n",
    "[A linear regression requires residuals to be normally distributed. Why do we need this assumption? What will happen if this assumption does not satisfy?](#61)<br>\n",
    "[For linear regression, how do we decide whether mean absolute error or mean square error is better? Are there other loss functions that are commonly used?](#56)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming\n",
    "[What Is Python?](#40)<br>\n",
    "[Describe a hash table.](#30)<br>\n",
    "[In Python, How is Memory Managed?](#51)<br>\n",
    "[Which Native Data Structures Can You Name in Python?](#41)<br>\n",
    "[Can You Explain What a List or Dict Comprehension Is?](#45)<br>\n",
    "[What is the Difference Between a List and a Dictionary?](#43)<br>\n",
    "[Is There a Way to Get a List of All the Keys in a Dictionary?](#46)<br>\n",
    "[How do you handle missing or corrupted data in a dataset?](#26)<br>\n",
    "[When Would You Use a List vs. a Tuple vs. a Set in Python?](#47)<br>\n",
    "[What is the Difference between a For Loop and a While Loop?](#48)<br>\n",
    "[What are some differences between a linked list and an array?](#29)<br>\n",
    "[Which Native Data Structures Are Mutable, and Which Are Immutable?](#42)<br>\n",
    "[In a List and in a Dictionary, What Are the Typical Characteristics of Elements?](#44)<br>\n",
    "[Do You Know Any Additional Data Structures Available in the Standard Library?](#50)<br>\n",
    "[Which Packages in the Standard Library - Useful for Data Science Work - Do You Know?](#49)<br>\n",
    "[Which data visualization libraries do you use? What are your thoughts on the best data visualization tools?](#31)<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the difference between a parametric model and a non-parametric model?<a id=\"76\"></a>\n",
    "\n",
    "In a parametric model, you know which model exactly you will fit to the data, e.g., linear regression line. In a non-parametric model, however, the data tells you what the 'regression' should look like."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does batch normalization help?<a id=\"75\"></a>\n",
    "\n",
    "Training very deep neural networks is hard. It turns out one significant issue with deep neural networks is that the activations of each layer tend to converge to 0 in the later layers, and therefore the gradients vanish as they backpropagate throughout the network.\n",
    "\n",
    "\n",
    "A lot of this has to do with the sheer size of the network - obviously as you multiply numbers less than zero together over and over, they’ll converge to zero, and that’s partially why network architectures such as InceptionV3 insert auxiliary classifiers after layers earlier on in their network, so there’s a stronger gradient signal back propagated during the first few epochs of training.\n",
    "\n",
    "\n",
    "However, there’s also a more subtle issue that leads to this problem of vanishing activations and gradients. It has to do with the initialization of the weights in each layer of our network, and the subsequent distributions of the activations in our network. Understanding this issue is key to understanding why batch normalization helps fix this issue.\n",
    "\n",
    "\n",
    "Batch normalization is a way to fix the root cause of our issue of zero activations and vanishing gradients: reducing internal covariate shift. We want to ensure that the variances of our activations do not differ too much from each other.\n",
    "\n",
    "\n",
    "With batch normalization, we can be confident that the distributions of our activations across hidden layers are reasonably similar. If this is true, then we know that the gradients should have a wider distribution, and not be nearly all zero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is the frequentist approach to machine learning more successful than the Bayesian approach?<a id=\"74\"></a>\n",
    "\n",
    "People often refer to the Bayesian approach as methods that take our uncertainty into account. In summary, this is typically more data efficient and generalizes better to unseen data, but also computationally challenging and conceptually more involved.\n",
    "\n",
    "\n",
    "For example, a standard neural network is a frequentist model and we only consider one neural network with a specific set of weights that we update over time. From a Bayesian view, the values of the weights that solve our problem best are unknown. So we model a distribution for each weight that we update based on training examples. To make a prediction, we can either sample a concrete value for each weight, or better, compute an average prediction over all possible weights weighted by its distribution obtained by training.\n",
    "\n",
    "\n",
    "The Bayesian approach here is often much more data efficient and generalizes better. At the same time, it is computationally more expensive and conceptually more complicated. This is the main reason frequentist methods dominate in machine learning. Right now, Bayesian models shine on small data where they are able to make better predictions, but many people are working on scaling them up to large data sets that are available for many tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the importance of nonparametric modeling in statistics?<a id=\"73\"></a>\n",
    "\n",
    "Rather than thinking of parametric/nonparametric as a binary choice, think of a continuum of modeling approaches. At the parametric extreme, we specify every aspect of the model, which can allow us to use fully optimal methods and specify exact confidence intervals or other conclusions.\n",
    "\n",
    "\n",
    "Unfortunately, the strong assumptions needed for fully parametric analysis are seldom true. So we develop a lot of diagnostics and adjustments to make sure our conclusions are driven by the data, and not by inappropriate assumptions.\n",
    "\n",
    "\n",
    "At the nonparametric extreme, we let the data speak to us. We make the fewest assumptions we can get away with to get useful conclusions.\n",
    "\n",
    "\n",
    "The advantages of the parametric approach are its precision if you have enough strong theory to support the assumptions, and that it often has closed form solutions. Those closed-form solutions can be useful for intuition and evaluation without a computer (that latter advantage isn’t very important today).\n",
    "\n",
    "\n",
    "The disadvantages of the parametric approach start with making statistics complex and intimidating. It often focuses attention on technical hair-splitting rather than thinking about the data. It encourages students to view violations of assumptions as problems to be fixed rather than useful signals from the data. It is more likely to lead to wildly incorrect conclusions than non-parametric methods. It gives a false veneer of indisputable science to results.\n",
    "\n",
    "\n",
    "You can think of non-parametric analysis as asking a lot of data points what they think; and parametric methods as picking the data points you think are more important and torturing them until they tell you what they want to hear. But there are other ways to think about it that make parametric methods more attractive."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the disadvantages of a random forest algorithm?<a id=\"72\"></a>\n",
    "\n",
    "Its main drawbacks include the poor performance on imbalanced data (rare outcomes or rare predictors) and lack of an interpretable model. For balanced data and pure prediction purposes, it is pretty good in general."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a confidence interval in statistics?<a id=\"71\"></a>\n",
    "\n",
    "There are three common definitions of confidence intervals, and people often confuse them.\n",
    "\n",
    "-  A Bayesian confidence interval is a betting range. For example, in a study of a new weight-loss program, participants lost an average of five pounds more than a matched control sample of people. If a Bayesian says a 95% confidence interval for the long-run average difference caused by the program is 2 pounds to 6 pounds, that means she’s willing to bet 95 dollars if the true long-term average is outside that range in order to earn 5 dollars if the true long-term average is inside that range.\n",
    "\n",
    "\n",
    "-  For the same confidence interval, some frequentists would say it mean if you did a very large number of similar experiments, 95% of them would result in a confidence interval that contained the true mean, and 5% of them would result in a confidence interval that did not include the true mean.\n",
    "\n",
    "\n",
    "-  Another type of frequentist would say, for any true mean less than the minimum of the interval (that is, less than 2 pounds) the probability of observing a sample mean of 5 pounds or greater is less than 5%; and for any true mean above the maximum of the interval (that is, greater than 6 pounds), the probability of observing a sample mean of 5 points or less is less than 5%; and for any true mean inside the interval (that is, from 2 to 6 pounds) neither of those statements is true (that is the probability of observing a sample mean less than 5 pounds is more than 5% and less than 95%).\n",
    "\n",
    "\n",
    "-  There is a fourth meaning which few people defend in theory, but is easier to compute than the three above and is often used. It says that if the true mean is the observed sample mean (5 pounds) then there is a 95% chance of observing a sample mean inside the interval (from 2 to 6 pounds).\n",
    "\n",
    "In simple cases, these four definitions often result in similar intervals, but there are cases in which the intervals can be significantly different. So you should always ask what type of interval you have been given."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the word \"embedding\" mean in the context of Machine Learning?<a id=\"70\"></a>\n",
    "\n",
    "In machine learning (ML), embedding is a special term that simply means projecting an input onto another more convenient representation space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the difference between logit and logistic regression?<a id=\"69\"></a>\n",
    "\n",
    "The logit is a transformation. Logistic regression is a regression model.\n",
    "\n",
    "The logit transformation transforms a line to a logistic curve. Logistic regression fits a logistic curve to set of data where the dependent variable can only take the values 0 and 1. It can be generalised to fitting ordinal data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we not initialize the weights of a neural network to zero?<a id=\"68\"></a>\n",
    "\n",
    "This is a problem with many models, not just neural networks. For example, it is also a problem with topic models and variational EM: if you set all of your parameters to zero, you will not learn anything.\n",
    "\n",
    "\n",
    "Many machine learning problems are non-convex. That is, there are many “solutions” that look good locally but aren’t the overall optimum answer. Recall that most modern machine learning problems use an objective function or loss to evaluate how good a solution is. It could be that every small neighborhood around an answer is worse than your current answer, but the current answer itself is pretty bad.\n",
    "\n",
    "\n",
    "This is a persistent problem. People use things like momentum, multiple restarts, and genetic-algorithm “mutations” to avoid these issues. But it’s possible to also have these problems because of poor initializations as well; if this happens, your model will just refuse to learn anything.\n",
    "\n",
    "\n",
    "This is because optimization usually works by looking at the gradient of a model. If you initialize everything to zero, every slight change won’t help the model: it doesn’t like any change more than any other. The gradient itself is uninformative (the same backprop signal goes to every internal node, EM assigns equal weight to cluster, etc.). So the gradient can’t tell your model how to improve, and you’re stuck.\n",
    "\n",
    "\n",
    "In addition, another reason to not initialize everything to zero is so that you get different answers. Some optimization techniques are deterministic, so if you initialize randomly, you’ll get different answers each time you run it. This helps you explore the space better and avoid (other) local optima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is an intuitive explanation of stochastic gradient descent?<a id=\"67\"></a>\n",
    "\n",
    "Lets say you are about to start a business that sells t-shirts, but you are unsure what are the best measures for a medium sized one for males. Luckily you have gathered a group of men that have all stated they tend to buy medium sized t-shirts. Now you figure you're going to use a gradient descent type method t get the size just right.\n",
    "\n",
    "-  Batch Gradient Descent\n",
    "\n",
    "\n",
    "Tailor makes initial estimate.\n",
    "Each person in the batch gets to try the t-shirt and write down feedback.\n",
    "Collect and summarize all feedback.\n",
    "If the feedback suggests a change, let the tailor adjust the t-shirt and go to 2.\n",
    "\n",
    "\n",
    "-  Stochastic Gradient Descent\n",
    "\n",
    "\n",
    "Tailor makes initial estimate.\n",
    "A random guy (or a subset of the full group) tries the t-shirt and gives feedback.\n",
    "Make a small adjustment according to feedback.\n",
    "While you still have time for this, go to 2.\n",
    "\n",
    "\n",
    "-  Highlighting the differences\n",
    "\n",
    "\n",
    "Batch gradient descent needs to collect lots of feedback before making adjustments, but needs to do fewer adjustments.\n",
    "\n",
    "\n",
    "Stochastic gradient descent makes many small adjustments, but spends less time collecting feedback in between.\n",
    "\n",
    "\n",
    "Batch gradient descent preferable if the full population is small, stochastic gradient descent preferable if the full population is very large.\n",
    "\n",
    "\n",
    "Batch gradient descent methods can be made parallel if you have access to more hardware (in this case, more tailors and materials) as you can collect all feedback in parallel.\n",
    "\n",
    "\n",
    "Stochastic gradient descent does not readily lend itself to parallelization as the you need the feedback from one iteration to proceed with the next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the major critisms against deep learning in the NLP field?<a id=\"66\"></a>\n",
    "\n",
    "-  Prone to implicitly introduced bias.\n",
    "\n",
    "\n",
    "-  Lack of explainability, inferability, transparency, or interpretability (degree to which humans can understand) because of black-boxed-ness.\n",
    "\n",
    "\n",
    "-  Lack of reproducibility.\n",
    "\n",
    "\n",
    "-  Data Hungriness. Requires large volumes of large numbers of labeled examples for initial learning.\n",
    "\n",
    "\n",
    "-  Higher risk of overfitting when large volumes of training data is not available.\n",
    "\n",
    "\n",
    "-  Data Similarity: The input data must be similar to the training data for interpolative generalization to work well.\n",
    "\n",
    "\n",
    "-  Limited Transfer capability: Deep learning is rigid and superficial in the sense that when confronted with scenarios that differ in minor ways from the one on which the system was trained on, breaks the inference-ability.\n",
    "\n",
    "\n",
    "-  Deep learning thus far has no natural way to deal with hierarchical structure\n",
    "\n",
    "\n",
    "-  Lack of open-ended inference: Can’t represent sentences “John promised Mary to leave” and “John promised to leave Mary” correctly and therefore not inferable.\n",
    "\n",
    "\n",
    "-  Lack of distinction between causation and correlation.\n",
    "\n",
    "\n",
    "-  Requires a lot of hyperparameter optimization and tuning. Minor changes in model require retuning.\n",
    "\n",
    "\n",
    "-  Deep learning presumes a stable world. Therefore works well problems unvarying rules (e.g board games) but not constantly changing systems such as politics and economics.\n",
    "\n",
    "\n",
    "-  Model is limited to generic representations and does not use prior knowledge or commonsense.\n",
    "\n",
    "\n",
    "-  Requires lot of compute resources.\n",
    "\n",
    "\n",
    "-  Lack of debuggability\n",
    "\n",
    "\n",
    "-  Prone to explicitly introduced bias. \n",
    "\n",
    "\n",
    "-  DL is “spoofable” and vulnerable (e.g. mistaking yellow-and-black patterns of stripes for school buses) and hackable (change inference by introducing data that humans cannot see but machines can e.g. 3d-printed turtles that have been mistake for rifles)\n",
    "\n",
    "\n",
    "-  DL increases technical debt. It is comparatively easy to make systems that work in some limited set of circumstances (short term gain), but quite difficult to guarantee that they will work in alternative circumstances with novel data that may not resemble previous training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which fallacies do machine learning engineers fall prey to?<a id=\"65\"></a>\n",
    "\n",
    "-  Assuming correlation implies causation. This is a big one, and the most well-known. Just because there is correlation between two variables, that doesn’t necessarily mean that one is causing the other, and it doesn’t mean that the relationship is any useful in a machine learning model. A common one I often hear is “we’ve identified that the target variable is correlated with weather”. Then the company spends weeks setting up a weather data collector and integrates it into a model, and it turns out the weather data didn’t help at all. Even if there was an indirect relationship, it was probably captured in another feature that was readily available in the data.\n",
    "\n",
    "\n",
    "-  Historian’s fallacy. This is pretty basic, but a model should always be evaluated with the data that would have been available at the time of prediction. If not, you get data leakage. Although most machine learning engineers intuititively understand this, it’s harder to avoid in practice than one would think. Setting up proper cross-validation routines requires some practice.\n",
    "\n",
    "\n",
    "-  Selection bias. The data is collected and selected in a way so that the population is affected by the sampling approach. Sometimes this is unavoidable, but the ability to sniff that out is the mark of an experienced machine learning engineer.\n",
    "\n",
    "\n",
    "-  Handling missing values dogmatically. There is no silver bullet for treating missing values. Inexperienced machine learning engineers will do things like replace missing values with zero, or remove them completely. When they figure out that the missing values are a problem, they will ask for the canonical way to handle them, but the truth is it doesn’t exist. It’s a complicated, situational problem. Solving it correctly requires experience.\n",
    "\n",
    "\n",
    "-  Using accuracy as the only metric to evaluate classifiers. It’s rare that the performance of a classification model can be described precisely using only a single metric. It’s even worse when that metric is plain accuracy. That would assume that false positives and false negatives are equally bad, which is almost never the case. Even ROC AUC falls short in most cases. The necessity of assessing models holistically is often underestimated by inexperienced machine learning engineers.\n",
    "\n",
    "\n",
    "-  Motivated reasoning. Because they want the model to work, they find ways to make it look better than it really is, losing objectivity in the process. A common attitude among juniors is that failure is not an option. Seniors will understand that failing is a natural part of a scientific process. The job of a data scientist is to test hypotheses. Sometimes, the hypothesis must be rejected, meaning there is insufficient data to build a predictive model. It doesn’t help when machine learning engineers try to fit squares into circles.\n",
    "\n",
    "\n",
    "-  Cherry picking. Demonstrating the effectiveness of a model by pointing at examples where it made good predictions instead of providing the full picture. This goes hand in hand with motivated reasoning.\n",
    "\n",
    "\n",
    "-  The quantitative fallacy. This is very common even among more experienced machine learning engineers and data scientists. It’s when you make decisions based solely on numbers, disregarding any qualitative considerations. The result is often a model that is technically sound but useless, because it ignores fundamental questions like “how will we use this model” and “how will it help our business”. Ignoring qualititative aspects also leads to a poor understanding of the domain which again hampers modelling.\n",
    "\n",
    "\n",
    "-  Preferring complexity over simplicity. Any experienced machine learning engineer will tell you that a simpler model is better than a complex one, all else being equal. Inexperienced machine learning engineers will often dive right into complex neural networks before testing simpler models, merely because they want to get their feet wet.\n",
    "\n",
    "\n",
    "-  Overestimating the importance of hyperparameter tuning. Junior practitioners will often spend too much time tweaking hyperparameters for small performance gains when much greater gains could have been made by working with the underlying data and features.\n",
    "\n",
    "\n",
    "-  Underestimating the importance of feature engineering. See above.\n",
    "\n",
    "\n",
    "-  Regarding domain knowledge as irrelevant. Domain knowledge is always relevant, even if you’re just implementing an already defined machine learning model. That’s why every machine learning engineer needs to be a data scientist too. It takes a bit of trial and error until you fully appreciate just how important domain knowledge is.\n",
    "\n",
    "\n",
    "-  Treating the modelling process linearly. Iteration is key, not just in the training algorithm, but in the modelling and experimentation process as well. Because of the nondeterministic nature of machine learning, you can’t specify how to solve a problem upfront, because there is no way of knowing what the best solution is before actually trying it out. Many beginners (and managers) make the mistake of planning a machine learning project like a software development project.\n",
    "\n",
    "\n",
    "-  Ignoring model decay. No, I’m not talking about learning rate decay, which is something completely different. I’m talking about the fact that models will degrade over time after being put into production. Sometimes they even affect the very phenomenon they’re trying to model, leading to self-reinforcing feedback. This is also known as the Hawthorne effect. Even though you’ve trained, validated and tested a model on three different datasets, it’s not necessarily the model that performs best in that test that will prove to be the best over time. This touches on preferring complexity over simplicity, since simpler models are less prone to degrade. It’s hard to grasp this concept intuitively without practical experience.\n",
    "\n",
    "\n",
    "-  Ignoring the cobra effect. Sometimes the solution to a problem can actually make the problem worse. Just because you have identified that some customers are more likely to buy a product, that doesn’t mean that a targeted marketing ad will increase sales. In some occasions, marketing actually produces the opposite outcome. This is unintuitive, but happens in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is backpropagation such a great deal and got a special name? Is it not just the natural way to think how to calculate the gradients to perform gradient descent?<a id=\"64\"></a>\n",
    "\n",
    "The name comes from the procedure of alternating between forward and back propagating through the network. The procedure first forward propagates training input data to the output node, and second backward propagates the resulting prediction error back through the layers of nodes in reverse order to find gradients.\n",
    "\n",
    "Gradient decent refers to the general method of using partial derivatives of an objective function as a guide to find a better approximation to a function optimum. Back propagation takes advantage of the special structure of the objective function being represented by a network of nodes. This enables the use of the chain rule in order to carry out the calculation of derivatives across network layers in a sequence of back propagation steps.\n",
    "\n",
    "It is a big deal because instead of having to deal with a very complicated objective function in its entirety it allows to split the procedure into simpler steps that propagate through the layers of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For what kind of classification problems is SVM a bad approach?<a id=\"63\"></a>\n",
    "\n",
    "-  SVMs do not perform well on highly skewed/imbalanced data sets. These are training data sets in which the number of samples that fall in one of the classes far outnumber those that are a member of the other class. Customer churn data sets are typically in this group because when you collect the training set, among a million customers during a particular time period, there would be very few who have actually churned. SMOTING is used to generate artificial samples in the minority class to balance the data set. On the other hand, Logistic Regression is good at handling skewed data sets.\n",
    "\n",
    "\n",
    "-  SVMs are also not a good option specially if you have multiple classes. Ultimately in this case, you get back to a binary classifier and then use some kind of a voting mechanism to classify a sample to one of the classes.\n",
    "\n",
    "\n",
    "-  SVMs are not efficient if the number of features are very huge in number compared to the training samples. \n",
    "\n",
    "\n",
    "-  If your data sets are such that they arrive in batches and everytime you want to increment your learning model, then SVMs are not a good option for incremental learning (I would confirm this with others though).\n",
    "\n",
    "\n",
    "-  SVMs are also not good if your data sets need to be trained in parallel. If your datasets are divided into subsets, and somehow you want to train them independently in parallel, and then combine the knowledge together, SVMs haven't reached that point yet I think."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What makes the ReLU function more appropriate in some cases over a Sigmoid activation function?<a id=\"62\"></a>\n",
    "\n",
    "ReLU prevents gradients from saturating in deep networks, and thus mitigates the risk of vanishing gradients. Ideally, you should be mixing in ReLUs and sigmoidal activation functions, since sigmoidal activation functions are better at capturing nonlinearities.\n",
    "\n",
    "Clarification: you should be mixing in ReLUs and sigmoidal activation functions across layers, each layer should use either of the two activations and still have the same activation for each hidden unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A linear regression requires residuals to be normally distributed. Why do we need this assumption? What will happen if this assumption does not satisfy?<a id=\"61\"></a>\n",
    "\n",
    "It is not true that linear regression requires normally distributed residuals. It is true that we make that assumption more often than not, but you should always check your assumptions, for example using a normal probability plot.\n",
    "\n",
    "Normality is not the only “usual” assumption. We also usually assume that the residuals have the same distribution for all values of the explanatory variables.\n",
    "\n",
    "You need assumptions about the distribution of the residuals in order to make inferences. You need probabilities to form confidence intervals and perform hypothesis tests.\n",
    "\n",
    "Fortunately regression is quite robust. The distribution doesn’t matter too much so long as there are no extreme observations. The central limit theorem applies which means that the statistics do approximately have normal distributions or distributions, such as chi-squared and F, derived from it. In fact I think that heterogeneity of variances is a worse violation of the usual assumptions than lack of normality. (To deal with this you can use generalised least squares fitting in stead of ordinary least squares.)\n",
    "\n",
    "So, to answer your last question, if the assumptions don’t hold, your inferences will use the incorrect probabilities. But if your regression diagnostics look reasonable, the inferences should not be far wrong.\n",
    "\n",
    "What should you do otherwise? There are various techniques such as transforming the data and robust regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is \"data leakage\" in data science?<a id=\"60\"></a>\n",
    "\n",
    "Data Leakage is the creation of unexpected additional information in the training data, allowing a model or machine learning algorithm to make unrealistically good predictions.\n",
    "\n",
    "Leakage is a pervasive challenge in applied machine learning, causing models to over-represent their generalization error and often rendering them useless in the real world. It can caused by human or mechanical error, and can be intentional or unintentional in both cases.\n",
    "\n",
    "Some types of data leakage include:\n",
    "\n",
    "-  Leaking test data into the training data.\n",
    "-  Leaking the correct prediction or ground truth into the test data.\n",
    "-  Leaking of information from the future into the past.\n",
    "-  Retaining proxies for removed variables a model is restricted from knowing.\n",
    "-  Reversing of intentional obfuscation, randomization or anonymization.\n",
    "-  Inclusion of data not present in the model's operational environment.\n",
    "-  Distorting information from samples outside of scope of the model's intended use.\n",
    "-  Any of the above present in third party data joined to the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the best method for determining the ideal depth for a neural network?<a id=\"59\"></a>\n",
    "\n",
    "It is worth noting that there is typically no “ideal” depth.\n",
    "\n",
    "First of all, the Universal approximation theorem tells us that even networks having only a single hidden layer can approximate any continuous function to arbitrary precision given a sufficient number of nodes. Furthermore, the non-convexity of the general neural network optimization objective implies that no matter how carefully you choose your architecture, the function it ends up approximating is still far from guaranteed to be the “ideal” one for your data.\n",
    "\n",
    "With that being said: at this point in time, the process of deciding on a network architecture largely resembles trial-and-error. However, there are some key considerations that can guide your search:\n",
    "\n",
    "-  Model complexity. A deeper network will tend to learn more complicated nonlinear hypotheses than will a shallow network. If you suspect your model is failing to pick up on patterns in your training data (i.e. underfitting), consider increasing the depth of your network. The standard way of diagnosing this problem is to compare your model’s error on both the training and validation data; if these errors are quite similar, you are likely underfitting. Conversely, if you suspect your model is hallucinating patterns in your training data (i.e. overfitting), consider decreasing the depth of your network. A good clue that you are overfitting is that your training error is very low while your validation error is considerably higher. This is known as the Bias–variance tradeoff; there are many other methods (e.g. parameter regularization, early stopping, dropout and other forms of model averaging) for balancing this tradeoff as well.\n",
    "\n",
    "\n",
    "-  Computational cost. A deeper network will have more parameters to estimate than a shallow network, so a single iteration of stochastic gradient descent, which consists of a forward pass (error evaluation) and a backward pass (backpropagation), will require both more time and more space. If your model is taking a really long time to train or you are working under certain computational constraints, you might consider reducing the number of hidden layers.\n",
    "\n",
    "\n",
    "-  Interpretability. Some neural networks are built to output structured data instead of a single regression/classification prediction. For example, convolutional neural networks are frequently employed for image segmentation tasks in which the network outputs an image mask. In this setting, the network will usually downsample the mask to a low-dimensional latent representation before inferring the mask and upsampling back to the original image size. Is the output mask just blatantly inaccurate? Perhaps the model is not learning a useful latent representation, and adding more layers in the downsampling stage of the network will help. Is the output mask generally accurate but of particularly poor resolution? Try adding layers in the upsampling stage.\n",
    "\n",
    "\n",
    "If manually tuning the number of hidden layers in your network using these heuristics feels to you like it should be a thing of the past, you’re not alone. Interesting work has been done recently on automating this process, for example by using reinforcement learning (see: Using Machine Learning to Explore Neural Network Architecture) and by framing architecture search as a continuous (rather than discrete) optimization problem in order to use gradient based methods to guide the search (see: DARTS: Differentiable Architecture Search)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the meaning of \"central\" in \"Central Limit Theorem\"?<a id=\"58\"></a>\n",
    "\n",
    "The central has nothing to do with the central limit. It really means that it is the “central theorem” in statistics. It is central in the sense that says the distribution sum of identically independently random variables will approach the normal distribution. The normal or Gaussian distribution is the most important distribution in probability and statistics.\n",
    "\n",
    "For example, given a person’s age, the height is approximately normally distributed. The distribution of SAT scores is normally distributed. The blood sugar level of individuals is approximately normally distributed. Physical measurement errors are approximately normally distributed.\n",
    "\n",
    "You may wonder why I say approximately. The normal distribution is an ideal distribution. The statistician Geary has said, “There never has been and never will be a normal distribution.” Real world cases are always approximations.\n",
    "\n",
    "As the great statistician, G. E. P. Box said, “All models are wrong but some are useful.” The normal distribution is extremely useful. That is why the central limit theorem is central. It is central to both theoretical topics in probability and statistics and very essential to applied statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are control variables and how do I use them in regression analysis?<a id=\"57\"></a>\n",
    "\n",
    "Control variables in statistics is defined as a constant whose value does not change throughout the experiment process. A control variable can strongly influence the results in an experiment.\n",
    "\n",
    "It is held constant during the experiment so that the experimenter can test the relationship between the dependent and independent variables. A control variable is the one element that is not changed throughout an experiment. It is because its unchanging state allows the relationship between the other variables being tested to be better understood. Any change in the control variable in an experiment would discredit the correlation of dependent variable to the independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For linear regression, how do we decide whether mean absolute error or mean square error is better? Are there other loss functions that are commonly used?<a id=\"56\"></a>\n",
    "\n",
    "Least squares is more influenced by extremes than mean absolute error. But it is optimal in various senses under the usual regression assumptions. In such cases, the tests and confidence interval formulae are much simpler and better understood than for other loss functions.\n",
    "\n",
    "But always use diagnostic plots to see if the usual regression assumptions are reasonable. If they are, it is OK to use least squares. If there is a linear trend in the residuals then maybe generalised least squares is better, i.e. use a weighted sum of squares criterion.\n",
    "\n",
    "Mean absolute error protects you against outliers but it is hard to fit. One approach is to use least squares with weights inversely proportional to the residuals. This may be done iteratively. Or you can use linear programming.\n",
    "\n",
    "But whatever you do, a plot of the residuals is an important diagnostic.\n",
    "\n",
    "Another trap with least squares is high leverage points. A value of the explanatory variable a long way from the rest is said to have high leverage. The residual at that point is often small because the point has had an undue influence on the fit. If that happens the point is influential. Influential points are outliers, but not seen as such when the residual is small.\n",
    "\n",
    "Most statistical software has a variety of plots and diagnostics to help you spot problems of the kinds mentioned above. And that’s how you decide what method to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression: Why sigmoid function?<a id=\"55\"></a>\n",
    "\n",
    "It is a great function for modeling binary classification. It gives 0 or 1 as outputs. But it also gives values in between 0 and 1. \n",
    "\n",
    "Other functions give similar outputs, such as the Step Function, which only gives values of 0 and 1, and nothing in between 0 and 1.\n",
    "\n",
    "The advantage of the Sigmoid function is that its derivative is easy to calculate. This is useful for the Steepest Descent method and for Neural Networks.\n",
    "\n",
    "Since the sigmoid function gives values between 0 and 1,  these values can be interpreted as uncertainties for the outputs of 0 and 1. For example, if the function value is at 0.25, then you can assign an uncertainty to its output of 0. I would assign it a 25% uncertainty that is is 0 and 75% uncertainty that it is 1. So my program would most likely choose the output of 0.\n",
    "\n",
    "On the other hand, if the function  is at 0.001, then it is very certain that its output is 0. Or if its value is 0.999, then it is very certain that its output is a 1.\n",
    "\n",
    "If the values are near 0.5 , then I would let a random number choose the output of 0 or 1.\n",
    "\n",
    "This allows for a \"fuzziness\" when doing classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some general ways to improve multiple linear regression models?<a id=\"54\"></a>\n",
    "\n",
    "Before running the regression, I often do exploratory analysis and look at plots of the different explanatory variables vs. the response variable. Sometimes the relationship between x and y isn't necessarily linear, and you might be better off with a transformation like y=log(x), e^x, or x^2. If there are interactions between features, also try using interaction terms to capture this.\n",
    "\n",
    "Stepwise regression is helpful for selecting the optimal subset of features to use in the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we know whether we use t-test, ANOVA, chi-square, correlation, or regression analysis?<a id=\"53\"></a>\n",
    "\n",
    "First, t-test, ANOVA and (OLS) regression are all the same model. You can use (some form of) regression for any problem that can be answered with a t-test or ANOVA. (Independent sample) t-tests can only handle the case where there is a single independent variable and it has precisely two levels. ANOVA handles the special case where all the independent variables are categorical, they can have any number of levels. Paired t tests can be dealt with in regression by making the dependent variable the difference in scores (this is what the paired t-test does). Repeated measures ANOVA shouldn’t really be used, it makes assumptions that are almost always unrealistic - better to use a multi-level model (which is a generalization of regression).\n",
    "\n",
    "What these methods have in common is that they have a single dependent variable and any number of independent variables and that the dependent variable is continuous. There are other regression methods for other kinds of dependent variables.\n",
    "\n",
    "Correlation and chi-square do not separate variables into dependent and independent. There are various kinds of correlation, but the most common is for two continuous variables. Chi-square is for categorical variables, most often for testing the association between two variables, but there are methods for one variable or more than two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is bagging in machine learning?<a id=\"52\"></a>\n",
    "\n",
    "Bagging is used typically when you want to reduce the variance while retaining the bias. This happens when you average the predictions in different spaces of the input feature space. \n",
    "\n",
    "In bagging, first you will have to sample the input data (with replacement) to generate multiple sets of input data. For each of those sets, the same baseline predictor (such as a SVM, Neural Net, etc) is run to get a trained model for each of the training set.\n",
    "\n",
    "Now, to do the prediction on an unseen test sample, it is run through these individual models and the predictions are now averaged to get the final decision.\n",
    "\n",
    "Bagging is effective because you are improving the accuracy of a single model by using multiple copies of it trained on different sets of data. \n",
    "\n",
    "Bagging is not recommended on models that have a high bias. In such cases, boosting (Adaboost) is used which goes a step ahead and eliminates the effect of a high bias present in the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Python, How is Memory Managed?<a id=\"51\"></a>\n",
    "\n",
    "In Python, memory is managed in a private heap space. This means that all the objects and data structures will be located in a private heap. However, the programmer will not be allowed to access this heap. Instead, the Python interpreter will handle it. At the same time, the core API will enable access to some Python tools for the programmer to start coding.\n",
    "\n",
    "The memory manager will allocate the heap space for the Python objects while the garbage collector will recycle all the memory that’s not being used to boost available heap space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do You Know Any Additional Data Structures Available in the Standard Library?<a id=\"50\"></a>\n",
    "\n",
    "Python’s standard library has a wealth of data structures, including:\n",
    "\n",
    "-  Float\n",
    "-  Bisect\n",
    "-  Heapq\n",
    "-  Deque\n",
    "-  Boolean\n",
    "-  Integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Packages in the Standard Library - Useful for Data Science Work - Do You Know?<a id=\"49\"></a>\n",
    "\n",
    "When Guido van Rossum created Python in the 1990s, it was not built for data science. Yet today, Python is the leading language for machine learning, predictive analytics, statistics, and simple data analytics.\n",
    "\n",
    "This is because Python is a free and open-source language that data professionals could easily use to develop tools that would help them complete data tasks more efficiently.\n",
    "\n",
    "The following packages in the Python Standard Library are very handy for data science projects:\n",
    "\n",
    "[NumPy](http://www.numpy.org/)\n",
    "\n",
    "NumPy (or Numerical Python) is one of the principle packages for data science applications. It is often used to process large multidimensional arrays, extensive collections of high-level mathematical functions, and matrices. Implementation methods also make it easy to conduct multiple operations with these objects.\n",
    "\n",
    "There have been many improvements made over the last year that have resolved several bugs and compatibility issues. NumPy is popular because it can be used as a highly efficient multi-dimensional container of generic data. It is also an excellent library as it makes data analysis simple by processing data faster while using a lot less code than lists.\n",
    "\n",
    "[Pandas](https://pandas.pydata.org/)\n",
    "\n",
    "Pandas is a Python library that provides highly flexible and powerful tools, and high-level data structures for analysis. Pandas is an excellent tool for data analytics because it can translate highly complex operations with data into just one or two commands.\n",
    "\n",
    "Pandas comes with a variety of built-in methods for combining, filtering, and grouping data. It also boasts time-series functionality that is closely followed by remarkable speed indicators.\n",
    "\n",
    "[SciPy](https://www.scipy.org/)\n",
    "\n",
    "SciPy is another outstanding library for scientific computing. It is based on NumPy and was created to extend its capabilities. Like NumPy, SciPy’s data structure is also a multidimensional array that is implemented by NumPy.\n",
    "\n",
    "The SciPy package contains powerful tools that help solve tasks related to integral calculus, linear algebra, probability theory, and much more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the Difference between a For Loop and a While Loop?<a id=\"48\"></a>\n",
    "\n",
    "In Python, a loop iterates over popular data types (like dictionaries, lists, or strings) while the condition is true. This means that the program control will pass to the line immediately following the loop whenever the condition is false. In this scenario, it is not a question of preference, but a question of what your data structures are.\n",
    "\n",
    "-  For Loop\n",
    "\n",
    "In Python (and in almost any other programming language), For Loop is the most common type of loop. For Loop is often leveraged to iterate through the elements of an array.\n",
    "\n",
    "For example:\n",
    "\n",
    "For i=0, N_Elements (array) do…\n",
    "\n",
    "For Loop can also be used to perform a fixed number of iterations and iterate by a given (positive or even negative) increment. It is important to note that by default, the increment will always be one.\n",
    "\n",
    "-  While Loop\n",
    "\n",
    "While Loop can be used in Python to perform an indefinite number of iterations as long as the condition remains true.\n",
    "\n",
    "For example:\n",
    "\n",
    "While (condition) do…\n",
    "\n",
    "When using the While Loop, you have to explicitly specify a counter to keep track of how many times the loop was executed. However, While Loop cannot define its own variable. Instead, it has to be previously defined and will continue to exist even after you exit the loop.\n",
    "\n",
    "When compared to For Loop, While Loop is inefficient because it is much slower. This can be attributed to the fact that it checks the condition after each iteration. However, if you need to perform one or more conditional checks in a For Loop, you will want to consider using While Loop instead (as these checks won’t be required)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When Would You Use a List vs. a Tuple vs. a Set in Python?<a id=\"47\"></a>\n",
    "\n",
    "A list is a common data type that is highly flexible. It can store a sequence of objects that are mutable, so it is ideal for projects that demand the storage of objects that can be changed later.\n",
    "\n",
    "A tuple is similar to a list in Python, but the key difference between them is that tuples are immutable. They also use less space than lists and can only be used as a key in a dictionary. Tuples are a perfect choice when you want a list of constants.\n",
    "\n",
    "Sets are a collection of unique elements that are used in Python. Sets are a good option when you want to avoid duplicate elements in your list. This means that whenever you have two lists with common elements between them, you can leverage sets to eliminate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is There a Way to Get a List of All the Keys in a Dictionary?<a id=\"46\"></a>\n",
    "\n",
    "mydict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can You Explain What a List or Dict Comprehension Is?<a id=\"45\"></a>\n",
    "\n",
    "new_list = [expression for_loop_one_or_more conditions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In a List and in a Dictionary, What Are the Typical Characteristics of Elements?<a id=\"44\"></a>\n",
    "\n",
    "Elements in lists maintain their ordering unless they are explicitly commanded to be re-ordered. They can be of any data type, they can all be the same, or they can be mixed. Elements in lists are always accessed through numeric, zero-based indices.\n",
    "\n",
    "In a dictionary, each entry will have a key and a value, but the order will not be guaranteed. Elements in the dictionary can be accessed by using their key.\n",
    "\n",
    "Lists can be used whenever you have a collection of items in an order. A dictionary can be used whenever you have a set of unique keys that map to values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the Difference Between a List and a Dictionary?<a id=\"43\"></a>\n",
    "\n",
    "Lists can be used to store a sequence of objects that are mutable. However, they have to be stored in a particular order that can be indexed into the list or iterated over it (and it can also take some time to apply iterations on list objects).  Lists are defined within square brackets.\n",
    "\n",
    "A dictionary is an unordered collection of key-value pairs. It is a perfect tool to work with an enormous amount of data as dictionaries are optimized for retrieving data (but you have to know the key to retrieve its value). Dictionaries can also be described as the implementation of a hash table and as a key-value store. In this scenario, you can quickly look up anything by its key, but since it is unordered, it will demand that keys are hashes. Dictionaries are defined within curly braces {} where each item will be a pair in the form key:value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Native Data Structures Are Mutable, and Which Are Immutable?<a id=\"42\"></a>\n",
    "\n",
    "Lists, dictionaries, and sets are mutable. This means that you can change their content without changing their identity. Strings and tuples are immutable, as their contents cannot be altered once they are created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which Native Data Structures Can You Name in Python?<a id=\"41\"></a>\n",
    "\n",
    "Common native data structures in Python are as follows:\n",
    "\n",
    "-  Sets<br>\n",
    "-  Lists<br>\n",
    "-  Tuples<br>\n",
    "-  Strings<br>\n",
    "-  Dictionaries<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Is Python?<a id=\"40\"></a>\n",
    "\n",
    "Python is an open-source interpreted language (like PHP and Ruby) with automatic memory management, exceptions, modules, objects, and threads.\n",
    "\n",
    "The benefits of Python include its simplicity, portability, extensibility, and built-in data structures. As it is open-source, there is also a massive community backing it. Python is best suited for object-oriented programming. It is dynamically typed, so you are not required to state the types of variables when you declare them. Unlike C++, it does not have access to public or private specifiers.\n",
    "\n",
    "Python’s functions are first-class objects that make difficult tasks simple. While you can write code quickly, running it will be comparatively slower than other compiled programming languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where do you usually source datasets?<a id=\"39\"></a>\n",
    "\n",
    "Machine learning interview questions like these try to get at the heart of your machine learning interest. Someone who is truly passionate about machine learning will have gone off and done side projects on their own, and have a good idea of what great datasets are out there. If you are missing any, check out [Quandl](https://www.quandl.com/) for economic and financial data and [Kaggle’s Datasets collection](https://www.kaggle.com/datasets) for another great list.\n",
    "\n",
    "[19 Free Public Data Sets for Your First Data Science Project](https://www.springboard.com/blog/free-public-data-sets-data-science-project/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are your favorite use cases of machine learning models?<a id=\"37\"></a>\n",
    "\n",
    "[An executive’s guide to AI](https://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/an-executives-guide-to-ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the last machine learning papers you have read?<a id=\"35\"></a>\n",
    "\n",
    "Keeping up with the latest scientific literature on machine learning is a must if you want to demonstrate interest in a machine learning position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you think of our current data process?<a id=\"34\"></a>\n",
    "\n",
    "This kind of question requires you to listen carefully and impart feedback in a manner that is constructive and insightful. Your interviewer is trying to gauge if you will be a valuable member of their team and whether you grasp the nuances of why certain things are set the way they are in the company’s data process based on company- or industry-specific conditions. They are trying to determine if you can be an intellectual peer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we use your machine learning skills to generate revenue?<a id=\"33\"></a>\n",
    "\n",
    "This is a tricky question. The ideal answer will demonstrate knowledge of what drives the business and how your skills could relate. For example, if you were interviewing for music-streaming startup Spotify, you could remark that your skills at developing a better recommendation model will increase user retention, which would then increase revenue in the long run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How would you implement a recommendation system for our company’s users?<a id=\"32\"></a>\n",
    "\n",
    "A lot of machine learning interview questions of this type will involve implementation of machine learning models to a company’s problems. You will have to research the company and its industry in-depth, especially the revenue drivers the company has, and the types of users the company takes on in the context of the industry it’s in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which data visualization libraries do you use? What are your thoughts on the best data visualization tools?<a id=\"31\"></a>\n",
    "\n",
    "What is important here is to define your views on how to properly visualize data and your personal preferences when it comes to tools. Popular tools include R’s ggplot, Python’s seaborn and matplotlib, and tools such as Plot.ly and Tableau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe a hash table.<a id=\"30\"></a>\n",
    "\n",
    "A hash table is a data structure that produces an associative array. A key is mapped to certain values through the use of a hash function. They are often used for tasks such as database indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some differences between a linked list and an array?<a id=\"29\"></a>\n",
    "\n",
    "An array is an ordered collection of objects. A linked list is a series of objects with pointers that direct how to process them sequentially. An array assumes that every element has the same size, unlike the linked list. A linked list can more easily grow organically: an array has to be pre-defined or re-defined for organic growth. Shuffling a linked list involves changing which points direct where — meanwhile, shuffling an array is more complex and takes more memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do you handle missing or corrupted data in a dataset?<a id=\"26\"></a>\n",
    "\n",
    "You could find missing/corrupted data in a dataset and either drop those rows or columns, or decide to replace them with another value.\n",
    "\n",
    "In Pandas, there are two very useful methods: isnull() and dropna() that will help you find columns of data with missing or corrupted data and drop those values. If you want to fill the invalid values with a placeholder value (for example, 0), you could use the fillna() method.\n",
    "\n",
    "[Handling missing data](https://www.oreilly.com/learning/handling-missing-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the “kernel trick” and how is it useful?<a id=\"25\"></a>\n",
    "\n",
    "The Kernel trick involves kernel functions that can enable in higher-dimension spaces without explicitly calculating the coordinates of points within that dimension: instead, kernel functions compute the inner products between the images of all pairs of data in a feature space. This allows them the very useful attribute of calculating the coordinates of higher dimensions while being computationally cheaper than the explicit calculation of said coordinates. Many algorithms can be expressed in terms of inner products. Using the kernel trick enables us effectively run algorithms in a high-dimensional space with lower-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How would you evaluate a logistic regression model?<a id=\"24\"></a>\n",
    "\n",
    "You have to demonstrate an understanding of what the typical goals of a logistic regression are (classification, prediction etc.) and bring up a few examples and use cases.\n",
    "\n",
    "There are many thousands of tests one can apply to inspect a logistic regression model, and much of this depends on whether one's goal is prediction, classification, variable selection, inference, causal modeling, etc. The Hosmer-Lemeshow test, for instance, assesses model calibration and whether predicted values tend to match the predicted frequency when split by risk deciles. Although, the choice of 10 is arbitrary, the test has asymptotic results and can be easily modified. The HL test, as well as AUC, have very uninteresting results when calculated on the same data that was used to estimate the logistic regression model. Tests of predictive accuracy (e.g. HL and AUC) are better employed with independent data sets, or (even better) data collected over different periods in time to assess a model's predictive ability.\n",
    "\n",
    "Another point to make is that prediction and inference are very different things. There is no objective way to evaluate prediction, an AUC of 0.65 is very good for predicting very rare and complex events like 1 year breast cancer risk. Similarly, inference can be accused of being arbitrary because the traditional false positive rate of 0.05 is commonly thrown around."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What evaluation approaches would you work to gauge the effectiveness of a machine learning model?<a id=\"23\"></a>\n",
    "\n",
    "You would first split the dataset into training and test sets, or perhaps use cross-validation techniques to further segment the dataset into composite sets of training and test sets within the data. You should then implement a choice selection of performance metrics: [here is a fairly comprehensive list.](https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/) You could use measures such as the F1 score, the accuracy, and the confusion matrix. What’s important here is to demonstrate that you understand the nuances of how a model is measured and how to choose the right performance measures for the right situations.\n",
    "\n",
    "[How to Evaluate Machine Learning Algorithms](https://machinelearningmastery.com/how-to-evaluate-machine-learning-algorithms/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do you ensure you’re not overfitting with a model?<a id=\"22\"></a>\n",
    "\n",
    "This is a simple restatement of a fundamental problem in machine learning: the possibility of overfitting training data and carrying the noise of that data through to the test set, thereby providing inaccurate generalizations.\n",
    "\n",
    "There are three main methods to avoid overfitting:\n",
    "\n",
    "1- Keep the model simpler: reduce variance by taking into account fewer variables and parameters, thereby removing some of the noise in the training data.<br>\n",
    "2- Use cross-validation techniques such as k-folds cross-validation.<br>\n",
    "3- Use regularization techniques such as LASSO that penalize certain model parameters if they’re likely to cause overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name an example where ensemble techniques might be useful.<a id=\"21\"></a>\n",
    "\n",
    "Ensemble techniques use a combination of learning algorithms to optimize better predictive performance. They typically reduce overfitting in models and make the model more robust (unlikely to be influenced by small changes in the training data). \n",
    "\n",
    "You could list some examples of ensemble methods, from bagging to boosting to a “bucket of models” method and demonstrate how they could increase predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When should you use classification over regression?<a id=\"20\"></a>\n",
    "\n",
    "Classification produces discrete values and dataset to strict categories, while regression gives you continuous results that allow you to better distinguish differences between individual points. You would use classification over regression if you wanted your results to reflect the belongingness of data points in your dataset to certain explicit categories (ex: If you wanted to know whether a name was male or female rather than just how correlated they were with male and female names.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How would you handle an imbalanced dataset?<a id=\"19\"></a>\n",
    "\n",
    "An imbalanced dataset is when you have, for example, a classification test and 90% of the data is in one class. That leads to problems: an accuracy of 90% can be skewed if you have no predictive power on the other category of data. Here are a few tactics to get over the hump:\n",
    "\n",
    "1- Collect more data to even the imbalances in the dataset.<br>\n",
    "2- Resample the dataset to correct for imbalances.<br>\n",
    "3- Try a different algorithm altogether on your dataset.<br>\n",
    "\n",
    "What is important here is that you have a keen sense for what damage an unbalanced dataset can cause, and how to balance that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[8 Tactics to Combat Imbalanced Classes in Your Machine Learning Dataset](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What’s the F1 score? How would you use it?<a id=\"18\"></a>\n",
    "\n",
    "The F1 score is a measure of a model’s performance. It is a weighted average of the precision and recall of a model, with results tending to 1 being the best, and those tending to 0 being the worst. You would use it in classification tests where true negatives do not matter much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which is more important to you– model accuracy, or model performance?<a id=\"17\"></a>\n",
    "\n",
    "The accuracy paradox is the paradoxical finding that accuracy is not a good metric for predictive models when classifying in predictive analytics. This is because a simple model may have a high level of accuracy but be too crude to be useful. For example, if the incidence of category A is dominant, being found in 99% of cases, then predicting that every case is category A will have an accuracy of 99%. Precision and recall are better measures in such cases. The underlying issue is that class priors need to be accounted for in error analysis. Precision and recall help, but precision too can be biased by very unbalanced class priors in the test sets.\n",
    "\n",
    "This question tests your grasp of the nuances of machine learning model performance. Machine learning interview questions often look towards the details. There are models with higher accuracy that can perform worse in predictive power — how does that make sense?\n",
    "\n",
    "It has everything to do with how model accuracy is only a subset of model performance, and at that, a sometimes misleading one. For example, if you wanted to detect fraud in a massive dataset with a sample of millions, a more accurate model would most likely predict no fraud at all if only a vast minority of cases were fraud. However, this would be useless for a predictive model — a model designed to find fraud that asserted there was no fraud at all. Questions like this help you demonstrate that you understand model accuracy is not the be-all and end-all of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is a decision tree pruned?<a id=\"16\"></a>\n",
    "\n",
    "Pruning is what happens in decision trees when branches that have weak predictive power are removed in order to reduce the complexity of the model and increase the predictive accuracy of a decision tree model. Pruning can happen bottom-up and top-down, with approaches such as reduced error pruning and cost complexity pruning.\n",
    "\n",
    "Reduced error pruning is perhaps the simplest version: replace each node. If it doesn’t decrease predictive accuracy, keep it pruned. While simple, this heuristic actually comes pretty close to an approach that would optimize for maximum accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What cross-validation technique would you use on a time series dataset?<a id=\"15\"></a>\n",
    "\n",
    "Instead of using standard k-folds cross-validation, you have to pay attention to the fact that a time series is not randomly distributed data — it is inherently ordered by chronological order. If a pattern emerges in later time periods for example, your model may still pick up on it even if that effect does not hold in earlier years.\n",
    "\n",
    "You will want to do something like forward chaining where you will be able to model on past data then look at forward-facing data:\n",
    "\n",
    "fold 1 : training [1], test [2]<br>\n",
    "fold 2 : training [1 2], test [3]<br>\n",
    "fold 3 : training [1 2 3], test [4]<br>\n",
    "fold 4 : training [1 2 3 4], test [5]<br>\n",
    "fold 5 : training [1 2 3 4 5], test [6]<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the difference between a generative and discriminative model?<a id=\"14\"></a>\n",
    "\n",
    "A generative model will learn categories of data while a discriminative model will simply learn the distinction between different categories of data. Discriminative models will generally outperform generative models on classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is deep learning and how does it contrast with other machine learning algorithms?<a id=\"13\"></a>\n",
    "\n",
    "Deep learning is a subset of machine learning that is concerned with neural networks: how to use backpropagation and certain principles from neuroscience to more accurately model large sets of unlabelled or semi-structured data. In that sense, deep learning represents an unsupervised learning algorithm that learns representations of data through the use of neural nets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the difference between probability and likelihood?<a id=\"12\"></a>\n",
    "\n",
    "Probability describes the plausibility of (random) observed data-- assumed to be described by a statistical model, and specified parameter value(s)-- without reference to any observed data.\n",
    "\n",
    "Likelihood describes the plausibility, given specific observed data, of a parameter value of the statistical model which is assumed to describe that data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Fourier transform?<a id=\"11\"></a>\n",
    "\n",
    "A Fourier transform is a generic method to decompose generic functions into a superposition of symmetric functions. The Fourier transform finds the set of cycle speeds, amplitudes and phases to match any time signal. A Fourier transform converts a signal from time to frequency domain — it is a very common way to extract features from audio signals or other time series such as sensor data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What’s the difference between Type I and Type II error?<a id=\"10\"></a>\n",
    "\n",
    "Type I error is a false positive, while Type II error is a false negative. Briefly stated, Type I error means claiming something has happened when it has not, while Type II error means that you claim nothing is happening when in fact something is.\n",
    "\n",
    "A clever way to think about this is to think of Type I error as telling a man he is pregnant, while Type II error means you tell a pregnant woman she isn’t carrying a baby."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What’s your favorite algorithm, and can you explain it to me in less than a minute?<a id=\"9\"></a>\n",
    "\n",
    "This type of question tests your understanding of how to communicate complex and technical nuances with poise and the ability to summarize quickly and efficiently. Make sure you have a choice and make sure you can explain different algorithms so simply and effectively that a five-year-old could grasp the basics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain the difference between L1 and L2 regularization.<a id=\"8\"></a>\n",
    "\n",
    "L2 regularization tends to spread error among all the terms, while L1 is more binary/sparse, with many variables either being assigned a 1 or 0 in weighting. L1 corresponds to setting a Laplacean prior on the terms, while L2 corresponds to a Gaussian prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbQAAAG8CAIAAAA5Bcd8AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAEvcSURBVHhe7d0JVFRnnjf+nunJnDn9n5zuk9N9OsecyZvOhH9PT7rfdPe0+mbSb9vTr2bS/3Q6Hd+oncTYiVlMjAkqRnFB3FBRUXAHFRUFXDCiogIC4gYuuKAguC8sCgjUdm/VvVW3/P8uz2OlpG5tQEEt38/JyXl+twopqu791vPc5bnfeQgAAC4QjgAAGsInHNva2g47KSsru3z5st1u5w8/fFhTU0PLm5qaeN0NpaWl9E/xwgf0SiovVJrNZl53Q2trK/3qqqoqXgdYfX09/brr16/zuoe4fhaKovBWr3D3d3V5Jbl79+7+/fs3bdpUVFQkCAJf2it6cMXu5Pz58/QvGwwGXvcQ182nlz99H4VPOJ44ceI7Ln7zm9/cv3+fPWH8+PG05ODBg6zsjn/8x3+kf8r3T/T555+n51+5coXX3UArFv1Tn3zyCa8DbMuWLfTr5s+fz+se4vxZGI3GefPmZWZmsod6h7u/qwsrCUXhn//8Z/ophx/96EcVFRX84cDrwRW7E/Z3Xbx4kdc9xHnzuXr16ltvvRWIZO++cAvHf/u3f9u2bVt2dvbGjRsHDRpES6Kjo9kTenAdSklJSUpKcu6WeoZw7KSgoIDewGvXrlF74cKF9Cuoz8Ue6h3u/i7nF+YLSvbXXnuN/qn//u//phWPDB06lEr6VpYkiT8pwPx9zb4LUDg6bz4vvvgi/QqEY2CxcPyv//ovXj98WF5eTkveeOMNVjqHY3FxcVpaWl1dHXto+/btVDpGQ0eOHJk7dy59fufOnWNLOklPT6fns0+XhvPr1q2bNm1aamrqmTNnNLuTHsKRVos9uXtmzpy5YsUK+hPYwhs3btC/f+HCBXoBCxYsWLlyZXNzM3uoUzhq/jihl7Fv377Zs2fTq3LevWCxWLZu3Tpjxoz169c7/s1O6H2gP5D+TXqvOoXI7du3ly9fHh8fv3v3bsdfSkMkerU03qfn00P0bphMJvYQexlz5syhv6KwsFCv17Pl9A7Tj9y8ebOqqurNN9+kXzFq1Cj6Q7755htaTmNe9jR6ApWHDh1ipStZlukJe/bsoQEg/Zbjx4/TQnd/o4e/y8Hxwqjt7sU7y8jIoH+H8tHxblit1r/85S9vv/12ZWUlW6L5Gd26dYt+Ef06VlJPk0q2t8Td7/X6ZrKS/hH6M+Pi4qh/cOfOHbbQw2fUCYUsrfn0jUWra6dw1Nwu6E2mzYdeDP1S+pfz8/P5A+43DcfmQx8c9bLpVyxZsoS2Vlro/B1ZVFRES2hb4HWvC7dw7N+/P60cly5dOn369HvvvUdLaDNgT3AOx08//ZTaFDTsoV/+8pdUsgF4QkICtf++AzVoDWPPceYYF9AHPGDAAGr/+te//sEPfkCNtWvX8ic5cReOtEo9++yz9NAPf/hD+j+hPi8tp22A2n/4wx/o/3/3d39H//+Xf/mXe/fu0UPO4ejux+mF0cZJJftZQj0aWk7pMHDgQCr/6Z/+if7/9NNP06CGljujZKH3kB79h3/4B/o/e+UsRE6dOvW9733P8eMUamx1//LLL6n805/+9MQTT3z3u9+lNnWjOv6xh8nJyVQ+99xz1KOnxpAhQ2w2Gy13fBa0nVCDoUcXLVpEDdpU2I9TwFHpYcRNfxE94Sc/+cmTTz5JjaVLl7r7Gz38Xc6cVxJ3L97ZmDFj6KEDBw7w2oW7z4h+BbXp17Gn0Z9MJb0b1Hb3e72+mdQuKSmhNn3u7A156qmnWA/Aw2fkjCKMrdv0f3omvXvUZuHobrugf+2ZZ5751a9+9f3vf5+WE/ZXeNg0HJsPvQb1BzpQjP7iF7+gRk1NDT2HHqV1nv6QlpYW9lO9L8z3OY4YMcLRafIlHCm/qPHTn/6U2vSFTyviP//zP7e3t7OnOTg+3bKyMmrQv0xt2izpS1JzS3YXjpTgo0ePpm4FtVkgUqA72oS+q+m303OoPXnyZHrIORzd/fiuXbuoTRsP/RV3796l7eR3v/sdpcOyZctoOb1aGvHR9kxt6q/R853RSk/LKZepz0W9mxdeeIFKFiI0VKQ2DeLox8eNG0dt9qsdGx69A5TgLJWok0LvCfULaBVne/Spi5GSnMIGUI7Pgn5k+vTp1KahFnU06DdS+6WXXqLn0I9TrNC2p9PpqNTEwpFQf4o6X/SPu/sbPfxdzhwvzMOLd8Y+Wcfxsc2bN1PMMfv376cl7j4jd+Ho7vf68mZSm7patM7Tp0DP//jjj2k5dcZpubvPiB5y9sc//pGWU8+U3j0aHFCbUDh62C5Y1FLw0YaWlZVFbfrWpOUeNg3H5kOfeFRUFLWrq6vpOTRCovbixYvpORTT1H7nnXfYj/SJcAtH+thopaHvWFrvBw8eTEuGDx/OnuC8DnUKR9oaqaQPnoZj1KDRRG6H119/ncqjR4+ypzk4Pl0ay7Av0h//+Mf0b9L6pLmnyV04EhoY0mpEXR7KMnoOW7HYVuT4bqfRIpX/+Z//SW3ncCSaPz5p0iRq0/bjeA5rsAEsdQHoT6NthroGtIqzhxy++uoreg778ifsn6I3kzYkatAfTn8j/TiFEZUxMTH0HLbhOXrorIN2+9Ztav/+97+nNm0/b7zxxurVqxsbG9lznD+LTvsc2fZ5+fJlCjtqULKw5Zoc4egYJLr7G939Xax0cH5h7l68M/pQ6DknT55k5SuvvEIl8/nnn7OFmp+Ru3Cktrvf68ubSShx9u7dS1851KGm5bRK00IPn5EDpRu9XbScNgQqafWmjieVFI4etgsWjixnaUhO7d/+9rfU9rBpODYfajvvc6TUpjb1H6k9c+ZMaufl5ak/0EfCeZ8jvfv0TUsLa2trqXQNRxqDdDzxIRun0DqRmppKjX79+tGIwMF1n5fzp0u/96233mLrAaFEZs9x5i4cqcdBv4seotUuNjaWGvRP0XIWjo5uHa1nVNL3NrWdw9Hdj7OeZk5OjvrDTujNoeW08vE/rIMoivzhDh988AE9x7FSLl++nEoKkfr6emrQmJT/WAf6pfQctuE5hpa0bVBJvTNq01tKMeQYUdK2x5Z7CMcdO3ZQmZiYGB8fTw0POxwJC0fniHf3N7r7u1jp4PzC3L14Z1OmTKGHKKpYSelAH8q8efNoIQtHd58RC0d66zp+7iG9EipZOLr7vb68mWwFpmHsZ599xg4NUW+Olnv4jBwovGghjWQdKcZ2UFA4etguWDiybeH27dvUZt/ixN2m4S4cCXV7qbx06RIlO43Te+Tsty4L53DU6/W02dDCs2fPUum8Dk2YMIHae/aogx0ab7JPi1Y+6t5Tgw18CPXtKdGsVisrHRyfLv3I4cOH6Tm0lVLjZz/7GS2nf4Q/7xF34ci2WLYPm40jnMORVg62U2n79u2Oh5zD0d2Psy2NdesIDa/GjBlD3+2UOLScjbNoA6Axe0NDA3uOA+sS0v9Z+Ze//IVKFiK0HtOWQO8qtWkzoHeVDfHYhkfDbfUHnDY8iiQaVNLbQm8UZQS9DFpOUUjPcf4s2H7GDRs2dPz0Q+oD0qCPfhe9adTpcHR7NbFwpPEmrx8+dPc3evi7nDlemIcX76y4uJiW0wtwHCymF0zvPC1k4ejuM2Kfo+P7j+27pHB093t9eTPp3aAPiLp7rB8dFxdHy53D0fUzYqUDW1EvXLhAbTZWIBSOHrYLFo5suXM4etg0nMOR7Wd09IL3799PJQ3/6f/0TcAW9pVwC0cKlGEdaPBCqywt+dWvfsW+CV2/YOmLcdu2bezYBaGPk0YWzz77LH3X0VZEayo1fv7znxuNxo7f8C3Hp3vu3Dlq0AdMowaKsGeeeYb6Vo6vQQe2zr366qvstTE0FmO722jERxsPGzSxcGfhSP7whz/Q1k4ZQW0aK9FDzuHo7sdppadVll48bRJs5+C7775Ly9nGST3QjRs3fvHFF9SeNm0aLXdGazMtJ/SDbIslLETYSIfeLvpL2biMRou03N2GR1/79OZTe9WqVfQKnTsyzp8Fdbuo/dprr6WlpXX8Aw+jo6NpCZkxYwZbQm81vbHUaWKlAwvHp59+mtfu/0YPf5czxwvz8OI7WbFiBT1Eq8Q777xDmfXcc89RSZYtW0aPuvuM6urqqE1/1Pr16xMSEtihM1rl3P1eX95MSiv222mtpoEwO3pGqzo9x8dwZMd8qONJa93LL79MbULh6GG7cBeOHjYN53D83e9+R+2vv/6aVmwqaVNlY3lCXwa0pA+FWzg60GdG4Ujf3o6zGZw3SPpqfeONN6ikz4kGQawzT+FID129epXtSKL19Y9//CP18Dt++jHOn252djZb6ckLL7ygOQxk4dgJhR2tTOxLkoZItF7SCkSdJtrgWTi+//77rIND6+LixYvZkSXncHT34/QQfVezX/rkk0/Skx1vAm027DuDNgB6c6jXzJY7o7WZHev8zW9+Q1sCNViI0L9MyUKrOC2hbcMxEPaw4V2+fJkig/Xf6Qdp02JvmvNncf36dZb+9G+qP99xWJxK4jgbhn6KSnrNrHRwDUfi7m9093c5c35h7l68q82bN9MnxdYKSnD2zcce8vAZUcneTHoCRR416FXRcne/15c3c0/uHraPiL4e2LEptqr4GI70XrFBFaHPmlCDwpEecrdduAtH4m7TcN586K1jT6A4Zo+yrxP6K9gK34fCJxy7gAYOjt0rndCAkY0ZfUSDzQcPHnTt46SX0WnwyMKRDSvon9WMMAfXH3dobm7WfIi+wF3PSnFGj7o7C5K6J+4ecod+hMa2rnsnHOhFsgOsrKSxMP351FFiJUMjMurp88IHmn+jh7/LHa8v3oH+Cvb96srdZ0SfrOu5EIy73+v19dBK6O/f2Al1Hdyt/P5uF8TrpkFPaGtr48WjXV5JSUm87jsRHY5ByzkcIwqNxWJiYtjAKiMjgy99+JAGlTRAo+EkryEcbdmy5b2Oc5OpN0pflnxp30E4BiMaO/fv3z8Yvjx72YkTJ/7+7/+eBr/Tpk3r1O+j3gdvQZiiD52SkQbUfXsGjwPCEYJLn+9pgj7kbq9un0A4AgBoQDgCAGhAOAIAaEA4AgBoQDgCAGhAOAIAaEA4AgBoQDgCAGhAOAIAaEA4AgBoQDgCAGhAOAIAaEA4AgBoQDgCAGhAOAIAaEA4AgBoQDgCAGhAOAIAaEA4AgBo6NVwLCgoOAAAEBxOnTrFs0lL74VjYWHhH//4x20AAMFh4MCBtbW1PKFc9F44Hjx4kF4NLwAA+lpCQsLly5d54QLhCAARCuEIAKAB4QgAoAHhCACgAeEIAKAB4QgAoAHhCACgwe9wTE1N7d+/f1RUVHp6Ol/kZMiQIT99xK+wQzgCQFDxLxwLCwsp/lpbWxsaGgYOHFheXs4f6KDT6X7wgx80PmIymfgDPkA4AkBQ8S8cJ0+enJ2dzdrUhYyNjWVt5tSpU0OHDhVF8dq1a4qi8KW+QTgCQFDxLxxHjBhRUlLC2rm5uaNHj2ZthgbaL7zwwi9/+csnnnhi8ODBRqORP/C4ioqKdS5GjRpFycufAdBVimJvavXvixlAk3/hOGjQIMdQOj8/f9iwYazN7N69e+XKlXa7ncbdr732Wk5ODn/gcbdv3z7u4qOPPvrwww/5MwC66k6j8osPTRSRvAboKv/Ccfjw4UVFRaxN2RcdHc3armiMTN1MXvhg7ty5X3zxBS8Auqqi2vpajPCgHeEI3eVfOE6cODEjI4O1U1JSlixZwtrMoUOHrl69ytqZmZljxoxhbV8gHKFH5B2Txy0RL9+w8Rqgq/wLx9LS0sGDB+t0upaWlgEDBlReqKSFjY2N9fX11Ni8efOoUaNoWC0Iwuuvv75nz56OH/IJwhF6xOZ9ltiV5tIKmdcAXeVfOCqKMn78+H79+kVFRaUkp7CF8fHxMTEx1KBMHDt27MCBA59++ml6miRJ7Am+QDhCj5i73rJul5R10I91D0CTf+HImEwmD8Fn7sALnyEcoUeMWSBSt3HpVguvAbqqK+EYCAhH6D5Zto+cJTQ2K5OW+/31DNAJwhHCR9195evlZqvV/k6cYLfjgDV0C8IRwkdFtTU5Sx1Qf7FYxNk80E0IRwgfB47LOw+px6kXbrLgbB7oJoQjhI91u6UjFVZqbNorFZ/C2TzQLQhHCB+z0sxXb6sdxoIyOfMAzuaBbkE4QvgYs0A0mNRdjRevWpO24Gwe6BaEI4QJs0U9j4e17z9QopNE1gboGoQjhInbDcrUVfz0RkVRz+bB3DzQHQhHCBOnq6yrd3y7n3H8UhETO0J3IBwhTOQelncVf3uEeskWS+UV9cg1QNcgHCFMrNkplVd+m4aZB6T8EzibB7oO4QhhIm6t+XbDt+Pow2fkzftwNg90HcIRwsT7swXR/O0RmJqbtnkbcDYPdB3CEcKB3mj/eP5j5+606e2fLcTZPNB1CEcIB9fu2Oau79xPHDVbkGSczQNdhHCEcHCkwrp+d+c9jFNWmO804mwe6CKEI4SD7HzpwPHOx6ZXbpdOXcLZPNBFCEcIB0u3Wi7Uds7B3SXynlKczQNdhHCEcDB+qXivpfMIurzSSp1HXgD4CeEIIc/dldS3G5TYlbiZDHQRwhFC3v0HysRkjRCUOu63xQsAPyEcIeSdq3E7e+NnC8U2Hc7mga5AOELIyzsmbyvQ3reYkI6byUAXIRwh5K3fLR07p33KzpY8qeQ0DlhDVyAcIeTNTDVfu6PdPTx0Us7IwwFr6AqEI4S8v80RTKL2jsXq67b56Zh+AroC4QihzSh0nnLCWZvePnYRpp+ArkA4Qmi7css2K83TyYzvxQsWCQeswW8IRwhth8/IG3I97VWcuuqxSXABfIRwhNCWdVAqLPd0PHptzmO3TwDwEcIRQlviZsvFq56yb2+pnHMIZ/OA3xCOENrGLhIftHvapVhRbV2WhQPW4DeEI4Qw2Yerp++1KJOWY/oJ8BvCEUKYL/PuuJuzB8AzhCOEsPJK65qd3i+AmZhsdp3tEcAzhCOEsJ2H5L0+zPW9LMtSUY0D1uAfhCOEsKWZlrM+pF6ObxkK4AzhCCGMxsv3H3gfL/s4+gZwhnCEUGWz+Xqk5U6jMmUFDliDfxCOEKoampSYFJ8iT5Lt784U7HYcsAY/IBwhVJ2psq7Y5utgOTpJbGlDOIIfEI4Qqr4pkXeX+HqYZckWy3mXG1sDeIBwhFCVkm2hziMvvNlRKOcdxQFr8APCEULVpOXmxmZfT+0uu2Bdm4MD1uAHhCOEJN8PVTO4wT/4C+EIIamhWfnan+kkfJmiAsAZwhFC0qlL1uXb/JuI7MsksakVV1iDrxCOEJJ2FftxqJpJ2mI5V4MD1uArhCOEpOQsy9nL/iVdziEcsAY/IBwhJI1fKvo7C1l5pXXVDhywBl8hHCH0yB2XA/o7f+3de/4dw4EIh3CE0FN3X5m6yu+YozB9Lx5TgoOvEI4QesoudHEKsgnLxAafzxuHCOd3OKampvbv3z8qKio9PZ0vchEbGzt16lRe+AbhCL7beUjef6wrh1ZWbJMwJTj4yL9wLCwsHDJkSGtra0NDw8CBA8vLy/kDTo4dO/bUU08hHCFwFm22VF7pSsbtLpF3FeOANfjEv3CcPHlydnY2a1MXknqIrO2g1+tfffXV5ORkhCMEzueJXu5V7c7ZauvSTNzDGnziXziOGDGipKSEtXNzc0ePHs3aDuPGjTt8+PCGDRs8hKPJZGpyMWXKlDFjxvBnALgnmu1/m9PFCwGbWpXoJJEXAB75F46DBg1yDKXz8/OHDRvG2kxBQcHYsWOp4Tkc6QejXfTv33/o0KH8GQDuXbtji0/VOFStiKJUeU4oOijs2ykWHZBqq+zWzkNvu109YC3JOGAN3vkXjsOHDy8qKmLtnJwcCjXWJjSgfu655w4ePHj69Onp06ePGjWqpqaGP+YDDKvBRyWn5Y17HztUbWtuMmaktn09xrh5LYWj+fhh4UCufk1S27RxQl6OIpj48zpMW22+WYcD1uCdf+E4ceLEjIwM1k5JSVmyZAlrk9u3b7/8yLPPPvujH/1owoQJ/DEfIBzBR5v3SZSPvKBRdnF++4xo89Eiu9z5SItiMgp5uygipfNn+KKHD9fvlo6exQFr8M6/cCwtLR08eLBOp2tpaRkwYEDlhUpa2NjYWF9fz57AeB5Wa0I4go9mpZmv3LJRwy5ZTDu2GNKWdeobdmK716hLmCoWH2RlYbm8JQ8XEYJ3/oWjoijjx4/v169fVFRUSnIKWxgfHx8TE8PaDMIRAmfUbEE02+2Kol+5SMjdxpd6RJ3K1rHvmrZvpnbNLVtCOg5Yg3f+hSNjMpkkqYe/exGO4IsH7fbPFooUdvrVS9qmfcmX+sBusz34aKh4uMBgsn84FweswbuuhGMgIBzBFxdqrYszLMaMVGHPTr7IZ5SP+jVJYkn+JwvENj0OWIMXCEcIJXlH5eNLcw1py3jtJ8Vi1iVMzVh05tI1HJMBLxCOEEq2rrh4c+LXitj1cbHtXsPlT6ILDjbxGsANhCOEDMrEY+9MvnfuGq+76mJ2adlXi3kB4AbCEUKGfmv62tHbuz8hY/195cBHSebjh3kNoAXhCKFBvlpTHz9zxkojr7uB4nVMbGPrrK+V9ja+CMAFwhFCgN1q1SXGndxXuyG3Z84hm7baXL+3yJiRymsAFwhHCAFiSb5p26bN+6RDJ3tmNsa0XdLRCkm/dI501e3aDxEO4QjBTjEY2ud8rejbZ68zX72tXjjYfYXlcuYBSb5+Rb9klt2Ocx5BA8IRgp2wO1s8lEeNkbPUCwfZwm6quWWbs169iNC4aY3l1HG2EMAZwhGCmq25STdvil2yNLcqX/XcPLUUsh90zJhra2rUzYt1ndEHAOEIQc20bZO5rJQaFdXWZVk9OWHE2EViS5vaDzXt2GI+WswWAjggHCF4qd26hKlsQu9dRXLu4Z7s3y3NtJztuBOh0trSPneyXcY8ZvAYhCMEL1NWOus2kiVbunjHQXcoar8p4WmLziO4QjhCkOJ7Gx/dB+bTnp5K51yNNWkLH6d3+l0ABOEIQcqUs9V85BBrG0z2j+f38CSMbTr72EXf/pvGzPWWkzhsDd9COEIwUgz69lkxdon37C5dsy7Y2PPTd38wRzAKvDdqbajTLZjB2gAE4QjBSCjYKxzI5UXHNI7bC3r+gMn8dEuV08SOhrRkqeo8LyDiIRwh6NhlmV0Sw+uHD1dul05X9fwOwR2F8v5j3x4Bl2qr9au/vaEmRDiEIwQdy6njpqx0XnT4Kkm8/6DnbzZ98qJ1xbbHOqS6RTOt9Xd5AZEN4QhBR58023rnJi8oKyX73zquZulxTa3KhGWPHecxlx817djCC4hsCEcILvKNq/rlC3jRwXEddI+z2+3vd9zolde0RLK0z4pRTD0waySEOoQjBBfj1nWWipO86JB/Qs46GKjLV+aut1y59dhMP8KenWJpIS8ggiEcIYgoBl377EmdpoFYtUMquxCo07MzD0gHjz/269RrFhfMwDxmgHCEICIWHxTycnjxyMRkc2Nzzx+NYcorrWt2du6W6tckSbXVvIBIhXCEYEGdNeqy2Zrv87qD2WIfOUsIXD+uoVkZv7TztTfShQrcQQEQjhAspKuX9WuX8uKR2lu2WWlmXgQAxS6FL0UwrzvYrVb1sIxBz2uISAhHCBbGzPXSudO8eKSgTL2fAS8CY+56C0UwLx4R9uKwTKRDOEJQUARTe3yM66SKAT0aw7gekyHWxnr94nheQERCOEJQMJeVCruzeeFkwrKAXBvjrLzSShHMCyf6lPnO56JDpEE4QlDQTCLRrJ6kHeizau61aN+dxlx+RPgmixcQeRCO0PdoDKtLjOOFk5qbtoT0gFwb08nf5mjc11AxGdtnTsDtEyIWwhH6nrD/G7EknxdO8o7J2fm9kU3zNliqr2vcEduYkSZVnuUFRBiEI/Q93bwpSlsbL5ws32ap6LgHVqDtPCRTEPPCiVRdaUhfxQuIMAhH6GPy1RrX0xuZzxP53VMDjSJ4aabG+N1us6knPBoNvIZIgnCEPmbascVy+gQvnBhM9o8Sevi+Me606uwUxLx4nLA723EHRIgoCEfoS3ZZap85QRE1pmu8UGtdtLk3jsYwH88X27XubijfvKZfsZAXEEkQjtCXpIvn3F3FvKtI3v3ovtK9YMkWy7ka7f2buvnTlNYWXkDEQDhCX6JkpHzkxeMSN1suXu2NozHMviMyxTEvHiccyBWLNQ6mQ3hDOEKfUcxie/zETrM3OjjfN7UXXLpmne/mnEprw1390jm8gIiBcIQ+Y6ko73QjLYemVo2ZxAJKNHu6U40uMc52r4EXEBkQjtBnDBtWSpcv8eJxJy5YV2td7xxQFMcNbmbVFQ/l0X+8gMiAcIS+oYiCOg2PVXuvYkaedOhk7x2NYdJ2ScfPa78e2/1GzQscIYwhHKFvWM6UebgJ6rTV5ht1gZ2Mx1XxKXlDrtvuqn7pHGsDbmkdQRCO0DcM61KkmipePE6W1cl4bLbeOxrD3GlUpqxwO+u4WLRfyN/LC4gACEfoA+rUtrNi7DaNuR7ItTu2uLUBvDWCO4pify9esEjaoazelRAj60iCcIQ+oB6ndj+mzj8R8FsjuKN5ywQHfdJsa2M9LyDcIRyhD3g4Tk2SsyynLvXe6d/OthVI+464PRCkjqwL9vECwh3CEXqbIortcePdnftNPlsotmld5twLztVYl251e0G37V4DRtaRA+EIvU06d9qUvZEXLh6028cs6NXTv53pjfYP53r67erZ4E2NvICwhnCE3qZeT33pPC9cnK6yLt/We5PxuPoySWxqdXsWkZC/F9dZRwiEI/Qqu2RRx9SS2/jbkifln+jt07+drd7h9lRwYr17W79sHi8grCEcoVdJVefdzVHGTF/TB6d/Oys5LW/c6+lYuS5hqtLWygsIXwhH6FWmbZssZ0/xwoUk20fO6oPTv53daVQmLfd0lqWwd6f5+GFeQPjyOxxTU1P79+8fFRWVnt55PhVFUeifo0dfeuml7GyNG7R7gHCMBHarVZ33WzDx2kXtLdvc9X25w5HRvFOrg3zjin71El5A+PIvHAsLC4cMGdLa2trQ0DBw4MDy8nL+QIeMjIyRI0dKktTW1vbcc895+HddIRwjgXT1siFtGS+07C2VdxT25Q5HZn665dI1t7sd7YrSHh+jGHDXrTDnXzhOnjzZ0SWkLmRsbCxrMxSLgqDOiGexWPr163funPYMz5oQjpFAyN1uLj/CCy2LMyyVV/rm9G9nuYflXcWeMlq9KdiZMl5AmPIvHEeMGFFSUsLaubm5o0ePZm1nBQUFr776anR0NI2y+SIfIBwjgW5erNKucX9qxm5Xp5s1iX25w5Gpvu5ldC9VXzRsXM0LCFP+heOgQYMcQ+n8/Pxhw4axtrO9e/d+9tlnv/3tb2/dusUXPW7Hjh1vuXjhhRf+/Oc/82dAOLLevaVPTuCFloYmJSalD+abcCV1TAukKG5jWj0haUa0hxOSIAz4F47Dhw8vKipi7ZycHOoesrYrGoDPnDmTFz5AzzHsCQX7PJ8+XXxKXr+7b+abcDV1lZczitRT2asu8ALCkX/hOHHixIyMDNZOSUlZsuSxY3Z5eXk1NTWsvXXr1rfffpu1fYFwDHv6pNmeb8OyeodUdqHvdzgymQe8nItuqTjpYWIhCAP+hWNpaengwYN1Ol1LS8uAAQMqL1TSwsbGxvp6dR6nzMzM0aNH2+12s9n8+uuvpySndPyQTxCO4c3W0qxLmMoLN8YsEB+09/0OR6ai2tMMFEQxGNRbJ/qzYx1Ci3/hqCjK+PHj+/XrFxUV5ci++Pj4mJgYalAmjhs37uWXX+7fv//8+fMtFj/2yCAcw5v5+GFh305eaKFYHLuoz+abcGUw2UfP8/J69GuS5JvXeAFhx79wZEwmkyS53TckiqLNzQzPHiAcw5shdZl8/QovtJy4YF25PVh2ODLRSWJDk6eOoflokbD/G15A2OlKOAYCwjGMqTdFmDnB3U0RmPQ9Usnpvj/92xm9pMNnPL0k9cYJCzG9Y9hCOELASefPuLt5v4PXblrv86Uzq07v2HyPFxBeEI4QcJSM0oUKXmgxmOwfzw+iHY5Mq87+2UIvr4qG1TS45gWEF4QjBBaNptvjxiuiel2pOxXV1pTsYDyh+ovFYrP7iW8JJqEIYwhHCCyKD0Oqp8kmyKa9UmF5cO1wZNbmSEcqPJ162TEJxUTP0Q8hCuEIgSXs22k+WswLNyYtN9fdD8YTBo+etabt8rLb0ZS9UTp/mhcQRhCOEFi6BTNszfd5ocVgsn8wR7Dbg+X0b2dNrcrniV52O0oXKoxb1/ECwgjCEQJIPdllkZdL7NVrUTKDdwYHCseWNk/Brd5pFpfKhCOEIwQQDaiFvF28cCNodzgya3Z62e1IvJ7iDqEI4QgBpF+71GtqxKQE6Q5H5tg5K+UjL9xQL5XxeHEkhCKEIwSKL+NNvVHd4ciLoERj6jELvOx2tDXf1yXiUplwg3CEQJEqzxoz1/PCjVOX+vgW/r4Yt8TTbf4Z3cI4W0szLyAsIBwhUEzZGz3chZXZkCsVnwreHY7M+t1SaYWXF6ner/UYv4MIhAeEIwSE3W5vnxWjmIy8doM6Zfdagv04b3ml9wt45Ks1hrRkXkBYQDhCQFjv3NSvWswLN3y5eDkY6I32jxK8vE71ltzxExVLUNwDB3oEwhECQji4x/MdY8iRCuvanOCaw9Gdicnmu/e89HDVu8pcOs8LCH0IRwgI/bJ51oY6XrgRVDeN8Wzrfi+3lCGWM2WmnbirTPhAOELPU9rbdPNieeEejVXb9MF41aCr87XWxM1edjsq+vb2uZN5AaEP4Qg9z1J+TNiVyQs36u4rNFblRdCzSPZRswWbzUuU65MTrHdv8wJCHMIRep5x81qp+iIv3KBR6pa80NjhyMxKM9fe8nJzJLEwTyw6wAsIcQhH6GF2WVZnt/V23HbhJguNVXkRCr4pkXce8rLbUT1Gv3wBLyDEIRyhh0lXLxvWr+CFG1arOkqlsSqvQ8H1Otv0NV4SXz27c/YkxWjgNYQyhCP0MPVakbJSXrhx+YaNRqm8CBGKYv/bHMEoeAl00/YMS8VJXkAoQzhCD9PNn25rbuKFG9sKpH1Hgv2qQVfJWZYzVV52BWDu27CBcISeZGu+53V2WzJlhflWfbBfNeiqtEJet9vLQaSOuYhiMPdtGEA4Qk8yHysR8nJ44YbOYB89TwzO+yJ41tJm/9Tb9GVEncXy5nVeQMhCOEJPMqQtk6/W8MKN4+etq3aE0kk8zsYvFRuavPQKxcMFwoHdvICQhXCEHqNYzO0zJ9htXk4GpGQ8ESJXDbraul86eNzL3lLbvUZ90mxeQMhCOEKPkaouGDNSeeGGoqhTf+uNoTemZi5etc7b4H12Xl3CVKWtjRcQmhCO0GNMO7dYTh3nhRs36pRpq0N4Xi9ZVs/QlGQv4S58k2UpP8YLCE0IR+gxunlTFJ2X7tKuIjnH23UmQS5xs+XsZW8n9NRUmXK8XF0OQQ7hCD3D2lCnT07ghXvUbbx+18tOySBXdFLeuNfLASW7ZFF3v8qh/TUQ4RCO0DPE4nzh4B5euGEweZ9SO/g9aPdpAnPDuhSptpoXEIIQjtAzjFkb5Ns3eOHGkQrr6pA9icfZ+KWi14nBzScO42bWIQ3hCD1AEUzqKNLbZSEp2ZaTF0P1JB5n2fnS3lJvJ/Q0N+kWzOAFhCCEI/QA6UKFKXsjL9yw2dSTeERzqJ7E46zmli0+1fsxd11inK35Pi8g1CAcoQeYtm+Wzp3mhRtV16whNxOPOz7O0CPk5ZiPH+YFhBqEI3QXjabVu5IavExiuHW/dMDbtSUhxJfrfNSpLdOW8QJCDcIRukud/npFIi/c+2JxCNy/33e+3OlfvZn1zAl2yfsVNRCEEI7QXeqNU4oP8sKNuvvKV0khfxKPM9Fsf3+2YLV6GVmrN7OuruQFhBSEI3SXeovquju8cGNvqby9IBxO4nE2b4Pl0jUvI2vLyeOmnK28gJCCcIRu8fFmzdPXmK/dCe0LY1wdOilv8napDL0/unlTeAEhBeEI3WI5c8K0cwsv3GjTq7PbKko4nMTjrKXN/okvc99Sz7qhjhcQOhCO0C3qPrVL53nhRslp73cXCFGxK80367wcZRIO7hGL83kBoQPhCF1nt9naZ07w5RbVlVfC4cIYV7mHvd/MWr59Q79qMS8gdCAcoevka7Vez+MTzeqFMV6P6oaouvvKhGVeRtYd54HGKIKJ1xAiEI7QdcK+neajxbxw4/h57+cDhrRxS7zfVcaUle71CiIINghH6LqOa4e93KJ6aabltLd7PYe0bQXSHm+TUEjnz3i99hyCDcIRusiXWWckWR1TW6TwHFMzN+qUKSu87HX1cdYiCCoIR+gi8/HDwl4v8xWeumRdsiX8L54bs0BsavUSfPo1SV7nu4SggnCELjKsS5Gv1fLCjZRsS+jehdV3W/dLece8jKzVm1l7mykdggrCEbpCMYvtceM93yNFlu0fzhXDYwJHz67dsXm9paK1sV6/ZBYvIBQgHKErpKrzxow0XrhxpsqaFAFjaoZG1s3eRta6ebFKWysvIOghHKErTDlbLWdO8MKN5CzL8fPhP6ZmtuRJ+454GVkLudvN5Ud5AUEP4Qh+s9vt7bNiFH07r7Ww49SRMKZmrt+1TV3lZWQt1VQZ1i/nBQQ9v8MxNTW1f//+UVFR6enpfNEjNpstMTHxlQ7Lli2zWv3oNSAcQ4j17m19ynxeuHHyonVZVmRN8jp2kZdj1nZZ7riZdXheZh5+/AvHwsLCIUOGtLa2NjQ0DBw4sLy8nD/QIScnZ+TIkUKHd955h0r+gA8QjiFELNovFh3ghRtLt1pOXYqUMTWzvUDKPexlZG3cvFaqvsgLCG7+hePkyZOzs7NZm7qQsbGxrM1UVVXdvXuXtRcvXhwTE8PavkA4hhB9cgJ1HnmhRRDto2YLNLLmdWS43aCMX+rlOmt17ltvM7xBkPAvHEeMGFFSUsLaubm5o0ePZu1OzGbzz372s/x87WmaKEN3uBg2bBj1OvkzIIgpujavs7ceqbCuCoub9/srOkm80+hpZO3LuwdBwr9wHDRokGMoTdlHicbazmw228cff/zBBx/w2sWVK1f2u3j33Xfff/99/gwIYtT3Eb7J4oUb8zaE7RxlntGwepu3u0HoU+Z77ndDkPAvHIcPH15UVMTaOTk50dHRrO1Ayfjpp58OHTpUkvzrOGBYHSoM6as83zGqXW//KCEM5/32RVOr8nmil5G1ej+yQ3m8gCDmXzhOnDgxIyODtVNSUpYsWcLajKIoX3311ZtvvimKft9nDuEYEh4db/V02OHgce93VgljM9aYa255uluOL8f6IRj4F46lpaWDBw/W6XQtLS0DBgyovKD2IBobG+vr66mRlZX1yiuvGI1GSkmm44d8gnAMCdLlS4aNq3nhxtfLw/BeWr4rLJfTdnn5bmifO1nRtfECgpV/4Uh5N378+H79+kVFRaUkp7CF8fHx7MD0iy+++B0nfh1gQTiGBNPOLZaTx3mhpf6+8mV43Z/aX0ZBvaLc88znwq5MS/kxXkCw8i8cGZPJ5O8uRa8QjiGhfc7XikHHCy3bCqTdJV7O9Qt7SVu8nOOpXirjrQMOfa4r4RgICMfg53Vnmd1u/9SH+RfC3pkq6+IMT1cHqbtu4yfapci6gijkIBzBV0LBXrFoPy+0XLpmnb3Oy/XFkYDG1KPniQaTp5G1eqlMlZdb2kLfQjiCr/RL51ob1SNv7izfZjlSEYmnN7rauNfL9LeWinLTDlwqE9QQjuATpe2Bbv40XmgRzeqBiPC+XYzvbtXb4tZ66kQrBkP77El2O96u4IVwBJ+Yj5UIuTt4oaX4lLw2J3JPb3QVk2K+Uedp96t+RaJ88zovIPggHMEnhrRl8vUrvNCyNNMSyac3uso/IW/I9fRtYT5ySDiwmxcQfBCO4J0iCu3xMR7uLHr3nhId2ac3ujIK6mEZD1MT2Zrv6RLjeAHBB+EI3knnThuzNvBCy+Z9Ut7RSD+90VVKtuXYOU9HqHQL42xNjbyAIINwBO+MW9dJled44YLdZdDzmSuRqeqa1fNhGeFArni4gBcQZBCO4AU7Y1kxux01Hz1rXZqJ85k12O32sYvEuvtud0dY79zUr0jkBQQZhCN4IdVWGdJX8ULLzFTzpWs4vVFb3lE5I8/tYRn1VmXersiEvoJwBC88TzZB3aJxS3Aoxi2DycthGWFXJu7XGpwQjuCJ2rWZPUkx6HntwuulIOD5wiGpthr3aw1OCEfwRL51Q79qMS9cWCT1UIxRwKEYT2puerqltd1mU3fp+j8/NAQawhE8EfbuNB/lN8ZwdeikvGYnrorxbmKyp6tljJnrpXOneQFBA+EInugWzFBaW3jhYsIy0fMVcsAUn5JXu78do3TpvDEjjRcQNBCO4Ja1/q4+OYEXLi7fsE1bjQnKfGKR1MMyeqP2/ge7ZFGnd5TRBw8uCEdwy/MEjklbLCcu4AweX23J8zRHunHTGkzvGGwQjuCWbtFM2z3ti9uaW5VPFog2Gw7F+KqpVfl0gds71loqTpq2beIFBAeEI2iz3W/0MC3C1v3SNxF/rxh/LXHf11ZEUZ3aw4qeeBBBOII2sThfyN/Li8eJZu+3AQBXl2/YYle63UtrWL9Cqq3mBQQBhCNo0y+dY627w4vH5Z+Q132Dowdd8fVyt7f8t5w6jhsnBBWEI2iwNTfpFmqPqRVFnUyhoQln8HTF8fNWGlzz4nGKydg+y9OkmdDLEI6gQTxcIBzI5cXjyiutiZsxB08X0VfLmAViY7N2AhrSkuWrNbyAvoZwBA36ZfPcjaknr3A7MARf5B1zu1PCcvK4KWcrL6CvIRyhM3VMnTCVF4+7dM3q4TJh8IVotn+UIOoMGoez1FsSxk/EyDpIIByhs44xtfaNn+ZtsJyuwukm3bW9QMrO1+480shauup2g4TehHCEzvTJCZpj6ut1tugkEbda7j69UT0XShA13kl1ZL0Tx6yDAsIRHqPeEm/+dF48bskWy9Gz6Db2jE17ta8m5CNrnA0eBBCO8Bix+KDmud937yljF7m9+g381apT9zxqzhBuWL9cqq3iBfQdhCM8Rr9klrWxnhdOUrItRSdxvWBPWveN9v1sLWfKTNszeAF9B+EI37Lda9AvjueFk4YmZcwC0WpFt7Ensck7XDuPHddZT7TL+CrqYwhH+JY6R1nxQV44Wb7NcgjdxgBI3aXdecQMZsEA4Qjf0iVMtTXf58Uj9ffVbiNmJwuEplZFc8+jdP4M5gbvcwhH4NQbzKfM54WTZVmW4lPoNgbK+t3SviOd3167LLXPnKCYcdetvoRwBK7jXlrFvHjkVr3yxWIcpA6gB+3qYWvR3PkdNmVvtJw9xQvoCwhHUNkVpX3O14q+ndePLNxkOXYO59wF1uZ90o7Czp1HqbbKsC6FF9AXEI6gUjfFtGW8eKTmlm3CMlwSE3B6o9p57HT7LfV+1rNiXL+uoNcgHEFl2rbJUlHOi0dmrDFXVKPb2BtyDskb93a+2lrI3WE+VsIL6HUIR+C3BlUsj023c6bKOjMVE/D0EtFs/2SB2NT62Hw87g6RQe9AOMJD6dxpY9YGXnRQFPtXSeLV25i3sfcUlMnJWZ1nEdYlxtnua98AEgIN4QgPDetSOl3Me+ikvMxlQ4WA0vxCUi91P7iHF9C7EI6RTmlra589yXmCVUHUGOJBL6iotk5b/diuDKW1pX3uZEx/2ycQjpFOndo2bxcvOmQekOg/XkDvmpVm7nRva/3apZj+tk8gHCOdLjHOeRqe+w+Uj+eLZgtO3+kbdxqVzxaKstMFhZaKclNWOi+gFyEcI5p8+0an46ELNloOn8HFgn1p/e7HzglXLOb2+BhFxKWEvQ3hGNGEXZnmslJePHx49rI1diVO3+ljRsFOnXfnfb6mHVssp47zAnoLwjFyqbMbqF0SgZU0lBu7SLxeh9N3+l7RSdn55uDyzWv6FQt5Ab0F4Ri5LBXlxsz1vHj4kIZy7u6nDL3MbrdTF9758iTdwjjbvQZeQK9AOEYu/arFjsOgjc3qrNQ0oGMl9Lkbdcrnid9O9aieVLB3J2tD70A4Rihb833d/Gm8ePgwPtV8pAKXUQeXjXuljDzel1cM+vZZMbgrYW9COEYoIW+XWJzP2qUV8pz1uB4m6Jgtduo83qrnR2aMGanShQrWhl6AcIxE1AFpnz2JTYelM6jHRu+14BqMYFRRbZ2ywswmG5Zqqw2pnaeVg8AJSDjabDZ/JwFEOPYm6oA4blGydKsl7xhObAxeyVmWvaX8A9LNn+56kx8IEL/DMTU1tX///lFRUenp2mftt7a2/vSnP62rq+O1bxCOvUm9Iq22mhrlldapq3jHBIKTwaRe6l5/X+3ai8X5Ql4OWw6B5l84FhYWDhkyhOKvoaFh4MCB5eWdp0fdt2/fyy+//N3vfhfhGLRs9xt1C2ZQg7a6jxLEuo6tDoIZfYfR4JpGY4rBoM4Sglta9wr/wnHy5MnZ2dmsTV3I2NhY1mZkWR42bNitW7eeeeYZhGPQUueXPlpEjcUZFtf73kFwWppp+aZE/bCMmestZ8rYQggo/8JxxIgRJSV83vbc3NzRo0ezdieew9FqtZpdzJw5c+zYsfwZEDD8Ql3BdKTCGrcWVwqGDEFUj5vdqlfkm9cxPXjv8C8cBw0a5BhK5+fnUz+RtTvxHI67d+9+38WLL7745ptv8mdAwJjLSk3bM5pa1VO+MWNjaKm8Yo1OUk8L1y+ZZb1zky+FgPEvHIcPH15UpI7ISE5OTnR0NGt3gmF10NIvjpfr7lCfEad8h6JNe6X1uyXLqeOYxKwX+BeOEydOzMjIYO2UlJQlS5awdicIx+AkXb2sX7V4W4G0NBOnfIckm81Onccz50X1NFWDni+FwPAvHEtLSwcPHqzT6VpaWgYMGFB5oZIWNjY21td/O1sqQTgGJ0P6qtr9p8YtEUUzzt0JVfX31dmIm3N2CwX7+CIIDP/CUVGU8ePH9+vXLyoqKiU5hS2Mj4+PiYlhbQbhGIRszfebZ8V+PM+ESclCXfEpec7ixrY5k3FOT0D5F46MyWSSpB6e2wrhGGimXVmZsQf242KYsJCSbTkxLc1y+gSvIQC6Eo6BgHAMKMVoqPpkwsrN6sXUEAYk2Z44t/Z67ExeQwAgHCNCdfrezC+yLRJ2NYaP+w+Ufe8tqD+u7veHQEA4hr+6OnPp0PGN1x/wGsLFxf2V+0cuEER85wUEwjHMGQX7yujCayk4LS48XYyOX5FY4+8kWOALhGM4UxT7zNXC5S+n2O438kUQXqRzpw+PX76747Jr6FkIx3C2aod0dMMx4+a1vIawY1eUtoRpiQtvFJQhH3sYwjFs7SqWZ6WZ2xNnWuvv8kUQjiwnj7du3DB2kXiuBpeE9iSEY3g6UmH9MknUnzpjWL+cL4IwZbdadfNi66sbR88Tr9/FGf49BuEYhiqvWGk7aW5V9IvjMX1LJDCfOGzK3nj5hu3DueL9B5hsqWcgHMPNtTu2v80R7t5TpAsVhnX8Ek8Ib3ZZ1iVMtTXfO3nR+skCsU2Hg9c9AOEYVigTqe9Qe0u9wZlu0Uzr3Vv8AQh35rJSY+Z6ahSfkr9YLBoF5GN3IRzDx70WdQrbi1fVvfKWinLDxtVsOUQCdc/j/OnWRnV+rP3H5JgUs96IfOwWhGOYuHTNSn3Gimo1Ge02m7qdNOAgdWRx/kbcUSjvKpYxvu4OhGM4oN7i998wsj4jMR8/jJmiI5BdUQ/ByTevs/LQSXlLnoQL6rsM4RjyDCb73+YIVdd4MioWs27eFKUNV1JHIqn6on7VYl48fLi9QFqw0WLCxdddgnAMbbcblFlp5rMdo2lGOJAr7NvJC4g8+tVLpIvnePHwYd5R+bOFIvY/dgHCMYRdvmEbOUtwvomg0tbaPudrRRR4DZHHeve2bsEMu/Xb70vKx9HzxJY25KN/EI6h6nSV9a9xwtXbj10RYdy6znyM31gcIpZp2yaxtJAXHY5UqMfr6u7j/HA/IBxDUkGZPGq2UP/4ui7fvKZfHG9XsAFEOsWg77g9oYHXHc7Xqvno2DcNXiEcQ0/WQSk6SWzXPzZKUo9ULp0rXXH7WUJEEQ8XUP+RF4/cqFNGzhJKTmP+Hp8gHEOJJNsLy+WEdIvr+Rk0mjZmpPICIp56rmtiHA0meP1IS5t9zAJx6/4evkFeWEI4hgxarT9PFNfv1litFYOufVaMosf9s+Bb8rVazd0sotken2pO3Gyh71q+CLQgHEND9XXbuzOF4lPaAyLj5rXmo8W8AHjElL1RLD7Ii8dR5/GLxWJjM/ZQu4VwDAHflMgfzxfd3YxfunRenzIfdxEBV+qe6OULlfY2Xj+u7IL1nTjh5EUcotGGcAxqRsE+K808e53Z3SQrdlnWr12qtGmv/QCWs6f0KxfxwkX9feXTBeKGXElR8OXaGcIxeNFQeuQsYechT8cWTVnpYtF+XgBoMW5a4+HsV0m2L8uyxKSYMUtuJwjHYERf45kHpNHzxJpbnma9lyrP6pMTcGIjeKae9jh3sq3J0x0oj52zvjtTOHQSZ/l8C+EYdO40KtFJYkq2RTR7Guko+vb2WTG25nu8BnBPunhOnzTbbvP0XdvUqkxaru7DwYXYDMIxiFCHcUehTF/g5ZXe95Hr1y41l5XyAsAb0/YMX2Yk2VUsvz9bOH4eR2kQjkHjZp0ybomYuNniy/e2WJhn2LCSFwA+sEsW3YIZ0uVLvHavoUmJXWmes97yoD2iu5AIx75Hw+e0XdLH8/k83l7J12ppLcfUO+Av272G9rmTfTy3Ie+YPGKGkHtYjtgD2QjHPnb4jPxevLBpr+Tj5Qq0ZtP6ba27w2sAf1jOlKlnxco+HXhp09kXbrLQgKb6eiTeDhvh2Geu37V9vdw8dZX57j1fDzerZzUunWM5eZzXAP4z7dji17x2F69av1gsUko6zxwaCRCOfeD+A2VxhuXzRPF0lX+7vY2b1nSapw/AX+plMysWurusUBONrGmU/dc4IX2P5PkkinCCcOxVOoM9dZf0TpxAq5q/u3KEvF36tUtxmSB0nyKY2udOli5U8No3FIub90mjZgu7iuRImLQC4dhL9EZ1xXp/trCtoCvfveayUl1inGIx8xqge2zNTe1x413nNPOqTW9fs1N6d6Z6rEYO64hEOAbcg3b7ut3S8OlC1kFJ6NJ94OgbXp3YWYcLqKEnWe/cbJ8RbW2o47U/7j9QUrItI2cJe0tlsyU8IxLhGEB37ymrd6jfsdRb7PLtMS1nT9EaTN/zvAboOVJNVdv0r3w5+VFTc6uS1rGbaEue1Glq+jCAcAyI87XWmanmjxLEA8e7tXdG2JXZ+vk7tnuerooF6A755vX7//Uz+WoNr/2nM9gpHEfMEJZvs9xuDJ8j2gjHniSa7fuPyWMWiJOWm8suWLt59qzlzImWkf+fIoq8BggMqba6fVaMtbGe111CnYD8E/LYRerKf/Rsd1f+YIBw7Bk1t2zJWZa/xgk0yuiRL09Kxvb4ibb76DNCb5Bqq9qmfdmF4zOuLtRaE9ItbKzd6QaZoQXh2C0tbfbdJfLniWJ0klhQ1mN7poWCfbqFcZjCFnoTja/bpo2TKs/xunva9Oqm8ckCccIykXqUoXjqD8KxK4yCehfABRst78ULmQekHrxXut1qNR8r0S9fgNE09D5b8z0aX/fs9MmXb9hSsi2TV5gXbrKUV1pDKCURjn7QGezUPYxPNY+cJazaIfX4/dGpq6hbNNOUle553j2AwFFMRv2KRGNGqo/XX/tIUeynq6yLNlveni4kbraUXbC63l442CAcvaOO4b4j8tfLzcOnC6t3SJVXArKzWaq+2D4j2lx+hNcAfcSuKMLenbp5U7p2CqRnZov9+Hkr5eO7M4W56y3U22jVBWlKIhy10dfa2Wrr2hzpb3OEzxNFGjt7vmNBd9BXtGnHFt2CGYFYFwG6Rqq60Db1C/FwAa97miyrmxj1Nt6LF75KErMOStXXbUF1jBvh+C2bzX75hm1bgTRlhZm+1hZusuSfkAM9E4l09XJ7/ETTzi09O4oB6D5F365fk6RfvsDWfJ8vCow7jcreUnnaavPQqcLsdeb9x+Rb9X1/mDvSw1E02y/UWrPzpRlrzCNmCPTx7CiUqZPYC/M7KAaDMWtD+5yv5Wu1fBFA8LGcKWufOUE4sLsXvr8lWd0eaaA2Mdn89nRhznpL7mH56m2b1doHPcqIC0dKvduNSvEpec1OKTpJpA9gVpp5V5EaiNRz5E8KMFrJxMK89rjx4qE8HHuB4KeIomn75rbpX1FQ8kWBRx2Xs5fVoJy+xvx/pwkxKeYNuVLZBWtzb00rGf7hSN85N+qUktPy+t3S1FXqu/xlkrg2R6IlDU293XWnWDQfLW6bNo76jIpBx5cChAJrw139qsW6xLieOhfSd4piv37XRsPtlGzLF4vF4dPVQV5GnpqVDc2B2orDLRzpTWxsVk5XWXMOyUlb1BneKQ2pi756h3TopEwp2VeTLCmCSSza3x4/0ZiR5vkOwgDBTL55Tb9ykW7eFHP5Ubu1b25SSJ3KqmvWfUfkFduk8UvFv8Sqh3SWb7McPC7TqLylrWe28dAOx3a9nYbDR89asw5KCzZaPluoRiG9WYszLNsLpJMXrfda+n63rvXOTWPm+rYpnwvfZCmtLXwpQCiTb98wbFzdNu1LIW+XraWZL+0jNDq83ahQDmTnSws3WUbPE9+cIrCbv1Mn6fh56/U6WxemxQrJcKTUoy7hnycLo2YLcWvN676R6DvkbLWV+ozBcyqArblJPJSnmz9dtzDOfKxEMeOKFwg3SnubULCvfeYE/dK55uOHFYOBP9DXaIB4p1E5dcn6TYlMPcqvl6sXbrwTp8aF77dsCslwbGpVqEvYJwewPLPb7dRPFA7k6uZPUw/w0ZfqvQb+GED4kq/WqEdspo3Tr0gUi/O7OcFPgEgydTBtBpOvueF3OKampvbv3z8qKio9PZ0vcuL5UQ8Cd0Am0OxWKw0xxJJ8Q9oyGjurty46lGdtuMsfBogY1DmQb1wV9uzUJUxtm/qFces6c/nR4AxKX/gXjoWFhUOGDGltbW1oaBg4cGB5eTl/oIPnRz0LoXBU9O20BtBIWcjN1iXGtcWOpUAU9u2UqisxdgZgaIgtXagQdmXSNtI6YbRx01ohd7ul/Bj1JBRR4E8Kbv6F4+TJk7Ozs1mbOomxsbGszXh+1LNgC0e7oijtbda7t6Sq8+bjh4X93xg3rdEtmtk66ZP2WTHUSRRyd1jOnqJRM24HCOCZXZblu7eoF0lZqU+ZTwMs6ldSl4K6lsKB3ebyI1JtlbWhTjHogmpr8i8cR4wYUVLCbweem5s7evRo1mY8P+pZz4aj3Wazy5IiiorJqOjalNYWW/M9Gupa79yUr1+Rqi/Sd5rlTBl9KmLRASEvx5Sz1Zi1wbB+uX7p3PbZk9omf0b/6eZN0ScnUCbSMIHykTqGahTiIj+AbqNt01p/V6o8KxbnU2IaNq6mba09fmJr9N8oN6kXQtFpSEtW0zN3m1CwVzxcYD5x2FJRLlWek2qraehG2zJtj7bm+0rbA0pVdUsXRdo8e/CqCv/CcdCgQY7Bcn5+/rBhw1ib8fyow549ez5y8Ytf/OL/+R8j/uP3E9l/ew5WsicfO3nDsdCxnKLq5CcfjPzXF0c+3/Hfv75YPGIodd1bx39w6t2333/+Z6N+8m/vP/dTWl70wSjKOF3C1JPjvhjx0wHD/u2Vt//9t//33wcVTImnQBTydpUtT/vvX7815D+G/Z8B7wz6Xx/szSxQ2tooWDV/r7vXg+VYjuU9tXzAoK9e/u1n//s/P/r9wPf3r8qi/qb5yCHaTt/4n6+9+fPBf/n5/3nrxd8XRMdQD1S/ZNbJr7786//7H+/+6/8c+a+/oCgofnd469efUsKe+utfKARGPv/v9N/7z/978V/fbpv+FYXvyS+//N3/+lDz9zoWOpb7F47Dhw8vKipi7ZycnOjoaNZmPD/qIEmS0cWOHTvi5qw9WXGb/feg1cSerDeYHQuxHMuxHMt7bbl/4Thx4sSMjAzWTklJWbJkCWsznh/17ODBg9u2beMFAEBf8y8cS0tLBw8erNPpWlpaBgwYUHlB7Xw2NjbW16tH6zUf9RHCEQCCin/hqCjK+PHj+/XrFxUVlZKcwhbGx8fHxMS4e9RHCEcACCr+hSNjMpkkSeKFC8+PuoNwBICg0pVwDASEIwAEFYQjAIAGhCMAgAaEIwCABoQjAIAGhCMAgAaEIwCAhmAJx9ra2ueff/5933zve9/7MTzy5JNPfv/73+cF/PjHTzzxBG/Bj39MG8tTTz3Fi4j3ow48R7wZMGBAU1MTTygXvReOfnnrrbd4Cx4+zMrKKigo4AVg9Xjc4sWLL168yIuI19bW9vnnn/OiexCOIQDh2AlWD2cIR2cIx8iCcOwEq4czhKMzhGNkQTh2gtXDGcLRGcIxsiAcO8Hq4Qzh6AzhGFkQjp1g9XCGcHSGcIwsCMdOsHo4Qzg6C/9wbGlp4S3omEBTFHG/7G9h9XCm1+u7ML9quFIUpbW1lRfdE6ThCADQtxCOAAAaEI4AABoQjgAAGhCOAAAaEI4AABoQjgAAGhCOAAAaEI4AABqCOhxPnDjxox/9iBcR7NatW19++eVLL700dOjQwsJCvjQinTlz5vXXX3/uuec+/fRTQRD40kiFFUNTbGzs1KlTedENwRuOBoPh5Zdf/uEPf8jrCPbXv/41MzPTbrffvn37mWee6amro0KOTqf7yU9+Unmh0mKxTJ8+vUc2gJCGFcPVsWPHnnrqqTAPxwkTJmzbtg09R0VRioqKrFYrK6mbQB8/a0caGklQF4m16+rqIvyLEyuGK71e/+qrryYnJ4dzONKnHh0dfe/ePYSjMxpUUiLQGsDrCLNjx46vvvqKtc1m83e+8x1ZllkZ4SJ8xXAYN27c4cOHN2zYELbh2NbWRgNqGkMhHJ3V19c///zze3L38DryrF271rHS01iSwhFxQLBiMAUFBWPHjqVGuIVjXFzcDzrMmDGD/sL4+PjTp08fOHDgySefpEakdRDKysrYu/GTn/yELaENICoqKj09nZWRafv27dQ1YG1BEJ544gmKSFZGLKwYDH1NPvfccwcPHqS4mD59+qhRo2pqavhjXRUs4Xj79u2KDtSgP4x6juTXv/419Q5YL5I/LzLQJ83ejfPnz1PZ1NT04osvpqamskcj1pEjR/70pz+x9vXr11966SXWjlhYMRwoN1hokGeffZZGnBMmTOCPdVXwHpAhGFYzb731VmJiovJIxHaXJEl65plnKisrbTYb9Q4SEhL4A5EKK4amcN7n6IBwJNXV1dR9dpabm8sfizz79+///ve///Of/3zo0KERfp4jVgx3IiIcAVxRtxHHYaAXIBwBADQgHAEANCAcAQA0IBwBADQgHAEANCAcAQA0IBwBADQgHAEANCAcAQA0IBwBADQgHAEANCAcAQA0IBwBADQgHAEANCAcAQA0IBwBADQgHAEANCAcAQA0IBwBADQgHAEANCAcAQA0IBwhZNTX17/yyit1dXXU3r1796effoo7NUPgIBwhlMyZM+fDDz988ODB008/fenSJb4UIAAQjhBKBEGIior6/e9/P3/+fL4IIDAQjhBi0tLSvvOd7zQ3N/MaIDAQjhBKjEYj9Rxff/31SZMm8UUAgYFwhFASGxs7fvx4nU73zDPPlJWV8aUAAYBwhJBx+vTpp59+uq2tjdp79ux58cUXRVFkDwH0OIQjAIAGhCMAgAaEIwCABoQjAICLhw//fzv++PaSOtj1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='LaplaceGaussian.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is “Naive” Bayes naive?<a id=\"7\"></a>\n",
    "\n",
    "Despite its practical applications, especially in text mining, Naive Bayes is considered “Naive” because it makes an assumption that is virtually impossible to see in real-life data: the conditional probability is calculated as the pure product of the individual probabilities of components. This implies the absolute independence of features — a condition probably never met in real life.\n",
    "\n",
    "As a Quora commenter put it whimsically, a Naive Bayes classifier that figured out that you liked pickles and ice cream would probably naively recommend you a pickle ice cream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Bayes’ Theorem? How is it useful in a machine learning context?<a id=\"6\"></a>\n",
    "\n",
    "Bayes’ Theorem gives you the posterior probability of an event given what is known as prior knowledge.\n",
    "\n",
    "Mathematically, it’s expressed as the true positive rate of a condition sample divided by the sum of the false positive rate of the population and the true positive rate of a condition. Say you had a 60% chance of actually having the flu after a flu test, but out of people who had the flu, the test will be false 50% of the time, and the overall population only has a 5% chance of having the flu. Would you actually have a 60% chance of having the flu after having a positive test?\n",
    "\n",
    "Bayes’ Theorem says no. It says that you have a (.6 * 0.05) (True Positive Rate of a Condition Sample) / (.6*0.05)(True Positive Rate of a Condition Sample) + (.5*0.95) (False Positive Rate of a Population)  = 0.0594 or 5.94% chance of getting a flu.\n",
    "\n",
    "Bayes’ Theorem is the basis behind a branch of machine learning that most notably includes the Naive Bayes classifier. That’s something important to consider when you’re faced with machine learning interview questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define precision and recall.<a id=\"5\"></a>\n",
    "\n",
    "Recall is also known as the true positive rate: the amount of positives your model claims compared to the actual number of positives there are throughout the data. Precision is also known as the positive predictive value, and it is a measure of the amount of accurate positives your model claims compared to the number of positives it actually claims. It can be easier to think of recall and precision in the context of a case where you’ve predicted that there were 10 apples and 5 oranges in a case of 10 apples. You’d have perfect recall (there are actually 10 apples, and you predicted there would be 10) but 66.7% precision because out of the 15 events you predicted, only 10 (the apples) are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain how a ROC curve works.<a id=\"4\"></a>\n",
    "\n",
    "The ROC curve is a graphical representation of the contrast between true positive rates and the false positive rate at various thresholds. It is often used as a proxy for the trade-off between the sensitivity of the model (true positives) versus the fall-out or the probability it will trigger a false alarm (false positives)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is KNN different from k-means clustering?<a id=\"3\"></a>\n",
    "\n",
    "K-Nearest Neighbors is a supervised classification algorithm, while k-means clustering is an unsupervised clustering algorithm. While the mechanisms may seem similar at first, what this really means is that in order for K-Nearest Neighbors to work, you need labeled data you want to classify an unlabeled point into (thus the nearest neighbor part). K-means clustering requires only a set of unlabeled points and a threshold: the algorithm will take unlabeled points and gradually learn how to cluster them into groups by computing the mean of the distance between different points.\n",
    "\n",
    "The critical difference here is that KNN needs labeled points and is thus supervised learning, while k-means doesn’t — and is thus unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the difference between supervised and unsupervised machine learning?<a id=\"2\"></a>\n",
    "\n",
    "Supervised learning requires training labeled data. For example, in order to do classification (a supervised learning task), you will need to first label the data you will use to train the model to classify data into your labeled groups. Unsupervised learning, in contrast, does not require labeling data explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the trade-off between bias and variance? <a id=\"1\"></a>\n",
    "\n",
    "Bias is error due to erroneous or overly simplistic assumptions in the learning algorithm you are using. This can lead to the model underfitting your data, making it hard for it to have high predictive accuracy and for you to generalize your knowledge from the training set to the test set.\n",
    "\n",
    "Variance is error due to too much complexity in the learning algorithm you’re using. This leads to the algorithm being highly sensitive to high degrees of variation in your training data, which can lead your model to overfit the data. You will be carrying too much noise from your training data for your model to be very useful for your test data.\n",
    "\n",
    "The bias-variance decomposition essentially decomposes the learning error from any algorithm by adding the bias, the variance, and a bit of irreducible error due to noise in the underlying dataset. Essentially, if you make the model more complex and add more variables, you will lose bias but gain some variance — in order to get the optimally reduced amount of error, you will have to tradeoff bias and variance. You do not want either high bias or high variance in your model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
