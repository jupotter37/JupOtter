{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beer Recommender System\n",
    "This notebook implements data preprocessing and modeling techniques to create a beer recommender system. I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agent\\anaconda3\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\agent\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Functions\n",
    "These functions clean the dataset by handling duplicates, missing values, and incorrect formats. They prepare the data for splitting and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    df_filtered = df.drop_duplicates([\"name\", \"reviewer\", \"review_text\"]) # Remove duplicate entries\n",
    "    print(\"Size after drop_duplicates: \", len(df_filtered))\n",
    "    \n",
    "    df_filtered['rating'] = pd.to_numeric(df_filtered['rating'], errors='coerce')  # Set erros to NaN\n",
    "    df_filtered = df_filtered.dropna(subset=['rating'])  # Drop rows where 'rating' is NaN\n",
    "    print(\"Size after drop rating NA: \", len(df_filtered))\n",
    "    \n",
    "    df_filtered['abv'] = pd.to_numeric(df_filtered['abv'].str.rstrip('%'), errors='coerce') \n",
    "    df_filtered = df_filtered.dropna(subset=['abv'])\n",
    "    print(\"Size after drop abv NA: \", len(df_filtered))\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "def create_test_train(df, reviewer_col=\"reviewer\", random_state=7, test_size=100, mask_percentage=0.10):\n",
    "    \"\"\"\n",
    "    Splits a dataset into training and test sets, masking a portion of test set entries.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The dataset to split.\n",
    "    - reviewer_col (str): The column name containing reviewer IDs.\n",
    "    - random_state (int): The random state for reproducibility.\n",
    "    - test_size (int): The number of reviewers to sample for the test set.\n",
    "    - mask_percentage (float): The percentage of beers to mask for each reviewer in the test set.\n",
    "\n",
    "    Returns:\n",
    "    - df_train (pd.DataFrame): The training set.\n",
    "    - df_test_masked (pd.DataFrame): The test set with masked entries.\n",
    "    \"\"\"\n",
    "    # Randomly sample reviewers\n",
    "    sampled_reviewers = df[reviewer_col].sample(n=test_size, random_state=random_state)\n",
    "    \n",
    "    # Get reviews from the sampled reviewers\n",
    "    df_test = df[df[reviewer_col].isin(sampled_reviewers)]\n",
    "    \n",
    "    # Group by reviewer to get each user's beers\n",
    "    df_test_grouped = df_test.groupby(reviewer_col)\n",
    "    \n",
    "    # Randomly mask a percentage of beers for each reviewer\n",
    "    test_set_masked = []\n",
    "    for reviewer, group in df_test_grouped:\n",
    "        # Calculate how many beers to mask\n",
    "        num_to_mask = max(int(len(group) * mask_percentage), 1)\n",
    "        \n",
    "        # Sample the calculated number of beers\n",
    "        masked_group = group.sample(n=num_to_mask, random_state=random_state)\n",
    "        test_set_masked.append(masked_group)\n",
    "    \n",
    "    # Combine masked reviews into a single DataFrame\n",
    "    df_test_masked = pd.concat(test_set_masked)\n",
    "    \n",
    "    # Remove masked reviews from the training data\n",
    "    df_train = df.drop(df_test_masked.index)\n",
    "    \n",
    "    # Display dataset summaries\n",
    "    print(\"\\n### Dataset Summary ###\")\n",
    "    print(f\"Total reviewers sampled: {len(sampled_reviewers)}\")\n",
    "    print(f\"Training set size: {df_train.shape}\")\n",
    "    print(f\"Test set size: {df_test_masked.shape}\")\n",
    "    \n",
    "    return df_train, df_test_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after drop_duplicates:  1157819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\agent\\AppData\\Local\\Temp\\ipykernel_1708\\3114772741.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['rating'] = pd.to_numeric(df_filtered['rating'], errors='coerce')  # Set erros to NaN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after drop rating NA:  1157807\n",
      "Size after drop abv NA:  1154739\n"
     ]
    }
   ],
   "source": [
    "# Load data and preprocess\n",
    "df = pd.read_pickle('encoded_beers_SBERT.pkl')\n",
    "\n",
    "df_filtered = preprocess_data(df)\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>brewery</th>\n",
       "      <th>subgenre</th>\n",
       "      <th>abv</th>\n",
       "      <th>location</th>\n",
       "      <th>rating</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_text</th>\n",
       "      <th>algorithm_rating</th>\n",
       "      <th>total_reviews</th>\n",
       "      <th>sbert_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Wild Dog Pale Ale</td>\n",
       "      <td>Wild Dog (Tiemann Beer)</td>\n",
       "      <td>American Pale Ale</td>\n",
       "      <td>5.2</td>\n",
       "      <td>ðŸ‡¯ðŸ‡ªJersey</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.99</td>\n",
       "      <td>Jerseyislandbeer</td>\n",
       "      <td>December 14, 2023</td>\n",
       "      <td>330ml can from Shoprite in Livingstone. At hom...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.037878353, 0.00593541, 0.0062317043, -0.011...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Wild Dog Pale Ale</td>\n",
       "      <td>Wild Dog (Tiemann Beer)</td>\n",
       "      <td>American Pale Ale</td>\n",
       "      <td>5.2</td>\n",
       "      <td>ðŸ‡¬ðŸ‡§Ipswich, England</td>\n",
       "      <td>3.2</td>\n",
       "      <td>2.99</td>\n",
       "      <td>Grumbo</td>\n",
       "      <td>February 28, 2022</td>\n",
       "      <td>18/2/2022. Can sample courtesy of fonefan, che...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>11</td>\n",
       "      <td>[-0.037820198, -0.044825517, 0.07764052, 0.065...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Wild Dog Pale Ale</td>\n",
       "      <td>Wild Dog (Tiemann Beer)</td>\n",
       "      <td>American Pale Ale</td>\n",
       "      <td>5.2</td>\n",
       "      <td>ðŸ‡¸ðŸ‡ªTyresÃ¶, Sweden</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.99</td>\n",
       "      <td>omhper</td>\n",
       "      <td>February 19, 2022</td>\n",
       "      <td>--Sample, thanks fonefan! -- Hazy deep golden,...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.056960188, -0.00059301173, 0.11057871, 0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Wild Dog Pale Ale</td>\n",
       "      <td>Wild Dog (Tiemann Beer)</td>\n",
       "      <td>American Pale Ale</td>\n",
       "      <td>5.2</td>\n",
       "      <td>ðŸ‡«ðŸ‡®Vasa, Finland</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.99</td>\n",
       "      <td>oh6gdx</td>\n",
       "      <td>January 31, 2022</td>\n",
       "      <td>Panda from a can, thanks fonefan!. Golden colo...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>11</td>\n",
       "      <td>[0.003549767, -0.010705345, 0.02083684, 0.0106...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Wild Dog Pale Ale</td>\n",
       "      <td>Wild Dog (Tiemann Beer)</td>\n",
       "      <td>American Pale Ale</td>\n",
       "      <td>5.2</td>\n",
       "      <td>ðŸ‡©ðŸ‡°Haderslev, Denmark</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.99</td>\n",
       "      <td>martin00sr</td>\n",
       "      <td>January 8, 2022</td>\n",
       "      <td>Can @Ulfborg. Cloudy amber, white head. Malty ...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>11</td>\n",
       "      <td>[-0.01005388, -0.02942978, 0.0016338513, 0.017...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id               name                  brewery           subgenre  abv  \\\n",
       "0   1  Wild Dog Pale Ale  Wild Dog (Tiemann Beer)  American Pale Ale  5.2   \n",
       "1   2  Wild Dog Pale Ale  Wild Dog (Tiemann Beer)  American Pale Ale  5.2   \n",
       "2   3  Wild Dog Pale Ale  Wild Dog (Tiemann Beer)  American Pale Ale  5.2   \n",
       "3   4  Wild Dog Pale Ale  Wild Dog (Tiemann Beer)  American Pale Ale  5.2   \n",
       "4   6  Wild Dog Pale Ale  Wild Dog (Tiemann Beer)  American Pale Ale  5.2   \n",
       "\n",
       "               location  rating  average_rating          reviewer  \\\n",
       "0              ðŸ‡¯ðŸ‡ªJersey     3.5            2.99  Jerseyislandbeer   \n",
       "1    ðŸ‡¬ðŸ‡§Ipswich, England     3.2            2.99            Grumbo   \n",
       "2      ðŸ‡¸ðŸ‡ªTyresÃ¶, Sweden     3.5            2.99            omhper   \n",
       "3       ðŸ‡«ðŸ‡®Vasa, Finland     2.8            2.99            oh6gdx   \n",
       "4  ðŸ‡©ðŸ‡°Haderslev, Denmark     2.6            2.99        martin00sr   \n",
       "\n",
       "         review_date                                        review_text  \\\n",
       "0  December 14, 2023  330ml can from Shoprite in Livingstone. At hom...   \n",
       "1  February 28, 2022  18/2/2022. Can sample courtesy of fonefan, che...   \n",
       "2  February 19, 2022  --Sample, thanks fonefan! -- Hazy deep golden,...   \n",
       "3   January 31, 2022  Panda from a can, thanks fonefan!. Golden colo...   \n",
       "4    January 8, 2022  Can @Ulfborg. Cloudy amber, white head. Malty ...   \n",
       "\n",
       "  algorithm_rating  total_reviews  \\\n",
       "0             28.0             11   \n",
       "1             28.0             11   \n",
       "2             28.0             11   \n",
       "3             28.0             11   \n",
       "4             28.0             11   \n",
       "\n",
       "                                     sbert_embedding  \n",
       "0  [0.037878353, 0.00593541, 0.0062317043, -0.011...  \n",
       "1  [-0.037820198, -0.044825517, 0.07764052, 0.065...  \n",
       "2  [0.056960188, -0.00059301173, 0.11057871, 0.02...  \n",
       "3  [0.003549767, -0.010705345, 0.02083684, 0.0106...  \n",
       "4  [-0.01005388, -0.02942978, 0.0016338513, 0.017...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Dataset Summary ###\n",
      "Total reviewers sampled: 100\n",
      "Training set size: (1149910, 14)\n",
      "Test set size: (4829, 14)\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test_masked = create_test_train(df_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n",
      "Maredsous 8 Brune / Bruin              11\n",
      "New Glarus Wisconsin Belgian Red       10\n",
      "AleSmith Speedway Stout                10\n",
      "Moinette Biologique                    10\n",
      "Russian River Pliny the Elder          10\n",
      "                                       ..\n",
      "Real Ale Devils Backbone                1\n",
      "Bockor Pils                             1\n",
      "Des Vignes Vent d'Ange                  1\n",
      "Birrificio del Ducato Wedding Rauch     1\n",
      "Harvest Moon Full Moon Pale Ale         1\n",
      "Name: count, Length: 2864, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get count of each unique beer in the training set\n",
    "beer_counts = df_test_masked['name'].value_counts()\n",
    "print(beer_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for retrieving beer information\n",
    "beer_info = df_train[['name', 'abv', 'subgenre']]\n",
    "\n",
    "# Drop duplicate rows based on the 'name' column (i.e. beers)\n",
    "beer_info = beer_info.drop_duplicates(subset='name')\n",
    "\n",
    "beer_info.set_index('name', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create locality-sensitive hashing (LSH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a variable to store the model\n",
    "sbert_model = None\n",
    "\n",
    "def encode_sbert(query, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    Encodes a query using SBERT. Loads the model if not already loaded.\n",
    "    \n",
    "    Parameters:\n",
    "        query (str or list of str): The query or list of queries to encode.\n",
    "        model_name (str): The name of the SBERT model to load (default is 'all-MiniLM-L6-v2').\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: The embedding(s) for the input query/queries.\n",
    "    \"\"\"\n",
    "    global sbert_model  # Use the global variable to store the model\n",
    "    \n",
    "    # Load the model if it's not already loaded\n",
    "    if sbert_model is None:\n",
    "        sbert_model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Encode the query and return the embeddings\n",
    "    return sbert_model.encode(query)\n",
    "\n",
    "def generate_hyperplanes(dim, num_hash_functions):\n",
    "    \"\"\"\n",
    "    Generate random hyperplanes for hash functions.\n",
    "    \n",
    "    Parameters:\n",
    "    - dim: Dimensionality of the embeddings.\n",
    "    - num_hash_functions: Number of hash functions per table.\n",
    "    \n",
    "    Returns:\n",
    "    - A matrix of shape (num_hash_functions, dim) where each row is a hyperplane.\n",
    "    \"\"\"\n",
    "    return np.random.randn(num_hash_functions, dim)\n",
    "\n",
    "def hash_vectors(vectors, hyperplanes):\n",
    "    \"\"\"\n",
    "    Hash a batch of vectors using a set of hyperplanes.\n",
    "\n",
    "    Parameters:\n",
    "    - vectors: Input vectors (2D array of shape [n_samples, d]).\n",
    "    - hyperplanes: Matrix of hyperplanes (2D array of shape [k, d]).\n",
    "\n",
    "    Returns:\n",
    "    - A matrix of binary hash values (shape [n_samples, k]).\n",
    "    \"\"\"\n",
    "    # Compute dot products and return binary hash values\n",
    "    return (np.dot(vectors, hyperplanes.T) > 0).astype(int)\n",
    "\n",
    "class LSHVectorized:\n",
    "    def __init__(self, d, k, L):\n",
    "        \"\"\"\n",
    "        Initialize the LSH scheme with vectorized support.\n",
    "\n",
    "        Parameters:\n",
    "        - d: Dimensionality of the input vectors.\n",
    "        - k: Number of hash functions per table.\n",
    "        - L: Number of hash tables.\n",
    "        \"\"\"\n",
    "        self.L = L\n",
    "        self.tables = [defaultdict(list) for _ in range(L)]\n",
    "        self.hyperplanes = [generate_hyperplanes(d, k) for _ in range(L)]\n",
    "\n",
    "    def add_vectors(self, vectors, identifiers):\n",
    "        \"\"\"\n",
    "        Add a batch of vectors to the LSH index.\n",
    "\n",
    "        Parameters:\n",
    "        - vectors: Input vectors (2D array of shape [n_samples, d]).\n",
    "        - identifiers: A list of unique identifiers for the vectors.\n",
    "        \"\"\"\n",
    "        for table, hyperplanes in zip(self.tables, self.hyperplanes):\n",
    "            # Compute hash values for all vectors at once\n",
    "            hash_values = hash_vectors(vectors, hyperplanes)\n",
    "            \n",
    "            # Convert binary hash values to tuples for dictionary keys\n",
    "            hash_keys = [tuple(h) for h in hash_values]\n",
    "            \n",
    "            # Add vectors to their corresponding buckets\n",
    "            for identifier, key in zip(identifiers, hash_keys):\n",
    "                table[key].append(identifier)\n",
    "\n",
    "    def query(self, vectors):\n",
    "        \"\"\"\n",
    "        Query the LSH index to find similar items for a batch of vectors.\n",
    "\n",
    "        Parameters:\n",
    "        - vectors: Query vectors (2D array of shape [n_samples, d]).\n",
    "\n",
    "        Returns:\n",
    "        - A list of sets, where each set contains the candidates for a query vector.\n",
    "        \"\"\"\n",
    "        candidates = [set() for _ in range(len(vectors))]\n",
    "        for table, hyperplanes in zip(self.tables, self.hyperplanes):\n",
    "            # Compute hash values for all query vectors\n",
    "            hash_values = hash_vectors(vectors, hyperplanes)\n",
    "            \n",
    "            # Convert binary hash values to tuples for dictionary keys\n",
    "            hash_keys = [tuple(h) for h in hash_values]\n",
    "            \n",
    "            # Retrieve candidates for each query\n",
    "            for i, key in enumerate(hash_keys):\n",
    "                candidates[i].update(table.get(key, []))\n",
    "        return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lsh(lsh, data, ground_truth, top_k=10):\n",
    "    retrieved_neighbors = []\n",
    "    for query in data:\n",
    "        neighbors = lsh.query(query, top_k=top_k)\n",
    "        retrieved_neighbors.append(neighbors)\n",
    "    \n",
    "    # Compute recall and precision\n",
    "    recall = sum(len(set(ground_truth[i]) & set(retrieved_neighbors[i])) / len(ground_truth[i])\n",
    "                 for i in range(len(data))) / len(data)\n",
    "    precision = sum(len(set(ground_truth[i]) & set(retrieved_neighbors[i])) / len(retrieved_neighbors[i])\n",
    "                    for i in range(len(data))) / len(data)\n",
    "    \n",
    "    return {\"recall\": recall, \"precision\": precision}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.vstack(df_train[\"sbert_embedding\"].values)  # Combine embeddings into a 2D array\n",
    "identifiers = df_train.index.tolist()  # Use review IDs as identifiers\n",
    "\n",
    "vectors_test = np.vstack(df_test_masked[\"sbert_embedding\"].values)  # Combine embeddings into a 2D array\n",
    "identifiers_test = df_test_masked.index.tolist()  # Use review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run LSH ##\n",
    "d = 384\n",
    "k = 7\n",
    "L = 50\n",
    "\n",
    "lsh = LSHVectorized(d, k, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add vectors to the LSH index\n",
    "lsh.add_vectors(vectors, identifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Collaborative Filtering (CF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded collab_df from pickle.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict_ratings_user_based(user_item_matrix, similarity_matrix):\n",
    "   \n",
    "    \"\"\"\n",
    "    this function predicts the ratings for the user_item_matrix using the similarity_matrix\n",
    "    \n",
    "\n",
    "    Parameters: \n",
    "    \n",
    "    - user_item_matrix (DataFrame): User-item matrix with ratings centered around the user mean.\n",
    "    - similarity_matrix (DataFrame): User-user similarity matrix.\n",
    "    \n",
    "    Returns:\n",
    "        - pred (DataFrame): Predicted ratings for all user-item pairs.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Compute predictions\n",
    "    similarity_sum = np.abs(similarity_matrix).sum(axis=1)[:, None]\n",
    "    pred = np.dot(similarity_matrix, user_item_matrix) / (similarity_sum + 1e-8)\n",
    "\n",
    "\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "def collaborative_filtering(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Predicts user ratings for items using user-based collaborative filtering with cosine similarity. \n",
    "    Preprocesses the input data to create a centered user-item matrix, computes user similarities, \n",
    "    and generates predicted ratings.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Input data with 'reviewer', 'name', and 'rating' columns.\n",
    "\n",
    "    Returns:\n",
    "    - pr_df (DataFrame): Predicted ratings for all user-item pairs.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    user_item_matrix = df.pivot_table(\n",
    "    index=\"reviewer\",     # Rows: Reviewers\n",
    "    columns=\"name\",       # Columns: Beer names\n",
    "    values=\"rating\",      # Values: Ratings\n",
    "    fill_value=0          # Fill missing ratings with 0\n",
    "    )\n",
    "    \n",
    "\n",
    "    user_item_np = np.where(user_item_matrix != 0, (user_item_matrix - 3) / 2, 0)\n",
    "    user_item_matrix = pd.DataFrame(user_item_np, index=user_item_matrix.index, columns=user_item_matrix.columns)\n",
    "\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_similarity_matrix = cosine_similarity(user_item_matrix)\n",
    "    \n",
    "    # Predict ratings\n",
    "    predicted_ratings = predict_ratings_user_based(user_item_matrix, cosine_similarity_matrix)\n",
    "\n",
    "    df_out = pd.DataFrame(predicted_ratings, index=user_item_matrix.index, columns=user_item_matrix.columns)\n",
    "   \n",
    "    return df_out\n",
    "\n",
    "\n",
    "pickle_file = 'collab_df.pkl'\n",
    "\n",
    "# Check if the pickle file exists\n",
    "if os.path.exists(pickle_file):\n",
    "    # Load the DataFrame from the pickle file\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        collab_df = pickle.load(f)\n",
    "    print(\"Loaded collab_df from pickle.\")\n",
    "else:\n",
    "    # Generate the DataFrame\n",
    "    collab_df = collaborative_filtering(df_train)\n",
    "    # Save it to a pickle file\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(collab_df, f)\n",
    "    print(\"Generated collab_df and saved to pickle.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    33531.000000\n",
       "mean         4.749092\n",
       "std         13.940267\n",
       "min          0.034176\n",
       "25%          0.128161\n",
       "50%          0.375940\n",
       "75%          1.768626\n",
       "max        100.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caculate fraction of beers per user which is zero\n",
    "zero_percentage = (collab_df == 0).mean(axis=1) * 100\n",
    "zero_percentage.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Term Explanations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define reference context for flavor-related words. The first 20 is from a aromatic kit used for sommeliers, the rest is ai-generated.\n",
    "context_words = [\n",
    "    \"bitter\", \"sweet\", \"salt\", \"sour\", \"umami\",\n",
    "    \"lemon\", \"grapefruit\", \"apple\", \"pear\", \"blackcurrant\", \"prune\", \"melon\", \n",
    "    \"banana\", \"acacia\", \"rose\", \"cut grass\", \"hay\", \"bay leaf\", \"thyme\", \n",
    "    \"tomato\", \"pepper\", \"nutmeg\", \"clove\", \"bread\", \"butter\", \"vanilla\", \n",
    "    \"hazelnut\", \"toast\", \"malt\", \"caramel\", \"honey\", \"coffee\", \"licorice\",\n",
    "    \"pine\", \"grass\", \"resin\", \"floral\", \"perfume\", \"incense\", \"cinnamon\",\n",
    "    \"ginger\", \"anise\", \"nut\", \"almond\", \"walnut\", \"chestnut\", \"peanut\",\n",
    "    \"soy\", \"mushroom\", \"earth\", \"dust\", \"wood\", \"barnyard\", \"horse\",\n",
    "    \"wet\", \"dry\", \"metallic\", \"sulfur\", \"fish\", \"cheese\", \"butter\",\n",
    "    \"cream\", \"leather\", \"silk\", \"rubber\", \"barnyard\", \"ammonia\",\n",
    "    \"rotten\", \"acid\"\n",
    "]\n",
    "\n",
    "custom_stop_words = [\"beer\", \"beers\", \"bottle\", \"taste\", \"nice\", \"aroma\", \"like\", \"good\", \"great\", \"head\", \"flavor\", \"flavors\", \"flavour\", \"flavours\", \"brew\", \"can\"]\n",
    "context_embeddings = encode_sbert(context_words)\n",
    "\n",
    "# Function to filter terms dynamically\n",
    "def is_flavor_related(term, context_embeddings, threshold=0.35):\n",
    "    term_embedding = sbert_model.encode([term])[0]\n",
    "    cosine_similarity = lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "    max_similarity = max(cosine_similarity(term_embedding, context) for context in context_embeddings)\n",
    "    return max_similarity > threshold\n",
    "\n",
    "def plot_bucket(bucket_vectors, cluster_labels, perplexity=30, n_iter=5000, learning_rate=200):\n",
    "    \"\"\"\n",
    "    Visualizes differences within an LSH bucket using t-SNE with configurable parameters.\n",
    "    \n",
    "    Args:\n",
    "        bucket_vectors (np.ndarray): High-dimensional vectors of beers in the bucket.\n",
    "        cluster_labels (np.ndarray): Cluster labels assigned to each vector.\n",
    "        subgenres (np.ndarray): Subgenre or categorical labels for each beer.\n",
    "        perplexity (int): The t-SNE perplexity parameter, balancing local/global data views.\n",
    "        n_iter (int): Number of iterations for t-SNE optimization.\n",
    "        learning_rate (float): Learning rate for t-SNE optimization.\n",
    "    \"\"\"\n",
    "    # t-SNE reducer with tuned parameters\n",
    "    reducer = TSNE(\n",
    "        n_components=2, \n",
    "        random_state=42, \n",
    "        perplexity=perplexity, \n",
    "        n_iter=n_iter, \n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "    reduced_vectors = reducer.fit_transform(bucket_vectors)\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    scatter = plt.scatter(\n",
    "        reduced_vectors[:, 0],\n",
    "        reduced_vectors[:, 1],\n",
    "        c=cluster_labels,\n",
    "        cmap='plasma',\n",
    "        alpha=0.7\n",
    "    )\n",
    "    plt.colorbar(scatter, label='Cluster Label')\n",
    "    plt.title(f\"t-SNE Visualization (Perplexity={perplexity}, n_iter={n_iter}, LR={learning_rate})\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "def getThemes(df_filtered, beer_name, query_embedding, limit=100):\n",
    "    # Initialize CountVectorizer with custom stopwords\n",
    "    default_stop_words = CountVectorizer(stop_words='english').get_stop_words()\n",
    "    all_stop_words = list(set(default_stop_words).union(custom_stop_words))\n",
    "\n",
    "    # Pass the combined stop words to CountVectorizer\n",
    "    vectorizer = CountVectorizer(max_features=100, stop_words=all_stop_words, token_pattern=r'\\b[a-zA-Z]{2,}\\b')\n",
    "    df_beer = df_filtered[df_filtered[\"name\"] == beer_name]\n",
    "    # Extract top terms from cluster reviews\n",
    "    term_matrix = vectorizer.fit_transform(df_beer[\"review_text\"])\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    term_counts = np.array(term_matrix.sum(axis=0)).flatten()\n",
    "    top_terms = [terms[i] for i in term_counts.argsort()[-limit:]]  # Top 5 terms\n",
    "    filtered_top_terms = [term for term in top_terms if is_flavor_related(term, context_embeddings)]\n",
    "    \n",
    "    # Return the terms most similar to the query\n",
    "    term_embeddings = np.vstack([encode_sbert(term) for term in filtered_top_terms])\n",
    "    \n",
    "    # Calculate cosine similarity between query and terms\n",
    "    similarities = cosine_similarity(query_embedding, term_embeddings)[0]\n",
    "    \n",
    "    # Create a DataFrame to store terms and their similarities\n",
    "    term_similarity_df = pd.DataFrame({\n",
    "        'term': filtered_top_terms,\n",
    "        'similarity': similarities\n",
    "    })\n",
    "    \n",
    "    # Sort terms by similarity to the query\n",
    "    term_similarity_df = term_similarity_df.sort_values(by='similarity', ascending=False)\n",
    "    \n",
    "    # Return the top similar themes\n",
    "    return term_similarity_df['term'].head(10).tolist()\n",
    "\n",
    "def compute_bucket_score(bucket_data, query_embedding, user_name, beta, abv_desired, beer_info, style_desired):\n",
    "    bucket_data = bucket_data.drop_duplicates(subset=\"name\").reset_index(drop=True)\n",
    "    # Calculate LSH scores for each beer in the bucket\n",
    "    bucket_vectors = np.vstack(bucket_data[\"sbert_embedding\"].to_numpy())\n",
    "    sims = cosine_similarity(query_embedding, bucket_vectors)[0]\n",
    "    \n",
    "    # Add LSH scores to the bucket data\n",
    "    bucket_data[\"LSH_score\"] = sims\n",
    "    \n",
    "    # Collaborative filtering scores for each beer\n",
    "    collab_scores = collab_df.loc[user_name, bucket_data[\"name\"].values]\n",
    "    bucket_data[\"collab_score\"] = collab_scores.values\n",
    "\n",
    "    # ABV weight calculation for each beer\n",
    "    if abv_desired:\n",
    "        abv_values = beer_info.loc[bucket_data[\"name\"], \"abv\"]\n",
    "        alpha = 0.05\n",
    "        if abv_desired == 0:\n",
    "            abv_weights = -2 * abs(abv_values - abv_desired)\n",
    "        else:\n",
    "            abv_weights = -alpha * ((abv_values - abv_desired)**2) / (abv_desired**1.5 + 1)\n",
    "        bucket_data[\"ABV_weight\"] = abv_weights.values\n",
    "    else:\n",
    "        bucket_data[\"ABV_weight\"] = 0\n",
    "\n",
    "    # Style bonus for each beer\n",
    "    if style_desired:\n",
    "        style_match = beer_info.loc[bucket_data[\"name\"], \"subgenre\"] == style_desired\n",
    "        bucket_data[\"style_bonus\"] = style_match.astype(float) * 0.05\n",
    "    else:\n",
    "        bucket_data[\"style_bonus\"] = 0\n",
    "\n",
    "    # Weighted score calculation for each beer\n",
    "    bucket_data[\"score\"] = (\n",
    "        beta * bucket_data[\"LSH_score\"] +\n",
    "        (1 - beta) * bucket_data[\"collab_score\"] +\n",
    "        bucket_data[\"ABV_weight\"] +\n",
    "        bucket_data[\"style_bonus\"]\n",
    "    )\n",
    "\n",
    "    # Return the final bucket score DataFrame\n",
    "    bucket_score = bucket_data[[\n",
    "        \"name\", \"LSH_score\", \"collab_score\", \"ABV_weight\", \"style_bonus\", \"score\"\n",
    "    ]].rename(columns={\"name\": \"beer\"})\n",
    "\n",
    "    return bucket_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make reccomendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_beer_B(query_embedding, df_train, user_name, beer_info, abv_desired=None, style_desired=None, n_clusters=15):\n",
    "    # Query the LSH index\n",
    "    candidates = lsh.query(query_embedding)\n",
    "\n",
    "    # Filter bucket vectors and metadata\n",
    "    bucket_data = df_train[df_train[\"id\"].isin(list(candidates[0]))]\n",
    "    bucket_vectors = np.vstack(bucket_data[\"sbert_embedding\"].to_numpy())\n",
    "    \n",
    "    # Extract subgenre information\n",
    "    subgenres = bucket_data[\"subgenre\"].values  # Adjust column name as necessary\n",
    "\n",
    "    # Perform clustering on bucket vectors\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "    \n",
    "    cluster_labels = kmeans.fit_predict(bucket_vectors)\n",
    "    \n",
    "    # Assign query to the nearest cluster\n",
    "    query_cluster = kmeans.predict(query_embedding)[0]\n",
    "\n",
    "    # Filter beers in the same cluster as the query\n",
    "    cluster_indices = np.where(cluster_labels == query_cluster)[0]\n",
    "    cluster_vectors = bucket_vectors[cluster_indices]\n",
    "    cluster_beers = bucket_data.iloc[cluster_indices]\n",
    "    \n",
    "    # Compute similarities within the selected cluster\n",
    "    sims = cosine_similarity(query_embedding, cluster_vectors)[0]\n",
    "\n",
    "    # Perform collaborative filtering\n",
    "    predcicted_rating_user = collab_df.loc[user_name]\n",
    "    \n",
    "    beer_LSH = pd.DataFrame({\n",
    "        'similarity': sims,\n",
    "        'beer': cluster_beers[\"name\"].values,  # Adjust column name if necessary\n",
    "    })\n",
    "    \n",
    "    LSH_score = beer_LSH.groupby('beer')['similarity'].mean()\n",
    "    \n",
    "    collab_filtering_scores = predcicted_rating_user[LSH_score.index.tolist()] # Get CF_score for user for the beers in cluster\n",
    "\n",
    "    if abv_desired:\n",
    "        # Penalise difference in abv by using a nonlinear function penalising greater differences more\n",
    "        abv = beer_info.loc[LSH_score.index.tolist()][\"abv\"]\n",
    "\n",
    "        alpha = 0.05\n",
    "        if abv_desired == 0:\n",
    "            abv_weight = -2 * abs(abv - abv_desired)\n",
    "        else:\n",
    "            abv_weight = -alpha * ((abv - abv_desired)**2) / (abv_desired**1.5 + 1)\n",
    "    else:\n",
    "        abv_weight = 0\n",
    "\n",
    "    beta = 0.6 + 0.004*zero_percentage.loc[user_name] # Linear function to scale beta such that persons with few reviews rely more on LSH and vice versa\n",
    "    \n",
    "    # Add bonus for match in style\n",
    "    style_bonus = np.zeros(len(LSH_score))\n",
    "    \n",
    "    if style_desired:\n",
    "        relevant_styles = beer_info.loc[LSH_score.index.tolist()][\"subgenre\"]\n",
    "        style_mask = relevant_styles == style_desired\n",
    "        style_bonus[style_mask] = 0.05\n",
    "    \n",
    "    \n",
    "    # Combine weights and scores\n",
    "    weighted_score = (\n",
    "        beta * LSH_score +\n",
    "        (1-beta) * collab_filtering_scores +\n",
    "        abv_weight +\n",
    "        style_bonus\n",
    "    )\n",
    "\n",
    "    # Create final DataFrame\n",
    "    beer_weighted_score = pd.DataFrame({\n",
    "        'beer': LSH_score.index,\n",
    "        'score': weighted_score,\n",
    "        'abv': abv.values,\n",
    "        'LSH_score': LSH_score.values,\n",
    "        'LSH_score_weighted': LSH_score.values * beta,\n",
    "        'collab_score': collab_filtering_scores.values,\n",
    "        'collab_score_weighted': collab_filtering_scores.values * (1-beta),\n",
    "        'abv_diff': abs(abv.values - abv_desired),\n",
    "        'abv_weight': abv_weight.values,\n",
    "        'Style bonus': style_bonus,\n",
    "        'Weight beta': beta\n",
    "    })\n",
    "    \n",
    "    bucket_weighted_score = compute_bucket_score(bucket_data, query_embedding, user_name, beta, abv_desired, beer_info, style_desired)\n",
    "    \n",
    "    # Get the 10 beers with the highest weighted scores\n",
    "    beer_weighted_score = beer_weighted_score.sort_values(by='score', ascending=False)\n",
    "    \n",
    "    # Apply getThemes to each beer in the DataFrame \n",
    "    #beer_weighted_score['notes'] = beer_weighted_score['beer'].apply(lambda x: getThemes(df_train, x, query_embedding))\n",
    "    beer_weighted_score['notes'] = \"\"\n",
    "    \n",
    "    return beer_weighted_score.sort_values(by='score', ascending=False), bucket_weighted_score.sort_values(by='score', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 recommended beers:\n"
     ]
    }
   ],
   "source": [
    "# Create a query\n",
    "test_query = \"Light, refreshing bitter beer with a orange taste\"\n",
    "user_name = \"Jerseyislandbeer\"\n",
    "query_embedding = encode_sbert(test_query).reshape(1, -1)\n",
    "beer_recommendations, bucket_recommendations = recommend_beer_B(query_embedding, df_train, user_name, beer_info, abv_desired=7)\n",
    "\n",
    "print(\"Top 5 recommended beers:\")\n",
    "# Set max column width to display full array\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(beer_recommendations.head(10))\n",
    "display(bucket_recommendations.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking in bucket 5261 / 7728\n",
      "Ranking in recommended 2 / 3538\n",
      "Ranking in bucket 1698 / 7682\n",
      "Ranking in recommended 81 / 3514\n",
      "Ranking in bucket 5545 / 7652\n",
      "Ranking in recommended 37 / 5708\n",
      "Ranking in bucket 5998 / 7747\n",
      "Ranking in recommended 16 / 6209\n",
      "Ranking in bucket 5620 / 7685\n",
      "Ranking in recommended 39 / 3597\n",
      "Ranking in bucket 1018 / 7694\n",
      "Ranking in recommended 4 / 3650\n",
      "Ranking in bucket 5554 / 7709\n",
      "Ranking in recommended 59 / 5720\n",
      "Ranking in bucket 7726 / 7737\n",
      "Ranking in recommended 37 / 3638\n",
      "Ranking in bucket 1566 / 7708\n",
      "Ranking in recommended 129 / 5831\n",
      "Ranking in bucket 1041 / 7731\n",
      "Ranking in recommended 133 / 3727\n",
      "Ranking in bucket 5567 / 7644\n",
      "Ranking in recommended 0 / 3752\n",
      "Ranking in bucket 7318 / 7656\n",
      "Ranking in recommended 14 / 5693\n",
      "Ranking in bucket 681 / 7705\n",
      "Ranking in recommended 69 / 5876\n",
      "Ranking in bucket 7425 / 7670\n",
      "Ranking in recommended 45 / 3594\n",
      "Ranking in bucket 6170 / 7564\n",
      "Ranking in recommended 27 / 3312\n",
      "Ranking in bucket 2849 / 7711\n",
      "Ranking in recommended 309 / 4291\n",
      "Ranking in bucket 7264 / 7589\n",
      "Ranking in recommended 0 / 5155\n",
      "Ranking in bucket 3789 / 7626\n",
      "Ranking in recommended 39 / 5104\n",
      "Ranking in bucket 4858 / 7725\n",
      "Ranking in recommended 12 / 3787\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m abv_desired \u001b[38;5;241m=\u001b[39m review_row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabv\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     17\u001b[0m style_desired \u001b[38;5;241m=\u001b[39m review_row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubgenre\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 19\u001b[0m beer_recommendations, bucket \u001b[38;5;241m=\u001b[39m recommend_beer_B(query_embedding\u001b[38;5;241m=\u001b[39mquery, df_train\u001b[38;5;241m=\u001b[39mdf_train, user_name\u001b[38;5;241m=\u001b[39muser, beer_info\u001b[38;5;241m=\u001b[39mbeer_info, abv_desired\u001b[38;5;241m=\u001b[39mabv_desired, style_desired\u001b[38;5;241m=\u001b[39mstyle_desired, n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Reset the index of beer_recommendations\u001b[39;00m\n\u001b[0;32m     22\u001b[0m beer_recommendations\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[121], line 15\u001b[0m, in \u001b[0;36mrecommend_beer_B\u001b[1;34m(query_embedding, df_train, user_name, beer_info, abv_desired, style_desired, n_clusters)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Perform clustering on bucket vectors\u001b[39;00m\n\u001b[0;32m     13\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39mn_clusters, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m cluster_labels \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mfit_predict(bucket_vectors)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Assign query to the nearest cluster\u001b[39;00m\n\u001b[0;32m     18\u001b[0m query_cluster \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mpredict(query_embedding)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\agent\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1033\u001b[0m, in \u001b[0;36m_BaseKMeans.fit_predict\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute cluster centers and predict cluster index for each sample.\u001b[39;00m\n\u001b[0;32m   1012\u001b[0m \n\u001b[0;32m   1013\u001b[0m \u001b[38;5;124;03m    Convenience method; equivalent to calling fit(X) followed by\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;124;03m        Index of the cluster each sample belongs to.\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1033\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[1;32mc:\\Users\\agent\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1441\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1439\u001b[0m \u001b[38;5;66;03m# subtract of mean of x for more accurate distance computations\u001b[39;00m\n\u001b[0;32m   1440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m-> 1441\u001b[0m     X_mean \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   1442\u001b[0m     \u001b[38;5;66;03m# The copy was already done above\u001b[39;00m\n\u001b[0;32m   1443\u001b[0m     X \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m X_mean\n",
      "File \u001b[1;32mc:\\Users\\agent\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:118\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    115\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m mu\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    116\u001b[0m         is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m ret \u001b[38;5;241m=\u001b[39m umr_sum(arr, axis, dtype, out, keepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _no_nep50_warning():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hide warnings\n",
    "import warnings\n",
    "## Run LSH ##\n",
    "d = 384\n",
    "k_values = [3, 5, 7, 10]\n",
    "L_values = [10, 20, 30, 40, 50]\n",
    "average_ratings = []\n",
    "num_true_rec_result = []\n",
    "num_true_bucket_result = []\n",
    "\n",
    "for L in L_values:\n",
    "    for k in k_values:\n",
    "        average_ranking_rec = 0\n",
    "        average_ranking_bucket = 0\n",
    "        \n",
    "        lsh = LSHVectorized(d, k, L)\n",
    "        lsh.add_vectors(vectors, identifiers)\n",
    "        warnings.filterwarnings('ignore')\n",
    "\n",
    "        num_true_bucket = 0\n",
    "        num_true_recommendations = 0\n",
    "\n",
    "        test_length = 100\n",
    "\n",
    "        for i in range(test_length):\n",
    "            review_row = df_test_masked.iloc[i]\n",
    "            \n",
    "            real_beer = review_row[\"name\"]\n",
    "            query = review_row[\"sbert_embedding\"].reshape(1, -1)\n",
    "            user = review_row[\"reviewer\"]\n",
    "            abv_desired = review_row[\"abv\"]\n",
    "            style_desired = review_row[\"subgenre\"]\n",
    "\n",
    "            beer_recommendations, bucket = recommend_beer_B(query_embedding=query, df_train=df_train, user_name=user, beer_info=beer_info, abv_desired=abv_desired, style_desired=style_desired, n_clusters=5)\n",
    "            \n",
    "            # Reset the index of beer_recommendations\n",
    "            beer_recommendations.reset_index(drop=True, inplace=True)\n",
    "            bucket.reset_index(drop=True, inplace=True)\n",
    "            \n",
    "            if real_beer in bucket[\"beer\"].to_list():\n",
    "                num_true_bucket += 1\n",
    "                average_ranking_bucket += bucket[bucket[\"beer\"] == real_beer].index[0] / len(bucket)\n",
    "                #print(\"Ranking in bucket\", bucket[bucket[\"beer\"] == real_beer].index[0], \"/\", len(bucket))\n",
    "            \n",
    "            if real_beer in beer_recommendations[\"beer\"].to_list():\n",
    "                num_true_recommendations += 1\n",
    "                average_ranking_rec += beer_recommendations[beer_recommendations[\"beer\"] == real_beer].index[0] / len(beer_recommendations)\n",
    "                #print(\"Ranking in recommended\", beer_recommendations[beer_recommendations[\"beer\"] == real_beer].index[0], \"/\", len(beer_recommendations))\n",
    "                \n",
    "        print(\"Number of true bucket: \", num_true_bucket, \"/\", test_length)\n",
    "        print(\"Number of true recommendations: \", num_true_recommendations, \"/\", test_length)\n",
    "        print(\"Average ranking in bucket: \", average_ranking_bucket / num_true_bucket)\n",
    "        print(\"Average ranking in recommendations: \", average_ranking_rec / num_true_recommendations)\n",
    "        \n",
    "        average_ratings.append((k, L, average_ranking_bucket / num_true_bucket, average_ranking_rec / num_true_recommendations))\n",
    "        num_true_rec_result.append((k, L, num_true_recommendations))\n",
    "        num_true_bucket_result.append((k, L, num_true_bucket))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTTEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\agent\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 recommended beers:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beer</th>\n",
       "      <th>score</th>\n",
       "      <th>abv</th>\n",
       "      <th>LSH_score</th>\n",
       "      <th>LSH_score_weighted</th>\n",
       "      <th>collab_score</th>\n",
       "      <th>collab_score_weighted</th>\n",
       "      <th>abv_diff</th>\n",
       "      <th>abv_weight</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Wacken Beer WalkÃ¼renschluck</td>\n",
       "      <td>0.601220</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.709152</td>\n",
       "      <td>0.602780</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.001639</td>\n",
       "      <td>[orange, bitter, citrus, sweetness, alcohol, bitterness, alcoholic, fruit, spicy, fruity]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Jabeerwocky / Profesja Basista</td>\n",
       "      <td>0.596330</td>\n",
       "      <td>5.9</td>\n",
       "      <td>0.705066</td>\n",
       "      <td>0.599306</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>1.1</td>\n",
       "      <td>-0.003099</td>\n",
       "      <td>[orange, bitter, citrus, sweetness, bitterness, sour, drinkable, grapes, fruit, spicy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>De Ranke CuvÃ©e De Ranke</td>\n",
       "      <td>0.595826</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.695959</td>\n",
       "      <td>0.591565</td>\n",
       "      <td>0.028405</td>\n",
       "      <td>0.004261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>[orange, bitter, citrus, sweetness, ale, wine, bitterness, grapefruit, aromas, sour]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Gypsy Inc Gyp Wit</td>\n",
       "      <td>0.589277</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.708873</td>\n",
       "      <td>0.602542</td>\n",
       "      <td>0.001899</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>2.3</td>\n",
       "      <td>-0.013550</td>\n",
       "      <td>[orange, oranges, bitter, citrus, citrusy, sweetness, brewdog, bitterness, grapefruit, drinking]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Galway Bay / Begyle Brewing Goodbye Blue Monday</td>\n",
       "      <td>0.585319</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.688692</td>\n",
       "      <td>0.585388</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.000410</td>\n",
       "      <td>[orange, bitter, citrus, ipa, bitterness, grapefruit, oatmeal, tasty, fruit, fresh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Trolden Railroad Rye</td>\n",
       "      <td>0.575663</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.688169</td>\n",
       "      <td>0.584944</td>\n",
       "      <td>-0.000227</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>1.9</td>\n",
       "      <td>-0.009247</td>\n",
       "      <td>[orange, oranges, bitter, citrus, sweetness, sour, fruit, bottled, fresh, fruity]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bayerischer Bahnhof Original Leipziger Gose</td>\n",
       "      <td>0.569178</td>\n",
       "      <td>4.6</td>\n",
       "      <td>0.684010</td>\n",
       "      <td>0.581408</td>\n",
       "      <td>0.016825</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>2.4</td>\n",
       "      <td>-0.014754</td>\n",
       "      <td>[orange, bitter, citrus, drink, sour, drinkable, sourness, fruit, spicy, acidic]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Le Trou du Diable La Buteuse Brassin SpÃ©cial (Calvados)</td>\n",
       "      <td>0.565809</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.691271</td>\n",
       "      <td>0.587581</td>\n",
       "      <td>0.008543</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.023053</td>\n",
       "      <td>[orange, tasting, bitter, citrus, sweetness, alcohol, ale, sour, fruit, spicy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Giesinger MÃ¤rzen Festbier</td>\n",
       "      <td>0.555965</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.658988</td>\n",
       "      <td>0.560140</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>1.3</td>\n",
       "      <td>-0.004329</td>\n",
       "      <td>[orange, bitter, brewery, sweetness, bitterness, drinkable, fruit, mellow, bottled, fruity]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Schneeeule Kennedy</td>\n",
       "      <td>0.555827</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.701773</td>\n",
       "      <td>0.596507</td>\n",
       "      <td>0.002021</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.040983</td>\n",
       "      <td>[orange, citrus, citrusy, bitterness, grapefruit, fruitiness, sour, sourness, sourish, fruit]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        beer     score   abv  \\\n",
       "129                              Wacken Beer WalkÃ¼renschluck  0.601220   7.8   \n",
       "55                            Jabeerwocky / Profesja Basista  0.596330   5.9   \n",
       "34                                   De Ranke CuvÃ©e De Ranke  0.595826   7.0   \n",
       "45                                         Gypsy Inc Gyp Wit  0.589277   4.7   \n",
       "42           Galway Bay / Begyle Brewing Goodbye Blue Monday  0.585319   6.6   \n",
       "125                                     Trolden Railroad Rye  0.575663   5.1   \n",
       "15               Bayerischer Bahnhof Original Leipziger Gose  0.569178   4.6   \n",
       "67   Le Trou du Diable La Buteuse Brassin SpÃ©cial (Calvados)  0.565809  10.0   \n",
       "43                                 Giesinger MÃ¤rzen Festbier  0.555965   5.7   \n",
       "100                                       Schneeeule Kennedy  0.555827   3.0   \n",
       "\n",
       "     LSH_score  LSH_score_weighted  collab_score  collab_score_weighted  \\\n",
       "129   0.709152            0.602780      0.000534               0.000080   \n",
       "55    0.705066            0.599306      0.000823               0.000123   \n",
       "34    0.695959            0.591565      0.028405               0.004261   \n",
       "45    0.708873            0.602542      0.001899               0.000285   \n",
       "42    0.688692            0.585388      0.002269               0.000340   \n",
       "125   0.688169            0.584944     -0.000227              -0.000034   \n",
       "15    0.684010            0.581408      0.016825               0.002524   \n",
       "67    0.691271            0.587581      0.008543               0.001281   \n",
       "43    0.658988            0.560140      0.001023               0.000153   \n",
       "100   0.701773            0.596507      0.002021               0.000303   \n",
       "\n",
       "     abv_diff  abv_weight  \\\n",
       "129       0.8   -0.001639   \n",
       "55        1.1   -0.003099   \n",
       "34        0.0   -0.000000   \n",
       "45        2.3   -0.013550   \n",
       "42        0.4   -0.000410   \n",
       "125       1.9   -0.009247   \n",
       "15        2.4   -0.014754   \n",
       "67        3.0   -0.023053   \n",
       "43        1.3   -0.004329   \n",
       "100       4.0   -0.040983   \n",
       "\n",
       "                                                                                                notes  \n",
       "129         [orange, bitter, citrus, sweetness, alcohol, bitterness, alcoholic, fruit, spicy, fruity]  \n",
       "55             [orange, bitter, citrus, sweetness, bitterness, sour, drinkable, grapes, fruit, spicy]  \n",
       "34               [orange, bitter, citrus, sweetness, ale, wine, bitterness, grapefruit, aromas, sour]  \n",
       "45   [orange, oranges, bitter, citrus, citrusy, sweetness, brewdog, bitterness, grapefruit, drinking]  \n",
       "42                [orange, bitter, citrus, ipa, bitterness, grapefruit, oatmeal, tasty, fruit, fresh]  \n",
       "125                 [orange, oranges, bitter, citrus, sweetness, sour, fruit, bottled, fresh, fruity]  \n",
       "15                   [orange, bitter, citrus, drink, sour, drinkable, sourness, fruit, spicy, acidic]  \n",
       "67                     [orange, tasting, bitter, citrus, sweetness, alcohol, ale, sour, fruit, spicy]  \n",
       "43        [orange, bitter, brewery, sweetness, bitterness, drinkable, fruit, mellow, bottled, fruity]  \n",
       "100     [orange, citrus, citrusy, bitterness, grapefruit, fruitiness, sour, sourness, sourish, fruit]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def recommend_beer(query_embedding, df_train, user_name, abv_desired, n_clusters=15):    \n",
    "    # Query the LSH index\n",
    "    candidates = lsh.query(query_embedding)\n",
    "\n",
    "    # Filter bucket vectors and metadata\n",
    "    bucket_data = df_train[df_train[\"id\"].isin(list(candidates[0]))]\n",
    "    bucket_vectors = np.vstack(bucket_data[\"sbert_embedding\"].to_numpy())\n",
    "    \n",
    "    # Extract subgenre information\n",
    "    subgenres = bucket_data[\"subgenre\"].values  # Adjust column name as necessary\n",
    "    \n",
    "    # Perform clustering on bucket vectors\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init='auto', random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(bucket_vectors)\n",
    "    \n",
    "    # Assign query to the nearest cluster\n",
    "    query_cluster = kmeans.predict(query_embedding)[0]\n",
    "    \n",
    "    perplexities = [50]\n",
    "    n_iters = [10000]\n",
    "    learning_rates = [100, 200]\n",
    "    \n",
    "    param_combinations = [(p, n, lr) for p in perplexities for n in n_iters for lr in learning_rates]\n",
    "    \n",
    "    #for perplexity, n_iter, learning_rate in param_combinations:\n",
    "    #    plot_bucket(bucket_vectors, cluster_labels, subgenres, perplexity, n_iter, learning_rate)\n",
    "        \n",
    "    # Filter beers in the same cluster as the query\n",
    "    cluster_indices = np.where(cluster_labels == query_cluster)[0]\n",
    "    cluster_vectors = bucket_vectors[cluster_indices]\n",
    "    cluster_beers = bucket_data.iloc[cluster_indices]\n",
    "    \n",
    "    # Compute similarities within the selected cluster\n",
    "    sims = cosine_similarity(query_embedding, cluster_vectors)[0]\n",
    "\n",
    "    # Perform collaborative filtering\n",
    "    predcicted_rating_user = collab_df.loc[user_name]\n",
    "    \n",
    "    beer_LSH = pd.DataFrame({\n",
    "        'similarity': sims,\n",
    "        'beer': cluster_beers[\"name\"].values,  # Adjust column name if necessary\n",
    "    })\n",
    "    \n",
    "    LSH_score = beer_LSH.groupby('beer')['similarity'].mean()\n",
    "    \n",
    "    collab_filtering_scores = predcicted_rating_user[LSH_score.index.tolist()]\n",
    "    \n",
    "    # Penalise difference in abv by using a nonlinear function penalising greater differences more\n",
    "    abv = beer_info.loc[LSH_score.index.tolist()][\"abv\"]\n",
    "\n",
    "    alpha = 0.05\n",
    "    if abv_desired == 0:\n",
    "        abv_weight = -2 * abs(abv - abv_desired)\n",
    "    else:\n",
    "        abv_weight = -alpha * ((abv - abv_desired)**2) / (abv_desired**1.5 + 1)\n",
    "    \n",
    "\n",
    "    # Combine weights and scores\n",
    "    weighted_score = (\n",
    "        0.85 * LSH_score +\n",
    "        0.15 * collab_filtering_scores +\n",
    "        abv_weight\n",
    "    )\n",
    "    \n",
    "    # Create final DataFrame\n",
    "    beer_weighted_score = pd.DataFrame({\n",
    "        'beer': LSH_score.index,\n",
    "        'score': weighted_score,\n",
    "        'abv': abv.values,\n",
    "        'LSH_score': LSH_score.values,\n",
    "        'LSH_score_weighted': LSH_score.values * 0.85,\n",
    "        'collab_score': collab_filtering_scores.values,\n",
    "        'collab_score_weighted': collab_filtering_scores.values * 0.15,\n",
    "        'abv_diff': abs(abv.values - abv_desired),\n",
    "        'abv_weight': abv_weight.values\n",
    "    })\n",
    "    \n",
    "    # Remove index of weighted score and keep the beer name as a column\n",
    "    beer_weighted_score.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Get the 10 beers with the highest weighted scores\n",
    "    beer_weighted_score = beer_weighted_score.sort_values(by='score', ascending=False).head(10)\n",
    "    \n",
    "    # Apply getThemes to each beer in the DataFrame\n",
    "    beer_weighted_score['notes'] = beer_weighted_score['beer'].apply(lambda x: getThemes(df_train, x, query_embedding))\n",
    "\n",
    "\n",
    "    return beer_weighted_score.sort_values(by='score', ascending=False)\n",
    "    \n",
    "\n",
    "# Create a query\n",
    "test_query = \"Light, refreshing bitter beer with a orange taste\"\n",
    "user_name = \"Jerseyislandbeer\"\n",
    "query_embedding = encode_sbert(test_query).reshape(1, -1)\n",
    "beer_recommendations= recommend_beer(query_embedding, df_train, user_name, 7)\n",
    "\n",
    "print(\"Top 5 recommended beers:\")\n",
    "# Set max column width to display full array\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(beer_recommendations.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes for improvement\n",
    "Add better stop-words, flavor, flavour, flavors etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation settup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
