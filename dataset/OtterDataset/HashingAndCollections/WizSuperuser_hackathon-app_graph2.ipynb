{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "MODEL_ID=\"gemini-1.5-flash-001\"\n",
    "PROJECT_ID=\"genai-exchange-hackathon\"\n",
    "REGION=\"asia-south1\"\n",
    "\n",
    "llm2 = ChatVertexAI(model_name=MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.2-90b-text-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import vertexai\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "vertexai.init(\n",
    "        project=os.environ.get(\"VERTEX_PROJECT_ID\"),\n",
    "        location=os.environ.get(\"VERTEX_PROJECT_LOCATION\")\n",
    "        )\n",
    "embeddings = VertexAIEmbeddings(model_name=\"text-embedding-004\", location=REGION, project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "def get_vector_store():\n",
    "    QDRANT_URL=os.environ.get(\"QDRANT_URL\")\n",
    "    QDRANT_API_KEY=os.environ.get(\"QDRANT_API_KEY\")\n",
    "    vectorstore = QdrantVectorStore.from_existing_collection(\n",
    "        collection_name=\"dsa_notes\",\n",
    "        embedding=embeddings,\n",
    "        url=QDRANT_URL,\n",
    "        api_key=QDRANT_API_KEY,\n",
    "    )\n",
    "    return vectorstore\n",
    "\n",
    "vectorstore = get_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "# from langchain_core.documents import Document\n",
    "# ds = datasets.load_from_disk(os.path.join(\"usaco_datasets\", \"usaco_v3_sampled_with_tests\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# input_states = [\n",
    "#     {\n",
    "#         \"name\": row[\"name\"],\n",
    "#         \"problems\": [(\"question\", row[\"description\"])] + [(\"solution\", row[\"solution\"])],\n",
    "#         \"test_cases\": row[\"test_cases\"],\n",
    "#         \"runtime_limit\": row[\"runtime_limit\"],\n",
    "#         \"problem_level\": row[\"problem_level\"],\n",
    "#     }\n",
    "#     for row in ds\n",
    "# ]\n",
    "\n",
    "# documents = [\n",
    "#     Document(\n",
    "#         page_content=str(row[\"problems\"]),\n",
    "#         metadata={\"runtime_limit\": row[\"runtime_limit\"], \"problem_level\": row[\"problem_level\"]}\n",
    "#         ) \n",
    "#     for row in input_states\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('question', '\\nFarmer John has $N$ ($1 \\\\leq N \\\\leq 2 \\\\cdot 10^5$) farms, numbered from $1$ to\\n$N$. It is known that FJ closes farm $i$ at time $c_i$. Bessie wakes up at time\\n$S$, and wants to maximize the productivity of her day by visiting as many farms\\nas possible before they close. She plans to visit farm $i$ on time $t_i + S$.\\nBessie must arrive at a farm strictly before Farmer John closes it to actually visit it.\\n\\nBessie has $Q$ $(1 \\\\leq Q \\\\leq 2 \\\\cdot 10^5)$ queries. For each query, she gives\\nyou two integers $S$ and $V$. For each query, output whether Bessie can visit at\\nleast $V$ farms if she wakes up at time $S$.\\n\\nINPUT FORMAT (input arrives from the terminal / stdin):\\nThe first line consists of $N$ and $Q$.\\n\\nThe second line consists of $c_1, c_2, c_3 \\\\dots c_N$ ($1 \\\\leq c_i \\\\leq 10^6$).\\n\\nThe third line consists of $t_1, t_2, t_3 \\\\dots t_N$ ($1 \\\\leq t_i \\\\leq 10^6$).\\n\\nThe next $Q$ lines each consist of two integers $V$ ($1 \\\\leq V \\\\leq N$) and $S$\\n($1 \\\\leq S \\\\leq 10^6$).\\n\\nOUTPUT FORMAT (print output to the terminal / stdout):\\nFor each of the $Q$ queries, output YES or NO on a new line.\\n\\nSAMPLE INPUT:\\n5 5\\n3 5 7 9 12\\n4 2 3 3 8\\n1 5\\n1 6\\n3 3\\n4 2\\n5 1\\nSAMPLE OUTPUT: \\nYES\\nNO\\nYES\\nYES\\nNO\\n\\nFor the first query, Bessie will visit the farms at time $t = [9, 7, 8, 8, 13]$,\\nso she will only get to visit farm $4$ on time before FJ closes the farm.\\n\\nFor the second query, Bessie will not be able to visit any of the farms on time.\\n\\nFor the third query, Bessie will visit farms $3, 4, 5$ on time.\\n\\nFor the fourth and fifth queries, Bessie will be able to visit all but the first\\nfarm on time.\\n\\nSCORING:\\nInputs 2-4: $N,Q\\\\le 10^3$Inputs 5-9: $c_i, t_i \\\\le 20$Inputs 10-17: No additional constraints.\\n\\n\\nProblem credits: Chongtian Ma\\n'), ('solution', '\\n(Analysis by Alex Fan)\\nSubtask 1: $N, Q < 10^3$.\\nWe can directly simulate the process according to the problem statement. For\\neach query, we iterate through all the farms and see if we can visit it in time,\\n$t_i + S < c_i$. Each query takes $\\\\mathcal{O}(N)$ operations, giving us\\nan $\\\\mathcal{O}(NQ)$ solution.\\nAlex Fan\\'s C++ Code:\\n\\nusing namespace std;\\n\\n#include <iostream>\\n\\n#define MAXN 200005\\n\\nint N,Q,c[MAXN],t[MAXN];\\n\\nint main() {\\n\\tcin >> N >> Q;\\n\\tfor(int i = 0;i < N;++i) {\\n\\t\\tcin >> c[i];\\n\\t}\\n\\tfor(int i = 0;i < N;++i) {\\n\\t\\tcin >> t[i];\\n\\t}\\n\\n\\tfor(int i = 0;i < Q;++i) {\\n\\t\\tint v,s;\\n\\t\\tcin >> v >> s;\\n\\t\\tint uwu = 0;\\n\\t\\tfor(int j = 0;j < N;++j) {\\n\\t\\t\\tif(s + t[j] < c[j]) {\\n\\t\\t\\t\\tuwu++;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\tcout << (uwu >= v ? \"YES\" : \"NO\") << endl;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nPython Code:\\n\\nN, Q = (int(x) for x in input().split())\\nc = [int(x) for x in input().split()]\\nt = [int(x) for x in input().split()]\\nfor _ in range(Q):\\n    V, S = (int(x) for x in input().split())\\n    uwu = 0\\n    for i in range(N):\\n        if(S + t[i] < c[i]):\\n            uwu += 1\\n    print(\"YES\" if (uwu >= V) else \"NO\")\\n\\nSubtask 2: $c_i, t_i < 20$\\nThe key observation is to notice that we can rearrange the formula\\n$t_i + S < c_i$ as $c_i - t_i > S$. Therefore, for each farm, we can take\\nadvantage of the fact that $c_i - t_i$ can only be between $-20$ to $20$.\\nIf $c_i - t_i \\\\leq 0$, then we can get rid of it since we can never make it in\\ntime. Otherwise, there are only $21$ possible values despite there being\\n$2 \\\\cdot 10^5$ farms. We exploit this observation by grouping the same\\n$c_i - t_i$ values together. This can be implemented with a map, or, since the\\nvalues are between $0$ and $20$, an array of length $21$.\\nFinally, to check if a query works, we iterate through the array and count the\\nnumber of valid barns that satisfy the inequality. The valid range is always\\nthe continuous interval to the right of $S$.\\nEach query takes $\\\\mathcal{O}(MAXC)$ time where $MAXC \\\\leq 20$ is the maximum\\nvalue of $c_i$, so the total complexity is $\\\\mathcal{O}(N + Q \\\\cdot MAXC)$\\nAlex Fan\\'s C++ Code:\\n\\nusing namespace std;\\n\\n#include <iostream>\\n#include <map>\\n\\n#define MAXN 200005\\n#define MAXA 1000005\\n\\nint N,Q,c[MAXN],t[MAXN],cnt[MAXA],max_element;\\n\\nint main() {\\n\\tcin >> N >> Q;\\n\\tfor(int i = 0;i < N;++i) {\\n\\t\\tcin >> c[i];\\n\\t}\\n\\tfor(int i = 0;i < N;++i) {\\n\\t\\tcin >> t[i];\\n\\t\\tif(c[i] > t[i]) {\\n\\t\\t\\t// Maintain the count array of c[i] - t[i]\\n\\t\\t\\tcnt[c[i] - t[i]]++;\\n\\t\\t}\\n\\t\\t// We can also replace max_element as the constant 20\\n\\t\\tmax_element = max(max_element,c[i] - t[i]);\\n\\t}\\n\\n\\tfor(int i = 0;i < Q;++i) {\\n\\t\\tint v,s;\\n\\t\\tcin >> v >> s;\\n\\t\\tint uwu = 0;\\n\\t\\tif(N <= 1e3) {\\n\\t\\t\\t// Subtask 1\\n\\t\\t\\tfor(int i = 0;i < N;++i) {\\n\\t\\t\\t\\tif(t[i] + s < c[i]) uwu++;\\n\\t\\t\\t}\\n\\t\\t}else if(max_element <= 20) {\\n\\t\\t\\t// Subatsk 2\\n\\t\\t\\tfor(int i = s + 1;i <= max_element;++i) {\\n\\t\\t\\t\\tuwu += cnt[i];\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\tcout << (uwu >= v ? \"YES\" : \"NO\") << endl;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nPython Code:\\n\\nN, Q = (int(x) for x in input().split())\\nc = [int(x) for x in input().split()]\\nt = [int(x) for x in input().split()]\\n\\ncnt = [0] * 25\\n\\nfor i in range(N):\\n\\tif(c[i] > t[i]):\\n\\t\\tcnt[c[i] - t[i]] += 1\\n\\nfor _ in range(Q):\\n    V, S = (int(x) for x in input().split())\\n    uwu = 0\\n    for i in range(S + 1,25,1):\\n        uwu += cnt[i]\\n    print(\"YES\" if (uwu >= V) else \"NO\")\\n\\n\\nFull Credit:\\nThe final step is to efficiently count the number of farms such that their\\n$c_i - t_i$ is greater than some query value $S$. This is a standard setup that\\ncan be solved by first sorting all the differences and applying binary search. This gives an $\\\\mathcal{O}(N\\\\log N + Q\\\\log N)$ solution.\\nAlternatively, you can further observe that after we sort all the values of\\n$c_i - t_i$, the query $(V, S)$ is \"YES\" if and only if the $V$-th largest value\\nof $c_i - t_i$ is more than $S$, which can be done by checking the $i$-th\\nelement of the sorted array. Reference Brandon Wang\\'s Python Code for the\\nimplementation. This also results in a $\\\\mathcal{O}(N\\\\log N + Q)$ solution.\\nAlex Fan\\'s C++ Code:\\n\\nusing namespace std;\\n\\n#include <iostream>\\n#include <algorithm>\\n#include <fstream>\\n\\n#define MAXN 200005\\n\\nint N,Q,c[MAXN],t[MAXN];\\n\\nint main() {\\n\\tcin >> N >> Q;\\n\\tfor(int i = 0;i < N;++i) {\\n\\t\\tcin >> c[i];\\n\\t}\\n\\tfor(int i = 0;i < N;++i) {\\n\\t\\tcin >> t[i];\\n\\t\\tc[i] -= t[i];\\n\\t}\\n\\n\\tsort(c,c + N);\\n\\n\\tfor(int i = 0;i < Q;++i) {\\n\\t\\tint v,s;\\n\\t\\tcin >> v >> s;\\n\\t\\tint uwu = N - (lower_bound(c,c + N,s + 1) - c);\\n\\t\\tcout << (uwu >= v ? \"YES\" : \"NO\") << endl;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nBrandon Wang\\'s Python Code\\n\\nN, Q = (int(x) for x in input().split())\\nc = [int(x) for x in input().split()]\\nt = [int(x) for x in input().split()]\\ndiffs = sorted([ci - ti for ci, ti in zip(c, t)], reverse=True)\\nfor _ in range(Q):\\n    V, S = (int(x) for x in input().split())\\n    print(\"YES\" if (V <= N and (V <= 0 or diffs[V-1] > S)) else \"NO\")\\n\\n')]\n"
     ]
    }
   ],
   "source": [
    "# Print the first row of input_states to see the structure\n",
    "# print(\"First row of input_states:\")\n",
    "print(input_states[0][\"problems\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bronze\n",
      "Maximizing Productivity\n"
     ]
    }
   ],
   "source": [
    "print(input_states[0][\"problem_level\"])\n",
    "print(input_states[0][\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['768a4e07-12ee-4b64-9f61-7737ffa3d5c0',\n",
       " '69c79a9a-6ab9-4b51-a2f9-1058c5749969',\n",
       " '1245a480-9814-4101-bfa2-56023e265245',\n",
       " 'd34212f7-3255-409e-85bf-0a3fa638585c',\n",
       " '0384379c-3f9e-49f8-b5a6-facf419d0b8f',\n",
       " '96afb12c-f547-4372-849c-9bc8fe84b0ab',\n",
       " '83314dd0-8e5a-4536-852c-cbc55a5a0431',\n",
       " '42df10b0-f55b-4823-a0fb-8b2d820d0f80',\n",
       " 'd5594839-4c9c-4fe6-b264-f84eafc923af',\n",
       " 'f0e31e80-93a2-44dc-b8d7-18daa9ea7369',\n",
       " '12cfc7dc-219b-4326-8468-506078e824ad',\n",
       " '6c0b5e6e-a050-449f-81db-26300127d1bc',\n",
       " '294511b0-4685-4eec-888b-012b2f4d1863',\n",
       " 'a000e747-fba6-4e56-890d-4838ea3a24ac',\n",
       " '1d3282f8-51c1-4333-9959-7f30fb342367',\n",
       " '7cd8b31a-ffab-4721-85e1-2d31103ab364',\n",
       " '8addfa88-aa46-43d7-8785-34879b018658',\n",
       " '31c6f01c-9f5a-40f4-af9e-23d03ce6d158',\n",
       " 'f82ea617-c822-4942-88e5-da4206b821e7',\n",
       " '219d4e16-f9de-466c-9ecb-e3bc6e9afe98',\n",
       " '4dea184a-6383-4f09-8b1f-89edab0be658',\n",
       " '6ed9ab88-1cca-4ace-ac67-e9dc3de1950d',\n",
       " '8d9cb9cc-f968-44d3-83a2-0ec10b6df132',\n",
       " 'fd58b349-8a7f-4327-ae3f-2f23588fe60d',\n",
       " 'e8ecfa5c-52e0-4bb8-9e42-a3dad9d8895e',\n",
       " '28fa0d46-dc3d-4a9d-ac9f-ed22630455aa',\n",
       " 'c3c09c05-cb4f-4bae-a2ab-3682a02f5e39',\n",
       " '6cdf4bbc-6c79-4dec-803b-eff781b08f57',\n",
       " '13f41759-7556-4e25-a291-49ec893af848',\n",
       " '9d7fcb25-ab26-4638-9580-74a83a807b90']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "vectorstore.add_documents(documents=documents, ids=uuids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [('question', '\\nFarmer John has $N$ ($1 \\\\leq N \\\\leq 2 \\\\cdot 10^5$) farms, numbered from $1$ to\\n$N$. It is known that FJ closes farm $i$ at time $c_i$. Bessie wakes up at time\\n$S$, and wants to maximize the productivity of her day by visiting as many farms\\nas possible before they close. She plans to visit farm $i$ on time $t_i + S$.\\nBessie must arrive at a farm strictly before Farmer John closes it to actually visit it.\\n\\nBessie has $Q$ $(1 \\\\leq Q \\\\leq 2 \\\\cdot 10^5)$ queries. For each query, she gives\\nyou two integers $S$ and $V$. For each query, output whether Bessie can visit at\\nleast $V$ farms if she wakes up at time $S$.\\n\\nINPUT FORMAT (input arrives from the terminal / stdin):\\nThe first line consists of $N$ and $Q$.\\n\\nThe second line consists of $c_1, c_2, c_3 \\\\dots c_N$ ($1 \\\\leq c_i \\\\leq 10^6$).\\n\\nThe third line consists of $t_1, t_2, t_3 \\\\dots t_N$ ($1 \\\\leq t_i \\\\leq 10^6$).\\n\\nThe next $Q$ lines each consist of two integers $V$ ($1 \\\\leq V \\\\leq N$) and $S$\\n($1 \\\\leq S \\\\leq 10^6$).\\n\\nOUTPUT FORMAT (print output to the terminal / stdout):\\nFor each of the $Q$ queries, output YES or NO on a new line.\\n\\nSAMPLE INPUT:\\n5 5\\n3 5 7 9 12\\n4 2 3 3 8\\n1 5\\n1 6\\n3 3\\n4 2\\n5 1\\nSAMPLE OUTPUT: \\nYES\\nNO\\nYES\\nYES\\nNO\\n\\nFor the first query, Bessie will visit the farms at time $t = [9, 7, 8, 8, 13]$,\\nso she will only get to visit farm $4$ on time before FJ closes the farm.\\n\\nFor the second query, Bessie will not be able to visit any of the farms on time.\\n\\nFor the third query, Bessie will visit farms $3, 4, 5$ on time.\\n\\nFor the fourth and fifth queries, Bessie will be able to visit all but the first\\nfarm on time.\\n\\nSCORING:\\nInputs 2-4: $N,Q\\\\le 10^3$Inputs 5-9: $c_i, t_i \\\\le 20$Inputs 10-17: No additional constraints.\\n\\n\\nProblem credits: Chongtian Ma\\n'), ('solution', '\\n(Analysis by Alex Fan)\\nSubtask 1: $N, Q < 10^3$.\\nWe can directly simulate the process according to the problem statement. For\\neach query, we iterate through all the farms and see if we can visit it in time,\\n$t_i + S < c_i$. Each query takes $\\\\mathcal{O}(N)$ operations, giving us\\nan $\\\\mathcal{O}(NQ)$ solution.\\nAlex Fan\\'s C++ Code:\\n\\nusing namespace std;\\n\\n#include <iostream>\\n\\n#define MAXN 200005\\n\\nint N,Q,c[MAXN],t[MAXN];\\n\\nint main() {\\n\\tcin >> N >> Q;\\n\\tfor(int i = 0;i < N;++i) {\\n\\t\\tcin >> c[i];\\n\\t}\\n\\tfor(int i = 0;i < N;++i) {\\n\\t\\tcin >> t[i];\\n\\t}\\n\\n\\tfor(int i = 0;i < Q;++i) {\\n\\t\\tint v,s;\\n\\t\\tcin >> v >> s;\\n\\t\\tint uwu = 0;\\n\\t\\tfor(int j = 0;j < N;++j) {\\n\\t\\t\\tif(s + t[j] < c[j]) {\\n\\t\\t\\t\\tuwu++;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\tcout << (uwu >= v ? \"YES\" : \"NO\") << endl;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nPython Code:\\n\\nN, Q = (int(x) for x in input().split())\\nc = [int(x) for x in input().split()]\\nt = [int(x) for x in input().split()]\\nfor _ in range(Q):\\n    V, S = (int(x) for x in input().split())\\n    uwu = 0\\n    for i in range(N):\\n        if(S + t[i] < c[i]):\\n            uwu += 1\\n    print(\"YES\" if (uwu >= V) else \"NO\")\\n\\nSubtask 2: $c_i, t_i < 20$\\nThe key observation is to notice that we can rearrange the formula\\n$t_i + S < c_i$ as $c_i - t_i > S$. Therefore, for each farm, we can take\\nadvantage of the fact that $c_i - t_i$ can only be between $-20$ to $20$.\\nIf $c_i - t_i \\\\leq 0$, then we can get rid of it since we can never make it in\\ntime. Otherwise, there are only $21$ possible values despite there being\\n$2 \\\\cdot 10^5$ farms. We exploit this observation by grouping the same\\n$c_i - t_i$ values together. This can be implemented with a map, or, since the\\nvalues are between $0$ and $20$, an array of length $21$.\\nFinally, to check if a query works, we iterate through the array and count the\\nnumber of valid barns that satisfy the inequality. The valid range is always\\nthe continuous interval to the right of $S$.\\nEach query takes $\\\\mathcal{O}(MAXC)$ time where $MAXC \\\\leq 20$ is the maximum\\nvalue of $c_i$, so the total complexity is $\\\\mathcal{O}(N + Q \\\\cdot MAXC)$\\nAlex Fan\\'s C++ Code:\\n\\nusing namespace std;\\n\\n#include <iostream>\\n#include <map>\\n\\n#define MAXN 200005\\n#define MAXA 1000005\\n\\nint N,Q,c[MAXN],t[MAXN],cnt[MAXA],max_element;\\n\\nint main() {\\n\\tcin >> N >> Q;\\n\\tfor(int i = 0;i < N;++i) {\\n\\t\\tcin >> c[i];\\n\\t}\\n\\tfor(int i = 0;i < N;++i) {\\n\\t\\tcin >> t[i];\\n\\t\\tif(c[i] > t[i]) {\\n\\t\\t\\t// Maintain the count array of c[i] - t[i]\\n\\t\\t\\tcnt[c[i] - t[i]]++;\\n\\t\\t}\\n\\t\\t// We can also replace max_element as the constant 20\\n\\t\\tmax_element = max(max_element,c[i] - t[i]);\\n\\t}\\n\\n\\tfor(int i = 0;i < Q;++i) {\\n\\t\\tint v,s;\\n\\t\\tcin >> v >> s;\\n\\t\\tint uwu = 0;\\n\\t\\tif(N <= 1e3) {\\n\\t\\t\\t// Subtask 1\\n\\t\\t\\tfor(int i = 0;i < N;++i) {\\n\\t\\t\\t\\tif(t[i] + s < c[i]) uwu++;\\n\\t\\t\\t}\\n\\t\\t}else if(max_element <= 20) {\\n\\t\\t\\t// Subatsk 2\\n\\t\\t\\tfor(int i = s + 1;i <= max_element;++i) {\\n\\t\\t\\t\\tuwu += cnt[i];\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\tcout << (uwu >= v ? \"YES\" : \"NO\") << endl;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nPython Code:\\n\\nN, Q = (int(x) for x in input().split())\\nc = [int(x) for x in input().split()]\\nt = [int(x) for x in input().split()]\\n\\ncnt = [0] * 25\\n\\nfor i in range(N):\\n\\tif(c[i] > t[i]):\\n\\t\\tcnt[c[i] - t[i]] += 1\\n\\nfor _ in range(Q):\\n    V, S = (int(x) for x in input().split())\\n    uwu = 0\\n    for i in range(S + 1,25,1):\\n        uwu += cnt[i]\\n    print(\"YES\" if (uwu >= V) else \"NO\")\\n\\n\\nFull Credit:\\nThe final step is to efficiently count the number of farms such that their\\n$c_i - t_i$ is greater than some query value $S$. This is a standard setup that\\ncan be solved by first sorting all the differences and applying binary search. This gives an $\\\\mathcal{O}(N\\\\log N + Q\\\\log N)$ solution.\\nAlternatively, you can further observe that after we sort all the values of\\n$c_i - t_i$, the query $(V, S)$ is \"YES\" if and only if the $V$-th largest value\\nof $c_i - t_i$ is more than $S$, which can be done by checking the $i$-th\\nelement of the sorted array. Reference Brandon Wang\\'s Python Code for the\\nimplementation. This also results in a $\\\\mathcal{O}(N\\\\log N + Q)$ solution.\\nAlex Fan\\'s C++ Code:\\n\\nusing namespace std;\\n\\n#include <iostream>\\n#include <algorithm>\\n#include <fstream>\\n\\n#define MAXN 200005\\n\\nint N,Q,c[MAXN],t[MAXN];\\n\\nint main() {\\n\\tcin >> N >> Q;\\n\\tfor(int i = 0;i < N;++i) {\\n\\t\\tcin >> c[i];\\n\\t}\\n\\tfor(int i = 0;i < N;++i) {\\n\\t\\tcin >> t[i];\\n\\t\\tc[i] -= t[i];\\n\\t}\\n\\n\\tsort(c,c + N);\\n\\n\\tfor(int i = 0;i < Q;++i) {\\n\\t\\tint v,s;\\n\\t\\tcin >> v >> s;\\n\\t\\tint uwu = N - (lower_bound(c,c + N,s + 1) - c);\\n\\t\\tcout << (uwu >= v ? \"YES\" : \"NO\") << endl;\\n\\t}\\n\\n\\treturn 0;\\n}\\n\\nBrandon Wang\\'s Python Code\\n\\nN, Q = (int(x) for x in input().split())\\nc = [int(x) for x in input().split()]\\nt = [int(x) for x in input().split()]\\ndiffs = sorted([ci - ti for ci, ti in zip(c, t)], reverse=True)\\nfor _ in range(Q):\\n    V, S = (int(x) for x in input().split())\\n    print(\"YES\" if (V <= N and (V <= 0 or diffs[V-1] > S)) else \"NO\")\\n\\n')] [{'runtime_limit': 2, 'problem_level': 'bronze', '_id': '768a4e07-12ee-4b64-9f61-7737ffa3d5c0', '_collection_name': 'dsa_notes'}]\n",
      "* [('question', \"\\nFarmer Nhoj dropped Bessie in the middle of nowhere! At time $t=0$, Bessie is\\nlocated at $x=0$ on an infinite number line. She frantically searches for an\\nexit by moving left or right by $1$ unit each second. However, there actually is\\nno exit and after $T$ seconds, Bessie is back at $x=0$, tired and resigned. \\n\\nFarmer Nhoj tries to track Bessie but only knows how many times Bessie crosses\\n$x=.5, 1.5, 2.5, \\\\ldots, (N-1).5$, given by an array $A_0,A_1,\\\\dots,A_{N-1}$\\n($1\\\\leq N \\\\leq 10^5$, $1 \\\\leq A_i \\\\leq 10^6$). Bessie never reaches $x>N$ nor\\n$x<0$.\\n\\nIn particular, Bessie's route can be represented by a string of\\n$T = \\\\sum_{i=0}^{N-1} A_i$ $L$s and $R$s where the $i$th character represents\\nthe direction Bessie moves in during the $i$th second. The number of direction\\nchanges is defined as the number of occurrences of  $LR$s plus the number of\\noccurrences of $RL$s. \\n\\nPlease help Farmer Nhoj count the number of routes Bessie could have taken that\\nare consistent with $A$ and minimize the number of direction changes. It is\\nguaranteed that there is at least one valid route.\\n\\nINPUT FORMAT (input arrives from the terminal / stdin):\\nThe first line contains $N$. The second line contains $A_0,A_1,\\\\dots,A_{N-1}$.\\n\\nOUTPUT FORMAT (print output to the terminal / stdout):\\nThe number of routes Bessie could have taken, modulo $10^9+7$.\\n\\nSAMPLE INPUT:\\n2\\n4 6\\nSAMPLE OUTPUT: \\n2\\n\\nBessie must change direction at least 5 times. There are two routes \\ncorresponding to Bessie changing direction exactly 5 times:\\n\\n\\nRRLRLLRRLL\\nRRLLRRLRLL\\n\\nSCORING:\\nInputs 2-4: $N\\\\le 2$ and $\\\\max(A_i)\\\\le 10^3$Inputs 5-7: $N\\\\le 2$Inputs 8-11: $\\\\max(A_i)\\\\le 10^3$Inputs 12-21: No additional constraints.\\n\\n\\nProblem credits: Brandon Wang, Claire Zhang, and Benjamin Qi\\n\"), ('solution', \"\\n(Analysis by Claire Zhang and Brandon Wang)\\nLet's annotate each character in the route with a subscript that indicates which\\n$i.5$ point it passes. That is, we denote $i+1\\\\to i$ moves by $L_i$, and\\n$i\\\\to i+1$ moves by $R_i$. Then, our string must contain exactly\\n$B_i = \\\\frac{A_i}{2}$ $L_i$'s, and $B_i$ $R_i$'s. In any route with minimal\\nturns, we must have:\\nIf $B_i \\\\geq B_{i+1}$, then any $L_{i+1}$ must be followed by a $L_i$. \\nFurthermore, exactly $B_{i+1}$ $R_i$'s are followed by $R_{i+1}$'s, and the\\nother $B_i - B_{i+1}$  are followed by $L_i$'s.If $B_i \\\\leq B_{i+1}$,\\nthen any $R_i$ must be followed by a $R_{i+1}$.  Furthermore, exactly $B_i$\\n$L_{i+1}$'s are followed by $L_i$'s, and the other $B_{i+1} - B_i$  are followed\\nby $R_{i+1}$'s.\\nIn addition, we note that in any route, the final $L_{i+1}$ must be followed by\\nan $L_i$ (and not an $R_{i+1}$)  since otherwise Bessie would not have a way to\\nreturn to $0$.\\nWe claim that this is the only restriction. That is, to count the number of\\npaths, for each $i = 0, 1, \\\\ldots, N-2$ it suffices to count the number of ways\\nto pick which $R_i$'s that are followed by $R_{i+1}$'s if $B_i \\\\geq B_{i+1}$, \\nor which $L_{i+1}$'s followed by $L_i$'s if $B_i \\\\leq B_{i+1}$ (such that the\\nlast $L_{i+1}$ is followed by an $L_i$).  Then, any such assignment will produce\\na unique valid path.\\nUniqueness is clear, but to show validity, suppose we construct the route by\\nfollowing the assignments, where the route ends when the last $L_0$ is reached\\n(and all previous $L_0$'s are followed by $R_0$'s). We need to check that all of\\nthe $L_i$'s and $R_i$'s are actually used; since the number of $R_i$'s and\\n$L_i$'s is the same, we need to check that this path visits $B_i$ $L_i$'s. \\nWe will do this by induction, where $L_0$ is true by assumption. For the\\ninductive step, suppose $B_i$ $L_i$'s appear in the path. Then, if\\n$B_i \\\\geq B_{i+1}$, then $B_i-B_{i+1}$ $L_i$'s are preceded by $R_i$'s, and\\n$B_{i+1}$ of them are preceded by $L_{i+1}$'s. So, in order for all the $L_i$'s\\nto appear, all the $L_{i+1}$'s must also appear. Conversely, if\\n$B_i \\\\leq B_{i+1}$, then since the last $L_{i+1}$ is immediately followed by an\\n$L_i$,  if not all $L_{i+1}$'s appear then not all $L_i$'s can appear,\\ncontradicting the inductive hypothesis. So, the constructed path contains $B_i$\\n$L_i$'s for each $i$, and thus crosses $i.5$ exactly $2B_i = A_i$ times.\\nMinimality (i.e. the fact that exactly\\n$(B_0 - 1) + \\\\left(\\\\sum_{i=0}^{n-2} |B_i - B_{i+1}|\\\\right) + (B_{n-1})$  turns are made) follows by the\\nconstruction.\\nNow, if $B_i \\\\geq B_{i+1}$, then the number of assignments is just\\n$\\\\binom{B_i}{B_{i+1}}$. In the second the other case, since the last $L_{i+1}$\\nmust be followed by $L_i$, the answer is\\n$\\\\binom{B_{i+1}-1}{B_i-1}$.\\nThus we obtain our answer as a product of binomial coefficients:\\n$$\\n\\\\prod_{i=0}^{N-2} \\\\begin{cases} \\\\binom{B_i}{B_{i+1}} & \\\\text{ if } B_i \\\\geq B_{i+1} \\\\\\\\\\n\\\\binom{B_{i+1}-1}{B_i-1} & \\\\text{ otherwise } \\\\end{cases}\\n$$\\nLet $T = \\\\max_i A_i$. We can precompute factorials in $O(T)$ time. We can\\ncompute inverse factorials by first computing the modular inverse of $T!$ (e.g.,\\nby raising it to $MOD-2$ with binary exponentiation). Then we can obtain all\\nsmaller inverse factorials in decreasing order. Now we can compute each binomial\\ncoefficient in the desired expression in $O(1)$ time, for a total runtime of\\n$O(\\\\log(MOD) + T+N)$.\\nPython solution:\\n\\nP = int(1e9+7)\\nMAX_A = int(1e6+1)\\n \\n# computes a^n mod P\\ndef exp(a, n):\\n    if n == 0:\\n        return 1\\n    base = exp((a*a)%P, n // 2)\\n    return base if n%2 == 0 else (a*base)%P\\n \\n# initialize\\nfacts = [1]\\nfor i in range(1, MAX_A):\\n    facts.append((facts[-1] * i)%P)\\ninv_facts = [exp(facts[-1], P-2)]\\nfor i in range(MAX_A-1, 0, -1):\\n    inv_facts.append((inv_facts[-1] * i)%P)\\ninv_facts.reverse()\\n \\n# binom(n, m) = n!/(m!(n-m)!)\\nbinom = lambda n, m : (inv_facts[m] * (facts[n] * inv_facts[n-m])%P)%P\\n \\nN = int(input())\\nA = [int(x) for x in input().split()]\\nB = [a // 2 for a in A]\\n \\nans = 1\\nfor i in range(N-1):\\n    if B[i] >= B[i+1]:\\n        ans *= binom(B[i], B[i+1])\\n    else:\\n        ans *= binom(B[i+1]-1, B[i]-1)\\n    ans %= P\\n \\nprint(ans)\\n\\n\")] [{'runtime_limit': 2, 'problem_level': 'gold', '_id': '31c6f01c-9f5a-40f4-af9e-23d03ce6d158', '_collection_name': 'dsa_notes'}]\n"
     ]
    }
   ],
   "source": [
    "results = vectorstore.similarity_search(\n",
    "    \"Farmer John has $N$ ($1 \\\\leq N \\\\leq 2 \\\\cdot 10^5$) farms, numbered from $1$ to\\n$N$. It is known that FJ closes farm $i$ at time $c_i$. Bessie wakes up at time\\n$S$, and wants to maximize the productivity of her day by visiting as many farms\\nas possible before they close. She plans to visit\", k=2\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "tavily_search = TavilySearchResults(max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from operator import add\n",
    "# from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from graphviz import Digraph\n",
    "\n",
    "class State(MessagesState):\n",
    "    summary: str\n",
    "    context: str\n",
    "    enough_context: bool = False\n",
    "    safe: bool = True\n",
    "    graph: Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "simple_solver_prompt = \"\"\"You are helping with solving a student's questions about data structures and algorithms.\n",
    "\n",
    "These can be questions about definitions, help with debugging code or hard leetcode style problems to solve.\n",
    "\n",
    "Think carefully before answering any question. Explain your reasoning.\n",
    "\n",
    "Do not hallucinate. Do not make up facts. If you don't know how to answer a problem, just say so.\n",
    "\n",
    "Be concise.\"\"\"\n",
    "\n",
    "def simple_solver(state: State):\n",
    "    \n",
    "    messages = [SystemMessage(content=simple_solver_prompt)]\n",
    "    \n",
    "    # get summary if it exists\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    \n",
    "    # if there is summary, we add it to prompt\n",
    "    if summary:\n",
    "        \n",
    "        # add summary to system message\n",
    "        summary_message = f\"Summary of conversation earlier: {summary}\"\n",
    "        \n",
    "        # append summary to any newer message\n",
    "        messages += [HumanMessage(content=summary_message)]\n",
    "    \n",
    "    messages += state[\"messages\"]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    # NEED TO PREVENT CONTEXT FROM BALLOONING if I change it to list and want to persist that\n",
    "    return {\"context\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "socratic_prompt = \"\"\"You are a tutor trying to help a student gain a very strong understanding of a concept/problem. \n",
    "\n",
    "You are helping them with a problem and want to help them understand the concepts by figuring out the solution themselves with only nudges in the right direction.\n",
    "\n",
    "You have the solution above but the student has never seen it.\n",
    "\n",
    "If the student wants to learn about a new concept: use the solution to provide the necessary context. Then, based on that ask the student a question that requires them to apply the concept in code to help enhance their understanding.\n",
    "\n",
    "If the question is a problem to solve: based on the solution to the question, use the socratic method to guide the student towards the answer.\n",
    "\n",
    "Provide hints or prompt the student to think of the next step. If the student seems to be really stuggling with a concept, provide a larger hint. Always take a code-first approach when explaining, giving examples, or solving a problem.\n",
    "\n",
    "After the student has sufficiently solved the problem, provide the full solution including the code for their reference. Then ask them a question based on the concepts learned.\"\"\"\n",
    "#### New line not in graph.py yet above\n",
    "#### TEST!\n",
    "####\n",
    "def socratic(state: State):\n",
    "    messages = [SystemMessage(content=state[\"context\"] + socratic_prompt)]\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    if summary:\n",
    "        summary_message = f\"Summary of your conversation with the student: {summary}\"\n",
    "        messages += [HumanMessage(content=summary_message)]\n",
    "    messages += state[\"messages\"]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import RemoveMessage\n",
    "\n",
    "def summarize_conversation(state: State):\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    \n",
    "    if summary:\n",
    "        summary_message = (\n",
    "            f\"This is the summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Be concise and extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "    \n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "\n",
    "def should_summarize(state: State):\n",
    "    \"\"\"Return whether to summarize depending on length of messages\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    if len(messages) > 6:\n",
    "        return \"summarize\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.errors import NodeInterrupt\n",
    "from langchain_core.messages import AIMessage\n",
    "# Human in the loop\n",
    "def give_answer(state: State):\n",
    "    \"\"\"Ask the student if they want a complete solution\"\"\"\n",
    "    messages = state['messages']\n",
    "    if len(messages) > 2:\n",
    "        state[\"messages\"] += [AIMessage(\"Would you like me to provide you the complete answer? Please reply as yes or no.\")]\n",
    "        raise NodeInterrupt(f\"Recent chat longer than 6 messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_to_answer(state: State):\n",
    "    if len(state['messages']) > 2:\n",
    "        message = state['messages'][-1]\n",
    "        if 'yes' in str(message).lower():\n",
    "            state[\"messages\"] += [HumanMessage(\"yes\")] + [HumanMessage(state['context'])]\n",
    "            return 'summarize'\n",
    "    else:\n",
    "        return 'socratic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever\n",
    "\n",
    "notes_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "def retriever(state: State):\n",
    "    additional_context = notes_retriever.invoke(state['context'])\n",
    "    return {'context': state['context'] + '-----------------------'.join(x.page_content for x in additional_context)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7 Quicksort\\nThe quicksort algorithm has a worst-case running time of ‚.n2/on an input array\\nofnnumbers. Despite this slow worst-case running time, quicksort is often the best\\npractical choice for sorting because it is remarkably efﬁcient on the average: itsexpected running time is ‚.n lgn/, and the constant factors hidden in the ‚.n lgn/\\nnotation are quite small. It also has the advantage of sorting in place (see page 17),and it works well even in virtual-memory environments.\\nSection 7.1 describes the algorithm and an important subroutine used by quick-\\nsort for partitioning. Because the behavior of quicksort is complex, we start with\\nan intuitive discussion of its performance in Section 7.2 and postpone its precise\\nanalysis to the end of the chapter. Section 7.3 presents a version of quicksort that\\nuses random sampling. This algorithm has a good expected running time, and no\\nparticular input elicits its worst-case behavior. Section 7.4 analyzes the random-ized algorithm, showing that it runs in ‚.n\\n2/time in the worst case and, assuming\\ndistinct elements, in expected O.n lgn/time.\\n7.1 Description of quicksort\\nQuicksort, like merge sort, applies the divide-and-conquer paradigm introduced\\nin Section 2.3.1. Here is the three-step divide-and-conquer process for sorting atypical subarray AŒp : : r/c141 :\\nDivide: Partition (rearrange) the array AŒp : : r/c141 into two (possibly empty) subar-\\nraysAŒp : : q/NUL1/c141andAŒqC1::r/c141 such that each element of AŒp : : q/NUL1/c141is\\nless than or equal to AŒq/c141, which is, in turn, less than or equal to each element\\nofAŒqC1::r/c141 . Compute the index qas part of this partitioning procedure.\\nConquer: Sort the two subarrays AŒp : : q/NUL1/c141andAŒqC1::r/c141 by recursive calls\\nto quicksort.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes = notes_retriever.invoke(\"quick sort\")\n",
    "notes[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7 Quicksort\\nThe quicksort algorithm has a worst-case running time of ‚.n2/on an input array\\nofnnumbers. Despite this slow worst-case running time, quicksort is often the best\\npractical choice for sorting because it is remarkably efﬁcient on the average: itsexpected running time is ‚.n lgn/, and the constant factors hidden in the ‚.n lgn/\\nnotation are quite small. It also has the advantage of sorting in place (see page 17),and it works well even in virtual-memory environments.\\nSection 7.1 describes the algorithm and an important subroutine used by quick-\\nsort for partitioning. Because the behavior of quicksort is complex, we start with\\nan intuitive discussion of its performance in Section 7.2 and postpone its precise\\nanalysis to the end of the chapter. Section 7.3 presents a version of quicksort that\\nuses random sampling. This algorithm has a good expected running time, and no\\nparticular input elicits its worst-case behavior. Section 7.4 analyzes the random-ized algorithm, showing that it runs in ‚.n\\n2/time in the worst case and, assuming\\ndistinct elements, in expected O.n lgn/time.\\n7.1 Description of quicksort\\nQuicksort, like merge sort, applies the divide-and-conquer paradigm introduced\\nin Section 2.3.1. Here is the three-step divide-and-conquer process for sorting atypical subarray AŒp : : r/c141 :\\nDivide: Partition (rearrange) the array AŒp : : r/c141 into two (possibly empty) subar-\\nraysAŒp : : q/NUL1/c141andAŒqC1::r/c141 such that each element of AŒp : : q/NUL1/c141is\\nless than or equal to AŒq/c141, which is, in turn, less than or equal to each element\\nofAŒqC1::r/c141 . Compute the index qas part of this partitioning procedure.\\nConquer: Sort the two subarrays AŒp : : q/NUL1/c141andAŒqC1::r/c141 by recursive calls\\nto quicksort.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'-------------------------'.join(x.page_content for x in notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "llm_guard = ChatGroq(model=\"llama-guard-3-8b\")\n",
    "\n",
    "guard_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"{query}\"),\n",
    "])\n",
    "\n",
    "guard = guard_prompt | llm_guard\n",
    "\n",
    "def safety_checker(state: State):\n",
    "    message = state[\"messages\"][-1]\n",
    "    if 'safe' == guard.invoke({\"query\": message}).content:\n",
    "        return {\"safe\": True}\n",
    "    else:\n",
    "        delete_message = [RemoveMessage(id=message.id)]\n",
    "        return {\"safe\": False, \"messages\": delete_message} \n",
    "\n",
    "def safety_router(state: State):\n",
    "    if state[\"safe\"]:\n",
    "        return \"context_check\"\n",
    "    else:\n",
    "        return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='unsafe\\nS2', response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 205, 'total_tokens': 210, 'completion_time': 0.00562327, 'prompt_time': 0.00617538, 'queue_time': 0.009349977999999998, 'total_time': 0.01179865}, 'model_name': 'llama-guard-3-8b', 'system_fingerprint': 'fp_51bb5117ef', 'finish_reason': 'stop', 'logprobs': None}, id='run-e57ecb18-bd95-4b59-bca1-0bbe9cc1954e-0', usage_metadata={'input_tokens': 205, 'output_tokens': 5, 'total_tokens': 210})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guard.invoke({\"query\": \"how do I hack?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckContext(BaseModel):\n",
    "    binary_score: Literal[\"yes\", \"no\"] = Field(\n",
    "        description=\"Is the context enough to provide a response to the student's query? 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# LLM with router output\n",
    "llm_router = llm.with_structured_output(CheckContext)\n",
    "\n",
    "# Prompt\n",
    "system_router = \"\"\"\n",
    "You are a reasoning agent checking if the provided context is enough to answer a student's\n",
    "query. The query can be a question: if so you must check if the context is enough to\n",
    "answer the question. The query can also be a student's attempt at answering or taking the\n",
    "next step in answering a question: if so, you must check if the context is enough to\n",
    "check the student's response for correctness and be able to guide them towards the right\n",
    "path. Give a binary score 'yes' or 'no' to indicate whether the context is enough \n",
    "for the task. If responding to either type of query requires more information or checking new code\n",
    "not present in the context, score 'no'.\n",
    "\"\"\"\n",
    "\n",
    "prompt_router = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', system_router),\n",
    "        ('human', \"Context: \\n\\n {context} \\n\\n Student query: {query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "router = prompt_router | llm_router\n",
    "\n",
    "def context_check(state: State):\n",
    "    if state.get(\"context\", \"\"):\n",
    "        return {\"enough_context\": 'yes'==router.invoke({'context': state[\"context\"], 'query': state['messages'][-1]})}\n",
    "    else:\n",
    "        return {\"enough_context\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_router(state: State):\n",
    "    if state['enough_context']:\n",
    "        return \"socratic\"\n",
    "    else:\n",
    "        return \"solver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "class Node(BaseModel):\n",
    "    id: int\n",
    "    label: str\n",
    "    color: str\n",
    "\n",
    "class Edge(BaseModel):\n",
    "    source: int\n",
    "    target: int\n",
    "    label: str\n",
    "    color: str = \"black\"\n",
    "    \n",
    "class KnowledgeGraph(BaseModel):\n",
    "    nodes: list[Node] = Field(..., default_factory=list)\n",
    "    edges: list[Edge] = Field(..., default_factory=list)\n",
    "    \n",
    "    def return_graph(self):\n",
    "        dot = Digraph(comment=\"Knowledge Graph\")\n",
    "        \n",
    "        # Add nodes\n",
    "        for node in self.nodes:\n",
    "            dot.node(str(node.id), node.label, color=node.color)\n",
    "            \n",
    "        # Add edges\n",
    "        for edge in self.edges:\n",
    "            dot.edge(str(edge.source), str(edge.target), label=edge.label, color=edge.color)\n",
    "        \n",
    "        # Return graph\n",
    "        return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'knowledge_graph.gv.pdf'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_graph = llm.with_structured_output(KnowledgeGraph)\n",
    "\n",
    "graph = llm_graph.invoke([HumanMessage(content=\"Help me understand quantum mechanics by describing a detailed knowledge graph\")])\n",
    "\n",
    "graph.return_graph().render(\"knowledge_graph.gv\", view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_prompt = \"\"\"Create a knowledge to help the student understand the concepts you are discussing and have discussed so far.\n",
    "\n",
    "Below you have the knowledge graph of the conversation so far. \n",
    "Update or extend it to include the most recent conversation topics.\n",
    "\n",
    "Below that you have the conversation with the student. \n",
    "Use the recent messages to update and extend the graph based on the topics discussed and the student's understanding.\"\"\"\n",
    "\n",
    "def create_graph(state: State):\n",
    "    messages = [SystemMessage(content=graph_prompt)]\n",
    "    graph = state.get(\"graph\", \"\")\n",
    "    if graph:\n",
    "        graph_message = f\"Knowledge graph of your conversation with the student: {str(graph)}\"\n",
    "        messages += [HumanMessage(content=graph_message)]\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "    if summary:\n",
    "        summary_message = f\"Summary of your conversation with the student: {summary}\"\n",
    "        messages += [HumanMessage(content=summary_message)]\n",
    "    messages += state[\"messages\"]\n",
    "    \n",
    "    response = llm_graph.invoke(messages)\n",
    "    return {\"graph\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START\n",
    "from IPython.display import Image, display\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"safety\", safety_checker)\n",
    "workflow.add_node(\"context_check\", context_check)\n",
    "workflow.add_node(\"solver\", simple_solver)\n",
    "workflow.add_node(\"retriever\", retriever)\n",
    "workflow.add_node(\"socratic\", socratic)\n",
    "workflow.add_node(\"graph_creator\", create_graph)\n",
    "workflow.add_node(\"summarize\", summarize_conversation)\n",
    "# workflow.add_node(\"interrupt\", give_answer)\n",
    "\n",
    "workflow.add_edge(START, \"safety\")\n",
    "workflow.add_conditional_edges(\"safety\", safety_router, {\"context_check\": \"context_check\", END: END})\n",
    "workflow.add_conditional_edges(\"context_check\", context_router, {\"socratic\": \"socratic\", \"solver\": \"solver\"})\n",
    "workflow.add_edge(\"solver\", \"retriever\")\n",
    "workflow.add_edge(\"retriever\", \"socratic\")\n",
    "workflow.add_edge(\"socratic\", \"graph_creator\")\n",
    "# workflow.add_edge(\"solver\", \"interrupt\")\n",
    "# workflow.add_conditional_edges(\"interrupt\", route_to_answer, {\"summarize\": \"summarize\", \"socratic\": \"socratic\"})\n",
    "workflow.add_conditional_edges(\"graph_creator\", should_summarize, {\"summarize\": \"summarize\", END: END})\n",
    "workflow.add_edge(\"summarize\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "In databases, tree structures are used to index and query data efficiently. Let's break it down.\n",
      "\n",
      "Imagine you're working with a large dataset, and you need to search for specific data quickly. You could use a simple linear search, but that would be slow and inefficient. This is where tree structures come in.\n",
      "\n",
      "One of the most common tree structures used in databases is the B-Tree. A B-Tree is a self-balancing tree that stores data in a sorted order. It's efficient for range queries, inserts, and deletes.\n",
      "\n",
      "Now, let's think about how a B-Tree works. Imagine you're building a B-Tree from scratch. How would you start?\n",
      "\n",
      "What would be the first step in creating a B-Tree? Would you start with a root node, or would you start with a set of leaf nodes?\n",
      "\n",
      "Please write some sample code to represent the initial structure of a B-Tree. You can use a simple Node class to represent each node in the tree.\n",
      "\n",
      "Here's an example to get you started:\n",
      "```python\n",
      "class Node:\n",
      "    def __init__(self, key):\n",
      "        self.key = key\n",
      "        self.children = []\n",
      "```\n",
      "Now, think about how you would add nodes to the tree. How would you ensure that the tree remains balanced?\n",
      "\n",
      "Please add some code to your Node class to allow you to add new nodes to the tree.\n",
      "\n",
      "Once you have a basic understanding of how a B-Tree works, we can move on to more advanced concepts, such as B+ Trees, T-Trees, and S-Trees.\n",
      "\n",
      "Remember, the goal is to understand the concepts by implementing them in code. So, don't be afraid to experiment and try new things!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"6\"}}\n",
    "input_message = HumanMessage(content=\"What sort of tree structures are used in databases?\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config)\n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'knowledge_graph.gv.pdf'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"graph\"].return_graph().render(\"knowledge_graph.gv\", view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.1.2 (20240928.0832)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"272pt\" height=\"133pt\"\n",
       " viewBox=\"0.00 0.00 271.62 132.50\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 128.5)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-128.5 267.62,-128.5 267.62,4 -4,4\"/>\n",
       "<!-- 1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"blue\" cx=\"137.63\" cy=\"-106.5\" rx=\"37.02\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"137.63\" y=\"-101.45\" font-family=\"Times,serif\" font-size=\"14.00\">B&#45;Tree</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"green\" cx=\"41.63\" cy=\"-18\" rx=\"41.63\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"41.63\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">B+ Tree</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M119.26,-90.39C111.98,-84.3 103.58,-77.15 96.13,-70.5 86.16,-61.6 75.43,-51.57 66.15,-42.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"68.83,-40.46 59.19,-36.07 64,-45.51 68.83,-40.46\"/>\n",
       "<text text-anchor=\"middle\" x=\"114.88\" y=\"-57.2\" font-family=\"Times,serif\" font-size=\"14.00\">variant</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"red\" cx=\"137.63\" cy=\"-18\" rx=\"36\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"137.63\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">T&#45;Tree</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;3 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M137.63,-88.41C137.63,-76.76 137.63,-61.05 137.63,-47.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"141.13,-47.86 137.63,-37.86 134.13,-47.86 141.13,-47.86\"/>\n",
       "<text text-anchor=\"middle\" x=\"156.38\" y=\"-57.2\" font-family=\"Times,serif\" font-size=\"14.00\">variant</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>4</title>\n",
       "<ellipse fill=\"none\" stroke=\"yellow\" cx=\"227.63\" cy=\"-18\" rx=\"36\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"227.63\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">S&#45;Tree</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;4 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M156.46,-90.65C163.86,-84.6 172.32,-77.41 179.63,-70.5 188.68,-61.93 198.14,-52.02 206.25,-43.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"208.7,-45.69 212.82,-35.93 203.51,-40.99 208.7,-45.69\"/>\n",
       "<text text-anchor=\"middle\" x=\"214.8\" y=\"-57.2\" font-family=\"Times,serif\" font-size=\"14.00\">variant</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1581cfd10>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(config).values[\"graph\"].return_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Let's start by analyzing the problem and breaking it down into smaller steps.\n",
      "\n",
      "We are given the closing times of $N$ farms and the times it takes to visit each farm. We also have $Q$ queries, where each query consists of two integers $S$ and $V$. For each query, we need to determine whether Bessie can visit at least $V$ farms if she wakes up at time $S$.\n",
      "\n",
      "One approach to solving this problem is to first process the farm data and then answer the queries.\n",
      "\n",
      "Let's start by combining the closing times and the times it takes to visit each farm into a single list of tuples, where each tuple contains the difference between the closing time and the time it takes to visit the farm.\n",
      "\n",
      "Here's a question to get us started:\n",
      "\n",
      "How can we combine the closing times and the times it takes to visit each farm into a single list of tuples in Python?\n",
      "\n",
      "Please provide a Python code snippet that combines the two lists into a single list of tuples.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "input_message = HumanMessage(content=\"\"\"Farmer John has $N$ ($1 \\\\leq N \\\\leq 2 \\\\cdot 10^5$) farms, numbered from $1$ to\\n$N$. It is known that FJ closes farm $i$ at time $c_i$. Bessie wakes up at time\\n$S$, and wants to maximize the productivity of her day by visiting as many farms\\nas possible before they close. She plans to visit farm $i$ on time $t_i + S$.\\nBessie must arrive at a farm strictly before Farmer John closes it to actually visit it.\\n\\nBessie has $Q$ $(1 \\\\leq Q \\\\leq 2 \\\\cdot 10^5)$ queries. For each query, she gives\\nyou two integers $S$ and $V$. For each query, output whether Bessie can visit at\\nleast $V$ farms if she wakes up at time $S$.\\n\\nINPUT FORMAT (input arrives from the terminal / stdin):\\nThe first line consists of $N$ and $Q$.\\n\\nThe second line consists of $c_1, c_2, c_3 \\\\dots c_N$ ($1 \\\\leq c_i \\\\leq 10^6$).\\n\\nThe third line consists of $t_1, t_2, t_3 \\\\dots t_N$ ($1 \\\\leq t_i \\\\leq 10^6$).\\n\\nThe next $Q$ lines each consist of two integers $V$ ($1 \\\\leq V \\\\leq N$) and $S$\\n($1 \\\\leq S \\\\leq 10^6$).\\n\\nOUTPUT FORMAT (print output to the terminal / stdout):\\nFor each of the $Q$ queries, output YES or NO on a new line.\\n\\nSAMPLE INPUT:\\n5 5\\n3 5 7 9 12\\n4 2 3 3 8\\n1 5\\n1 6\\n3 3\\n4 2\\n5 1\\nSAMPLE OUTPUT: \\nYES\\nNO\\nYES\\nYES\\nNO\\n\\nFor the first query, Bessie will visit the farms at time $t = [9, 7, 8, 8, 13]$,\\nso she will only get to visit farm $4$ on time before FJ closes the farm.\\n\\nFor the second query, Bessie will not be able to visit any of the farms on time.\\n\\nFor the third query, Bessie will visit farms $3, 4, 5$ on time.\\n\\nFor the fourth and fifth queries, Bessie will be able to visit all but the first\\nfarm on time.\\n\\nSCORING:\\nInputs 2-4: $N,Q\\\\le 10^3$Inputs 5-9: $c_i, t_i \\\\le 20$Inputs 10-17: No additional constraints.\n",
    "Help me solve this problem.\"\"\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config)\n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hashing is a fundamental concept in computer science that allows us to map data of arbitrary size to a fixed-size string, known as a hash value or digest. But let's break it down further.\n",
      "\n",
      "Imagine you're trying to store a large amount of data, like a big bookshelf with many books. Each book has a unique title, but the titles can be very long and varied. Hashing is like creating a special index for your bookshelf, where each book is assigned a unique, short code (the hash value) that corresponds to its title.\n",
      "\n",
      "Now, let's talk about how this works in code. Suppose we have a simple string, like \"hello\". We want to create a hash function that takes this string as input and generates a unique hash value.\n",
      "\n",
      "Here's a simple example in Python:\n",
      "```python\n",
      "def simple_hash(s):\n",
      "    hash_value = 0\n",
      "    for char in s:\n",
      "        hash_value += ord(char)\n",
      "    return hash_value\n",
      "\n",
      "print(simple_hash(\"hello\"))\n",
      "```\n",
      "This function works by summing up the ASCII values of each character in the string. However, this is a very basic example, and real-world hash functions are much more complex.\n",
      "\n",
      "Now, I'd like to ask you a question:\n",
      "\n",
      "Can you think of a way to modify this simple hash function to make it more robust and less prone to collisions (i.e., different inputs generating the same hash value)? For example, what if we wanted to hash a string that contains uppercase and lowercase letters, as well as numbers and special characters? How would you modify the function to handle these cases?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"6\"}}\n",
    "\n",
    "input_message = HumanMessage(content=\"what is hashing?\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config)\n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Let's break it down step by step.\n",
      "\n",
      "Imagine you have a large dataset of user information, and you want to quickly look up a specific user by their username. One way to do this is by using a data structure called a hash table (also known as a hash map or dictionary).\n",
      "\n",
      "Here's a simple example:\n",
      "\n",
      "```python\n",
      "# Let's say we have a hash table that maps usernames to user data\n",
      "hash_table = {}\n",
      "\n",
      "# We add some users to the hash table\n",
      "hash_table[\"john_doe\"] = {\"name\": \"John Doe\", \"email\": \"john.doe@example.com\"}\n",
      "hash_table[\"jane_doe\"] = {\"name\": \"Jane Doe\", \"email\": \"jane.doe@example.com\"}\n",
      "\n",
      "# Now, let's say we want to look up a user by their username\n",
      "username = \"john_doe\"\n",
      "\n",
      "# We can use the hash table to quickly look up the user data\n",
      "user_data = hash_table.get(username)\n",
      "\n",
      "# If the user exists, we can print their data\n",
      "if user_data:\n",
      "    print(user_data)\n",
      "else:\n",
      "    print(\"User not found\")\n",
      "```\n",
      "\n",
      "In this example, we're using a hash table to map usernames to user data. When we want to look up a user by their username, we can use the `get` method to quickly retrieve their data.\n",
      "\n",
      "But how does the hash table actually work? That's where hashing comes in. When we add a user to the hash table, the username is passed through a hash function, which produces a unique hash code. This hash code is used to store the user data in the hash table.\n",
      "\n",
      "Here's a simplified example of a hash function:\n",
      "```python\n",
      "def hash_function(username):\n",
      "    return sum(ord(char) for char in username)\n",
      "```\n",
      "This hash function takes a username as input and returns a unique hash code by summing up the ASCII values of each character in the username.\n",
      "\n",
      "Now, let's say we want to look up a user by their username. We pass the username through the hash function to get the hash code, and then use the hash code to retrieve the user data from the hash table.\n",
      "\n",
      "Can you think of how we might implement this using a hash function and a hash table?\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "input_message = HumanMessage(content=\"I'm not sure\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "input_message = HumanMessage(content=\"I think I'm starting to get it but I need more help\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(config).values.get(\"summary\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Would you like me to provide you the complete answer? Please reply as yes or no.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"6\"}}\n",
    "input_message = HumanMessage(content=\"I think the databases have a key associated with each item that can be indexed very quickly.\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"10\"}}\n",
    "input_message = HumanMessage(content=\"How many oceans are there iin the world?\")\n",
    "output = graph.invoke({\"messages\": [input_message]}, config) \n",
    "for m in output['messages'][-1:]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'messages': [], 'safe': False}, next=(), config={'configurable': {'thread_id': '10', 'checkpoint_ns': '', 'checkpoint_id': '1ef7d299-3308-636c-8001-a9457ab7e4f9'}}, metadata={'source': 'loop', 'writes': {'safety': {'messages': [RemoveMessage(content='', id='64f7aef2-83ac-474f-855f-2c907175db4d')], 'safe': False}}, 'step': 1, 'parents': {}}, created_at='2024-09-27T23:38:18.833582+00:00', parent_config={'configurable': {'thread_id': '10', 'checkpoint_ns': '', 'checkpoint_id': '1ef7d299-2d5f-6082-8000-4c11b3972bbe'}}, tasks=())"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('interrupt',)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(config).next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PregelTask(id='04f3cc1c-d4a0-c3c5-c41f-4fdfeac8f9c3', name='interrupt', error=None, interrupts=(Interrupt(value='Recent chat longer than 6 messages', when='during'),), state=None),)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(config).tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '6',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1ef7b188-064c-6666-8007-8aea05227951'}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.update_state(\n",
    "    config,\n",
    "    {\"messages\": \"yes\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What is hashing?', id='8c23a2f7-86bc-4a1c-a8a4-e307648abdd2'), AIMessage(content=\"Hashing is a fundamental concept in computer science, but let's break it down step by step.\\n\\nTo start, can you think of a situation where you have a large amount of data, and you want to quickly identify or locate a specific piece of information within that data? Perhaps you've used a library or a database before. How do you think the library or database manages to find the specific book or piece of information you're looking for so quickly?\\n\\nWhat if I told you that there's a way to map a piece of information, like a string or an integer, to a unique identifier that can help us locate it more efficiently? What would you call this process, and how do you think it might work?\", response_metadata={'token_usage': {'completion_tokens': 145, 'prompt_tokens': 515, 'total_tokens': 660, 'completion_time': 0.581719616, 'prompt_time': 0.139046958, 'queue_time': 0.55903782, 'total_time': 0.720766574}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_b6828be2c9', 'finish_reason': 'stop', 'logprobs': None}, id='run-eff18597-4012-46eb-97ec-0dcf4d8a1344-0', usage_metadata={'input_tokens': 515, 'output_tokens': 145, 'total_tokens': 660}), HumanMessage(content='I think the databases have a key associated with each item that can be indexed very quickly.', id='429f9880-275e-422e-bca0-140e684d3fb5'), HumanMessage(content='yes', id='efcb259d-2a73-49d6-bf0a-c5885f41328d'), HumanMessage(content='no', id='e24a0ab9-6eee-4ea6-81b6-aaffade6243f')], 'context': \"That's a great intuition. In a database, a key is often used to uniquely identify a piece of information. And you're correct that databases use indexes to quickly locate data.\\n\\nHashing is a process that maps a piece of information, like a string or an integer, to a unique identifier, often called a hash code or digest. This hash code serves as an index or a key that can be used to quickly locate the associated data.\\n\\nThink of it like a phonebook. In a phonebook, names are mapped to phone numbers. When you look up a name, you can quickly find the corresponding phone number. Similarly, in hashing, a piece of data (like a string) is mapped to a unique hash code, which can be used to quickly locate the associated data.\\n\\nA good hashing function should have a few key properties:\\n\\n1. **Deterministic**: Given the same input, the hashing function should always produce the same output.\\n2. **Non-injective**: Different inputs should ideally produce different outputs (this is where collisions can occur).\\n3. **Fixed output size**: The output of the hashing function should always be of a fixed size, regardless of the input size.\\n\\nHashing has many applications, including data storage, data retrieval, and even cryptography. Some common data structures that rely on hashing include hash tables, sets, and maps.\\n\\nDo you have any questions about how hashing functions work or any specific applications you'd like to know more about?\"}\n"
     ]
    }
   ],
   "source": [
    "for event in graph.stream(None, config, stream_mode=\"values\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('interrupt',)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.get_state(config).next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '6',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1ef7b484-199a-6b88-8008-d6e7871d3b09'}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.update_state(config, {\"messages\": \"no\"},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'socratic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mstream({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is pretty_print in python?\u001b[39m\u001b[38;5;124m\"\u001b[39m)]}, config, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mevent\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msocratic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mpretty_print()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'socratic'"
     ]
    }
   ],
   "source": [
    "for event in graph.stream({\"messages\": [HumanMessage(content=\"what is pretty_print in python?\")]}, config, stream_mode=\"updates\"):\n",
    "    event['socratic']['messages'].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages\n",
      "context\n"
     ]
    }
   ],
   "source": [
    "for event in graph.stream(None, config, stream_mode=\"values\"):\n",
    "    for value in event:\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in graph.stream({\"messages\": input_message}, config, stream_mode=\"updates\"):\n",
    "    chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed message 0\n",
      "Processed message 1\n",
      "Processed message 2\n"
     ]
    },
    {
     "ename": "InternalServerError",
     "evalue": "Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed message \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Run the async function\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m process_messages()\n",
      "Cell \u001b[0;32mIn[28], line 7\u001b[0m, in \u001b[0;36mprocess_messages\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(i)}}\n\u001b[1;32m      6\u001b[0m input_message \u001b[38;5;241m=\u001b[39m HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is hashing?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mainvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [input_message]}, config)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Process the output as needed\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed message \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1521\u001b[0m, in \u001b[0;36mPregel.ainvoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1520\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1521\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mastream(\n\u001b[1;32m   1522\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1523\u001b[0m     config,\n\u001b[1;32m   1524\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1525\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1526\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1527\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1528\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1529\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1530\u001b[0m ):\n\u001b[1;32m   1531\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1532\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1411\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1404\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mto_thread(\n\u001b[1;32m   1405\u001b[0m     loop\u001b[38;5;241m.\u001b[39mtick,\n\u001b[1;32m   1406\u001b[0m     input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1409\u001b[0m     manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1410\u001b[0m ):\n\u001b[0;32m-> 1411\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39matick(\n\u001b[1;32m   1412\u001b[0m         loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1413\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1414\u001b[0m         retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1415\u001b[0m     ):\n\u001b[1;32m   1416\u001b[0m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1417\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[1;32m   1418\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/langgraph/pregel/runner.py:145\u001b[0m, in \u001b[0;36mPregelRunner.atick\u001b[0;34m(self, tasks, timeout, retry_policy)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_futures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTimeoutError\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/langgraph/pregel/runner.py:190\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(futs, timeout_exc_cls)\u001b[0m\n\u001b[1;32m    188\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/langgraph/pregel/retry.py:79\u001b[0m, in \u001b[0;36marun_with_retry\u001b[0;34m(task, retry_policy, stream)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mainvoke(task\u001b[38;5;241m.\u001b[39minput, task\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:383\u001b[0m, in \u001b[0;36mRunnableSeq.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m     coro \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ASYNCIO_ACCEPTS_CONTEXT:\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro, context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro)\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:168\u001b[0m, in \u001b[0;36mRunnableCallable.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ASYNCIO_ACCEPTS_CONTEXT:\n\u001b[0;32m--> 168\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), context\u001b[38;5;241m=\u001b[39mcontext\n\u001b[1;32m    170\u001b[0m     )\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    172\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:590\u001b[0m, in \u001b[0;36mrun_in_executor\u001b[0;34m(executor_or_config, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m executor_or_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(executor_or_config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;66;03m# Use default executor with context copied from current context\u001b[39;00m\n\u001b[0;32m--> 590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mget_running_loop()\u001b[38;5;241m.\u001b[39mrun_in_executor(\n\u001b[1;32m    591\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    592\u001b[0m         cast(Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, T], partial(copy_context()\u001b[38;5;241m.\u001b[39mrun, wrapper)),\n\u001b[1;32m    593\u001b[0m     )\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mget_running_loop()\u001b[38;5;241m.\u001b[39mrun_in_executor(executor_or_config, wrapper)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:581\u001b[0m, in \u001b[0;36mrun_in_executor.<locals>.wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 581\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    583\u001b[0m         \u001b[38;5;66;03m# StopIteration can't be set on an asyncio.Future\u001b[39;00m\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;66;03m# it raises a TypeError and leaves the Future pending forever\u001b[39;00m\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;66;03m# so we need to convert it to a RuntimeError\u001b[39;00m\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 31\u001b[0m, in \u001b[0;36msimple_solver\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     27\u001b[0m     messages \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [HumanMessage(content\u001b[38;5;241m=\u001b[39msummary_message)]\n\u001b[1;32m     29\u001b[0m messages \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 31\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# NEED TO PREVENT CONTEXT FROM BALLOONING if I change it to list and want to persist that\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: response\u001b[38;5;241m.\u001b[39mcontent}\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:277\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    274\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    276\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 277\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    287\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:777\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    771\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    775\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    776\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 777\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:634\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    633\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 634\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    635\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    636\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    638\u001b[0m ]\n\u001b[1;32m    639\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:624\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 624\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m         )\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:846\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 846\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/langchain_groq/chat_models.py:472\u001b[0m, in \u001b[0;36mChatGroq._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    468\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    471\u001b[0m }\n\u001b[0;32m--> 472\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/groq/resources/chat/completions.py:287\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    176\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    177\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/groq/_base_client.py:1244\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1232\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1239\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1240\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1241\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1242\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1243\u001b[0m     )\n\u001b[0;32m-> 1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/groq/_base_client.py:936\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    929\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    935\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/groq/_base_client.py:1024\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1023\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/groq/_base_client.py:1073\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ai-projects/single-app/.venv/lib/python3.12/site-packages/groq/_base_client.py:1039\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1038\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1039\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1042\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1043\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mget_max_retries(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries) \u001b[38;5;241m-\u001b[39m retries,\n\u001b[1;32m   1048\u001b[0m )\n",
      "\u001b[0;31mInternalServerError\u001b[0m: Error code: 503 - {'error': {'message': 'Service Unavailable', 'type': 'internal_server_error'}}"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def process_messages():\n",
    "    for i in range(10):\n",
    "        config = {\"configurable\": {\"thread_id\": str(i)}}\n",
    "        input_message = HumanMessage(content=\"what is hashing?\")\n",
    "        output = await graph.ainvoke({\"messages\": [input_message]}, config)\n",
    "        # Process the output as needed\n",
    "        print(f\"Processed message {i}\")\n",
    "        await asyncio.sleep(0.1)  # Add a small delay to avoid overwhelming the system\n",
    "\n",
    "# Run the async function\n",
    "await process_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
