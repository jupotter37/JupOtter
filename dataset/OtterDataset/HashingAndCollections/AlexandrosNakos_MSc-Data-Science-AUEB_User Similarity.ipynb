{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd11f5d3",
   "metadata": {},
   "source": [
    "# User Similarity Using Jaccard, MinHash & LSH\n",
    "\n",
    "> *Data Mining*  \n",
    "> *MSc in Data Science, Department of Informatics*  \n",
    "> *Athens University of Economics and Business*\n",
    "\n",
    "---\n",
    "\n",
    "1) **Compute exact Jaccard similarity of users**\n",
    "\n",
    "Download the movieLens dataset.To assess the similarity between users you should compute the exact Jaccard Similarity for all pairs of users and only output the pairs of users (unique) that have similarity at least 0,5 (>=50%). For each pair denote their ids and the similarity score.\n",
    "\n",
    "3) **Compute similarity using Min-hash signatures**\n",
    "\n",
    "In this step you compute min-hash signatures for each user and use them to evaluate their similarity.\n",
    "Description of hash functions: use the following family of hash functions: ha,b(x)=(ax+b) mod R, with a,b random integers in the interval (0,R) and R a large\n",
    "enough prime number that you may want to finetune in your initial experimentation. Make sure that each hash function uses different values of a,b pairs.\n",
    "Evaluation of Min-hashing: Use 50, 100, and 200 hash functions. For each value, output the pair of users that have estimated similarity at least 0.5, and report the number of false positives and false negatives (against the exact Jaccard similarity) that you obtain. For the false positives and negatives, report the averages for 5 different runs using different functions. \n",
    "\n",
    "4) **Locate similar users using LSH index**\n",
    "\n",
    "Using a set of 200 hash functions break up the signatures into b bands with r hash functions per band (bxr=200) and implement *Locality Sensitive Hashing*.\n",
    "Recall that with LSH we first locate users that are similar (have the same mini-signatures) across at least one band and then assess their true similarity using their initial representations. Use the following two instances of LSH:\n",
    "\n",
    "- LSH instance 1: b = 25, r = 8\n",
    "- LSH instance 2: b = 40, r = 5\n",
    "\n",
    "Using each instance find the pair of users with similarity at least 0.5 and report:\n",
    "\n",
    "- The number of true pairs returned (true positives).\n",
    "- The number of similarity evaluations performed using the initial representations.\n",
    "\n",
    "Report the averages for 5 different runs using different functions.\n",
    "Based on the reported results, what do we gain/loose by using LSH instead of directly comparing users on their true representations?\n",
    "\n",
    "### *Libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3339d971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fe8594",
   "metadata": {},
   "source": [
    "### *Data*\n",
    "\n",
    "- We will use the movieLens dataset.\n",
    "- The dataset pertains to movie ratings provided by users.\n",
    "- It comprises 100,000 ratings (1-5) from 943 users on 1682 movies.\n",
    "- Each user has rated a minimum of 20 movies.\n",
    "- The dataset is distributed across three files: users.txt, movies.txt, and ratings.txt.\n",
    "  - users.txt: Contains id, age, gender, occupation, and postcode separated by |.\n",
    "  - movies.txt: Includes id, title (with release year), and additional unrelated information separated by |.\n",
    "  - ratings.txt: Tab-separated file containing userid, movieid, rating (1-5), and timestamp.\n",
    "- For this assignment, only the set of movies that a user has rated will be used, excluding the ratings themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4b6e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings\n",
    "ratings_df=pd.read_csv(os.getcwd() + '/Movie Lens Dataset/ratings.txt',sep='\\t',header = None,names = [\n",
    "    'userId','movieId','rating','timestamp'])\n",
    "\n",
    "# movies\n",
    "movies_df=pd.read_csv(os.getcwd() + '/Movie Lens Dataset/movies.txt',sep='|', header = None, usecols=[0, 1], encoding='ANSI',\n",
    "                     names = ['movieId','Title']).set_index('movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "746f7e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the set of distinct user IDs from the 'userId' column in the ratings DataFrame\n",
    "distinct_users = set(ratings_df['userId'])\n",
    "\n",
    "# Dictionary to store user IDs as keys and the set of movie IDs they have rated as values\n",
    "user_films = {}\n",
    "\n",
    "# Iterate through each distinct user\n",
    "for user in distinct_users:\n",
    "    # Extract movie IDs that the current user has rated and convert to a list\n",
    "    my_films = ratings_df[ratings_df['userId'] == user]['movieId'].values.tolist()\n",
    "    \n",
    "    # Store the set of movie IDs in the user_films dictionary\n",
    "    user_films[user] = set(my_films)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5dfa40",
   "metadata": {},
   "source": [
    "## *Compute Exact Jaccard Similarity*\n",
    "\n",
    "##### *Compute user similarity using Jaccard coefficient*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65e50db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Pair: (197, 600) Jaccard Similarity: 0.5\n",
      "User Pair: (197, 826) Jaccard Similarity: 0.512987012987013\n",
      "User Pair: (328, 788) Jaccard Similarity: 0.6729559748427673\n",
      "User Pair: (408, 898) Jaccard Similarity: 0.8387096774193549\n",
      "User Pair: (451, 489) Jaccard Similarity: 0.5333333333333333\n",
      "User Pair: (489, 587) Jaccard Similarity: 0.6299212598425197\n",
      "User Pair: (554, 764) Jaccard Similarity: 0.5170068027210885\n",
      "User Pair: (600, 826) Jaccard Similarity: 0.5454545454545454\n",
      "User Pair: (674, 879) Jaccard Similarity: 0.5217391304347826\n",
      "User Pair: (800, 879) Jaccard Similarity: 0.5\n"
     ]
    }
   ],
   "source": [
    "def user_jaccard_similarity(user_films, threshold_similarity=0.5):\n",
    "    \"\"\"\n",
    "    Calculate Jaccard similarity between pairs of users based on their rated movies.\n",
    "\n",
    "    Parameters:\n",
    "    - user_films: Dictionary with user IDs as keys and sets of movie IDs they have rated as values.\n",
    "    - threshold_similarity: Minimum Jaccard similarity threshold to print pairs.\n",
    "\n",
    "    Returns:\n",
    "    - usim: Dictionary containing Jaccard similarities between user pairs.\n",
    "    - max_pair: Tuple containing the user pair with the maximum Jaccard similarity.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate pairs of user combinations\n",
    "    pairs = list(combinations(list(user_films.keys()), 2))\n",
    "\n",
    "    # Dictionary to store Jaccard similarities between user pairs\n",
    "    usim = defaultdict(dict)\n",
    "\n",
    "    # Variable to track the maximum Jaccard similarity\n",
    "    max_jacc = 0\n",
    "    max_pair = None\n",
    "\n",
    "    # Iterate through pairs of users\n",
    "    for u1, u2 in pairs:\n",
    "        # Calculate Jaccard similarity\n",
    "        union = user_films[u1].union(user_films[u2])\n",
    "        intersection = user_films[u1].intersection(user_films[u2])\n",
    "        jaccard_similarity = len(intersection) / len(union)\n",
    "        \n",
    "        # Store Jaccard similarity in the usim dictionary\n",
    "        usim[u1][u2] = jaccard_similarity\n",
    "\n",
    "        # Print user pairs with Jaccard similarity above the threshold\n",
    "        if jaccard_similarity >= threshold_similarity:\n",
    "            print(f'User Pair: ({u1}, {u2}) Jaccard Similarity: {jaccard_similarity}')\n",
    "\n",
    "        # Update maximum Jaccard similarity and corresponding pair\n",
    "        if jaccard_similarity > max_jacc:\n",
    "            max_jacc = jaccard_similarity\n",
    "            max_pair = (u1, u2)\n",
    "\n",
    "    return usim, max_pair, max_jacc\n",
    "\n",
    "usim_result, max_jacc_pair, max_jacc = user_jaccard_similarity(user_films)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28c4e6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Similar User Pair: (408, 898) Jaccard Similarity: 0.8387096774193549\n",
      "\n",
      "Common Films:\n",
      "\n",
      "Contact (1997)\n",
      "Gattaca (1997)\n",
      "Starship Troopers (1997)\n",
      "Indian Summer (1996)\n",
      "Good Will Hunting (1997)\n",
      "Mouse Hunt (1997)\n",
      "English Patient, The (1996)\n",
      "Scream (1996)\n",
      "Rocket Man (1997)\n",
      "Air Force One (1997)\n",
      "L.A. Confidential (1997)\n",
      "Jackal, The (1997)\n",
      "Rainmaker, The (1997)\n",
      "Midnight in the Garden of Good and Evil (1997)\n",
      "Titanic (1997)\n",
      "Apt Pupil (1998)\n",
      "Everyone Says I Love You (1996)\n",
      "Lost Highway (1997)\n",
      "Cop Land (1997)\n",
      "Conspiracy Theory (1997)\n",
      "U Turn (1997)\n",
      "Wag the Dog (1997)\n",
      "Spawn (1997)\n",
      "Saint, The (1997)\n",
      "Tomorrow Never Dies (1997)\n",
      "Kolya (1996)\n",
      "\n",
      "\n",
      "Unique Films:\n",
      "\n",
      "Contact (1997)\n",
      "Gattaca (1997)\n",
      "Starship Troopers (1997)\n",
      "Indian Summer (1996)\n",
      "Good Will Hunting (1997)\n",
      "Mouse Hunt (1997)\n",
      "English Patient, The (1996)\n",
      "Scream (1996)\n",
      "Liar Liar (1997)\n",
      "Rocket Man (1997)\n",
      "Air Force One (1997)\n",
      "L.A. Confidential (1997)\n",
      "Jackal, The (1997)\n",
      "Deceiver (1997)\n",
      "Rainmaker, The (1997)\n",
      "Midnight in the Garden of Good and Evil (1997)\n",
      "Titanic (1997)\n",
      "Apt Pupil (1998)\n",
      "As Good As It Gets (1997)\n",
      "Everyone Says I Love You (1996)\n",
      "Lost Highway (1997)\n",
      "Cop Land (1997)\n",
      "Conspiracy Theory (1997)\n",
      "U Turn (1997)\n",
      "Alien: Resurrection (1997)\n",
      "Wag the Dog (1997)\n",
      "Spawn (1997)\n",
      "Saint, The (1997)\n",
      "Tomorrow Never Dies (1997)\n",
      "Kolya (1996)\n",
      "Jungle2Jungle (1997)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the set of all films and common films for the most similar user pair\n",
    "all_films = user_films[max_jacc_pair[0]].union(user_films[max_jacc_pair[1]])\n",
    "common_films = user_films[max_jacc_pair[0]].intersection(user_films[max_jacc_pair[1]])\n",
    "\n",
    "# Print the most similar user pair and Jaccard similarity\n",
    "print(f'Most Similar User Pair: ({max_jacc_pair[0]}, {max_jacc_pair[1]}) Jaccard Similarity: {max_jacc}\\n')\n",
    "\n",
    "# Print common films\n",
    "print('Common Films:\\n')\n",
    "for film in common_films:\n",
    "    print(movies_df.loc[film]['Title'])\n",
    "\n",
    "# Print unique films\n",
    "print('\\n\\nUnique Films:\\n')\n",
    "for film in all_films:\n",
    "    print(movies_df.loc[film]['Title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e76c0f",
   "metadata": {},
   "source": [
    "## *Compute Similarity Using MinHash Signatures* \n",
    "\n",
    "##### *Compute user similarity using MinHash signatures*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b052fc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a universal hash function\n",
    "def universal_hash(p, a, b):\n",
    "    # Return a lambda function representing the hash function\n",
    "    return lambda x: (a * x + b) % p\n",
    "\n",
    "# Function to get a random hash function with given prime 'p'\n",
    "def get_random_hash_fn(p):\n",
    "    # Generate random coefficients 'a' and 'b' within the range [1, p-1] and [0, p-1] respectively\n",
    "    a = random.randint(1, p - 1)\n",
    "    b = random.randint(0, p - 1)\n",
    "    # Return a universal hash function with the generated coefficients\n",
    "    return universal_hash(p, a, b)\n",
    "\n",
    "def generate_min_hash_signatures(user_films, num_hashes, R):\n",
    "    hash_functions = [get_random_hash_fn(R) for _ in range(num_hashes)]\n",
    "\n",
    "    min_hash_signatures = {}\n",
    "    for user, films in user_films.items():\n",
    "        signature = [float('inf')] * num_hashes\n",
    "        for film in films:\n",
    "            for i, hash_fn in enumerate(hash_functions):\n",
    "                h = hash_fn(film)\n",
    "                if h < signature[i]:\n",
    "                    signature[i] = h\n",
    "        min_hash_signatures[user] = signature\n",
    "\n",
    "    return min_hash_signatures\n",
    "\n",
    "def compute_minhash_similarity(u1, u2, min_hash_signatures):\n",
    "    signature1 = min_hash_signatures[u1]\n",
    "    signature2 = min_hash_signatures[u2]\n",
    "    common_hashes = sum(s1 == s2 for s1, s2 in zip(signature1, signature2))\n",
    "    union_hashes = len(set(signature1 + signature2))\n",
    "    similarity = common_hashes / union_hashes\n",
    "    return similarity\n",
    "\n",
    "def evaluate_min_hashing(num_hashes, user_films, usim, print_flag = True, similarity_threshold=0.5):\n",
    "    pairs = list(combinations(list(user_films.keys()), 2))\n",
    "\n",
    "    # Generate MinHash signatures for all users\n",
    "    min_hash_signatures = generate_min_hash_signatures(user_films, num_hashes, 2**32)\n",
    "\n",
    "    False_Positives = 0\n",
    "    False_Negatives = 0\n",
    "    Similar_Pairs = []\n",
    "\n",
    "    if (print_flag):\n",
    "        print(f'Similar Pairs ({num_hashes} Hash Functions)')\n",
    "        print('='*100) \n",
    "    \n",
    "    for u1, u2 in pairs:\n",
    "        # Compute MinHash similarity between two users\n",
    "        minhash_similarity = compute_minhash_similarity(u1, u2, min_hash_signatures)\n",
    "\n",
    "        # Evaluate false positives and false negatives\n",
    "        if minhash_similarity > round(usim[u1][u2], 2):\n",
    "            False_Positives += 1\n",
    "        elif minhash_similarity < round(usim[u1][u2], 2):\n",
    "            False_Negatives += 1   \n",
    "            \n",
    "        if minhash_similarity >= similarity_threshold:\n",
    "            \n",
    "            Similar_Pairs.append((u1, u2, minhash_similarity))\n",
    "\n",
    "            if (print_flag):\n",
    "                print(f'User Pair: ({u1}, {u2}) MinHash Similarity: {minhash_similarity}'\n",
    "                      f' Jaccard Similarity: {round(usim[u1][u2], 2)}')\n",
    "\n",
    "    if (print_flag):\n",
    "        print('\\n')\n",
    "        print(f'FP, FN ({num_hashes} Hash Functions)')\n",
    "        print('='*100)\n",
    "        print(f'\\nFalse Positives ({num_hashes} Hash Functions) : {False_Positives}') \n",
    "        print(f'False Negatives   ({num_hashes} Hash Functions) : {False_Negatives}') \n",
    "        \n",
    "    return False_Positives, False_Negatives, Similar_Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be000f4",
   "metadata": {},
   "source": [
    "##### *Report the average number of False Positives (FP) and False Negatives (FN) for 5 different runs using different functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5c2b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mulpiple_min_hash_evaluations(user_films, usim_result, hash_num, eval_runs = 5):\n",
    "    \n",
    "    # initialize empty lists\n",
    "    # to store FP and FN values\n",
    "    false_positives = []\n",
    "    false_negatives = []\n",
    "    \n",
    "    for i in range(eval_runs):\n",
    "        \n",
    "        # compute FP FN and similar pairs\n",
    "        FP, FN, Similar_Pairs = evaluate_min_hashing(hash_num, user_films, usim_result, False)\n",
    "        \n",
    "        # append the values from each iteration\n",
    "        false_positives.append(FP)\n",
    "        false_negatives.append(FN)\n",
    "        \n",
    "    # calculate the averages\n",
    "    FP_avg = round(np.mean(false_positives))\n",
    "    FN_avg = round(np.mean(false_negatives))\n",
    "    \n",
    "    # print\n",
    "    print(f'Average False Positives ({hash_num} Hash Functions) : {FP_avg}')\n",
    "    print(f'Average False Negatives ({hash_num} Hash Functions) : {FN_avg}')\n",
    "    \n",
    "    return FP_avg, FN_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab289b44",
   "metadata": {},
   "source": [
    "### *Using 50 Hash Functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "440a10e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Pairs (50 Hash Functions)\n",
      "====================================================================================================\n",
      "User Pair: (408, 898) MinHash Similarity: 0.7241379310344828 Jaccard Similarity: 0.84\n",
      "User Pair: (489, 587) MinHash Similarity: 0.5384615384615384 Jaccard Similarity: 0.63\n",
      "\n",
      "\n",
      "FP, FN (50 Hash Functions)\n",
      "====================================================================================================\n",
      "\n",
      "False Positives (50 Hash Functions) : 60212\n",
      "False Negatives   (50 Hash Functions) : 366821\n"
     ]
    }
   ],
   "source": [
    "False_Positives_50, False_Negatives_50, Similar_Pairs_50 = evaluate_min_hashing(50, user_films, usim_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a329ae06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average False Positives (50 Hash Functions) : 43591\n",
      "Average False Negatives (50 Hash Functions) : 383539\n",
      "\n",
      "Elapsed time: 45 secs.\n"
     ]
    }
   ],
   "source": [
    "# start time\n",
    "st = time.time()\n",
    "\n",
    "FP_avg_50, FN_avg_50 = mulpiple_min_hash_evaluations(user_films, usim_result, 50)\n",
    "\n",
    "# end time\n",
    "et = time.time()\n",
    "\n",
    "print(f'\\nElapsed time: {round(et-st)} secs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d697f6",
   "metadata": {},
   "source": [
    "### *Using 100 Hash Functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a148b6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Pairs (100 Hash Functions)\n",
      "====================================================================================================\n",
      "User Pair: (408, 898) MinHash Similarity: 0.7857142857142857 Jaccard Similarity: 0.84\n",
      "\n",
      "\n",
      "FP, FN (100 Hash Functions)\n",
      "====================================================================================================\n",
      "\n",
      "False Positives (100 Hash Functions) : 12967\n",
      "False Negatives   (100 Hash Functions) : 414316\n"
     ]
    }
   ],
   "source": [
    "False_Positives_100, False_Negatives_100, Similar_Pairs_100 = evaluate_min_hashing(100, user_films, usim_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62bdf986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average False Positives (100 Hash Functions) : 21937\n",
      "Average False Negatives (100 Hash Functions) : 405578\n",
      "\n",
      "Elapsed time: 70 secs.\n"
     ]
    }
   ],
   "source": [
    "# start time\n",
    "st = time.time()\n",
    "\n",
    "FP_avg_100, FN_avg_100 = mulpiple_min_hash_evaluations(user_films, usim_result, 100)\n",
    "\n",
    "# end time\n",
    "et = time.time()\n",
    "\n",
    "print(f'\\nElapsed time: {round(et-st)} secs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8c70c8",
   "metadata": {},
   "source": [
    "### *Using 200 Hash Functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11890b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Pairs (200 Hash Functions)\n",
      "====================================================================================================\n",
      "User Pair: (408, 898) MinHash Similarity: 0.6460905349794238 Jaccard Similarity: 0.84\n",
      "\n",
      "\n",
      "FP, FN (200 Hash Functions)\n",
      "====================================================================================================\n",
      "\n",
      "False Positives (200 Hash Functions) : 5883\n",
      "False Negatives   (200 Hash Functions) : 422165\n"
     ]
    }
   ],
   "source": [
    "False_Positives_200, False_Negatives_200, Similar_Pairs_200 = evaluate_min_hashing(200, user_films, usim_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "de54ba75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average False Positives (200 Hash Functions) : 8967\n",
      "Average False Negatives (200 Hash Functions) : 419150\n",
      "\n",
      "Elapsed time: 103 secs.\n"
     ]
    }
   ],
   "source": [
    "# start time\n",
    "st = time.time()\n",
    "\n",
    "FP_avg_200, FN_avg_200 = mulpiple_min_hash_evaluations(user_films, usim_result, 200)\n",
    "\n",
    "# end time\n",
    "et = time.time()\n",
    "\n",
    "print(f'\\nElapsed time: {round(et-st)} secs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb748e2",
   "metadata": {},
   "source": [
    "### *Comments* \n",
    "\n",
    "- **Number of Hash Functions and False Positives:**\n",
    "  - **Decreasing False Positives:** Increasing the number of hash functions often leads to a decrease in false positives.\n",
    "    - More hash functions provide a more accurate representation of the sets, making it less likely for dissimilar sets to have a high fraction of matching hash values.\n",
    "  - **Improved Precision:** Higher precision means that sets with a MinHash similarity above the threshold are more likely to be truly similar.\n",
    "\n",
    "- **Number of Hash Functions and False Negatives:**\n",
    "  - **Increasing False Negatives:** Conversely, as the number of hash functions increases, false negatives may also increase.\n",
    "    - More hash functions make it less likely for sets to have a high fraction of matching hash values by chance.\n",
    "    - This may lead to similar sets having a lower estimated similarity, increasing false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48add8",
   "metadata": {},
   "source": [
    "## *Locate Similar Users Using LSH Index*\n",
    "\n",
    "##### *Locate similar users using LSH index*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a2af3c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_signature_table(signature_matrix, num_bands):\n",
    "    \"\"\"\n",
    "    Create a signature table from the given MinHash signatures.\n",
    "\n",
    "    Parameters:\n",
    "    - signature_matrix: 3D array representing MinHash signatures (users, bands, rows)\n",
    "    - num_bands: Number of bands used in the LSH process\n",
    "\n",
    "    Returns:\n",
    "    - List of dictionaries representing signature tables for each band\n",
    "    \"\"\"\n",
    "    users = signature_matrix.shape[0]\n",
    "    bands = signature_matrix.shape[1]\n",
    "    \n",
    "    signature_tables = [{} for _ in range(num_bands)]\n",
    "    \n",
    "    for i in range(users):\n",
    "        for b in range(bands):\n",
    "            band = signature_matrix[i, b, :]\n",
    "            hash_value = tuple(band)\n",
    "            \n",
    "            if hash_value not in signature_tables[b]:\n",
    "                signature_tables[b][hash_value] = []\n",
    "            \n",
    "            signature_tables[b][hash_value].append(i)\n",
    "            \n",
    "    return signature_tables\n",
    "\n",
    "\n",
    "def LSH_similar_users(user_films, signature_table, hash_tables, threshold):\n",
    "    \"\"\"\n",
    "    Find similar users using Locality-Sensitive Hashing (LSH) on MinHash signatures.\n",
    "\n",
    "    Parameters:\n",
    "    - user_films: Dictionary mapping user IDs to sets of movies\n",
    "    - signature_table: 3D array representing MinHash signatures (users, bands, rows)\n",
    "    - hash_tables: List of dictionaries representing hash tables for each band\n",
    "    - threshold: Jaccard similarity threshold for considering users as similar\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary of similar user pairs with Jaccard similarity scores\n",
    "    - Count of true similar user pairs\n",
    "    - Count of total evaluations performed\n",
    "    \"\"\"\n",
    "    users = signature_table.shape[0]\n",
    "    bands = signature_table.shape[1]\n",
    "    \n",
    "    similar_users = defaultdict()\n",
    "    true_pairs = 0\n",
    "    evaluations = 0\n",
    "    \n",
    "    for i in range(users):\n",
    "        for j in range(i + 1, users):\n",
    "            for b in range(bands):\n",
    "                \n",
    "                band = signature_table[i, b, :]\n",
    "                hash_value = tuple(band)\n",
    "\n",
    "                if hash_value in hash_tables[b]:                    \n",
    "                    if j in hash_tables[b][hash_value]:\n",
    "                        \n",
    "                        union = user_films[i + 1].union(user_films[j + 1])\n",
    "                        inter = user_films[i + 1].intersection(user_films[j + 1])\n",
    "\n",
    "                        jacc = len(inter) / len(union)\n",
    "                \n",
    "                        evaluations += 1\n",
    "                        \n",
    "                        if jacc >= threshold:                           \n",
    "                            true_pairs += 1\n",
    "                            key = str(i + 1) + \"_\" + str(j + 1)\n",
    "                    \n",
    "                            similar_users[key] = jacc\n",
    "                            \n",
    "                            break\n",
    "                        \n",
    "    similar_users = sorted(similar_users.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return dict(similar_users), true_pairs, evaluations\n",
    "\n",
    "def run_lsh_evaluation(users, user_films, b, r, num_runs=5, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Run Locality Sensitive Hashing (LSH) evaluation multiple times and collect results.\n",
    "\n",
    "    Parameters:\n",
    "    - user_films: Dictionary mapping user IDs to sets of movies\n",
    "    - b: Number of bands\n",
    "    - r: Number of rows in each band\n",
    "    - num_runs: Number of runs for LSH evaluation\n",
    "    - threshold: Jaccard similarity threshold for considering users as similar\n",
    "\n",
    "    Returns:\n",
    "    - List of true positive pairs for each run\n",
    "    - List of total evaluations (candidate pairs) for each run\n",
    "    \"\"\"\n",
    "    True_Pairs = []\n",
    "    Candidates_Cnt = []\n",
    "\n",
    "    for k in range(num_runs):\n",
    "        \n",
    "        # Generate MinHash signatures for each user\n",
    "        min_hash_signatures = generate_min_hash_signatures(user_films, 200, 2**32)\n",
    "\n",
    "        # Reshape MinHash signatures into a 3D array\n",
    "        signature_t = np.array(min_hash_signatures).reshape(len(min_hash_signatures), b, r)\n",
    "\n",
    "        # Create hash tables for LSH\n",
    "        hash_t = create_hash_tables(signature_t)\n",
    "\n",
    "        # Run LSH to find similar users\n",
    "        similar_users, true_pairs, evaluations = LSH_similar_users(user_films, signature_t, hash_t, threshold)\n",
    "\n",
    "        # Collect results for each run\n",
    "        True_Pairs.append(true_pairs)\n",
    "        Candidates_Cnt.append(evaluations)\n",
    "\n",
    "    return True_Pairs, Candidates_Cnt\n",
    "\n",
    "def run_lsh_evaluation(user_films, num_hashes, b, r, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Run Locality-Sensitive Hashing (LSH) evaluation using MinHash signatures.\n",
    "\n",
    "    Parameters:\n",
    "    - user_films: Dictionary mapping user IDs to sets of movies\n",
    "    - num_hashes: Number of hash functions used in generating MinHash signatures\n",
    "    - b: Number of bands in the LSH process\n",
    "    - r: Number of rows in each band\n",
    "    - threshold: Jaccard similarity threshold for considering users as similar\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary of similar user pairs with Jaccard similarity scores\n",
    "    - Count of true similar user pairs\n",
    "    - Count of total evaluations performed\n",
    "    \"\"\"\n",
    "    # Generate MinHash signatures for all users\n",
    "    min_hash_signatures = generate_min_hash_signatures(user_films, num_hashes, 2**32)\n",
    "\n",
    "    # Create a 3D array representing MinHash signatures\n",
    "    signature_matrix = np.array(list(min_hash_signatures.values())).reshape(len(min_hash_signatures), b, r)\n",
    "\n",
    "    # Create signature tables from the MinHash signatures\n",
    "    sig_tables = create_signature_table(signature_matrix, b)\n",
    "\n",
    "    # Create hash tables for each band\n",
    "    hash_tables = [defaultdict(list) for _ in range(b)]\n",
    "    for i, sig_table in enumerate(sig_tables):\n",
    "        for hash_value, users in sig_table.items():\n",
    "            hash_tables[i][hash_value] = users\n",
    "\n",
    "    # Run LSH to find similar users\n",
    "    similar_users, true_pairs, evaluations = LSH_similar_users(user_films, signature_matrix, hash_tables, threshold)\n",
    "\n",
    "    return similar_users, true_pairs, evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44134abc",
   "metadata": {},
   "source": [
    "### *LSH Instance 1* \n",
    "\n",
    "- $b = 25$\n",
    "- $r = 8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ef788099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Pairs Average (b = 25, r = 8): 2.2\n",
      "Evaluations Average (b = 25, r = 8): 50.0\n",
      "\n",
      "Elapsed time: 135 secs.\n"
     ]
    }
   ],
   "source": [
    "# start time\n",
    "st = time.time()\n",
    "\n",
    "# initialization\n",
    "b = 25\n",
    "r = 8\n",
    "\n",
    "# lists to hold values for each iteration\n",
    "True_Pairs = []\n",
    "Candidates = []\n",
    "\n",
    "# five runs\n",
    "for i in range(5):\n",
    "    \n",
    "    similar_users, true_pairs, evaluations = run_lsh_evaluation(user_films, 200, b, r, threshold=0.5)\n",
    "    \n",
    "    #append \n",
    "    True_Pairs.append(true_pairs)\n",
    "    Candidates.append(evaluations)\n",
    "    \n",
    "# end time\n",
    "et = time.time()\n",
    "\n",
    "print(f'True Pairs Average (b = {b}, r = {r}): {sum(True_Pairs) / len(True_Pairs)}')\n",
    "print(f'Evaluations Average (b = {b}, r = {r}): {sum(Candidates) / len(Candidates)}')\n",
    "\n",
    "print(f'\\nElapsed time: {round(et-st)} secs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e2ccd",
   "metadata": {},
   "source": [
    "### *LSH Instance 2*  <a class='anchor' id='lsh_instance_2'></a>\n",
    "\n",
    "- $b = 40$\n",
    "- $r = 5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "308dfe7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Pairs Average (b = 40, r = 5): 8.2\n",
      "Evaluations Average (b = 40, r = 5): 2751.6\n",
      "\n",
      "Elapsed time: 173 secs.\n"
     ]
    }
   ],
   "source": [
    "# start time\n",
    "st = time.time()\n",
    "\n",
    "# initialization\n",
    "b = 40\n",
    "r = 5\n",
    "\n",
    "# lists to hold values for each iteration\n",
    "True_Pairs = []\n",
    "Candidates = []\n",
    "\n",
    "# five runs\n",
    "for i in range(5):\n",
    "    \n",
    "    similar_users, true_pairs, evaluations = run_lsh_evaluation(user_films, 200, b, r, threshold=0.5)\n",
    "    \n",
    "    #append \n",
    "    True_Pairs.append(true_pairs)\n",
    "    Candidates.append(evaluations)\n",
    "    \n",
    "# end time\n",
    "et = time.time()\n",
    "\n",
    "print(f'True Pairs Average (b = {b}, r = {r}): {sum(True_Pairs) / len(True_Pairs)}')\n",
    "print(f'Evaluations Average (b = {b}, r = {r}): {sum(Candidates) / len(Candidates)}')\n",
    "\n",
    "print(f'\\nElapsed time: {round(et-st)} secs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714a9d62",
   "metadata": {},
   "source": [
    "### *Comments* \n",
    "\n",
    "1. **True Pairs Average and Evaluations Average:**\n",
    "   - As `b` and `r` increase, the number of true pairs identified tends to increase.\n",
    "   - The number of evaluations (comparisons between pairs of users) also increases with higher values of `b` and `r`.\n",
    "\n",
    "2. **Effect of `b` (Number of Bands):**\n",
    "   - Increasing `b` means fewer rows in each band, leading to a more granular hashing process.\n",
    "   - A higher number of bands (`b`) might increase the precision of the LSH method, resulting in a better identification of true similar pairs.\n",
    "   - This could explain the increase in the average number of true pairs as `b` increases.\n",
    "\n",
    "3. **Effect of `r` (Number of Rows in Each Band):**\n",
    "   - Increasing `r` means each band has more rows, which makes the hashing process more coarse.\n",
    "   - A higher number of rows in each band (`r`) might increase the recall of the LSH method by covering a larger portion of the MinHash signature.\n",
    "   - This might be the reason for a higher number of evaluations as `r` increases, as more pairs need to be compared.\n",
    "\n",
    "4. **Trade-off between Precision and Recall:**\n",
    "   - There is often a trade-off between precision (accuracy of identified similar pairs) and recall (ability to find all similar pairs).\n",
    "   - Higher values of `b` and lower values of `r` could lead to better precision but lower recall, while lower values of `b` and higher values of `r` could result in higher recall but lower precision.\n",
    "\n",
    "5. **Computational Cost:**\n",
    "   - A higher number of evaluations generally increases computational cost, which is reflected in the elapsed time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
