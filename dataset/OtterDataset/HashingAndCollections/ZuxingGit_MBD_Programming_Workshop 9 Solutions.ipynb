{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characteristic Matrix\n",
    "In natural language processing, a characteristic matrix (also known as a term-document matrix) is a matrix that represents the presence or absence of each term (word) in each document in a collection. The rows of the matrix correspond to the terms, and the columns correspond to the documents. Each element in the matrix represents whether the corresponding term appears in the corresponding document. Typically, the values in the matrix are binary, indicating presence or absence, but they can also be weighted to represent the importance of each term in each document. Characteristic matrices are commonly used as a basis for various text analysis tasks such as information retrieval, topic modeling, and text classification.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In natural language processing, a k-shingle (also known as k-gram) refers to a sequence of k contiguous tokens (usually words) from a document. For example, if we have the sentence \"The quick brown fox jumps over the lazy dog\", a 3-shingle of this sentence would be \"The quick brown\", \"quick brown fox\", \"brown fox jumps\", and so on.\n",
    "\n",
    "K-shingles are often used in text similarity and plagiarism detection algorithms, as they can help identify similar phrases or sentences that share common subsequences of words. The Jaccard similarity metric, for instance, can be used to compare the set of k-shingles between two documents to measure their similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shingles(text, k):\n",
    "    # split the text into words\n",
    "    words = re.findall(r'\\w+', text)\n",
    "\n",
    "    # create a set of k-grams\n",
    "    shingles = set()\n",
    "    for i in range(len(words) - k + 1):\n",
    "        shingle = \" \".join(words[i:i+k])\n",
    "        shingles.add(shingle)\n",
    "\n",
    "    return shingles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard Similarity\n",
    "Jaccard similarity is a measure of similarity between two sets of elements. It is defined as the size of the intersection divided by the size of the union of the sets. In language processing, Jaccard similarity is often used to compare the similarity between two documents by considering the set of words present in each document. The Jaccard similarity between two documents is the ratio of the number of common words to the total number of distinct words in both documents. This measure ranges from 0 (no overlap) to 1 (identical documents). Jaccard similarity is widely used in applications such as document clustering, search engines, and recommendation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jaccard similarity measures the similarity between two sets of data to see which members are shared and distinct. The Jaccard similarity is calculated by dividing the number of observations in both sets by the number of observations in either set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate the Jaccard similarity of two sets\n",
    "def get_jaccard_similarity(s1, s2):\n",
    "    # calculate the Jaccard index\n",
    "    non_zero = np.where(s1 + s2 > 0)[0]\n",
    "    set_intersect = np.sum(s1[non_zero] & s2[non_zero])\n",
    "    set_union = len(non_zero)\n",
    "    return set_intersect / set_union\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minhashing \n",
    "\n",
    "Minhashing is a technique used in language processing for efficiently estimating the similarity between two large sets of data, such as documents or web pages. The basic idea of minhashing is to represent each set as a signature, which is a much shorter sequence of values that still captures the most important characteristics of the original set.\n",
    "\n",
    "To create the signature, minhashing uses a set of hash functions to map the elements of each set to a smaller range of values. The signature for a set is then constructed by selecting the minimum hash value among all the elements in the set for each hash function. The resulting signature is typically much shorter than the original set, which makes it more efficient to compare signatures of different sets to estimate their similarity.\n",
    "\n",
    "One common application of minhashing is in the construction of Locality-Sensitive Hashing (LSH) algorithms, which are used to quickly find similar pairs of documents or other data in large datasets. By dividing the signature space into a set of \"buckets\" using a hash function, LSH algorithms can efficiently identify pairs of signatures that are likely to belong to similar sets, allowing for further processing to determine the actual degree of similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MinHash algorithm will provide us with a fast approximation to the Jaccard Similarity between two sets.\n",
    "\n",
    "For each set in our data, we are going to calculate a MinHash signature. The MinHash signatures will all have a fixed length, independent of the size of the set. \n",
    "\n",
    "Minhashing involves compressing the large sets of unique shingles into a much smaller representation called “signatures”.\n",
    "We then use these signatures to measure the similarity between documents.\n",
    "Although it is impossible for these signatures to give the exact similarity measure, the estimates are pretty close.\n",
    "The larger the number of signatures chosen, the more accurate the estimate is.\n",
    "\n",
    "More info here: https://www.youtube.com/watch?v=R-iFka68ZwM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get the min-hash values for a set of shingles\n",
    "def get_min_hash(shingles, num_hashes):\n",
    "    # create a list of hash functions\n",
    "    hash_functions = [hashlib.sha1, hashlib.md5, hashlib.sha256]\n",
    "\n",
    "    # create a dictionary of min-hash values\n",
    "    min_hashes = defaultdict(lambda: float(\"inf\"))\n",
    "    for shingle in shingles:\n",
    "        # hash the shingle using each of the hash functions\n",
    "        for i, hf in enumerate(hash_functions):\n",
    "            hash_value = int(hf(shingle.encode()).hexdigest(), 16)\n",
    "\n",
    "            # update the min-hash value for this hash function\n",
    "            if hash_value < min_hashes[i]:\n",
    "                min_hashes[i] = hash_value\n",
    "\n",
    "    return [min_hashes[i] for i in range(num_hashes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality Sensitivity Hashing (LSH)\n",
    "\n",
    "Locality-sensitive hashing (LSH) is a technique used in natural language processing (NLP) to find similar documents efficiently. It is a technique that involves hashing high-dimensional vectors such that similar vectors are mapped to the same hash buckets with high probability. This allows for the efficient retrieval of similar documents by querying only a small subset of the hash buckets, rather than the entire collection of documents.\n",
    "\n",
    "In LSH, a set of hash functions are generated such that each function maps a high-dimensional vector to a low-dimensional hash value. The idea is to partition the vectors into hash buckets based on their hash values. Similar vectors are more likely to be hashed to the same bucket, allowing for efficient retrieval of similar documents. The number of hash functions and the number of buckets can be tuned to achieve a desired trade-off between accuracy and speed.\n",
    "\n",
    "LSH has a wide range of applications in NLP, including near-duplicate detection, plagiarism detection, and document clustering. It is particularly useful in scenarios where the size of the document collection is large and a brute-force search is not feasible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the information necessary to compute the similarity between documents has been compressed from the original sparse characteristic matrix into a much smaller signature matrix, but the underlying problem or need to perform pairwise comparisons on all the documents still exists.\n",
    "\n",
    "\n",
    "The concept for locality-sensitive hashing (LSH) is that given the signature matrix of size n (row count), we will partition it into b bands, resulting in each band with r rows. This is equivalent to the simple math formula — n = br, thus when we are doing the partition, we have to be sure that the b we choose is divisible by n.\n",
    "\n",
    "Locality-Sensitive Hashing (LSH)\n",
    "Locality-Sensitive Hashing (LSH) is a technique for approximate nearest neighbor search, which is the problem of finding the data point in a dataset that is most similar to a given query point. LSH works by dividing the data points into multiple hash tables, where each table is designed to capture the similarity between points.\n",
    "\n",
    "To use LSH for text similarity, we can first represent each piece of text as a set of shingles. Then, we can use a hash function to map each shingle to a bucket in a hash table. The hash function should be designed such that shingles that are similar are more likely to be mapped to the same bucket.\n",
    "\n",
    "For example, let's say we have the following two pieces of text:\n",
    "\n",
    "To use LSH for text similarity, we can first represent each piece of text as a set of shingles. Then, we can use a hash function to map each shingle to a bucket in a hash table. The hash function should be designed such that shingles that are similar are more likely to be mapped to the same bucket.\n",
    "\n",
    "For example, let's say we have the following two pieces of text:\n",
    "\n",
    "\"The cat sat on the mat\"\n",
    "\"The dog lay on the rug\"\n",
    "\n",
    "We can generate their shingles and map them to buckets using a hash function:\n",
    "\n",
    "\"The cat sat\" => h(\"The cat sat\") => bucket 1\n",
    "\n",
    "\"cat sat on\" => h(\"cat sat on\") => bucket 2\n",
    "\n",
    "\"sat on the\" => h(\"sat on the\") => bucket 3\n",
    "\n",
    "\"on the mat\" => h(\"on the mat\") => bucket 4\n",
    "\n",
    "As you can see, the shingles \"The cat sat\" and \"The dog lay\" are mapped to the same bucket (bucket 1), which indicates that they are similar. Similarly, the shingles \"cat sat on\" and \"dog lay on\" are mapped to the same bucket (bucket 2), which indicates that they are also similar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate the LSH similarity of two min-hash values\n",
    "def get_lsh_similarity(m1, m2, num_bands, band_size):\n",
    "    # split the min-hash values into bands\n",
    "    bands1 = [m1[i:i+band_size] for i in range(0, len(m1), band_size)]\n",
    "    bands2 = [m2[i:i+band_size] for i in range(0, len(m2), band_size)]\n",
    "\n",
    "    # create a set of hash values for each band\n",
    "    band_hashes1 = [hash(tuple(band)) for band in bands1]\n",
    "    band_hashes2 = [hash(tuple(band)) for band in bands2]\n",
    "\n",
    "    # calculate the number of bands that have the same hash value\n",
    "    num_common_bands = len(set(band_hashes1) & set(band_hashes2))\n",
    "\n",
    "    # return the LSH similarity\n",
    "    return num_common_bands / num_bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the main function\n",
    "def main():\n",
    "    # create some sample text\n",
    "    text1 = \"I like apples\"\n",
    "    text2 = \"I like bananas\"\n",
    "    text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import re\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'cat', 'sat'), ('cat', 'sat', 'on'), ('sat', 'on', 'the'), ('on', 'the', 'mat')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"The cat sat on the mat\"\n",
    "shingles = nltk.ngrams(text.split(), 3)\n",
    "print(list(shingles))  # [\"The cat sat\", \"cat sat on\", \"sat on the\", \"on the mat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat sat: 3\n",
      "cat sat on: 6\n",
      "sat on the: 2\n",
      "on the mat: 1\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "def hash_shingle(shingle):\n",
    "    # Create a hash object\n",
    "    h = hashlib.sha1()\n",
    "    \n",
    "    # Update the hash object with the shingle\n",
    "    h.update(shingle.encode(\"utf-8\"))\n",
    "    \n",
    "    # Return the hash value modulo 10 (to map it to a bucket)\n",
    "    return int(h.hexdigest(), 16) % 10\n",
    "\n",
    "shingles = [\"The cat sat\", \"cat sat on\", \"sat on the\", \"on the mat\"]\n",
    "\n",
    "for shingle in shingles:\n",
    "    print(f\"{shingle}: {hash_shingle(shingle)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shingles for text1:\n",
      "('The', 'cat', 'sat'): 1\n",
      "('cat', 'sat', 'on'): 7\n",
      "('sat', 'on', 'the'): 0\n",
      "('on', 'the', 'mat'): 9\n",
      "\n",
      "Shingles for text2:\n",
      "('The', 'dog', 'lay'): 1\n",
      "('dog', 'lay', 'on'): 7\n",
      "('lay', 'on', 'the'): 4\n",
      "('on', 'the', 'rug'): 9\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import nltk\n",
    "\n",
    "def hash_shingle(shingle):\n",
    "    # Split the shingle into individual words\n",
    "    #words = shingle.split()\n",
    "    \n",
    "    # Create a hash object\n",
    "    h = hashlib.sha1()\n",
    "    \n",
    "    # Update the hash object with the concatenated hashes of each word in the shingle\n",
    "    for word in shingle:\n",
    "        h.update(hashlib.sha1(word.encode(\"utf-8\")).hexdigest().encode(\"utf-8\"))\n",
    "    \n",
    "    # Return the hash value modulo 10 (to map it to a bucket)\n",
    "    return int(h.hexdigest(), 16) % 10\n",
    "\n",
    "# Define the two pieces of text\n",
    "text1 = (\"The cat sat on the mat\")\n",
    "text2 = (\"The dog lay on the rug\")\n",
    "\n",
    "# Generate the shingles for each piece of text\n",
    "shingles1 = nltk.ngrams(text1.split(), 3)\n",
    "shingles2 = nltk.ngrams(text2.split(), 3)\n",
    "\n",
    "# Print the shingles and their corresponding hash values\n",
    "print(\"Shingles for text1:\")\n",
    "for shingle in shingles1:\n",
    "    print(f\"{shingle}: {hash_shingle(shingle)}\")\n",
    "\n",
    "print(\"\\nShingles for text2:\")\n",
    "for shingle in shingles2:\n",
    "    print(f\"{shingle}: {hash_shingle(shingle)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('on', 'the', 'mat')\n",
      "('sat', 'on', 'the')\n",
      "('The', 'cat', 'sat')\n",
      "('cat', 'sat', 'on')\n",
      "('The', 'dog', 'lay')\n",
      "('dog', 'lay', 'on')\n",
      "('lay', 'on', 'the')\n",
      "('on', 'the', 'rug')\n",
      "('up', 'the', 'clock')\n",
      "('ran', 'up', 'the')\n",
      "('The', 'mouse', 'ran')\n",
      "('mouse', 'ran', 'up')\n",
      "('rain', 'fell', 'from')\n",
      "('from', 'the', 'sky')\n",
      "('fell', 'from', 'the')\n",
      "('The', 'rain', 'fell')\n",
      "('on', 'the', 'earth')\n",
      "('shone', 'down', 'on')\n",
      "('sun', 'shone', 'down')\n",
      "('The', 'sun', 'shone')\n",
      "('down', 'on', 'the')\n",
      "('sang', 'in', 'the')\n",
      "('bird', 'sang', 'in')\n",
      "('in', 'the', 'tree')\n",
      "('The', 'bird', 'sang')\n",
      "Similar documents to The bird sang in the tree:\n",
      "The mouse ran up the clock\n",
      "The sun shone down on the earth\n",
      "The cat sat on the mat\n",
      "The dog lay on the rug\n",
      "The rain fell from the sky\n"
     ]
    }
   ],
   "source": [
    " \n",
    "#Here is an example of how LSH can be used in practice to find similar documents:\n",
    "\n",
    "#Suppose we have a dataset of documents, where each document is a string of text. \n",
    "#We want to use LSH to find the documents that are most similar to a given query document. \n",
    "#Here is how we can do this in Python:\n",
    "\n",
    "import hashlib\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "def hash_shingle(shingle):\n",
    "    # Split the shingle into individual words\n",
    "    print(shingle)\n",
    "    #words = shingle.split()\n",
    "    \n",
    "    # Create a hash object\n",
    "    h = hashlib.sha1()\n",
    "    \n",
    "    # Update the hash object with the concatenated hashes of each word in the shingle\n",
    "    for word in shingle:\n",
    "        h.update(hashlib.sha1(word.encode(\"utf-8\")).hexdigest().encode(\"utf-8\"))\n",
    "    \n",
    "    # Return the hash value modulo 10 (to map it to a bucket)\n",
    "    return int(h.hexdigest(), 16) % 10\n",
    "\n",
    "# Define the dataset of documents\n",
    "documents = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog lay on the rug\",\n",
    "    \"The mouse ran up the clock\",\n",
    "    \"The rain fell from the sky\",\n",
    "    \"The sun shone down on the earth\"\n",
    "]\n",
    "\n",
    "# Define the query document\n",
    "query_document = \"The bird sang in the tree\"\n",
    "\n",
    "# Generate the shingles for each document\n",
    "shingles_by_doc = {}\n",
    "for document in documents:\n",
    "    shingles_by_doc[document] = set(nltk.ngrams(document.split(), 3))\n",
    "\n",
    "# Generate the shingles for the query document\n",
    "query_shingles = set(nltk.ngrams(query_document.split(), 3))\n",
    "\n",
    "# Create a hash table for each possible hash value\n",
    "hash_tables = [[] for _ in range(10)]\n",
    "\n",
    "# Map each shingle in each document to a hash table\n",
    "for document, shingles in shingles_by_doc.items():\n",
    "    for shingle in shingles:\n",
    "        hash_tables[hash_shingle(shingle)].append(document)\n",
    "\n",
    "# Find the documents that have at least one shingle in common with the query document\n",
    "similar_documents = set()\n",
    "for shingle in query_shingles:\n",
    "    similar_documents.update(hash_tables[hash_shingle(shingle)])\n",
    "\n",
    "# Print the similar documents\n",
    "print(f\"Similar documents to {query_document}:\")\n",
    "for document in similar_documents:\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between Document 1 and Document 2: 0.15\n",
      "Similarity between Document 2 and Document 5: 0.15\n",
      "Similarity between Document 3 and Document 4: 0.09\n",
      "Similarity between Document 1 and Document 5: 0.12\n",
      "Similarity between Document 1 and Document 3: 0.13\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "documents = [\"The cat sat on the mat\",\n",
    "    \"The dog lay on the rug\",\n",
    "    \"The mouse ran up the clock\",\n",
    "    \"The rain fell from the sky\",\n",
    "    \"The sun shone down on the earth\"]\n",
    "# define k for k-shingles\n",
    "k = 3\n",
    "\n",
    "# define number of hash functions\n",
    "n_hash = 100\n",
    "\n",
    "# define number of bands\n",
    "n_bands = 50\n",
    "\n",
    "# define number of rows per band\n",
    "n_rows = int(n_hash / n_bands)\n",
    "\n",
    "# define prime number for hashing\n",
    "p = 4294967311\n",
    "\n",
    "# function to generate k-shingles\n",
    "def generate_k_shingles(text, k):\n",
    "    shingles = set()\n",
    "    text = text.replace(' ', '')\n",
    "    for i in range(len(text) - k + 1):\n",
    "        shingles.add(text[i:i+k])\n",
    "    return shingles\n",
    "\n",
    "# create a set of all shingles\n",
    "all_shingles = set()\n",
    "for doc in documents:\n",
    "    doc_shingles = generate_k_shingles(doc, k)\n",
    "    all_shingles = all_shingles.union(doc_shingles)\n",
    "all_shingles = sorted(list(all_shingles))\n",
    "\n",
    "# create a characteristic matrix\n",
    "char_matrix = np.zeros((len(all_shingles), len(documents)), dtype=np.int8)\n",
    "for j, doc in enumerate(documents):\n",
    "    doc_shingles = generate_k_shingles(doc, k)\n",
    "    for i, shingle in enumerate(all_shingles):\n",
    "        if shingle in doc_shingles:\n",
    "            char_matrix[i,j] = 1\n",
    "\n",
    "# generate hash functions\n",
    "hash_funcs = []\n",
    "for i in range(n_hash):\n",
    "    a = random.randint(0, p-1)\n",
    "    b = random.randint(0, p-1)\n",
    "    hash_funcs.append((a,b))\n",
    "\n",
    "# create minhash signature for each document\n",
    "minhash_sig = np.zeros((n_hash, len(documents)), dtype=np.int64)\n",
    "for j in range(len(documents)):\n",
    "    for i in range(n_hash):\n",
    "        minhash_sig[i,j] = p+1\n",
    "        for shingle in all_shingles:\n",
    "            if char_matrix[all_shingles.index(shingle), j] == 1:\n",
    "                h = (hash_funcs[i][0] * hash(shingle) + hash_funcs[i][1]) % p\n",
    "                if h < minhash_sig[i,j]:\n",
    "                    minhash_sig[i,j] = h\n",
    "\n",
    "# apply LSH to minhash signatures\n",
    "candidates = set()\n",
    "for band in range(n_bands):\n",
    "    band_sig = minhash_sig[band*n_rows:(band+1)*n_rows, :]\n",
    "    buckets = {}\n",
    "    for j in range(len(documents)):\n",
    "        bucket_key = hash(tuple(band_sig[:,j]))\n",
    "        if bucket_key not in buckets:\n",
    "            buckets[bucket_key] = []\n",
    "        buckets[bucket_key].append(j)\n",
    "    for key in buckets:\n",
    "        if len(buckets[key]) > 1:\n",
    "            pairs = itertools.combinations(buckets[key], 2)\n",
    "            for pair in pairs:\n",
    "                candidates.add(pair)\n",
    "\n",
    "# calculate Jaccard similarity for candidate pairs\n",
    "for pair in candidates:\n",
    "    doc1 = pair[0]\n",
    "    doc2 = pair[1]\n",
    "    intersection = np.sum(char_matrix[:,doc1] & char_matrix[:,doc2])\n",
    "    union = np.sum(char_matrix[:,doc1] | char_matrix[:,doc2])\n",
    "    similarity = intersection / union\n",
    "    print(f\"Similarity between Document {doc1+1} and Document {doc2+1}: {similarity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
