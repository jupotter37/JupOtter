{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data anonymisation in Python\n",
    "**James Tait, Marina Berger, Yuju Ahn, Robert Campbell**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CEO of the insurance company iInsureU123 is collaborating with researchers at Imperial to investigate the association between the presence of the \"Wanderlust\" gene variant, *DRD4*, and travel behavior in order to assess customer risk profiles. The CEO is also a part of a larger insurance project with the government that explores the relationship between the *DRD4* variant, geographical location, and educational attainment. She would like to anonymize her data so that it can be shared with the researchers at Imperial and with the government. She knows that the data given to the government will be released to the public. \n",
    "\n",
    "In order to better meet the disparate needs of the two groups we create two separate anonymized datasets - one suitable for the iInsureU123 project, and one for the government that is suitable for public release. These datasets are created with a balance between providing the most usable information while still preserving an acceptable degree of privacy.  \n",
    "\n",
    "We use the packages pandas (1), numpy (2), datetime (3), iso3166 (4), hdx.location.country (5), and hashlib (6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from iso3166 import countries\n",
    "from hdx.location.country import Country\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Anonymisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Removal of direct identifiers\n",
    "\n",
    "The first step in anonymising the customer_information dataset for both datasets is to remove direct identifiers. Based on HIPAA guidelines (7), the following direct identifiers are removed:\n",
    "\n",
    "- Given_name\n",
    "- Surname\n",
    "- Birthdate\n",
    "- Phone_number\n",
    "- Bank_account_number\n",
    "\n",
    "In compliance with HIPAA, the birthdate variable is converted to age to reduce the identifiability of the variable. National Insurance Number is preserved so that it can be hashed and used as a pseudonymised unique identifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a csv file with customer information - dataset to be anonymised \n",
    "df = pd.read_csv(\"Data/customer_information.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Banding birthdate into an age variable\n",
    "today = datetime.today()\n",
    "df[\"birthdate\"] = pd.DatetimeIndex(df[\"birthdate\"])\n",
    "df['age'] = df['birthdate'].apply(lambda x: today.year - x.year - ((today.month, today.day) < (x.month, x.day)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing direct identifiers\n",
    "df = df.drop(columns=['given_name', 'surname', 'birthdate', 'phone_number', 'bank_account_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender',\n",
       " 'country_of_birth',\n",
       " 'current_country',\n",
       " 'postcode',\n",
       " 'national_insurance_number',\n",
       " 'cc_status',\n",
       " 'weight',\n",
       " 'height',\n",
       " 'blood_group',\n",
       " 'avg_n_drinks_per_week',\n",
       " 'avg_n_cigret_per_week',\n",
       " 'education_level',\n",
       " 'n_countries_visited',\n",
       " 'age']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remaining columns in the dataset\n",
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Pseudonymisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The national insurance number (NIN) is a direct identifier that cannot be published by the government or shared with the researchers at Imperial. However, trusted parties in the government may wish to have access to NIN data in order to link datasets or contact individuals for the purpose of public health interventions. In order to preserve this information for trusted parties while maintaining public anonymity, the NIN column is anonymized using a hash function and a de-anonymizing hash table is created. We include the hash table in the data given to the government with the understanding that they can publish the dataset but not the hash table. The SHA-256 hashing algorithm is utilised per recommendations by the the National Institute of Standards and Technology (8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying a hash function to the national insurance number and generating a hash table mapping between the original and hashed values \n",
    "df['nat_insurance_nb_hash'] = df['national_insurance_number'].apply(lambda x:hashlib.sha256(x.encode()).hexdigest())\n",
    "\n",
    "hash_table = df[['national_insurance_number', 'nat_insurance_nb_hash']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the original national insurance number column\n",
    "df = df.drop(columns=['national_insurance_number'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the hash table into a csv file\n",
    "hash_table.to_csv(\"hash_table.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Banding\n",
    "\n",
    "The following variables will be banded in an attempt to augment the anonymity of the dataset and not share any information in a raw format:\n",
    "\n",
    "* postcode\n",
    "* education_level\n",
    "* age\n",
    "* weight\n",
    "* height\n",
    "* blood_group\n",
    "* avg_n_drinks_per_week\n",
    "* avg_n_cigret_per_week\n",
    "* n_countries_visited\n",
    "\n",
    "Variables including weight, height, blood_group, avg_n_drinks_per_week, avg_n_cigret_per_week, n_countries_visited are not considered to be quasi-identifiers because they are generally not publically avaliable. Despite this, we lightly band them as a precaution against the possibility of future data breaches, which may transform them into quasi-identifiers retroactively. We have made efforts to ensure this banding creates a minimal loss of data.\n",
    "\n",
    "For age, weight, height, avg_n_drinks_per_week, avg_n_cigret_per_week and n_countries_visited, the minimum and maximum values in the dataset are determined. This allows us to create adequate bands, ensuring every value in the dataset is banded.\n",
    "\n",
    "For blood group, the rhesus status is removed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c1. postcode\n",
    "\n",
    "We transform the direct identifier 'postcode' into a quasi-identifier 'region' by extracting the postcode area and assigning it to a region value of 'north' or 'south' based on geographical location. Postcodes beyond the contiguous United Kingdom are dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  region  Count\n",
      "1  south    419\n",
      "0  north    563\n"
     ]
    }
   ],
   "source": [
    "## Postcode\n",
    "\n",
    "# function to obtain a partial postcode from full postcode information\n",
    "def partial_postcode(postcode):\n",
    "\n",
    "    # get the first half of the postcode by splitting the full postcode with a space\n",
    "    postcode = postcode.split(' ')[0]\n",
    "\n",
    "    # partial postcode (regional information) \n",
    "    partial_postcode = ''\n",
    "    for chr in postcode:\n",
    "        if chr.isnumeric():\n",
    "            break\n",
    "        partial_postcode += chr\n",
    "\n",
    "    return partial_postcode\n",
    "\n",
    "#apply this function to the postcode \n",
    "df['postcode'] = df['postcode'].apply(lambda x: partial_postcode(x))\n",
    "\n",
    "#establish lists of prefixes to regions\n",
    "north = ['AB','BB', 'BD', 'BL','CA','DD','DG','DH','DL','EH','FK','L','LA','IV','KA','HS','NE','ML','HU','M','PA','PR','PH','HX','FY','G','HD','TD','HG','SR','WA','WN','YO','ZE','B','CF', 'CH', 'CV','CW','DE','DN','DY','GL','HR','IP','LD','LE','LL','LN','LS','NG','NN','NP','OL','PE','S','SA','SK','SK','ST','SY','TF','TS','WF','WR','WS','WV']\n",
    "south = ['AL','BA','BH','BN','BR','BS','CB','CM','CO','CR','IG','CT','KT','KW','KY','DA','N','MK','ME','RH','SE','SG','RM','PO','NR','SL','SW','TA','TN','TW','UB','WC','WD','W','TQ','TR','SM','SN','SO','SS','SP','RG','OX','PL','NW','LU','DT','E','EC','EN','EX','GU','HA','HP']\n",
    "regions = ['north','south']\n",
    "\n",
    "#map prefixes to regions\n",
    "df['region'] = df['postcode'].apply(lambda x: 'north' if x in north else x)\n",
    "df['region'] = df['region'].apply(lambda x: 'south' if x in south else x)\n",
    "df['region'] = df['region'].apply(lambda x: x if x in regions else np.nan)\n",
    "\n",
    "# Printing the region group counts\n",
    "print(df.groupby(['region']).size().reset_index(name=\"Count\").sort_values([\"Count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c2. education"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As primary and PhD education levels are the least common, they are both banded. PhD and master's students are compared into a 'graduate' level, primary and secondary education are banded to 'no_college' and bachelor education is changed to 'undergraduate' for clarity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  education_level  Count\n",
      "3             phD     52\n",
      "4         primary     61\n",
      "2           other    108\n",
      "1         masters    112\n",
      "0        bachelor    209\n",
      "5       secondary    458\n"
     ]
    }
   ],
   "source": [
    "# Printing the education bin counts\n",
    "print(df.groupby(['education_level']).size().reset_index(name=\"Count\").sort_values([\"Count\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#banding education into no college, undergraduate, and graduate\n",
    "df.loc[df.education_level == 'phD', 'education_level'] = 'graduate'\n",
    "df.loc[df.education_level == 'masters', 'education_level'] = 'graduate'\n",
    "df.loc[df.education_level == 'bachelor', 'education_level'] = 'undergraduate'\n",
    "df.loc[df.education_level == 'primary', 'education_level'] = 'no_college'\n",
    "df.loc[df.education_level == 'secondary', 'education_level'] = 'no_college'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  education_level  Count\n",
      "2           other    108\n",
      "0        graduate    164\n",
      "3   undergraduate    209\n",
      "1      no_college    519\n"
     ]
    }
   ],
   "source": [
    "# Printing the education bin counts\n",
    "print(df.groupby(['education_level']).size().reset_index(name=\"Count\").sort_values([\"Count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c3. Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  Count\n",
      "3   22     16\n",
      "4   23     16\n",
      "0   19     17\n",
      "1   20     19\n",
      "2   21     19\n",
      "8   27     20\n",
      "6   25     21\n",
      "5   24     24\n",
      "9   28     24\n",
      "7   26     26\n"
     ]
    }
   ],
   "source": [
    "# Printing the 10 lowest age counts\n",
    "print(df.groupby(['age']).size().reset_index(name=\"Count\").head(10).sort_values([\"Count\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum age of individual in the dataset is 19\n",
      "The maximum age of individual in the dataset is 67\n"
     ]
    }
   ],
   "source": [
    "# Assessing the minimum and maximum age values\n",
    "print('The minimum age of individual in the dataset is', min(df['age']))\n",
    "print('The maximum age of individual in the dataset is', max(df['age']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banding age values into 3 age categories\n",
    "bins= [18, 30, 50, 70]\n",
    "labels = ['19-30','31-50','51-70']\n",
    "df['age'] = pd.cut(df['age'], bins=bins, labels=labels, right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     age  Count\n",
      "0  19-30    222\n",
      "2  51-70    369\n",
      "1  31-50    409\n"
     ]
    }
   ],
   "source": [
    "# Printing the age group counts\n",
    "print(df.groupby(['age']).size().reset_index(name=\"Count\").sort_values([\"Count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c4. Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   weight  Count\n",
      "6    35.7      1\n",
      "7    35.8      1\n",
      "9    36.0      1\n",
      "0    35.0      2\n",
      "1    35.1      2\n",
      "5    35.6      2\n",
      "8    35.9      2\n",
      "4    35.5      3\n",
      "2    35.2      4\n",
      "3    35.4      4\n"
     ]
    }
   ],
   "source": [
    "# Printing the 10 lowest weight counts\n",
    "print(df.groupby(['weight']).size().reset_index(name=\"Count\").head(10).sort_values([\"Count\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum weight of individual in the data set is 35.0\n",
      "The maximum weight of individual in the data set is 100.0\n"
     ]
    }
   ],
   "source": [
    "# Assessing the minimum and maximum weight values\n",
    "print('The minimum weight of individual in the data set is', min(df['weight']))\n",
    "print('The maximum weight of individual in the data set is', max(df['weight']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banding weight values into 7 weight categories\n",
    "bins= [30, 40, 50, 60, 70, 80, 90, 100]\n",
    "labels = ['31-40','41-50', '51-60', '61-70', '71-80', '81-90', '91-100']\n",
    "df['weight'] = pd.cut(df['weight'], bins=bins, labels=labels, right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   weight  Count\n",
      "0   31-40     84\n",
      "2   51-60    136\n",
      "6  91-100    149\n",
      "5   81-90    151\n",
      "3   61-70    155\n",
      "4   71-80    158\n",
      "1   41-50    166\n"
     ]
    }
   ],
   "source": [
    "# Printing the weight group counts\n",
    "print(df.groupby(['weight']).size().reset_index(name=\"Count\").sort_values([\"Count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c5. Height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    height  Count\n",
      "32    1.72      9\n",
      "0     1.40     10\n",
      "36    1.76     10\n",
      "23    1.63     10\n",
      "13    1.53     10\n",
      "33    1.73     11\n",
      "39    1.79     12\n",
      "24    1.64     12\n",
      "49    1.89     13\n",
      "47    1.87     13\n",
      "26    1.66     13\n",
      "20    1.60     13\n",
      "3     1.43     14\n",
      "18    1.58     14\n",
      "17    1.57     14\n",
      "7     1.47     15\n",
      "22    1.62     15\n",
      "21    1.61     15\n",
      "1     1.41     15\n",
      "15    1.55     15\n",
      "10    1.50     15\n",
      "4     1.44     16\n",
      "14    1.54     16\n",
      "41    1.81     16\n",
      "37    1.77     17\n",
      "35    1.75     17\n",
      "34    1.74     17\n",
      "44    1.84     17\n",
      "27    1.67     17\n",
      "11    1.51     17\n",
      "31    1.71     17\n",
      "40    1.80     18\n",
      "28    1.68     18\n",
      "48    1.88     18\n",
      "5     1.45     18\n",
      "16    1.56     18\n",
      "9     1.49     18\n",
      "42    1.82     19\n",
      "45    1.85     19\n",
      "8     1.48     20\n",
      "19    1.59     20\n",
      "25    1.65     21\n",
      "38    1.78     21\n",
      "30    1.70     21\n",
      "43    1.83     21\n",
      "2     1.42     23\n",
      "12    1.52     25\n",
      "29    1.69     26\n",
      "6     1.46     26\n",
      "46    1.86     27\n"
     ]
    }
   ],
   "source": [
    "# Printing the 10 lowest height counts\n",
    "print(df.groupby(['height']).size().reset_index(name=\"Count\").head(50).sort_values([\"Count\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum height of individual in the data set is 1.4\n",
      "The maximum height of individual in the data set is 2.0\n"
     ]
    }
   ],
   "source": [
    "# Assessing the minimum and maximum height values\n",
    "print('The minimum height of individual in the data set is', min(df['height']))\n",
    "print('The maximum height of individual in the data set is', max(df['height']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banding height values into 6 height categories\n",
    "bins= [1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.1]\n",
    "labels = ['140-150', '150-160', '160-170', '170-180', '180-190', '190-200']\n",
    "df['height'] = pd.cut(df['height'], bins=bins, labels=labels, right=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c6. Blood group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  blood_group  Count\n",
      "3         AB-      6\n",
      "5          B-     18\n",
      "2         AB+     25\n",
      "7          O-     65\n",
      "1          A-     70\n",
      "4          B+     90\n",
      "0          A+    361\n",
      "6          O+    365\n"
     ]
    }
   ],
   "source": [
    "# Printing the 10 lowest blood group counts\n",
    "print(df.groupby(['blood_group']).size().reset_index(name=\"Count\").head(10).sort_values([\"Count\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banding blood group values by removing the rhesus information\n",
    "df['blood_group']= df['blood_group'].apply(lambda x:x[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  blood_group  Count\n",
      "1          AB     31\n",
      "2           B    108\n",
      "3           O    430\n",
      "0           A    431\n"
     ]
    }
   ],
   "source": [
    "# Printing the blood group counts\n",
    "print(df.groupby(['blood_group']).size().reset_index(name=\"Count\").head(10).sort_values([\"Count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c7. Average number of drinks per week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   avg_n_drinks_per_week  Count\n",
      "0                    0.0      5\n",
      "5                    0.5      5\n",
      "8                    0.8      7\n",
      "1                    0.1      8\n",
      "9                    0.9      9\n",
      "2                    0.2     10\n",
      "3                    0.3     10\n",
      "6                    0.6     11\n",
      "7                    0.7     11\n",
      "4                    0.4     13\n"
     ]
    }
   ],
   "source": [
    "# Printing the 10 lowest average number of drinks per week counts\n",
    "print(df.groupby(['avg_n_drinks_per_week']).size().reset_index(name=\"Count\").head(10).sort_values([\"Count\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum average number of drinks per week of individual in the data set is 0.0\n",
      "The maximum average number of drinks per week of individual in the data set is 10.0\n"
     ]
    }
   ],
   "source": [
    "# Assessing the minimum and maximum average number of drinks per week values\n",
    "print('The minimum average number of drinks per week of individual in the data set is', min(df['avg_n_drinks_per_week']))\n",
    "print('The maximum average number of drinks per week of individual in the data set is', max(df['avg_n_drinks_per_week']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banding average number of drinks per week values into 5 average number of drinks per week categories\n",
    "bins= [0.0, 2.0, 4.0, 6.0, 8.0, 10.1]\n",
    "labels = ['0--2', '2--4', '4--6', '6--8', '8--10']\n",
    "df['avg_n_drinks_per_week'] = pd.cut(df['avg_n_drinks_per_week'], bins=bins, labels=labels, right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  avg_n_drinks_per_week  Count\n",
      "3                  6--8    172\n",
      "4                 8--10    190\n",
      "2                  4--6    206\n",
      "0                  0--2    214\n",
      "1                  2--4    218\n"
     ]
    }
   ],
   "source": [
    "# Printing the average number of drinks per week counts\n",
    "print(df.groupby(['avg_n_drinks_per_week']).size().reset_index(name=\"Count\").sort_values([\"Count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c8. Average number of cigarettes per week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   avg_n_cigret_per_week  Count\n",
      "0                    0.3      1\n",
      "1                    0.7      1\n",
      "2                    0.8      1\n",
      "4                    2.0      1\n",
      "5                    2.2      1\n",
      "6                    2.6      1\n",
      "7                    4.2      1\n",
      "8                    4.3      1\n",
      "9                    4.6      1\n",
      "3                    1.0      2\n"
     ]
    }
   ],
   "source": [
    "# Printing the 10 lowest average number of cigarettes per week counts\n",
    "print(df.groupby(['avg_n_cigret_per_week']).size().reset_index(name=\"Count\").head(10).sort_values([\"Count\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum average number of cigarettes per week of individual in the data set is 0.3\n",
      "The maximum average number of cigarettes per week of individual in the data set is 500.0\n"
     ]
    }
   ],
   "source": [
    "# Assessing the minimum and maximum average number of cigarettes per week values\n",
    "print('The minimum average number of cigarettes per week of individual in the data set is', min(df['avg_n_cigret_per_week']))\n",
    "print('The maximum average number of cigarettes per week of individual in the data set is', max(df['avg_n_cigret_per_week']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banding average number of cigarettes per week values into 5 average number of cigarettes per week categories\n",
    "bins= [0.0, 100.0, 200.0, 300.0, 400.0, 500.1]\n",
    "labels = ['0-100', '100-200', '200-300', '300-400', '400-500']\n",
    "df['avg_n_cigret_per_week'] = pd.cut(df['avg_n_cigret_per_week'], bins=bins, labels=labels, right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  avg_n_cigret_per_week  Count\n",
      "4               400-500    190\n",
      "2               200-300    191\n",
      "1               100-200    194\n",
      "3               300-400    199\n",
      "0                 0-100    226\n"
     ]
    }
   ],
   "source": [
    "# Printing the average number of cigarettes per week counts\n",
    "print(df.groupby(['avg_n_cigret_per_week']).size().reset_index(name=\"Count\").sort_values([\"Count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c9. The number of countries visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   n_countries_visited  Count\n",
      "9                   11      9\n",
      "0                    2     12\n",
      "4                    6     14\n",
      "6                    8     16\n",
      "2                    4     19\n",
      "3                    5     19\n",
      "5                    7     24\n",
      "8                   10     24\n",
      "1                    3     26\n",
      "7                    9     30\n"
     ]
    }
   ],
   "source": [
    "# Printing the 10 lowest number of countries visited counts\n",
    "print(df.groupby(['n_countries_visited']).size().reset_index(name=\"Count\").head(10).sort_values([\"Count\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum number of countries visited in the data set is 2\n",
      "The maximum number of countries visited in the data set is 50\n"
     ]
    }
   ],
   "source": [
    "# Assessing the minimum and maximum number of countries visited values\n",
    "print('The minimum number of countries visited in the data set is', min(df['n_countries_visited']))\n",
    "print('The maximum number of countries visited in the data set is', max(df['n_countries_visited']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banding number of countries visited values into 5 number of countries visited categories\n",
    "bins= [0, 10, 20, 30, 40, 50]\n",
    "labels = ['1--10', '11--20', '21--30', '31--40', '41--50']\n",
    "df['n_countries_visited'] = pd.cut(df['n_countries_visited'], bins=bins, labels=labels, right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  n_countries_visited  Count\n",
      "0               1--10    160\n",
      "1              11--20    184\n",
      "4              41--50    197\n",
      "2              21--30    221\n",
      "3              31--40    232\n"
     ]
    }
   ],
   "source": [
    "# Printing the number of countries visited counts\n",
    "print(df.groupby(['n_countries_visited']).size().reset_index(name=\"Count\").sort_values([\"Count\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c10. Country of birth \n",
    "\n",
    "To anonymize the country of birth column, birth countries of customers are banded to continents using the Python package *iso3166 and Country*. The module iso3166 contains name, alpha-2, alpha-3, and numeric codes of countries and this allows for a look-up between country name and alpha3 code. With obtained alpha-3 codes of country of birth, a continent of each country is then retrieved from a GitHub repository known as 'ISO-3166-Countries-with-Regional-Codes' (9). This contains an 'all.csv' file which houses country name, alpha-2 code, alpha-3 code, country-code, region, and other variables as columns and a continent look up dictionary is generated using country alpha-3 codes as keys and regions as values. The 'all.csv' file is loaded in as the 'country_df' dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves url of GitHub all.csv file as vector, before loading it in as the country_df dataframe \n",
    "url = \"https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv\"\n",
    "country_df = pd.read_csv(url,index_col=0,parse_dates=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a look up continent dictionary with alpha-3 codes of countries as keys and continents as values\n",
    "look_up_continent = dict(zip(country_df['alpha-3'], country_df['region']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a list with countries existing in iso3166 package\n",
    "existing_countries = [country.name for country in countries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert country names into the relevant continent\n",
    "def country_to_continent(country_name):\n",
    "    # If country name exists in iso3166 package, find a continent with a look up dictionary\n",
    "    if country_name in existing_countries:\n",
    "        iso3_code = countries.get(country_name).alpha3\n",
    "        continent = look_up_continent[iso3_code]\n",
    "    # Perform a fuzzy search if it does not exist in iso3166 package, this allows to find country names that match defined name approximately\n",
    "    # Eg. Korea -> matches with South Korea/North Korea\n",
    "    else:\n",
    "        iso3_code = Country.get_iso3_country_code_fuzzy(country_name)[0]\n",
    "        continent = look_up_continent[iso3_code]\n",
    "    return continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying a defined banding strategy to a country of birth column and dropping the column country of birth\n",
    "df['continent_of_birth'] = df['country_of_birth'].map(lambda x:country_to_continent(x))\n",
    "df = df.drop(columns=['country_of_birth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  continent_of_birth  Count\n",
      "4            Oceania    135\n",
      "2               Asia    193\n",
      "1           Americas    212\n",
      "0             Africa    227\n",
      "3             Europe    229\n"
     ]
    }
   ],
   "source": [
    "# Checking if the country_of_birth column is sufficiently banded by counting each distinct value in the column\n",
    "print(df.groupby(['continent_of_birth']).size().reset_index(name=\"Count\").sort_values([\"Count\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Attribution of variables\n",
    "\n",
    "We create two datasets, each containing the relevant variables for their research program. \n",
    "\n",
    "The iInsureU123 research question is limited to Wanderlust gene status and number of countries visited. We include in their dataset the hashed NIN to uniquely identify rows, 'cc_status' as an exposure variable, 'n_countries_visited' as an outcome variable, and 'age', 'current_country', and 'continent_of_birth' as plausible confounders. Note that variables like 'age' are not plausible confounders under random sampling because the expoure variable (ie genetic data) is antecedant to age. However, we do not know how this data was gathered, which leaves open the possibility that it may confound the outcome within the dataset if sampling was non-random. \n",
    "\n",
    "The government research question includes the Wanderlust gene, geographic data, and educational attainment. As the government's dataset will be made publicly available, external researchers may wish to utilise the dataset to investigate a variety of research questions. With this in mind, we include 'cc_status,' all geographic and educational variables, and all other non-identifiying variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting variables of interest for the company iInsureU123 - researchers at Imperial\n",
    "df_c = df[['nat_insurance_nb_hash', 'age', 'region', 'current_country', 'cc_status', 'continent_of_birth', 'n_countries_visited']]\n",
    "df_c.to_csv('Data/data_company.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting variables of interest for the government - public\n",
    "df_g = df[['nat_insurance_nb_hash', 'region', 'age', 'current_country', 'cc_status', 'continent_of_birth', 'n_countries_visited', 'education_level', 'weight', 'height', 'blood_group', 'avg_n_drinks_per_week', 'avg_n_cigret_per_week' ]]\n",
    "df_g.to_csv('Data/data_government.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Identification of k-anonymity \n",
    "\n",
    "\n",
    "The first step in calculating k-anonymity is to select quasi-identifiers. Quasi-identifiers contain information available in public datasets that could be used to deanonymize individuals in our dataset. We include the following variables as quasi-identifiers:\n",
    "- gender\n",
    "- age\n",
    "- continent_of_birth\n",
    "- current_country\n",
    "- education_level\n",
    "\n",
    "We exclude all medical data, including height, weight, blood_group, cc_status because this information is not publicly avaliable. We also exclude the hashed national_insurance_number because it has been pseudonymised. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4a. iInsureU123 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining quasi-identifiers of the company dataset\n",
    "quasi_identifiers_c = ['age', 'region','continent_of_birth', 'current_country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      age region continent_of_birth current_country  Count\n",
      "9   19-30  south            Oceania  United Kingdom     13\n",
      "4   19-30  north            Oceania  United Kingdom     14\n",
      "8   19-30  south             Europe  United Kingdom     15\n",
      "6   19-30  south           Americas  United Kingdom     18\n",
      "7   19-30  south               Asia  United Kingdom     19\n",
      "19  31-50  south            Oceania  United Kingdom     20\n",
      "29  51-70  south            Oceania  United Kingdom     23\n",
      "5   19-30  south             Africa  United Kingdom     24\n",
      "2   19-30  north               Asia  United Kingdom     26\n",
      "3   19-30  north             Europe  United Kingdom     27\n",
      "27  51-70  south               Asia  United Kingdom     28\n",
      "1   19-30  north           Americas  United Kingdom     29\n",
      "22  51-70  north               Asia  United Kingdom     31\n",
      "14  31-50  north            Oceania  United Kingdom     31\n",
      "25  51-70  south             Africa  United Kingdom     32\n",
      "26  51-70  south           Americas  United Kingdom     32\n",
      "15  31-50  south             Africa  United Kingdom     33\n",
      "24  51-70  north            Oceania  United Kingdom     33\n",
      "0   19-30  north             Africa  United Kingdom     33\n",
      "18  31-50  south             Europe  United Kingdom     38\n",
      "16  31-50  south           Americas  United Kingdom     39\n",
      "17  31-50  south               Asia  United Kingdom     39\n",
      "21  51-70  north           Americas  United Kingdom     43\n",
      "23  51-70  north             Europe  United Kingdom     43\n",
      "28  51-70  south             Europe  United Kingdom     45\n",
      "11  31-50  north           Americas  United Kingdom     47\n",
      "10  31-50  north             Africa  United Kingdom     49\n",
      "12  31-50  north               Asia  United Kingdom     50\n",
      "20  51-70  north             Africa  United Kingdom     53\n",
      "13  31-50  north             Europe  United Kingdom     53\n",
      "The dataset is  13 - anonymous.\n"
     ]
    }
   ],
   "source": [
    "# Identifying k-anonymity by counting the unique combination of quasi-identifiers in the dataset\n",
    "df_c_count = df.groupby(quasi_identifiers_c).size().reset_index(name='Count')\n",
    "print(df_c_count.sort_values(by='Count'))\n",
    "print(\"The dataset is \", min(df_c_count['Count']), \"- anonymous.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4b. Government dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining quasi-identifiers of the government dataset\n",
    "quasi_identifiers_g = ['current_country', 'region', 'education_level', 'continent_of_birth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   current_country region education_level continent_of_birth  Count\n",
      "34  United Kingdom  south           other            Oceania      4\n",
      "30  United Kingdom  south           other             Africa      6\n",
      "14  United Kingdom  north           other            Oceania      7\n",
      "10  United Kingdom  north           other             Africa      7\n",
      "33  United Kingdom  south           other             Europe      8\n",
      "4   United Kingdom  north        graduate            Oceania      9\n",
      "32  United Kingdom  south           other               Asia     10\n",
      "36  United Kingdom  south   undergraduate           Americas     11\n",
      "31  United Kingdom  south           other           Americas     12\n",
      "11  United Kingdom  north           other           Americas     13\n",
      "12  United Kingdom  north           other               Asia     13\n",
      "37  United Kingdom  south   undergraduate               Asia     13\n",
      "2   United Kingdom  north        graduate               Asia     13\n",
      "35  United Kingdom  south   undergraduate             Africa     14\n",
      "3   United Kingdom  north        graduate             Europe     15\n",
      "22  United Kingdom  south        graduate               Asia     15\n",
      "21  United Kingdom  south        graduate           Americas     16\n",
      "20  United Kingdom  south        graduate             Africa     16\n",
      "39  United Kingdom  south   undergraduate            Oceania     16\n",
      "24  United Kingdom  south        graduate            Oceania     16\n",
      "23  United Kingdom  south        graduate             Europe     17\n",
      "1   United Kingdom  north        graduate           Americas     18\n",
      "29  United Kingdom  south      no_college            Oceania     20\n",
      "18  United Kingdom  north   undergraduate             Europe     20\n",
      "17  United Kingdom  north   undergraduate               Asia     21\n",
      "19  United Kingdom  north   undergraduate            Oceania     23\n",
      "0   United Kingdom  north        graduate             Africa     24\n",
      "13  United Kingdom  north           other             Europe     27\n",
      "38  United Kingdom  south   undergraduate             Europe     28\n",
      "16  United Kingdom  north   undergraduate           Americas     28\n",
      "15  United Kingdom  north   undergraduate             Africa     29\n",
      "9   United Kingdom  north      no_college            Oceania     39\n",
      "28  United Kingdom  south      no_college             Europe     45\n",
      "27  United Kingdom  south      no_college               Asia     48\n",
      "26  United Kingdom  south      no_college           Americas     50\n",
      "25  United Kingdom  south      no_college             Africa     53\n",
      "7   United Kingdom  north      no_college               Asia     60\n",
      "6   United Kingdom  north      no_college           Americas     60\n",
      "8   United Kingdom  north      no_college             Europe     61\n",
      "5   United Kingdom  north      no_college             Africa     75\n",
      "The dataset is  4 - anonymous.\n"
     ]
    }
   ],
   "source": [
    "# Identifying k-anonymity by counting the unique combination of quasi-identifiers in the dataset\n",
    "df_g_count = df_g.groupby(quasi_identifiers_g).size().reset_index(name='Count')\n",
    "\n",
    "# Removing counts for the combination of all quasi-identifiers with 0 individual corresponding to that combination\n",
    "df_g_count.drop(df_g_count[df_g_count['Count'] == 0].index, inplace=True)\n",
    "print(df_g_count.sort_values(by='Count'))\n",
    "print(\"The dataset is \", min(df_g_count['Count']), \"- anonymous.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Safe data sharing strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OneDrive for Business cloud storage software is used to safely share each dataset. In the case of the dataset for researchers at Imperial, the option \"people in Imperial College London with the link\" enables the view access to people with the Imperial College London account and link. For the government dataset, the link enabling view access can be shared. This dataset can be downloaded by users with the link, and edited on their downloaded version. Two different links to these dataset csv files are saved in a txt file, submitted together in a zip folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this coursework, we performed the anonymisation of the customer_information dataset using several strategies including removal of direct identifiers, pseudonymisation with a hash function, banding, and splitting the original dataset into relevant targeted datasets.\n",
    "\n",
    "We find our Imperial dataset has a k-anonymity of 13 and our government dataset has a k-anonymity of 4. We consider both dataset anonymities to be adequate. The government dataset presented a particularly challenging tradeoff between data usability and anonymity. We anonymized the government data several different ways in search of the best balance between usability and anonymity. We find that this is the best tradeoff, and that the anonymity can be further improved only by removing one of the core variables for the government research program or by draconian banding. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. References\n",
    "We use the packages pandas[1], numpy[2], datetime[3], iso3166[4], hdx.location.country[5], and hashlib[6].\n",
    "\n",
    "(1) Python Package Index - PyPI. (n.d.). Python Software Foundation. Retrieved from https://pypi.org/\n",
    "\n",
    "(2) Harris, C.R., Millman, K.J., van der Walt, S.J. et al. (2020) Array programming with NumPy. (Nature 585, pp. 357–362).\n",
    "\n",
    "(3) Dataflake, hannosch, icemac, tseaver (n.d.) DateTime 4.7. Retreived from https://pypi.org/project/DateTime/\n",
    "\n",
    "(4) Spindel, Mike (n.d.) iso3166. Retreived from https://pypi.org/project/iso3166/\n",
    "\n",
    "(5) mcarans (n.d.) hdx-python-country. Retreived from https://pypi.org/project/hdx-python-country/\n",
    "\n",
    "(6) Smith, P. Gregory (n.d.) hashlib 20081119. Retreived from https://pypi.org/project/hashlib/\n",
    "\n",
    "(7) U.S. Department of Health and Human Services. (n.d.). Guidance Regarding Methods for De-identification of Protected Health Information in Accordance with the Health Insurance Portability and Accountability Act (HIPAA) Privacy Rule. Retreived from https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html\n",
    "\n",
    "(8) Lowery, J. M. (2020, March 26). MD5 vs SHA-1 vs SHA-2 - Which is the Most Secure Encryption Hash and How to Check Them. Retreived from https://www.freecodecamp.org/news/md5-vs-sha-1-vs-sha-2-which-is-the-most-secure-encryption-hash-and-how-to-check-them/\n",
    "\n",
    "(9) Duncalfe, L. (2019). ISO-3166-Countries-with-Regional-Codes, GitHub repository. Retrieved from https://github.com/lukes/ISO-3166-Countries-with-Regional-Codes/blob/master/all/all.csv\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
