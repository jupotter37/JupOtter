{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/dataquestio\n",
    "\n",
    "# 1 - PYTHON INTRO\n",
    "## 1.1 - Basic Python\n",
    "- create dictionaries, check if element is in a list, read Files, create functions\n",
    "- print(\"%d\" % var, end=''): don't add new line\n",
    "- order of operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 2 - PYTHON INTERMEDIATE\n",
    "\n",
    "## 2.1 - Classes\n",
    "- read files using the csv module\n",
    "- create class, instantiate a class object, acess atributes (variables), the __init__() method, add data upon instantiation, create class methods (functions)\n",
    "- enumerate\n",
    "\n",
    "## 2.2 - Error Handling\n",
    "- set\n",
    "- try/except block\n",
    "\n",
    "## 2.3 - List Comprehension\n",
    "- is None \n",
    "- loop on dictionary with \"for key, value in dict.items():\" (print dictionary)\n",
    "- get key with max value: max(dict, key = dict.get)\n",
    "- a list with the number 10, 5 times [10 for _ in range(5)]\n",
    "\n",
    "## 2.4 - Boolean stuff\n",
    "- statements evaluated from left to right\n",
    "- ~ :negation\n",
    "\n",
    "## 2.5 - Built in Functions and scope\n",
    "- local variables can overite built in functions\n",
    "- python looks for variables in LEGBE order: \"Local, Enclosing, Global, Built-ins, Error\".\n",
    "\n",
    "## 2.6 - Regular Expressions\n",
    "- re module. Search for and extract data.\n",
    "- speacial characters: . (any character), ^a (string starting with a), a$ (string finishing with a), [bcr]at (bat, cat or rat), \"\\\" to escape character, | to combine expressions \"cat|dog\" (cat or dog), [...-...] match range ([0-9], [a-z]\n",
    "- sub: re.sub(\"[\\(\\[] [Ss]erious [\\)\\]]\",\"[Serious]\",string)\n",
    "- re.findall(regex)\n",
    "\n",
    "## 2.7 - Dates\n",
    "- import time, time.time(), time.gmtime()\n",
    "- import datetime, datetime.datetime(), datetime.datetime.utcnow() \n",
    "- datetime.timedelta(weeks=15)\n",
    "- datetime.datetime.strftime() \n",
    "- get evenly spaced dates when plotting:\n",
    "  - import matplotlib.dates as mdates\n",
    "  - days_locator = mdates.DayLocator(bymonthday=[1,])\n",
    "  - ax.xaxis.set_major_locator(days_locator)\n",
    "  - ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "\n",
    "## 2.8 - Misc\n",
    "- random numbers:\n",
    "  - random int between 0 and 10: random.randint(0, 10)\n",
    "  - sequence of 10 random numbers between the values of 0 and 10: [random.randint(0, 10) for _ in range(10)]\n",
    "- random.sample(array, N_sample)\n",
    "- unique elements of list: np.unique(time_delta, return_counts=True)\n",
    "- import os\n",
    "  - os.remove(\"./file.txt\")\n",
    "- import shutil\n",
    "  - shutil.copy(src_file, dst_file)\n",
    "- np.absolute()\n",
    "- print formated: print(\"MSE = {:0.2f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 3 - PANDAS INTRO\n",
    "- from IPython.display import display\n",
    "\n",
    "## 3.1 - Numpy\n",
    "- np.array, array.shape, slicing (array[1:3,0:2])\n",
    "- np.genfromtxt (read data from file)\n",
    "- array.sum(axis=), array.mean(), array.astype(float)\n",
    "\n",
    "## 3.2 - Pandas basics\n",
    "- read_csv, df.head(), df.columns (get column names), df.shape (returns [nrows, ncolumns])\n",
    "- df=pd.read_csv(\"file.csv\", index_col=0)\n",
    "- df=pd.read_csv(\"file.csv\", delimiter=\"\\t\", encoding='windows-1252')\n",
    "- Series.str.contains() and Series.str.endswith()   \n",
    "- type()\n",
    "- new_df = df.copy()\n",
    "\n",
    "## 3.3 - Loc & iloc\n",
    "- loc and iloc, df[cols].iloc[0].values, df.loc[rows,cols], df.iloc[:,0:3] \n",
    "- Chained assignment: https://www.dataquest.io/blog/settingwithcopywarning/\n",
    "\n",
    "## 3.4 - Basic DataFrame operations\n",
    "- arithmetic operations on columns\n",
    "- Series.max(), Series.mean(), inplace = True, df.sort_values(\"column\", inplace=True, ascending=False)\n",
    "- df.max(numeric_only=True)\n",
    "\n",
    "## 3.5 - Advanced DataFrame operations (Working with missing data)\n",
    "- pd.isnull(df[column]) or Series.isnull(), Series.isnull()==False, Series.notnull()\n",
    "- Pivot Tables\n",
    "- df.dropna(axis=0, subset=['','']) - axis=0: rows, axis=1: columns\n",
    "- .reset_index(drop=True) (after sort_values, for example)\n",
    "- df.apply(function) with the optional axis=1, iterate over rows\n",
    "- df.describe(), df.value_counts().head()\n",
    "- pd.to_datetime(df['DATE'])\n",
    "\n",
    "## 3.6 - Series\n",
    "- string indexes\n",
    "- Series.sort_index(), Series.sort_values()\n",
    "- numpy functions can be applied on Series\n",
    "- data alignment and calculations on multiple Series\n",
    "\n",
    "## 3.7 - DataFrames\n",
    "- df.set_index(col, drop=False, inplace=True) - keep the column used for index\n",
    "- after creating index .loc[] to select rows\n",
    "- Apply, examples with lambda function \n",
    "- lambda x: np.std(x)\n",
    "\n",
    "## 3.8 Challenge: Summarizing data\n",
    "- Series.unique()\n",
    "\n",
    "## 3.9 Additional basic Pandas stuff from later missions:\n",
    "- pd.set_option('max_columns', 180)\n",
    "- pd.set_option('max_rows', 200000)\n",
    "- pd.set_option('max_colwidth', 5000)\n",
    "\n",
    "## 3.10 - Misc\n",
    "- pd.crosstab(df[\"col1\"],[df[\"col2\"]]): value counts for more than 1 column\n",
    "- create dataframe:\n",
    "  - df = pd.DataFrame(data=G_MR)\n",
    "- randomize ordering of dataframe\n",
    "  - np.random.seed(1)\n",
    "  - df = df.iloc[np.random.permutation(len(df))]\n",
    "- DataFrame.info(): returns number of non-null values in each column\n",
    "- get columns as list, then remove some\n",
    "  - col_list = df.columns.tolist()\n",
    "  - col_list.remove('col1') \n",
    "- new_df = df.select_dtypes(include=['int', 'float'])\n",
    "- find columns with highest value: new_df = df.idxmax(axis=1)\n",
    "- convert categorical variables to numerical\n",
    "  - col = pd.Categorical(df['col1'])\n",
    "  - df['col1'] = col.codes\n",
    "- The method index() returns the lowest index in list that obj appears.\n",
    "  - highest_gain_index = information_gains.index(max(information_gains))\n",
    "- convert numerical to categorical\n",
    "  - cut_points=[-1, 0, 18, 100], label_names=['Missing', 'Child','Adult']\n",
    "  - df[\"Age_categories\"] = pd.cut(df[\"Age\"],cut_points,labels=label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 4 - Data Visualization\n",
    "\n",
    "## 4.1 - Line Charts\n",
    "- import matplotlib.pyplot as plt, %matplotlib inline\n",
    "- plt.plot(x, y)\n",
    "- plt.xticks(rotation=90)\n",
    "- plt.xlabel('xx'), plt.ylabel('yy'), plt.title('title')\n",
    "- plt.legend(loc='upper left')\n",
    "- plt.show()\n",
    "\n",
    "## 4.2 - Multiple plots\n",
    "- definition of figure (container for all plots) and axis object (single plot, same as a subplot)\n",
    "- fig = plt.figure(figsize=(10,6)) (non preferred method)\n",
    "  - multiple separate plots: \n",
    "    - ax1 = fig.add_subplot(2,1,1), ax2 = fig.add_subplot(2,1,2) (2 rows, 1 columns)\n",
    "    - ax1.plot(x,y), ax2.plot(x,y)\n",
    "- fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=(15,4))  (preferred method)\n",
    "\n",
    " \n",
    "## 4.3 - Bar Plots And Scatter Plots\n",
    "- fig, ax = plt.subplots() \n",
    "  - ax.set_xticks(tick_positions), ax.set_xticklabels(num_cols, rotation=90)\n",
    "  - ax.set_xlabel('xx'),ax.set_ylabel('yy'), ax.set_title('title')\n",
    "- vertical bar plot\n",
    "  - cols = ['col1', 'col2', 'col3']\n",
    "  - bar_heights = xx[0:3]\n",
    "  - bar_positions = arange(0,3,1.) + 1.\n",
    "  - tick_positions = range(1,4)\n",
    "  - ax.bar(bar_positions,bar_heights,width=0.5)\n",
    "- horizontal bar plot\n",
    "  - cols = ['col1', 'col2', 'col3']\n",
    "  - bar_widths = xx[0:3]\n",
    "  - bar_positions = arange(3) + 1.\n",
    "  - tick_positions = range(1,4)\n",
    "  - ax.barh(bar_positions,bar_widths,height=0.5)\n",
    "- scatter plot\n",
    "  - ax.scatter(x,y)\n",
    "\n",
    "## 4.4 - Histograms And Box Plots\n",
    "- Histograms\n",
    "  - Series.value_counts().sort_index() can be used to create frequency distribution\n",
    "  - ax.hist(x, bins=20, range=(0,5))\n",
    "- box-and-whisker plot\n",
    "  - ax.boxplot(df[col_1])\n",
    "  - ax.boxplot(df[[\"col1\", \"col2\"]].values)\n",
    "  - ax.set_xticklabels(cols, rotation=90)\n",
    "\n",
    "## 4.5 - Pandas Plotting\n",
    "- kind = scatter\n",
    "  - ax = recent_grads.plot(x='col1', y='col2', kind='scatter', title = 'tittle', figsize=(5,5))\n",
    "- kind = hist\n",
    "  - df[col1].plot(kind='hist')\n",
    "- Series.hist\n",
    "  - df[col1].hist(bins=20, range=(0,120000))\n",
    "- Scatter Matrix\n",
    "  - from pandas.tools.plotting import scatter_matrix\n",
    "  - scatter_matrix(df[[col1,col2]], figsize=(10,10))\n",
    "- df.plot.bar(x=col1, y=[col2,col3])\n",
    "- Series.plot.box()\n",
    "- df.plot.hexbin(x=col1, y=col2, gridsize=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 5 - Storytelling Through Data Visualization (Seaborn)\n",
    "\n",
    "## 5.1 - Improving Plot Aesthetics (Gender Gap)\n",
    "- ax.tick_params(left=False, right=False, top=False, bottom=False)\n",
    "- for key,spine in ax.spines.items(): spine.set_visible(False) (same as ax.spines['left'].set_visible(False))\n",
    "- Pass a tuple to the c parameter to have RGB color (range between 0 and 1 (not 0 and 255))\n",
    "- ax.text(x, y, 'text')\n",
    "- ax.axhline(y, c=cb_grey, alpha=0.3)  \n",
    "- fig.savefig()\n",
    "\n",
    "## 5.2 - Seaborn (Conditional Plots)\n",
    "- import seaborn as sns\n",
    "- sns.kdeplot(xx, shade=True)\n",
    "- seaborn.set_style() to change style\n",
    "- sns.despine(left=True, bottom=True) (remove axis)\n",
    "- FacetGrid 2 variables\n",
    "  - grid = sns.FacetGrid(df, col=col1, size=6) (size in inches)\n",
    "  - grif.map(sns.kdeplot, col2, shade=True)\n",
    "- FacetGrid plots on 3 variables\n",
    "  - grid = sns.FacetGrid(df, col=col1, row=col2, size=4)\n",
    "  - grid.map(sns.kdeplot, col3, shade=True)\n",
    "- FacetGrid plots on 4 variables\n",
    "  - grid = sns.FacetGrid(df, col=col1, row=col2, hue=col3, size=3)\n",
    "  - grid.map(sns.kdeplot, col4, shade=True)\n",
    "- grid.add_legend()\n",
    "\n",
    "## 5.3 - Basemap toolkit (Visualizing Geographic Data)\n",
    "- workflow\n",
    "  - Create a new basemap instance\n",
    "  - Convert spherical coordinates to Cartesian\n",
    "  - Use the matplotlib and basemap methods to customize the map.\n",
    "  - Display the map.\n",
    "\n",
    "- create basemap instance:\n",
    "  - import matplotlib.pyplot as plt\n",
    "  - from mpl_toolkits.basemap import Basemap\n",
    "  - m = Basemap(projection='merc', llcrnrlat=-80, urcrnrlat=80, llcrnrlon=-180, urcrnrlon=180)\n",
    "  - x,y = m(airports['longitude'].tolist(), airports['latitude'].tolist())\n",
    "  - m.scatter(x,y,s=1)\n",
    "  - m.drawcoastlines()\n",
    "- Routes (great Circles)\n",
    "  - if((abs(row['end_lat']-row['start_lat']) < 180) & (abs(row['end_lon']-row['start_lon']) < 180)):\n",
    "  - m.drawgreatcircle(row['start_lon'], row['start_lat'], row['end_lon'], row['end_lat'])\n",
    "- Another example on section 6.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 6 - Data Cleaning\n",
    "\n",
    "## 6.1 - Data Cleaning Walkthrough\n",
    "- Webpages with data:\n",
    "  - https://www.data.gov/ <br>\n",
    "  - https://www.reddit.com/r/datasets/ <br>\n",
    "  - https://github.com/caesar0301/awesome-public-datasets <br>\n",
    "  - http://rs.io/100-interesting-data-sets-for-statistics/ <br\n",
    "- store multiple dfs in a dictionary\n",
    "  - data = {}, data['df_name']=df\n",
    "- pd.concat([df1, df2], axis=0)\n",
    "- df.drop('col1', axis = 1, inplace = True)\n",
    "- pd.to_numeric(df['col1'], errors=\"coerce\") #coerce, treat invalid strins as missing values\n",
    "- string.split(\",\"), string.replace('(','')\n",
    "  - lon = coords[0].split(\",\")[1].replace(\")\", \"\").strip()\n",
    "- .strip() (remove leading and trailing characters), .lstrip() (remove leading characters), .rstrip() (remove trailing characters)\n",
    "  \n",
    "## 6.2 - Combining the Data\n",
    "- new_df = df.groupby(\"col1\").agg(numpy.mean)\n",
    "- Merge DFs (pd.DataFrame.merge())\n",
    "  - inner: join rows with commons col1 values\n",
    "  - left: join rows with col1 values from left df\n",
    "  - outer: join rows with col1 values from either df\n",
    "- df.mean() : mean of every col in df\n",
    "- df.fillna(means) \n",
    "\n",
    "## 6.3 - Analyzing And Visualizing The Data\n",
    "- df.corr(): correlations bettween all columns\n",
    "- Basemap toolkit:\n",
    "  - m.scatter(lon, lat, s=50, zorder=2, latlon=True, c=df['col1'], cmap='summer')\n",
    "  - m.drawmapboundary(fill_color='#D0D8F1')\n",
    "  - m.drawrivers(color='#6D5F47', linewidth=.4)\n",
    "\n",
    "\n",
    "## 6.4 - Pandas map and rename\n",
    "- Series.map(dict)\n",
    "  - dict = {'Yes': True, 'No': False}\n",
    "- df = df.rename(columns={df.columns[0:3] : 'seen'+str(idy)})\n",
    "\n",
    "## 6.5 - Strings\n",
    "- join strings: s='', s+= str(xx) + ' '\n",
    "- words = strings.lower().split(' ')  \n",
    "- Counter(list_of_strings): returns dictionary with value counts\n",
    "- formated printing\n",
    "  - print(\"%s : %d\" %(name, row))\n",
    "- sort dictionary\n",
    "  - s = [(k, count[k]) for k in sorted(count, key=count.get, reverse=True)]\n",
    "  - print(s[0:99])\n",
    "- parse: convert time stamp to datetime object\n",
    "  - from dateutil.parser import parse\n",
    "  - df['col2'] = df['col1'].apply(lambda x: parse(x).hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 7 - Command Line: Beginner\n",
    "\n",
    "## 7.1 - Command Line Basics\n",
    "- whoami: check user name\n",
    "- mkdir -v test2: to use the verbose option\n",
    "- mkdir --help : most commands have a help flag\n",
    "- rmdir: remove directory\n",
    "- channing:\n",
    "  - ls -la same as ls -a -l.\n",
    "- longer flags with two dashes:\n",
    "  - ls --ignore=test.txt\n",
    "- count number of rows\n",
    "  - wc -l file\n",
    "\n",
    "## 7.2 - Working with Files\n",
    "- touch: creates an empty file\n",
    "- 'echo 'Hello' > test.txt'\n",
    "- nano test.txt: open file for editing, ctrl+x to quit\n",
    "- permissions\n",
    "  - rwx / oga\n",
    "  - stat file: check permissions on a file\n",
    "- MIME types and extensions\n",
    "\n",
    "## 7.3 - Bash Variables\n",
    "- bash is the default shell on most systems\n",
    "- Assign Variables \n",
    "  - FOOD='Shrimp gumbo'\n",
    "- Retreive variable's values\n",
    "  - echo $FOOD\n",
    "- Environment variables\n",
    "  - export FOOD='Chicken and waffles'\n",
    "\n",
    "## 7.4 - Command Line Python Scripting\n",
    "- type python on the command line to start python\n",
    "- Access the environment variables:\n",
    "  - import os \n",
    "  - print(os.environ[\"FOOD\"])\n",
    "- exit()\n",
    "- PATH (any program in any folder that PATH directs to can be run just by typing the name)\n",
    "- echo $PATH, gives all the folders in the PATH\n",
    "- python file.py\n",
    "- Virtual environments\n",
    "  - virtualenv -p /usr/bin/python3 new_env_folder\n",
    "  - source python3/bin/activate\n",
    "  - deactivate\n",
    "- python -V: check python version \n",
    "- pip freeze: check installed packages\n",
    "- import sys\n",
    "  - print(sys.argv[1])\n",
    "  - python script.py 'Hello'\n",
    "\n",
    "## 7.8 - Installing Cygwin (removed)\n",
    "## 7.9 - Installing Windows subsystem on linux (removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 8 - Command Line: Intermediate\n",
    "\n",
    "## 8.1 - Working with Jupyter console (removed)\n",
    "\n",
    "\n",
    "## 8.2 - Piping And Redirecting Output\n",
    "- \"Wake up!\" >> dream.txt: add text to a file\n",
    "- sort -r < beer.txt: redirect from file to the input\n",
    "- grep \"beer\" beer.txt coffee.txt\n",
    "- grep \"beer\" beer?.txt: ? represents a single unknown character\n",
    "- | Pipe character: tail -n 10 logs.txt | grep \"Error\"\n",
    "- run two commands: echo \"All the beers are gone\" >> beer.txt && cat beer.txt\n",
    "\n",
    "## 8.3 - Data Cleaning And Exploration Using Csvkit\n",
    "- csvstack: for stacking rows from multiple CSV files.\n",
    "  - merge 3 files: csvstack file1.csv file2.csv file3.csv > final.csv\n",
    "  - create new column with file origin: csvstack -n origin -g 1,2,3 file1.csv file2.csv file3.csv > final.csv   \n",
    "- csvlook: renders CSV in pretty table format.\n",
    "  - tabular format output: head -10 file.csv | csvlook\n",
    "- csvcut: for selecting specific columns from a CSV file.\n",
    "  - display columns: csvcut -n file.csv\n",
    "  - select specific column: csvcut -c 1 file.csv\n",
    "  - Display first 10 value of column 2: csvcut -c 2 file.csv | head -n10    \n",
    "- csvstat: for calculating descriptive statistics for some or all columns.\n",
    "  - csvcut -c 4 file.csv | csvstat\n",
    "  - mean of all columns: csvstat --mean file.csv\n",
    "  - csvcut -c 2 file.csv | csvstat --max<br>\n",
    "  - csvcut -c 2 Combined_hud.csv | csvstat --nulls  \n",
    "- csvgrep: for filtering tabular data using specific criteria.\n",
    "  - csvgrep -c 2 -m -9 file.csv\n",
    "  - csvgrep -c 2 -m -9 file.csv | head -n10 | csvlook\n",
    "  - -i for non-problematic rows: csvgrep -c 2 -r -9 -i file.csv > file_2.csv\n",
    "  - -m PATTERN, -r REGEX, -f MATCHFILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 9 - Git\n",
    "\n",
    "## 9.1- Introduction To Git\n",
    "- git init: to initialize\n",
    "- setup\n",
    "- git add script.py: add to the staging area\n",
    "- git commit -m \"Commit message here\": to commit, -m to include message\n",
    "- git diff: to see modifications\n",
    "- git log --stat: see commit history, --statt to see more details\n",
    "\n",
    "## 9.2 - Git Remotes\n",
    "- git clone: to clone a remote repository.\n",
    "- git branch: see all of the branches\n",
    "- git push origin master: pushe commits from our local repo to the remote repo\n",
    "- git remote: list all of the repo's remotes. -v to get information about where the remote repos are located.\n",
    "- git log: to see all the git hashes (numbers)\n",
    "- git show: with a hash to see what changed in a specific commit\n",
    "- git diff hash1 hash2: difference between two commits\n",
    "- git reset --hard c12 switches back to the commit with the hash c12\n",
    "- git pull: update the current branch with the latest commits.\n",
    "- HEAD: refers to the most recent commit in the current branch (HEAD~1, HEAD~2, etc earlier commits)\n",
    "- git rev-parse HEAD~*to find the commit hash\n",
    "- git commit workflow\n",
    "\n",
    "## 9.3 - Git Branches\n",
    "- git branch branch_name: create a branch\n",
    "- git checkout: switch to the new branch\n",
    "- git checkout -b branch_name: create branch and change to it\n",
    "- git clone rep_name\n",
    "- git branch -r: show remote branches\n",
    "- git branch -a: show local branches\n",
    "- git merge other_branch: to merge a branch into another branch\n",
    "- git branch -d branch_name: delete a branch\n",
    "- git fetch: fetch all of the current branches and commits from the remote\n",
    "- git checkout branch_name: look for the branch_name branch in the local repo and remote repo\n",
    "- git rm bot.py: Stage a deleted file\n",
    "\n",
    "## 9.4 - Merge Conflicts\n",
    "- git merge --abort: abort the merge altogether to solve the conflit\n",
    "- git mergetool --tool: grafic interface\n",
    "- git checkout with --ours . and --theirs . (. means all files)\n",
    "- ignore files: .gitignore\n",
    "- git rm --cached file\n",
    "\n",
    "\n",
    "## 9.5 - Git Installation And GitHub Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 10 - Web Scraping\n",
    "\n",
    "## 10.1 - Working With APIs: Application Program Interface\n",
    "- endpoints (to retrieve certain types of data from an API)\n",
    "- requests library\n",
    "  - import requests\n",
    "  - response = requests.get(\"http://api.open-notify.org/iss-now.json\")\n",
    "- print(response.status_code)\n",
    "- JSON format\n",
    "- json.dumps (convert python obj to string), json.loads (convert Json to python obj)\n",
    "- json_data = requests.get(\"url\", params=parameters).json()\n",
    "- response.headers, response.headers['content-type']\n",
    "\n",
    "## 10.2 - Intermediate APIs\n",
    "- APIs that require authentication: access tokens\n",
    "  - headers = {\"Authorization\": \"token 453f88d45ff91fa5433cec363273de3b58c59cca\"}\n",
    "  - response = requests.get('url', headers=headers)\n",
    "- pagination\n",
    "  - params = {'per_page': 50, 'page': 1}\n",
    "  - response = requests.get('url', headers=headers, params=params)\n",
    "- requests.post, requests.patch, requests.put, requests.delete\n",
    "\n",
    "## 10.3 - Reddit API using OAuth\n",
    "- connect with OAuth (https://github.com/reddit-archive/reddit/wiki/OAuth2-Quick-Start-Example)\n",
    "- find the ID of the most upvoted comment\n",
    "- retrieve all comments associated with that post\n",
    "- upvote\n",
    "\n",
    "## 10.4 - Web Scraping (requests + beautifulsoup)\n",
    "- HTML summary\n",
    "- BeautifulSoup\n",
    "  - from bs4 import BeautifulSoup\n",
    "  - content = requests.get('url').content\n",
    "  - parser = BeautifulSoup(content, 'html.parser')\n",
    "  - body = parser.body, p = body.p, print(p.text)\n",
    "- find_all\n",
    "  - body = parser.find_all(\"body\")\n",
    "  - p = parser.find_all(\"p\")\n",
    "  - p_text = p[0].text\n",
    "- ID\n",
    "  - parser.find_all(\"p\", id=\"first\")\n",
    "- Classes\n",
    "  - parser.find_all(\"p\", class_=\"inner-text\")\n",
    "- CSS\n",
    "  - Class: parser.select('p.outer-text')\n",
    "  - ID: parser.select('#second')\n",
    "  - Multiple Classes: parser.select('p.outer-text.first-item')\n",
    "  \n",
    "## 10.5 - Web Scraping project (Xrates)\n",
    "- FFT analysis of time series\n",
    "- formating time axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 11 - SQL (Structured Query Language) Fundamentals\n",
    "\n",
    "## 11.1 - Intro to SQL\n",
    "- select * from table limit 10\n",
    "- Comparison Operators: <, <=, >, >=, =, !=\n",
    "- Select and From always first\n",
    "- limit, order by DESC\n",
    "\n",
    "## 11.2 - SQL, Summary Statistics\n",
    "- Aggregate Functions: COUNT(), MIN(), MAX(), SUM(), AVG(), TOTAL()\n",
    "- Alias: SELECT COUNT(*) as \"my text\" FROM table\n",
    "  - SELECT col1 m, col2 mc, col3 ur\n",
    "  - FROM rtable\n",
    "  - WHERE (mc = 'Engineering') AND (ur > 0.04 and ur < 0.08)\n",
    "  - ORDER BY ur DESC (or ASC)\n",
    "- DISTINCT \n",
    "  - We can use DISTINCT with multiple columns to return unique pairings of those columns\n",
    "- Nested COUNT()+DISTINCT():\n",
    "  - SELECT COUNT(DISTINCT(col1)) unique_col1 FROM table\n",
    "- Arithmetic )perators  \n",
    "  - SELECT col1, col2-col3 as col_diff from table\n",
    "  - order by col_diff \n",
    "  \n",
    "## 11.3 - Group Summary Statistics\n",
    "- GROUP BY\n",
    "  - SELECT Major_category, SUM(Employed) \n",
    "  - FROM recent_grads \n",
    "  - GROUP BY Major_category;\n",
    "- HAVING\n",
    "  - SELECT Major_category, AVG(Employed) / AVG(Total) AS share_employed FROM recent_grads \n",
    "  - GROUP BY Major_category \n",
    "  - HAVING share_employed > .8;\n",
    "- ROUND(col1)\n",
    "- PRAGMA TABLE_INFO(table)\n",
    "- CAST(col1) as float\n",
    "\n",
    "## 11.4 - Subqueries\n",
    "\n",
    "## 11.5 - Querying SQLite3 from Python\n",
    "- sqlite basic:\n",
    "  - import sqlite3\n",
    "  - conn = sqlite3.connect('./data/jobs.db')\n",
    "  - df = conn.execute(query).fetchall()\n",
    "  - conn.close()\n",
    "- Tupples, my_tupple=(), my_tupple=('Apple', 'Banana'), apple = t[0] \n",
    "- cursor.fetchone(), cursor.fetchmany(N)\n",
    "\n",
    "## 11.6 - SQLite3 with Pandas (Project: CIA Factbook)\n",
    "- SQLite3 with Pandas:\n",
    "  - conn = sqlite3.connect('./data/factbook.db')\n",
    "  - get table information:\n",
    "    - query = 'Select * from sqlite_master where type=\"table\"'\n",
    "    - df = pd.read_sql_query(query, conn)\n",
    "  - normal query:   \n",
    "    - df = pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 12 - SQL Intermediate \n",
    "\n",
    "## 12.1 - Joining Data in SQL\n",
    "- Join is used to join columns\n",
    "- Load sql table using pandas\n",
    "  - df = pd.read_csv('./data/cities.csv')\n",
    "  - df.columns=[\"col1\",\"col2\"...]\n",
    "  - df.to_sql('cities', conn, if_exists='replace', index=False, chunksize=)\n",
    "- INNER JOIN, LEFT JOIN, RIGHT JOIN, FULL OUTER JOIN\n",
    "- INNER JOIN (includes only rows where there is a mutual match from both tables)\n",
    "  - Select b.*, a.name as a_name from table_1 a\n",
    "  - INNER JOIN table_2 b on a.id = b.id \n",
    "- LEFT JOIN (Include all rows from table 1 and matches from table 2)\n",
    "  - check non matches using \"where table_2.col IS NULL\"\n",
    "- ordder by col result number\n",
    "  - order by 3\n",
    "- JOIN with subqueries\n",
    "\n",
    "## 12.2 - Intermediate Joins in SQL\n",
    "- Multiple Joins\n",
    "- Recursive Join\n",
    "- LIKE\n",
    "- CASE WHEN\n",
    "\n",
    "## 12.3 - Additional Joining Examples (From Old Mission)\n",
    "- many-to-many relation: Intermediate Join table\n",
    "- NNER JOIN and WHERE table_1.ID = table_2.ID are the same\n",
    "\n",
    "\n",
    "## 12.4 - Building and Organizing Complex Queries\n",
    "- With (subquery)\n",
    "- View (permanent with)\n",
    "- UNION, INTERSECT, EXCEPT (to join rows)\n",
    "- Multiple With subqueries\n",
    "\n",
    "## 12.5 - Guided Project: Answering Business Questions using SQL\n",
    "- Jupyter Nootebook tricks\n",
    "- context managers: https://jeffknupp.com/blog/2016/03/07/python-with-context-managers/\n",
    "- run_query, run_command, show_tables\n",
    "- NOT IN\n",
    "- EXCEPT (Compare two tables of values)\n",
    "\n",
    "## 12.6 - Create Tables and Modify Data: Basics\n",
    "- start sqlite3 on a shell\n",
    "- CREATE TABLE IF NOT EXISTS\n",
    "  - create table IF NOT EXISTS table1 (col1 integer primary key, col2 integer, FOREIGN KEY (col2) REFERENCES table2 (col1) )\n",
    "  - data types\n",
    "  - PRIMARY and FOREIGN KEYS\n",
    "- TABLE SCHEMA: PRAGMA table_info  \n",
    "- DROP TABLE IF EXISTS\n",
    "- Modify Data\n",
    "  - ALTER TABLE table table_name ADD COLUMN col_name\n",
    "  - INSERT OR IGNORE INTO\n",
    "  - INSERT INTO with python loop and template.format\n",
    "  - UPDATE table SET col=: change row values  \n",
    "- NaN\n",
    "- Dates\n",
    "  \n",
    "## 12.7 - Create Tables and Modify Data: Workflow\n",
    "- create table and incert values using place holders: query = \"INSERT INTO ceremonies (Year, Host) VALUES (?,?);\"\n",
    "- PRAGMA foreign_keys = ON\n",
    "- change table: impossible in SQLite3, create new and adapt\n",
    "- Create joint table\n",
    "  \n",
    "## 12.8 - DataBase Normalization\n",
    "\n",
    "## 12.9 - Guided Project: Designing and Creating a Database\n",
    "- database design: https://www.dbdesigner.net/\n",
    "\n",
    "## 12.10 - introduction to indexes\n",
    "- Explain Query\n",
    "- Indexes (search on indexed column to speed up query with binnary search)\n",
    "- CREATE INDEX IF NOT EXISTS area_idx ON facts(area);\n",
    "\n",
    "## 12.11 - Multi-column indexing\n",
    "- Multi column index: CREATE INDEX IF NOT EXISTS index_col1_col2 on table(col1, col2)\n",
    "- Covering index (all information asked in a query is contained in an index, very fast if you ask just for the index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 13 - Postgres\n",
    "\n",
    "## 13.1 - Using Postgres\n",
    "- SQLite pros and cons\n",
    "  - pros: single file(portable), run uinside python (no additional software), has most SQL commands\n",
    "  - conns: 1 process at a time, no perfomance features, no built-in security (different access level for different users)\n",
    "- PostgreSQL info: Server, client and ports\n",
    "- psycopg2 info: Multiple databases and users (default: \"dbname=postgres user=postgres\"), default port 5432 on current computer (server)\n",
    "- transactions: changes only made after commit or rollback for a transaction block\n",
    "- autocommit: conn.autocommit=True\n",
    "- CREATE DATABASE notes OWNER postgres\n",
    "\n",
    "## 13.2 - psql (Command line PostgreSQL)\n",
    "- Run queries, Manage users and permissions, Manage databases, See PostgreSQL system information.\n",
    "- special commands: \\l (list databases), \\dt (list tables in current database), \\du (list users)\n",
    "- connect to a different database: psql -d dataquest\n",
    "- Create users: CREATE ROLE userName WITH CREATEDB LOGIN PASSWORD 'password';\n",
    "- Permissions: \n",
    "  - GRANT SELECT, INSERT, UPDATE, DELETE ON tableName TO userName;\n",
    "  - GRANT ALL PRIVILEGES ON tableName TO userName; \n",
    "  - check permissions \\dp tableName\n",
    "- Revoke permissions:\n",
    "  - REVOKE SELECT, INSERT, UPDATE, DELETE ON tableName FROM userName;\n",
    "  - REVOKE ALL PRIVILEGES ON tableName FROM userName;\n",
    "- Super user:\n",
    "  - CREATE ROLE userName WITH LOGIN PASSWORD 'password' SUPERUSER;\n",
    "\n",
    "## 12.3 - Install Postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 14 - Statistics Fundamentals\n",
    "\n",
    "## 14.1 - Sampling\n",
    "- population, sample, parameter, statistic, sampling error, descriptive statistics vs inferential statistics \n",
    "- pandas simple random sampling: sample = df['col1'].sample(n=30, random_state = 1)\n",
    "- random sample using import random: sample = random.sample(population, N_sample)\n",
    "- random sample using import random: sample = random.sample(population, N_sample)\n",
    "- stratified sampling\n",
    "- Proportionate Stratified Random Sampling\n",
    "- Guidelines for choosing good strata\n",
    "  - Minimize the variability within each stratum.\n",
    "  - Maximize the variability between strata.\n",
    "  - The stratification criterion should be strongly correlated with the property you're trying to measure.\n",
    "- Cluster Sampling\n",
    "\n",
    "## 14.2 - Variables in Statistics\n",
    "- qualitative: nominal\n",
    "- quantitative: ordinal, interval, ratio\n",
    "\n",
    "## 14.3 - Frequency Distributions\n",
    "- frequency distribution table: .value_counts()\n",
    "- .value_counts().sort_index()\n",
    "- sort frequency table for ordinal scale: .value_counts().iloc[[4,3,0,2,1,5]]\n",
    "- normalized frequency table: df['col1'].value_counts(normalize=True).sort_index()*100\n",
    "- percentileofscore (to get percentile for a given value)\n",
    "- get averages and percentiles:\n",
    "  - .describe()\n",
    "  - .describe(percentiles = [.95]).iloc[3:])\n",
    "- .value_counts(bins = 10) (10 is good balance betwwen information and comprehensibility)\n",
    "- frequency table with costumized interval range\n",
    "  - intervals = [x for x in np.arange(0, 600, 60)]\n",
    "  - df['col1'].value_counts(bins = intervals, normalize=True)\n",
    "  - for idx, element in grouped_freq_table.iteritems(): print(\"[%d, %d]: %0.2f\" %(idx, idx+bin_size, element))\n",
    "\n",
    "## 14.4 - Visualizing Frequency Distributions\n",
    "- bar plot for nominal or ordinal variables\n",
    "- horizontal bar plot to read labels better\n",
    "- Pie chart\n",
    "  - df['col1'].value_counts().plot.pie(title=tittle, figsize = (6,6), autopct = '%.1f%%')\n",
    "  - last two \"%\" ito include the % symbol\n",
    "- Histogram (visual form of a frequency table)\n",
    "- Skew: left or right skewed\n",
    "  - from scipy.stats import skew\n",
    "- Kurtosis: whether the distribution is short and flat, or tall and skinny\n",
    "  - from scipy.stats import kurtosis\n",
    "- central tendency measures: how likely the data points are to cluster around a central value\n",
    "  - mean and media\n",
    "  \n",
    "\n",
    "## 14.5 - Visualizing Multiple Frequency Distributions\n",
    "- seaborn countplot (multiple frequency distributions)\n",
    "- seaborn kde plot (smooth histograms)\n",
    "- seaborn stripplot\n",
    "- seaborn boxplot\n",
    "- remove outliers: if >upper_quartile + 1.5*interquartile_range or <lower_quartile + 1.5*interquartile_range\n",
    "- change style: plt.style.use('ggplot'): https://tonysyu.github.io/raw_content/matplotlib-style-gallery/gallery.html\n",
    "- plt.rcParams.update(plt.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 15 - Statistics Intermediate: Averages and Variability\n",
    "\n",
    "## 15.1 - The Mean\n",
    "- sampling_error = $\\mu$ - $\\overline{x}$\n",
    "- unbiased estimator: sample mean $\\overline{x}$ is an unbiased estimator for the population mean $\\mu$\n",
    "\n",
    "## 15.2 - The Weighted Mean and the Median\n",
    "- weighted mean: np.average(df['col1'], weights=df['col2'])\n",
    "- Median (very useful in open ended distributions)\n",
    "  - Series.median()\n",
    "- mean and median for ordinal values  \n",
    "  \n",
    "## 15.3 - The Mode\n",
    "- the most frequent value on a distribution: for nominal variables, ordinal variables with words and some times discrete variables\n",
    "- continuous variables: create frequency table, get the interval with highest frequency\n",
    "- mean, median and mode position in skewed distributions\n",
    "\n",
    "## 15.4 - Measures of Variability\n",
    "- mean absolute deviation (mad)\n",
    "- $\\sigma^2 = \\frac{\\sum^{N}_{i=1}(x_i-\\mu)^2}{N}$ (variance, mean squared distance)\n",
    "- $\\sigma = \\sqrt{\\frac{\\sum^{N}_{i=1}(x_i-\\mu)^2}{N}}$ (standard deviation, std): np.std()\n",
    "- s (standard deviation sample)\n",
    "  - the standard deviation of the sample normally underestimates that of the population\n",
    "- Bessel's correction $s = \\sqrt{\\frac{\\sum^{n}_{i=1}(x_i-\\overline{x})^2}{n-1}}$\n",
    "- np.std(sample, ddof=1)\n",
    "- df[\"col1\"].std()\n",
    "\n",
    "\n",
    "## 15.5 - Z-scores\n",
    "- z-score (population and sample):\n",
    "  - standard deviations away: $z=\\frac{x-\\mu}{\\sigma}$\n",
    "- standard distribution: convert variable to z-scores to get a distribution with mean=0 and s=1\n",
    "  - Series.apply(lambda x: ((x - mean) / st_dev))\n",
    "  - if the sample formula is used to standardize the distribution then the sample form must be used in the standardized_sample to get std=1\n",
    "  - standardize distribution are very useful to compare two distributions with different scales\n",
    "  - convert back to more intuitive values using: $x = z\\sigma + \\mu$\n",
    "\n",
    "## 15.6 - Create a new DF from a frequency distribution\n",
    "- df1 = df['col1'].value_counts().reset_index().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 16 - Statistics Intermediate: Simple Correlation and Regression\n",
    "\n",
    "## 16.1 - Standard Deviation And Correlation\n",
    "- normal distribution\n",
    "  - from scipy.stats import norm\n",
    "  - points = np.arange(-1, 1, 0.01)\n",
    "  - dist = norm.pdf(points, 0, .3)\n",
    "- Pearson's r (r-value): values range from -1 to 1\n",
    "  - from scipy.stats.stats import pearsonr\n",
    "  - r, p_value = pearsonr(Series_1, Series_2)\n",
    "- Co-variance: $cov(x,y) = \\sum\\limits_{i=1}^n \\frac{(x_i-\\mu_x)(y_i-\\mu_y)}{n}$\n",
    "  - cov = np.cov(df[\"col1\"], df[\"col2\"])[0,1]\n",
    "- Correlation Coefficient: $r = \\frac{cov(x,y)}{\\sigma_x \\sigma_y}$\n",
    "\n",
    "## 16.2 - Linear regression\n",
    "- slope: $\\frac{cov(x,y)}{\\sigma_x^2}$\n",
    "- Intercept: $\\overline{y}−m\\overline{x}$\n",
    "- predict: a*x+b\n",
    "- rss, error estimate: $\\sum\\limits_{i=1}^n (y_i−\\hat{y}_i)^2$ (distance between each prediction and the actual value: residuals)\n",
    "- linregress and predictions\n",
    "  - from scipy.stats import linregress\n",
    "  - slope, intercept, r_value, p_value, stderr_slope = linregress(df[\"col1\"], df[\"col2\"])\n",
    "- standard error: $\\sqrt{\\frac{RSS}{n-2}}$\n",
    "\n",
    "## 16.3 - Significance of a variation\n",
    "- In order to test for significance, we check what fraction of the samples had a fraction >= the value after the experiment, before the experiment.\n",
    "\n",
    "\n",
    "## 16.4 - Probabiblity in rooling a dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 17 - Probability and Significance Testing\n",
    "\n",
    "## 17.1 - Intro to Probability\n",
    "- basic probability: N_favorable_cases/N_possible_cases\n",
    "- conjunctive probability: probability of a sequence of events. If independet P = P1*P2*Pn\n",
    "- Disjunctive Probabilities: if independent, prob(A or B) = PA+PB\n",
    "- Disjunctive not independent, Conditional probability: prob(A or B) = prob(A) + prob(B) - prob(A & B)\n",
    "- Disjunctive probabilities, not independent, many conditions: P(A or B or C) = 1 - P(!A and !B and !C)\n",
    "\n",
    "## 17.2 - Disjunctive probabilities, independent, many conditions: Combinations\n",
    "- Probability = $p^k \\times q^{N−k} \\times \\frac{N!}{k!(N-k)!}$\n",
    "\n",
    "## 17.3 - Probability and Significance\n",
    "\n",
    "## 17.4 - Probability distributions\n",
    "- Probability distribution: given N events, it plots the probabilities of getting different numbers of successful outcomes.\n",
    "- binom.pmf function from SciPy: dist = binom.pmf(outcome_counts, N, p)\n",
    "- expected value of normal probability distribution: $N \\times p$\n",
    "- standard deviation: $ \\sqrt{N \\times p \\times q}$\n",
    "- cumulative density function, probability that k or less witll occur: binom.cdf(outcome_counts, N, p)\n",
    "- How likely is an outcome from z-scores\n",
    "- How likely is an outcome from CFD\n",
    "\n",
    "## 17.5 - Significance Testing\n",
    "- test statistic: example difference between means\n",
    "- statistical test: permutation test and p-value(rerun study with each data point randomly reassigned to either group A or B)\n",
    "  - p_value = np.sum(frequencies)/N_samples\n",
    "  - threshold normally 5%\n",
    "  - always check if a small change in the theshold changes conclusions\n",
    "\n",
    "## 17.6 - Chi-squared tests\n",
    "- Chi-squared test for categorical values: $\\sum \\frac{(obs-exp)^2}{exp}$\n",
    "- scipy.stats import chisquare: chisq, pvalue = chisquare(observed, expected)\n",
    "\n",
    "## 17.7 - Chi-squared to check correlation between variables\n",
    "- contingency function: scipy.stats.chi2_contingency\n",
    "  - from scipy.stats import chi2_contingency\n",
    "  - observed = pd.crosstab(df['col1'],[df['col2']]) (frequency counts for multiple columns)\n",
    "  - chisq, pvalue, degrees_of_f, expected = chi2_contingency(observed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 18 - Calculus For Machine Learning\n",
    "- For most ML techniques, the model is a mathematical function that aproximates how features are related to the target atribute\n",
    "\n",
    "## 18.1 - Understanding Linear and Nonlinear Functions\n",
    "- linear functions\n",
    "- non linear functions\n",
    "- secant line: slope between any two points in a non linear curve\n",
    "- tangent: secant when x1=x2. Only intersects the line in one point\n",
    "\n",
    "## 18.2 - Understanding Limits (derivatives)\n",
    "- even though slope is undefined if x1=x2, we can define it as x2 approaches x1 as a limit\n",
    "- defined limit: can be determined just by replacement\n",
    "- undefined limit: can be solved by converting to defined limit and then replacemente\n",
    "- sympy: to calculate limits\n",
    "  - use sympy.symbols('x y') to declare variables\n",
    "  - sympy.limit(function, input variable, value input vriable approaches)\n",
    "- Properties of limits\n",
    "  - $\\lim_{x\\to a}[f(x)+g(x)] = \\lim_{x\\to a}f(x) + \\lim_{x\\to a}g(x)$\n",
    "  - $\\lim_{x\\to a}[f(x)-g(x)] = \\lim_{x\\to a}f(x) - \\lim_{x\\to a}g(x)$\n",
    "  - $\\lim_{x\\to a}[cf(x] = c\\lim_{x\\to a}f(x)$\n",
    "\n",
    "## 18.3 - Finding Extreme Points\n",
    "- Derivative: slope at different points, $\\lim_{h\\to 0}\\frac{f(x+h)-f(x)}{h}$\n",
    "- Differentiation: process of finding derivative \n",
    "- Critical point = derivative = 0 \n",
    "- Critical points can be minimum or maximum extreme values if the slope changes from positive to negative (or vice versa)\n",
    "- Differentiation Rules\n",
    "  - $\\frac{d}{dx}f(x) = rx^{r-1}$\n",
    "  - $\\frac{d}{dx}[f(x)+g(x)] = \\frac{d}{dx}[f(x)] + \\frac{d}{dx}[g(x)]$\n",
    "  - $\\frac{d}{dx}[cf(x)] = c\\frac{d}{dx}[f(x)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 19 - Linear Algebra For Machine Learning\n",
    "## 19.1 - Linear Systems\n",
    "- Solving by elimination: solve for x in one equation, replace x in the other equation \n",
    "- Gaussian Elimination, solving using Linear Algebra (Matrixes): represent the coefficients in linear systems using matrixes\n",
    "- general form in linear algebra: $Ax + By = c$\n",
    "- functions: \n",
    "  - $30x-y=-1000$ \n",
    "  - $50x-y=-100$ \n",
    "  - $\\left[\n",
    "\\begin{array}{cc|c}\n",
    "30 & -1 & -1000 \\\\\n",
    "50 & -1 & -100 \\\\\n",
    "\\end{array}\n",
    "\\right]$\n",
    "- Augmented Matrix: np.asarray()\n",
    "- So solve the systems of equations we need to transform the matrix into diagonal form \n",
    "- Row operations:\n",
    "  - swap second row (at index value 1) with third row (at index value 2): matrix[[1,2]] = matrix[[2,1]]\n",
    "  - multiply second row by 2: matrix[1]=2*matrix[1]\n",
    "  - add third to second row: matrix[1] = matrix[1]+matrix[2]\n",
    "- Echelon form\n",
    "- Reduced Echelon form\n",
    "\n",
    "## 19.2 - Vectors\n",
    "- We can have row vectors or column vectors. Normally vector refer to column vectors (list of numbers in single column)\n",
    "- Visualize vectors: plt.quiver\n",
    "- Adding vectors: elements in the same position are added and new vector created\n",
    "- Multiply by scalar\n",
    "- np.asarray can also represent vectors\n",
    "- Dot product (vectors with same number of elements): np.dot(vector_one[:,0], vector_two)\n",
    "- Can a vector be obtain by combining others? Is V0 a linear combination of $c_1*V_1 + c_2*V_2$\n",
    "- Linear combination of vectors and augmented matrixes\n",
    "- Matrix equation\n",
    "\n",
    "\n",
    "## 19.3 - Matrix Algebra\n",
    "- Sum two matrixes with same number of rows and columns\n",
    "- Multiply by scalar\n",
    "- Matrix multiplication: number of columns in first must match number of rows in second\n",
    "- np.dot for matrix multiplication\n",
    "- Transpose: switches rows and colums of matrix \n",
    "  - $A^T + B^T = C$\n",
    "  - $(A+B)^T = A^T + B^T$\n",
    "  - $(AB)^T = B^TA^T$\n",
    "- Identity matrix: np.identity(N)\n",
    "- solving equations with matrixes\n",
    "  - $A \\overrightarrow{x} = \\overrightarrow{b}$\n",
    "  - $A^{-1} A \\overrightarrow{x} = A^{-1}\\overrightarrow{b}$\n",
    "  - $\\overrightarrow{x} = A^{-1}\\overrightarrow{b}$\n",
    "- Inverse of a matrix (in python np.linalg.inv())\n",
    "  - If $A = \\left[\\begin{array}{cc} a & b \\\\ c & d \\\\ \\end{array}\\right] $ then $A^{-1} = \\frac{1}{ad-bc}\\left[\\begin{array}{cc} d & -b \\\\ -c & a \\\\ \\end{array}\\right]$\n",
    "  - the term $ad-bc$ is the determinant and the matric is only invertible if it is not equal to zero\n",
    "- determinant in python: np.linalg.det()\n",
    "\n",
    "## 19.4 - Solution Sets\n",
    "- Soliving Equations previous chapters:\n",
    "  - Elimination: solve for x in one equation, replace x in the other equation \n",
    "  - Gaussian Elimination: Use matrix operations to transform system into Reduced Echelon form\n",
    "  - $\\overrightarrow{x} = A^{-1}\\overrightarrow{b}$\n",
    "- What to do when:\n",
    "  - there is no solution to the linear system (inconsistent system)\n",
    "    - if $det = 0$  \n",
    "  - b is equal to $\\overrightarrow{0}$ (homogeneous system)\n",
    "    - always have a solution: the zero vector, trivial solution\n",
    "    - then we need to find if there are also infinitely many solutions\n",
    "  - the solution isn't a single vector\n",
    "- Square vs Rectangular:\n",
    "  - When A is a square matrix we can compute the determinant to determine if there is a solution ($det \\neq 0$). Very expensive for large matrixes, very useful for small linear systems.\n",
    "  - When A is rectangular we can use Gaussian elimination to find if it has 0 or infinite solutions (if the latter express in parametric vector form).\n",
    "- Nonhomogeneous vs Homogeneous:\n",
    "  - When A is nonhomogeneous the system can have 0, 1 or infinite solutions. If nonhomogeneous and rectangular it cannot contain just 1 solution (Number of variables > Number of Equations).\n",
    "  - When A is homogeneous there is always the trivial solution. Use Gaussian Elimination to determine if there is a solution space or if equations are inconsistent (if the former express solution in parametric form using the free variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 20 - Machine Learning Fundamentals (Basic Workflow with KNN)\n",
    "- basic workflow:\n",
    "  - select model\n",
    "  - chose features\n",
    "  - chose error metric for evaluation\n",
    "  - hyperparameter search\n",
    "  - perform cross validation: to understand performance of given model (with given features and hyperparamters)\n",
    "  - STD of RMSE is a proxy for variance, while AVG RMSE is a proxy for bias\n",
    "- Classification trees have dependent variables that are categorical and unordered. Classification means to group the output into a class.\n",
    "- Regression trees have dependent variables that are continuous values or ordered whole values. Regression means to predict the output value using training data. \n",
    "  \n",
    "## 20.1 - Introduction to K-Nearest Neighbors\n",
    "- Expensive because to predict for a new observation we need to compute the distance to everypoint in the training set\n",
    "- Instance based learning: relies on previous instances to make predictions\n",
    "- Euclidean distance: $d = \\sqrt{(q_1-p_1)^2+(q_2-p_2)^2+...+(q_n-p_n)^2}$\n",
    "- need to randomize ordering of dataframe (if we are selecting the closest N neighbours and there are many more rows with the same value for the \"similarity stat\")\n",
    "  - np.random.seed(1)\n",
    "  - df = df.iloc[np.random.permutation(len(df))]\n",
    "\n",
    "  \n",
    "## 20.2 - Evaluating Model Performance (Train/Test validation)\n",
    "- train/test validation\n",
    "- Error metric: quantify how good the predictions are  (how far predictions are from the true values)\n",
    "- mean absolute error: $MAE=\\frac{|actual_1-pred_1|+|actual_2-pred_2|+...+|actual_n-pred_n|}{n}$\n",
    "- mean squared error: $MSE=\\frac{(actual_1-pred_1)^2+(actual_2-pred_2)^2+...+(actual_n-pred_n)^2}{n}$\n",
    "- Root mean squared error: $RMSE = \\sqrt{MSE}$\n",
    "- ratio between MAE and RMSE to look for large but infrequent errors\n",
    " \n",
    "\n",
    "## 20.3 - Multivariate K-Nearest Neighbors\n",
    "- sklearn library workflow: \n",
    "  - instantiate a ML model\n",
    "  - fit the model to the train set\n",
    "  - use model to make predictions\n",
    "  - evaluate accuracy of predictions\n",
    "- Classes of ML models:\n",
    "  - Regression model: predict numerical values\n",
    "  - Classification model: predict a label from a set of labels\n",
    "- increase the number of attributes to calculate similarity between observations\n",
    " - avoid features with: non numerical values, missing values, non ordinal values\n",
    "- DataFrame.info() returns number of non-null values in each column\n",
    "- Normalizing columns: gaussian distribution mean 0, std 1: $x = \\frac{x-\\mu}{\\sigma}$\n",
    "- Euclidean Distance function: \n",
    "  - from scipy.spatial import distance\n",
    "  - distance.euclidean(list_1,list_2)\n",
    "- kNeighborsRegressor instantiate:\n",
    "  - default: n_neighbors = 5, algorithm = auto (nearest neighbors), p = 2 (euclidean distance)\n",
    "- kNeighborsRegressor fit and kNeighborsRegressor predict\n",
    "- MSE from sklearn: takes in true values + predicted values from the model  \n",
    "  - from sklearn.metrics import mean_squared_error\n",
    "  - mse = mean_squared_error(test_df['target'], predictions)\n",
    "- RSME = np.sqrt(mse)\n",
    "- full workflow for all features (except price)\n",
    "  - features = train_df.columns.tolist()\n",
    "  - features.remove('target') \n",
    "- All features in not necessary better, FEATURE ENGENEERING IS CRUCIAL\n",
    "- Bias and Variance\n",
    "  - bias: errors resulting from assumptions in th elearning algorithm (e.g. wrong features)\n",
    "  - variance: errors due to the variability of a model predicted values\n",
    "  - STD of RMSE is a proxy for variance, while AVG RMSE is a proxy for bias\n",
    "  - they keep implementing this on k-fold validation, when it should be implemented on model selection (look at section 23.5)\n",
    "  \n",
    "## 20.4 - Hyperparameter Optimization\n",
    "- hyperparameters: values that affect model behaviour and perfomance that are unrelated to the data\n",
    "- Grid search for hyperparameter optimization: \n",
    "  - select subset of paramters values, train model with each parameter, evaluate performance, select parameter that gives lowest error\n",
    "- workflow to find best model:\n",
    "  - select features\n",
    "  - grid search optimization of hyperparameters\n",
    "  - evaluate accuracy\n",
    "\n",
    "## 20.5 - Cross Validation (all the stuff with bias and variance here is rubbish, look at section 22.5)\n",
    "- Holdout Validation\n",
    "  - train/test validation with 2 random halves\n",
    "  - switch train and test, perform validation\n",
    "  - average errors\n",
    "- K-fold validation\n",
    "  - split data set into k equal length partitions\n",
    "    - select k-1 partitions as training set\n",
    "    - the other as test\n",
    "  - train model, fit, predict and validate (compute error)\n",
    "  - repeat k-1 times\n",
    "  - calculate the mean error of the different iterations\n",
    "  - holdout validation is k-fold validation with k=2, normally 5 to 10 is used. \n",
    "  - large variation in RMSE means poor model/poor evaluation criteria (or both)\n",
    "- K-fold validation with sklearn kfold class\n",
    "  - kf = KFold(n_folds, shuffle=True, random_state=1)\n",
    "  - model = KNeighborsRegressor()\n",
    "  - mses = cross_val_score(model, df[['col1']], df[['col2']], scoring = 'neg_mean_squared_error', cv=kf)\n",
    "    - gives mses for all the different permutations of folds, average to get mse for current kfold number\n",
    "- chosing right kfold value is hard\n",
    "  - 10 is the most common (normally give the average RMSE of different kfold values)\n",
    "  - 2 is holdout, kfold=n is known as leave-one-out cross validation LOOCV\n",
    "\n",
    "## 20.6 - ML Workflow\n",
    "- Feature Engeneering: one feature at a time, different combinations of the best individual features\n",
    "- Hyperparameter Optimization for 3 best models \n",
    "- K-fold Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 21 - Linear Regression For Machine Learning\n",
    "## 21.1 - The Linear Regression Model\n",
    "- Non-Parametric, Instance based Machine Learning: relies on previous instances to make predictions\n",
    "- Parametric based Machine Learning: like linear and logistics regression the result of the training is a fitting formula or model\n",
    "- Linear regression, linear relation between features and targets: $y=a_1x1+a_2x_2+...+a_nx_n$\n",
    "- Simple Linear Regression (Univariate): $\\hat{y} = a_1x_1+a_0$ ($y$ is the target, $x_1$ is the feature)\n",
    "  - Basic feature selection checking correlations using scatter plots\n",
    "  - Find most correlated variables using df.corr()\n",
    "  - Model fitting: residual sum of squares (RSS)\n",
    "    - $RSS = \\sum_{i=1}^n(y_i-\\hat{y_i})^2$\n",
    "- Simple Linear Regression with scikit-learn  \n",
    "  - Predict, quantify the fit using RMSE, do simple train/test validation\n",
    "- Multiple Linear Regression (multivariate case, feature selection is essential)\n",
    "\n",
    "## 21.2 - Feature Selection\n",
    "- Use correlations between features and target, correlations between features, and variance of features\n",
    "- Feature Selection using df.corr(): first look just at the correlation between all features and target\n",
    "- Collinearity with sns.heatmap (identify highly correlated variables to eliminate)\n",
    "- Remove features with low variance (on the extreme case of 0 variance, all values for that feature are the same and meaningless)\n",
    "  - standardize columns to vary between 0 and 1 (norm_train = train[features]/train[features].max())\n",
    "  - sorted_vars = norm_train.var().sort_values()\n",
    "\n",
    "## 21.3 - Gradient Descent (Model fitting with)\n",
    "- Model fitting (optimization problem) technique to minimize: $MSE = \\frac{1}{n}\\sum_{i=1}^n(\\hat{y_i}-y_i)^2$    \n",
    "- Gradient Descent Algorithm: iteratively find the parameter values with the lowest MSE (also used for neural nets)\n",
    "  - select initial values of a parameters: $a_1$\n",
    "  - repeat until converge (max number of iterations):\n",
    "    - calculate MSE with current parameter value\n",
    "    - calculate derivative of MSE with current parameter value\n",
    "    - update parameter value with: $a_1 := a_1-\\alpha \\frac{d}{da_1}MSE(a_1)$ (the parameters $\\alpha$ is called the learning rate)\n",
    "  - Single parameter:\n",
    "    - $MSE(a_1) = \\frac{1}{n}\\sum_{i=1}^n(a_1x_1^i-y^i)^2$\n",
    "    - $\\frac{d}{da_1}MSE(a_1) = \\frac{d}{da_1} \\frac{1}{n}\\sum_{i=1}^n(a_1x_1^i-y^i)^2 = \\frac{1}{n}\\sum_{i=1}^n\\frac{d}{da_1}(a_1x_1^i-y^i)^2 = \\frac{1}{n}\\sum_{i=1}^n 2x_1^i(a_1x_1^i-y^i) $ \n",
    "    - for iteration in the gradient descent: compute derivative using $a_1$, update $a_1$\n",
    "  - 2 parameters:\n",
    "    - $MSE(a_0, a_1) = \\frac{1}{n}\\sum_{i=1}^n(a_0 + a_1x_1^i - y^i)^2$\n",
    "    - $\\frac{d}{da_1}MSE(a_0, a_1) = \\frac{2}{n}\\sum_{i=1}^n x_1^i(a_0 + a_1x_1^i-y^i) $ \n",
    "    - $\\frac{d}{da_0}MSE(a_0, a_1) = \\frac{2}{n}\\sum_{i=1}^n (a_0 + a_1x_1^i-y^i) $ \n",
    "    - $a_0 := a_0-\\alpha \\frac{d}{da_0}MSE(a_0, a_1)$\n",
    "    - $a_1 := a_1-\\alpha \\frac{d}{da_1}MSE(a_0, a_1)$ \n",
    "- Many Parameters\n",
    "\n",
    "## 21.4 - Ordinary Least Squares (OLS, multivariate linear regression, Model fitting with)\n",
    "- Using matrix notation:\n",
    "  - $Xa = \\hat{y}$\n",
    "  - optimal $a = (X^TX)^{-1}X^Ty$\n",
    " \n",
    "## 21.5 - Processing And Transforming Features (feature engineering)\n",
    "- Critical to gain knowledge on the domain\n",
    "- Domain independent strategies to address:\n",
    "  - non-numerical columns\n",
    "  - numerical but not ordinal\n",
    "  - numerical but not representative of the relationship with target columns\n",
    "- Deal with missing values:\n",
    "  - remove rows containing missing values\n",
    "    - pro: only clean data remains\n",
    "    - con: entire observation reduced with can reduce accuracy\n",
    "  - inputation: replace missing value with discriptive stat from that column\n",
    "    - pro: preserve the observation\n",
    "    - con: can add noise\n",
    "    - For inputation select columsn with less than 25% to 50% missing values\n",
    "- dummy coding (needed for regression problems)\n",
    "  - Transform string columns with only a few values to categorical features: use Series.astype('category') \n",
    "  - pd.get_dummies(df[cols]): to transform categorical variables to dummy columns and use linear regression\n",
    "  - the same dummy coding can be used to convert numerical columns with just a few unique values\n",
    "- Replace NaN in Numerical Columns with mean\n",
    "  - df.fillna(df.mean(), inplace=True)\n",
    "- replace NaN in object columns with mode\n",
    "  - df.fillna(df.mode().iloc[0], inplace=True)\n",
    "\n",
    "## 21.6 - ML Workflow (Predicting House Sale Prices)\n",
    "- Linear Regression Pipeline: Train -> Transform Features -> Select Features -> Train and Test\n",
    "- Features Engeneering:\n",
    "  - drop columns that leak information about target column\n",
    "  - Deal with missing values:\n",
    "    - remove columns with more than 25% missing values\n",
    "    - imputation: fillNaN with mean\n",
    "  - create new features by combining old ones\n",
    "  - DUMMY CODING (needed for regression problems)\n",
    "    - transform numerical to categorical (for those where numbers have no meaning)\n",
    "    - identify text columns to make categorical:\n",
    "      - only those with only a few unique values\n",
    "      - if they have only a few unique values, but more that 95% of the values are the same, this is a low variance feature\n",
    "      - deal with NaNs, then dummy code\n",
    "- Select Features:\n",
    "  - generate sns.heatmap to identify interesting features \n",
    "  - calculate correlations to check\n",
    "- Train and Test:\n",
    "  - implement simple cross validation\n",
    "    - train model on train and test on test\n",
    "    - train model on test and test on train\n",
    "  - implement k-fold cross valiation\n",
    "- Feature selection with SelectKBest\n",
    "- Feature Extraction with PCA (need to learn more about PCA in ML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 22 - Machine Learning Project\n",
    "\n",
    "## The models used here, logistics regression and random forests, are only described in subsequent sections\n",
    "\n",
    "## 22.1 - Machine Learning Project Walkthrough: Data Cleaning\n",
    "- identify columns that:\n",
    "  - leak information about the future (after he loan was funded)\n",
    "  - don't affect the borrower's ability to pay back the loan\n",
    "  - formatted poorly and need to be clean up\n",
    "  - require more data or a lot of processing\n",
    "  - contain redundant information\n",
    "- identify and clean up target column\n",
    "- class inbalance in the target column, keep in mind \n",
    "- find columns with only one unique value (after removing nan), drop them\n",
    " \n",
    "## 22.2 - Preparing the features\n",
    "- handle missing values and convert categorical columns to numeric (scikit-learn will return an error if columns contain such values when using linear or logistic regression)\n",
    "  - remove columns with more than 1% missing values, keep employment length as it is likely a good predictor\n",
    "  - drop rows with missing values\n",
    "  - identify numerical needing conversion and extraneous columns\n",
    "  - dummy coding\n",
    "  \n",
    "## 22.3 Making Predictions\n",
    "- Error Metric\n",
    "  - mostly concerned with false positives (cost money) and false negatives (loose potential money)\n",
    "  - since we are considering a conservative investor false positives are worse\n",
    "  - create filters to identify true/false positives/negatives\n",
    "  - because of class inalance a classifier can predict one ofr every row and still have high accuracy\n",
    "  - don't use accuracy and look instead at the number of false positives and negatives\n",
    "  - optimize for high recall (true positive rate) and low fall-out (false positive rate):\n",
    "    - $TPR=\\frac{\\mathrm{True Positives}}{\\mathrm{True Positives+False Negatives}}$\n",
    "- model\n",
    "  - A good first algorithm to apply to binary classification problems is logistic regression, for the following reasons:\n",
    "    - it's quick to train and we can iterate more quickly,\n",
    "    - it's less prone to overfitting than more complex models like decision trees,\n",
    "    - it's easy to interpret.  \n",
    "- test accuracy\n",
    "  - test the accuracy of the algorithm using cross validation to generate predictions \n",
    "    - train and test on different subsets of the data\n",
    "    - use scikit-lear cross_val_predict (pass in a classifier, features and target)\n",
    "    - create instance of Kfold, which will perform 3 fold cross validation\n",
    "    - pass Kfold into cross_val_predict\n",
    "- Correct for inbalance, 2 ways:\n",
    "  - Use oversampling and undersampling to ensure that the classifier gets input that has a balanced number of each class.\n",
    "    - take a sample that contains equal numbers of rows with the target equals 0 and 1\n",
    "    - down side is to throw away a lot of rows, or duplicate rows with lower representation\n",
    "  - Tell the classifier to penalize misclassifications of the less prevalent class more than the other class.\n",
    "    - easy to do with scikit-learn, class_weight=balanced when creating the logistic regression instance (penalty is inversily proportional to the target class frequencies)\n",
    "- we can set the penalty manually to try to lower the false positive rates (higher penalty for misclassifying the negative class)\n",
    "- Try random forest agorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 23 - Machine Learning in Python: Intermediate\n",
    "## 23.1 - Logistic Regression, Binary Classification (first model to try for discrete categorical variables)\n",
    "- Classification problems: ML with discrete target variables\n",
    "- Binary Classification: only two possible values for target\n",
    "- Logistic Regression: outputs a probability values. In binary classification if probability is higher than given threshold we we assign 1, otherwise 0\n",
    "- Logistic function: $\\sigma(t)=\\frac{e^t}{1+e^t}$\n",
    "  - exponential transformation $e^t$: making all values posetive\n",
    "  - normalization transformation $\\frac{x}{1+x}$: making all values range between 0 and 1\n",
    "- from sklearn.linear_model import LogisticRegression\n",
    "  - descrimination threshold (probability above which prediction=1) = 0.5 by default\n",
    "- pred_probs = logistic_model.predict_proba to return probability\n",
    "  - pred_probs[:,0]: probability that row should be 0\n",
    "  - pred_probs[:,1]: probability that row should be 1\n",
    "- logistic_model.predict\n",
    "\n",
    "## 23.2 - Introduction to evaluating binary classifiers \n",
    "- Accuracy - What fraction of the predictions was correct (Simplest model test)\n",
    "  - $Accuracy = \\frac{N correctly \\, predicted}{N \\, observations}$\n",
    "  - Doesn't tells us how the model perfoms in data that it has not been trained on.\n",
    "  - Doesn't descriminate between classification outcomes\n",
    "- Classification outcomes: true positives, false positives, true negatives, false negatives\n",
    "<br>\n",
    "- Sensitivity (True positice rate): $TPR = \\frac{True\\,Positives}{True\\,Positives+False\\,Negatives}$\n",
    "  - of all the students that should have been adimited (True Positives+False negatives) what fraction did the model got right (True Positives)\n",
    "  - How effective is the model at identifying positive outcomes\n",
    "  - in some problems high sensitivity is key (e.g. detecting deseases, catch all positive cases, 1-sensitivity would be the number of undetected hillnesses)\n",
    "- Specificity (True Negative rate): $TNR = \\frac{True\\,Negatives}{False\\,Positives+True\\,Negatives}$\n",
    "  - of all the students that should have been rejected (True Negatives+False Positives) what fraction did the model got right (True Negatives)\n",
    "  - How effective is the model at identifying negative outcomes\n",
    "\n",
    "## 23.3 - One-versus-all Method, Multiclass Classification (discrete categorical variables, N labels > 2)\n",
    "- Discrete categorical variables with more than 2 outcomes\n",
    "- Create dummys for categorical columns with a prefix:\n",
    "  - dummy_col1 = pd.get_dummies(df[\"col1\"], prefix=\"cyl\")\n",
    "  - df = pd.concat([df, dummy_col1], axis=1)\n",
    "- One versus all method: choose one category as the positive case and group all others in the False case \n",
    "  - convert the problem into n binary classification problems \n",
    "      \n",
    "## 23.4 - Intermediate linear regression\n",
    "- A linear regression model is defined has:\n",
    "  - $y_i = \\beta_0+\\beta_1x_i+e_i$ where $e_i \\sim N(0,\\sigma^2)$ is the error term for each observation (betas are the slope and intercept)\n",
    "  - the residual for the prediction of observation i is $e_i = \\hat{y}_i-y_i$ ($\\hat{y}_i$ is the prediction)\n",
    "  - $N(0,\\sigma^2)$ is a normal distribution with mean 0 and variance $\\sigma^2$ (errors are normally distributed with 0 average)\n",
    "  - $\\hat{\\beta_i}$ are estimated coefficient, $\\beta_i$ are true coefficients\n",
    "  - estimated model: $\\hat{y}_i = \\hat{\\beta}_0+\\hat{\\beta}_1x_i+e_i$\n",
    "- Statsmodels library:\n",
    "   - sm.OLS: ordinary least squares (doesn't automatically include an intercept) \n",
    "- Evaluation of Linear Regression Models:\n",
    "  - Sum of Square Error (SSE) = $\\sum_{i=1}^n(y_i-\\hat{y}_i)^2=\\sum_{i=1}^ne_i^2$ (sum of all residuals)\n",
    "  - Regression Sum of Squares (RSS) = $\\sum_{i=1}^n(\\overline{y}-\\hat{y}_i)^2$ where $\\overline{y}=\\frac{1}{n}\\sum_{i=1}^ny_i$ (measures amount of explained variance, 0 if all predictions=mean)\n",
    "  - Total Sum of Squares (TSS) = $\\sum_{i=1}^n(y_i-\\overline{y})^2$ (measures total amount of variation in the data)\n",
    "  - TSS = RSS+SSE\n",
    "- R-Squared (best-single measure ofr linear relations, coefficient of determination): percentage of variation in the target variable present in our model\n",
    "  - $R^2 = 1-\\frac{SSE}{TSS}=\\frac{RSS}{TSS}$\n",
    "- print(linearfit.summary())\n",
    "- print(linearfit.params)\n",
    "- Variance of Coefficients: expected range in predicted coefficients\n",
    "  - estimated variance for a linear model with 1 variable: $s^2\\hat{\\beta_1}=\\frac{\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{(n-2)\\sum_{i=1}^n(x_i-\\bar{x})^2}=\\frac{SSE}{(n-2)\\sum_{i=1}^n(x_i-\\bar{x})^2}$\n",
    "  - the numerator represents the error within the model, the numerator measures the amount of variance within x (n-2 for two degrees of freedom)\n",
    "- Student t-test is a common statistical tests to understand if y depends on x. \n",
    "  - t-distribution takes into account finite samples (increased variance in smaller samples)\n",
    "  - approaches gaussian distribution for large samples (has a lower peak for small samples)\n",
    "  - pdf: models the likelihood of a continuous random variable and are used for significance testing\n",
    "  - cdf: models the probability of a random variable being less or equal to a point\n",
    "  - degrees of freedom: number of observations\n",
    "  - from scipy.stats import t, print(t.pdf(x=x, df=3))\n",
    "- significance testing:\n",
    "  - null hypothesis: lean doesn't depend on year, coefficient equal 0 \n",
    "  - $H_0:\\beta_1=0 \\; H_1:\\beta_1 \\neq0$\n",
    "  - test null hypothesis: $t=\\frac{|\\hat{\\beta_1}-0|}{\\sqrt{s^2(\\hat{\\beta_1})}}$\n",
    "    - how many standard deviations the coefficient is from 0, if $\\beat_1$ is far from 0 with low variance t is high (from the plot a t-stat far from 0 has very low probability)\n",
    "    - tstat = linearfit.params[\"year\"]/np.sqrt(s2b1)\n",
    "- test our t-stat: 95% confidence level that $\\beta_1$ is not 0. Compute cdf at the given p-value (95%) and degrees of freedom and compute probability\n",
    "- since t-distribution is symmetrical we can take |t-stat| and look at the 97.5 percentile\n",
    "- if $Tcdf(|t|,df)<0.975$: accept $H_0: \\beta_1=0$, else: accept $H_1$\n",
    "\n",
    "## 23.5 - Overfitting (bias vs variance)\n",
    "- bias: errors resulting from the learning lagorithm (for e.g. using a single variable)\n",
    "- variance: errors due to the variability of model's predicted values (e.g. use all variables, low bias high variance)\n",
    "- Overfitting: perform well on traing set, bad on new data\n",
    "- calculate bias: trainning different models from the same class using different features and calculate errors (with linear regression, MAE, MSE or R-squared)\n",
    "- calculate variance: we'll see and increase in variance as we build more complex, multi-variate models\n",
    "- detect overfitting: compare **in-sample error** and **out-of-sample error**, or **trainning** with **test** error.\n",
    "- use cross validation to check out-of-sample error\n",
    "- if cross validation error is much larger than in-sample error - overfitting\n",
    "- kfold validation on a linear regression model\n",
    "\n",
    "## 23.6 - Clustering Basics\n",
    "- first explore the data with unsupervised learning, then predict with supervised learning\n",
    "- supervised learning: predicting target from known variables, regressiona and classification\n",
    "  - good when there is a clear metric and we want to optimize lots of pre labelled data\n",
    "- unsupervised learning: finding patterns in data, clustering\n",
    "  - when there is no clear value to optimize\n",
    "- Clustering: group similar row together to find new patterns in data\n",
    "- k-means clustering - clustering using euclidean distance\n",
    "  - group similar rows in clusters\n",
    "  - each cluster is assigned a center and the distance is between each row and the center calculated\n",
    "  - specify number of clusters upfront\n",
    "  - use kmeans (use all sample since we are not predicting and there is no risk of overfitting)\n",
    "  - kmeans_model = KMeans(n_clusters=2, random_state=1)\n",
    "  - distances = kmeans_model.fit_transform(df.iloc[:,:])\n",
    "- use crosstab to find the clusters:\n",
    "  - labels = kmeans_model.labels_\n",
    "  - pd.crosstab(labels,df['party'])\n",
    "- find rows that belong to a given cluster: democratic_outliers = votes[(labels==1) & (votes['party']=='D')]\n",
    "- plot the distances to each cluster\n",
    "\n",
    "  \n",
    "## 23.7 - Centroid based clustering (K-means Clustering)\n",
    "- Centroid based clustering (K-Means Clustering)\n",
    "  - works well when cluster resemble circles\n",
    "  - The centroid is the arithmetic mean of all the data points in that cluster\n",
    "  - K-Means Clustering: k is the number of clusters (the key part is that we need to specify k)\n",
    "- clustering of players in the position point guards \n",
    "- should have large assist to turn over ratio: $ATR = \\frac{Assists}{Turnovers}$\n",
    "- plot ppg vs ATR to try to identify how many clusters there are  \n",
    "- setup up a k-means iterative algorithm that switches between calculating the centroid of each cluster and the players that belong to that cluster\n",
    "  - step 0: select 5 players at random and assign their coordinates as the initial centroids\n",
    "  - step 1 (assign point to cluster): for each player calculate the euc distance between their atr & ppg and each centroid. assign the player to the closest cluster\n",
    "  - step 2 (update centroids): for each cluster calculate the new centroid\n",
    "  - step 3: iterate through 1 and 2 until clusters no longer change\n",
    "- k-means sklearn\n",
    "  - k-means doesn't change the clusters a lot between iterations so it converges quickly\n",
    "  - the initial values we pick are crucial\n",
    "  - sklearn can mitigate this with intelligent algorithms (like re-running the clustering process lots of times at random starting points)\n",
    "  - from sklearn.cluster import KMeans\n",
    "  - kmeans = KMeans(n_clusters=num_clusters)\n",
    "  - kmeans.fit(point_guards[['ppg', 'atr']])\n",
    "  - point_guards['cluster'] = kmeans.labels_\n",
    "  \n",
    "## 23.8 - Gradient Descent\n",
    "- gradient descent vs linear regression\n",
    "  - http://sdsawtelle.github.io/blog/output/week2-andrew-ng-machine-learning-with-python.html\n",
    "- to predict accuracy using distance (we expect the further the distance, the smaller the accuracy)\n",
    "- First normalize data: x-mean/std\n",
    "- use a linear model: $accuracy = \\theta_1 distance_i+\\theta_0+\\epsilon_i$\n",
    "- perform linear regression\n",
    "- often we have too much data to fit into memory and can't use least squares\n",
    "- gradient descent can be used to estimate coefficients of any model\n",
    "  - it minimizes the residuals in the estimated model by updating each coefficient based on it's gradient\n",
    "  - cost function: measure the difference between model and observations \n",
    "  - for a model of the form: $h_{\\theta}(x)=\\theta_1x+\\theta_0$, cost function is $J(\\theta_0,\\theta_1)=\\frac{1}{2m}\\sum_{i=1}^m(h_{\\theta}(x_i)-y_i)^2$\n",
    "  - cost is one half the average differenc between predictions and observations squared\n",
    "  - during the modelling we randomly choose the coefficients and updated them to minimize the cost\n",
    "- 3D plot to show cost for $\\theta_0$ and $\\theta_1$\n",
    "- Gradient descent relies on finding the largest gradient of a multivariate function\n",
    "- take partial derivative in terms of each parameter in the cost function\n",
    "- $\\frac{\\delta J(\\theta_0, \\theta_1)}{\\delta \\theta_0}=\\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x_i)-y_i)$\n",
    "- $\\frac{\\delta J(\\theta_0, \\theta_1)}{\\delta \\theta_1}=\\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x_i)-y_i)*x_i$\n",
    "- Gradient Descent: find the optimal parameters that reduce the cost function\n",
    "  - randomly choose starting parameters and update them in the direction of the cost function's steepes slope (move towards global minimum)\n",
    "  - when paramters converge we are close to global minimum\n",
    "  - convergence: difference between the cost in previous and current iteration\n",
    "  - algorithm for 2 variables:\n",
    "    - $\\theta_1 := \\theta_1-\\alpha*\\frac{\\delta J(\\theta_0,\\theta_1)}{\\delta \\theta_1}$\n",
    "    - $\\theta_0 := \\theta_0-\\alpha*\\frac{\\delta J(\\theta_0,\\theta_1)}{\\delta \\theta_0}$\n",
    "    - generaly the learning rate $\\alpha$ will be between 0.0001 and 1 (too small takes too long, too large it overshoots the minimum)\n",
    "    \n",
    "## 23.9 - Introduction to neural networks (scikit example in Project_235)\n",
    "- Neural Networks: class of models to learn about non-linear relations btween variables\n",
    "- So far we only had models that do not allow for a lot of non-linearity.\n",
    "- Neural Networs are loosely inspired by the structure of neurons in the brain\n",
    "  - NN models are built using a series of activation units, neurons, to make predictions of some outcome\n",
    "  - take in input, apply transformation function, return and output\n",
    "- a neuron takes in some units (a bias unit and features)\n",
    "- the bias unit is similar in concept to the intercept in linear regression\n",
    "- these units are then fed into an activation function h \n",
    "  - sigmoid (logistic) activation function is a popular choice because it returns values between 0 and 1 that can be used as probabilities\n",
    "  - Sigmoid Function: $g(z)=\\frac{1}{1+e^{-z}}$\n",
    "  - Sigmoid Activation Function: $h_{\\Theta}(x)=\\frac{1}{1+e^{-\\Theta^T x}}=\\frac{1}{1+e^{-(\\theta_0+\\theta_1x_1+\\theta_2x_2)}}$\n",
    "- we can train a single neuron as a two layer network using gradient descent\n",
    "  - cost function -- $J(\\Theta)=-\\frac{1}{m}\\sum_{i=1}^m(y_i*\\log(h_{\\Theta}(x_i))+(1-y_i)\\log(1-h_{\\Theta}(x_i)))$\n",
    "  - since our targets, $y_i$, are binary, either $y_i$ or $(1-y_i)$ will equal zero (one of the terms in the sum will disappear)\n",
    "  - if we observe a true target, $y_i=1$, we want $h_{\\Theta}(x_i)$ to also be close to 1. So as $h_{\\Theta}(x_i)$ approaches 1, $\\log(h_{\\Theta}(x_i))$ approaches 0\n",
    "  - since the log of a value between 0 and 1 is negative we need to take the negative of the sum to compute the cost - We need partial derivatives to get the gradients:\n",
    "  - $\\frac{\\delta J}{\\delta \\theta_j}=\\frac{\\delta J}{\\delta h(\\Theta)}\\frac{\\delta h(\\Theta)}{\\delta  \\theta_j}$\n",
    "  - the first part of the function is computing the error between target and prediction, the second computes the sensitivity relative to each parameter\n",
    "- gradients:\n",
    "  - $\\delta = (y_i-h_{\\Theta}(x_i))*h_{\\Theta}(x_i)*(1-h_{\\Theta}(x_i))*x_i$\n",
    "  - $(y_i-h_{\\Theta}(x_i))$ is a scalar and the error between target and prediction\n",
    "  - $h_{\\Theta}(x_i)*(1-h_{\\Theta}(x_i))$ is also a sclar nad the sensitivity of the activation function\n",
    "  - $x_i$ are the features from our observation\n",
    "  - $\\delta$ is a vector with length = number of input units (5 in this case), with the gradients\n",
    "  - we compute $\\delta$ for each observation, then average and use the averaged gradient to update parameters \n",
    "- NN with Gradient Descent\n",
    "  - adjust the parameters by adding the product of the gradients and the learning rate from previous parameters\n",
    "  - reapeat until convergence or max iterations has been reached  \n",
    "- NN multiple layers  \n",
    "- Cost function for multipel layers is identical but $h_{\\Theta}(x_i)$ is more comlicated:\n",
    "  - $J(\\Theta)=-\\frac{1}{m}\\sum_{i=1}^m(y_i*\\log(h_{\\Theta}(x_i))+(1-y_i)\\log(1-h_{\\Theta}(x_i))$  \n",
    "- With multiple layers of parameters to learn with need to implement **backpropagation**\n",
    "  - update paramters starting at the last layer and circling back through each layer, updating accordingly\n",
    "  - need to compute multiple layers: $\\frac{\\delta}{\\delta \\Theta_i,j^{(l)}}J(\\Theta)$ (where l is the layer)\n",
    "  - theree layer network:\n",
    "    - $\\delta_j^l$ is the error for unit j, layer l\n",
    "    - $\\delta^4=h_{\\Theta}(X)-y$\n",
    "    - $\\delta^2=(\\Theta^{(2)})^T\\delta^3.*g'(z^{(2)})$\n",
    "    - there is no $\\delta^1$ since the first layer are the features and have no error  \n",
    "- to benchmark performance of NN we need AUC, area under the curve\n",
    "- Neural Net with three layers example\n",
    "\n",
    "## 23.10 - Guided Project: Predicting the stock market\n",
    "- Dataset: S&P500 stock market index\n",
    "  - some companies are publicly traded and have shares on the market\n",
    "  - a share entitles the owner to some earning and control of companies\n",
    "  - price of share is based on supply and demand\n",
    "  - stocks in more demand are traded more often\n",
    "- Indexes aggregate prices of multiple stocks and indicate how the market (or certain sectors) as a whole is performing\n",
    "  - e.g. Dow Jones aggregates 30 stocks from large american companies\n",
    "  - S&P500: 500 large companies\n",
    "  - ETFs: exchange traded funds allow one to buy and sell indexes like stocks\n",
    "- data on stocks is not independent (depends on previous values)\n",
    "- extra careful to not inject future knowledge into past rows (algorithm will look very good on trainning set, but fail on test)\n",
    "- we can create indicators to make the model more accurate (e.g. average price for the last 10 trades): information from multiple rows\n",
    "- do not use current row in the average! (otherwise the model will know the answer)\n",
    "- interesting indicators:\n",
    "  - 5 day average, 30 day average, 365 day average\n",
    "  - 5 day average / 365 day average\n",
    "  - std(5 day average), std(365 day average)\n",
    "  - std(5 day average) / std(365 day average)\n",
    "- price is the close column\n",
    "- Define error metric: MAE (easier to interpret) or MSE (better, but harder to intuitively know how far the model is)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 24 - Decision Trees\n",
    "## 24.1 - Introduction to Decision Trees - ID3 algorithm\n",
    "- Decision Tree - supervised learning algorithm\n",
    "  - can be used both for classification and regression\n",
    "  - can pick up nonlinear interactions between variables in the data that linear regression can't\n",
    "- first convert categorical variables to numerical: col = pd.Categorical(df['col1']); df['col1'] = col.codes\n",
    "  - no need to dummy code since this is a classification problem (not regression)\n",
    "- nodes at the bottom of a tree are **leaves**: must have only one value for our target column, split nodes until this is the case\n",
    "- which variables should we split the nodes on?:\n",
    "  - each split needs to get us closer to have leaves with only 0 or 1 for the target: try to put the maximum number of 0's and 1's together after split\n",
    "  - entropy from information theory: highest with more disorder (larger mix of 0's and 1's in target column)\n",
    "    - $\\sum_{i=1}^cP(x_i)\\log_b P(x_i)$\n",
    "    - iterate through each value in the target column and compute the probability of it occurring in the data ($P(x_i)$)\n",
    "- Information gain (for a given target (T) and variable to split on (A)): which split will reduce entropy the most\n",
    "  - $IG(T, A)=Entropy(T)-\\sum_{v \\epsilon A}\\frac{T_v}{T}.Entropy(T_v)$\n",
    "  - We are finding the entropy post-split, weighting by the number of items in each split, then subtracting from current entropy\n",
    "- One strategy is to have as many branches in each node as unique values for the variable we are splitting on (normally involves more complexity than needed)\n",
    "- We can find the median for the variable we are splitting on to do the splitting\n",
    "\n",
    "\n",
    "## 24.2 - Building a Decision Tree (ID3 Algorithm)\n",
    "- ID3 Algorithm involves recursion and time complexity\n",
    "- at each node we call a recursive function that will split the data in 2 branches, each branch will lead to a node and the function will call itself again\n",
    "  - Create a node for the tree\n",
    "  - if all values of the target are 1, return the node, with label 1 (this will be changed to a tree dict later)\n",
    "  - if all values of the target are 0, return the node, with label 0 (this will be changed to a tree dict later)\n",
    "  - using IG find the best columns to split on (A)\n",
    "  - Split column A into values below or equal to the median (0), and values above (1)\n",
    "  - for each possible value (0 or 1) of A (v_i)\n",
    "    - add a new branch below root that corresponds to rows of data where A=v_i\n",
    "    - below the new branch add the subtree id3(data[A==v_i], target, columns)\n",
    "- Build ID3 Algorithm\n",
    "- Create print function for the tree dictionary:\n",
    "- Create Predicit function  \n",
    "  \n",
    "## 24.3 - Applying Decision Trees\n",
    "- We have used an ID3 algorithm (there is also C4.5 and CART)\n",
    "- sklearn tree algorithms\n",
    "  - DecisionTreeClassifier - binary outcome \n",
    "  - DecisionTreeRegressor - continous\n",
    "- We can avoid overfitting by always making predictions and evaluating error on data that we haven't trained our algorithm with.\n",
    "- Error metric: AUC is ideal for binary classification (the higher the AUC the more accurate the predictions)\n",
    "- If the AUC between training set predictions and actual values is significantly higher than the AUC between test set predictions and actual values, it's a sign that the model may be overfitting.\n",
    "- Trees overfit when they have too much depth (only work on the specific training set)\n",
    "  - prune the tree after build to remove unnecessary leaves\n",
    "  - Use ensembling to blend predictions of many trees (Random Forests)\n",
    "  - restrict the depth of the tree while building it:\n",
    "    - max_depth - restricts how deep the tree can go\n",
    "    - min_samples_split - The minimum number of rows a node should have before it can be split; e.g. if set to 2, nodes with 2 rows won't be split, and become leaves \n",
    "    - min_samples_leaf - minimum number of rows a leaf must have\n",
    "    - min_weight_fraction_leaf - fraction of input rows a leaf must have\n",
    "    - max_leaf_nodes - The maximum number of total leaves\n",
    "- overfit and underfit:\n",
    "  - if we restrict the depth too much we might prevent the model from being complex enough to correclty categorize the rows\n",
    "  - if we don't restrict the tree becomes too complex and fits noise in the trainning set\n",
    "  - high bias can cause underfitting, high variance can cause overfitting (model varies too much on small input changes)\n",
    "    - high-variance: large difference between train and test\n",
    "    - high-bias: low auc on test\n",
    "  - we introduce artifical noise to check for overfitting\n",
    "- Decision trees pros:\n",
    "  - easy to interpret\n",
    "  - fast to fit and make predictions\n",
    "  - able to handle multiple types of data\n",
    "  - able to pick nonlinearities in data, and usually accurate\n",
    "  -usefull when it is important to interpret the results of the algorithm\n",
    "- Decision trees cons:\n",
    "  - tendency to overfit\n",
    "  \n",
    "## 24.4 - Introduction to Random Forests\n",
    "- Most powerfull tool to reduce overfitting\n",
    "- Random Forests: ensemble model, combine predcitions from multiple models to get better results\n",
    "- Random forest: ensemble of trees\n",
    "  - Combining ensembles:\n",
    "    - Majority votings: final prediction is the majority vote from different models\n",
    "    - use the mean of predict_proba() from the different models\n",
    "  - Variation (both done always done in RandomForest)\n",
    "    - bagging: train tree on a random subset of data with replacement(bag)\n",
    "    - or random feature subset\n",
    "      - pick number of features to evaluate on split (must be smaller than number of columns in data)\n",
    "      - at every split pick a random sample of features from data, compute IG for each and pick the one with the highest IF to split on \n",
    "      - random feature subset using DecisionTreeClassifier: splitter='random', max_features=\"auto\" \n",
    "- sklearn RandomForestClassifier and RandomForestRegressor: build multiple trees\n",
    "- Parameters to tweak:\n",
    "  - min_samples_leaf, min_samples_split, max_depth, max_leaf_nodes \n",
    "  - n_estimators (number of trees), larger improvement from 10 to 100 than from 100 to 500\n",
    "  - bootstrap (bootstrap aggregation is another name for bagging, true by default)\n",
    "- Random Forests pros:\n",
    "  - very accurate, along with neural networks and gradient-boosted trees are typically the best\n",
    "  - resistance to overfitting (still need to tweak max_depth though)\n",
    "  - use when accuracy is crucial and not interpreting the model\n",
    "- Random Forests cons:\n",
    "  - difficult to interpret\n",
    "  - take longer to create, but we can parallelize it: n_jobs on RandomForestClassifier\n",
    "  \n",
    "## 24.5 - Guided Project: Predicting Bike Rentals\n",
    "- use MSE as error metric\n",
    "- Linear Regression model\n",
    "- Decision Tree model\n",
    "- Random Forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 25 - Kaggle Fundamentals\n",
    "- Approach a Kaggle competion\n",
    "- Explore and prepare data and learn about topic\n",
    "- Train a model\n",
    "- Measure the accuracy of the model\n",
    "- prepare and make Kaggle submission\n",
    "\n",
    "## 25.1 - Getting Started with Kaggle\n",
    "- Titanic data, predict survival. Train and test sets (latter without survival colum)\n",
    "- acquiring domain knowledge\n",
    "  - sex_pivot = train.pivot_table(index=\"Sex\",values=\"Survived\")\n",
    "  - sex_pivot.plot.bar()\n",
    "- Sex and Pclass are categorical features, age is not. We can use pandas.cut() to transform it\n",
    "  - any change to the train data needs to be done on the test as well\n",
    "  - handle missing values  \n",
    "  - use pd.cut(df[\"Age\"],cut_points,labels=label_names)\n",
    "- Dummy coding\n",
    "- LogisticRegression - first model to try in classification problems   \n",
    "- Use sklearn train_test_split to split the train data into train and test set (test does not have the survival column, so we cannot check our predictions)\n",
    "- Accuracy percentage of target labels correctly predicted: from sklearn.metrics import accuracy_score\n",
    "- Cross validation: from sklearn.model_selection import cross_val_score\n",
    "- Make predictions omn the holdout (test) set\n",
    "- Create a submission file\n",
    "\n",
    "## 25.2 - Feature Preparation, Selection and Engineering\n",
    "- boost accuracy: improve features or improve model\n",
    "- improve features: \n",
    "  - remove features which are not good predictors or are closely related to each other (collinearity) (both will cause the model to overfit and not be accurate on new data)\n",
    "  - create good new features\n",
    "- look at additional features:\n",
    "  - display(train[columns].describe(include='all', percentiles=[]))\n",
    "  - keep categorical columns with few unique values and few NaNs, fill NaNs for these\n",
    "  - rescale numerical columns: from sklearn.preprocessing import minmax_scale\n",
    "- Feature selection using LogisticRegression.coef_\n",
    "  - coefficients = lr.coef_\n",
    "  - feature_importance = pd.Series(coefficients[0], index=columns)\n",
    "  - ordered_feature_importance = feature_importance.abs().sort_values()\n",
    "  - ordered_feature_importance.plot.barh()\n",
    "- Feature Engeneering: dummies are only needed for regression problems\n",
    "  - binning + create dummies\n",
    "  - Feature Engeneering: extract data from text columns and create new categorical values + create dummies\n",
    "- Check for collinearity with sns.heatmap\n",
    "  - one example: dummy variable trap: always drop one columns when making dummies  \n",
    "- RFECV: recursive feature elimination with cross validation \n",
    "  - start with a model wtih all features and then remove the ones with lowest scores\n",
    "  - lr = LogisticRegression(solver='lbfgs')\n",
    "  - selector = RFECV(lr, cv=10)\n",
    "  - selector.fit(all_X, all_y)\n",
    "  - optimized_columns = all_X.columns[selector.support_]\n",
    "\n",
    "  \n",
    "## 25.3 - Model Selection and Tuning\n",
    "- Logistic Regression: calculate linear relationships between features and target\n",
    "- K-nearest neighbors: finds the observations in the training set closer to the observation in test set and uses the average to make a prediction\n",
    "- RandomForestClassifier  \n",
    "- hyperparameter optimization using grid seach and cross validation\n",
    "  - from sklearn.model_selection import GridSearchCV\n",
    "  - grid = GridSearchCV(knn, param_grid=hyperparameters, cv=10)\n",
    "  - grid.fit(all_X,all_y)\n",
    "  - best_knn = grid.best_estimator_ will have the best model\n",
    "  \n",
    "## 25.4 - Guided Project: Creating a Kaggle Workflow\n",
    "- Best summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 26 - Additional ML topics\n",
    "## 26.1 - Naive Bayes for Sentiment Analysis (only look at sklearn) - classification\n",
    "- Naive Bayes: how likely data attributes ar to be associated with a certain class\n",
    "- based on Bayes theorem: $P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$\n",
    "- Naive Bayes: extended Bayes theorem for multiple points assuming independence between points:\n",
    "  - $ P(y|x_1,...x_n)=\\frac{P(y)\\prod_{i=1}^n P(x_i|y)}{P(x_1,...,x_n)} $\n",
    "  - to find the right classification we find the classification $P(y|x_1,...x_n)$ with the highest probability\n",
    "- in our problem we have a single sentence, we can generate features by splitting it into words\n",
    "  - count how many times each word occurs in negative and positive reviews\n",
    "  - use the counts to compute probability\n",
    "- compute error from the area under the ROC curve \n",
    "- we could use n-grams instead of unigrams, remove punctuation and stop words, perform stemming or lemmatization\n",
    "- sklearn: from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "## 26.2 - Natural Language Processing - regression\n",
    "- go from text to numerical representation, then make predictions\n",
    "- train a Linear Regression algorithm to predict number of upvotes an headline will receive using **bag of words model**\n",
    "  - get a unique list of words to create column names, for each sentence (row) have 0 in a column if the word doesn't apper in that sentence (1 if it does)\n",
    "- features that occur only a few times produce overfitting\n",
    "- features that appear too many time don't necessarily correlate with target label\n",
    "- improve results:\n",
    "  - use a larger trainning set (the file used here is a small subset)\n",
    "  - add meta-features like headline length and average word length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 27 - Data Structures and Algorithms\n",
    "## 27.1 Memory and Unicode\n",
    "- print binary, base 2, numbers as int\n",
    "- add binary numbers\n",
    "- if we add one unit to the same binary and base ten numbers the result will still be the same\n",
    "- ord() to get int of a character, then bin() to get binary value\n",
    "- code points (\\u is the prefix for unicode)\n",
    "- We can combine ASCII and unicode strings\n",
    "- \\x is the prefix for an hexadecimal character, says the next 2 digits are hexadecimal (values from 0 to 9 and A to F, 9+1=A)\n",
    "  - heaxadecimal is an easy way to represent bytes. byte is 8 bits, or 8 binary digits. \n",
    "  - highest value we can express in a byte is 11111111, or 255 in base 10, of FF in hexadecimal (2 hexadecimal = 8 binary)\n",
    "- convert between hexadecimal, binary and base 10\n",
    "- string_bytes.decode(\"utf-8\")\n",
    "- read data with unicode and symbols\n",
    "  - clean data from  weird symbols\n",
    "  - combine sentences and slipt into words: \" \".join, split(\" \")\n",
    "  - remove stop words (len<5)\n",
    "- count words:\n",
    "  - from collections import Counter\n",
    "  - filtered_token_counts = Counter(filtered_tokens)\n",
    "  - print(filtered_token_counts.most_common(10))\n",
    "\n",
    "## 27.2 - Algorithms\n",
    "- create a linear search algorithm\n",
    "- modular code: use functions\n",
    "- abstraction: someone can use our code without knowing how it is implemented\n",
    "- time complexity: the most common factor to consider when choosing an algorithm (smaller time complexty, better)\n",
    "- how long code takes with respect to input size\n",
    "- time complexity is given by worst case scenario\n",
    "- linear search is linear in time (so are functions that take 0.5n or 5n time): O(n), constant is written as O(1)\n",
    "\n",
    "## 27.3 - Binary Search\n",
    "- binary search: ordered data, go half the way and then either continue on first or second half: O(log(n)) time complexity\n",
    "\n",
    "## 27.4 - Data Structures\n",
    "- insert values into lists:\n",
    "- data structures: lists, dictionaries, arrays\n",
    "- 2-dimensional arrays\n",
    "- hash tables: dictionaries, access values by keys (constant time complexity)\n",
    "- hash function converts key into index, avoid collisions\n",
    "- fast but can use a lot of memory\n",
    "\n",
    "## 27.5 - Recursion and Advanced Data Structures\n",
    "- recursion is like figuring out in which row you are sitting in a cinema. you can ask the person in front and add 1 to that. She asks the person in front and so on. The functions are not terminated until they get a response. when the person in the first row gets asked, we get the base case.\n",
    "- linked lists: sinly linked list (each node contains its data and the next node)\n",
    "- recursive data structure\n",
    "- constant time complexity to add or delete element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 28 - Python Programming: Advanced\n",
    "## 28.1 - Object-Oriented Programming\n",
    "### 23.1.6 - inheritance\n",
    "### 23.1.7 - overloading\n",
    "## 28.2 - Exception Handling\n",
    "## 28.3 - Lambda Functions\n",
    "### 23.3.3 - step value\n",
    "## 28.4 - Introduction to Computer Architecture\n",
    "## 28.5 - Parallel Processing\n",
    "### 23.5.3 - threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 29 - Spark and Map Reduce\n",
    "## 29.1 - Introduction to Spark\n",
    "### 27.1.7 - Transformations and Actions & Immutability\n",
    "## 29.2 - Project: Spark Installation and Jupyter Notebook Integration\n",
    "## 29.3 - Transformations and Actions\n",
    "### 27.3.6 - Actions: count & collect\n",
    "## 29.4 - Challenge: Transforming Hamlet into a Data Set\n",
    "## 29.5 - Spark DataFrames\n",
    "### 27.5.7 - Boolean indexing and filtering\n",
    "## 29.6 - Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
