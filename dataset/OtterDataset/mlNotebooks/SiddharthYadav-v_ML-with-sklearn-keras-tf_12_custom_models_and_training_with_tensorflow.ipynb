{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project requires Python 3.7 or above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And Tensorflow >= 2.8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging import version\n",
    "import tensorflow as tf\n",
    "\n",
    "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorFlow like numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors and Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])  # matrix\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[2., 3.],\n",
       "       [5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[2.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[..., 1, tf.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[11., 12., 13.],\n",
       "       [14., 15., 16.]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t @ tf.transpose(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=42>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras's low-level API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may still run across code that used Keras's low-level API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[11., 26.],\n",
       "       [14., 35.],\n",
       "       [19., 46.]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = tf.keras.backend\n",
    "K.square(K.transpose(t)) + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But since Keras does not support multiple backends anymore, you should instead use TF's low-level API directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[11., 26.],\n",
       "       [14., 35.],\n",
       "       [19., 46.]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(tf.transpose(t)) + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors and NumPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([2., 4., 5.])\n",
    "tf.constant(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 4., 16., 25.])>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.constant(2.0) + tf.constant(40)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:AddV2]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tf.constant(2.0) + tf.constant(40, dtype=tf.float64)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=42.0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = tf.constant(40., dtype=tf.float64)\n",
    "tf.constant(2.0) + tf.cast(t2, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2.,  4.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.assign(2 * v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0, 1].assign(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  0.],\n",
       "       [ 8., 10.,  1.]], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[:, 2].assign([0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[100.,  42.,   0.],\n",
       "       [  8.,  10., 200.]], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.scatter_nd_update(\n",
    "    indices=[[0, 0], [1, 2]], updates=[100., 200.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[4., 5., 6.],\n",
       "       [1., 2., 3.]], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_delta = tf.IndexedSlices(values=[[1., 2., 3.], [4., 5., 6.]],\n",
    "                                indices=[1, 0])\n",
    "v.scatter_update(sparse_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ResourceVariable' object does not support item assignment\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    v[1] = [7., 8., 9.]\n",
    "except TypeError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hello world'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(b\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'caf\\xc3\\xa9'>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(\"café\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([ 99,  97, 102, 233])>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = tf.constant([ord(c) for c in \"café\"])\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=4>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = tf.strings.unicode_encode(u, \"UTF-8\")\n",
    "tf.strings.length(b, unit=\"UTF8_CHAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([ 99,  97, 102, 233])>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.unicode_decode(b, \"UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'hello world'>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(b\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'caf\\xc3\\xa9'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(\"café\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([ 99,  97, 102, 233])>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = tf.constant([ord(c) for c in \"café\"])\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'caf\\xc3\\xa9'>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = tf.strings.unicode_encode(u, \"UTF-8\")\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=4>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.length(b, unit=\"UTF8_CHAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([ 99,  97, 102, 233])>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.unicode_decode(b, \"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tf.constant([\"Café\", \"Coffee\", \"caffè\", \"咖啡\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([4, 6, 5, 2])>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.length(p, unit=\"UTF8_CHAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101],\n",
       " [99, 97, 102, 102, 232], [21654, 21857]]>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = tf.strings.unicode_decode(p, \"UTF8\")\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ragged tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=int32, numpy=array([ 67, 111, 102, 102, 101, 101])>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[67, 111, 102, 102, 101, 101], [99, 97, 102, 102, 232]]>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101],\n",
       " [99, 97, 102, 102, 232], [21654, 21857], [65, 66], [], [67]]>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2 = tf.ragged.constant([[65, 66], [], [67]])\n",
    "tf.concat([r, r2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[67, 97, 102, 233, 68, 69, 70], [67, 111, 102, 102, 101, 101, 71],\n",
      " [99, 97, 102, 102, 232], [21654, 21857, 72, 73]]>\n"
     ]
    }
   ],
   "source": [
    "r3 = tf.ragged.constant([[68, 69, 70], [71], [], [72, 73]])\n",
    "print(tf.concat([r, r3], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 6), dtype=int32, numpy=\n",
       "array([[   67,    97,   102,   233,     0,     0],\n",
       "       [   67,   111,   102,   102,   101,   101],\n",
       "       [   99,    97,   102,   102,   232,     0],\n",
       "       [21654, 21857,     0,     0,     0,     0]])>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.to_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tf.SparseTensor(indices=[[0, 1], [1, 0], [2, 3]],\n",
    "                    values=[1., 2., 3.],\n",
    "                    dense_shape=[3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0.],\n",
       "       [2., 0., 0., 0.],\n",
       "       [0., 0., 0., 3.]], dtype=float32)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.to_dense(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.sparse_tensor.SparseTensor at 0x13902d6f2e0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s * 42.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsupported operand type(s) for +: 'SparseTensor' and 'float'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    s + 42.0\n",
    "except TypeError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[ 30.,  40.],\n",
       "       [ 20.,  40.],\n",
       "       [210., 240.]], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows how to multiply a sparse tensor and a dense tensor\n",
    "s4 = tf.constant([[10., 20.], [30., 40.], [50., 60.], [70., 80.]])\n",
    "tf.sparse.sparse_dense_matmul(s, s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{function_node __wrapped__SparseToDense_device_/job:localhost/replica:0/task:0/device:GPU:0}} indices[1] is out of order. Many sparse ops require sorted indices.\n",
      "  Use `tf.sparse.reorder` to create a correctly ordered copy.\n",
      "\n",
      "\n",
      "\t [[{{node SparseToDense}}]] [Op:SparseToDense]\n"
     ]
    }
   ],
   "source": [
    "# when creating a sparse tensor, values must be given in \"reading order\",\n",
    "# or else `to_dense()` will fail.\n",
    "s5 = tf.SparseTensor(indices=[[0, 2], [0, 1]],  # WRONG ORDER!\n",
    "                 values=[1., 2.],\n",
    "                 dense_shape=[3, 4])\n",
    "try:\n",
    "    tf.sparse.to_dense(s5)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[0., 2., 1., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows how to fix the sparse tensor s5 by reordering its values\n",
    "s6 = tf.sparse.reorder(s5)\n",
    "tf.sparse.to_dense(s6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = tf.TensorArray(dtype=tf.float32, size=3)\n",
    "array = array.write(0, tf.constant([1., 2.]))\n",
    "array = array.write(1, tf.constant([3., 10.]))\n",
    "array = array.write(2, tf.constant([5., 7.]))\n",
    "tensor1 = array.read(1)  # returns (and zeros out!) tf.constant([3., 10.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[1., 2.],\n",
       "       [0., 0.],\n",
       "       [5., 7.]], dtype=float32)>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[ 1.,  2.],\n",
       "       [ 3., 10.],\n",
       "       [ 5.,  7.]], dtype=float32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows how to disable clear_after_read\n",
    "array2 = tf.TensorArray(dtype=tf.float32, size=3, clear_after_read=False)\n",
    "array2 = array2.write(0, tf.constant([1., 2.]))\n",
    "array2 = array2.write(1, tf.constant([3., 10.]))\n",
    "array2 = array2.write(2, tf.constant([5., 7.]))\n",
    "tensor2 = array2.read(1)\n",
    "array2.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[1., 2.],\n",
       "       [0., 0.],\n",
       "       [5., 7.]], dtype=float32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows how to create and use a tensor array with a dynamic size\n",
    "array3 = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "array3 = array3.write(0, tf.constant([1., 2.]))\n",
    "array3 = array3.write(1, tf.constant([3., 10.]))\n",
    "array3 = array3.write(2, tf.constant([5., 7.]))\n",
    "tensor3 = array3.read(1)\n",
    "array3.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.sparse_tensor.SparseTensor at 0x13902cdb6d0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([[1, 5, 9]])\n",
    "b = tf.constant([[5, 6, 9, 11]])\n",
    "u = tf.sets.union(a, b)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5), dtype=int32, numpy=array([[ 1,  5,  6,  9, 11]])>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.to_dense(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=int32, numpy=\n",
       "array([[ 1,  5,  6,  9, 11],\n",
       "       [ 0, 10, 13,  0,  0]])>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([[1, 5, 9], [10, 0, 0]])\n",
    "b = tf.constant([[5, 6, 9, 11], [13, 0, 0, 0]])\n",
    "u = tf.sets.union(a, b)\n",
    "tf.sparse.to_dense(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=int32, numpy=\n",
       "array([[ 1,  5,  6,  9, 11],\n",
       "       [-1, 10, 13, -1, -1]])>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows how to use a different default value: -1 in this case\n",
    "a = tf.constant([[1, 5, 9], [10, -1, -1]])\n",
    "b = tf.constant([[5, 6, 9, 11], [13, -1, -1, -1]])\n",
    "u = tf.sets.union(a, b)\n",
    "tf.sparse.to_dense(u, default_value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "array([[2, 3, 7],\n",
       "       [7, 0, 0]])>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows how to use `tf.sets.difference()`\n",
    "set1 = tf.constant([[2, 3, 5, 7], [7, 9, 0, 0]])\n",
    "set2 = tf.constant([[4, 5, 6], [9, 10, 0]])\n",
    "tf.sparse.to_dense(tf.sets.difference(set1, set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[5, 0],\n",
       "       [0, 9]])>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows how to use `tf.sets.intersection()`\n",
    "tf.sparse.to_dense(tf.sets.intersection(set1, set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check whether set1[0] contains 5\n",
    "tf.sets.size(tf.sets.intersection(set1[:1], tf.constant([[5, 0, 0, 0]]))) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Queues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=2>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = tf.queue.FIFOQueue(3, [tf.int32, tf.string], shapes=[(), ()])\n",
    "q.enqueue([10, b\"windy\"])\n",
    "q.enqueue([15, b\"sunny\"])\n",
    "q.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int32, numpy=10>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'windy'>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.dequeue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "q.enqueue_many([[13, 16], [b'cloudy', b'rainy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(3,), dtype=int32, numpy=array([15, 13, 16])>,\n",
       " <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'sunny', b'cloudy', b'rainy'], dtype=object)>]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.dequeue_many(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing Models and Training Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x350 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 3.5))\n",
    "z = np.linspace(-4, 4, 200)\n",
    "z_center = np.linspace(-1, 1, 200)\n",
    "plt.plot(z, huber_fn(0, z), \"b-\", linewidth=2, label=\"huber($z$)\")\n",
    "plt.plot(z, z ** 2 / 2, \"r:\", linewidth=2)\n",
    "plt.plot(z_center, z_center ** 2 /2, \"r\", linewidth=2)\n",
    "plt.plot([-1, -1], [0, huber_fn(0., 1.)], \"k--\")\n",
    "plt.plot([1, 1], [0, huber_fn(0., 1.)], \"k-\")\n",
    "plt.gca().axhline(y=0, color='k')\n",
    "plt.gca().axvline(x=0, color='k')\n",
    "plt.text(2.1, 3.5, r\"$\\frac{1}{2}z^2$\", color=\"r\", fontsize=15)\n",
    "plt.text(3.0, 2.2, r\"$|z| - \\frac{1}{2}$\", color=\"b\", fontsize=15)\n",
    "plt.axis([-4, 4, 0, 4])\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"$z$\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.title(\"Huber loss\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our custom loss function, let's create a basic Keras model and train it on the California house dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation='relu', kernel_initializer=\"he_normal\",\n",
    "                          input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=huber_fn, optimizer=\"nadam\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 3s 6ms/step - loss: 0.4827 - mae: 0.8318 - val_loss: 0.3478 - val_mae: 0.6525\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.2411 - mae: 0.5412 - val_loss: 0.2641 - val_mae: 0.5484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13906004550>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving/Loading Models with Custom Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model_with_a_custom_loss\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"my_model_with_a_custom_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving a model containing a custom loss function works fine, but when you load it, you'll need to provide a dictionary that maps the function name to the actual function. More generally, when you load a model containing custom objects, you need to map the names to the objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"my_model_with_a_custom_loss\",\n",
    "                                   custom_objects={\"huber_fn\": huber_fn})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "106/363 [=======>......................] - ETA: 1s - loss: 0.2195 - mae: 0.5130"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 3s 6ms/step - loss: 0.2050 - mae: 0.4906 - val_loss: 0.2199 - val_mae: 0.4934\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.1885 - mae: 0.4677 - val_loss: 0.2025 - val_mae: 0.4776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1390750df40>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2, \n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = threshold * tf.abs(error) - threshold ** 2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 2s 4ms/step - loss: 0.2049 - mae: 0.4594 - val_loss: 0.2229 - val_mae: 0.4561\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.1977 - mae: 0.4527 - val_loss: 0.2056 - val_mae: 0.4533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x139087672e0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model_with_a_custom_loss_threshold_2\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"my_model_with_a_custom_loss_threshold_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you save the model, the threshold will not be saved. This means that you will have to specify the threshold value when loading the model (note that the name to use is `\"huber_fn\"`, which is the name of the function you gave Keras, not the name of the function that created it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"my_model_with_a_custom_loss_threshold_2\",\n",
    "                                   custom_objects={\"huber_fn\": create_huber(2.0)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can solve this by creating a subclass of the `tf.keras.losses.Loss` class, and then implementing its `get_config()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold ** 2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through this code:\n",
    "\n",
    "- The constructor accepts `**kwargs` and passes them to the parent constructor, which handles standard hyperparameters: the name of the loss and the `reduction` algorithm to use to aggregate the individual instance losses. By default this is `\"AUTO\"`, which is equivalent to `\"SUM_OVER_BATCH_SIZE\"`: the loss will be the sum of the instances losses, weighted by the sample weights, if any, and divided by the batch size (not by the sum of weights, so this is *not* the weighted mean). Other possible values are `\"SUM\"` and `\"NONE\"`.\n",
    "- The `call()` method takes the labels and predictions, computes all the instance losses, and returns them.\n",
    "- The `get_config()` method returns a dictionary mapping each hyperparameter name to its value. It first calls the parent class's `get_config()` method, then adds the new hyperparameters to this dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                          input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 3s 6ms/step - loss: 0.6910 - mae: 0.8725 - val_loss: 0.7029 - val_mae: 0.7884\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3202 - mae: 0.5807 - val_loss: 0.4857 - val_mae: 0.6377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1390892e8e0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model_with_a_custom_loss_class\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"my_model_with_a_custom_loss_class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"my_model_with_a_custom_loss_class\",\n",
    "                 custom_objects={\"HuberLoss\": HuberLoss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 2s 4ms/step - loss: 0.2528 - mae: 0.5159 - val_loss: 725.0286 - val_mae: 363.5143\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.2253 - mae: 0.4856 - val_loss: 629.8856 - val_mae: 315.9428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13908c1f730>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss.threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Custom Funcitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softplus(z):\n",
    "    return tf.math.log(1.0 + tf.exp(z))\n",
    "\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "def my_positive_weights(weights):\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(1, activation=my_softplus,\n",
    "                              kernel_initializer=my_glorot_initializer,\n",
    "                              kernel_regularizer=my_l1_regularizer,\n",
    "                              kernel_constraint=my_positive_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 3s 5ms/step - loss: 1.1375 - mae: 0.7484 - val_loss: 3.2480 - val_mae: 0.6367\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.6627 - mae: 0.5745 - val_loss: 2.2848 - val_mae: 0.5706\n",
      "INFO:tensorflow:Assets written to: my_model_with_many_custom_parts\\assets\n",
      "Epoch 1/2\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.5571 - mae: 0.5278 - val_loss: 1.3089 - val_mae: 0.5194\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.5032 - mae: 0.5012 - val_loss: 1.1106 - val_mae: 0.5018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13908e5ee20>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                          input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(1, activation=my_softplus,\n",
    "                          kernel_initializer=my_glorot_initializer,\n",
    "                          kernel_regularizer=my_l1_regularizer,\n",
    "                          kernel_constraint=my_positive_weights)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.save(\"my_model_with_many_custom_parts\")\n",
    "model = tf.keras.models.load_model(\n",
    "    \"my_model_with_many_custom_parts\",\n",
    "    custom_objects={\n",
    "        \"my_l1_regularizer\": my_l1_regularizer,\n",
    "        \"my_positive_weights\": my_positive_weights,\n",
    "        \"my_glorot_initializer\": my_glorot_initializer,\n",
    "        \"my_softplus\": my_softplus,\n",
    "    }\n",
    ")\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyL1Regularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 2s 4ms/step - loss: 1.2673 - mae: 0.7621 - val_loss: inf - val_mae: inf\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.7992 - mae: 0.5976 - val_loss: inf - val_mae: inf\n",
      "INFO:tensorflow:Assets written to: my_model_with_many_custom_parts\\assets\n",
      "Epoch 1/2\n",
      "363/363 [==============================] - 3s 6ms/step - loss: 0.5840 - mae: 0.5263 - val_loss: inf - val_mae: inf\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.5121 - mae: 0.4970 - val_loss: 2.9269 - val_mae: 0.5177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13a1e99f2b0>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                          input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(1, activation=my_softplus,\n",
    "                          kernel_regularizer=MyL1Regularizer(0.01),\n",
    "                          kernel_constraint=my_positive_weights,\n",
    "                          kernel_initializer=my_glorot_initializer),\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "        validation_data=(X_valid_scaled, y_valid))\n",
    "model.save(\"my_model_with_many_custom_parts\")\n",
    "model = tf.keras.models.load_model(\n",
    "    \"my_model_with_many_custom_parts\",\n",
    "    custom_objects={\n",
    "        \"MyL1Regularizer\": MyL1Regularizer,\n",
    "        \"my_positive_weights\": my_positive_weights,\n",
    "        \"my_glorot_initializer\": my_glorot_initializer,\n",
    "        \"my_softplus\": my_softplus,\n",
    "    }\n",
    ")\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Losses and metrics are conceptually not the same thing: losses (e.g., cross-entropy) are used by gradient descent to *train* a model, so they must be differentiable (at least at the points where they are evaluated), and their gradients should not be zero everywhere. Plus, it's OK if they are not easily interpretable by humans. In contrast, metrics (e.g., accuracy) are used to *evaluate* a model: they must be more easily interpretable, and they can be nondifferentiable or have zero gradients everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                          input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 2s 4ms/step - loss: 1.9848 - huber_fn: 0.8613\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.7115 - huber_fn: 0.3432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13a2763cf10>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** if you use the same function as the loss and a metric, you may be suprised to see slightly different results. This is in part because the operations are not computed exaclty in the same order, so there might be tiny floating point errors. More importantly, if you use sample weights or class weights, then the equations are a bit different:\n",
    "\n",
    "* the `fit()` method keeps track of the mean of all batch losses seen so far since the start of the epoch. Each batch loss is the sum of the weighted instance losses divided by the batch size (not the sum of weights, so the batch loss is not the weighted mean of the losses).\n",
    "* the metric since the start of the epoch is equal to the sum of weighted instance losses divided by sum of all weights seen so far. In other words, it is the weighted mean of all the instance losses. Not the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.8>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = tf.keras.metrics.Precision()\n",
    "precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1,0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a streaming metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)   # handles base args (e.g., dtype)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        sample_metrics = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(sample_metrics))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through this code:\n",
    "\n",
    "- The constructor uses the `add_weight()` method to create the variables needed to keep track of the metric's state over multiple batches- in this case, the sum of all Huber loss (total) and the number of instances seen so far (count). You could just create variables manually if you preferred. Keras tracks any `tf.Variable` that is set as an attribute (and more generally, any \"trackable\" object, such as layers or models).\n",
    "- The `update_state()` method is called when you use an instance of this class as a function (as we did with the `Precision` object). It updates the variables, given the labels and predictions for one batch (and sample weights, but in this case we ignore them).\n",
    "- The `result()` method computes and returns the final result, in this case the mean Huber metric over all instances. When you use the metric as a function, the `update_state()` method gets caalled first, then the `result()` method is called, and its output is returned.\n",
    "- We also implement the `get_config()` method to ensure the `threshold` gets saved along with the model.\n",
    "- The default implementation of the `reset_states()` method resets all vairables to 0.0 (but you can override it if needed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponential_layer = tf.keras.layers.Lambda(lambda x: tf.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.36787948, 1.        , 2.7182817 ], dtype=float32)>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exponential_layer([-1., 0., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding an exponential layer at the output of a regression model can be useful if the values to predict are positive and with very different scales (e.g., 0.001, 10., 100000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "207/363 [================>.............] - ETA: 0s - loss: 0.9643"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 1s 3ms/step - loss: 0.7705 - val_loss: 0.4597\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4768 - val_loss: 0.3868\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4186 - val_loss: 0.3578\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4349 - val_loss: 0.3634\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3919 - val_loss: 0.3548\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 0.3742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3742235004901886"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(1),\n",
    "    tf.keras.layers.Dense(1),\n",
    "    exponential_layer\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "model.fit(X_train_scaled, y_train, epochs=5,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
    "            initializer=\"he_normal\")\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "    \n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units,\n",
    "                \"activation\": tf.keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through this code:\n",
    "\n",
    "- The constructor takes all the hyperparameters as arguments (in this example, `units` and `activation`), and importantly it also takes a `**kwargs` argument. It calls the parent constructor, passing it the `kwargs`: this takes care of standard arguments such as `input_shape`, `trainable`, and `name`. Then it saves the hyperparameters as attributes, converting the `activation` argument to the appropriate activation function using the `tf.keras.activations.get()` function (it accepts functions, standard string like `\"relu\"`, `\"swish\"`, or simply `None`).\n",
    "- The `build()` method's role is to create the layer's variables by calling the `add_weight()` method for each weight. The `build()` method is called the first time the layer is used. At that point, Keras will know the shape of this layer's inputs, and it will pass it to the `build()` method, which is often necessary to create some of the weights. For example, we need to know the number of neurons in the previous layer in order to create the connection weights matrix (i.e., the `\"kernel\"`): this corresponds to the size of the last dimension of the inputs. At the end of the `build()` method (and only at the end), you must call the parent's `build()` method: this tells Keras that the layer is built (it just sets `self.built = True`).\n",
    "- The `call()` method performs the desired operations. In this case, we compute the matrix multiplication of the inputs `X` and the layer's kernel, we add the bias vector, and we apply the activation function to the result, and this gives us the output of the layer.\n",
    "- The `get_config()` method is just like in the previous custom classes. Note that we saw the activation function's full configuration by calling `tf.keras.activations.serialize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 2s 5ms/step - loss: 3.1061 - val_loss: 7.1579\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.8700 - val_loss: 3.3903\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.7029\n",
      "INFO:tensorflow:Assets written to: my_model_with_a_custom_layer\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    MyDense(30, activation=\"relu\", input_shape=input_shape),\n",
    "    MyDense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)\n",
    "model.save(\"my_model_with_a_custom_layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.5935 - val_loss: 0.5096\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4701 - val_loss: 0.5947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13909382a60>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"my_model_with_a_custom_layer\",\n",
    "                             custom_objects={\"MyDense\": MyDense})\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a layer with mulitple inputs (e.g., `Concatenate`), the argument to the `call()` method should be a tuple containing all the inputs. To create a layer with multiple outputs, the `call()` method should return the list of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMulitLayer(tf.keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        X1, X2 = X\n",
    "        return X1 + X2, X1 * X2, X1 / X2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our custom layer can be called using the functional API like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'my_mulit_layer')>,\n",
       " <KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'my_mulit_layer')>,\n",
       " <KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'my_mulit_layer')>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs1 = tf.keras.layers.Input(shape=[2])\n",
    "inputs2 = tf.keras.layers.Input(shape=[2])\n",
    "MyMulitLayer()((inputs1, inputs2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `call()` method receives symbolic inputs, and it returns symbolic outputs. The shapes are only partially specified at this stage: we don't know the batch size, which is why the first dimension is `None`.\n",
    "\n",
    "We can also pass actual data to the custom layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[ 9., 18.],\n",
       "        [ 6., 10.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[18., 72.],\n",
       "        [ 8., 21.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       " array([[0.5      , 0.5      ],\n",
       "        [0.5      , 2.3333333]], dtype=float32)>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1, X2 = np.array([[3., 6.], [2., 7.]]), np.array([[6., 12.], [4., 3.]])\n",
    "MyMulitLayer()((X1, X2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a layer with a different behaviour during training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGaussianNoise(tf.keras.layers.Layer):\n",
    "    def __init__(self, stddev,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 2s 4ms/step - loss: 2.2197 - val_loss: 25.3962\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 1.4133 - val_loss: 17.3022\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 1.1056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1055686473846436"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    MyGaussianNoise(stddev=1.0, input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom model example: The inputs go through a first dense layer, then through a *residual block* composed of two dense layers and an addition operation (a residual block adds its inputs to its outputs), then through this same residual block three more times, then through a seccond residual block, and the final result goes through a dense output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [tf.keras.layers.Dense(n_neurons, activation=\"relu\",\n",
    "                                             kernel_initializer=\"he_normal\")\n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer is a bit special since it contains other layers. This is handled transparently by Keras: it automatically detects that the hidden attribute contains trackable objects (layers in this case), so their variables are automatically added to this layer's list of variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualRegressor(tf.keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = tf.keras.layers.Dense(30, activation=\"relu\",\n",
    "                                             kernel_initializer=\"he_normal\")\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1 + 3):\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 4s 7ms/step - loss: 30.7860\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 1.3682\n",
      "162/162 [==============================] - 0s 2ms/step - loss: 1.1461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_17_layer_call_fn, dense_17_layer_call_and_return_conditional_losses, dense_18_layer_call_fn, dense_18_layer_call_and_return_conditional_losses, dense_19_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_custom_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_custom_model\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(42)\n",
    "model = ResidualRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2)\n",
    "score = model.evaluate(X_test_scaled, y_test)\n",
    "model.save(\"my_custom_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "363/363 [==============================] - 4s 8ms/step - loss: 1.0766\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.9895\n",
      "1/1 [==============================] - 0s 81ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.6306203],\n",
       "       [1.3134364],\n",
       "       [4.16115  ]], dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"my_custom_model\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2)\n",
    "model.predict(X_test_scaled[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have defined the model using the sequential API instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)\n",
    "block1 = ResidualBlock(2, 30)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    block1, block1, block1, block1, block1,\n",
    "    ResidualBlock(2, 30),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, you can naturally and concisely build almost any model that you find in a paper, using the sequential API, the functional API, the subclassing API, or even a mix of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses and Metrics based on Model Internals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a custom loss based on model internals, compute it based on any part of the model you want, then pass the result to the `add_loss()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructingRegressor(tf.keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [tf.keras.layers.Dense(30, activation=\"relu\",\n",
    "                                             kernel_initializer=\"he_normal\")]\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "        self.reconstruction_mean = tf.keras.metrics.Mean(\n",
    "            name=\"reconstruction_error\")\n",
    "        \n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = tf.keras.layers.Dense(n_inputs)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss)\n",
    "        if training:\n",
    "            result = self.reconstruction_mean(recon_loss)\n",
    "            self.add_metric(result)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through this code:\n",
    "\n",
    "- The constructor creates the DNN with five dense hidden layers and one dense output layer. We also create a `Mean` streaming metric to keep track of the recosntruction error during training.\n",
    "- The `build()` method creates an extra dense layer that will be used to reconstruct the inputs of the model. It must be created here because its number of units must be equal to the number of inputs, and this number is unknown before `build()` method is called.\n",
    "- The `call()` method processes the inputs through all five hidden layers, then passes the result through the reconstruction layer, which produces the reconstruction.\n",
    "- Then the `call()` method computes the reconstruction loss (the mean squared difference between the reconstruction and the inputs), and adds it to the model's list of losses using the `add_loss()` method. Notice that we scale down the reconstruction loss by multiplying it by 0.05 (this is a hyperparameter you can tune). This ensures that the reconstruction loss does not dominate the main loss.\n",
    "- Next, during the training only, the `call()` method updates the reconstruction metric and adds it to the model so it can be displayed. This code example can actually be simplified by calling `self.add_metric(recon_loss)` instead: Keras will automatically track the mean for you.\n",
    "- Finally, the `call()` method passes the output of the hidden layers to the output layer and returns its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 2s 4ms/step - loss: 2.2356 - reconstruction_error: 1.3610\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.8526 - reconstruction_error: 0.5407\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.6422 - reconstruction_error: 0.3985\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.5297 - reconstruction_error: 0.3117\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4739 - reconstruction_error: 0.2534\n",
      "162/162 [==============================] - 0s 801us/step\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(42)\n",
    "model = ReconstructingRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=5)\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Gradients Using Autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + 2 * w1 * w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the partial derivative for *f* at (5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.000003007075065"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1, w2 = 5, 3\n",
    "eps = 1e-6\n",
    "(f(w1 + eps, w2) - f(w1, w2)) / eps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.000000003174137"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(f(w1, w2 + eps) - f(w1, w2)) / eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having to call `f()` at least once per parameter makes this approach intractable for large neural networks. So, instead, we should use reverse-mode autodiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define two variables `w1` and `w2`, then we create a `tf.GradientTape` context that will automatically record every operation that involves a variable, and finally we ask this tape to compute the gradients of the result `z` with regard to both variables `[w1, w2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tape is automatically erased immediately after you call its `gradient()` method, so you will get an exception if you try to call `gradient()` twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1)  # returns tensor 36.0\n",
    "try:\n",
    "    dz_dw2 = tape.gradient(z, w2)  # raises a RuntimeError!\n",
    "except RuntimeError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to call `gradient()` more than once, you must make the tape persistence and delete it each time you are done with it to free resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1)  # returns tensor 36.0\n",
    "dz_dw2 = tape.gradient(z, w2)  # returns tensor 10.0, works fine now!\n",
    "del tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz_dw1, dz_dw2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the tape will only track operations involving variables, so if you try to compute the gradient of `z` with regard to anything other than a variable, the result will be None:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you can force the tape to watch any tensors you like, to record every operation that involves them. You can then compute gradients with regard to these tensors, as if they were variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1)\n",
    "    tape.watch(c2)\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, c2])  # returns [tensor 36., tensor 10.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be useful in some cases, like if you want to implement a regularization loss that penalizes activations that vary a lot when the inputs vary little: the loss will be based on the gradient of the activations with regard to the inputs. Since the inputs are not variables, you'll need to tell the tape to watch them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=136.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=30.0>]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if given a vector, tape.gradient() will compute the gradient of the vector's sum\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z1 = f(w1, w2 + 2.)\n",
    "    z2 = f(w1, w2 + 5.)\n",
    "    z3 = f(w1, w2 + 7.)\n",
    "\n",
    "tape.gradient([z1, z2, z3], [w1, w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=136.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=30.0>]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shows that we get the same result as the previous cell\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z1 = f(w1, w2 + 2.)\n",
    "    z2 = f(w1, w2 + 5.)\n",
    "    z3 = f(w1, w2 + 7.)\n",
    "    z = z1 + z2 + z3\n",
    "\n",
    "tape.gradient(z, [w1, w2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you try to compute the gradients of a vector, for example a vector containing multiple losses, then Tensorflow will compute the gradients of the vector's sum. So if you ever need to get the individual gradients (e.g., the gradients of each loss with regard to the model parameters), you must call the tape's `jacobian()` method: it will perform reverse-mode autodiff once for each loss in the vector (all in parallel by default). It is even possible to compute second-order partial derivatives (the Hessians, i.e., the partial derivatives of the partial derivatives), but this is rarely needed in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows how to compute the jacobians and the hessians\n",
    "with tf.GradientTape(persistent=True) as hessian_tape:\n",
    "    with tf.GradientTape() as jacobian_tape:\n",
    "        z = f(w1, w2)\n",
    "    jacobians = jacobian_tape.gradient(z, [w1, w2])\n",
    "\n",
    "hessians = [hessian_tape.gradient(jacobian, [w1, w2])\n",
    "            for jacobian in jacobians]\n",
    "del hessian_tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacobians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<tf.Tensor: shape=(), dtype=float32, numpy=6.0>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.0>],\n",
       " [<tf.Tensor: shape=(), dtype=float32, numpy=2.0>, None]]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hessians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases you may want to stop gradients from backpropagating through some part of you neural network. To do this, you must use the `tf.stop_gradient()` function. The function returns its inputs during the forward pass (like `tf.identity()`), but it does not let gradients through during backpropagation (it acts like a constant):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)  # same result as without stop_gradient()\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you may occasionally run into some numerical issues when computing gradients. For example, if you compute the gradients of the square root function at $x = 10^{-50}$, the result will be infinite. In reality, the slope at that point is not infinite, but it's more than 32-bits floats can handle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=inf>]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(1e-50)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = tf.sqrt(x)\n",
    "\n",
    "tape.gradient(z, [x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this, it's often a good idea to add a tiny value *x* (such as $10^{-6}$) when computing its square root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softplus(z):\n",
    "    return tf.math.log(1 + tf.exp(-tf.abs(z))) + tf.maximum(0., z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the proof that this equation is equal to log(1 + exp(_z_)):\n",
    "* softplus(_z_) = log(1 + exp(_z_))\n",
    "* softplus(_z_) = log(1 + exp(_z_)) - log(exp(_z_)) + log(exp(_z_)) ; **just adding and subtracting the same value**\n",
    "* softplus(_z_) = log\\[(1 + exp(_z_)) / exp(_z_)\\] + log(exp(_z_)) ; **since log(_a_) - log(_b_) = log(_a_ / _b_)**\n",
    "* softplus(_z_) = log\\[(1 + exp(_z_)) / exp(_z_)\\] + _z_ ; **since log(exp(_z_)) = _z_**\n",
    "* softplus(_z_) = log\\[1 / exp(_z_) + exp(_z_) / exp(_z_)\\] + _z_ ; **since (1 + _a_) / _b_ = 1 / _b_ + _a_ / _b_**\n",
    "* softplus(_z_) = log\\[exp(–_z_) + 1\\] + _z_ ; **since 1 / exp(_z_) = exp(–z), and exp(_z_) / exp(_z_) = 1**\n",
    "* softplus(_z_) = softplus(–_z_) + _z_ ; **we recognize the definition at the top, but with –_z_**\n",
    "* softplus(_z_) = softplus(–|_z_|) + max(0, _z_) ; **if you consider both cases, _z_ < 0 or _z_ ≥ 0, you will see that this works**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some rare cases, a numerically stable function my still have numerically unstable gradients. In such cases, you will have to tell TensorFlow which equation to use for the gradients, rather than letting it use autodiff. For this, you must use the `@tf.custom_gradient` decorator when defining the function, and return both the function's usual result and a function that computes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def my_softplus(z):\n",
    "    def my_softplus_gradients(grads):  # grads = backprop'ed from upper layers\n",
    "        return grads * (1 - 1 / (1 + tf.exp(z)))  # stable grads of softplus\n",
    "    \n",
    "    result = tf.math.log(1 + tf.exp(-tf.abs(z))) + tf.maximum(0., z)\n",
    "    return result, my_softplus_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1000.], dtype=float32)>,\n",
       " [<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable([1000.])\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_softplus(x)\n",
    "\n",
    "z, tape.gradient(z, [x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Training Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless you're learning or you really need the extra flexibility, you should prefer using the `fit()` method rather than implementing your own training loop, especially if you work in a team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's build a simple model. There's no need to compile it, since we will handle the training loop manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_reg = tf.keras.regularizers.l2(0.05)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                          kernel_regularizer=l2_reg)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a tiny function that will randomly sample a batch of instances from the training se:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define a function that will display the training status, including the number of steps, the total number of steps, the mean loss since the start of the epoch (we will use the `Mean` metric to compute it), and other metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status_bar(step, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([f\"{m.name}: {m.result():.4f}\"\n",
    "                          for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if step < total else \"\\n\"\n",
    "    print(f\"\\r{step}/{total} - \" + metrics, end=end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some hyperparameters and choose the optimizer, the loss function, and the metrics (just the MAE in this example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.mean_squared_error\n",
    "mean_loss = tf.keras.metrics.Mean(name=\"mean_loss\")\n",
    "metrics = [tf.keras.metrics.MeanAbsoluteError()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we are ready to build the custom loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "233/362 - mean_loss: 6.9538 - mean_absolute_error: 1.7780"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362/362 - mean_loss: 6.6039 - mean_absolute_error: 1.7567\n",
      "Epoch 2/5\n",
      "362/362 - mean_loss: 5.3193 - mean_absolute_error: 1.7009\n",
      "Epoch 3/5\n",
      "362/362 - mean_loss: 4.2194 - mean_absolute_error: 1.5365\n",
      "Epoch 4/5\n",
      "362/362 - mean_loss: 3.2713 - mean_absolute_error: 1.3382\n",
      "Epoch 5/5\n",
      "362/362 - mean_loss: 2.3903 - mean_absolute_error: 1.1082\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch, training=True) \n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        # if your model has variable constraints\n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "            \n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        \n",
    "        print_status_bar(step, n_steps, mean_loss, metrics)\n",
    "\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x0000013A40A99EE0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\.conda\\envs\\tf\\lib\\site-packages\\tqdm\\std.py\", line 1149, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\HP\\.conda\\envs\\tf\\lib\\site-packages\\tqdm\\notebook.py\", line 278, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[155], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trange\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtrange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAll epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m epochs:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m epochs:\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m trange(\u001b[38;5;241m1\u001b[39m, n_steps \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m steps:\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\tf\\lib\\site-packages\\tqdm\\notebook.py:311\u001b[0m, in \u001b[0;36mtnrange\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtnrange\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    310\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Shortcut for `tqdm.notebook.tqdm(range(*args), **kwargs)`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tqdm_notebook(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39margs), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\tf\\lib\\site-packages\\tqdm\\notebook.py:233\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m unit_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    232\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m*\u001b[39m unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n\u001b[1;32m--> 233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer\u001b[38;5;241m.\u001b[39mpbar \u001b[38;5;241m=\u001b[39m proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\tf\\lib\\site-packages\\tqdm\\notebook.py:108\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[0;32m    110\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m IProgress(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mtotal)\n",
      "\u001b[1;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "# shows how to use the tqdm package to display nice progress bars\n",
    "\n",
    "from tqdm.notebook import trange\n",
    "from collections import OrderedDict\n",
    "with trange(1, n_epochs + 1, desc=\"All epochs\") as epochs:\n",
    "    for epoch in epochs:\n",
    "        with trange(1, n_steps + 1, desc=f\"Epoch {epoch}/{n_epochs}\") as steps:\n",
    "            for step in steps:\n",
    "                X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "                with tf.GradientTape() as tape:\n",
    "                    y_pred = model(X_batch)\n",
    "                    main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "                    loss = tf.add_n([main_loss] + model.losses)\n",
    "                \n",
    "                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "                for variable in model.variables:\n",
    "                    if variable.constraint is not None:\n",
    "                        variable.assign(variable.constraint(variable))\n",
    "                    \n",
    "                status = OrderedDict()\n",
    "                mean_loss(loss)\n",
    "                status[\"loss\"] = mean_loss.result().numpy()\n",
    "                for metric in metrics:\n",
    "                    metric(y_batch, y_pred)\n",
    "                    status[metric.name] = metric.result().numpy()\n",
    "                \n",
    "                steps.set_postfix(status)\n",
    "            \n",
    "        for metric in [mean_loss] + metrics:\n",
    "            metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cube(x):\n",
    "    return x ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.def_function.Function at 0x13a40c06280>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube = tf.function(cube)\n",
    "tf_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=8>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def tf_cube(x):\n",
    "    return x ** 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF Functions and Concrete Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConcreteFunction tf_cube(x) at 0x139073FB820>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function = tf_cube.get_concrete_function(tf.constant(2.0))\n",
    "concrete_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function is tf_cube.get_concrete_function(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Function Definitions and Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.func_graph.FuncGraph at 0x1390899ee80>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'x' type=Placeholder>,\n",
       " <tf.Operation 'pow/y' type=Const>,\n",
       " <tf.Operation 'pow' type=Pow>,\n",
       " <tf.Operation 'Identity' type=Identity>]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ops = concrete_function.graph.get_operations()\n",
    "ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'x:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'pow/y:0' shape=() dtype=float32>]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow_op = ops[2]\n",
    "list(pow_op.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'pow:0' shape=() dtype=float32>]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow_op.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'x' type=Placeholder>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function.graph.get_operation_by_name('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Identity:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function.graph.get_tensor_by_name('Identity:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"__inference_tf_cube_256781\"\n",
       "input_arg {\n",
       "  name: \"x\"\n",
       "  type: DT_FLOAT\n",
       "}\n",
       "output_arg {\n",
       "  name: \"identity\"\n",
       "  type: DT_FLOAT\n",
       "}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function.function_def.signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How TF Functions Trace Python Functions to Extract Their Computation Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def tf_cube(x):\n",
    "    print(f\"x = {x}\")\n",
    "    return x ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = Tensor(\"x:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "result = tf_cube(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 2\n"
     ]
    }
   ],
   "source": [
    "result = tf_cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 3\n"
     ]
    }
   ],
   "source": [
    "result = tf_cube(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = Tensor(\"x:0\", shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "result = tf_cube(tf.constant([[1., 2.]]))  # New shape: trace!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = Tensor(\"x:0\", shape=(2, 2), dtype=float32)\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function tf_cube at 0x0000013A3E583700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function tf_cube at 0x0000013A3E583700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "result = tf_cube(tf.constant([[3., 4.], [5., 6.]]))  # New shape: trace!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to specify a particular input signature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[tf.TensorSpec([None, 28, 28], tf.float32)])\n",
    "def shrink(images):\n",
    "    print(\"Tracing\", images)  # to show when tracing happens\n",
    "    return images[:, ::2, ::2]  # drop half the rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing Tensor(\"images:0\", shape=(None, 28, 28), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "img_batch_1 = tf.random.uniform(shape=[100, 28, 28])\n",
    "img_batch_2 = tf.random.uniform(shape=[50, 28, 28])\n",
    "preprocessed_images = shrink(img_batch_1)  # Works fine, traces the function\n",
    "preprocessed_images = shrink(img_batch_2)  # Works fine, same concrete function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Python inputs incompatible with input_signature:\n  inputs: (\n    tf.Tensor(\n[[[0.7413678  0.62854624]\n  [0.01738465 0.3431449 ]]\n\n [[0.51063764 0.3777541 ]\n  [0.07321596 0.02137029]]], shape=(2, 2, 2), dtype=float32))\n  input_signature: (\n    TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name=None)).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[183], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m img_batch_3 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 3\u001b[0m     preprocessed_images \u001b[38;5;241m=\u001b[39m \u001b[43mshrink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_batch_3\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# TypeError! Incompatible inputs\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ex)\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function_spec.py:531\u001b[0m, in \u001b[0;36m_convert_inputs_to_signature\u001b[1;34m(inputs, input_signature, flat_input_signature)\u001b[0m\n\u001b[0;32m    523\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen input_signature is provided, all inputs to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    524\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe Python function must be convertible to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    525\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    526\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformat_error_message(inputs,\u001b[38;5;250m \u001b[39minput_signature)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mis_compatible_with(other) \u001b[38;5;28;01mfor\u001b[39;00m spec, other \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[0;32m    529\u001b[0m     flat_input_signature,\n\u001b[0;32m    530\u001b[0m     flatten_inputs)):\n\u001b[1;32m--> 531\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython inputs incompatible with input_signature:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    532\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mformat_error_message(inputs,\u001b[38;5;250m \u001b[39minput_signature)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m need_packing:\n\u001b[0;32m    535\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(\n\u001b[0;32m    536\u001b[0m       structure\u001b[38;5;241m=\u001b[39minput_signature,\n\u001b[0;32m    537\u001b[0m       flat_sequence\u001b[38;5;241m=\u001b[39mflatten_inputs,\n\u001b[0;32m    538\u001b[0m       expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: Python inputs incompatible with input_signature:\n  inputs: (\n    tf.Tensor(\n[[[0.7413678  0.62854624]\n  [0.01738465 0.3431449 ]]\n\n [[0.51063764 0.3777541 ]\n  [0.07321596 0.02137029]]], shape=(2, 2, 2), dtype=float32))\n  input_signature: (\n    TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name=None))."
     ]
    }
   ],
   "source": [
    "img_batch_3 = tf.random.uniform(shape=[2, 2, 2])\n",
    "try:\n",
    "    preprocessed_images = shrink(img_batch_3)  # TypeError! Incompatible inputs\n",
    "except TypeError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Autograph to Capture Control Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how does TensorFlow generate graphs? It starts by analyzing the Python function's soure code to capture all the control flow statements, such as `for` loops, `while` loops, and `if` statements, as well as `break`, `continue`, and `return` statements. This first step is called *AutoGraph*. The reason TensorFlow has to analyze the source code is that Python does not provide any other way to capture control flow statements:it offers magic methods like `__add__()` and `__mul__()` to capture operators like + and *, but there are no `__while__()` or `__if__()` magic methods. After analyzing the function's code, Autograph outputs an upgraded version of that function in which all the control flow statements are replaced by the appropriate TensorFlow operations such as `tf.while_loop()` for loops and `tf.cond()` for if statements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"static\" `for` loop using `range()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def add_10(x):\n",
    "    for i in range(10):\n",
    "        x += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=15>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_10(tf.constant(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'x' type=Placeholder>,\n",
       " <tf.Operation 'add/y' type=Const>,\n",
       " <tf.Operation 'add' type=AddV2>,\n",
       " <tf.Operation 'add_1/y' type=Const>,\n",
       " <tf.Operation 'add_1' type=AddV2>,\n",
       " <tf.Operation 'add_2/y' type=Const>,\n",
       " <tf.Operation 'add_2' type=AddV2>,\n",
       " <tf.Operation 'add_3/y' type=Const>,\n",
       " <tf.Operation 'add_3' type=AddV2>,\n",
       " <tf.Operation 'add_4/y' type=Const>,\n",
       " <tf.Operation 'add_4' type=AddV2>,\n",
       " <tf.Operation 'add_5/y' type=Const>,\n",
       " <tf.Operation 'add_5' type=AddV2>,\n",
       " <tf.Operation 'add_6/y' type=Const>,\n",
       " <tf.Operation 'add_6' type=AddV2>,\n",
       " <tf.Operation 'add_7/y' type=Const>,\n",
       " <tf.Operation 'add_7' type=AddV2>,\n",
       " <tf.Operation 'add_8/y' type=Const>,\n",
       " <tf.Operation 'add_8' type=AddV2>,\n",
       " <tf.Operation 'add_9/y' type=Const>,\n",
       " <tf.Operation 'add_9' type=AddV2>,\n",
       " <tf.Operation 'Identity' type=Identity>]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_10.get_concrete_function(tf.constant(5)).graph.get_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"dynamic\" loop using `tf.while_loop()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows how to use tf.while_loop (usually @tf.function is simpler)\n",
    "@tf.function\n",
    "def add_10(x):\n",
    "    condition = lambda i, x: tf.less(i, 10)\n",
    "    body = lambda i, x: (tf.add(i, 1), tf.add(x, 1))\n",
    "    final_i, final_x = tf.while_loop(condition, body, [tf.constant(0), x])\n",
    "    return final_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=15>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_10(tf.constant(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'x' type=Placeholder>,\n",
       " <tf.Operation 'Const' type=Const>,\n",
       " <tf.Operation 'while/maximum_iterations' type=Const>,\n",
       " <tf.Operation 'while/loop_counter' type=Const>,\n",
       " <tf.Operation 'while' type=StatelessWhile>,\n",
       " <tf.Operation 'Identity' type=Identity>]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_10.get_concrete_function(tf.constant(5)).graph.get_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"dynamic\" `for` loop using `tf.range()` (captured by autograph):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def add_10(x):\n",
    "    for i in tf.range(10):\n",
    "        x = x + 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'x' type=Placeholder>,\n",
       " <tf.Operation 'range/start' type=Const>,\n",
       " <tf.Operation 'range/limit' type=Const>,\n",
       " <tf.Operation 'range/delta' type=Const>,\n",
       " <tf.Operation 'range' type=Range>,\n",
       " <tf.Operation 'sub' type=Sub>,\n",
       " <tf.Operation 'floordiv' type=FloorDiv>,\n",
       " <tf.Operation 'mod' type=FloorMod>,\n",
       " <tf.Operation 'zeros_like' type=Const>,\n",
       " <tf.Operation 'NotEqual' type=NotEqual>,\n",
       " <tf.Operation 'Cast' type=Cast>,\n",
       " <tf.Operation 'add' type=AddV2>,\n",
       " <tf.Operation 'zeros_like_1' type=Const>,\n",
       " <tf.Operation 'Maximum' type=Maximum>,\n",
       " <tf.Operation 'while/maximum_iterations' type=Const>,\n",
       " <tf.Operation 'while/loop_counter' type=Const>,\n",
       " <tf.Operation 'while' type=StatelessWhile>,\n",
       " <tf.Operation 'Identity' type=Identity>]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_10.get_concrete_function(tf.constant(0)).graph.get_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Variables and Other Resources in TF Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=2>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = tf.Variable(0)\n",
    "\n",
    "@tf.function\n",
    "def increment(counter, c=1):\n",
    "    return counter.assign_add(c)\n",
    "\n",
    "increment(counter)  # counter is now equal to 1\n",
    "increment(counter)  # counter is now equal to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"counter\"\n",
       "type: DT_RESOURCE"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_def = increment.get_concrete_function(counter).function_def\n",
    "function_def.signature.input_arg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Counter:\n",
    "    def __init__(self):\n",
    "        self.counter = tf.Variable(0)\n",
    "    \n",
    "    @tf.function\n",
    "    def increment(self, c=1):\n",
    "        return self.counter.assign_add(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=2>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Counter()\n",
    "c.increment()\n",
    "c.increment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def tf__add(x):\n",
      "    with ag__.FunctionScope('add_10', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n",
      "        do_return = False\n",
      "        retval_ = ag__.UndefinedReturnValue()\n",
      "\n",
      "        def get_state():\n",
      "            return (x,)\n",
      "\n",
      "        def set_state(vars_):\n",
      "            nonlocal x\n",
      "            (x,) = vars_\n",
      "\n",
      "        def loop_body(itr):\n",
      "            nonlocal x\n",
      "            i = itr\n",
      "            x = ag__.ld(x)\n",
      "            x += 1\n",
      "        i = ag__.Undefined('i')\n",
      "        ag__.for_stmt(ag__.converted_call(ag__.ld(tf).range, (10,), None, fscope), None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'i'})\n",
      "        try:\n",
      "            do_return = True\n",
      "            retval_ = ag__.ld(x)\n",
      "        except:\n",
      "            do_return = False\n",
      "            raise\n",
      "        return fscope.ret(retval_, do_return)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def add_10(x):\n",
    "    for i in tf.range(10):\n",
    "        x += 1\n",
    "    return x\n",
    "\n",
    "print(tf.autograph.to_code(add_10.python_function))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows how to display autograph code with syntax highlighting\n",
    "def display_tf_code(func):\n",
    "    from IPython.display import display, Markdown\n",
    "    if hasattr(func, \"python_function\"):\n",
    "        func = func.python_function\n",
    "    code = tf.autograph.to_code(func)\n",
    "    display(Markdown(f'```python\\n{code}\\n```'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def tf__add(x):\n",
       "    with ag__.FunctionScope('add_10', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n",
       "        do_return = False\n",
       "        retval_ = ag__.UndefinedReturnValue()\n",
       "\n",
       "        def get_state():\n",
       "            return (x,)\n",
       "\n",
       "        def set_state(vars_):\n",
       "            nonlocal x\n",
       "            (x,) = vars_\n",
       "\n",
       "        def loop_body(itr):\n",
       "            nonlocal x\n",
       "            i = itr\n",
       "            x = ag__.ld(x)\n",
       "            x += 1\n",
       "        i = ag__.Undefined('i')\n",
       "        ag__.for_stmt(ag__.converted_call(ag__.ld(tf).range, (10,), None, fscope), None, loop_body, get_state, set_state, ('x',), {'iterate_names': 'i'})\n",
       "        try:\n",
       "            do_return = True\n",
       "            retval_ = ag__.ld(x)\n",
       "        except:\n",
       "            do_return = False\n",
       "            raise\n",
       "        return fscope.ret(retval_, do_return)\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_tf_code(add_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TF Functions with tf.Keras (or Not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, tf.keras will automatically convert your custom code into TF Functions, no need to use `tf.function()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function\n",
    "def my_mse(y_true, y_pred):\n",
    "    print(\"Tracing loss my_mse()\")\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom metric function\n",
    "def my_mae(y_true, y_pred):\n",
    "    print(\"Tracing metric my_mae()\")\n",
    "    return tf.reduce_mean(tf.abs(y_pred - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom layer\n",
    "class MyDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(input_shape[1], self.units),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.biases = self.add_weight(name='bias',\n",
    "                                      shape=(self.units,),\n",
    "                                      initializer='zeros',\n",
    "                                      trainable=True)\n",
    "    \n",
    "    def call(self, X):\n",
    "        print(\"Tracing MyDense.call()\")\n",
    "        return self.activation(X @ self.kernel + self.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom model\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = MyDense(30, activation=\"relu\")\n",
    "        self.hidden2 = MyDense(30, activation=\"relu\")\n",
    "        self.output_ = MyDense(1)\n",
    "    \n",
    "    def call(self, input):\n",
    "        print(\"Tracing MyModel.call()\")\n",
    "        hidden1 = self.hidden1(input)\n",
    "        hidden2 = self.hidden2(input)\n",
    "        concat = tf.keras.layers.concatenate([input, hidden2])\n",
    "        output = self.output_(concat)\n",
    "        return output\n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=my_mse, optimizer=\"nadam\", metrics=[my_mae])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model/my_dense_2/kernel:0', 'my_model/my_dense_2/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model/my_dense_2/kernel:0', 'my_model/my_dense_2/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing metric my_mae()\n",
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model/my_dense_2/kernel:0', 'my_model/my_dense_2/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model/my_dense_2/kernel:0', 'my_model/my_dense_2/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing metric my_mae()\n",
      "350/363 [===========================>..] - ETA: 0s - loss: 1.9840 - my_mae: 1.0268Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "Tracing metric my_mae()\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 1.9382 - my_mae: 1.0107 - val_loss: 0.5619 - val_my_mae: 0.5464\n",
      "Epoch 2/2\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5041 - my_mae: 0.5170 - val_loss: 0.8437 - val_my_mae: 0.4905\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.4504 - my_mae: 0.4893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4504045844078064, 0.489301472902298]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can turn this off by creating the model with `dynamic=True` (or calling `super().__init__(dynamic=True, **kwargs)` in the model's constructor):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(dynamic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=my_mse, optimizer=\"nadam\", metrics=[my_mae])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the custom code will be called at each iteration. Let's fit, validate and evaluate with tiny datasets to avoid getting too much output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_1/my_dense_5/kernel:0', 'my_model_1/my_dense_5/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_1/my_dense_5/kernel:0', 'my_model_1/my_dense_5/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing metric my_mae()\n",
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_1/my_dense_5/kernel:0', 'my_model_1/my_dense_5/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_1/my_dense_5/kernel:0', 'my_model_1/my_dense_5/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing metric my_mae()\n",
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "Tracing metric my_mae()\n",
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "Tracing metric my_mae()\n",
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "Tracing metric my_mae()\n",
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "Tracing metric my_mae()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.543581008911133, 2.059741497039795]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled[:64], y_train[:64], epochs=1,\n",
    "          validation_data=(X_valid_scaled[:64], y_valid[:64]), verbose=0)\n",
    "model.evaluate(X_test_scaled[:64], y_test[:64], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can compile a model with `run_eagerly=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=my_mse, optimizer=\"nadam\", metrics=[my_mae], run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_2/my_dense_8/kernel:0', 'my_model_2/my_dense_8/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_2/my_dense_8/kernel:0', 'my_model_2/my_dense_8/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing metric my_mae()\n",
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_2/my_dense_8/kernel:0', 'my_model_2/my_dense_8/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['my_model_2/my_dense_8/kernel:0', 'my_model_2/my_dense_8/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing metric my_mae()\n",
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "Tracing metric my_mae()\n",
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "Tracing metric my_mae()\n",
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "Tracing metric my_mae()\n",
      "Tracing MyModel.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing MyDense.call()\n",
      "Tracing loss my_mse()\n",
      "Tracing metric my_mae()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.543581008911133, 2.059741497039795]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled[:64], y_train[:64], epochs=1,\n",
    "          validation_data=(X_valid_scaled[:64], y_valid[:64]), verbose=0)\n",
    "model.evaluate(X_test_scaled[:64], y_test[:64], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF Functions Rules\n",
    "\n",
    "* If you call any external library, including NumPy or even the standard library, this call will run only during tracing; it will not be part of the graph. Indeed, a TensorFlow graph can only include TensorFlow constructs (tensors, operations, variables, datasets, and so on). So, make sure you use `tf.reduce_sum()` instead of `np.sum()`, `tf.sort()` instead of the built-in `sorted()` function, and so on (unless you really want the code to run only during tracing). This has a few additional implications:\n",
    "    - If you define a TF function `f(x)` that just returns `n.random.rand()`, a random number will only be generated when the function is traced, so `f(tf.constant(2.))` and `f(tf.constant(3.))` will return a different one. If you replace `np.random.rand()` with `tf.random.uniform([])`, then a new random number will be generated upon every call, since the operation will be part of the graph.\n",
    "    - If your non-TensorFlow code has side effects (such as logging something or updating a Python counter), then you should not expect those side-effects to occur everytime you call the TF function, as they will only occur when the function is traced.\n",
    "    - You can wrap arbitrary Python code in a `tf.py_function()` operation, but doing so will hinder performance, as TensorFlow will not be able to do any graph optimization on this code. It will also reduce portability, as the graph will only run on platforms where Python is available (and where the right libraries are installed).\n",
    "* You can call other Python functions or TF functions, but they should follow the same rules, as TensorFlow will capture their operations in the computation graph. Note that these other functions do not need to be decorated with `@tf.function`.\n",
    "* If the function creates a TensorFlow variable (or any other stateful TensorFlow object, such as a dataset or a queue), it must do so upon the very first call, and only then, or else you will get an exception. It is usually preferable to create variables outside of the TF function (e.g., in the `build()` method of a custom layer). If you want to assign a new value to the variable, make sure you call its `assign()` method instead of using the = operator.\n",
    "* The source code of your Python function should be available to TensorFlow. If the source code is unavailable (for example, if you define your function in the Python shell, which does not give access to the source code, or if you deploy only the compiled _*.pyc_ Python files to production), then the graph generation process will fail or have limited functionality.\n",
    "* TensorFlow will only capture for loops that iterate over a tensor or a `tf.data.Dataset`. Therefore, make sure you use `for i in tf.range(x)` rather than `for i in range(x)`, or else the loop will not be captured in the graph. Instead, it will run during tracing. (This may be what you want if the ``for loop is meant to build the graph; for example, to create each layer in a neural network.)\n",
    "* As always, for performance reasons, you should prefer a vectorized implementation whenever you can rather than using loops. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMomentumOptimizer(tf.keras.optimizers.Optimizer):\n",
    "    def __init__(self, learning_rate=0.001, momentum=0.9, name=\"MyMomentumOptimizer\", **kwargs):\n",
    "        \"\"\"Gradient descent with momentum optimizer\"\"\"\n",
    "        super().__init__(name, **kwargs)\n",
    "        self._learning_rate= self._build_learning_rate(learning_rate)\n",
    "        self.momentum = momentum\n",
    "    \n",
    "    def build(self, var_list):\n",
    "        \"\"\"Initialize optimizer variables\n",
    "        \n",
    "        Args:\n",
    "            var_list: list of model variables to build SGD variables on.\n",
    "        \"\"\"\n",
    "        super().build(var_list)\n",
    "        if getattr(self, \"_built\", False):\n",
    "            return\n",
    "        self.momentums = []\n",
    "        for var in var_list:\n",
    "            self.momentums.append(\n",
    "                self.add_variable_from_reference(\n",
    "                    model_variable=var, variable_name=\"m\"\n",
    "                )\n",
    "            )\n",
    "        self._built = True\n",
    "    \n",
    "    def update_step(self, gradient, variable):\n",
    "        \"\"\"Update step given gradient and the associated model variable\"\"\"\n",
    "        lr = tf.cast(self._learning_rate, variable.dtype)\n",
    "        m = None\n",
    "        var_key = self._var_key(variable)\n",
    "        momentum = tf.cast(self.momentum, variable.dtype)\n",
    "        m = self.momentums[self._index_dict[var_key]]\n",
    "        if m is None:\n",
    "            variable.assign_add(-gradient * lr)\n",
    "        else:\n",
    "            m.assign(-gradient * lr + m * momentum)\n",
    "            variable.assign_add(m)\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        print(\"Config!\")\n",
    "        return {\n",
    "            **base_config,\n",
    "            \"learning_rate\": self._serialize_hyperparameter(self.learning_rate),\n",
    "            \"momentum\": self.momentum\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MyMomentumOptimizer' object has no attribute '_build_learning_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[215], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mMyMomentumOptimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mset_random_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m8\u001b[39m])])\n",
      "Cell \u001b[1;32mIn[214], line 5\u001b[0m, in \u001b[0;36mMyMomentumOptimizer.__init__\u001b[1;34m(self, learning_rate, momentum, name, **kwargs)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Gradient descent with momentum optimizer\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_learning_rate\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_learning_rate\u001b[49m(learning_rate)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;241m=\u001b[39m momentum\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\tf\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py:975\u001b[0m, in \u001b[0;36mOptimizerV2.__getattribute__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hyper:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_hyper(name)\n\u001b[1;32m--> 975\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\HP\\.conda\\envs\\tf\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py:965\u001b[0m, in \u001b[0;36mOptimizerV2.__getattribute__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Overridden to support hyperparameter access.\"\"\"\u001b[39;00m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;66;03m# Needed to avoid infinite recursion with __setattr__.\u001b[39;00m\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hyper\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MyMomentumOptimizer' object has no attribute '_build_learning_rate'"
     ]
    }
   ],
   "source": [
    "optimizer = MyMomentumOptimizer()\n",
    "\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=[8])])\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "model.fit(X_train_scaled, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. to 11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. TensorFlow is an open-source library for numerical computation, particularly well suited and fine-tuned for large-scale Machine Learning. Its core is similar to NumPy, but it also features GPU support, support for distributed computing, computation graph analysis and optimization capabilities (with a portable graph format that allows you to train a TensorFlow model in one environment and run it in another), an optimization API base on reverse-mode autodiff, and several powerful API such as tf.keras, tf.data, tf.image, tf.signal, and more. Other popular Deep Learning libraries include PyTorch, MXNet, Microsoft Cognitive Toolkit, Theano, Caffe2, and Chainer.\n",
    "2. Although TensorFlow offers most of the functionalities provided by NumPy, it is not a drop-in replacement, for a few reasons. First, the names of the functions are not always the same (for example, `tf.reduce_sum()` versus `np.sum()`). Second, some functions do not behave in exactly the same way (for example, `tf.transpose()` creates a transposed copy of a tensor, while NumPy's `T` attribute creates a transposed view, without actually copying any data). Lastly, NumPy arrays are mutable, while TensorFlow tensors are not (but you can use a `tf.Variable` if you need a mutable object).\n",
    "3. Both `tf.range(10)` and `tf.constant(np.arange(10))` return a one-dimensional tensor containing the integers 0 to 9. However, the former uses 32-bit integers while the latter uses 64-bit integers. Indeed, TensorFlow defaults to 32 bits, while NumPy defaults to 64 bits.\n",
    "4. Beyond regular tensors, TensorFlow offers several other data structures, including sparse tensors, tensor arrays, ragged tensors, queues, string tensors, and sets. The last two are actually represented as regular tensors, but TensorFlow provides special functions to manipulate them (in `tf.strings` and `tf.sets`).\n",
    "5. When you want to define a custom loss function, in general you can just implement it as a regular Python function. However, if your custom loss function must support some hyperparameter (or any other state), then you should subclass the `keras.losses.Loss` class and implement the `__init__()` and `call()` methods. If you want the loss function's hyperparameters to be saved along with the model, then you must also implement the `get_config()` method.\n",
    "6. Much like custom loss function, most metrics can be defined as regular Python functions. But if you want your custom metric to support some hyperparameters (or any state), then you should subclass the `keras.metrics.Metric` class. Moreover, if computing the metric over a whole epoch is not equivalent to computing the mean metric over all batches in that epoch (e.g., as for the precision and recall metrics), then you should subclass the  `keras.metrics.Metric` class and implement the `__init__()`, `update_state()`, and `result()` methods to keep track of a running metric during each epoch. You should also implement the `reset_states()` method unless all it needs to do is reset all variables to 0.0. If you want the state to be saved along with the model, then you should implement the `get_config()` method as well.\n",
    "7. You should distinguish the internal components of your model (i.e., layers or reusable blocks of layers) from the model itself (i.e., the object that you will train). The former should subclass the `keras.layers.Layer` class, while the latter should subclass the `keras.models.Model` class.\n",
    "8. Writing your own custom training loop is fairly advanced, so you should only do it if you really need to. Keras provides several tools to customize training without having to write a custom training loop: callbacks, custom regularizers, custom constraints, custom losses, and so on. You should use these instead of writing a custom training loop whenever possible: writing a custom training loop is more error-prone, and it will be harder to reuse the custom code you write. However, in some cases writing a custom training loop is necessay- for example, if you want to use different optimizers for different parts of your neural network, like in the [Wide & Deep paper](https://homl.info/widedeep). A custom training loop can also be useful when debugging, or when trying to understand exactly how training works.\n",
    "9. Custom Keras components should be convertible to TF Functions, which means they should stick to TF operations as much as possible and respect all the rules listed in the _TF Function rules section_. If you absolutely need to include arbitrary Python code in a custom component, you can either wrap it in a `tf.py_function()` operation (but this will reduce performance and limit your model's portability) or set `dynamic=True` when creating the custom layer or model (or set `run_eagerly=True` when calling the model's `compile()` method).\n",
    "10. Rules are given above\n",
    "11. Creating a dynamic Keras model can be useful for debugging, as it will not compile any custom component to a TF Function, and you can use any Python debugger to debug your code. It can also be useful if you want to include arbitrary Python code in your model (or in your training code), including calls to external libraries. To make a model dynamic, you must set `dynamic=True` when creating it. Alternatively, you can set `run_eagerly=True` when calling the model's `compile()` method. Making a model dynamic prevents Keras from using any of TensorFlow's graph features, so it will slow down training and inference, and you will not have the possibility to export the computation graph, which will limit your model's portability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a custom layer that performs _layer normalization_:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `build()` method should define two trainable weights $\\mathbf{\\alpha}$ and $\\mathbf{\\beta}$, both of shape `input_shape[-1:]` and data type `tf.float32`, $\\mathbf{\\alpha}$ should be initialized with 1s, and $\\mathbf{\\beta}$ with 0s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `call()` method should compute the mean $\\mu$ and standard deviation $\\sigma$ of each instance's features. For this, you can use `tf.nn.moments(inputs, axes=-1, keepdims=True)`, which returns the mean $\\mu$ and the variance $\\sigma ^ 2$ of all instances (compute the square root of the variance to get the standard deviation). Then the function should compute and return $\\mathbf{\\alpha} \\otimes (\\mathbf{X} - \\mu) / (\\sigma + \\epsilon) + \\mathbf{\\beta}$, where $\\otimes$ represents itemwise multiplication (*) and $\\epsilon$ is the smoothing term (a small constant to avoid division by zero, e.g., 0.001). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(tf.keras.layers.Layer):\n",
    "    def __init__(self, eps=0.001, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def build(self, batch_input_shape):\n",
    "        self.alpha = self.add_weight(\n",
    "            name=\"alpha\", shape=batch_input_shape[-1:],\n",
    "            initializer=\"ones\")\n",
    "        self.beta = self.add_weight(\n",
    "            name=\"beta\", shape=batch_input_shape[-1:],\n",
    "            initializer=\"zeros\",)\n",
    "        \n",
    "    def call(self, X):\n",
    "        mean, variance = tf.nn.moments(X, axes=-1, keepdims=True)\n",
    "        return self.alpha * (X - mean) / (tf.sqrt(variance) + self.eps) + self.beta\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"eps\": self.eps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that making $\\epsilon$ a hyperparameter (`eps`) was not compulsory. Also note that it's preferable to compute `tf.sqrt(variance + self.epsilon)` rather than `tf.sqrt(variance) + self.eps`. Indeed, the derivative of sqrt(z) is undefined when z=0, so training will bomb whenever the variance vector has at least one component equal to 0. Adding $\\epsilon$ within the square root guarantees that this will never happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that your custom layer produces the same (or very nearly the same) output as the `tf.keras.layers.LayerNormalization` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create one instance of each class, apply the to some data (e.g., the training set), and ensure that the difference is negligeable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.1146584e-06>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X_train.astype(np.float32)\n",
    "\n",
    "custom_layer_norm = LayerNormalization()\n",
    "keras_layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "tf.reduce_mean(tf.keras.losses.mean_absolute_error(\n",
    "    keras_layer_norm(X), custom_layer_norm(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's close enough. To be extra sure, let's make alpha and beta completely random and compare again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.511791e-07>"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.set_random_seed(42)\n",
    "random_alpha = np.random.rand(X.shape[-1])\n",
    "random_beta = np.random.rand(X.shape[-1])\n",
    "\n",
    "custom_layer_norm.set_weights([random_alpha, random_beta])\n",
    "keras_layer_norm.set_weights([random_alpha, random_beta])\n",
    "\n",
    "tf.reduce_mean(tf.keras.losses.mean_absolute_error(\n",
    "    keras_layer_norm(X), custom_layer_norm(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still a negligeable difference! Our custom layer works fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Train a model using custom training loop to tackle the Fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the epoch, iteration, mean training loss, and mean accuracy over each epoch (updated at each iteration), as well as the validation loss and accuracy at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full.astype(np.float32) / 255.\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test.astype(np.float32) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n",
    "mean_loss = tf.keras.metrics.Mean()\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trange' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtrange\u001b[49m(\u001b[38;5;241m1\u001b[39m, n_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m epochs:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m epochs:\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m trange(\u001b[38;5;241m1\u001b[39m, n_steps \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m steps:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trange' is not defined"
     ]
    }
   ],
   "source": [
    "with trange(1, n_epochs + 1, desc=\"All epochs\") as epochs:\n",
    "    for epoch in epochs:\n",
    "        with trange(1, n_steps + 1, desc=f\"Epoch {epoch}/{n_epochs}\") as steps:\n",
    "            for step in steps:\n",
    "                X_batch, y_batch = random_batch(X_train, y_train)\n",
    "                with tf.GradientTape() as tape:\n",
    "                    y_pred = model(X_batch)\n",
    "                    main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "                    loss = tf.add_n([main_loss] + model.losses)\n",
    "                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                for variable in model.variables:\n",
    "                    if variable.constraint is not None:\n",
    "                        variable.assign(variable.constraint(variable))\n",
    "                status = OrderedDict()\n",
    "                mean_loss(loss)\n",
    "                status[\"loss\"] = mean_loss.result().numpy()\n",
    "                for metric in metrics:\n",
    "                    metric(y_batch, y_pred)\n",
    "                    status[metric.name] = metric.result().numpy()\n",
    "                steps.set_postfix(status)\n",
    "            y_pred = model(X_valid)\n",
    "            status[\"val_loss\"] = np.mean(loss_fn(y_valid, y_pred))\n",
    "            status[\"val_accuracy\"] = np.mean(tf.keras.metrics.sparse_categorical_accuracy(\n",
    "                tf.constant(y_valid, dtype=np.float32), y_pred))\n",
    "            status.set_postfix(status)\n",
    "        for metric in [mean_loss] + metrics:\n",
    "            metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using a different optimizer with a different learning rate for the upper layers and the lower layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_layers = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "])\n",
    "upper_layers = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "model = tf.keras.Sequential([\n",
    "    lower_layers, upper_layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
    "upper_optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n",
    "mean_loss = tf.keras.metrics.Mean()\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with trange(1, n_epochs + 1, desc=\"All epochs\") as epochs:\n",
    "    for epoch in epochs:\n",
    "        with trange(1, n_steps + 1, desc=f\"Epoch {epoch}/{n_epochs}\") as steps:\n",
    "            for step in steps:\n",
    "                X_batch, y_batch = random_batch(X_train, y_train)\n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    y_pred = model(X_batch)\n",
    "                    main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "                    loss = tf.add_n([main_loss] + model.losses)\n",
    "                for layers, optimizer in ((lower_layers, lower_optimizer),\n",
    "                                          (upper_layers, upper_optimizer)):\n",
    "                    gradients = tape.gradient(loss, layers.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(gradients, layers.trainable_variables))\n",
    "                del tape\n",
    "                for variable in model.variables:\n",
    "                    if variable.constraint is not None:\n",
    "                        variable.assign(variable.constraint(variable))                    \n",
    "                status = OrderedDict()\n",
    "                mean_loss(loss)\n",
    "                status[\"loss\"] = mean_loss.result().numpy()\n",
    "                for metric in metrics:\n",
    "                    metric(y_batch, y_pred)\n",
    "                    status[metric.name] = metric.result().numpy()\n",
    "                steps.set_postfix(status)\n",
    "            y_pred = model(X_valid)\n",
    "            status[\"val_loss\"] = np.mean(loss_fn(y_valid, y_pred))\n",
    "            status[\"val_accuracy\"] = np.mean(tf.keras.metrics.sparse_categorical_accuracy(\n",
    "                tf.constant(y_valid, dtype=np.float32), y_pred))\n",
    "            steps.set_postfix(status)\n",
    "        for metric in [mean_loss] + metrics:\n",
    "            metric.reset_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
