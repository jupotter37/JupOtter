{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d3c2e27",
   "metadata": {},
   "source": [
    "# Chapter 5: Multilayer Perceptron\n",
    "\n",
    "### Dhuvi Karthikeyan\n",
    "\n",
    "##### 1/29/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2224cb8",
   "metadata": {},
   "source": [
    "## 5.1 Multilayer Perceptron Theory\n",
    "\n",
    "### 5.1.1 Hidden Layers\n",
    "\n",
    "* Affine transformation = linear transformation with bias.\n",
    "* Linearity implies monotonicity\n",
    "* While linear assumptions may work for various monotonic relationships that are naturally occurring or artificially engineered, unless and until we can learn that mapping from input to monotonic feature, we will have a very brittle model\n",
    "\n",
    "\n",
    "**Note:** The deep learning paradigm is to use observational data to jointly learn a representation (via hidden layers) and a linear predictor that acts upon that hidden representation.\n",
    "\n",
    "#### Incorporating Hidden Layers\n",
    "\n",
    "When stacking a number of dense layers (fully connected), we say that the representation is handled by the first L-1 layers and the Lth layer is the \"output layer\". \n",
    "   \n",
    "   * Interesting that the output of the L-1th layer isnt the representation but rather the entire set of L-1 layers \n",
    "   * Addition of the non-linearity $\\sigma$ makes the MLP > linear model\n",
    "   \n",
    "   \n",
    "**Universal Approximators**: Single hidden layer network given sufficient width can learn any function to arbitrary accuracy/error.\n",
    "\n",
    "* Kernel methods can solve the problem of fitting a function exaclty even in infinite dimensional spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a25a30",
   "metadata": {},
   "source": [
    "### 5.1.2 Activation Functions\n",
    "\n",
    "#### ReLU Function\n",
    "\n",
    "Rectified linear unit: ReLU(x) = max(x,0)\n",
    "Stepwise linear, so formally, the derivative isn't defined @0 but in applied maths we can use the left hand derivative to say its equal to 0. \n",
    "    \n",
    "    * ReLU derivatives are well behaved. They are either 1 or 0; helped block the vanishing gradient problem\n",
    "    \n",
    "#### Sigmoid Function\n",
    "\n",
    "Known as a squashing function for taking values from $-\\infty$ to $\\infty$ to 0 to 1.\n",
    "* The sigmoid activation usually replaced by the ReLU for training stability purposes, it is uniquely suited in the recurrent neural network architecture for its ability to control information flow over time. \n",
    "\n",
    "    * Derivative of sigmoid(x) = sigmoid(x)[1-sigmoid(x)]\n",
    "    \n",
    "#### Tanh Function\n",
    "\n",
    "Tanh function is also a squashing function, this time from -1,1 instead of 0,1.\n",
    "Very similar to sigmoid but with point symmetry about the origin. \n",
    "\n",
    "    * Derivative of the tanh(x) = 1 - tanh(x)^2\n",
    "    \n",
    "#### Swish Activation \n",
    "\n",
    "$$ \\sigma(x) = x*\\text{sigmoid}(\\beta x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bea94e",
   "metadata": {},
   "source": [
    "## 5.3 Forward Propagation, Backward Propagation, and Computational Graphs\n",
    "\n",
    "### 5.3.1 Foward Propagation\n",
    "\n",
    "\"Forward pass\" is calculating and storing the intermediate variables (outputs) of a neural network, in order, from input to output layer.\n",
    "\n",
    "### 5.3.2 Computational Graph of Forward Propagation\n",
    "\n",
    "Visualization of the computational graph and flow of information is very useful for understanding the flow of information\n",
    "\n",
    "### 5.3.3 Backpropagation\n",
    "\n",
    "Traversing the network in reverse order thanks to the chain rule from calculus to identify the gradients of the layer outs w.r.t the previous layers inputs. \n",
    "\n",
    "\n",
    "# *SUPPLANT WITH CLASS NOTES.*\n",
    "\n",
    "### 5.3.4 Training Neural Networks\n",
    "\n",
    "Forward pass goes through the computational graph and, in order, computes lal the dependencies that the backprop does in reverse. Waves of forward and back pass upon incorrect predictions results in training on incorrect examples where the magnitude of weight updates depends on initialization and contribution on the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5c8e3e",
   "metadata": {},
   "source": [
    "## 5.4 Numerical Stability and Initialization\n",
    "\n",
    "Parameter weight initialization in many cases is intimately linked with choices of activation function. The choice of pairing often drives training stability/emergence of vanishing or exploding gradients, or even gradients that just stall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4ccc70",
   "metadata": {},
   "source": [
    "### 5.4.1 Vanishing and Exploding Gradients\n",
    "\n",
    "Vanishing and exploding gradients pose a distinct problem compared to the issue of underflow/overflow encountered when multiplying probabilities or exponentiating. It is unpredictable whether the eigenvalues of the chain rule matrices will be really large or really small. This has impact on numerical stability as well as training stability as we may quickly diverge.\n",
    "\n",
    "#### Vanishing Gradients\n",
    "\n",
    "The reason that practitioners have opted for ReLU by default; Also ReLU helped push the field forward with deep learning because stacking many layers was now possible.\n",
    "\n",
    "#### Exploding Gradients\n",
    "\n",
    "Matrix products explode given enough layers and there are entries > 1.0\n",
    "\n",
    "#### Breaking the Symmetry\n",
    "\n",
    "Initialization of all the parameters to a constant within a layer would result in symmetric gradient sharing. This would effectively turn our MLP into a single unit MLP @ that layer. This can be fixed using dropout regularization and random initialization (although neither can guarantee it do to stochasticity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e96ce68",
   "metadata": {},
   "source": [
    "### 5.4.2 Parameter Initialization\n",
    "\n",
    "Default initialization is the normal distribution (usually standard normal)\n",
    "\n",
    "#### Xavier Initialilzation\n",
    "\n",
    "Ignoring the nonlinearities of a neural network at layer i:\n",
    "\n",
    "$$ o_i = \\sum_{j=1}^{n_{in}} w_{ij}x_j$$\n",
    "\n",
    "If we take $w_{ij}$ as mean 0 and variance $\\sigma^2$ and make the assumption that $x_j$ is mean 0 and variance $\\gamma^2$, the output $o_i$ has a mean of 0 and variance of $n_{in} \\sigma^2 \\gamma^2$.\n",
    "\n",
    "On the forward pass the variance scales with $n_{in}$ which is the dimension of the input whereas during backwards pass the variance scales with $n_{out}$ which is the dimension of the layer output. To keep a fixed variance we would need to scale $n_{in}\\sigma^2 = 1$ **and** $n_{out} \\sigma^2 = 1$ which is impossible.\n",
    "\n",
    "Xavier instead proposed the following condition to satisfy:\n",
    "\n",
    "$$ \\frac{1}{2}(n_{in} + n_{out})\\sigma^2 = 1$$\n",
    "\n",
    "which can be re-written as an instantiation of the variance to:\n",
    "\n",
    "$$ \\sigma^2 = \\frac{2}{n_{in} + n_{out}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad96bfd2",
   "metadata": {},
   "source": [
    "## 5.5 Generalization in Deep Learning\n",
    "\n",
    "Deep learning models have generalized exceptionally well on a vast myriad of tasks and while there a number of hypotheses, it is still virgin territory as to how the optimization fits training data and how generalization actually occurs. A brief discussion is offered in the book\n",
    "\n",
    "### 5.5.1 Revisiting Overfitting and Regularization\n",
    "\n",
    "* \"No free lunch\" theorem Wolpert et al (1995) says that learning algs generalize better on data w specific distributions and worse on data from others\n",
    "\n",
    "* Careful construction of inductive biases can help boost generalization to unseen examples\n",
    "\n",
    "* Counterintuitively a number of tricks exist in deep learning and not standard machine learning that lead to increased generalization as opposed to decreased:\n",
    "    * Training for more epochs\n",
    "    * Increasing model complexity (Double descent)\n",
    "\n",
    "### 5.5.2 Inspiration from Nonparametrics\n",
    "\n",
    "Nonparametric methods often grow in complexity along with the data and can fit the training data exactly. Modern deep learning methods although having parameters that are updated are often likened in behavior to nonparametric methods (Jacot et al 2018) like kernel methods. \n",
    "\n",
    "### 5.5.3 Early Stopping\n",
    "\n",
    "Deep neural networks are capable of fitting labels (artifically generated or not) with ease, but the fitting to random labels is something that happens after fitting correctly labeled data (Rolnick et al 2017). This guarantees that whenever a model fits to cleanly labeled data and not teh incorrectly labeled one, it will generalize to data drawn from that distribution.\n",
    "\n",
    "EARLY STOPPING IS CRUCIAL WITH LABEL NOISE.\n",
    "\n",
    "### 5.5.4 Classical Regularization for Deep Networks\n",
    "\n",
    "The rationale for regularization in the deep learning setting is often different from the rationale in the standard case however their efficacy (albeit a little attenuated on their own) remains. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa7dc03",
   "metadata": {},
   "source": [
    "## 5.6 Dropout\n",
    "\n",
    "* In regularizaiton we want to learn smooth functions such that the addition of arbirary noise to our data does not derail the outputs\n",
    "* 1995 - Christopher Bishop showed that training with input noise on purpose is equal to Tikonov regularization\n",
    "* 2014 - Dropout: a simple way to prevent neural networks from overfitting \n",
    "    * Zero out outputs at each layer and that way we \"noise\" the data during forward prop (How?)\n",
    "    * This allows for it to gradients to be applied in backprop to only a certain subset of the data\n",
    "* How is the noise injected?\n",
    "    * In the original version in 1995, Bishop stochastically added Gaussian noise to the input\n",
    "        * In expectation $\\mathbb{E}[h'] = h$\n",
    "    * In the 2014 dropout version they set the output of a node to 0 with probability p and renormalize the remaining outputs to h/1-p with probability 1-p so in expectation: $\\mathbb{E}[h']= p*0 + (1-p)*\\frac{h}{1-p} = h$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760a05fb",
   "metadata": {},
   "source": [
    "### Class Notes\n",
    "\n",
    "* Pit-falls/Qualifications of \"Linear models\" \n",
    "    * increase in a feature must always increase or always decrease models output (monotonic relationship to input features)\n",
    "\n",
    "    * The effect on the output is proportional to the change in input\n",
    "    \n",
    "**How do we model non-linearities?**\n",
    "\n",
    "* Composing multiple functions (stack multiple transformations)\n",
    "* However, compositions of linear transformations results in a linear transformation\n",
    "* Introduce non-linearity -> (activation function)\n",
    "* Transformation ~ Layer: composite of linear transformation and nonlinearity\n",
    "* $\\sigma$ the nonlinearity function is *usually* elementwise\n",
    "* What to use for $\\sigma$?\n",
    "    * Simgoid(x) = $\\frac{1}{1+\\exp(-x)}$\n",
    "    \n",
    "#### Universal Approximation Theorem\n",
    "\n",
    "A sufficiently \"wide\" MLP with just one hidden layer can approximate any function arbitrarily well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff6c4d1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
