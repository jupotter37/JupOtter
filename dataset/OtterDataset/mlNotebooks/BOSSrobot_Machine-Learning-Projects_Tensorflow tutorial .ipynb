{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Top'></a>\n",
    "\n",
    "# TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "#### <a href = '#Tensors'>Section: Tensors</a>\n",
    "#### <a href = '#Variables'>Section: Variables</a>\n",
    "#### <a href = '#Automatic Differentiation and Gradients'>Section: Automatic Differentiation and Gradients</a>\n",
    "#### <a href = '#Graphs and Functions'>Section: Graphs and Functions</a>\n",
    "#### <a href = '#Modules, layers, and models'>Section: Modules, layers, and models</a>\n",
    "#### <a href = '#Index'>The Index</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction:\n",
    "\n",
    "Tensorflow is used to make building deep learning models easy. It works by creating computational graphs to represent the math going on. When you do a forward pass it uses this graph to efficiently propogate forward. To calculate gradients it also uses the graph with something called backdrop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eager Execution \n",
    "\n",
    "Tensorflow uses eager execution which means that instead of creating an abstract computational graph and then using that during a session, operations return concrete values which can be used at the moment. This makes it easier to debug and interact with however optimizations in the graph and deployment become hard to reach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Tensors'> </a>\n",
    "\n",
    "## Tensors\n",
    "\n",
    "Everything in tensorflow is a tensor, which is a generalization of vectors. The way they work is that vectors are tensors of rank 1 and a tensor of rank n+1 is a set of tensors of rank 1. Yes, this does mean rank 2 tensors are matricies. This is very similar to how numpy arrays work and in fact most numpy methods persist to tensors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation/Basic info\n",
    "\n",
    "All tensors are immutable (like numbers and strings) so you can't update its contents, rather you create new ones. Note that they must be rectangular and have a well defined shape (like 5 by 3 by 2). They also have a certain data type. To specify the data type just use the argument dtype, otherwise its inferred. To create a tensor use \n",
    "* <b>tf.constant(val)</b> => creates a tensor with specified val\n",
    "\n",
    "To convert a tensor to a numpy array use \n",
    "* <b>tensor.numpy()</b> => return numpy form of array\n",
    "or just do \n",
    "* <b>np.array(val)</b> => create numpy array from val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=complex128, numpy=(5+3j)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(5 + 3j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[5., 6.],\n",
       "       [1., 5.]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([[5,6],[1,5]], dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.2, 4.5], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([1.2,4.5]).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Math/Operations on tensors\n",
    "\n",
    "Note: |# APPLIES| means that the two inputs can have different shapes, the values will just be broadcasted \n",
    "\n",
    "|# APPLIES| To add use + or\n",
    "* <b>tf.add(a,b)</b> => return result of adding tensor a and b\n",
    "\n",
    "To do elementwise multiplication (hadamard product) use * or\n",
    "* <b>tf.multiply(a,b)</b> => return result of hadamard product of a and b\n",
    "\n",
    "To apply one tensor to another (AB) use @ or\n",
    "* <b>tf.matmul(a,b)</b> => apply tensor a to b - Note: ORDER MATTERS\n",
    "\n",
    "|# APPLIES| To raise to some power use ** or \n",
    "* <b>tf.pow(a,b)</b> => return result of raising a to power of b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       " array([[4, 4],\n",
       "        [5, 7]])>,\n",
       " <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       " array([[3, 0],\n",
       "        [0, 6]])>,\n",
       " <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       " array([[3, 4],\n",
       "        [5, 6]])>,\n",
       " <tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       " array([[1, 0],\n",
       "        [0, 1]])>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([[1,0],[0,1]])\n",
    "b = tf.constant([[3,4],[5,6]])\n",
    "a+b, a*b, a@b, a**b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cont.\n",
    "\n",
    "To find the largest value\n",
    "* <b>tf.reduce_max(t)</b> => Find max value in t\n",
    "\n",
    "To find the index of the largest value\n",
    "* <b>tf.argmax(t)</b> => Find the index of the max value in t\n",
    "\n",
    "Other standard operations also exist. For example, to find the softmax of a tensor\n",
    "* <b>tf.nn.softmax(t)</b> => Apply softmax to every element in the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=6.0>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1>,\n",
       " <tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.24472848, 0.66524094, 0.09003057], dtype=float32)>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([5,6,4], dtype=tf.float32)\n",
    "tf.reduce_max(t), tf.argmax(t), tf.nn.softmax(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other notes\n",
    "\n",
    "Due to tensor being so similar to numpy, brodcasting and indexing works the same. Also reshape() and transpose() still work with the exception that both are tf methods so you pass in the tensor as an argument too. Both are very fast operation too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[6 7 8]\n",
      " [2 3 4]], shape=(2, 3), dtype=int32) 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
       " array([[5, 6],\n",
       "        [7, 1],\n",
       "        [2, 3]])>,\n",
       " <tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
       " array([[5, 1],\n",
       "        [6, 2],\n",
       "        [7, 3]])>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([[5,6,7],[1,2,3]])\n",
    "print(a+1, a[0,2].numpy())\n",
    "tf.reshape(a,[3,2]), tf.transpose(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Variables'> </a>\n",
    "\n",
    "## Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic info\n",
    "\n",
    "Variables are how you store values that are meant to change over time. In Tensorflow they look and act like a tensor (because they are backed by tensors) so shape, dtype, and numpy persist. All tensor operations also work HOWEVER none of them are inplace so if you try to transpose/reshape a variable you won't actually change.\n",
    "\n",
    "* <b>tf.Variable(val, dtype = None)</b> => create a variable with an intial value of val and data type of dtype if specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(1, 2) dtype=float32, numpy=array([[1., 2.]], dtype=float32)>\n",
      "<tf.Variable 'Variable:0' shape=(1, 2) dtype=float32, numpy=array([[1., 2.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "v = tf.Variable([[1,2.0]])\n",
    "print(v)\n",
    "tf.transpose(v)\n",
    "print(v) # Still the same!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cont.\n",
    "\n",
    "Once you have a variable and want to edit it, instead of recreating a Variable instance you can just use\n",
    "* <b>v.assign(val)</b> => overwrites value variable used to have with val. NOTE shape and dtype stay the same. So you can't assign something funky.\n",
    "\n",
    "You can also add or subtract the variable using\n",
    "* <b>v.assign_add(val)</b> => adds val to previous value of variable. (Previous value gets overwritten again) \n",
    "* <b>v.assign_sub(val)</b> => above but subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tf.Variable(5.1)\n",
    "v.assign(3)\n",
    "v.assign_sub(1)\n",
    "v # Note how dtype is still int. If initial value was 5 instead of 5.1 this would be int32 instead of float32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lifecycle, Naming, and Watching\n",
    "\n",
    "Variables have the same lifecycle as Python object, when there are no references to an object it is automatically deallocated.\n",
    "\n",
    "You can also name variables to help with debugging and tracking them (also useful for tensorboard!). Note that two variables with the same name are still not equal, they are just grouped together. Use the name parameter\n",
    "* <b>tf.Variable(..., name = None,...)</b> => Make variable with name of name to help track it. Useful for debugging and TensorBoard\n",
    "\n",
    "In addition, all variables have a parameter trainable set to True which means that their values and interactions are also watched (to help compute gradients later). To save that extra memory and computation you can just set it to False. \n",
    "* <b>tf.Variable(..., trainable = True,...)</b> => Make variable which gets watched or not watched (by GradientTape coming later).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable(4, name = 'Variable', trainable = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Automatic Differentiation and Gradients'></a>\n",
    "\n",
    "## Automatic Differentiation and Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Info\n",
    "\n",
    "AutoDiff is something that allows you to compute gradients easily. It works by recording the computations you make into a tree like structure and then traversing backward through the graph when computing gradients. \n",
    "\n",
    "Tensorflow uses the GradientTape API to let users take control over this. Under the context of a GradientTape object you perform the operations relative to the gradient calculation and then after it you use \n",
    "* <b>tape.gradient(target, sources)</b> => target and source can be list of or individual tensor. For each tensor in source, it returns sum of partial derivatives of each target in gradients with respect to source tensor. If the target is a tensor, it will also return the sum with respect to the targets elements. For example, if you had MSE and regularization, you would put those two in target or put them in a tensor and pass that to target and then put the weights in sources.\n",
    "\n",
    "Note that if the source is not connected to the target in any way the gradient will just return None. If you want 0 instead you can use\n",
    "* <b>tape.gradient(..., unconnected_gradients=tf.UnconnectedGradients.ZERO, ...)</b> => makes unconnected gradients what you pass in instead of None. Type must derived from tf.UnconnectedGradients. Example argument could be tf.UnconnectedGradients.ZERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=7.0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(3, dtype = tf.float32) # Important to use floats otherwise you get an warning (3 here would make dtype int)\n",
    "\n",
    "#Context of tape\n",
    "with tf.GradientTape() as tape:\n",
    "    y1 = x**2\n",
    "    y2 = tf.abs(x)\n",
    "\n",
    "#Get Gradient\n",
    "tape.gradient(target = [y1, y2], sources = x) #dy/dx + dz/dx = 2x+sgn(x) = 6+1 = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([3., 4.], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Another example\n",
    "error = tf.Variable([3.0,4.0])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = 1/2*tf.norm(error)**2 # 1/2*||error||^2 => derivative is [x, y]\n",
    "\n",
    "tape.gradient(loss, error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[9., 3.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 1.]], dtype=float32)>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable([[1,3.0]])\n",
    "w = tf.Variable([[4,5.0],[1,2]])\n",
    "b = tf.Variable([[2,2.0]])\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x@w + b\n",
    "tape.gradient(y,[x,b]) #This time a list because there are multiple sources!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing gradients with respect to a model\n",
    "\n",
    "Collecting all the variables you have into one place (like tf.Module, tf.keras.layers.Layer, tf.keras.Model of which both derive from tf.Module), makes things much more organized. You can get all the sources from\n",
    "* <b>layer.trainable_variables</b> => returns a list containing all trainable variables that layer has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense/kernel:0, shape: (4, 2)\n",
      "dense/bias:0, shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "#Don't worry about the Dense, its just a place where all the variables are collected!\n",
    "layer = tf.keras.layers.Dense(2, activation = 'relu') \n",
    "x = tf.constant([[1,2,3,4]], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = layer(x)  #Since we don't define the # of neurons in the input layer passing in x creates the layer and returns y!\n",
    "    loss = tf.reduce_mean(y**2) # a^2/2 + b^2/2\n",
    "    \n",
    "grad = tape.gradient(loss, layer.trainable_variables) \n",
    "\n",
    "for var, g in zip(layer.trainable_variables, grad):\n",
    "    print(f'{var.name}, shape: {g.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Controlling what the tape keep tracks of\n",
    "\n",
    "Note that using gradient tape takes up memory and a bit of overhead due to it recording and storing everything. \n",
    "\n",
    "The default behavior is to only record the operations that a trainable tf.Variable makes.\n",
    "* <b>tf.Variable(..., trainable = True, ...)</b> => the trainable parameter decides whether the variable is going to be tracked or not \n",
    "\n",
    "So if you tried to get the gradient with respect to a constant that wouldn't work (you would get None). Note that a variable + a tensor returns a tensor so if you want to add to a variable make sure you use assign add\n",
    "\n",
    "To see what variables the tape is currently watching use \n",
    "* <b>tape.watched_variables()</b> => returns a list of the variables currently being watched\n",
    "    \n",
    "To tell the tape to watch a specific constant or non trainable variable use\n",
    "* <b>tape.watch(v)</b> => Forces the tape to make sure its watching v as well\n",
    "\n",
    "If you want the tape to stop watching all trainable variables and just not watch anything use \n",
    "* <b>tf.GradientTape(..., watch_accessed_variables=False, ...)</b> => Disables default tape behavior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 4.], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 4.], dtype=float32)>,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.Variable([1., 2])\n",
    "b = tf.constant([1, 2.])\n",
    "c = tf.Variable([1, 2.], trainable = False)\n",
    "d = tf.Variable([1, 2.]) + 1\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(b)\n",
    "    y = a**2 + b**2 + c**2\n",
    "    \n",
    "tape.gradient(y, [a,b,c,d])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Control Flow\n",
    "\n",
    "Note that if you have if or else statements inside a tape's context everything will work fine. The tape just records the operations so the control flow logic will be invisible to the tape. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Errors\n",
    "\n",
    "You can get a gradient of None when you don't expect it to happen. Here are some common errors:\n",
    "1. Replacing variable with tensor (use assign_add but not +1) => GradientTape() doesn't watch tensors\n",
    "2. Doing calculations outside of tensorflow => tensorflow can't record calculation done outside of tensorflow \n",
    "3. Taking gradients of an integer or string => dtype is int. No example shown here\n",
    "4. Taking gradients through a stateful object => A tensor is immutable so it has a value but no state. Doing operations on them create new tensors and everyone's happy. With variables, they do have a state, which becomes a wall preventing you from going further back\n",
    "\n",
    "If you are expecting gradients of None and want to make them return some other value you can use\n",
    "* <b>tape.gradient(..., unconnected_gradients, ...)</b> => Change None gradients to what you pass in. Try to subclass tf.UnconnectedGradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResourceVariable : tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "EagerTensor : None\n"
     ]
    }
   ],
   "source": [
    "ex_1 = tf.Variable(2.0)\n",
    "\n",
    "for epoch in range(2):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = ex_1+1\n",
    "\n",
    "    print(type(ex_1).__name__, \":\", tape.gradient(y, ex_1))\n",
    "    ex_1 = ex_1 + 1   # This should be `ex_1.assign_add(1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "ex_2 = tf.Variable([3.1,2.9])\n",
    "with tf.GradientTape() as tape:\n",
    "    \n",
    "    outside_tf = np.mean(ex_2)\n",
    "    bad = tf.reduce_mean(outside_tf)\n",
    "\n",
    "print(tape.gradient(bad, ex_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "ex_4 = tf.Variable(3.0)\n",
    "x1 = tf.Variable(0.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    x1.assign_add(ex_4) # Update x1 = x1 + x0.\n",
    "    \n",
    "    # The tape starts recording from x1.\n",
    "    y = x1**2   # y = (x1 + ex_4)**2\n",
    "\n",
    "print(tape.gradient(y, ex_4))   #dy/dx0 = 2*(x1 + x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(3.1)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.Variable(4.0)\n",
    "    \n",
    "tape.gradient(y, x, unconnected_gradients = tf.UnconnectedGradients.ZERO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Graphs and Functions'> </a>\n",
    "\n",
    "## Graphs and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About Graphs\n",
    "Running eagerly is great but there are a bunch of speedups you can see when you start to use graphs. Graphs are data structures which contain tf.Operation (represent units of computation) and tf.Tensor (units of data that flow between operations). They are defined under a tf.Graph context and due to being data structures they can be run and saved without the original python code.  Creating graphs allows you to use them in places that don't have a python interpreter like a phone. They are also easily optimized and can give you significant speedups (although with small operations they are already optimized so it won't be that much better). In short, when creating and testing use eager execution but at the end creating a graph can make things much faster.\n",
    "\n",
    "Creating a graph essentially has two parts to it. Tracing and using. When you first create a graph tensorflow will have to trace the graph so it may take some additional time. On the other hand, next time you use your graph all the speedups will make it significantly faster. Note that graphs are traced for different data types and different branches of control flow. By default, tf.autograph is used to convert loops and flow control into graphs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracing a graph\n",
    "\n",
    "To create a graph you can simply use\n",
    "* <b>tf.function(func)</b> => makes a graph out of func\n",
    "* <b>@tf.function =></b> Apply this decorator to your function \n",
    "\n",
    "A tf.function will also recursively trace any function that it calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def outside(x,w,b):\n",
    "    return x@w +b\n",
    "\n",
    "#@tf.function - could have also done this\n",
    "def layer_op(b):\n",
    "    x1 = tf.constant([[1.0, 2.0]])\n",
    "    w1 = tf.constant([[2.0], [3.0]])\n",
    "    return outside(x1, w1, b)\n",
    "\n",
    "graph_func = tf.function(layer_op) #Outside was traced too!\n",
    "graph_func(tf.constant(4.0)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting Polymorphic Functions\n",
    "\n",
    "When you trace a function it becomes polymorphic which means that it stays callables and encapsulates several concrete functions behind one API. Every time you pass in new argument signatures (ex: different dtype or shape), the graph is retraced and then the tf.Graph object is stored in a concrete_function. You can view all the concrete functions with \n",
    "* <b>func.pretty_printed_concrete_signatures()</b> => Prints all of the available traces the function has (refer to <a href='https://www.tensorflow.org/guide/function'>function</a> for this one)\n",
    "\n",
    "To retrieve and handle these concrete functions use \n",
    "* <b>func.get_concrete_function(spec)</b> => if exists, gets concrete function that fits specification in spec\n",
    "* <b>tf.TensorSpec(shape=, dtype=)</b> => Way to create specifications on a tensor, what you pass into above. If you have a None in a dimension in shape it will be flexible to any shape in that dimension\n",
    "\n",
    "To limit the tracing that occurs with a functions you can also use \n",
    "* <b>tf.function(..., input_signature=spec_tuple, ...)</b> => restricts tracing that occurs with function using specifications tuple. Will raise ValueError if inputs don't match. If you had an LSTM for example, passing in None would stop a new graph from being created every time a new input shape is seen\n",
    "\n",
    "Note: Examples not shown refer to basics guide for more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python Side Effects\n",
    "\n",
    "Switching from this tracing and Pythonic side of view has a couple side effects. As you know, the first time you call a function whose arguments are new it gets traced. This means that all the python code inside runs as well! However, the next time you call the function, the traced version is run instead meaning that any \"Python code\" (printing, appending to lists, mutating globals, etc...) in the function doesn't run. Other features like Python generators and iterators work using the Python runtime so things like advancing an iterator's state also don't work with functions. If you want to avoid this it is better to use tensorflow ops such as <b>tf.print()</b>, <b>tf.gather()</b>, <b>tf.stack()</b>, and <b>tf.TensorArray()</b>. If you still want to use python side effects you can use \n",
    "* <b>tf.py_function()</b> => Converts normal function to tensorflow function that is capable of dealing with python side effects. Problem is that it isn't that good with speedups and isn't portable. \n",
    "\n",
    "Examples not shown here. See Debugging/tensorflow section for more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looping\n",
    "\n",
    "A common pitfall is to loop over Python/Numpy data within a tf.function. As you might expect, this looping will only occur during a tracing process, adding a copy of your model each time you loop over it. If you really want the training loop inside your function you can wrap your data around a <b>tf.data.Dataset</b> object and then AutoGraph will automatically be able to deal with it. See the <a href='https://www.tensorflow.org/guide/data'>data guide</a> for this. Another thing you might want to do is accumulate intermediary values from a loop. Use <b>tf.TensorArray()</b> for this kind of thing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debugging \n",
    "\n",
    "Debugging code is generally easier in eager mode than in in graph mode so make sure your functions work properly in eager mode before decorating with <b>tf.function()</b>. To ensure that your running or not running in eager mode you can use \n",
    "* <b>tf.config.run_functions_eagerly(bool)</b> => makes sure eager mode is on or off depending on bool \n",
    "\n",
    "If errors are only occuring with <b>tf.function()</b> you can try a couple things. For example, <b>print()</b> helps with tracing issues, <b>tf.print()</b> always executes and can help you track intermediate values, and <b>tf.debugging.enable_check_numerics()</b> lets you check where NaNs and Infs are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracing!\n",
      "Running!\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "Running!\n",
      "tf.Tensor(11, shape=(), dtype=int32)\n",
      "Tracing!\n",
      "Running!\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "Tracing!\n",
      "Running!\n",
      "tf.Tensor(11, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Using python side effects cleverly to detect when tracing occurs. Note that each new integer argument creates a new trace. \n",
    "# This is because tensorflow uses their value as a way to distinguish them\n",
    "@tf.function\n",
    "def a_function_with_python_side_effect(x):\n",
    "    print(\"Tracing!\")  \n",
    "    tf.print(\"Running!\")\n",
    "    return x * x + tf.constant(2)\n",
    "\n",
    "print(a_function_with_python_side_effect(tf.constant(2)))\n",
    "print(a_function_with_python_side_effect(tf.constant(3)))\n",
    "\n",
    "print(a_function_with_python_side_effect(2))\n",
    "print(a_function_with_python_side_effect(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables\n",
    "\n",
    "If you create a new variable inside a function, due to trace resuse the variable might not be created every time. This behavior defers from what would happen in eager mode so to avoid the ambiguity a ValueError is raised. To avoid any issues just make sure the variables are only created the first time the function is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# How NOT to do things\n",
    "@tf.function()\n",
    "def f():\n",
    "    v = tf.Variable(5.0)\n",
    "    return v\n",
    "try:\n",
    "    f()\n",
    "except ValueError:\n",
    "    print(\"Expected\")\n",
    "    \n",
    "# How things should be done   \n",
    "class Count(tf.Module):\n",
    "    def __init__(self):\n",
    "        self.count = None\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self):\n",
    "        if not self.count: #Needed to make sure variable isn't recreated everytime \n",
    "            self.count = tf.Variable(0)\n",
    "        return self.count.assign_add(1)\n",
    "\n",
    "c = Count()\n",
    "print(c())\n",
    "print(c())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another problem you might face is with the garbage collector. Since tensorflow only retains WeakRefs to a variable you must keep a reference to it at all time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling concrete function...\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "\n",
      "Calling concrete function after garbage collecting its closed Variable...\n",
      "FailedPreconditionError happened. Can't do operation when part of it doesn't exist!\n"
     ]
    }
   ],
   "source": [
    "external_var = tf.Variable(3)\n",
    "@tf.function\n",
    "def f(x):\n",
    "    return x * external_var\n",
    "\n",
    "traced_f = f.get_concrete_function(4)\n",
    "print(\"Calling concrete function...\")\n",
    "print(traced_f(4))\n",
    "\n",
    "del external_var\n",
    "print()\n",
    "print(\"Calling concrete function after garbage collecting its closed Variable...\")\n",
    "try: \n",
    "    traced_f(4)\n",
    "except:\n",
    "    print(\"FailedPreconditionError happened. Can't do operation when part of it doesn't exist!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Modules, layers, and models'></a>\n",
    "\n",
    "## Modules, layers, and models\n",
    "\n",
    "To do machine learning in tensorflow you will likely need to define, save, and restore a model. A model is something that can compute a forward pass and update its contents in response to that pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining models and layers in tensorflow\n",
    "\n",
    "Models are usually made up of layers. Layers are functions with a known mathematical structure that can be reused and have trainable variables. Most high level implementations of layers and models are built on one foundational class, <b>tf.Module</b>\n",
    "\n",
    "When inheriting <b>tf.Module</b> you normally want to write init. Inside of init make sure you accept an optional name argument and call super with name passed in. You also want to somehow call the module so writing call is also a good idea although you can use any function you want. \n",
    "\n",
    "By subclassing <b>tf.Module</b>, any <b>tf.Variable</b> or <b>tf.Module</b> instances assigned to this object's properties are automatically collected. This allows you to save and load variables, and also create collections of <b>tf.Modules</b>. You can also set trainability of variables through your module and change that any time. \n",
    "\n",
    "A couple things you can do with tf.Module:\n",
    "* <b>module.submodules</b> => property containing all the modules inside the current module (all the submodules)\n",
    "* <b>module.trainable_variables</b> => property containing all the variables that gradient tape will watch\n",
    "* <b>module.variables</b> => property containing all the variables that exist in the module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<__main__.SimpleModule object at 0x000002B5F680B910>, <__main__.SimpleModule object at 0x000002B5ED65DB80>) \n",
      "\n",
      "(<tf.Variable 'trainable:0' shape=() dtype=float32, numpy=5.0>, <tf.Variable 'trainable:0' shape=() dtype=float32, numpy=5.0>) \n",
      "\n",
      "(<tf.Variable 'not trainable:0' shape=() dtype=float32, numpy=1.0>, <tf.Variable 'trainable:0' shape=() dtype=float32, numpy=5.0>, <tf.Variable 'not trainable:0' shape=() dtype=float32, numpy=1.0>, <tf.Variable 'trainable:0' shape=() dtype=float32, numpy=5.0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class SimpleModule(tf.Module):\n",
    "    \n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name = name)\n",
    "        self.train = tf.Variable(5.0, name = 'trainable')\n",
    "        self.non_train = tf.Variable(1.0, trainable = False, name = 'not trainable')\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.train*x + self.non_train \n",
    "\n",
    "class finalModule(tf.Module):\n",
    "    \n",
    "    def __init__(self, name = None):\n",
    "        super().__init__(name = name)\n",
    "        self.sm1 = SimpleModule()\n",
    "        self.sm2 = SimpleModule()\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        x = self.sm1(x)\n",
    "        return self.sm2(x)\n",
    "\n",
    "fm = finalModule('simple')\n",
    "\n",
    "print(fm.submodules,'\\n')\n",
    "print(fm.trainable_variables,'\\n')\n",
    "print(fm.variables,'\\n')\n",
    "\n",
    "#Using GradientTape()\n",
    "with tf.GradientTape() as tape:\n",
    "    alpha = 1\n",
    "    x = tf.Variable(3.0)\n",
    "    y = fm(x)\n",
    "    loss = tf.abs(y)\n",
    "    \n",
    "for v, dv in zip(fm.trainable_variables, tape.gradient(y, fm.trainable_variables)):\n",
    "    v.assign_sub(dv*alpha) # plain ol' gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Waiting to create variables\n",
    "\n",
    "In practice, the shape of the inputs to a layer might not be known or may be changed later. To be flexible about this you can be a bit clever and defer this until the first input is passed in. This flexibility is also present in Keras and its good practice to have. Note that we use\n",
    "* <b>tf.random.normal(shape, name = None)</b> => creates an array from a normal distribution using arguments\n",
    "* <b>tf.zeros(shape, dtype, name = None)</b> => creates an array of zeros with argument specifications\n",
    "\n",
    "and the second way of doing things\n",
    "\n",
    "* <b>tf.random_normal_initializer()</b> => creates an object that can be used to initalize variables with random values\n",
    "* <b>tf.zeros_initializer()</b> => creates an object that can be used to initalize variables with values of zero\n",
    "* <b>initializer(shape)</b> => creates array using initializer with given shape\n",
    "\n",
    "and finally create our variable with \n",
    "* <b>tf.Variable(initial_value, dtype,))</b> => Create a variable with starting values. Important to specify dtype as the intial values could be zero as well\n",
    "\n",
    "NOTE: DONT PASS IN Keras.layers.Input(). This makes it a lambda layer and weird stuff happens. Do that when you subclass keras :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.99993557,  0.9827529 , -0.9952498 ], dtype=float32)>, <tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
      "array([[-0.99993557,  0.9827529 , -0.9952498 ],\n",
      "       [-1.9998711 ,  1.9655058 , -1.9904996 ]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "class FlexibleDense(tf.Module):\n",
    "    \n",
    "    def __init__(self, outshape, name = None):\n",
    "        super().__init__(name = name)\n",
    "        self.outshape = outshape\n",
    "        self.is_built = False\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        if not self.is_built:\n",
    "            w_init = tf.random_normal_initializer()\n",
    "            self.W = tf.Variable(initial_value = w_init([x.shape[-1], self.outshape]),dtype= tf.float32, name='w')\n",
    "            \n",
    "            b_init = tf.zeros([self.outshape], name = 'b', dtype=tf.float32)\n",
    "            self.B = tf.Variable(initial_value = b_init, name = 'b')\n",
    "            \n",
    "            self.is_built = True\n",
    "            \n",
    "        return tf.nn.tanh(x @ self.W + self.B)\n",
    "    \n",
    "x = tf.constant([[1.0, 2]])\n",
    "f = FlexibleDense(3)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = f(x)\n",
    "    loss = tf.abs(y-tf.zeros(y.shape))\n",
    "\n",
    "print(tape.gradient(loss, f.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving a module\n",
    "You can visit the <a href = 'https://www.tensorflow.org/guide/intro_to_modules#saving_weights'>docs</a> for that one. Since the end goal is the keras functional api imma cut this one short.\n",
    "\n",
    "As a another note, there is more to come with subclassing when looking at keras but for now this should suffice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process:\n",
    "\n",
    "With any application these are the steps you will usually need to follow\n",
    "\n",
    "1. Start by getting your data ready. You need to preprocess your data and turn it into a tf.data object\n",
    "\n",
    "\n",
    "2. Create your model. This process can either be very easy but not as flexible or it can be more lengthy but to your purpose. Spectrum below\n",
    "    - Sequential API + built in layers - new users and simple models\n",
    "    - Functional API + built in layers - engineers with standard use cases\n",
    "    - Functional API + custom layers, metrics, losses - engineers requiring increasing control\n",
    "    - Subclassing: do everything yourself - researchers\n",
    "\n",
    "\n",
    "3. Train the model. Distribute it across cpus, gpus, tpus, and even computers!\n",
    "    - Set up TensorBoard to help analyze for the future (optional)\n",
    "\n",
    "\n",
    "4. Serialize model to save it (can store in a model repository too - use TensorFlow Hub) \n",
    "\n",
    "\n",
    "5. Deploy model. For each case listed below use API on the right\n",
    "    - Browser and Node: TensorFlow.js\n",
    "    - Android and iOS: TensorFlow Lite\n",
    "    - Cloud, on perm: TensorFlow Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Index'> </a>\n",
    "\n",
    "## Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href = '#Top'>Back to top?</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a href = '#Tensors'>Section: Tensors</a>\n",
    "\n",
    "* <b>tf.constant(val)</b> => creates a tensor with specified val\n",
    "* <b>tensor.numpy()</b> => return numpy form of array\n",
    "* <b>np.array(val)</b> => create numpy array from val \n",
    "* <b>tf.add(a,b)</b> => return result of adding tensor a and b\n",
    "* <b>tf.multiply(a,b)</b> => return result of hadamard product of a and b\n",
    "* <b>tf.matmul(a,b)</b> => apply tensor a to b - Note: ORDER MATTERS\n",
    "* <b>tf.pow(a,b)</b> => return result of raising a to power of b\n",
    "* <b>tf.reduce_max(t)</b> => Find max value in t\n",
    "* <b>tf.argmax(t)</b> => Find the index of the max value in t\n",
    "* <b>tf.nn.softmax(t)</b> => Apply softmax to every element in the tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a href = '#Variables'>Section: Variables</a>\n",
    "\n",
    "* <b>tf.Variable(val, dtype = None)</b> => create a variable with an intial value of val and data type of dtype if specified\n",
    "* <b>v.assign(val)</b> => overwrites value variable used to have with val. NOTE shape and dtype stay the same. So you can't assign something funky.\n",
    "* <b>v.assign_add(val)</b> => adds val to previous value of variable. (Previous value gets overwritten again) \n",
    "* <b>v.assign_sub(val)</b> => above but subtraction\n",
    "* <b>tf.Variable(..., name = None,...)</b> => Make variable with name of name to help track it. Useful for debugging and TensorBoard\n",
    "* <b>tf.Variable(..., trainable = True,...)</b> => Make variable which gets watched or not watched (by GradientTape coming later).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a href = '#Automatic Differentiation and Gradients'>Section: Automatic Differentiation and Gradients</a>\n",
    "\n",
    "* <b>tape.gradient(target, sources)</b> => target and source can be list of or individual tensor. For each tensor in source, it returns sum of partial derivatives of each target in gradients with respect to source tensor. If the target is a tensor, it will also return the sum with respect to the targets elements. For example, if you had MSE and regularization, you would put those two in target or put them in a tensor and pass that to target and then put the weights in sources.\n",
    "* <b>tape.gradient(..., unconnected_gradients, ...)</b> => makes unconnected gradients what you pass in instead of None. Type must derived from tf.UnconnectedGradients. Example argument could be tf.UnconnectedGradients.ZERO\n",
    "* <b>layer.trainable_variables</b> => returns a list containing all trainable variables that layer has\n",
    "* <b>tf.Variable(..., trainable = True, ...)</b> => the trainable parameter decides whether the variable is going to be tracked or not \n",
    "* <b>tape.watched_variables()</b> => returns a list of the variables currently being watched\n",
    "* <b>tape.watch(v)</b> => Forces the tape to make sure its watching v as well\n",
    "* <b>tf.GradientTape(..., watch_accessed_variables=False, ...)</b> => Disables default tape behavior "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a href = '#Graphs and Functions'>Section: Graphs and Functions</a>\n",
    "\n",
    "* <b>tf.function(func)</b> => makes a graph out of func\n",
    "* <b>@tf.function =></b> Apply this decorator to your function \n",
    "* <b>func.pretty_printed_concrete_signatures()</b> => Prints all of the available traces the function has (refer to <a href='https://www.tensorflow.org/guide/function'>function</a> for this one)\n",
    "* <b>func.get_concrete_function(spec)</b> => if exists, gets concrete function that fits specification in spec\n",
    "* <b>tf.TensorSpec(shape=, dtype=)</b> => Way to create specifications on a tensor, what you pass into above. If you have a None in a dimension in shape it will be flexible to any shape in that dimension\n",
    "* <b>tf.function(..., input_signature=spec_tuple, ...)</b> => restricts tracing that occurs with function using specifications tuple. Will raise ValueError if inputs don't match. If you had an LSTM for example, passing in None would stop a new graph from being created every time a new input shape is seen    \n",
    "* <b>tf.py_function()</b> => Converts normal function to tensorflow function that is capable of dealing with python side effects. Problem is that it isn't that good with speedups and isn't portable.   \n",
    "* <b>tf.config.run_functions_eagerly(bool)</b> => makes sure eager mode is on or off depending on bool  \n",
    "\n",
    "\n",
    "<b>tf.debugging.enable_check_numerics()</b>  \n",
    "<b>tf.print()</b>   \n",
    "<b>tf.gather()</b>  \n",
    "<b>tf.stack()</b>  \n",
    "<b>tf.TensorArray()</b>   \n",
    "<b>tf.data.Dataset</b>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a href = '#Modules, layers, and models'>Section: Modules, layers, and models</a>\n",
    "\n",
    "* <b>module.submodules</b> => property containing all the modules inside the current module (all the submodules)\n",
    "* <b>module.trainable_variables</b> => property containing all the variables that gradient tape will watch\n",
    "* <b>module.variables</b> => property containing all the variables that exist in the module \n",
    "* <b>tf.random.normal(shape, name = None)</b> => creates an array from a normal distribution with shape shape and name name\n",
    "* <b>tf.zeros(shape, dtype, name = None)</b> => creates an array of zeros with argument specifications\n",
    "* <b>tf.random_normal_initializer()</b> => creates an object that can be used to initalize variables with random values\n",
    "* <b>tf.zeros_initializer()</b> => creates an object that can be used to initalize variables with values of zero\n",
    "* <b>initializer(shape)</b> => creates array using initializer with given shape\n",
    "* <b>tf.Variable(initial_value, dtype,))</b> => Create a variable with starting values. Important to specify dtype as the intial values could be zero as well\n",
    "\n",
    "<b>tf.Module</b>  \n",
    "<b>tf.Variable</b>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
