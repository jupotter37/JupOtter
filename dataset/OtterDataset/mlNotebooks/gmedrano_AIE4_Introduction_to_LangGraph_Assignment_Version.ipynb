{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "\n",
        "  - ü§ù Breakout Room #2:\n",
        "  1. Evaluating the LangGraph Application with LangSmith\n",
        "  2. Adding Helpfulness Check and \"Loop\" Limits\n",
        "  3. LangGraph for the \"Patterns\" of GenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effectively allowing us to recreate application flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KaVwN269EttM"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain_openai langchain-community langgraph arxiv duckduckgo_search==5.3.1b1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "42b2ba5e-11ae-4f4d-e68e-77607bb794ad"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "30aa2260-50f1-4abf-b305-eed0ee1b9008"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE4 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tool_belt = [\n",
        "    DuckDuckGoSearchRun(),\n",
        "    ArxivQueryRun(),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "model = model.bind_tools(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### ‚ùì Question #1:\n",
        "How does the model determine which tool to use?\n",
        "\n",
        "\n",
        "When the model gets a user input, it figures out what kind of task is needed by analyzing the text. Based on the context and the tools it has in its tool belt, like the DuckDuckGo search or Arxiv paper search, it decides which one to use. This decision is influenced by how the tools are set up and the model‚Äôs training, which helps it recognize when a specific tool is the right fit for the job.\n",
        "\n",
        "Once the model picks the tool, it calls it up to do the work, like finding information or retrieving data. The tool does its thing (as Chris would say!), and the model then uses the results to craft the final response.\n",
        "\n",
        "The model isn‚Äôt directly running Python code or invoking tools itself. Instead, it generates outputs that suggest what tools should be used, and the surrounding system handles the execution of those tools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]\n",
        "  cycle_count: int  # Adding a counter to track cycles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "def call_model(state):\n",
        "  # Ensure the cycle_count is initialized\n",
        "  if \"cycle_count\" not in state:\n",
        "    state[\"cycle_count\"] = 0\n",
        "  \n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "\n",
        "  # Increment the cycle count after processing\n",
        "  state[\"cycle_count\"] += 1\n",
        "\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "tool_node = ToolNode(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `tool_node` is a node which can call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "uncompiled_graph = StateGraph(AgentState)\n",
        "\n",
        "uncompiled_graph.add_node(\"agent\", call_model)\n",
        "uncompiled_graph.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "uncompiled_graph.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  # Check if the cycle limit has been reached\n",
        "  if state.get(\"cycle_count\", 0) >= 10:  # Cycle limit\n",
        "    return END\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  return END\n",
        "\n",
        "uncompiled_graph.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [],
      "source": [
        "uncompiled_graph.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "compiled_graph = uncompiled_graph.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### ‚ùì Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?\n",
        "\n",
        "There isn‚Äôt a built-in limit to how many times we can cycle through nodes in a workflow. While this provides a lot of flexibility, it also means that without any safeguards, a workflow could potentially loop forever if the conditions for continuing the loop are always met.\n",
        "\n",
        "This can be avoided by adding a cycle limit through a counter in the state object. This notebook has been modified to include this feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "a5d7ef7a-13f2-4066-df52-b0df89eae2ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_WahdEh5bloLbjxUG1Znlod9L', 'function': {'arguments': '{\"query\":\"current captain of the Winnipeg Jets 2023\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 156, 'total_tokens': 181}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d6deef53-4a77-4ac7-bfb6-52f311c25e51-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'current captain of the Winnipeg Jets 2023'}, 'id': 'call_WahdEh5bloLbjxUG1Znlod9L', 'type': 'tool_call'}], usage_metadata={'input_tokens': 156, 'output_tokens': 25, 'total_tokens': 181})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content='The Winnipeg Jets will have a captain for the 2023-24 season. After going captain-less in 2022-23, the Winnipeg Jets unveiled Adam Lowry as the club\\'s new captain on Tuesday morning. \"When I ... Adam Lowry was named captain of the Winnipeg Jets on Tuesday. ... Sep 20, 2023. Latest News. Inside look at Vegas Golden Knights Aug 30, 2024. Vegas Golden Knights fantasy projections for 2024-25 Posted September 12, 2023 9:29 am. Centre Adam Lowry was named the Winnipeg Jets new captain on Tuesday. Lowry is the third Jets captain since the team moved from Atlanta to Winnipeg in 2011. He follows Andrew Ladd and Blake Wheeler, who served as captain for five and six years respectively. Lowry will follow Andrew Ladd and Blake Wheeler to serve as the third captain of the new Winnipeg Jets franchise. - Sep 12, 2023. After a season without a captain, the Winnipeg Jets have named ... ‚Äî Winnipeg Jets (@NHLJets) September 6, 2023. In some ways, the Jets are now one more step removed from the Paul Maurice and Wheeler-led era, and further into the next phase of the Jets. Which will evidently be led by Adam Lowry and Josh Morrissey, and for now, Mark Scheifele and Rick Bowness.', name='duckduckgo_search', tool_call_id='call_WahdEh5bloLbjxUG1Znlod9L')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='The current captain of the Winnipeg Jets is Adam Lowry. He was named the captain on September 12, 2023.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 478, 'total_tokens': 505}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-79409798-fda0-415f-94bc-c134999d3d06-0', usage_metadata={'input_tokens': 478, 'output_tokens': 27, 'total_tokens': 505})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# Initialize the inputs with the cycle_count set to 0\n",
        "inputs = {\n",
        "    \"messages\": [HumanMessage(content=\"Who is the current captain of the Winnipeg Jets?\")],\n",
        "    \"cycle_count\": 0  # Initializing cycle_count here\n",
        "}\n",
        "\n",
        "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"tool_calls\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"tool_calls\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "026a3aa3-1c3e-4016-b0a3-fe98b1d25399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_PEoi6GkzYbpAzCNChpPxjrnD', 'function': {'arguments': '{\"query\": \"QLoRA\"}', 'name': 'arxiv'}, 'type': 'function'}, {'id': 'call_FIaUESayTuPtOvZTiyYqOKyi', 'function': {'arguments': '{\"query\": \"latest Tweet Tim Dettmers\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_4Pdo2jbdPvdZd8G2nnRxYdVC', 'function': {'arguments': '{\"query\": \"latest Tweet Mike Lewis\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_FIkQCcNnv1VMA4EzF3JsLrS6', 'function': {'arguments': '{\"query\": \"latest Tweet Sam Shleifer\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_QI8mgBvXvep0ZqgQoBUOkiWC', 'function': {'arguments': '{\"query\": \"latest Tweet Luke Zettlemoyer\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 119, 'prompt_tokens': 173, 'total_tokens': 292}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-48d64d4c-8625-4e3c-98e9-88f977077374-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'QLoRA'}, 'id': 'call_PEoi6GkzYbpAzCNChpPxjrnD', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'latest Tweet Tim Dettmers'}, 'id': 'call_FIaUESayTuPtOvZTiyYqOKyi', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'latest Tweet Mike Lewis'}, 'id': 'call_4Pdo2jbdPvdZd8G2nnRxYdVC', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'latest Tweet Sam Shleifer'}, 'id': 'call_FIkQCcNnv1VMA4EzF3JsLrS6', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'latest Tweet Luke Zettlemoyer'}, 'id': 'call_QI8mgBvXvep0ZqgQoBUOkiWC', 'type': 'tool_call'}], usage_metadata={'input_tokens': 173, 'output_tokens': 119, 'total_tokens': 292})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: arxiv\n",
            "[ToolMessage(content='Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2024-05-27\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnologies derived from the perspective of unified information: (1)\\nstatistics-based Information Calibration Quantization allows the quantized\\nparameters of LLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\\nrepresentation transformation with diverse information. Comprehensive\\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\\nimprovement on MMLU compared with the state-of-the-art methods. The significant\\nperformance gain requires only a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IR-QLoRA. We highlight that\\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\\nThe code is available at https://github.com/htqin/ir-qlora.\\n\\nPublished: 2024-06-12\\nTitle: Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods\\nAuthors: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\\nSummary: There are various methods for adapting LLMs to different domains. The most\\ncommon methods are prompting, finetuning, and RAG. In this w', name='arxiv', tool_call_id='call_PEoi6GkzYbpAzCNChpPxjrnD'), ToolMessage(content='Allen School Ph.D. student Tim Dettmers accepted the grand prize for QLoRA, a novel approach to finetuning pretrained models that significantly reduces the amount of GPU memory required ‚Äî from over 780GB to less than 48GB ‚Äî to finetune a 65B parameter model. With QLoRA, the largest publicly available models can be finetuned on a single ... Tech Moves: AI researcher Yejin Choi leaves Univ. of Washington and Allen Institute for AI. by Todd Bishop & Taylor Soper on August 2, 2024. Yejin Choi, who was named a 2022 MacArthur Fellow and ... ‚Äî Tim Dettmers is joining Ai2 as an AI researcher. Dettmers specializes in efficient deep learning at the intersection of machine learning, NLP, and computer systems with a focus on quantization ... In the chat logs quoted in the complaint, researcher Tim Dettmers describes his back-and-forth with Meta\\'s legal department over whether use of the book files as training data would be \"legally ok.\" Essentially, a CPU is a latency-optimized device while GPUs are bandwidth-optimized devices. If a CPU is a race car, a GPU is a cargo truck. The main job in deep learning is to fetch and move ...', name='duckduckgo_search', tool_call_id='call_FIaUESayTuPtOvZTiyYqOKyi'), ToolMessage(content='By Christopher Beam. Oct. 6, 2023. A hike is Michael Lewis\\'s interview format of choice. When he first met the FTX founder Sam Bankman-Fried in late 2021, he took the cargo-shorted chief ... By Andrew R. Chow. October 9, 2023 9:20 AM EDT. W hen the journalist Michael Lewis announced in May that he was writing a book on Sam Bankman-Fried after spending months with the FTX crypto mogul ... Author Michael Lewis spent countless hours with FTX founder and alleged fraudster Sam Bankman-Fried. He details the cryptocurrency wunderkind\\'s rise and fall in his new book, \"Going Infinite.&quot; The Industry Michael Lewis\\' Front-Row Seat at the Sh*t Show The author got a blurry view of Sam Bankman-Fried at a very close distance. When I met SBF, I had a very different impression. Michael Lewis gave a peek into details from his new book on Sam Bankman-Fried in a tell-all interview. \"The Big Short\" author said he had over 100 meetings with Bankman-Fried. Lewis said SBF was ...', name='duckduckgo_search', tool_call_id='call_4Pdo2jbdPvdZd8G2nnRxYdVC'), ToolMessage(content='November 21, 2023 at 7:11 PM EST. Save. Tiger Global Management\\'s Scott Shleifer, the partner overseeing a majority of the firm\\'s assets and who helped launch its venture arm, is stepping down ... Details: Shleifer will remain a senior advisor to Tiger, while firm founder Chase Coleman will assume the chairman role on a new private fund investment committee. Tiger is in the midst of raising ... Tiger Global Management is going through a major management change. Per a message that founder Chase Coleman sent this afternoon to investors of the 22-year-old venture- and hedge-fund outfit and ... Scott Shleifer is stepping down as head of private equity at Tiger Global, one of the driving forces behind the tech \"unicorn\" boom, Axios has learned. Details: Shleifer will remain a senior advisor to Tiger, which was founded to manage hedge funds, while firm founder Chase Coleman will assume the chairman role on a new private equity ... Chase Coleman, the founder of Tiger Global, has announced that he will be taking over both the public company investing and private equity businesses, while Scott Shleifer, the longtime head of the private equity division, will transition into a senior advisory role starting January 1. Shleifer\\'s new position as a full-time advisor has no ...', name='duckduckgo_search', tool_call_id='call_FIkQCcNnv1VMA4EzF3JsLrS6'), ToolMessage(content=\"Luke Zettlemoyer is a research manager and site lead for FAIR Seattle. He is also a Professor in the Allen School of Computer Science & Engineering at the University of Washington. His research is in empirical computational semantics, where the goal is to build models that recover representations of the meaning of natural language text. Hibs sporting director Malky Mackay admits the club could delve into the free agent market after missing out to Luke McCowan to Celtic in a dramatic deadline day at Easter Road.. The Easter Road ... Today we're joined by Luke Zettlemoyer, professor at University of Washington and a research manager at Meta. In our conversation with Luke, we cover multimodal generative AI, the effect of data on models, and the significance of open source and open science. We explore the grounding problem, the need for visual grounding and embodiment in AUTHORS Written by Scott Yih Luke Zettlemoyer Mike Lewis Hannaneh Hajishirzi Kalpesh Krishna Mohit Iyyer Pang Wei Koh Sewon Min Xinxi Lyu Publisher EMNLP Research Topics Natural Language Processing (NLP) Yichao LuSrihari JayakumarDebojeet ChatterjeeMohsen MoslehpourPierce ChuangAbhay HarpaleVikas BhardwajDi Xu (SWE)Shicong ZhaoAnkit ... Luke Zettlemoyer, a professor at University of Washington and a research manager at Meta, discusses multimodal generative AI, visual grounding and embodiment in text-based models, advantages of discretization tokenization in image generation, self-alignment with instruction backtranslation, generalizability of language models, model performance and evaluation, the importance of open source and ...\", name='duckduckgo_search', tool_call_id='call_QI8mgBvXvep0ZqgQoBUOkiWC')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='### QLoRA Paper on Arxiv\\n**Title:** QLoRA: Efficient Finetuning of Quantized LLMs  \\n**Authors:** Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer  \\n**Summary:** QLoRA is an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. The paper introduces several innovations to save memory without sacrificing performance, such as 4-bit NormalFloat (NF4), double quantization, and paged optimizers. The best model family, Guanaco, outperforms previous models on the Vicuna benchmark, reaching 99.3% of ChatGPT\\'s performance level with only 24 hours of finetuning on a single GPU. The paper also provides a detailed analysis of chatbot performance and highlights the limitations of current chatbot benchmarks.\\n\\n### Latest Tweets of Authors\\n\\n#### Tim Dettmers\\n- **Latest Tweet:** Tim Dettmers is joining Ai2 as an AI researcher. Dettmers specializes in efficient deep learning at the intersection of machine learning, NLP, and computer systems with a focus on quantization.\\n\\n#### Mike Lewis\\n- **Latest Tweet:** Michael Lewis gave a peek into details from his new book on Sam Bankman-Fried in a tell-all interview. \"The Big Short\" author said he had over 100 meetings with Bankman-Fried. Lewis said SBF was ...\\n\\n#### Sam Shleifer\\n- **Latest Tweet:** Scott Shleifer is stepping down as head of private equity at Tiger Global, one of the driving forces behind the tech \"unicorn\" boom, Axios has learned. Details: Shleifer will remain a senior advisor to Tiger, which was founded to manage hedge funds, while firm founder Chase Coleman will assume the chairman role on a new private equity ...\\n\\n#### Luke Zettlemoyer\\n- **Latest Tweet:** Luke Zettlemoyer, a professor at University of Washington and a research manager at Meta, discusses multimodal generative AI, visual grounding and embodiment in text-based models, advantages of discretization tokenization in image generation, self-alignment with instruction backtranslation, generalizability of language models, model performance and evaluation, the importance of open source and ...', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 478, 'prompt_tokens': 2392, 'total_tokens': 2870}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-378aae6c-36d7-4cba-8589-e3be64ec6da0-0', usage_metadata={'input_tokens': 2392, 'output_tokens': 478, 'total_tokens': 2870})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Search Arxiv for the QLoRA paper, then search each of the authors to find out their latest Tweet using DuckDuckGo.\")]}\n",
        "\n",
        "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        if node == \"action\":\n",
        "          print(f\"Tool Used: {values['messages'][0].name}\")\n",
        "        print(values[\"messages\"])\n",
        "\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "####üèóÔ∏è Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer.\n",
        "\n",
        "1. Initial Request: The state object was initialized with the user‚Äôs query asking to \"Search Arxiv for the QLoRA paper, then search each of the authors to find out their latest Tweet using DuckDuckGo.\"\n",
        "2. State Passed to Agent Node: The state object, containing the initial request, was passed to the entry point (agent node). The agent node processed the request and recognized that the query involves multiple actions (searching Arxiv and DuckDuckGo).\n",
        "3. First Tool Invocation (Arxiv): The agent node added a message indicating a tool call for Arxiv to the state object and passed it along the conditional edge to the action node. The action node then executed the Arxiv search tool and added the results to the state.\n",
        "4. Loop Back to Agent Node: After performing the Arxiv search, the action node passed the updated state back to the agent node. The agent node reviewed the updated state and recognized that there was still an outstanding task (searching DuckDuckGo).\n",
        "5. Second Tool Invocation (DuckDuckGo): The agent node added another message indicating a tool call for DuckDuckGo to the state object and passed it along the conditional edge to the action node again. The action node executed the DuckDuckGo search tool, retrieved the latest tweets of the authors, and added these results to the state.\n",
        "6. Final State Update: After the DuckDuckGo tool completed its task, the action node passed the final updated state back to the agent node. The agent node added the final response to the state object and, since there were no more tasks to complete, passed the state to the END node.\n",
        "7. Completion: The process completed successfully, with the final output including both the results from the Arxiv search and the DuckDuckGo search, demonstrating the agent‚Äôs ability to handle multiple tools in sequence based on a single complex user query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7c8-Uyarh1v"
      },
      "source": [
        "## Part 2: LangSmith Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | compiled_graph | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "b160f3e4-89d1-4411-ed96-fe17b3cad200"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'RAG stands for Retrieval-Augmented Generation. It is a technique used in natural language processing (NLP) and machine learning to improve the performance of language models by combining retrieval-based methods with generative models. Here‚Äôs a brief overview of how it works:\\n\\n1. **Retrieval**: In the first step, the system retrieves relevant documents or pieces of information from a large corpus or database. This is typically done using a retrieval model, such as BM25 or a dense retrieval model like DPR (Dense Passage Retrieval).\\n\\n2. **Augmentation**: The retrieved documents are then used to augment the input to the generative model. This means that the generative model has access to additional context or information that can help it produce more accurate and relevant responses.\\n\\n3. **Generation**: Finally, the generative model (such as GPT-3 or BERT) uses the augmented input to generate a response. The additional context provided by the retrieved documents helps the model generate more informed and contextually appropriate responses.\\n\\nRAG is particularly useful in scenarios where the generative model needs to produce responses based on a large and diverse set of information, such as in question-answering systems, chatbots, and other applications requiring detailed and accurate information retrieval and generation.'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain.invoke({\"question\" : \"What is RAG?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "### Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "####üèóÔ∏è Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What are the differences between CSS Grid and Flexbox?\",\n",
        "    \"How do you create a responsive design using media queries?\",\n",
        "    \"What is the purpose of the 'position' property in CSS?\",\n",
        "    \"How can you center a div both horizontally and vertically using CSS?\",\n",
        "    \"What are CSS Grid template areas, and how are they used?\",\n",
        "    \"How does the 'auto-fill' value work in CSS Grid layout?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"flexible\", \"one-dimensional\", \"two-dimensional\"]},\n",
        "    {\"must_mention\" : [\"media queries\", \"breakpoints\", \"responsive\"]},\n",
        "    {\"must_mention\" : [\"relative\", \"absolute\", \"fixed\", \"sticky\"]},\n",
        "    {\"must_mention\" : [\"flexbox\", \"grid\", \"align-items\", \"justify-content\"]},\n",
        "    {\"must_mention\" : [\"grid-template-areas\", \"named areas\", \"layout\"]},\n",
        "    {\"must_mention\" : [\"auto-fill\", \"grid-template-columns\", \"responsive grid\"]}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about advanced layouts with CSS to Evaluate RAG over this topic.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### ‚ùì Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not\n",
        "\n",
        "The correct answers are matched with the questions based on their order in the lists. So, the first question is paired with the first answer, the second question with the second answer, and so on. This method is easy to set up and works fine when the dataset is small and straightforward. But it does have a downside‚Äîif the order gets mixed up, the wrong answers might get paired with the wrong questions, leading to incorrect evaluations. Plus, as the list grows, keeping everything in the right order can become a bit of a hassle.\n",
        "\n",
        "A more reliable approach could be to use explicit pairing, where each question is directly linked to its correct answer in a dictionary. This way, even if the order changes, each question will still have the right answer attached to it. It‚Äôs a bit more work upfront, but it makes the evaluation process more robust and less prone to errors, especially when dealing with larger or more complex datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "### Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### ‚ùì Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method.\n",
        "\n",
        "The must_mention evaluator you've set up is straightforward and effective for basic checks, but there are several ways to enhance its functionality. One current limitation is the strict matching approach, where every required phrase must be present in the prediction for it to pass. This could be improved by implementing partial matching or fuzzy matching, using a library like fuzzywuzzy. This would allow the evaluator to recognize synonymous or similarly structured phrases as valid matches, making the evaluation more flexible.\n",
        "\n",
        "Another area for improvement is assessing the contextual relevance of the key phrases. Currently, the evaluator only checks for the presence of phrases, but it doesn't determine whether they are used correctly or meaningfully. Enhancing the evaluator to analyze the surrounding words or using a more advanced language model could help ensure that key phrases are not just mentioned, but also applied appropriately in context.\n",
        "\n",
        "The current setup treats all required phrases equally, which might not reflect their true importance in the context of the answer. Introducing a weighting system, where some phrases carry more weight than others, could provide a more nuanced evaluation. For example, essential phrases could have a higher impact on the final score, rather than a simple binary pass/fail system.\n",
        "\n",
        "Also, the evaluator could be improved by handling negations and ambiguities better. Right now, it doesn't account for cases where a phrase is mentioned in a negating or contradictory manner, such as \"X is not important\" versus \"X is important.\" Implementing logic to detect these situations, possibly through sentence structure analysis or sentiment analysis, would make the evaluation more accurate. Additionally, introducing criteria to assess the overall quality of the response, such as its length, conciseness, and structure, would ensure that the agent provides not just correct, but well-formed answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4DVSXl0BX5"
      },
      "source": [
        "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "8b98fff1-bd75-4dbe-cadc-a76d8a82577c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'RAG Pipeline - Evaluation - 706efb33' at:\n",
            "https://smith.langchain.com/o/55f39e46-bf14-5a73-afb7-c39afc25b44b/datasets/83682756-3794-4e35-9043-99e56ff53b36/compare?selectedSessions=55f8f793-69f7-4ddd-bf1a-510d95f1d2cc\n",
            "\n",
            "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - e2e8ca8d at:\n",
            "https://smith.langchain.com/o/55f39e46-bf14-5a73-afb7-c39afc25b44b/datasets/83682756-3794-4e35-9043-99e56ff53b36\n",
            "[------------------------------------------------->] 6/6"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h3>Experiment Results:</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feedback.must_mention</th>\n",
              "      <th>error</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>run_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>83b39819-7d83-413a-b786-8c78913cf5ea</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.964740</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.099104</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.948715</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.477056</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.822384</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.397398</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.910604</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       feedback.must_mention error  execution_time  \\\n",
              "count                      6     0        6.000000   \n",
              "unique                     2     0             NaN   \n",
              "top                     True   NaN             NaN   \n",
              "freq                       5   NaN             NaN   \n",
              "mean                     NaN   NaN        8.964740   \n",
              "std                      NaN   NaN        2.099104   \n",
              "min                      NaN   NaN        5.948715   \n",
              "25%                      NaN   NaN        7.477056   \n",
              "50%                      NaN   NaN        9.822384   \n",
              "75%                      NaN   NaN       10.397398   \n",
              "max                      NaN   NaN       10.910604   \n",
              "\n",
              "                                      run_id  \n",
              "count                                      6  \n",
              "unique                                     6  \n",
              "top     83b39819-7d83-413a-b786-8c78913cf5ea  \n",
              "freq                                       1  \n",
              "mean                                     NaN  \n",
              "std                                      NaN  \n",
              "min                                      NaN  \n",
              "25%                                      NaN  \n",
              "50%                                      NaN  \n",
              "75%                                      NaN  \n",
              "max                                      NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'RAG Pipeline - Evaluation - 706efb33',\n",
              " 'results': {'51bf3b9a-3980-4f29-bf04-30eaa088e597': {'input': {'question': 'How can you center a div both horizontally and vertically using CSS?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('1850a3e3-fa84-4a88-ba94-42707fc395bc'), target_run_id=None)],\n",
              "   'execution_time': 5.948715,\n",
              "   'run_id': '83b39819-7d83-413a-b786-8c78913cf5ea',\n",
              "   'output': 'To center a `div` both horizontally and vertically using CSS, you can use several methods. Here are a few common approaches:\\n\\n### 1. Using Flexbox\\nFlexbox is a modern and flexible way to center elements.\\n\\n```css\\n.container {\\n  display: flex;\\n  justify-content: center; /* Center horizontally */\\n  align-items: center;    /* Center vertically */\\n  height: 100vh;          /* Full viewport height */\\n}\\n\\n.centered-div {\\n  /* Your div styles */\\n}\\n```\\n\\n### 2. Using Grid\\nCSS Grid is another powerful layout system that can be used to center elements.\\n\\n```css\\n.container {\\n  display: grid;\\n  place-items: center; /* Center both horizontally and vertically */\\n  height: 100vh;       /* Full viewport height */\\n}\\n\\n.centered-div {\\n  /* Your div styles */\\n}\\n```\\n\\n### 3. Using Positioning\\nYou can also use absolute positioning along with transform to center a `div`.\\n\\n```css\\n.container {\\n  position: relative;\\n  height: 100vh; /* Full viewport height */\\n}\\n\\n.centered-div {\\n  position: absolute;\\n  top: 50%;\\n  left: 50%;\\n  transform: translate(-50%, -50%);\\n  /* Your div styles */\\n}\\n```\\n\\n### 4. Using Margin Auto\\nThis method works if the `div` has a fixed width and height.\\n\\n```css\\n.container {\\n  height: 100vh; /* Full viewport height */\\n  display: flex;\\n  justify-content: center; /* Center horizontally */\\n  align-items: center;    /* Center vertically */\\n}\\n\\n.centered-div {\\n  width: 200px;  /* Fixed width */\\n  height: 200px; /* Fixed height */\\n  margin: auto;\\n  /* Your div styles */\\n}\\n```\\n\\n### Example HTML\\nHere\\'s an example of how you might structure your HTML:\\n\\n```html\\n<div class=\"container\">\\n  <div class=\"centered-div\">\\n    <!-- Your content here -->\\n  </div>\\n</div>\\n```\\n\\nChoose the method that best fits your layout requirements and browser support needs. Flexbox and Grid are generally the most flexible and easiest to use for modern web development.',\n",
              "   'reference': {'must_mention': ['flexbox',\n",
              "     'grid',\n",
              "     'align-items',\n",
              "     'justify-content']}},\n",
              "  '29976ff9-b284-491c-ae46-17ca63967612': {'input': {'question': 'What are CSS Grid template areas, and how are they used?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('1decc759-12ad-497f-95e8-1b7c5510e493'), target_run_id=None)],\n",
              "   'execution_time': 10.910604,\n",
              "   'run_id': '427968f1-f1d5-4868-8178-335fb8b6f5ba',\n",
              "   'output': 'CSS Grid template areas are a feature of the CSS Grid Layout module that allows you to define named areas within a grid container. These named areas can then be referenced to place grid items in specific locations within the grid. This makes it easier to create complex layouts with a more readable and maintainable code.\\n\\n### How to Use CSS Grid Template Areas\\n\\n1. **Define the Grid Container:**\\n   First, you need to define a container element as a grid using the `display: grid;` property.\\n\\n2. **Define the Grid Template Areas:**\\n   Use the `grid-template-areas` property to define the layout of the grid. This property takes a string value where each string represents a row in the grid, and each named area is defined by a unique identifier.\\n\\n3. **Assign Grid Items to Areas:**\\n   Use the `grid-area` property on the grid items to place them in the defined areas.\\n\\n### Example\\n\\nHere is a simple example to illustrate the use of CSS Grid template areas:\\n\\n```html\\n<!DOCTYPE html>\\n<html lang=\"en\">\\n<head>\\n    <meta charset=\"UTF-8\">\\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\\n    <title>CSS Grid Template Areas</title>\\n    <style>\\n        .grid-container {\\n            display: grid;\\n            grid-template-areas:\\n                \\'header header header\\'\\n                \\'sidebar content content\\'\\n                \\'footer footer footer\\';\\n            grid-gap: 10px;\\n            padding: 10px;\\n        }\\n\\n        .header {\\n            grid-area: header;\\n            background-color: #f1f1f1;\\n        }\\n\\n        .sidebar {\\n            grid-area: sidebar;\\n            background-color: #f1f1f1;\\n        }\\n\\n        .content {\\n            grid-area: content;\\n            background-color: #f1f1f1;\\n        }\\n\\n        .footer {\\n            grid-area: footer;\\n            background-color: #f1f1f1;\\n        }\\n\\n        .grid-item {\\n            padding: 20px;\\n            text-align: center;\\n        }\\n    </style>\\n</head>\\n<body>\\n    <div class=\"grid-container\">\\n        <div class=\"grid-item header\">Header</div>\\n        <div class=\"grid-item sidebar\">Sidebar</div>\\n        <div class=\"grid-item content\">Content</div>\\n        <div class=\"grid-item footer\">Footer</div>\\n    </div>\\n</body>\\n</html>\\n```\\n\\n### Explanation\\n\\n- **Grid Container:**\\n  ```css\\n  .grid-container {\\n      display: grid;\\n      grid-template-areas:\\n          \\'header header header\\'\\n          \\'sidebar content content\\'\\n          \\'footer footer footer\\';\\n      grid-gap: 10px;\\n      padding: 10px;\\n  }\\n  ```\\n  This defines a grid with three rows and three columns. The `grid-template-areas` property specifies that the first row will be occupied by the \"header\" area, the second row will have \"sidebar\" and \"content\" areas, and the third row will be the \"footer\" area.\\n\\n- **Grid Items:**\\n  ```css\\n  .header {\\n      grid-area: header;\\n      background-color: #f1f1f1;\\n  }\\n\\n  .sidebar {\\n      grid-area: sidebar;\\n      background-color: #f1f1f1;\\n  }\\n\\n  .content {\\n      grid-area: content;\\n      background-color: #f1f1f1;\\n  }\\n\\n  .footer {\\n      grid-area: footer;\\n      background-color: #f1f1f1;\\n  }\\n  ```\\n  Each grid item is assigned to a specific area using the `grid-area` property. The names used in `grid-area` must match the names defined in `grid-template-areas`.\\n\\nThis approach makes it easy to visualize and manage complex layouts, as you can see the structure of the grid directly in the CSS.',\n",
              "   'reference': {'must_mention': ['grid-template-areas',\n",
              "     'named areas',\n",
              "     'layout']}},\n",
              "  '035cd2a8-3664-4c2e-9647-f8570da25e4e': {'input': {'question': \"How does the 'auto-fill' value work in CSS Grid layout?\"},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('01ca474d-6d46-4345-be00-c4a6ccd75a42'), target_run_id=None)],\n",
              "   'execution_time': 6.710183,\n",
              "   'run_id': '9f35d18c-5a52-43aa-b4e6-3329d1613ed9',\n",
              "   'output': \"In CSS Grid Layout, the `auto-fill` value is used in conjunction with the `repeat()` function to create a flexible grid that can automatically adjust the number of columns or rows based on the available space. This is particularly useful for responsive design, where the layout needs to adapt to different screen sizes.\\n\\nHere's how `auto-fill` works:\\n\\n### Syntax\\n```css\\ngrid-template-columns: repeat(auto-fill, minmax(min, max));\\ngrid-template-rows: repeat(auto-fill, minmax(min, max));\\n```\\n\\n### Explanation\\n- **`repeat(auto-fill, ...)`**: This tells the grid to automatically create as many columns (or rows) as will fit into the container, based on the specified size.\\n- **`minmax(min, max)`**: This function is used to define a flexible size range for the columns (or rows). The `min` value is the minimum size, and the `max` value is the maximum size.\\n\\n### Example\\n```css\\n.container {\\n  display: grid;\\n  grid-template-columns: repeat(auto-fill, minmax(100px, 1fr));\\n  gap: 10px;\\n}\\n```\\n\\nIn this example:\\n- The container will create as many columns as will fit into the available space.\\n- Each column will be at least 100px wide but can grow to fill the remaining space (`1fr` stands for one fraction of the available space).\\n- The `gap` property adds a 10px gap between the grid items.\\n\\n### Behavior\\n- **Filling the Space**: `auto-fill` will create as many columns (or rows) as possible, even if some of them are empty. This ensures that the grid items are evenly distributed across the available space.\\n- **Responsive Design**: As the container resizes (e.g., when the browser window is resized), the number of columns (or rows) will adjust automatically to fit the new size.\\n\\n### Practical Use Case\\nConsider a photo gallery where you want the images to be displayed in a grid that adjusts to the screen size:\\n```css\\n.gallery {\\n  display: grid;\\n  grid-template-columns: repeat(auto-fill, minmax(150px, 1fr));\\n  gap: 15px;\\n}\\n```\\nIn this case, the gallery will:\\n- Create as many columns as will fit into the container, with each column being at least 150px wide.\\n- Adjust the number of columns as the screen size changes, ensuring a responsive layout.\\n\\n### Summary\\nThe `auto-fill` value in CSS Grid Layout is a powerful tool for creating flexible, responsive grids that adapt to different screen sizes by automatically adjusting the number of columns or rows based on the available space.\",\n",
              "   'reference': {'must_mention': ['auto-fill',\n",
              "     'grid-template-columns',\n",
              "     'responsive grid']}},\n",
              "  '9e801cb3-77b7-486f-b510-0260f0c2ea45': {'input': {'question': 'What are the differences between CSS Grid and Flexbox?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('28aeff18-6d6f-4314-b922-3891023f4187'), target_run_id=None)],\n",
              "   'execution_time': 10.574168,\n",
              "   'run_id': 'cb748ff1-9722-4b02-8b82-ed32f0838ee4',\n",
              "   'output': 'CSS Grid and Flexbox are both powerful layout systems in CSS, but they are designed for different purposes and have distinct features. Here are the key differences between them:\\n\\n### 1. Layout Scope\\n- **CSS Grid**: Designed for two-dimensional layouts. It can handle both rows and columns, making it ideal for complex layouts that require precise control over both dimensions.\\n- **Flexbox**: Designed for one-dimensional layouts. It excels at distributing space along a single axis (either horizontally or vertically), making it ideal for simpler layouts or for aligning items within a container.\\n\\n### 2. Container vs. Item\\n- **CSS Grid**: The layout is defined on the container (the grid container), and the items (grid items) are placed within this grid according to the defined grid lines.\\n- **Flexbox**: The layout is also defined on the container (the flex container), but the items (flex items) are aligned and distributed along the main axis and cross axis based on the properties applied to the container and the items.\\n\\n### 3. Alignment and Distribution\\n- **CSS Grid**: Offers more control over both horizontal and vertical alignment and distribution of items. You can place items in specific grid cells, span multiple cells, and control the size of rows and columns independently.\\n- **Flexbox**: Provides powerful alignment and distribution capabilities along a single axis. Items can be aligned and distributed within the container using properties like `justify-content`, `align-items`, and `align-self`.\\n\\n### 4. Item Placement\\n- **CSS Grid**: Items can be explicitly placed in specific grid cells using properties like `grid-row`, `grid-column`, `grid-area`, etc. This allows for precise control over the layout.\\n- **Flexbox**: Items are placed in the order they appear in the source code, but their order can be changed using the `order` property. Flexbox does not provide the same level of control over item placement as Grid.\\n\\n### 5. Use Cases\\n- **CSS Grid**: Best suited for complex layouts that require a two-dimensional approach, such as web page layouts, dashboards, and other grid-based designs.\\n- **Flexbox**: Best suited for simpler, one-dimensional layouts, such as navigation bars, form controls, and aligning items within a container.\\n\\n### 6. Browser Support\\n- **CSS Grid**: Modern browsers have good support for CSS Grid, but older browsers may not support it fully.\\n- **Flexbox**: Has been around longer than CSS Grid and has broader support across both modern and older browsers.\\n\\n### 7. Syntax\\n- **CSS Grid**: Uses properties like `display: grid`, `grid-template-rows`, `grid-template-columns`, `grid-gap`, `grid-area`, etc.\\n- **Flexbox**: Uses properties like `display: flex`, `flex-direction`, `flex-wrap`, `justify-content`, `align-items`, `align-content`, etc.\\n\\n### Summary\\n- **CSS Grid**: Two-dimensional, complex layouts, precise control over rows and columns.\\n- **Flexbox**: One-dimensional, simpler layouts, powerful alignment and distribution along a single axis.\\n\\nBoth CSS Grid and Flexbox can be used together to create responsive and flexible web layouts. The choice between them depends on the specific requirements of the layout you are trying to achieve.',\n",
              "   'reference': {'must_mention': ['flexible',\n",
              "     'one-dimensional',\n",
              "     'two-dimensional']}},\n",
              "  '255495a6-9d4a-4b1b-9c13-0a965d2bb5b1': {'input': {'question': 'How do you create a responsive design using media queries?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('67d0dc4d-485e-4d1b-a540-17fc190d43f7'), target_run_id=None)],\n",
              "   'execution_time': 9.86709,\n",
              "   'run_id': '8ba2e6ad-3ac5-442b-ac30-9d5f226b580a',\n",
              "   'output': 'Creating a responsive design using media queries involves writing CSS rules that apply only when certain conditions are met, such as the width of the viewport. Here are the steps to create a responsive design using media queries:\\n\\n1. **Basic Structure**:\\n   Start with the basic HTML and CSS structure for your webpage.\\n\\n   ```html\\n   <!DOCTYPE html>\\n   <html lang=\"en\">\\n   <head>\\n       <meta charset=\"UTF-8\">\\n       <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\\n       <title>Responsive Design</title>\\n       <link rel=\"stylesheet\" href=\"styles.css\">\\n   </head>\\n   <body>\\n       <div class=\"container\">\\n           <header>Header</header>\\n           <main>Main Content</main>\\n           <footer>Footer</footer>\\n       </div>\\n   </body>\\n   </html>\\n   ```\\n\\n   ```css\\n   /* styles.css */\\n   body {\\n       font-family: Arial, sans-serif;\\n       margin: 0;\\n       padding: 0;\\n   }\\n\\n   .container {\\n       display: flex;\\n       flex-direction: column;\\n       min-height: 100vh;\\n   }\\n\\n   header, main, footer {\\n       padding: 20px;\\n       text-align: center;\\n   }\\n\\n   header {\\n       background-color: #f8f9fa;\\n   }\\n\\n   main {\\n       flex: 1;\\n       background-color: #e9ecef;\\n   }\\n\\n   footer {\\n       background-color: #f8f9fa;\\n   }\\n   ```\\n\\n2. **Add Media Queries**:\\n   Add media queries to your CSS file to adjust the layout based on the viewport size.\\n\\n   ```css\\n   /* Default styles for mobile devices */\\n   .container {\\n       flex-direction: column;\\n   }\\n\\n   /* Media query for tablets and larger devices */\\n   @media (min-width: 768px) {\\n       .container {\\n           flex-direction: row;\\n       }\\n\\n       header, main, footer {\\n           flex: 1;\\n           padding: 40px;\\n       }\\n   }\\n\\n   /* Media query for desktops and larger devices */\\n   @media (min-width: 1024px) {\\n       header, main, footer {\\n           padding: 60px;\\n       }\\n   }\\n   ```\\n\\n3. **Test and Adjust**:\\n   Test your design on different devices and screen sizes to ensure it looks good and functions well. Adjust the media queries and styles as needed.\\n\\n### Example Explanation\\n\\n- **Default Styles**: The default styles apply to mobile devices. The `.container` is set to a column layout, and each section (header, main, footer) has padding and text alignment.\\n- **Media Query for Tablets**: When the viewport width is at least 768px, the `.container` switches to a row layout, and the padding for each section increases.\\n- **Media Query for Desktops**: When the viewport width is at least 1024px, the padding for each section increases further.\\n\\n### Tips for Effective Media Queries\\n\\n- **Mobile-First Approach**: Start with styles for the smallest screens and add media queries for larger screens. This approach ensures a good experience on mobile devices by default.\\n- **Use Relative Units**: Use relative units like percentages, ems, and rems instead of fixed units like pixels to make your design more flexible.\\n- **Test on Real Devices**: Emulators and browser tools are helpful, but testing on real devices ensures your design works in real-world conditions.\\n- **Keep It Simple**: Avoid overly complex media queries. Focus on key breakpoints that cover the majority of devices.\\n\\nBy following these steps and tips, you can create a responsive design that adapts to different screen sizes and provides a good user experience across devices.',\n",
              "   'reference': {'must_mention': ['media queries',\n",
              "     'breakpoints',\n",
              "     'responsive']}},\n",
              "  '5aacc125-903c-48a2-9ae9-00f3a3b04538': {'input': {'question': \"What is the purpose of the 'position' property in CSS?\"},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('032695fc-b99a-4465-8deb-c1cfe35093b3'), target_run_id=None)],\n",
              "   'execution_time': 9.777677,\n",
              "   'run_id': 'f595da8f-9b84-4226-91ed-e7585110b285',\n",
              "   'output': \"The `position` property in CSS is used to specify the positioning method for an element. It determines how an element is positioned in the document and how it interacts with other elements. The `position` property can take several values, each of which affects the element's positioning behavior differently:\\n\\n1. **static**: This is the default value. The element is positioned according to the normal flow of the document. The `top`, `right`, `bottom`, and `left` properties have no effect.\\n\\n2. **relative**: The element is positioned relative to its normal position. The `top`, `right`, `bottom`, and `left` properties can be used to adjust the element's position from where it would normally be.\\n\\n3. **absolute**: The element is positioned relative to its nearest positioned ancestor (an ancestor with a `position` value other than `static`). If there is no such ancestor, it is positioned relative to the initial containing block (usually the viewport). The `top`, `right`, `bottom`, and `left` properties specify the offsets from the edges of the containing block.\\n\\n4. **fixed**: The element is positioned relative to the viewport, which means it stays in the same place even when the page is scrolled. The `top`, `right`, `bottom`, and `left` properties specify the offsets from the edges of the viewport.\\n\\n5. **sticky**: The element is treated as `relative` until it crosses a specified threshold (defined by `top`, `right`, `bottom`, or `left`), at which point it is treated as `fixed`. This is useful for creating elements that stick to the viewport as you scroll past them.\\n\\nHere is an example of how each value can be used:\\n\\n```css\\n/* Static positioning (default) */\\n.static-element {\\n  position: static;\\n}\\n\\n/* Relative positioning */\\n.relative-element {\\n  position: relative;\\n  top: 10px; /* Moves the element 10px down from its normal position */\\n  left: 20px; /* Moves the element 20px to the right from its normal position */\\n}\\n\\n/* Absolute positioning */\\n.absolute-element {\\n  position: absolute;\\n  top: 50px; /* Moves the element 50px down from the top of the containing block */\\n  left: 100px; /* Moves the element 100px to the right from the left of the containing block */\\n}\\n\\n/* Fixed positioning */\\n.fixed-element {\\n  position: fixed;\\n  top: 0; /* Sticks the element to the top of the viewport */\\n  right: 0; /* Sticks the element to the right of the viewport */\\n}\\n\\n/* Sticky positioning */\\n.sticky-element {\\n  position: sticky;\\n  top: 0; /* The element will stick to the top of the viewport when you scroll past it */\\n}\\n```\\n\\nEach of these positioning methods can be used to create different layouts and effects in a web page.\",\n",
              "   'reference': {'must_mention': ['relative',\n",
              "     'absolute',\n",
              "     'fixed',\n",
              "     'sticky']}}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=agent_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTNe4kWrplB"
      },
      "source": [
        "## Part 3: LangGraph with Helpfulness:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "### Task 1: Adding Helpfulness Check and \"Loop\" Limits\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add to our existing conditional edge to obtain the behaviour we desire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oajBwLkFVi1N"
      },
      "source": [
        "####üèóÔ∏è Activity #5:\n",
        "\n",
        "Please write markdown for the following cells to explain what each is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6rN7feNVn9f"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "Here we are creating a new Graph called \"graph_with_helpfulness_check\" and are adding two Nodes to it (agent and action)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "6r6XXA5FJbVf"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ22o2mWVrfp"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "Here we are telling the Graph which Node will be the entry point (the first node to be called when there is an user input)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "HNWHwWxuRiLY"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsXeF6xlaXOZ"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "Here we use conditional logic to determine whether the agent should continue looping or stop. This includes a helpfulness check and a cycle limit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "z_Sq3A9SaV1O"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def tool_call_or_helpful(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "  helpfulness_chain = prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    return \"end\"\n",
        "  else:\n",
        "    return \"continue\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      },
      "source": [
        "####üèóÔ∏è Activity #4:\n",
        "\n",
        "Please write what is happening in our `tool_call_or_helpful` function!\n",
        "\n",
        "It checks whether the last message from the agent triggered a tool call or whether the response was helpful. If the response is deemed helpful, the process ends. If not, it continues, but only until the loop limit (e.g., 10 cycles) is reached."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BhnBW2YVsJO"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "This code is adding conditional edges that basically route the flow back to the Agent if the function returns \"continue\", or moves to the Action node if it returns \"action\" so that a tool might be used. Otherwise, it will terminate the cycle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "aVTKnWMbP_8T"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    tool_call_or_helpful,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"action\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGDLEWOIVtK0"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "Here an edge or connector is added between the Action and the Agent nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "cbDK2MbuREgU"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSI8AOaEVvT-"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "Now that the Graph is ready, we proceed to compile it so that it can be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F67FGCMRVwGz"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "Now we test our new cycles limit and helpfullness logic by passing the Graph a user input to trigger the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "22470df4-9aa7-4751-95b6-d30ce71b17db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_omaXZSHD3h8xWicGb5B5J5XG', 'function': {'arguments': '{\"query\": \"LoRA machine learning\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_3N8Wq91qQK0PxVZfSAC6e3jq', 'function': {'arguments': '{\"query\": \"Tim Dettmers\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_dQcicleGeV2AyP6WYkiatKdI', 'function': {'arguments': '{\"query\": \"Attention in machine learning\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 171, 'total_tokens': 247}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-3de33e75-3fe1-40ff-969a-e7c50d96d8a9-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'LoRA machine learning'}, 'id': 'call_omaXZSHD3h8xWicGb5B5J5XG', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Tim Dettmers'}, 'id': 'call_3N8Wq91qQK0PxVZfSAC6e3jq', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Attention in machine learning'}, 'id': 'call_dQcicleGeV2AyP6WYkiatKdI', 'type': 'tool_call'}], usage_metadata={'input_tokens': 171, 'output_tokens': 76, 'total_tokens': 247})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content='Let\\'s jump on LoRA. Low-Rank Adaptation of LLMs (LoRA) So, in usual fine-tuning, we. Take a pretrained model. Do Transfer Learning over new training data to slightly adjust these pre-trained weights LoRA\\'s approach to decomposing ( Œî W ) into a product of lower rank matrices effectively balances the need to adapt large pre-trained models to new tasks while maintaining computational efficiency. The intrinsic rank concept is key to this balance, ensuring that the essence of the model\\'s learning capability is preserved with significantly ... Low-Rank Adaptation (LoRA) is a widely-used parameter-efficient finetuning method for large language models. LoRA saves memory by training only low rank perturbations to selected weight matrices. In this work, we compare the performance of LoRA and full finetuning on two target domains, programming and mathematics. We consider both the instruction finetuning ($\\\\\\\\approx$100K prompt-response ... Feb 18, 2024. Low-rank adaptation (LoRA) is a machine learning technique that modifies a pretrained model (for example, an LLM or vision transformer) to better suit a specific, often smaller, dataset by adjusting only a small, low-rank subset of the model\\'s parameters. This approach is important because it allows for efficient finetuning of ... \"Lora The Tuner\" By Daniel Warfield using MidJourney. All images by the author unless otherwise specified. Fine tuning is the process of tailoring a machine learning model to a specific application, which can be vital in achieving consistent and high quality performance.', name='duckduckgo_search', tool_call_id='call_omaXZSHD3h8xWicGb5B5J5XG'), ToolMessage(content='Allen School Ph.D. student Tim Dettmers accepted the grand prize for QLoRA, a novel approach to finetuning pretrained models that significantly reduces the amount of GPU memory required ‚Äî from over 780GB to less than 48GB ‚Äî to finetune a 65B parameter model. With QLoRA, the largest publicly available models can be finetuned on a single ... ‚Äî Tim Dettmers is joining Ai2 as an AI researcher. Dettmers specializes in efficient deep learning at the intersection of machine learning, NLP, and computer systems with a focus on quantization ... Tim Dettmers. Video. Tech Moves: AI researcher Yejin Choi leaves Univ. of Washington and Allen Institute for AI. by Todd Bishop & Taylor Soper on August 2, 2024 August 2, 2024 at 11:59 am. Its purpose is to make cutting-edge research by Tim Dettmers, a leading academic expert on quantization and the use of deep learning hardware accelerators, accessible to the general public. QLoRA: One of the core contributions of bitsandbytes towards the democratization of AI. If you have a curiosity about how fancy graphics cards actually work, and why they are so well-suited to AI-type applications, then take a few minutes to read [Tim Dettmers] explain why this is so.‚Ä¶', name='duckduckgo_search', tool_call_id='call_3N8Wq91qQK0PxVZfSAC6e3jq'), ToolMessage(content='Learn how attention mechanisms in deep learning enable models to focus on relevant information and improve performance in tasks such as machine translation, image captioning, and speech recognition. Understand the steps and components of attention mechanism architecture and see examples of its applications. Attention mechanism is a fundamental invention in artificial intelligence and machine learning, redefining the capabilities of deep learning models. This mechanism, inspired by the human mental process of selective focus, has emerged as a pillar in a variety of applications, accelerating developments in natural language processing, computer vision, and beyond. There are several types of attention mechanisms, each designed to cater to specific use cases. Here are a few notable ones: 1. Self-Attention Mechanism. Self-attention, also known as intra ... They all use transformer architecture with attention mechanisms at their core to solve problems across domains. In the Transformer series, we go over the ingredients that have made Transformers a universal recipe for machine learning. First up, we take a visual dive to understand the attention mechanism: Why transformers and attention took over. The concept of \"attention\" in deep learning has its roots in the effort to improve Recurrent Neural Networks (RNNs) for handling longer sequences or sentences. For instance, consider translating a sentence from one language to another. Translating a sentence word-by-word is usually not an option because it ignores the complex grammatical ...', name='duckduckgo_search', tool_call_id='call_dQcicleGeV2AyP6WYkiatKdI')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content=\"### LoRA in Machine Learning\\nLoRA, or Low-Rank Adaptation, is a machine learning technique used to fine-tune large pre-trained models, such as language models or vision transformers, to better suit specific, often smaller, datasets. Instead of adjusting all the parameters of the model, LoRA modifies only a small, low-rank subset of the model's parameters. This approach is computationally efficient and saves memory, making it possible to adapt large models to new tasks without the need for extensive computational resources.\\n\\n### Tim Dettmers\\nTim Dettmers is a researcher specializing in efficient deep learning, particularly at the intersection of machine learning, natural language processing (NLP), and computer systems. He is known for his work on quantization and the use of deep learning hardware accelerators. One of his notable contributions is QLoRA, a method that significantly reduces the GPU memory required to fine-tune large pre-trained models. Tim Dettmers is associated with the Allen School and has joined the Allen Institute for AI (Ai2) as an AI researcher.\\n\\n### Attention in Machine Learning\\nAttention mechanisms are a fundamental component in modern deep learning models, particularly in natural language processing (NLP) and computer vision. Inspired by the human cognitive process of selective focus, attention mechanisms allow models to focus on relevant parts of the input data, improving performance in tasks such as machine translation, image captioning, and speech recognition.\\n\\nThere are several types of attention mechanisms, including:\\n1. **Self-Attention**: Also known as intra-attention, it allows a model to focus on different parts of a single sequence.\\n2. **Multi-Head Attention**: Extends self-attention by using multiple attention mechanisms in parallel, allowing the model to focus on different parts of the sequence simultaneously.\\n\\nAttention mechanisms are a core component of transformer architectures, which have become the standard for many deep learning tasks due to their ability to handle long-range dependencies and parallelize training.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 399, 'prompt_tokens': 1151, 'total_tokens': 1550}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-f7ff273e-59cf-4b75-8219-8b0798a48d97-0', usage_metadata={'input_tokens': 1151, 'output_tokens': 399, 'total_tokens': 1550})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "\n",
        "async for chunk in agent_with_helpfulness_check.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVmZPs6lnpsM"
      },
      "source": [
        "### Task 2: LangGraph for the \"Patterns\" of GenAI\n",
        "\n",
        "Let's ask our system about the 4 patterns of Generative AI:\n",
        "\n",
        "1. Prompt Engineering\n",
        "2. RAG\n",
        "3. Fine-tuning\n",
        "4. Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "ZoLl7GlXoae-"
      },
      "outputs": [],
      "source": [
        "patterns = [\"prompt engineering\", \"RAG\", \"fine-tuning\", \"LLM-based agents\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkh0YJuCp3Zl",
        "outputId": "0593c6f5-0a1d-41f6-960b-f32d101b3ade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt engineering is a concept primarily associated with the field of artificial intelligence, particularly in the context of natural language processing (NLP) and large language models like GPT-3. It involves the design and crafting of prompts (input text) to elicit desired responses from AI models. The goal is to optimize the input to get the most accurate, relevant, or useful output from the model.\n",
            "\n",
            "### Key Aspects of Prompt Engineering:\n",
            "1. **Crafting Effective Prompts**: Designing prompts that guide the AI to produce the desired output.\n",
            "2. **Iterative Testing**: Continuously refining prompts based on the responses received.\n",
            "3. **Understanding Model Behavior**: Knowing how different models respond to various types of input.\n",
            "4. **Use Cases**: Applications range from chatbots and virtual assistants to content generation and data analysis.\n",
            "\n",
            "### Emergence of Prompt Engineering:\n",
            "Prompt engineering became more prominent with the advent of large-scale language models like OpenAI's GPT-3, which was released in June 2020. The ability of these models to generate human-like text based on prompts led to a growing interest in how to effectively communicate with them to achieve specific goals.\n",
            "\n",
            "To get more precise information on when prompt engineering became a recognized field and its development, I can look up recent articles or papers. Would you like me to do that?\n",
            "\n",
            "\n",
            "\n",
            "RAG stands for Retrieval-Augmented Generation. It is a technique in natural language processing (NLP) that combines retrieval-based methods with generative models to improve the quality and relevance of generated text. The key idea is to retrieve relevant documents or pieces of information from a large corpus and use this retrieved information to guide the generation process.\n",
            "\n",
            "RAG was introduced by researchers at Facebook AI in a paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\" which was published in 2020. The approach leverages both retrieval and generation to handle knowledge-intensive tasks more effectively than using either method alone.\n",
            "\n",
            "Would you like more detailed information or specific papers on RAG?\n",
            "\n",
            "\n",
            "\n",
            "Fine-tuning is a process in machine learning where a pre-trained model is further trained on a new, often smaller, dataset to adapt it to a specific task. This approach leverages the knowledge the model has already acquired during its initial training on a large dataset, making it more efficient and effective for specialized tasks.\n",
            "\n",
            "### Key Points of Fine-Tuning:\n",
            "1. **Pre-trained Model**: Start with a model that has been trained on a large dataset.\n",
            "2. **New Dataset**: Use a smaller, task-specific dataset to further train the model.\n",
            "3. **Adaptation**: The model adjusts its parameters to better perform the new task.\n",
            "4. **Efficiency**: Reduces the need for large amounts of data and computational resources compared to training a model from scratch.\n",
            "\n",
            "### When Did Fine-Tuning Break Onto the Scene?\n",
            "Fine-tuning became particularly prominent with the advent of deep learning and transfer learning techniques. It gained significant attention in the late 2010s, especially with the development of large pre-trained models like BERT (Bidirectional Encoder Representations from Transformers) by Google in 2018. These models demonstrated that fine-tuning could achieve state-of-the-art results on a variety of natural language processing (NLP) tasks.\n",
            "\n",
            "Would you like more detailed information or specific examples of fine-tuning in practice?\n",
            "\n",
            "\n",
            "\n",
            "LLM-based agents, or Large Language Model-based agents, are systems that utilize large language models to interact with their environment, perceive data, and take actions based on the information they process. These agents can be part of multi-agent systems where each agent has a specific role and task specialization.\n",
            "\n",
            "The concept of LLM-based agents has gained significant attention with the rise of large language models, which have shown remarkable success in various domains such as code generation, vulnerability detection, and more. The development and exploration of LLM-based agents have been particularly prominent in recent years, especially with advancements in AI and machine learning technologies.\n",
            "\n",
            "While the exact timeline of when LLM-based agents broke onto the scene is not specified, it is clear that their emergence is closely tied to the advancements in large language models and AI research in the past few years.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for pattern in patterns:\n",
        "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
        "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
        "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "  print(messages[\"messages\"][-1].content)\n",
        "  print(\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
