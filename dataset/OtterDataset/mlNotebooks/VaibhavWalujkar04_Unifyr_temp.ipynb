{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deap\n",
      "  Downloading deap-1.4.1.tar.gz (1.1 MB)\n",
      "     ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "     --------------------- ------------------ 0.6/1.1 MB 12.0 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 0.8/1.1 MB 9.6 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 0.9/1.1 MB 7.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 1.0/1.1 MB 5.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.1/1.1 MB 5.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\adpha\\anaconda3\\lib\\site-packages (from deap) (1.25.0)\n",
      "Building wheels for collected packages: deap\n",
      "  Building wheel for deap (setup.py): started\n",
      "  Building wheel for deap (setup.py): finished with status 'done'\n",
      "  Created wheel for deap: filename=deap-1.4.1-py3-none-any.whl size=97349 sha256=b4800980ee0ec752b77738328a998be69ff4ec03d5901d71dd1276448b4993c1\n",
      "  Stored in directory: c:\\users\\adpha\\appdata\\local\\pip\\cache\\wheels\\f8\\64\\b8\\65eacfbff3024ae2e2beb22e691d5c8abb89fbd863b8049b5f\n",
      "Successfully built deap\n",
      "Installing collected packages: deap\n",
      "Successfully installed deap-1.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install deap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adpha\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\adpha\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import torch\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Ignore warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "experience_weight = 0.2 \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "candidates_data = {\n",
    "    'id': [1, 2, 3, 4],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'skills': [\n",
    "        'Python, Machine Learning, Data Analysis, SQL, TensorFlow, Pandas, NumPy, Keras, Scikit-Learn, ETL',\n",
    "        'Java, Spring, Microservices, Docker, Kubernetes, Maven, RESTful APIs, Hibernate, Git, CI/CD',\n",
    "        'Python, Deep Learning, NLP, PyTorch, Data Visualization, Flask, AWS, Spark, Big Data, SQL',\n",
    "        'JavaScript, React, Node.js, TypeScript, Web Development, Angular, HTML, CSS, Redux, Webpack'\n",
    "    ],\n",
    "    'experience': [7, 5, 4, 6]  # Years of experience\n",
    "}\n",
    "\n",
    "\n",
    "experts_data = {\n",
    "    'id': [1, 2, 3, 4],\n",
    "    'name': ['Dr. Johnson','Dr. Smith', 'Dr. Brown', 'Dr. Taylor'],\n",
    "    'skills': [\n",
    "        'Python, Machine Learning, Data Analysis, SQL, TensorFlow, Pandas, NumPy, Keras, Scikit-Learn, ETL',\n",
    "        'Java, Spring, Microservices, Docker, Kubernetes, Maven, RESTful APIs, Keras, Scikit-Learn, ETL',\n",
    "        'C++, High-Performance Computing, Algorithm Optimization, Data Structures, Machine Learning, SQL, Parallel Computing, MPI, OpenMP',\n",
    "        'JavaScript, Full Stack Development, React, Node.js, TypeScript, Angular, Web Development, HTML, CSS, Redux'\n",
    "    ],\n",
    "    'experience': [15, 10, 12, 8]  # Years of experience\n",
    "}\n",
    "# Candidate David is best matched with Expert Dr. Taylor (Combined Score: 0.98)\n",
    "# Candidate Alice is best matched with Expert Dr. Johnson (Combined Score: 0.90)\n",
    "# Candidate Bob is best matched with Expert Dr. Smith (Combined Score: 0.76)\n",
    "# Candidate Charlie is best matched with Expert Dr. Johnson (Combined Score: 0.30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_with_BERT_embeddings(candidates_data, experts_data):\n",
    "    # Create DataFrames\n",
    "    candidates_df = pd.DataFrame(candidates_data)\n",
    "    experts_df = pd.DataFrame(experts_data)\n",
    "\n",
    "    # Load BERT model and tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Function to get BERT embeddings\n",
    "    def get_bert_embedding(text):\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # Mean pooling over the token embeddings\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        return embeddings.numpy().flatten()\n",
    "\n",
    "    # Generate embeddings for candidates\n",
    "    candidates_df['embedding'] = candidates_df['skills'].apply(get_bert_embedding)\n",
    "\n",
    "    # Generate embeddings for experts\n",
    "    experts_df['embedding'] = experts_df['skills'].apply(get_bert_embedding)\n",
    "\n",
    "    # Convert the embeddings column into a list of arrays for similarity calculation\n",
    "    candidates_embeddings = np.array(candidates_df['embedding'].tolist())\n",
    "    experts_embeddings = np.array(experts_df['embedding'].tolist())\n",
    "\n",
    "    # Normalize experience values to be between 0 and 1\n",
    "    candidates_df['normalized_experience'] = candidates_df['experience'] / candidates_df['experience'].max()\n",
    "    experts_df['normalized_experience'] = experts_df['experience'] / experts_df['experience'].max()\n",
    "\n",
    "    # Compute cosine similarity based on embeddings\n",
    "    similarity_matrix = cosine_similarity(candidates_embeddings, experts_embeddings)\n",
    "\n",
    "    # Factor in the experience by multiplying the cosine similarity with experience ratios\n",
    "    for i in range(similarity_matrix.shape[0]):\n",
    "        for j in range(similarity_matrix.shape[1]):\n",
    "            # Calculate the absolute difference in experience\n",
    "            experience_diff = abs(candidates_df['experience'].iloc[i] - experts_df['experience'].iloc[j])\n",
    "            \n",
    "            # Compute inverse experience ratio (add a small epsilon to avoid division by zero)\n",
    "            epsilon = 1e-6\n",
    "            experience_ratio = 1 / (experience_diff + epsilon)\n",
    "            \n",
    "            # Normalize the experience ratio (optional, for better scaling)\n",
    "            experience_ratio /= (1 + experience_ratio)\n",
    "            \n",
    "            # Adjust the similarity score\n",
    "            similarity_matrix[i, j] *= (1 - experience_weight + experience_weight * experience_ratio)\n",
    "\n",
    "\n",
    "    # Create a DataFrame to easily visualize the similarities\n",
    "    similarity_df = pd.DataFrame(similarity_matrix, \n",
    "                                 index=candidates_df['name'], \n",
    "                                 columns=experts_df['name'])\n",
    "\n",
    "    # Convert the DataFrame into the desired dictionary format\n",
    "    similarity_dict = {}\n",
    "    for candidate in similarity_df.index:\n",
    "        similarity_dict[candidate] = {}\n",
    "        for expert in similarity_df.columns:\n",
    "            similarity_dict[candidate][expert] = similarity_df.loc[candidate, expert]\n",
    "\n",
    "    return similarity_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679947b27d744307a591186a2a7252cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alice': {'Dr. Johnson': 0.8222226, 'Dr. Smith': 0.78318316, 'Dr. Brown': 0.76879954, 'Dr. Taylor': 0.8025037}, 'Bob': {'Dr. Johnson': 0.7377848, 'Dr. Smith': 0.8194764, 'Dr. Brown': 0.7040785, 'Dr. Taylor': 0.76663446}, 'Charlie': {'Dr. Johnson': 0.7819507, 'Dr. Smith': 0.7558123, 'Dr. Brown': 0.7550342, 'Dr. Taylor': 0.7477189}, 'David': {'Dr. Johnson': 0.73017985, 'Dr. Smith': 0.7476584, 'Dr. Brown': 0.6970253, 'Dr. Taylor': 0.8539074}}\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity_with_BERT_embeddings(candidates_data, experts_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_with_topic_modelling(candidates_data, experts_data):\n",
    "    # Convert data to DataFrames\n",
    "    candidates_df = pd.DataFrame(candidates_data)\n",
    "    experts_df = pd.DataFrame(experts_data)\n",
    "\n",
    "    # Combine skills from both candidates and experts\n",
    "    all_skills = pd.concat([candidates_df['skills'], experts_df['skills']])\n",
    "\n",
    "    # Preprocessing - Vectorization using TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(all_skills)\n",
    "\n",
    "    # Apply LDA to extract topics\n",
    "    n_topics = 5  # Number of topics to extract\n",
    "    lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "    # Split LDA matrix back into candidates and experts\n",
    "    candidates_lda = lda_matrix[:len(candidates_df)]\n",
    "    experts_lda = lda_matrix[len(candidates_df):]\n",
    "\n",
    "    # Compute cosine similarity between candidates and experts based on topic distribution\n",
    "    similarity_matrix = cosine_similarity(candidates_lda, experts_lda)\n",
    "\n",
    "    # Normalize expert experience and factor it into the similarity score\n",
    "    experts_df['normalized_experience'] = experts_df['experience'] / experts_df['experience'].max()\n",
    "\n",
    "    for i in range(similarity_matrix.shape[0]):\n",
    "        for j in range(similarity_matrix.shape[1]):\n",
    "            # Calculate the absolute difference in experience between candidate and expert\n",
    "            experience_diff = abs(candidates_df['experience'].iloc[i] - experts_df['experience'].iloc[j])\n",
    "            \n",
    "            # Compute inverse experience ratio (add a small epsilon to avoid division by zero)\n",
    "            epsilon = 1e-6\n",
    "            experience_ratio = 1 / (experience_diff + epsilon)\n",
    "            \n",
    "            # Normalize the experience ratio (optional for scaling)\n",
    "            experience_ratio /= (1 + experience_ratio)\n",
    "            \n",
    "            # Adjust similarity score with experience ratio\n",
    "            similarity_matrix[i, j] *= (1 - experience_weight + experience_weight * experience_ratio)\n",
    "\n",
    "    # Convert similarity matrix to nested dictionary format\n",
    "    similarity_dict = {}\n",
    "    for i, candidate in enumerate(candidates_df['name']):\n",
    "        similarity_dict[candidate] = {}\n",
    "        for j, expert in enumerate(experts_df['name']):\n",
    "            similarity_dict[candidate][expert] = similarity_matrix[i][j]\n",
    "\n",
    "    # Display the similarity dictionary\n",
    "    # for candidate, experts in similarity_dict.items():\n",
    "    #     print(f\"\\nSimilarity scores for {candidate}:\")\n",
    "    #     for expert, score in experts.items():\n",
    "    #         print(f\"  {expert}: {score:.6f}\")\n",
    "\n",
    "    return similarity_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alice': {'Dr. Johnson': 0.822222219753087, 'Dr. Smith': 0.8499948172266458, 'Dr. Brown': 0.09743965749469527, 'Dr. Taylor': 0.10497358350459542}, 'Bob': {'Dr. Johnson': 0.8181762717371288, 'Dr. Smith': 0.8333333120702044, 'Dr. Brown': 0.09816141290626416, 'Dr. Taylor': 0.10100640012049644}, 'Charlie': {'Dr. Johnson': 0.09550338994923883, 'Dr. Smith': 0.0984093176516828, 'Dr. Brown': 0.09841666529184659, 'Dr. Taylor': 0.10059091474959825}, 'David': {'Dr. Johnson': 0.09423165466013468, 'Dr. Smith': 0.09827805159432658, 'Dr. Brown': 0.09779218694259567, 'Dr. Taylor': 0.8666626523490856}}\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity_with_topic_modelling(candidates_data, experts_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Faiss_search(candidates_data, experts_data):\n",
    "\n",
    "    # Convert data to DataFrames\n",
    "    candidates_df = pd.DataFrame(candidates_data)\n",
    "    experts_df = pd.DataFrame(experts_data)\n",
    "\n",
    "    # Load pre-trained Sentence-BERT model to generate embeddings\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Generate embeddings for candidate and expert skills\n",
    "    candidates_embeddings = model.encode(candidates_df['skills'].tolist())\n",
    "    experts_embeddings = model.encode(experts_df['skills'].tolist())\n",
    "\n",
    "    # Normalize embeddings for better FAISS performance\n",
    "    candidates_embeddings = np.array(candidates_embeddings).astype('float32')\n",
    "    experts_embeddings = np.array(experts_embeddings).astype('float32')\n",
    "\n",
    "    # Create a FAISS index\n",
    "    index = faiss.IndexFlatL2(candidates_embeddings.shape[1])  # L2 distance\n",
    "    index.add(experts_embeddings)  # Add expert embeddings to the index\n",
    "\n",
    "    # Search for the nearest experts for each candidate\n",
    "    k = len(experts_df)  # Search for all experts to get similarity with each expert\n",
    "    distances, indices = index.search(candidates_embeddings, k)\n",
    "\n",
    "    # Normalize expert experience and factor it into the similarity score\n",
    "    experts_df['normalized_experience'] = experts_df['experience'] / experts_df['experience'].max()\n",
    "\n",
    "    # Store the matches and similarity scores for each candidate-expert pair\n",
    "    similarity_scores = {}\n",
    "\n",
    "    # Iterate over candidates and store all similarity scores\n",
    "    for i, candidate in enumerate(candidates_df['name']):\n",
    "        candidate_scores = {}\n",
    "        for j in range(k):\n",
    "            expert_idx = indices[i][j]\n",
    "            expert_name = experts_df.iloc[expert_idx]['name']\n",
    "            similarity_score = 1 / (1 + distances[i][j])  # Convert distance to similarity score\n",
    "            \n",
    "            # Calculate the absolute difference in experience between candidate and expert\n",
    "            experience_diff = abs(candidates_df['experience'].iloc[i] - experts_df['experience'].iloc[expert_idx])\n",
    "            \n",
    "            # Compute inverse experience ratio (add a small epsilon to avoid division by zero)\n",
    "            epsilon = 1e-6\n",
    "            experience_ratio = 1 / (experience_diff + epsilon)\n",
    "            \n",
    "            # Normalize the experience ratio (optional for scaling)\n",
    "            experience_ratio /= (1 + experience_ratio)\n",
    "            \n",
    "            # Adjust the similarity score based on the experience ratio\n",
    "            adjusted_similarity_score = similarity_score * (1 - experience_weight + experience_weight * experience_ratio)\n",
    "            candidate_scores[expert_name] = adjusted_similarity_score\n",
    "        \n",
    "        similarity_scores[candidate] = candidate_scores\n",
    "\n",
    "\n",
    "    # Return similarity scores\n",
    "    return similarity_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cda45867e4b47e7b3981848b4057266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809d50b6bcb947cdb90f5d225ff92847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896fc1f2afe747c190b1fbd39fa3902b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee04e24951741c3b838cf3063c59de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f76662cd03b54ce4a976f2a94f5badf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471aade5a9874cfa93e86aec1f5c5448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f24569c755945e38b985c81e874b50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dbba88ee18341eb923bb88e351e7891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86621df057354ff2b4824702c3f72cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c93a5ea3ee84ad2825f70a3cd99c700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664e01a574054ed495b200b807d25ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alice': {'Dr. Johnson': 0.8222222197530867, 'Dr. Brown': 0.45165265724624315, 'Dr. Smith': 0.4356168526591091, 'Dr. Taylor': 0.40463357863721433}, 'Bob': {'Dr. Smith': 0.6425088252451913, 'Dr. Taylor': 0.3937052091258927, 'Dr. Johnson': 0.3305211499619502, 'Dr. Brown': 0.32558745190525046}, 'Charlie': {'Dr. Johnson': 0.5751003083195932, 'Dr. Brown': 0.41159342629668083, 'Dr. Smith': 0.41328184713197835, 'Dr. Taylor': 0.37401273799610585}, 'David': {'Dr. Taylor': 0.7838279698940521, 'Dr. Smith': 0.398487663380908, 'Dr. Johnson': 0.37382813102159523, 'Dr. Brown': 0.3438181601003029}}\n"
     ]
    }
   ],
   "source": [
    "print(Faiss_search(candidates_data, experts_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bipartite_graph_matching(candidates_data, experts_data):\n",
    "    # Convert data to DataFrames\n",
    "    candidates_df = pd.DataFrame(candidates_data)\n",
    "    experts_df = pd.DataFrame(experts_data)\n",
    "\n",
    "    # Load pre-trained Sentence-BERT model to generate embeddings\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Generate embeddings for candidate and expert skills\n",
    "    candidates_embeddings = model.encode(candidates_df['skills'].tolist())\n",
    "    experts_embeddings = model.encode(experts_df['skills'].tolist())\n",
    "\n",
    "    # Compute cosine similarity between each candidate and expert\n",
    "    similarity_matrix = cosine_similarity(candidates_embeddings, experts_embeddings)\n",
    "\n",
    "    # Normalize expert experience and factor it into the similarity scores\n",
    "    experts_df['normalized_experience'] = experts_df['experience'] / experts_df['experience'].max()\n",
    "\n",
    "    # Adjust similarity scores based on expert experience\n",
    "    for candidate_idx in range(len(candidates_df)):\n",
    "        for expert_idx in range(len(experts_df)):\n",
    "            # Calculate the absolute difference in experience between candidate and expert\n",
    "            experience_diff = abs(candidates_df['experience'].iloc[candidate_idx] - experts_df['experience'].iloc[expert_idx])\n",
    "            \n",
    "            # Compute inverse experience ratio (add a small epsilon to avoid division by zero)\n",
    "            epsilon = 1e-6\n",
    "            experience_ratio = 1 / (experience_diff + epsilon)\n",
    "            \n",
    "            # Normalize the experience ratio (optional for scaling)\n",
    "            experience_ratio /= (1 + experience_ratio)\n",
    "            \n",
    "            # Adjust the similarity score based on the experience ratio\n",
    "            similarity_matrix[candidate_idx, expert_idx] *= (\n",
    "                1 - experience_weight + experience_weight * experience_ratio\n",
    "            )\n",
    "\n",
    "\n",
    "    # Create a DataFrame for storing the results\n",
    "    results = []\n",
    "\n",
    "    # Find best matches for each candidate\n",
    "    for candidate_idx, candidate_name in enumerate(candidates_df['name']):\n",
    "        # For each candidate, get the best matches (top experts)\n",
    "        best_experts_idx = np.argsort(-similarity_matrix[candidate_idx])  # Sort indices based on similarity scores in descending order\n",
    "        for expert_idx in best_experts_idx:\n",
    "            expert_name = experts_df.iloc[expert_idx]['name']\n",
    "            similarity_score = similarity_matrix[candidate_idx, expert_idx]\n",
    "            results.append((candidate_name, expert_name, similarity_score))\n",
    "\n",
    "    # Create a dictionary to store similarity scores\n",
    "    similarity_scores = {}\n",
    "    for candidate, expert, score in results:\n",
    "        if candidate not in similarity_scores:\n",
    "            similarity_scores[candidate] = {}\n",
    "        similarity_scores[candidate][expert] = score\n",
    "\n",
    "    return similarity_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alice': {'Dr. Johnson': 0.82222223, 'Dr. Brown': 0.4812183, 'Dr. Smith': 0.44571617, 'Dr. Taylor': 0.34909457}, 'Bob': {'Dr. Smith': 0.7095839, 'Dr. Taylor': 0.35743538, 'Dr. Johnson': 0.21459691, 'Dr. Brown': 0.1922739}, 'Charlie': {'Dr. Johnson': 0.6451495, 'Dr. Smith': 0.4122731, 'Dr. Brown': 0.41207463, 'Dr. Taylor': 0.31671667}, 'David': {'Dr. Taylor': 0.8208697, 'Dr. Smith': 0.37465268, 'Dr. Johnson': 0.330656, 'Dr. Brown': 0.24446538}}\n"
     ]
    }
   ],
   "source": [
    "print(Bipartite_graph_matching(candidates_data, experts_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_jaccard_similarity(candidates_data, experts_data, weight_vector=None):\n",
    "    candidates_df = pd.DataFrame(candidates_data)\n",
    "    experts_df = pd.DataFrame(experts_data)\n",
    "\n",
    "    # Convert skills into binary vectors (0 or 1)\n",
    "    vectorizer = CountVectorizer(binary=True)\n",
    "    all_skills = candidates_df['skills'].tolist() + experts_df['skills'].tolist()\n",
    "    skills_matrix = vectorizer.fit_transform(all_skills)\n",
    "\n",
    "    # Split into candidates and experts vectors\n",
    "    candidates_skills = skills_matrix[:len(candidates_df)].toarray()\n",
    "    experts_skills = skills_matrix[len(candidates_df):].toarray()\n",
    "\n",
    "    if weight_vector is None:\n",
    "        weight_vector = np.ones(candidates_skills.shape[1])\n",
    "\n",
    "    # Apply weights to skills\n",
    "    weighted_candidates_skills = candidates_skills * weight_vector\n",
    "    weighted_experts_skills = experts_skills * weight_vector\n",
    "\n",
    "    # Calculate weighted Jaccard similarity using jaccard_score\n",
    "    similarity_matrix = np.zeros((len(candidates_df), len(experts_df)))\n",
    "\n",
    "    for i, candidate in enumerate(weighted_candidates_skills):\n",
    "        for j, expert in enumerate(weighted_experts_skills):\n",
    "            similarity_matrix[i, j] = jaccard_score(candidate > 0, expert > 0)\n",
    "\n",
    "    similarity_dict = {c: {e: similarity_matrix[i, j] for j, e in enumerate(experts_df['name'])} for i, c in enumerate(candidates_df['name'])}\n",
    "\n",
    "    return similarity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alice': {'Dr. Johnson': 1.0, 'Dr. Smith': 0.19047619047619047, 'Dr. Brown': 0.18181818181818182, 'Dr. Taylor': 0.0}, 'Bob': {'Dr. Johnson': 0.0, 'Dr. Smith': 0.5, 'Dr. Brown': 0.0, 'Dr. Taylor': 0.0}, 'Charlie': {'Dr. Johnson': 0.19047619047619047, 'Dr. Smith': 0.0, 'Dr. Brown': 0.13636363636363635, 'Dr. Taylor': 0.0}, 'David': {'Dr. Johnson': 0.0, 'Dr. Smith': 0.0, 'Dr. Brown': 0.0, 'Dr. Taylor': 0.7857142857142857}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(weighted_jaccard_similarity(candidates_data, experts_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alice': {'Dr. Smith': 1.0}, 'Charlie': {'Dr. Smith': 1.0}, 'Bob': {'Dr. Johnson': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "# print(K_means_Clustering_Algorithm(candidates_data, experts_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_recommendation_algorithm(candidates_data, experts_data):    \n",
    "    # Create DataFrames\n",
    "    candidates_df = pd.DataFrame(candidates_data)\n",
    "    experts_df = pd.DataFrame(experts_data)\n",
    "\n",
    "    # Combine skills to create a vocabulary\n",
    "    all_skills = candidates_df['skills'].tolist() + experts_df['skills'].tolist()\n",
    "\n",
    "    # Vectorize skills\n",
    "    vectorizer = CountVectorizer()\n",
    "    skills_matrix = vectorizer.fit_transform(all_skills)\n",
    "\n",
    "    # Split into candidates and experts features\n",
    "    candidates_skills_matrix = skills_matrix[:len(candidates_df)]\n",
    "    experts_skills_matrix = skills_matrix[len(candidates_df):]\n",
    "\n",
    "    # Convert to DataFrames\n",
    "    candidates_skills_df = pd.DataFrame(candidates_skills_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    experts_skills_df = pd.DataFrame(experts_skills_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # Add experience as a feature\n",
    "    candidates_features = pd.concat([candidates_skills_df, candidates_df['experience']], axis=1)\n",
    "    experts_features = pd.concat([experts_skills_df, experts_df['experience']], axis=1)\n",
    "\n",
    "    # Convert all column names to strings\n",
    "    candidates_features.columns = candidates_features.columns.astype(str)\n",
    "    experts_features.columns = experts_features.columns.astype(str)\n",
    "\n",
    "    # Ensure all features are numeric\n",
    "    candidates_features = candidates_features.apply(pd.to_numeric, errors='ignore')\n",
    "    experts_features = experts_features.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    candidates_features_scaled = scaler.fit_transform(candidates_features)\n",
    "    experts_features_scaled = scaler.transform(experts_features)\n",
    "\n",
    "    # Combine all features for collaborative filtering\n",
    "    all_features_scaled = np.vstack([candidates_features_scaled, experts_features_scaled])\n",
    "\n",
    "    # Perform Dimensionality Reduction\n",
    "    svd = TruncatedSVD(n_components=10)\n",
    "    all_features_reduced = svd.fit_transform(all_features_scaled)\n",
    "\n",
    "    # Calculate cosine similarity for collaborative filtering\n",
    "    similarity_matrix = cosine_similarity(all_features_reduced)\n",
    "\n",
    "    # Split back into candidates and experts similarity matrix\n",
    "    candidates_similarity_matrix = similarity_matrix[:len(candidates_df), len(candidates_df):]\n",
    "    experts_similarity_matrix = similarity_matrix[len(candidates_df):, :len(candidates_df)]\n",
    "\n",
    "    # Transpose the experts_similarity_matrix to match the shape\n",
    "    experts_similarity_matrix = experts_similarity_matrix.T\n",
    "\n",
    "    # Define weights for hybrid recommendation\n",
    "    alpha = 0.5  # Weight for content-based scores\n",
    "    beta = 0.5   # Weight for collaborative scores\n",
    "\n",
    "    # Compute final scores\n",
    "    final_scores = alpha * candidates_similarity_matrix + beta * experts_similarity_matrix\n",
    "    experts_df['normalized_experience'] = experts_df['experience'] / experts_df['experience'].max()\n",
    "\n",
    "    for i in range(len(candidates_df)):\n",
    "        for j in range(len(experts_df)):\n",
    "            # Calculate the absolute difference in experience between candidate and expert\n",
    "            experience_diff = abs(candidates_df['experience'].iloc[i] - experts_df['experience'].iloc[j])\n",
    "            \n",
    "            # Compute inverse experience ratio (add a small epsilon to avoid division by zero)\n",
    "            epsilon = 1e-6\n",
    "            experience_ratio = 1 / (experience_diff + epsilon)\n",
    "            \n",
    "            # Normalize the experience ratio (optional for scaling)\n",
    "            experience_ratio /= (1 + experience_ratio)\n",
    "            \n",
    "            # Adjust the final score based on the experience ratio\n",
    "            final_scores[i, j] *= (1 - experience_weight + experience_weight * experience_ratio)\n",
    "\n",
    "    # Construct output in desired format\n",
    "    output_similarity_matrix = {}\n",
    "    for i, candidate_name in enumerate(candidates_df['name']):\n",
    "        output_similarity_matrix[candidate_name] = {}\n",
    "        for j, expert_name in enumerate(experts_df['name']):\n",
    "            output_similarity_matrix[candidate_name][expert_name] = final_scores[i, j]\n",
    "\n",
    "    return output_similarity_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alice': {'Dr. Johnson': 0.6185295952415933, 'Dr. Smith': 0.14673675318234203, 'Dr. Brown': 0.2302580634126436, 'Dr. Taylor': -0.24590039996073507}, 'Bob': {'Dr. Johnson': -0.2337851425944892, 'Dr. Smith': 0.384396793375986, 'Dr. Brown': -0.10926319014592228, 'Dr. Taylor': -0.22831531671184094}, 'Charlie': {'Dr. Johnson': -0.20074282779498767, 'Dr. Smith': -0.35878861776101106, 'Dr. Brown': -0.07343747072075288, 'Dr. Taylor': -0.3218150677853088}, 'David': {'Dr. Johnson': -0.15111722167349037, 'Dr. Smith': -0.18696324424108163, 'Dr. Brown': -0.034049975615631334, 'Dr. Taylor': 0.7574836205993248}}\n"
     ]
    }
   ],
   "source": [
    "print(hybrid_recommendation_algorithm(candidates_data, experts_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_with_BERT_embeddings_matrix = cosine_similarity_with_BERT_embeddings(candidates_data, experts_data)\n",
    "cosine_similarity_with_topic_modelling_matrix = cosine_similarity_with_topic_modelling(candidates_data, experts_data)\n",
    "Faiss_search_matrix = Faiss_search(candidates_data, experts_data)\n",
    "Bipartite_graph_matching_matrix = Bipartite_graph_matching(candidates_data, experts_data)\n",
    "hybrid_recommendation_algorithm_matrix = hybrid_recommendation_algorithm(candidates_data, experts_data)\n",
    "\n",
    "# without precomputation - 8.9 s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Unique Matching based on Combined Scores:\n",
      "Candidate David is best matched with Expert Dr. Taylor (Combined Score: 0.98)\n",
      "Candidate Alice is best matched with Expert Dr. Johnson (Combined Score: 0.90)\n",
      "Candidate Bob is best matched with Expert Dr. Smith (Combined Score: 0.76)\n",
      "Candidate Charlie is best matched with Expert Dr. Johnson (Combined Score: 0.30)\n"
     ]
    }
   ],
   "source": [
    "# Example scores from each method with desired format\n",
    "methods_scores = {\n",
    "    'cosine_similarity_with_topic_modelling': cosine_similarity_with_topic_modelling_matrix,\n",
    "    'cosine_similarity_with_BERT_embeddings': cosine_similarity_with_BERT_embeddings_matrix,\n",
    "    'faiss_searching': Faiss_search_matrix,\n",
    "    'hybrid_recommendation': hybrid_recommendation_algorithm_matrix\n",
    "}\n",
    "\n",
    "# Normalizing function\n",
    "def normalize_scores(scores):\n",
    "    all_scores = [v for candidate_scores in scores.values() for v in candidate_scores.values()]\n",
    "    min_score = min(all_scores)\n",
    "    max_score = max(all_scores)\n",
    "    return {candidate: {expert: (score - min_score) / (max_score - min_score)\n",
    "                        for expert, score in candidate_scores.items()}\n",
    "            for candidate, candidate_scores in scores.items()}\n",
    "\n",
    "# Normalize all methods' scores\n",
    "normalized_methods_scores = {method: normalize_scores(scores) for method, scores in methods_scores.items()}\n",
    "\n",
    "# Step 2: Combine the scores\n",
    "# Assigning equal weight to all methods\n",
    "weights = {method: 1 / len(normalized_methods_scores) for method in normalized_methods_scores}\n",
    "\n",
    "combined_scores = {candidate: {} for candidate in normalized_methods_scores['faiss_searching'].keys()}\n",
    "\n",
    "for candidate in combined_scores.keys():\n",
    "    for expert in normalized_methods_scores['faiss_searching'][candidate].keys():\n",
    "        combined_score = sum(weights[method] * normalized_methods_scores[method][candidate][expert]\n",
    "                             for method in normalized_methods_scores)\n",
    "        combined_scores[candidate][expert] = combined_score\n",
    "\n",
    "# Step 3: Rank the matches and ensure unique matches per candidate\n",
    "final_matches = sorted([(candidate, expert, score) for candidate, experts in combined_scores.items() \n",
    "                        for expert, score in experts.items()],\n",
    "                       key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Ensure unique matching per candidate\n",
    "assigned_candidates = set()\n",
    "unique_matches = []\n",
    "\n",
    "# Iterate over sorted matches\n",
    "for match in final_matches:\n",
    "    candidate, expert, score = match\n",
    "    if candidate not in assigned_candidates:\n",
    "        unique_matches.append(match)\n",
    "        assigned_candidates.add(candidate)\n",
    "\n",
    "# Display final unique matches\n",
    "print(\"Final Unique Matching based on Combined Scores:\")\n",
    "for match in unique_matches:\n",
    "    print(f\"Candidate {match[0]} is best matched with Expert {match[1]} (Combined Score: {match[2]:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "def optimal_transport_similarity(candidates_data, experts_data):\n",
    "    candidates_df = pd.DataFrame(candidates_data)\n",
    "    experts_df = pd.DataFrame(experts_data)\n",
    "\n",
    "    similarity_dict = {}\n",
    "\n",
    "    for i, candidate in candidates_df.iterrows():\n",
    "        candidate_experience = np.array([candidate['experience']], dtype=float)\n",
    "        candidate_skills = candidate['skills'].split(', ')\n",
    "\n",
    "        similarity_scores = {}\n",
    "\n",
    "        for j, expert in experts_df.iterrows():\n",
    "            expert_experience = np.array([expert['experience']], dtype=float)\n",
    "            expert_skills = expert['skills'].split(', ')\n",
    "\n",
    "            # Convert skills into binary vectors\n",
    "            all_skills = list(set(candidate_skills + expert_skills))\n",
    "            candidate_skill_vector = np.array([1 if skill in candidate_skills else 0 for skill in all_skills], dtype=float)\n",
    "            expert_skill_vector = np.array([1 if skill in expert_skills else 0 for skill in all_skills], dtype=float)\n",
    "\n",
    "            # Calculate Wasserstein distance for skills and experience\n",
    "            skill_distance = wasserstein_distance(candidate_skill_vector, expert_skill_vector)\n",
    "            experience_distance = wasserstein_distance(candidate_experience, expert_experience)\n",
    "\n",
    "            # Combine distances (you can adjust the weights here)\n",
    "            combined_distance = 0.5 * skill_distance + 0.5 * experience_distance\n",
    "\n",
    "            # Convert distance to similarity score (the smaller the distance, the higher the similarity)\n",
    "            similarity_score = 1 / (1 + combined_distance)\n",
    "            similarity_scores[expert['name']] = similarity_score\n",
    "\n",
    "        similarity_dict[candidate['name']] = similarity_scores\n",
    "\n",
    "    return similarity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(wasserstein_distance(candidates_data, experts_data))\n",
      "File \u001b[1;32mc:\\Users\\adpha\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:9076\u001b[0m, in \u001b[0;36mwasserstein_distance\u001b[1;34m(u_values, v_values, u_weights, v_weights)\u001b[0m\n\u001b[0;32m   9002\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwasserstein_distance\u001b[39m(u_values, v_values, u_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, v_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   9003\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   9004\u001b[0m \u001b[38;5;124;03m    Compute the first Wasserstein distance between two 1D distributions.\u001b[39;00m\n\u001b[0;32m   9005\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9074\u001b[0m \n\u001b[0;32m   9075\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 9076\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cdf_distance(\u001b[38;5;241m1\u001b[39m, u_values, v_values, u_weights, v_weights)\n",
      "File \u001b[1;32mc:\\Users\\adpha\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:9209\u001b[0m, in \u001b[0;36m_cdf_distance\u001b[1;34m(p, u_values, v_values, u_weights, v_weights)\u001b[0m\n\u001b[0;32m   9165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cdf_distance\u001b[39m(p, u_values, v_values, u_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, v_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   9166\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   9167\u001b[0m \u001b[38;5;124;03m    Compute, between two one-dimensional distributions :math:`u` and\u001b[39;00m\n\u001b[0;32m   9168\u001b[0m \u001b[38;5;124;03m    :math:`v`, whose respective CDFs are :math:`U` and :math:`V`, the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9207\u001b[0m \n\u001b[0;32m   9208\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 9209\u001b[0m     u_values, u_weights \u001b[38;5;241m=\u001b[39m _validate_distribution(u_values, u_weights)\n\u001b[0;32m   9210\u001b[0m     v_values, v_weights \u001b[38;5;241m=\u001b[39m _validate_distribution(v_values, v_weights)\n\u001b[0;32m   9212\u001b[0m     u_sorter \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(u_values)\n",
      "File \u001b[1;32mc:\\Users\\adpha\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:9273\u001b[0m, in \u001b[0;36m_validate_distribution\u001b[1;34m(values, weights)\u001b[0m\n\u001b[0;32m   9253\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   9254\u001b[0m \u001b[38;5;124;03mValidate the values and weights from a distribution input of `cdf_distance`\u001b[39;00m\n\u001b[0;32m   9255\u001b[0m \u001b[38;5;124;03mand return them as ndarray objects.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9270\u001b[0m \n\u001b[0;32m   9271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   9272\u001b[0m \u001b[38;5;66;03m# Validate the value array.\u001b[39;00m\n\u001b[1;32m-> 9273\u001b[0m values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(values, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m   9274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(values) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   9275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistribution can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'dict'"
     ]
    }
   ],
   "source": [
    "print(wasserstein_distance(candidates_data, experts_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # takes too much time\n",
    "# def Multi_Objective_Genetic_Algorithms(candidates_data, experts_data):\n",
    "#     # Convert data to DataFrames\n",
    "#     candidates_df = pd.DataFrame(candidates_data)\n",
    "#     experts_df = pd.DataFrame(experts_data)\n",
    "\n",
    "#     # Load pre-trained Sentence-BERT model to generate embeddings\n",
    "#     model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#     # Generate embeddings for candidate and expert skills\n",
    "#     candidates_embeddings = model.encode(candidates_df['skills'].tolist())\n",
    "#     experts_embeddings = model.encode(experts_df['skills'].tolist())\n",
    "\n",
    "#     # Compute cosine similarity between each candidate and expert\n",
    "#     similarity_matrix = cosine_similarity(candidates_embeddings, experts_embeddings)\n",
    "\n",
    "#     # Create a similarity matrix DataFrame\n",
    "#     similarity_matrix_df = pd.DataFrame(similarity_matrix,\n",
    "#                                         index=candidates_df['name'],\n",
    "#                                         columns=experts_df['name'])\n",
    "\n",
    "#     # Genetic Algorithm Setup\n",
    "#     creator.create(\"FitnessMulti\", base.Fitness, weights=(1.0, -1.0))  # Maximize similarity, minimize experience difference\n",
    "#     creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
    "\n",
    "#     toolbox = base.Toolbox()\n",
    "\n",
    "#     # Initialize individual with random assignment of experts to candidates, allowing duplicates\n",
    "#     def init_individual():\n",
    "#         return random.choices(range(len(experts_df)), k=len(candidates_df))\n",
    "\n",
    "#     toolbox.register(\"individual\", tools.initIterate, creator.Individual, init_individual)\n",
    "#     toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "#     def evaluate(individual):\n",
    "#         total_similarity = 0.0\n",
    "#         total_experience_difference = 0.0\n",
    "        \n",
    "#         for i, expert_idx in enumerate(individual):\n",
    "#             candidate_experience = candidates_df.iloc[i]['experience']\n",
    "#             expert_experience = experts_df.iloc[expert_idx]['experience']\n",
    "            \n",
    "#             similarity = similarity_matrix[i, expert_idx]\n",
    "#             experience_difference = abs(candidate_experience - expert_experience)\n",
    "            \n",
    "#             total_similarity += similarity\n",
    "#             total_experience_difference += experience_difference\n",
    "        \n",
    "#         return total_similarity / len(individual), total_experience_difference / len(individual)\n",
    "\n",
    "#     toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "#     toolbox.register(\"mutate\", tools.mutShuffleIndexes, indpb=0.2)\n",
    "#     toolbox.register(\"select\", tools.selNSGA2)\n",
    "#     toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "#     # Genetic Algorithm Execution\n",
    "#     population = toolbox.population(n=50)  # Population size\n",
    "#     ngen = 50  # Number of generations\n",
    "#     cxpb = 0.7  # Crossover probability\n",
    "#     mutpb = 0.2  # Mutation probability\n",
    "\n",
    "#     result_population = algorithms.eaMuPlusLambda(population, toolbox, mu=50, lambda_=100, cxpb=cxpb, mutpb=mutpb, ngen=ngen, \n",
    "#                                                 stats=None, halloffame=None, verbose=False)\n",
    "\n",
    "#     # Extract best individuals\n",
    "#     all_individuals = result_population[0]\n",
    "    \n",
    "#     # Create a dictionary to store the similarity scores for all individuals\n",
    "#     similarity_scores_all = {}\n",
    "\n",
    "#     for ind in all_individuals:\n",
    "#         individual_score = {}\n",
    "#         for i, expert_idx in enumerate(ind):\n",
    "#             candidate_name = candidates_df.iloc[i]['name']\n",
    "#             expert_name = experts_df.iloc[expert_idx]['name']\n",
    "#             similarity_score = similarity_matrix[i, expert_idx]\n",
    "\n",
    "#             if candidate_name not in individual_score:\n",
    "#                 individual_score[candidate_name] = {}\n",
    "            \n",
    "#             individual_score[candidate_name][expert_name] = similarity_score\n",
    "        \n",
    "#         similarity_scores_all[str(ind)] = individual_score\n",
    "\n",
    "#     # Print all matches with similarity scores\n",
    "#     print(\"Similarity Scores for All Individuals:\")\n",
    "#     for individual, scores in similarity_scores_all.items():\n",
    "#         print(f\"Individual {individual}: {scores}\")\n",
    "\n",
    "#     return similarity_scores_all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def K_means_Clustering_Algorithm(candidates_data, experts_data):\n",
    "#     # Create DataFrames\n",
    "#     candidates_df = pd.DataFrame(candidates_data)\n",
    "#     experts_df = pd.DataFrame(experts_data)\n",
    "\n",
    "#     # Combine skills to create a vocabulary\n",
    "#     all_skills = candidates_df['skills'].tolist() + experts_df['skills'].tolist()\n",
    "\n",
    "#     # Vectorize skills\n",
    "#     vectorizer = CountVectorizer()\n",
    "#     skills_matrix = vectorizer.fit_transform(all_skills)\n",
    "\n",
    "#     # Split into candidates and experts features\n",
    "#     candidates_skills_matrix = skills_matrix[:len(candidates_df)]\n",
    "#     experts_skills_matrix = skills_matrix[len(candidates_df):]\n",
    "\n",
    "#     # Convert to DataFrames\n",
    "#     candidates_skills_df = pd.DataFrame(candidates_skills_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "#     experts_skills_df = pd.DataFrame(experts_skills_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "#     # Add experience as a feature\n",
    "#     candidates_features = pd.concat([candidates_skills_df, candidates_df['experience']], axis=1)\n",
    "#     experts_features = pd.concat([experts_skills_df, experts_df['experience']], axis=1)\n",
    "\n",
    "#     # Ensure 'experience' is numeric\n",
    "#     candidates_features['experience'] = pd.to_numeric(candidates_features['experience'], errors='coerce')\n",
    "#     experts_features['experience'] = pd.to_numeric(experts_features['experience'], errors='coerce')\n",
    "\n",
    "#     # Combine all features for standardization\n",
    "#     all_features = pd.concat([candidates_features, experts_features], axis=0)\n",
    "\n",
    "#     # Handle missing values (e.g., fill with 0)\n",
    "#     all_features.fillna(0, inplace=True)\n",
    "\n",
    "#     # Standardize the combined features\n",
    "#     scaler = StandardScaler()\n",
    "#     all_features_scaled = scaler.fit_transform(all_features)\n",
    "\n",
    "#     # Split back into candidates and experts\n",
    "#     candidates_features_scaled = all_features_scaled[:len(candidates_df)]\n",
    "#     experts_features_scaled = all_features_scaled[len(candidates_df):]\n",
    "\n",
    "#     # Apply K-Means\n",
    "#     kmeans = KMeans(n_clusters=len(experts_df), random_state=42)\n",
    "#     kmeans.fit(all_features_scaled)\n",
    "\n",
    "#     # Get cluster labels\n",
    "#     candidates_clusters = kmeans.predict(candidates_features_scaled)\n",
    "#     experts_clusters = kmeans.predict(experts_features_scaled)\n",
    "\n",
    "#     # Assign cluster labels back to DataFrames\n",
    "#     candidates_df['cluster'] = candidates_clusters\n",
    "#     experts_df['cluster'] = experts_clusters\n",
    "\n",
    "#     # Compute similarity matrix for each cluster\n",
    "#     combined_similarity_matrix = {}\n",
    "#     for cluster in np.unique(candidates_clusters):\n",
    "#         cluster_candidates = candidates_df[candidates_df['cluster'] == cluster]\n",
    "#         cluster_experts = experts_df[experts_df['cluster'] == cluster]\n",
    "        \n",
    "#         if not cluster_experts.empty and not cluster_candidates.empty:\n",
    "#             # Combine skills and experience for similarity calculation\n",
    "#             cluster_candidates_features = cluster_candidates.drop(columns=['id', 'name', 'cluster'])\n",
    "#             cluster_experts_features = cluster_experts.drop(columns=['id', 'name', 'cluster'])\n",
    "\n",
    "#             # Convert all columns to numeric and handle missing values\n",
    "#             cluster_candidates_features = cluster_candidates_features.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "#             cluster_experts_features = cluster_experts_features.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "#             # Compute cosine similarity between candidates and experts in the same cluster\n",
    "#             similarity_matrix = cosine_similarity(cluster_candidates_features, cluster_experts_features)\n",
    "            \n",
    "#             # Store similarity scores in the desired dictionary format\n",
    "#             for i, candidate_name in enumerate(cluster_candidates['name']):\n",
    "#                 if candidate_name not in combined_similarity_matrix:\n",
    "#                     combined_similarity_matrix[candidate_name] = {}\n",
    "#                 for j, expert_name in enumerate(cluster_experts['name']):\n",
    "#                     combined_similarity_matrix[candidate_name][expert_name] = similarity_matrix[i][j]\n",
    "\n",
    "#     return combined_similarity_matrix\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
