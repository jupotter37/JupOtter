{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45a0c98f",
   "metadata": {},
   "source": [
    "### Universidad Autónoma de Aguascalientes\n",
    "\n",
    "### Centro de Ciencias Básicas\n",
    "\n",
    "### Departamento de Ciencias de la Computación\n",
    "\n",
    "### Optativa Profesionalizante II: Machine Learning y Deep Learning\n",
    "\n",
    "### 10° \"A\"\n",
    "\n",
    "### Proyecto Final\n",
    "\n",
    "### Docente: Dr. Francisco Javier Luna Rosas\n",
    "\n",
    "### Alumno: Joel Alejandro Espinoza Sánchez (211800)\n",
    "\n",
    "### Fecha de Entrega: Aguascalientes, Ags., 15 de mayo del 2023.\n",
    "\n",
    "---\n",
    "\n",
    "## Proyecto final de Deep Learning\n",
    "\n",
    "### Introducción\n",
    "\n",
    "Un sistema de reconocimiento de patrones de rostros permite identificar a una persona en una imagen digital, mediante un análisis de las características faciales del sujeto extraídas y comparándolas en una base de datos. Las expresiones faciales son los cambios faciales como consecuencia de los estados emocionales internos o las comunicaciones sociales de una persona y este es un tema investigado activamente en la visión por computadora (Tian et al. 2011).\n",
    "\n",
    "En este proyecto se desarrollará una Red Neuronal Convolucional (CNN), la cual podrá clasificar diferentes expresiones faciales de una persona, colocando las etiquetas de la emoción detectada: Feliz, Enojado, Triste, Sorprendido y Neutral.\n",
    "\n",
    "El desarrollo del reconocimiento de expresiones faciales de una persona será dividido en tres secciones (Figura 1).\n",
    "\n",
    "<img src='assets/001.png' width='800'>\n",
    "\n",
    "> Figura 1: Gráfica de flujo de tres secciones en el desarrollo del sistema reconocedor de expresión facial.\n",
    "\n",
    "**Recolección de Datos.** Se hará una recolección de las imágenes de expresiones faciales por medio de una webcam.\n",
    "\n",
    "**Red CNN.** Las imágenes de expresiones faciales se usarán como Dataset para generar un Modelo de Red Neuronal CNN.\n",
    "\n",
    "**Reconocimiento de Expresiones Faciales de la CNN.** Este modelo servirá para hacer el reconocimiento de la expresión facial de la imagen capturada por medio de una webcam en tiempo real.\n",
    "\n",
    "### Propósito de la actividad\n",
    "\n",
    "Comprender el funcionamiento de las Redes Neuronales Convolucionales (CNN) aplicadas al reconocimiento de expresiones faciales en tiempo real.\n",
    "\n",
    "### Instrucciones\n",
    "\n",
    "Analicé, implemente y pruebe una Red Neuronal Covolucional (CNN) para reconocer expresiones faciales en tiempo real, la implementación de la CNN será en el lenguaje de su preferencia. El proyecto deberá ser presentada en un archivo PDF y en un archivo auto-reproducible HTML, que deberán contener:\n",
    "\n",
    "- Portada\n",
    "- Análisis de la CNN (Generación del Dataset, Generación del Modelo CNN, Reconocimiento de Expresiones Faciales en Tiempo Real).\n",
    "- Implementación de la CNN (Implementación de la CNN para Reconocer Expresiones Faciales en Tiempo Real).\n",
    "- Prueba de la CNN (Prueba de la CNN para Reconocer Expresiones Faciales en Tiempo Real).\n",
    "- Conclusiones.\n",
    "- Referencias en formato APA\n",
    "\n",
    "Letra Arial, tamaño 12 e interlineado de 1.5 (solo archivo *.PDF).\n",
    "\n",
    "### Características y/o criterios a evaluar\n",
    "\n",
    "Calidad de la práctica, ortografía y cumplir claramente con los temas solicitados en el apartado de instrucciones.\n",
    "\n",
    "---\n",
    "\n",
    "El Deep Learning es un área de la Inteligencia Artificial que se enfoca en el desarrollo de algoritmos de aprendizaje automático que simulan el comportamiento del cerebro humano mediante el uso de redes neuronales artificiales. Estas redes neuronales son capaces de aprender a partir de grandes cantidades de datos y realizar tareas complejas de forma autónoma, lo que ha llevado a importantes avances en campos como el procesamiento de imágenes, el reconocimiento de voz, la traducción de idiomas y muchos otros.\n",
    "\n",
    "El reconocimiento facial es una de las aplicaciones más populares del Deep Learning, que ha encontrado su uso en diversas áreas, como la seguridad, la publicidad y la psicología. El reconocimiento facial se refiere a la capacidad de las máquinas para detectar, analizar y reconocer caras humanas a partir de imágenes o vídeos. A través del Deep Learning, es posible desarrollar modelos que sean precisos y confiables en esta tarea.\n",
    "\n",
    "Las redes neuronales son la columna vertebral del Deep Learning. Son modelos matemáticos que simulan la estructura y el comportamiento de las neuronas biológicas en el cerebro humano. Estas redes están compuestas por capas de neuronas interconectadas y cada neurona procesa y transmite información a otras neuronas de la siguiente capa. La arquitectura y la topología de estas redes neuronales pueden variar según la tarea que se esté abordando.\n",
    "\n",
    "En el contexto de la clasificación de emociones, los proyectos de Deep Learning se enfocan en la identificación de las emociones humanas a través del reconocimiento facial. Los modelos de Deep Learning pueden detectar y analizar características faciales sutiles, como las expresiones de los ojos, la boca, las cejas y la frente, para clasificar las emociones en categorías como la felicidad, la tristeza, el enojo y el miedo. Esto tiene aplicaciones en la psicología, la publicidad, la salud mental y muchos otros campos.\n",
    "\n",
    "A continuación se presenta el proyecto de Deep Learning para reconocimiento facial y clasificación de emociones. El uso de redes neuronales artificiales en este proyecto permite el análisis preciso de grandes cantidades de datos de imágenes faciales para identificar las emociones humanas. Esto tiene un gran potencial en diversos campos y podría ayudar a comprender mejor la forma en que las emociones influyen en nuestro comportamiento y bienestar.\n",
    "\n",
    "La primera parte de este proyecto aborda la construcción del conjunto de imágenes con la que se entrenará la red neuronal; para ello se construyó el siguiente código como función para la captura de imágenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b51d5fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def captureImages(directory = 'img/Feliz'):\n",
    "    web_cam = cv2.VideoCapture(0) # Webcam\n",
    "    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml') # Carga Haarcascades\n",
    "    count = 0\n",
    "\n",
    "    while(web_cam.isOpened()):\n",
    "        ret, frame = web_cam.read() # Lee la webcam\n",
    "\n",
    "        # Preprocesamiento\n",
    "        grises = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        rostro = faceCascade.detectMultiScale(grises, 1.5, 5)\n",
    "        cv2.imshow(\"Creando Dataset\", frame) # Muestra lectura de la webcam\n",
    "\n",
    "        for(x,y,w,h) in rostro:\n",
    "            cv2.rectangle(frame, (x,y), (x + w, y + h), (255, 0, 0), 4) # Enmarca rostro\n",
    "            count += 1 # Rostros detectados\n",
    "            cv2.imwrite(directory + str(count) + '.jpg', grises[y:y + h, x:x + w]) # Guarda el rostro\n",
    "            #\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        elif count >= 400:\n",
    "            break\n",
    "\n",
    "    # Liberamos la captura\n",
    "    web_cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d8a15e",
   "metadata": {},
   "source": [
    "De este modo, se llama la función para capturar las imágenes para cada una de las cinco emociones. Se muestra la ejecución y ejemplo de la emoción etiquetada como \"Feliz\" a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "809c6aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "captureImages(\"img/Feliz/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9328fff2",
   "metadata": {},
   "source": [
    "Se observan algunos ejemplos de captura a continuación:\n",
    "\n",
    "<img src='img/Feliz/1.jpg' width='100'>\n",
    "\n",
    "<img src='img/Feliz/134.jpg' width='100'>\n",
    "\n",
    "<img src='img/Feliz/298.jpg' width='100'>\n",
    "\n",
    "A continuación se observa la ejecución para la captura de la etiqueta \"Enojado\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a06d2e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "captureImages(\"img/Enojado/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9075b7e",
   "metadata": {},
   "source": [
    "Se observan algunos ejemplos de captura a continuación:\n",
    "\n",
    "<img src='img/Enojado/1.jpg' width='100'>\n",
    "\n",
    "<img src='img/Enojado/134.jpg' width='100'>\n",
    "\n",
    "<img src='img/Enojado/298.jpg' width='100'>\n",
    "\n",
    "A continuación se observa la ejecución para la captura de la etiqueta \"Sorprendido\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ce84368",
   "metadata": {},
   "outputs": [],
   "source": [
    "captureImages(\"img/Sorprendido/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b445b1d",
   "metadata": {},
   "source": [
    "Se observan algunos ejemplos de captura a continuación:\n",
    "\n",
    "<img src='img/Sorprendido/280.jpg' width='100'>\n",
    "\n",
    "<img src='img/Sorprendido/315.jpg' width='100'>\n",
    "\n",
    "<img src='img/Sorprendido/320.jpg' width='100'>\n",
    "\n",
    "A continuación se observa la ejecución para la captura de la etiqueta \"Triste\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1db76d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "captureImages(\"img/Triste/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02538889",
   "metadata": {},
   "source": [
    "Se observan algunos ejemplos de captura a continuación:\n",
    "\n",
    "<img src='img/Triste/280.jpg' width='100'>\n",
    "\n",
    "<img src='img/Triste/315.jpg' width='100'>\n",
    "\n",
    "<img src='img/Triste/320.jpg' width='100'>\n",
    "\n",
    "A continuación se observa la ejecución para la captura de la etiqueta \"Neutral\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "627390e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "captureImages(\"img/Neutral/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23390fc5",
   "metadata": {},
   "source": [
    "Se observan algunos ejemplos de captura a continuación:\n",
    "\n",
    "<img src='img/Neutral/40.jpg' width='100'>\n",
    "\n",
    "<img src='img/Neutral/315.jpg' width='100'>\n",
    "\n",
    "<img src='img/Neutral/400.jpg' width='100'>\n",
    "\n",
    "La fase dos de este proyecto es el entrenamiento de la red neuronal que predice los sentimientos. Para ello se realizó el siguiente código a modo de función y conseguir el entrenamiento deseado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03abeac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "def CNN():\n",
    "    # Configuración\n",
    "\n",
    "    # Directorio de imágenes\n",
    "    train_data_dir = 'img/'\n",
    "    validation_data_dir = 'img/'\n",
    "\n",
    "    # Función de lotes con modificaciones aleatorias\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rotation_range = 30,\n",
    "        width_shift_range = 0.4,\n",
    "        height_shift_range = 0.4,\n",
    "        shear_range = 0.3,\n",
    "        zoom_range = 0.3,\n",
    "        horizontal_flip = True,\n",
    "        rescale = 1./255)\n",
    "    validation_datagen = ImageDataGenerator(rescale = 1./255) # Cambiar la escala de los valores en la matriz\n",
    "\n",
    "    # Generación de lotes normalizados\n",
    "    batch_size = 32\n",
    "    img_rows, img_cols = 48, 48\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size = (img_rows, img_cols), # Cada imagen se redimensionará a este tamaño\n",
    "        color_mode = 'grayscale',\n",
    "        batch_size = batch_size,\n",
    "        class_mode = 'categorical',\n",
    "        shuffle = True) # Mezclar el orden\n",
    "\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size = (img_rows, img_cols),\n",
    "        color_mode = 'grayscale',\n",
    "        batch_size = batch_size,\n",
    "        class_mode = 'categorical',\n",
    "        shuffle = True)\n",
    "\n",
    "    # Modelado de la CNN\n",
    "\n",
    "    num_classes = 5 # Feliz, enojado\n",
    "    model = Sequential()\n",
    "\n",
    "    # Block 1 (esta capa crea un núcleo de convolución que se convoluciona con la entrada de la capa para producir un tensor de salida)\n",
    "    model.add(Conv2D(32, 3, padding = 'same', kernel_initializer = 'he_normal', input_shape = (img_rows, img_cols, 1)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization()) # Entradas cambian, lo que provoca la variedad de no estacionariedad\n",
    "\n",
    "    model.add(Conv2D(32, 3, padding = 'same', kernel_initializer = 'he_normal', input_shape = (img_rows, img_cols, 1)))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization()) # Entradas cambian, lo que provoca la variedad de no estacionariedad\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2))) # Reduce la dimensionalidad de la imagen, tomando grupos de 2x2 y nos quedamos con el máximo de pool\n",
    "    model.add(Dropout(0.2)) # La capa de abandono establece aleatoriamente las unidades de entrada en 0 para evitar sobreajuste\n",
    "\n",
    "    # Block 2\n",
    "    model.add(Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal'))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(64, 3, padding = 'same', kernel_initializer = 'he_normal'))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Block 3\n",
    "    model.add(Conv2D(128, 3, padding = 'same', kernel_initializer = 'he_normal'))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(128, 3, padding = 'same', kernel_initializer = 'he_normal'))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Block 4\n",
    "    model.add(Conv2D(256, 3, padding = 'same', kernel_initializer = 'he_normal'))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(256, 3, padding = 'same', kernel_initializer = 'he_normal'))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Block 5\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, kernel_initializer = 'he_normal'))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Block 6\n",
    "    model.add(Dense(64, kernel_initializer = 'he_normal'))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Block 7\n",
    "    model.add(Dense(num_classes, kernel_initializer = 'he_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    print(model.summary()) # Resumen de cada capa\n",
    "\n",
    "    model.compile(optimizer = Adam(lr = 0.001), # Optimizer that implements the Adam algorithm\n",
    "                  loss = 'categorical_crossentropy', # Calcula la pérdida de entropía cruzada entre las etiquetas y predicciones\n",
    "                  metrics = ['accuracy']) # Calcula la frecuencia con la que las predicciones son iguales a las etiquetas\n",
    "\n",
    "    # Entrenamiento de la CNN\n",
    "\n",
    "    # Se detiene el entrenamiento cuando una métrica monitoreada haya dejado de mejorar\n",
    "    earlystop = EarlyStopping(monitor = 'val_loss',\n",
    "                              min_delta = 0, # min change\n",
    "                              patience = 5, # no change\n",
    "                              verbose = 1,\n",
    "                              restore_best_weights = True)\n",
    "\n",
    "    # Configuración para guardar un modelo\n",
    "    checkpoint = ModelCheckpoint(filepath = 'Modelo_3.h5', # Nombre\n",
    "                                 monitor = 'val_loss',\n",
    "                                 verbose = 1,\n",
    "                                 save_best_only = True,\n",
    "                                 mode = 'min')\n",
    "                                 #patience = 5,\n",
    "                                 #verbose = 1,\n",
    "                                 #min_delta = 0.0001)\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                  factor=0.2,\n",
    "                                  patience=5,\n",
    "                                  verbose=1,\n",
    "                                  min_lr=0.00001)\n",
    "\n",
    "    callbacks = [earlystop, checkpoint, reduce_lr]\n",
    "    epochs = 25\n",
    "    history = model.fit(\n",
    "        train_generator, # 1,600 imágenes\n",
    "        steps_per_epoch = None, # Hasta que se acaben las imágenes de train según los lotes es la época\n",
    "        epochs = epochs,\n",
    "        callbacks = callbacks,\n",
    "        validation_data = validation_generator, # 400 imágenes\n",
    "        validation_steps = None) # Hasta que se acaben las imágenes de validation según los lotes es la época\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daac8b5a",
   "metadata": {},
   "source": [
    "La ejecución de la función se da a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a11f791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2002 images belonging to 5 classes.\n",
      "Found 2002 images belonging to 5 classes.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_16 (Conv2D)          (None, 48, 48, 32)        320       \n",
      "                                                                 \n",
      " activation_22 (Activation)  (None, 48, 48, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 48, 48, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 48, 48, 32)        9248      \n",
      "                                                                 \n",
      " activation_23 (Activation)  (None, 48, 48, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 48, 48, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 24, 24, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 24, 24, 32)        0         \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " activation_24 (Activation)  (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 24, 24, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 24, 24, 64)        36928     \n",
      "                                                                 \n",
      " activation_25 (Activation)  (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 24, 24, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPooling  (None, 12, 12, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 12, 12, 64)        0         \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " activation_26 (Activation)  (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 12, 12, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 12, 12, 128)       147584    \n",
      "                                                                 \n",
      " activation_27 (Activation)  (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " batch_normalization_25 (Bat  (None, 12, 12, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 6, 6, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 6, 6, 128)         0         \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 6, 6, 256)         295168    \n",
      "                                                                 \n",
      " activation_28 (Activation)  (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " batch_normalization_26 (Bat  (None, 6, 6, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 6, 6, 256)         590080    \n",
      "                                                                 \n",
      " activation_29 (Activation)  (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " batch_normalization_27 (Bat  (None, 6, 6, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 3, 3, 256)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 3, 3, 256)         0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                147520    \n",
      "                                                                 \n",
      " activation_30 (Activation)  (None, 64)                0         \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " activation_31 (Activation)  (None, 64)                0         \n",
      "                                                                 \n",
      " batch_normalization_29 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      " activation_32 (Activation)  (None, 5)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,328,037\n",
      "Trainable params: 1,325,861\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/25\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.4577 - accuracy: 0.2328\n",
      "Epoch 1: val_loss improved from inf to 4.43512, saving model to Modelo_3.h5\n",
      "63/63 [==============================] - 35s 457ms/step - loss: 2.4577 - accuracy: 0.2328 - val_loss: 4.4351 - val_accuracy: 0.2003 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "63/63 [==============================] - ETA: 0s - loss: 2.1019 - accuracy: 0.2797\n",
      "Epoch 2: val_loss improved from 4.43512 to 2.83630, saving model to Modelo_3.h5\n",
      "63/63 [==============================] - 28s 448ms/step - loss: 2.1019 - accuracy: 0.2797 - val_loss: 2.8363 - val_accuracy: 0.2003 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.9386 - accuracy: 0.2607\n",
      "Epoch 3: val_loss did not improve from 2.83630\n",
      "63/63 [==============================] - 29s 455ms/step - loss: 1.9386 - accuracy: 0.2607 - val_loss: 3.2289 - val_accuracy: 0.2003 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.7611 - accuracy: 0.3132\n",
      "Epoch 4: val_loss did not improve from 2.83630\n",
      "63/63 [==============================] - 28s 445ms/step - loss: 1.7611 - accuracy: 0.3132 - val_loss: 2.9052 - val_accuracy: 0.2233 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.5624 - accuracy: 0.3736\n",
      "Epoch 5: val_loss improved from 2.83630 to 1.76028, saving model to Modelo_3.h5\n",
      "63/63 [==============================] - 28s 449ms/step - loss: 1.5624 - accuracy: 0.3736 - val_loss: 1.7603 - val_accuracy: 0.2512 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.4652 - accuracy: 0.4021\n",
      "Epoch 6: val_loss improved from 1.76028 to 1.12693, saving model to Modelo_3.h5\n",
      "63/63 [==============================] - 28s 446ms/step - loss: 1.4652 - accuracy: 0.4021 - val_loss: 1.1269 - val_accuracy: 0.4041 - lr: 0.0010\n",
      "Epoch 7/25\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2885 - accuracy: 0.4790\n",
      "Epoch 7: val_loss improved from 1.12693 to 0.87029, saving model to Modelo_3.h5\n",
      "63/63 [==============================] - 28s 447ms/step - loss: 1.2885 - accuracy: 0.4790 - val_loss: 0.8703 - val_accuracy: 0.6563 - lr: 0.0010\n",
      "Epoch 8/25\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.2137 - accuracy: 0.4890\n",
      "Epoch 8: val_loss did not improve from 0.87029\n",
      "63/63 [==============================] - 28s 445ms/step - loss: 1.2137 - accuracy: 0.4890 - val_loss: 1.2464 - val_accuracy: 0.3871 - lr: 0.0010\n",
      "Epoch 9/25\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0932 - accuracy: 0.5465\n",
      "Epoch 9: val_loss did not improve from 0.87029\n",
      "63/63 [==============================] - 28s 438ms/step - loss: 1.0932 - accuracy: 0.5465 - val_loss: 0.9755 - val_accuracy: 0.4900 - lr: 0.0010\n",
      "Epoch 10/25\n",
      "63/63 [==============================] - ETA: 0s - loss: 1.0394 - accuracy: 0.5709\n",
      "Epoch 10: val_loss improved from 0.87029 to 0.60951, saving model to Modelo_3.h5\n",
      "63/63 [==============================] - 28s 442ms/step - loss: 1.0394 - accuracy: 0.5709 - val_loss: 0.6095 - val_accuracy: 0.6658 - lr: 0.0010\n",
      "Epoch 11/25\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.9207 - accuracy: 0.6179\n",
      "Epoch 11: val_loss improved from 0.60951 to 0.41744, saving model to Modelo_3.h5\n",
      "63/63 [==============================] - 32s 512ms/step - loss: 0.9207 - accuracy: 0.6179 - val_loss: 0.4174 - val_accuracy: 0.8047 - lr: 0.0010\n",
      "Epoch 12/25\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.8415 - accuracy: 0.6578\n",
      "Epoch 12: val_loss improved from 0.41744 to 0.40346, saving model to Modelo_3.h5\n",
      "63/63 [==============================] - 31s 494ms/step - loss: 0.8415 - accuracy: 0.6578 - val_loss: 0.4035 - val_accuracy: 0.7957 - lr: 0.0010\n",
      "Epoch 13/25\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.7952 - accuracy: 0.6688\n",
      "Epoch 13: val_loss did not improve from 0.40346\n",
      "63/63 [==============================] - 29s 455ms/step - loss: 0.7952 - accuracy: 0.6688 - val_loss: 0.6116 - val_accuracy: 0.7468 - lr: 0.0010\n",
      "Epoch 14/25\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.7436 - accuracy: 0.6988\n",
      "Epoch 14: val_loss did not improve from 0.40346\n",
      "63/63 [==============================] - 30s 469ms/step - loss: 0.7436 - accuracy: 0.6988 - val_loss: 0.4631 - val_accuracy: 0.8177 - lr: 0.0010\n",
      "Epoch 15/25\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.6619 - accuracy: 0.7193\n",
      "Epoch 15: val_loss did not improve from 0.40346\n",
      "63/63 [==============================] - 31s 488ms/step - loss: 0.6619 - accuracy: 0.7193 - val_loss: 0.6490 - val_accuracy: 0.7577 - lr: 0.0010\n",
      "Epoch 16/25\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.6204 - accuracy: 0.7512\n",
      "Epoch 16: val_loss improved from 0.40346 to 0.28574, saving model to Modelo_3.h5\n",
      "63/63 [==============================] - 31s 485ms/step - loss: 0.6204 - accuracy: 0.7512 - val_loss: 0.2857 - val_accuracy: 0.8691 - lr: 0.0010\n",
      "Epoch 17/25\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.6300 - accuracy: 0.7333\n",
      "Epoch 17: val_loss did not improve from 0.28574\n",
      "63/63 [==============================] - 29s 461ms/step - loss: 0.6300 - accuracy: 0.7333 - val_loss: 1.5054 - val_accuracy: 0.6124 - lr: 0.0010\n",
      "Epoch 18/25\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.6028 - accuracy: 0.7582\n",
      "Epoch 18: val_loss did not improve from 0.28574\n",
      "63/63 [==============================] - 29s 453ms/step - loss: 0.6028 - accuracy: 0.7582 - val_loss: 0.7829 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 19/25\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.5315 - accuracy: 0.7932\n",
      "Epoch 19: val_loss improved from 0.28574 to 0.25050, saving model to Modelo_3.h5\n",
      "63/63 [==============================] - 27s 430ms/step - loss: 0.5315 - accuracy: 0.7932 - val_loss: 0.2505 - val_accuracy: 0.9191 - lr: 0.0010\n",
      "Epoch 20/25\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.5382 - accuracy: 0.7972\n",
      "Epoch 20: val_loss improved from 0.25050 to 0.17963, saving model to Modelo_3.h5\n",
      "63/63 [==============================] - 30s 472ms/step - loss: 0.5382 - accuracy: 0.7972 - val_loss: 0.1796 - val_accuracy: 0.9570 - lr: 0.0010\n",
      "Epoch 21/25\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.5128 - accuracy: 0.8077\n",
      "Epoch 21: val_loss improved from 0.17963 to 0.16657, saving model to Modelo_3.h5\n",
      "63/63 [==============================] - 27s 434ms/step - loss: 0.5128 - accuracy: 0.8077 - val_loss: 0.1666 - val_accuracy: 0.9500 - lr: 0.0010\n",
      "Epoch 22/25\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.4579 - accuracy: 0.8407\n",
      "Epoch 22: val_loss improved from 0.16657 to 0.14275, saving model to Modelo_3.h5\n",
      "63/63 [==============================] - 29s 469ms/step - loss: 0.4579 - accuracy: 0.8407 - val_loss: 0.1428 - val_accuracy: 0.9800 - lr: 0.0010\n",
      "Epoch 23/25\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.4298 - accuracy: 0.8482\n",
      "Epoch 23: val_loss did not improve from 0.14275\n",
      "63/63 [==============================] - 28s 447ms/step - loss: 0.4298 - accuracy: 0.8482 - val_loss: 0.2568 - val_accuracy: 0.9311 - lr: 0.0010\n",
      "Epoch 24/25\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.4229 - accuracy: 0.8601\n",
      "Epoch 24: val_loss did not improve from 0.14275\n",
      "63/63 [==============================] - 24s 387ms/step - loss: 0.4229 - accuracy: 0.8601 - val_loss: 1.0645 - val_accuracy: 0.6738 - lr: 0.0010\n",
      "Epoch 25/25\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3869 - accuracy: 0.8751\n",
      "Epoch 25: val_loss did not improve from 0.14275\n",
      "63/63 [==============================] - 21s 341ms/step - loss: 0.3869 - accuracy: 0.8751 - val_loss: 0.3520 - val_accuracy: 0.8576 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bd38b3",
   "metadata": {},
   "source": [
    "Finalmente se realizó la ejecución de lectura de cámara a tiempo real en la siguiente función que evaluaría el sentimiento con el que se entrenó previamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c68a0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from time import sleep\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def evaluateLive():\n",
    "    #\n",
    "    face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml') # Carga Haarcascades\n",
    "    classifier = load_model('Modelo_3.h5') # Carga modelo\n",
    "    #class_labels = ['Feliz', 'Enojado', 'Sorpresa', 'Neutral', 'Triste']\n",
    "    class_labels = ['Enojado', 'Feliz', 'Neutral','Sorprendido', 'Triste']\n",
    "\n",
    "    # Para cargarle imágenes\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    # Detección en tiempo real con webcam\n",
    "    cap = cv2.VideoCapture(0) # Webcam\n",
    "    while True:\n",
    "        ret, frame = cap.read() # Lee webcam\n",
    "        # Preprocesa la imagen\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # escala de grises\n",
    "        faces = face_classifier.detectMultiScale(gray, 1.3, 5) # Detección de caras (ubicación de la cara)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2) # Obtiene sólo el área de la cara (recorta la cara)\n",
    "\n",
    "            roi_gray = gray[y:y + h, x:x + w] # Vuelve a generar la escala de grises\n",
    "            roi_gray = cv2.resize(roi_gray, (48, 48), interpolation = cv2.INTER_AREA) # Interpolación -> para minimizar la imagen\n",
    "\n",
    "            # Si detecta rostro genera un recuadro\n",
    "            if np.sum([roi_gray]) != 0:\n",
    "                roi = roi_gray.astype('float')/255.0 # escala y convierte a grises\n",
    "                roi = img_to_array(roi) # Convierte a números\n",
    "                roi = np.expand_dims(roi, axis = 0)\n",
    "\n",
    "                preds = classifier.predict(roi)[0] # Predicción (el cero implica el canal que hace referencia a la webcam)\n",
    "                label = class_labels[preds.argmax()]\n",
    "                label_position = (x, y) # Posición x, y para escribir la etiqueta (feliz o enojado) encima del recuadro\n",
    "                cv2.putText(frame, label, label_position, cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 3) # Escribe la etiqueta (feliz o enojado)\n",
    "\n",
    "            else:\n",
    "                cv2.putText(frame, 'No Face Found', (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 3) # No abre cámara hasta detectar rostro\n",
    "\n",
    "        cv2.imshow('Detector de emociones', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a565565d",
   "metadata": {},
   "source": [
    "La ejecución se muestra a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88d649fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    }
   ],
   "source": [
    "evaluateLive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2e65f9",
   "metadata": {},
   "source": [
    "Se observa el resultado de ejecutar el anterior código en las siguientes capturas:\n",
    "\n",
    "<img src='assets/002.png' width='500'>\n",
    "\n",
    "<img src='assets/003.png' width='500'>\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusiones\n",
    "\n",
    "Es interesante e importante poder implementar las bases de estos temas para entenderlos en un futuro, pues, posteriormente no basta con sólo importar librerías que realicen el trabajo pesado, ya que, implementar manualmente estos algoritmos nos enseña a qué hay detrás del algoritmo, cómo funciona y poder comprender realmente qué está ocurriendo como la base de una red neuronal convolucional y sus alpicaciones en detección de sentimientos a tiempo real. Es muy útil la implementación de estos algoritmos en estas tareas para las futuras tareas de la materia y aplicaciones de Machine Learning en la vida personal.\n",
    "\n",
    "## Referencias\n",
    "\n",
    "- Anónimo (s.f.) “Red neuronal artificial”. Obtenido de Wikipedia: https://es.wikipedia.org/wiki/Red_neuronal_artificial.\n",
    "- Data Scientest (2021) “Perceptrón. ¿Qué es y para qué sirve?”. Obtenido de Data Scientest: https://datascientest.com/es/perceptron-que-es-y-para-que-sirve.\n",
    "- Luna, F. (2023) “El Modelo de McCulloch – Pitts”. Apuntes de ICI 10°."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
