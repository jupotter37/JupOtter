{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ceaf2d-bc1a-466d-8d2e-c0f56090cdbb",
   "metadata": {},
   "source": [
    "# Conversation history | memory\n",
    "\n",
    "* Uses the OpenAI gpt3.5-turbo model\n",
    " https://python.langchain.com/docs/integrations/chat/openai/\n",
    "* Message history strategy:\n",
    "  1. Raw messages\n",
    "  2. Summary\n",
    "  3. Limit number of messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f5906f9-76c7-4ccb-ab19-2bdab9d7a49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the file that contains the API keys\n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edc19be2-2de8-4d20-84a5-38d8d6fbf62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  get_llm():\n",
    "     model = 'gpt-3.5-turbo-0125'\n",
    "     return ChatOpenAI(model=model, openai_api_key=os.getenv('OPEN_AI_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da85fb-9810-49d8-a525-eafb75251b57",
   "metadata": {},
   "source": [
    "## 1.Conversation Buffer Memory\n",
    "\n",
    "https://python.langchain.com/docs/modules/memory/types/buffer/\n",
    "\n",
    "https://api.python.langchain.com/en/latest/memory/langchain.memory.buffer.ConversationBufferMemory.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b20a2ca8-a0ad-4c69-8030-f9a966885366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferMemory, ConversationSummaryMemory, ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# To get history as a list, pass the parameter return_messages=True\n",
    "conv_buffer_memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccf5080c-4fe6-4738-9621-748cf483a4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_conv = ConversationChain(llm = get_llm(), memory = conv_buffer_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42cd6d0f-dbf1-4451-96ec-acf7cd9692cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain_conv.invoke('what is a large language model')\n",
    "response = chain_conv.invoke('how is it different than a classical ML model?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc4bc37d-e311-4f5c-8458-300cef2f7dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: what is a large language model\n",
      "AI: A large language model is a type of artificial intelligence system that is trained on vast amounts of text data to understand and generate human language. These models use deep learning techniques to process and generate text, and they are capable of understanding context, generating responses, and even carrying on conversations like we are having now. Some well-known examples of large language models include GPT-3 (Generative Pre-trained Transformer 3) developed by OpenAI and BERT (Bidirectional Encoder Representations from Transformers) developed by Google. These models have been used in a variety of applications, such as chatbots, language translation, and content generation.\n",
      "Human: how is it different than a classical ML model?\n",
      "AI: A classical machine learning model relies on pre-defined features and algorithms to make predictions or classifications based on the input data. In contrast, a large language model like GPT-3 or BERT is based on deep learning techniques, specifically transformer architectures, which allow it to learn directly from the raw text data without the need for manual feature engineering. This enables the language model to capture complex patterns and relationships in language, making it more versatile and capable of understanding and generating human language in a way that traditional machine learning models cannot. Additionally, large language models typically have a much larger number of parameters and require massive amounts of training data compared to classical ML models.\n",
      "Word count =  1567\n"
     ]
    }
   ],
   "source": [
    "print(conv_buffer_memory.buffer)\n",
    "print(\"Word count = \", len(conv_buffer_memory.buffer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf7f296-bde5-4a31-aab4-349896252dc4",
   "metadata": {},
   "source": [
    "## 2. Conversation Summary Memory\n",
    "\n",
    "https://api.python.langchain.com/en/latest/memory/langchain.memory.summary.ConversationSummaryMemory.html#langchain.memory.summary.ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52d0d3e2-9b58-4a86-9c1d-084f06890b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires an LLM to summarize the conversation\n",
    "conv_summary_memory = ConversationSummaryMemory(llm = get_llm(), return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3709cf95-a5c4-490d-9c30-444c7b0f048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_summary = ConversationChain(llm = get_llm(), memory = conv_summary_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "394e4d20-3c21-44de-bffa-bc3a89681bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain_summary.invoke('what is a large language model')\n",
    "response = chain_summary.invoke('how is it different than a classical ML model?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "907b33fa-ff08-4b5f-ac71-54b9db17140f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The human asks what a large language model is. The AI explains that a large language model is a type of artificial intelligence program trained on vast amounts of text data to understand and generate human language. Examples include GPT-3 and BERT, which can perform various natural language processing tasks. A large language model differs from a classical machine learning model in several ways, such as being specifically designed for language processing, trained on massive amounts of text data, and having more parameters and layers for capturing complex language patterns.\n",
      "Word count =  578\n"
     ]
    }
   ],
   "source": [
    "print(conv_summary_memory.buffer)\n",
    "print(\"Word count = \", len(conv_summary_memory.buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "884dec89-cce3-43c7-a856-5f63ba9d5c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain_summary.invoke('how is it different than a classical deep learning model?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e09ace24-404b-4c7a-b234-738b0623dd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The human asks what a large language model is. The AI explains that a large language model is a type of artificial intelligence program trained on vast amounts of text data to understand and generate human language. Examples include GPT-3 and BERT, which can perform various natural language processing tasks. A large language model differs from a classical machine learning model in several ways, such as being specifically designed for language processing, trained on massive amounts of text data, and having more parameters and layers for capturing complex language patterns. A large language model is different from a classical deep learning model in that it is specifically designed for language processing tasks, trained on massive text data, and has more parameters and layers to capture complex language patterns.\n",
      "Word count =  821\n"
     ]
    }
   ],
   "source": [
    "print(conv_summary_memory.buffer)\n",
    "print(\"Word count = \", len(conv_summary_memory.buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abef4c4-2df4-4230-9898-288340e2200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Raw message count = \", len(conv_summary_memory.chat_memory.messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3974f29d-fe1e-4e8e-bd9c-5f0181b887a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_summary_memory.chat_memory.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a04413-b6d8-4af6-9d1f-1ce31d5f8447",
   "metadata": {},
   "source": [
    "## 3. ConversationBufferWindowMemory\n",
    "\n",
    "https://api.python.langchain.com/en/latest/memory/langchain.memory.buffer_window.ConversationBufferWindowMemory.html#langchain.memory.buffer_window.ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1c4dc6-0af6-4614-842d-912afeec7c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of request/response message pairs to store in buffer.\n",
    "conv_buf_window_memnory = ConversationBufferWindowMemory(k=2, return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39930411-4079-45f5-9158-b69109e0f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_conv_window = ConversationChain(llm = get_llm(), memory = conv_buf_window_memnory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063d8092-7cf3-4b58-ade3-d8e5c172900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain_conv_window.invoke('what is a large language model')\n",
    "response = chain_conv_window.invoke('how is it different than a classical ML model?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b24c24-407a-4ecd-bbb7-9c1072b00181",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of messages in history = \", len(conv_buf_window_memnory.buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258150e4-63fd-48a9-8c11-d4be7d7e5fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain_summary.invoke('how is it different than a classical deep learning model?')\n",
    "response = chain_summary.invoke('how is it different than a neural network?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a33958-2e0c-4cf9-bcf8-c135b1cb4ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of messages in history = \", len(conv_buf_window_memnory.buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eedac4c-8d91-4fa2-b7e0-70e7590f2d68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
