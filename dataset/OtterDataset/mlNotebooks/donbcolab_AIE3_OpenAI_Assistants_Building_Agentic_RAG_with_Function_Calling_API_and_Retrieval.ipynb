{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgDepNVhvzIr"
      },
      "source": [
        "# OpenAI Assistants - Building Agentic RAG with the Function Calling, Retrieval, and Code Interpreter Tools\n",
        "\n",
        "## here's my twist to the default scenario\n",
        "\n",
        "- I used a quantized version of Whisper to create a [speech transcript of Andrej Karpathy's recent \"Let's reproduce GPT-2\" YouTube video](https://huggingface.co/datasets/dwb2023/yt-transcripts-v3/viewer/default/train?q=Andrej+Karpathy)\n",
        "  - I'm using a Hugging Face dataset for Whisper transcripts - I'll be doing a YouTube video about this in the next couple weeks\n",
        "- the transcript and some code extracts were used to populate the vector db (used by the file search tool)\n",
        "\n",
        "### Lessons Leaarned\n",
        "- 2a. The Assistant with File Search tool is smart like Andre Karpathy thanks to OpenAI Vector Store functionality and the Whisper transcript\n",
        "- 2b. The Assistant with Code Interpreter tool is analyzing the code to build GPT-2 from scratch\n",
        "- 2c. Our amazing Function Calling assistant figured out how to output results in JSON format (with some help from friends)\n",
        "\n",
        "### Lessons not yet learned\n",
        "- it's okay to complete assignments without \"perfect code\" (and document it under lessons not yet learned)\n",
        "- ability for an assistant to use more than one tool.  (really wanted 2b to have both the code and file search tools on their toolbelt)\n",
        "- Effective evaluation and metrics for OpenAI outside of fine tuning (only area that's supported out of the box with WandB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## now back to the regularly scheduled programming...\n",
        "\n",
        "Today we'll explore using OpenAI's Python SDK to create, manage, and use the OpenAI Assistant API!\n",
        "\n",
        "We'll be doing the following in today's notebook:\n",
        "\n",
        "1. Task 1: Simple Assistant\n",
        "2. Task 2: Adding Tools\n",
        "  - Task 2a: Creating an Assistant with File Search Tool\n",
        "  - Task 2b: Creating an Assistant with Code Interpreter Tool\n",
        "  - Task 2c: Creating an Assistant with Function Calling Tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### OpenAI Reference:\n",
        "\n",
        "**Function Calling**\n",
        "- [OpenAI Python Library - helpers.md](https://github.com/openai/openai-python/blob/main/helpers.md)\n",
        "- [OpenAI Assistants docs](https://platform.openai.com/docs/assistants/overview?lang=python)\n",
        "\n",
        "**APIs**\n",
        "- [OpenAI Python Library - api.md](https://github.com/openai/openai-python/blob/main/api.md)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNU6b3ymwOWq"
      },
      "source": [
        "## Dependencies\n",
        "\n",
        "We'll start, as we usually do, with some dependiencies and our API key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePayyL6at6LS",
        "outputId": "4fb742b3-c650-44c2-8c58-2d83c81a766b"
      },
      "outputs": [],
      "source": [
        "!pip install -qU openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unKr3HZdu-1V",
        "outputId": "9c567820-9633-44fd-dad6-e7dd1ca46d97"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwNE7N4HwhXH"
      },
      "source": [
        "## Task 1: Simple Assistant\n",
        "\n",
        "Let's create a simple Assistant to understand more about how the API works to start!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKEwbLMFxNKs"
      },
      "source": [
        "### OpenAI Client\n",
        "\n",
        "At the core of the OpenAI Python SDK is the Client!\n",
        "\n",
        "> NOTE: For ease of use, we'll start with the synchronous `OpenAI()`. OpenAI does provide an `AsyncOpenAI()` that you could leverage as well!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wF-mBZwtuavl"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aIx4GZ2w_1c"
      },
      "source": [
        "### Creating An Assistant\n",
        "\n",
        "Leveraging what we know about the OpenAI API from previous sessions - we're going to start by simply initializing an Assistant.\n",
        "\n",
        "Before we begin, we need to think about a few customization options we have:\n",
        "\n",
        "- `name` - Straight forward enough, this is what our Assistant's name will be\n",
        "- `instructions` - similar to a system message, but applied at an Assistant level, this is how we can guide the Assistant's tone, behaviour, functionality, and more!\n",
        "- `model` - this will allow us to choose which model we would prefer to use for our Assistant\n",
        "\n",
        "Let's start by setting some instructions for our Assistant.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "Paqd6zWMyMAJ"
      },
      "outputs": [],
      "source": [
        "# @markdown #### üèóÔ∏è Build Activity üèóÔ∏è\n",
        "# @markdown Fill out the fields below to add your Assistant's name, instructions, and desired model!\n",
        "\n",
        "name = \"Andre Karpathy's GPT From Scratch Teacher Assistant\" # @param {type: \"string\"}\n",
        "instructions = \"You are Andre Karpathy's AI teacher assistant created to help people learn to code GPT2 from scratch with the same passion and dedication as Andrej himself. Provide clear explanations and helpful examples to the user as they work through the concepts. Be friendly, curious and extremely knowledgeable about ML and generative AI concepts.  You bring an irresistble GenZ flair to the discussion.\" # @param {type: \"string\"}\n",
        "model = \"gpt-4o\" # @param [\"gpt-3.5-turbo\", \"gpt-4-turbo-preview\", \"gpt-4\", \"gpt-4o\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "VkvsXv5_3cyQ"
      },
      "outputs": [],
      "source": [
        "# @markdown #### üèóÔ∏è Build Activity üèóÔ∏è\n",
        "# @markdown We can also override the Assistant's instructions when we run a thread.\n",
        "\n",
        "# @markdown Use one of the [Prompt Principles for Instruction](https://arxiv.org/pdf/2312.16171v1.pdf) to improve the likeliehood of a correct or valuable response from your Assistant.\n",
        "\n",
        "additional_instructions = \"You are Andre Karpathy's AI teacher assistant created to help people learn to code GPT-2 from scratch. Provide clear, step-by-step explanations and helpful examples to the user. Be friendly, curious, and extremely knowledgeable about ML and generative AI concepts. Use an engaging and approachable tone to make the learning process enjoyable.\" # @param {type: \"string\"}\n",
        "\n",
        "# for simplicity setting instruction equal to additional_instructions\n",
        "instructions = additional_instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Principles from the paper that were used to improve the quality of the instructions:\n",
        "\n",
        "1. **Prompt Structure and Clarity**: Ensure the instructions are clear, concise, and free of ambiguity. For instance, define specific tasks and desired outcomes clearly.\n",
        "\n",
        "2. **Specificity and Information**: Provide detailed examples and context to illustrate concepts. This can help users better understand complex ideas.\n",
        "\n",
        "3. **User Interaction and Engagement**: Create interactive prompts that encourage user engagement. Ask questions that require the user to think and respond actively.\n",
        "\n",
        "4. **Content and Language Style**: Assign a clear role to the assistant (e.g., \"You are Andrej Karpathy's AI teacher assistant\") and use direct, engaging language.\n",
        "\n",
        "5. **Complex Tasks and Coding Prompts**: Break down complex coding tasks into simpler, manageable steps. Use step-by-step instructions to guide users through learning GPT-2 from scratch.\n",
        "\n",
        "**Reasoning**: This approach ensures the instructions are clear and engaging while providing detailed guidance and fostering user interaction. It aligns with the principles of clarity, specificity, engagement, and structured learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUeaDsLMzcv-"
      },
      "source": [
        "### Initialize Assistants\n",
        "\n",
        "Now that we have our desired name, instruction, and model - we can initialize our Assistants!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "615NK1Qj2e_Z"
      },
      "source": [
        "There are a number of useful parameters here, but we'll call out a few:\n",
        "\n",
        "- `id` - since we may have multiple Assistant's, knowing which Assistant we're interacting with will help us ensure the desired user experience!\n",
        "- `description` - A natrual language description of our Assistant could help others understand what it's supposed to do!\n",
        "- `file_ids` - if we wanted to use the Retrieval tool, this would let us know what files we had given our Assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Assistant without tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6-4MgVLbu8rO"
      },
      "outputs": [],
      "source": [
        "assistant = client.beta.assistants.create(\n",
        "    name=name,\n",
        "    instructions=instructions,\n",
        "    model=model,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QskO5n5W2X6t"
      },
      "source": [
        "Let's examine our `assistant` object and see what we find!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkkIC_JP2bG0",
        "outputId": "86ecb682-a04d-4687-94d3-18a4b99ffc07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Assistant(id='asst_Au06w4YEO2tE75GGBwFYTHke', created_at=1718121766, description=None, instructions=\"You are Andre Karpathy's AI teacher assistant created to help people learn to code GPT-2 from scratch. Provide clear, step-by-step explanations and helpful examples to the user. Be friendly, curious, and extremely knowledgeable about ML and generative AI concepts. Use an engaging and approachable tone to make the learning process enjoyable.\", metadata={}, model='gpt-4o', name=\"Andre Karpathy's GPT From Scratch Teacher Assistant\", object='assistant', tools=[], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=None, file_search=None), top_p=1.0)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Assistant with File Search tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jfn_MlJqFiEe"
      },
      "outputs": [],
      "source": [
        "fs_assistant = client.beta.assistants.create(\n",
        "  name=name + \" File Search\",\n",
        "  instructions=instructions,\n",
        "  model=model,\n",
        "  tools=[{\"type\": \"file_search\"}],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Assistant with Code Interpreter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IC81y_VtH9lw"
      },
      "outputs": [],
      "source": [
        "ci_assistant = client.beta.assistants.create(\n",
        "  name=name + \" + Code Interpreter\",\n",
        "  instructions=instructions,\n",
        "  model=model,\n",
        "  tools=[{\"type\": \"code_interpreter\"}],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3HhlqtM0AhW"
      },
      "source": [
        "### Creating a Thread\n",
        "\n",
        "Behind the scenes our Assistant is powered by the idea of \"threads\".\n",
        "\n",
        "You can think of threads as individual conversations that interact with the Assistant.\n",
        "\n",
        "Let's create a thread now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iFVM39vevT5f"
      },
      "outputs": [],
      "source": [
        "thread = client.beta.threads.create()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y7jelq01PoG"
      },
      "source": [
        "Let's look at our `thread` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V8WAKDZ1Uf2",
        "outputId": "6ba2a680-a595-40e8-fa5d-5784b9614b97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Thread(id='thread_y0JC4fKUZ5rWJhqNRTQunOlU', created_at=1718121767, metadata={}, object='thread', tool_resources=ToolResources(code_interpreter=None, file_search=None))"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "thread"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k6S_e501V4z"
      },
      "source": [
        "Notice some key attributes:\n",
        "\n",
        "- `id` - since each Thread is like a conversation, we need some way to specify which thread we're dealing with when interacting with them\n",
        "- `tool_resources` - this will become more relevant as we add tools since we'll need a way to verify which tools we have access to when interacting with our Assistant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5BvGv1N0c2h"
      },
      "source": [
        "### Adding Messages to Our Thread\n",
        "\n",
        "Now that we have our Thread (or conversation) we can start adding messages to it!\n",
        "\n",
        "Let's add a simple message that asks about how our Assistant is feeling.\n",
        "\n",
        "Notice the parameters we're leveraging:\n",
        "\n",
        "- `thread_id` - since each Thread is like a conversation, we need some way to address a specific conversation. We can use `thread.id` to do this.\n",
        "- `role` - similar to when we used our chat completions endpoint, this parameter specifies who the message is coming from. You can leverage this in the same ways you would through the chat completions endpoint.\n",
        "- `content` - this is where we can place the actual text our Assistant will interact with\n",
        "\n",
        "> NOTE: Feel free to substitute a relevant message based on the Assistant you created"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "R7ZNCfGivagg"
      },
      "outputs": [],
      "source": [
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=f\"What is GPT2?  Can I build it with legos?  Explain it to me like I'm 5 years old.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc7R3Sr32P0b"
      },
      "source": [
        "Again, let's examine our `message` object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMLvyZDA2S_D",
        "outputId": "dfde63a0-3b6f-4f01-b76c-ec84baefe5a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Message(id='msg_t0m1UKG1k3KWHxCzxgLch3Az', assistant_id=None, attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value=\"What is GPT2?  Can I build it with legos?  Explain it to me like I'm 5 years old.\"), type='text')], created_at=1718121767, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_y0JC4fKUZ5rWJhqNRTQunOlU')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "message"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI3Pctpk29og"
      },
      "source": [
        "### Running Our Thread\n",
        "\n",
        "Now that we have an Assistant, and we've given that Assistant a Thread, and we've added a Message to that Thread - we're ready to run our Assistant!\n",
        "\n",
        "Notice that this process lets us add (potentially) multiple messages to our Assistant. We can leverage that behaviour for few/many-shot examples, and more!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CuGbTrL5QEc"
      },
      "source": [
        "Let's run our Thread!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fpWNl3UVvdW4"
      },
      "outputs": [],
      "source": [
        "run = client.beta.threads.runs.create(\n",
        "  thread_id=thread.id,\n",
        "  assistant_id=assistant.id,\n",
        "  instructions=additional_instructions\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erMGdU7y6la2"
      },
      "source": [
        "Now that we've run our thread, let's look at the object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kz_rfwi869YI",
        "outputId": "56aa670d-8f22-4ef4-b5f4-e568c6f7cefb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Run(id='run_sgmDEn2FP3esrVKV1gqPxTRW', assistant_id='asst_Au06w4YEO2tE75GGBwFYTHke', cancelled_at=None, completed_at=None, created_at=1718121767, expires_at=1718122367, failed_at=None, incomplete_details=None, instructions=\"You are Andre Karpathy's AI teacher assistant created to help people learn to code GPT-2 from scratch. Provide clear, step-by-step explanations and helpful examples to the user. Be friendly, curious, and extremely knowledgeable about ML and generative AI concepts. Use an engaging and approachable tone to make the learning process enjoyable.\", last_error=None, max_completion_tokens=None, max_prompt_tokens=None, metadata={}, model='gpt-4o', object='thread.run', parallel_tool_calls=True, required_action=None, response_format='auto', started_at=None, status='queued', thread_id='thread_y0JC4fKUZ5rWJhqNRTQunOlU', tool_choice='auto', tools=[], truncation_strategy=TruncationStrategy(type='auto', last_messages=None), usage=None, temperature=1.0, top_p=1.0, tool_resources={})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h2SH_347JJb"
      },
      "source": [
        "Notice we have access to a few very powerful parameters in this `run` object.\n",
        "\n",
        "- `completed_at` - this will help us determine when we can expect to retrieve a response\n",
        "- `failed_at` - this can highlight any issues our run ran into\n",
        "- `status` - is another way we can understand how the flow is going"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVBNagBU7kpx"
      },
      "source": [
        "### Retrieving Our Run\n",
        "\n",
        "Now that we've created our run, let's retrieve it.\n",
        "\n",
        "We're going to wrap this in a simple loop to make sure we're not retrieving it too early."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "itz5_otPvfkV"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "while run.status == \"in_progress\" or run.status == \"queued\":\n",
        "  time.sleep(1)\n",
        "  run = client.beta.threads.runs.retrieve(\n",
        "    thread_id=thread.id,\n",
        "    run_id=run.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgGE1uUJ7z3h",
        "outputId": "89f3df27-d5a3-4095-9422-6271db52cfba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "completed\n"
          ]
        }
      ],
      "source": [
        "print(run.status)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHVTS4hD7-fv"
      },
      "source": [
        "Now that our run is completed - we can retieve the messages from our thread!\n",
        "\n",
        "Notice that our run helps us understand how things are going - but it isn't where we're going to find our responses or messages. Those are added on the backend into our thread.\n",
        "\n",
        "This leads to a simple, but important, flow:\n",
        "\n",
        "1. We add messages to a thread.\n",
        "2. We create a run on that thread.\n",
        "3. We wait until the run is finished.\n",
        "4. We check our thread for the new messages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGBNpGmh-ZpW"
      },
      "source": [
        "### Checking Our Thread\n",
        "\n",
        "Now we can get a list of messages from our thread!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Av-OQDUPvhAd"
      },
      "outputs": [],
      "source": [
        "messages = client.beta.threads.messages.list(\n",
        "  thread_id=thread.id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM6cZk-GviqX",
        "outputId": "3916ed59-7eba-471f-9eb7-d8d2f4658012"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SyncCursorPage[Message](data=[Message(id='msg_NWVjLEsuh03FfVvghbP2Ajlr', assistant_id='asst_Au06w4YEO2tE75GGBwFYTHke', attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value=\"Alright, imagine you have a giant box of Legos and you want to build a fun castle. You can picture each brick in the box as a piece of a much bigger structure. Now, if you follow the instructions step by step, you can build that castle, right?\\n\\nGPT-2 is a bit like that castle, but instead of using Lego bricks, it uses words and logic to build sentences, paragraphs, and even entire articles! GPT-2 is a type of computer brain called an AI (artificial intelligence) model, which is designed to understand and generate human language. \\n\\nWhen you're using GPT-2, it's like asking it to make up stories using the words and rules it has learned from reading lots and lots of books and articles. It knows which words fit together well, just like knowing which Lego bricks snap together to make the cool castle.\\n\\nHowever, unlike Legos, you can't build GPT-2 with physical blocks. Instead, we use computer code and lots of data to train it, kind of like teaching it to be really, really good at understanding and creating language.\\n\\nSo, while you can't literally stack Legos to make GPT-2, you can think of it as putting together many tiny pieces of knowledge to create something amazing, just like a Lego masterpiece!\"), type='text')], created_at=1718121768, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_sgmDEn2FP3esrVKV1gqPxTRW', status=None, thread_id='thread_y0JC4fKUZ5rWJhqNRTQunOlU'), Message(id='msg_t0m1UKG1k3KWHxCzxgLch3Az', assistant_id=None, attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value=\"What is GPT2?  Can I build it with legos?  Explain it to me like I'm 5 years old.\"), type='text')], created_at=1718121767, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_y0JC4fKUZ5rWJhqNRTQunOlU')], object='list', first_id='msg_NWVjLEsuh03FfVvghbP2Ajlr', last_id='msg_t0m1UKG1k3KWHxCzxgLch3Az', has_more=False)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT9_V_XlaVH7"
      },
      "source": [
        "### Streaming Our Runs\n",
        "\n",
        "With recent upgrades to the Assistant API - we can now *stream* our outputs!\n",
        "\n",
        "In order to do this - we'll need something called an `EventHandler` which will help us to decide on what actions to take based on the output of the LLM.\n",
        "\n",
        "Let's build it below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YvH-FUbQawcN"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import override\n",
        "from openai import AssistantEventHandler\n",
        "\n",
        "class EventHandler(AssistantEventHandler):\n",
        "  @override\n",
        "  def on_text_created(self, text) -> None:\n",
        "    print(f\"\\nassistant > \", end=\"\", flush=True)\n",
        "\n",
        "  @override\n",
        "  def on_text_delta(self, delta, snapshot):\n",
        "    print(delta.value, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvBieRYvbAmt"
      },
      "source": [
        "Now we can create our `run` and stream the output as it comes in!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8ax_0ZfbF_I",
        "outputId": "1bbddea1-a145-4a0d-b662-73f7703a511f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "assistant > Alright, imagine you have a giant box of Legos and you want to build a fun castle. You can picture each brick in the box as a piece of a much bigger structure. Now, if you follow the instructions step by step, you can build that castle, right?\n",
            "\n",
            "GPT-2 is a bit like that castle, but instead of using Lego bricks, it uses words and logic to build sentences, paragraphs, and even entire articles! GPT-2 is a type of computer brain called an AI (artificial intelligence) model, which is designed to understand and generate human language. \n",
            "\n",
            "When you're using GPT-2, it's like asking it to make up stories using the words and rules it has learned from reading lots and lots of books and articles. It knows which words fit together well, just like knowing which Lego bricks snap together to make the cool castle.\n",
            "\n",
            "However, unlike Legos, you can't build GPT-2 with physical blocks. Instead, we use computer code and lots of data to train it, kind of like teaching it to be really, really good at understanding and creating language.\n",
            "\n",
            "So, while you can't literally stack Legos to make GPT-2, you can think of it as putting together many tiny pieces of knowledge to create something amazing, just like a Lego masterpiece!"
          ]
        }
      ],
      "source": [
        "with client.beta.threads.runs.stream(\n",
        "  thread_id=thread.id,\n",
        "  assistant_id=assistant.id,\n",
        "  instructions=additional_instructions,\n",
        "  event_handler=EventHandler(),\n",
        ") as stream:\n",
        "  stream.until_done()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgnY16tjCmc6"
      },
      "source": [
        "## Task 2: Adding Tools\n",
        "\n",
        "Now that we have an understanding of how Assistant works, we can start thinking about adding tools.\n",
        "\n",
        "We'll go through 3 separate tools and explore how we can leverage them!\n",
        "\n",
        "Let's start with the most familiar tool - the Retriever!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0NagnlZC8g9"
      },
      "source": [
        "### Task 2a: Creating an Assistant with the File Search Tool\n",
        "\n",
        "The first thing we'll want to do is create an assistant with the File Search tool.\n",
        "\n",
        "This is also going to require some data. We'll provided data - but you're very much encouraged to use your own files to explore how the Assistant works for your use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HInYwNiQEjQH"
      },
      "source": [
        "#### Collect and Add Data to Vector Store\n",
        "\n",
        "1. First, we need some data.\n",
        "2. Second, we need to add the data to our Assistant!\n",
        "\n",
        "Let's start with grabbing some data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2EpY1w_FQ3m"
      },
      "source": [
        "Now we can upload our files to our Vector Store!\n",
        "\n",
        "Pay attention to [this](https://platform.openai.com/docs/assistants/tools/file-search/supported-files) documentation to see what kinds of files can be uploaded.\n",
        "\n",
        "> NOTE: Per the OpenAI [docs](https://platform.openai.com/docs/assistants/tools/file-search/vector-stores) The maximum file size is 512 MB and no more than 2,000,000 tokens (computed automatically when you attach a file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6UgUYNTaOpUK"
      },
      "outputs": [],
      "source": [
        "vector_store = client.beta.vector_stores.create(name=\"Andrej Karpathy GPT2 from Scratch Compilation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "dpVoe2SMFI6s"
      },
      "outputs": [],
      "source": [
        "# set up the file paths\n",
        "file_paths = [\"karpathy/karpathy_build_nanogpt_code.md\", \"karpathy/karpathy_gpt2_from_scratch_audio_transcript.md\", \"karpathy/karpathy_gpt2_from_scratch_overview.md\", \"karpathy/karpathy_llmc_code.md\"]\n",
        "file_streams = [open(path, \"rb\") for path in file_paths]\n",
        "\n",
        "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
        "  vector_store_id=vector_store.id, files=file_streams\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### add vector store id to assistants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "AhXXOXp1eybd"
      },
      "outputs": [],
      "source": [
        "fs_assistant = client.beta.assistants.update(\n",
        "  assistant_id=fs_assistant.id,\n",
        "  tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "ci_assistant = client.beta.assistants.update(\n",
        "  assistant_id=ci_assistant.id,\n",
        "  tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBQOSGyyF2u5"
      },
      "source": [
        "Let's look at what our `file_batch` contains!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "zJrVLkMpFgwf"
      },
      "outputs": [],
      "source": [
        "while file_batch.status != \"completed\":\n",
        "  time(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd4O4dpZF-eH"
      },
      "source": [
        "#### Create and Use Assistant\n",
        "\n",
        "Now that we have our file - we can attach it to an Assistant, and we can give that Assistant the ability to use it for retrieval through the Retrieval tool!\n",
        "\n",
        "> NOTE: Your first GB is free and beyond that, usage is billed at $0.10/GB/day of vector storage. There are no other costs associated with vector store operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "JM9iJhoMcUfW"
      },
      "outputs": [],
      "source": [
        "fs_thread = client.beta.threads.create(\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"What will we learn from building GPT2 from scratch?  Why not just use a textbook?\",\n",
        "      \n",
        "    }\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0-mmRjQGeUR"
      },
      "source": [
        "We can use an extension of the `EventHandler` that we created above to stream our `run`!\n",
        "\n",
        "Let's add a few things:\n",
        "\n",
        "1. A `on_tool_call_created` function which tells us which tool is being used.\n",
        "2. A `on_message_done` that includes citations that were used by our File Search tool - this is like return the context *and* the response that we saw in our Pythonic RAG implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "av5A_tm0bpvf"
      },
      "outputs": [],
      "source": [
        "class FSEventHandler(AssistantEventHandler):\n",
        "  @override\n",
        "  def on_text_created(self, text) -> None:\n",
        "    print(f\"\\nassistant > \", end=\"\", flush=True)\n",
        "\n",
        "  @override\n",
        "  def on_tool_call_created(self, tool_call):\n",
        "    print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
        "\n",
        "  @override\n",
        "  def on_message_done(self, message) -> None:\n",
        "    message_content = message.content[0].text\n",
        "    annotations = message_content.annotations\n",
        "    citations = []\n",
        "    for index, annotation in enumerate(annotations):\n",
        "      message_content.value = message_content.value.replace(\n",
        "        annotation.text, f\"[{index}]\"\n",
        "      )\n",
        "      if file_citation := getattr(annotation, \"file_citation\", None):\n",
        "        cited_file = client.files.retrieve(file_citation.file_id)\n",
        "        citations.append(f\"[{index}] {cited_file.filename}\")\n",
        "\n",
        "    print(message_content.value)\n",
        "    print(\"\\n\".join(citations))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxJfa-saHbNQ"
      },
      "source": [
        "Let's look at the final result!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19nDqjRgHaNA",
        "outputId": "b33c4c2d-e8a7-432c-b596-a2169c487f9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "assistant > file_search\n",
            "\n",
            "\n",
            "assistant > When you choose to build GPT-2 from scratch instead of just studying from a textbook, you gain a deep and practical understanding of several key aspects of machine learning and neural networks. Here's an overview of what you'll learn:\n",
            "\n",
            "1. **Understanding the GPT-2 Architecture:**\n",
            "    - You'll explore the original GPT-2 code and Hugging Face Transformers implementation.\n",
            "    - Compare it to the original transformer architecture from the \"Attention is All You Need\" paper[0].\n",
            "\n",
            "2. **Implementing the GPT-2 Network:**\n",
            "    - Build a PyTorch `nn.Module` for GPT-2 from scratch, including implementations of token and positional embeddings, transformer blocks, MLPs, and attention mechanisms[0][2][3].\n",
            "\n",
            "3. **Optimizing for Speed:**\n",
            "    - Use techniques such as mixed precision training, Torch Compile, and Flash Attention to drastically improve training speed and throughput[0].\n",
            "\n",
            "4. **Hyperparameter Tuning and Optimization:**\n",
            "    - Apply advanced hyperparameters and optimization strategies, including AdamW optimizer, gradient clipping, learning rate schedulers, weight decay, and fused AdamW[0].\n",
            "\n",
            "5. **Data Handling and Preparation:**\n",
            "    - Learn about dataset selection, processing large datasets, and implementing efficient data loaders[0].\n",
            "\n",
            "6. **Distributed Training:**\n",
            "    - Understand and implement gradient accumulation to simulate large batch sizes on smaller GPUs and Distributed Data Parallel (DDP) to train across multiple GPUs[0].\n",
            "\n",
            "7. **Validation and Evaluation:**\n",
            "    - Implement validation loss calculation and use evaluation benchmarks to assess model performance[0].\n",
            "\n",
            "8. **Building Intuition and Confidence:**\n",
            "    - By reproducing a model like GPT-2 from scratch, you'll gain confidence in your ability to handle complex machine learning projects, understand underlying mechanisms, and troubleshoot issues effectively[0][3].\n",
            "\n",
            "Textbooks provide theoretical knowledge, which is essential. However, practical experience like building a model from scratch:\n",
            "- Solidifies your understanding by applying theory to practice.\n",
            "- Reveals the real-world challenges and intricacies of machine learning projects.\n",
            "- Develops problem-solving skills that are crucial for advancing in the field[0][2][3].\n",
            "\n",
            "In summary, building GPT-2 from scratch offers an immersive way to not only learn about neural networks and transformers but also to develop a hands-on skillset that textbooks alone can't provide. It's a comprehensive learning approach that combines theory with practical application and problem-solving.\n",
            "[0] karpathy_gpt2_from_scratch_overview.md\n",
            "[1] karpathy_gpt2_from_scratch_overview.md\n",
            "[2] karpathy_gpt2_from_scratch_audio_transcript.md\n",
            "[3] karpathy_gpt2_from_scratch_audio_transcript.md\n",
            "[4] karpathy_gpt2_from_scratch_overview.md\n",
            "[5] karpathy_gpt2_from_scratch_overview.md\n",
            "[6] karpathy_gpt2_from_scratch_overview.md\n",
            "[7] karpathy_gpt2_from_scratch_overview.md\n",
            "[8] karpathy_gpt2_from_scratch_overview.md\n",
            "[9] karpathy_gpt2_from_scratch_overview.md\n",
            "[10] karpathy_gpt2_from_scratch_audio_transcript.md\n",
            "[11] karpathy_gpt2_from_scratch_overview.md\n",
            "[12] karpathy_gpt2_from_scratch_audio_transcript.md\n",
            "[13] karpathy_gpt2_from_scratch_audio_transcript.md\n"
          ]
        }
      ],
      "source": [
        "with client.beta.threads.runs.stream(\n",
        "  thread_id=fs_thread.id,\n",
        "  assistant_id=fs_assistant.id,\n",
        "  event_handler=FSEventHandler(),\n",
        ") as stream:\n",
        "  stream.until_done()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pln9uYoJICno"
      },
      "source": [
        "### Task 2b: Creating an Assistant with the Code Interpreter Tool\n",
        "\n",
        "Now that we've explored the Retrieval Tool - let's try the Code Interpreter tool!\n",
        "\n",
        "The process will be almost exactly the same - but we can explore a different query, and we'll add our file at the Message level!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "7IPHZswUg6RH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'build-nanogpt' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/build-nanogpt.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_a8BLVdog1X8"
      },
      "outputs": [],
      "source": [
        "file = client.files.create(\n",
        "  file=open(\"build-nanogpt/train_gpt2.py\", \"rb\"),\n",
        "  purpose='assistants'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJPHAJCQJbgi"
      },
      "source": [
        "In the following example, we'll also see how we can package the Thread creation with the Message adding step!\n",
        "\n",
        "> NOTE: Files added at the message/thread level will not be available to the Assistant outside of that Thread."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "5xVdjH6EJQrr"
      },
      "outputs": [],
      "source": [
        "ci_thread = client.beta.threads.create(\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"This file looks pretty amazing.  Can you break it down step by step?  How would we go about training a GPT2 model from scratch ourselves\",\n",
        "      \"attachments\": [\n",
        "          {\n",
        "              \"file_id\" : file.id,\n",
        "              \"tools\" : [{\"type\" : \"code_interpreter\"}]\n",
        "          }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiIYut_dJ0Cv"
      },
      "source": [
        "We'll once again need to create an `EventHandler`, except this time it will have an `on_tool_call_delta` method which will let us see the output of the code interpreter tool as well!\n",
        "\n",
        "> NOTE: Remember that we create runs at the *thread* level - and so don't need the message object to continue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "or7iJ492KI2P"
      },
      "outputs": [],
      "source": [
        "class CIEventHandler(AssistantEventHandler):\n",
        "  @override\n",
        "  def on_text_created(self, text) -> None:\n",
        "    print(f\"\\nassistant > \", end=\"\", flush=True)\n",
        "\n",
        "  @override\n",
        "  def on_text_delta(self, delta, snapshot):\n",
        "    print(delta.value, end=\"\", flush=True)\n",
        "\n",
        "  def on_tool_call_created(self, tool_call):\n",
        "    print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
        "\n",
        "  def on_tool_call_delta(self, delta, snapshot):\n",
        "    if delta.type == 'code_interpreter':\n",
        "      if delta.code_interpreter.input:\n",
        "        print(delta.code_interpreter.input, end=\"\", flush=True)\n",
        "      if delta.code_interpreter.outputs:\n",
        "        print(f\"\\n\\noutput >\", flush=True)\n",
        "        for output in delta.code_interpreter.outputs:\n",
        "          if output.type == \"logs\":\n",
        "            print(f\"\\n{output.logs}\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPiivot-n5A8"
      },
      "source": [
        "Once again, we can use the streaming interface thanks to creating our `EventHandler`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXsmBqFfiSvT",
        "outputId": "dbfed50c-4e66-4a01-e36a-9651be586be0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "assistant > Of course! Let's start by taking a look at the contents of the file you have uploaded. I'll first check what type of file it is, and then we'll break down the steps to train a GPT-2 model from scratch.\n",
            "\n",
            "Let's begin by opening and inspecting the file.\n",
            "assistant > code_interpreter\n",
            "\n",
            "# Let's inspect the file to understand its contents. \n",
            "\n",
            "file_path = '/mnt/data/file-zMuynmpm10xTpOn98gDrZgmt'\n",
            "\n",
            "# Try to determine what type of file it is by reading the file and check initial few lines\n",
            "with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
            "    contents = file.readlines()\n",
            "\n",
            "# Display the first few lines of the file to understand its format and content\n",
            "contents[:10]  # Displaying the first 10 lines for inspection\n",
            "assistant > Great, it looks like the file contains Python code related to training a model, possibly using PyTorch. Let's walk through a typical process to train a GPT-2 model from scratch.\n",
            "\n",
            "### Steps to Train a GPT-2 Model from Scratch\n",
            "\n",
            "1. **Set up the environment**: Ensure you have the required libraries installed and a suitable environment (e.g., GPU for training).\n",
            "\n",
            "2. **Data Preparation**: Gather and preprocess the text data for training. The data should be tokenized and formatted appropriately for the model.\n",
            "\n",
            "3. **Model Architecture**: Define the GPT-2 model architecture. This involves specifying the hyperparameters and building the Transformer decoder model.\n",
            "\n",
            "4. **Training Loop**: Implement the training loop, including forward pass, loss calculation, and backpropagation.\n",
            "\n",
            "5. **Evaluation**: Periodically evaluate the model on a validation set to monitor its performance.\n",
            "\n",
            "6. **Checkpointing**: Save the model checkpoints during training to avoid losing progress.\n",
            "\n",
            "7. **Fine-tuning and Inference**: Optionally fine-tune the model on specific tasks and perform inference using the trained model.\n",
            "\n",
            "Let's go through each of these steps in more detail, referencing the code from your file as we go along.\n",
            "\n",
            "### 1. Set Up the Environment\n",
            "Ensure you have the necessary libraries installed:\n",
            "\n",
            "```bash\n",
            "pip install torch transformers\n",
            "```\n",
            "\n",
            "We'll start by importing the necessary libraries and setting up device configurations.\n",
            "\n",
            "### 2. Data Preparation\n",
            "Use a dataset of text for training. Libraries like Hugging Face's `datasets` can be useful. Tokenize the text data to convert it into a format suitable for training.\n",
            "\n",
            "### 3. Model Architecture\n",
            "Define the GPT-2 configuration and model. This involves setting the number of layers, heads, and other crucial parameters.\n",
            "\n",
            "### 4. Training Loop\n",
            "Implement the loop that handles the training processes, such as forward and backward propagation.\n",
            "\n",
            "### 5. Evaluation\n",
            "Use a validation set to periodically check how well the model is doing during training.\n",
            "\n",
            "### 6. Checkpointing\n",
            "Save the model's state frequently to prevent losing progress.\n",
            "\n",
            "### 7. Fine-tuning and Inference\n",
            "Once trained, the model can be fine-tuned for specific downstream tasks and used for generating text.\n",
            "\n",
            "Let's break down the provided code to fit these steps. We will address each part accordingly by using the structural cues in the code you provided.\n",
            "\n",
            "#### Checking Data Preparation\n",
            "\n",
            "Let's start by seeing if there are any data loading or preprocessing steps in the file.# Extract more lines from the file to inspect for data loading or preprocessing steps\n",
            "more_content = contents[10:50]  # Displaying more lines to locate data preparation steps\n",
            "more_content\n",
            "assistant > It looks like the provided lines of code are focusing on defining components of the GPT-2 model, such as the `CausalSelfAttention` and `MLP` (Multi-Layer Perceptron) classes. This is part of the model architecture step.\n",
            "\n",
            "To support you further, we will need to locate sections of the file dedicated to:\n",
            "\n",
            "1. **Data Preparation**\n",
            "2. **Model Architecture** (which we can see portions of already)\n",
            "3. **Training Loop**\n",
            "4. **Evaluation**\n",
            "5. **Checkpointing**\n",
            "\n",
            "Let's break each part down starting with the **Model Architecture** since we already see parts of it.\n",
            "\n",
            "### Model Architecture\n",
            "Based on the shared code, here are simplified versions of the `CausalSelfAttention` and `MLP` classes:\n",
            "\n",
            "```python\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "import torch.nn.functional as F\n",
            "\n",
            "class CausalSelfAttention(nn.Module):\n",
            "\n",
            "    def __init__(self, config):\n",
            "        super().__init__()\n",
            "        assert config.n_embd % config.n_head == 0\n",
            "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
            "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
            "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
            "        self.n_head = config.n_head\n",
            "        self.n_embd = config.n_embd\n",
            "        self.register_buffer(\n",
            "            \"bias\", \n",
            "            torch.tril(torch.ones(config.block_size, config.block_size))\n",
            "                .view(1, 1, config.block_size, config.block_size)\n",
            "        )\n",
            "\n",
            "    def forward(self, x):\n",
            "        B, T, C = x.size()\n",
            "        qkv = self.c_attn(x)\n",
            "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
            "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
            "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
            "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
            "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
            "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
            "        y = self.c_proj(y)\n",
            "        return y\n",
            "\n",
            "class MLP(nn.Module):\n",
            "\n",
            "    def __init__(self, config):\n",
            "        super().__init__()\n",
            "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
            "        self.gelu = nn.GELU(approximate='tanh')\n",
            "```\n",
            "\n",
            "These snippets highlight parts of the GPT-2 model responsible for self-attention and feedforward operations. \n",
            "\n",
            "### Finding Other Sections\n",
            "Let's explore more lines to find the sections for **Data Preparation**, **Training Loop**, **Evaluation** and **Checkpointing**.# Extract additional lines from the file to locate other sections\n",
            "more_content_2 = contents[50:100]  # Extracting additional lines to locate more sections\n",
            "more_content_2\n",
            "assistant > This additional code contains further definitions for the GPT-2 model. It defines classes for `Block`, `GPTConfig`, and the main `GPT` model itself.\n",
            "\n",
            "### Model Architecture (Continued)\n",
            "\n",
            "Here's the rest from the snippets provided:\n",
            "\n",
            "- **Block Class**: Represents a transformer block consisting of layer normalization, self-attention, and MLP.\n",
            "- **GPTConfig Class**: Configuration for the GPT model including vocabulary size, number of layers, heads, and embedding dimensions.\n",
            "- **GPT Class**: The main GPT model that uses the above components.\n",
            "\n",
            "Let's complete the GPT architecture overview with this new information.\n",
            "\n",
            "```python\n",
            "class Block(nn.Module):\n",
            "\n",
            "    def __init__(self, config):\n",
            "        super().__init__()\n",
            "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
            "        self.attn = CausalSelfAttention(config)\n",
            "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
            "        self.mlp = MLP(config)\n",
            "\n",
            "    def forward(self, x):\n",
            "        x = x + self.attn(self.ln_1(x))\n",
            "        x = x + self.mlp(self.ln_2(x))\n",
            "        return x\n",
            "\n",
            "@dataclass\n",
            "class GPTConfig:\n",
            "    block_size: int = 1024  # Max sequence length\n",
            "    vocab_size: int = 50257  # Tokens: 50,000 BPE merges + 256 bytes + 1 end token\n",
            "    n_layer: int = 12  # Number of layers\n",
            "    n_head: int = 12  # Number of heads\n",
            "    n_embd: int = 768  # Embedding dimension\n",
            "\n",
            "class GPT(nn.Module):\n",
            "\n",
            "    def __init__(self, config):\n",
            "        super().__init__()\n",
            "        self.config = config\n",
            "\n",
            "        self.transformer = nn.ModuleDict(\n",
            "            dict(\n",
            "                wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
            "                wpe=nn.Embedding(config.block_size, config.n_embd),\n",
            "                h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
            "                ln_f=nn.LayerNorm(config.n_embd),\n",
            "            )\n",
            "        )\n",
            "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
            "\n",
            "        # Weight sharing\n",
            "        self.transformer.wte.weight = self.lm_head.weight\n",
            "\n",
            "        # Initialize weights\n",
            "        self.apply(self._init_weights)\n",
            "\n",
            "    def _init_weights(self, module):\n",
            "        if isinstance(module, nn.Linear):\n",
            "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
            "            if module.bias is not None:\n",
            "                module.bias.data.zero_()\n",
            "        elif isinstance(module, nn.Embedding):\n",
            "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
            "        elif isinstance(module, nn.LayerNorm):\n",
            "            module.bias.data.zero_()\n",
            "            module.weight.data.fill_(1.0)\n",
            "```\n",
            "\n",
            "Next, let‚Äôs locate the other sections (Data Preparation, Training Loop, Evaluation, and Checkpointing). We need to check more lines.# Continue extracting lines to find data preparation, training, evaluation, and checkpointing sections\n",
            "more_content_3 = contents[100:150]\n",
            "more_content_3\n",
            "assistant > ### Model Architecture (Continued)\n",
            "\n",
            "We see additional details for the GPT class including initialization of weights, forward pass, and a class method to load pretrained models. \n",
            "\n",
            "Here is a more complete version of the `GPT` class integrated with the previous portions:\n",
            "\n",
            "```python\n",
            "class GPT(nn.Module):\n",
            "\n",
            "    def __init__(self, config):\n",
            "        super().__init__()\n",
            "        self.config = config\n",
            "\n",
            "        self.transformer = nn.ModuleDict(\n",
            "            dict(\n",
            "                wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
            "                wpe=nn.Embedding(config.block_size, config.n_embd),\n",
            "                h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
            "                ln_f=nn.LayerNorm(config.n_embd),\n",
            "            )\n",
            "        )\n",
            "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
            "\n",
            "        # Weight sharing\n",
            "        self.transformer.wte.weight = self.lm_head.weight\n",
            "\n",
            "        # Initialize weights\n",
            "        self.apply(self._init_weights)\n",
            "\n",
            "    def _init_weights(self, module):\n",
            "        if isinstance(module, nn.Linear):\n",
            "            std = 0.02\n",
            "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
            "                std *= (2 * self.config.n_layer) ** -0.5\n",
            "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
            "            if module.bias is not None:\n",
            "                torch.nn.init.zeros_(module.bias)\n",
            "        elif isinstance(module, nn.Embedding):\n",
            "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
            "        elif isinstance(module, nn.LayerNorm):\n",
            "            module.bias.data.zero_()\n",
            "            module.weight.data.fill_(1.0)\n",
            "\n",
            "    def forward(self, idx, targets=None):\n",
            "        B, T = idx.size()\n",
            "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
            "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
            "        pos_emb = self.transformer.wpe(pos)\n",
            "        tok_emb = self.transformer.wte(idx)\n",
            "        x = tok_emb + pos_emb\n",
            "        for block in self.transformer.h:\n",
            "            x = block(x)\n",
            "        x = self.transformer.ln_f(x)\n",
            "        logits = self.lm_head(x)\n",
            "        loss = None\n",
            "        if targets is not None:\n",
            "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
            "        return logits, loss\n",
            "\n",
            "    @classmethod\n",
            "    def from_pretrained(cls, model_type):\n",
            "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
            "        from transformers import GPT2LMHeadModel\n",
            "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
            "\n",
            "        config_args = {\n",
            "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),\n",
            "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024),\n",
            "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280),\n",
            "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600),\n",
            "        }[model_type]\n",
            "        config_args['vocab_size'] = 50257\n",
            "        config_args['block_size'] = 1024\n",
            "        config = GPTConfig(**config_args)\n",
            "```\n",
            "\n",
            "### Data Preparation, Training Loop, Evaluation, Checkpointing\n",
            "\n",
            "Since GPT‚Äôs model architecture is mostly covered, let‚Äôs now look for\tdata preparation (loading and tokenizing data), training loop implementation, evaluation steps, and checkpointing logic.\n",
            "\n",
            "The foundation is set. Next, we search for the remaining portions, scanning more lines.# Continue extracting lines to find data preparation, training, evaluation, and checkpointing sections\n",
            "more_content_4 = contents[150:250]\n",
            "more_content_4\n",
            "assistant > Now we're finding the areas related to **Data Preparation** and some hints towards the **Training Loop**.\n",
            "\n",
            "### Data Preparation\n",
            "\n",
            "Here‚Äôs a straightforward data loading and tokenization portion from the provided snippet:\n",
            "\n",
            "```python\n",
            "import numpy as np\n",
            "import torch\n",
            "\n",
            "def load_tokens(filename):\n",
            "    npt = np.load(filename)\n",
            "    ptt = torch.tensor(npt, dtype=torch.long)\n",
            "    return ptt\n",
            "\n",
            "class DataLoaderLite:\n",
            "    def __init__(self, B, T, process_rank, num_processes, split):\n",
            "        self.B = B\n",
            "        self.T = T\n",
            "        self.process_rank = process_rank\n",
            "        self.num_processes = num_processes\n",
            "        assert split in {'train', 'val'}\n",
            "\n",
            "        data_root = \"edu_fineweb10B\"\n",
            "        shards = os.listdir(data_root)\n",
            "        shards = [s for s in shards if split in s]\n",
            "        shards = sorted(shards)\n",
            "        shards = [os.path.join(data_root, s) for s in shards]\n",
            "        self.shards = shards\n",
            "        assert len(shards) > 0, f\"no shards found for split {split}\"\n",
            "        if master_process:\n",
            "            print(f\"found {len(shards)} shards for split {split}\")\n",
            "        self.reset()\n",
            "\n",
            "    def reset(self):\n",
            "        self.current_shard = 0\n",
            "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
            "        self.current_position = self.B * self.T * self.process_rank\n",
            "\n",
            "    def next_batch(self):\n",
            "        B, T = self.B, self.T\n",
            "        buf = self.tokens[self.current_position : self.current_position + B * T + 1]\n",
            "        x = (buf[:-1]).view(B, T)  # Inputs\n",
            "        y = (buf[1:]).view(B, T)   # Targets\n",
            "        self.current_position += B * T * self.num_processes\n",
            "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
            "            self.current_position = 0\n",
            "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
            "```\n",
            "\n",
            "### Training Loop and Checkpointing\n",
            "\n",
            "Next are methods related to configuring optimizers, saving, and resuming the state, which hint towards checkpointing:\n",
            "\n",
            "```python\n",
            "def configure_optimizers(self, weight_decay, learning_rate, device):\n",
            "    param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
            "    decay_params = [p for n, p in param_dict.items() if any(nd in n for nd in ['bias', 'ln', 'ln_', 'b_'])]\n",
            "    nodecay_params = [p for n, p in param_dict.items() if not any(nd in n for nd in ['bias', 'ln', 'ln_', 'b_'])]\n",
            "    optim_groups = [\n",
            "        {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
            "        {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
            "    ]\n",
            "    use_fused = torch.backends.cuda.matmul.allow_tf32    # use Fused Adamw if on cuda\n",
            "    optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
            "    return optimizer\n",
            "```\n",
            "\n",
            "### Checkpoints\n",
            "\n",
            "Saving and resuming states:\n",
            "```python\n",
            "def save_checkpoint(self, checkpoint_path):\n",
            "    torch.save({\n",
            "        'model_state_dict': self.state_dict(),\n",
            "        'optimizer_state_dict': self.optimizer.state_dict(),\n",
            "    }, checkpoint_path)\n",
            "\n",
            "def load_checkpoint(self, checkpoint_path):\n",
            "    checkpoint = torch.load(checkpoint_path)\n",
            "    self.load_state_dict(checkpoint['model_state_dict'])\n",
            "    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
            "```\n",
            "\n",
            "**Next Steps**: Let's put these parts into context and focus on completion especially the training loop.\n",
            "\n",
            "Let's continue exploring more lines to complete our inspection.# Continue extracting lines to find more about the training loop, evaluation and checkpointing sections\n",
            "more_content_5 = contents[250:350]\n",
            "more_content_5\n",
            "assistant > This snippet covers a lot of ground on the **Data Preparation**, **Training Loop**, and **Evaluation**.\n",
            "\n",
            "### Training Loop\n",
            "\n",
            "Here‚Äôs how the training loop and some of the supporting setup are structured:\n",
            "\n",
            "1. **Initial Configuration for Training**:\n",
            "   ```python\n",
            "   B = 64  # Batch size\n",
            "   T = 1024  # Sequence length\n",
            "   ```\n",
            "\n",
            "2. **Data Loaders Initialization**:\n",
            "   ```python\n",
            "   train_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"train\")\n",
            "   val_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size, split=\"val\")\n",
            "   ```\n",
            "\n",
            "3. **Model Setup and Distributed Training**:\n",
            "   ```python\n",
            "   model = GPT(GPTConfig(vocab_size=50304))\n",
            "   model.to(device)\n",
            "   if ddp:\n",
            "       model = DDP(model, device_ids=[ddp_local_rank])\n",
            "   raw_model = model.module if ddp else model\n",
            "   ```\n",
            "\n",
            "4. **Optimizer Configuration**:\n",
            "   ```python\n",
            "   max_lr = 6e-4\n",
            "   min_lr = max_lr * 0.1\n",
            "   warmup_steps = 715\n",
            "   ```\n",
            "\n",
            "5. **Running the Training Loop**:\n",
            "   ```python\n",
            "   for epoch in range(epochs):\n",
            "       model.train()  # Set the model to training mode\n",
            "       total_loss = 0\n",
            "       \n",
            "       for batch in train_loader:\n",
            "           inputs, targets = batch\n",
            "           inputs, targets = inputs.to(device), targets.to(device)\n",
            "           \n",
            "           optimizer.zero_grad()\n",
            "           outputs, loss = model(inputs, targets)\n",
            "           loss.backward()\n",
            "           optimizer.step()\n",
            "           total_loss += loss.item()\n",
            "\n",
            "       print(f\"Epoch {epoch}, Loss: {total_loss/len(train_loader)}\")\n",
            "       \n",
            "       # Evaluation\n",
            "       model.eval()\n",
            "       with torch.no_grad():\n",
            "           val_loss = 0\n",
            "           for batch in val_loader:\n",
            "               inputs, targets = batch\n",
            "               inputs, targets = inputs.to(device), targets.to(device)\n",
            "               outputs, loss = model(inputs, targets)\n",
            "               val_loss += loss.item()\n",
            "           print(f\"Validation Loss after Epoch {epoch}: {val_loss/len(val_loader)}\")\n",
            "       \n",
            "       # Checkpointing\n",
            "       if master_process and (epoch % checkpoint_interval == 0 or epoch == epochs - 1):\n",
            "           save_checkpoint(f'checkpoint_epoch_{epoch}.pt')\n",
            "   ```\n",
            "\n",
            "### Checkpointing\n",
            "Implement checkpointing (saving the model state) at the end of each epoch or at intervals:\n",
            "```python\n",
            "if master_process and (epoch % checkpoint_interval == 0 or epoch == epochs - 1):\n",
            "    torch.save(model.state_dict(), f'checkpoint_epoch_{epoch}.pt')\n",
            "```\n",
            "\n",
            "### Evaluation\n",
            "A helper function for evaluation:\n",
            "```python\n",
            "def evaluate_model(model, dataloader, device):\n",
            "    model.eval()\n",
            "    total_loss = 0\n",
            "    with torch.no_grad():\n",
            "        for inputs, targets in dataloader:\n",
            "            inputs, targets = inputs.to(device), targets.to(device)\n",
            "            outputs, loss = model(inputs, targets)\n",
            "            total_loss += loss.item()\n",
            "    return total_loss / len(dataloader)\n",
            "\n",
            "val_loss = evaluate_model(model, val_loader, device)\n",
            "print(f\"Validation loss: {val_loss}\")\n",
            "```\n",
            "\n",
            "### Final Assembly\n",
            "We have gathered bits and pieces from different areas. Here is a concise version putting it all together:\n",
            "\n",
            "```python\n",
            "import os\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "import torch.nn.functional as F\n",
            "from torch.optim import AdamW\n",
            "from torch.utils.data import DataLoader, Dataset\n",
            "from dataclasses import dataclass\n",
            "\n",
            "@dataclass\n",
            "class GPTConfig:\n",
            "    block_size: int = 1024\n",
            "    vocab_size: int = 50257\n",
            "    n_layer: int = 12\n",
            "    n_head: int = 12\n",
            "    n_embd: int = 768\n",
            "\n",
            "class CausalSelfAttention(nn.Module):\n",
            "    def __init__(self, config):\n",
            "        super().__init__()\n",
            "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
            "        # ... (rest of the implementation)\n",
            "        \n",
            "class MLP(nn.Module):\n",
            "    def __init__(self, config):\n",
            "        super().__init__()\n",
            "        # ... (rest of the implementation)\n",
            "        \n",
            "class Block(nn.Module):\n",
            "    def __init__(self, config):\n",
            "        super().__init__()\n",
            "        # ... (rest of the implementation)\n",
            "        \n",
            "class GPT(nn.Module):\n",
            "    def __init__(self, config):\n",
            "        super().__init__()\n",
            "        # ... (rest of the implementation)\n",
            "        \n",
            "    def forward(self, idx, targets=None):\n",
            "        # ... (rest of the implementation)\n",
            "\n",
            "class TextDataset(Dataset):\n",
            "    # Custom dataset class providing tokenized inputs and targets\n",
            "    pass\n",
            "\n",
            "class DataLoaderLite:\n",
            "    # Custom data loader as provided\n",
            "\n",
            "def configure_optimizers(model, learning_rate, weight_decay):\n",
            "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
            "    return optimizer\n",
            "\n",
            "def train_gpt2(model, train_loader, val_loader, epochs, lr, device):\n",
            "    optimizer = configure_optimizers(model, lr, weight_decay=0.1)\n",
            "    model.to(device)\n",
            "    for epoch in range(epochs):\n",
            "        model.train()\n",
            "        total_loss = 0\n",
            "        for x,y in train_loader:\n",
            "            x, y = x.to(device), y.to(device)\n",
            "            optimizer.zero_grad()\n",
            "            logits, loss = model(x, y)\n",
            "            loss.backward()\n",
            "            optimizer.step()\n",
            "            total_loss += loss.item()\n",
            "\n",
            "        print(f\"Epoch {epoch}, Train Loss: {total_loss / len(train_loader)}\")\n",
            "        val_loss = evaluate_model(model, val_loader, device)\n",
            "        print(f\"Validation Loss: {val_loss}\")\n",
            "        torch.save(model.state_dict(), f\"checkpoint_{epoch}.pt\")\n",
            "\n",
            "def evaluate_model(model, dataloader, device):\n",
            "    model.eval()\n",
            "    total_loss = 0\n",
            "    with torch.no_grad():\n",
            "        for inputs, targets in dataloader:\n",
            "            inputs, targets = inputs.to(device), targets.to(device)\n",
            "            outputs, loss = model(inputs, targets)\n",
            "            total_loss += loss.item()\n",
            "    return total_loss / len(dataloader)\n",
            "\n",
            "# Assuming data loaders prepared, initialized model, and configured device.\n",
            "train_gpt2(model, train_loader, val_loader, epochs=10, lr=5e-5, device=device)\n",
            "```\n",
            "\n",
            "### Conclusion\n",
            "This assembled version compiles the major components necessary to train a GPT-2 model from scratch as per your file's provided insights. We ensured various aspects such as data handling, model architecture, training and evaluation loops, and checkpointing procedures are covered."
          ]
        }
      ],
      "source": [
        "with client.beta.threads.runs.stream(\n",
        "  thread_id=ci_thread.id,\n",
        "  assistant_id=ci_assistant.id,\n",
        "  instructions=additional_instructions,\n",
        "  event_handler=CIEventHandler(),\n",
        ") as stream:\n",
        "  stream.until_done()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi10hON2LQmc"
      },
      "source": [
        "And there you go!\n",
        "\n",
        "We've fit our Assistant with an awesome Code Interpreter that lets our Assistant run code on our provided files!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdJxt77oLzu7"
      },
      "source": [
        "### Task 2c: Creating an Assistant with a Function Calling Tool\n",
        "\n",
        "Let's finally create an Assistant that utilizes the Function Calling API.\n",
        "\n",
        "We'll start by creating a function that we wish to be called.\n",
        "\n",
        "We'll utilize DuckDuckGo search to allow our Assistant to have the most up to date information!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5eKEC2wMMVI",
        "outputId": "1712c586-0c40-410c-e2e6-07ea7c918c54"
      },
      "outputs": [],
      "source": [
        "!pip install -qU duckduckgo_search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "YPFZ_Uq_LawH"
      },
      "outputs": [],
      "source": [
        "from duckduckgo_search import DDGS\n",
        "\n",
        "def duckduckgo_search(query):\n",
        "  with DDGS() as ddgs:\n",
        "    results = [r for r in ddgs.text(query, max_results=5)]\n",
        "    return \"\\n\".join(result[\"body\"] for result in results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-o1TBFpMSvR"
      },
      "source": [
        "Let's test our function to make sure it behaves as we expect it to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "mCUr9jFCMWBw",
        "outputId": "ea9bb779-f7c3-49e5-b5d4-c12be875a460"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Andrej Karpathy (born 23 October 1986) is a Slovak-Canadian computer scientist who served as the director of artificial intelligence and Autopilot Vision at Tesla. He co-founded and formerly worked at OpenAI, where he specialized in deep learning and computer vision. ...\\nAndrej Karpathy, Tesla\\'s director of artificial intelligence, announced Wednesday he\\'s leaving the company only months before its anticipated release of its long-delayed \"full self-driving ...\\nAndrej Karpathy. 2024 -. coming soon üßë\\u200düç≥. 2023 - 2024. Back to OpenAI. Built a small team, launched a model to ChatGPT, great pleasure to build with the top notch talent within. 2017 - 2022. I was the Sr. Director of AI at Tesla, where I led the computer vision team of Tesla Autopilot. This includes in-house data labeling, neural ...\\nAndrej Karpathy is a computer scientist who has a passion for training deep neural nets on large datasets. He is best known for his principal roles at OpenAI and Tesla and also designed and instructed the first deep learning class at Stanford University. Post-university, Karpathy joined OpenAI as one of its founding research scientists.\\nAndrej Karpathy, a widely respected research scientist, announced today that he has left OpenAI. This is the second time Karpathy has left the top AI firm and his departure is not because of any ...'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "duckduckgo_search(\"Who is Andrej Karpathy?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzE1nxt5Mi80"
      },
      "source": [
        "Now we need to express how our function works in a way that is compatible with the OpenAI Function Calling API.\n",
        "\n",
        "We'll want to provide a `JSON` object that includes what parameters we have, how to call them, and a short natural language description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "8ElrWvBnMY_s"
      },
      "outputs": [],
      "source": [
        "ddg_function = {\n",
        "    \"name\" : \"duckduckgo_search\",\n",
        "    \"description\" : \"Answer non-technical questions. \",\n",
        "    \"parameters\" : {\n",
        "        \"type\" : \"object\",\n",
        "        \"properties\" : {\n",
        "            \"query\" : {\n",
        "                \"type:\" : \"string\",\n",
        "                \"description\" : \"The search query to use. For example: 'Who is the current Goalie of the Colorado Avalance?'\"\n",
        "            }\n",
        "        },\n",
        "        \"required\" : [\"query\"]\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyRmJgEQVuGs"
      },
      "source": [
        "#### ‚ùì Question\n",
        "\n",
        "Why does the description key-value pair matter? (with the OpenAI function calling api)\n",
        "\n",
        "##### ANSWER\n",
        "\n",
        "(it helps out developers as well, but the following reasons seemd more relevant to the question...)\n",
        "\n",
        "Yes, the `description` key-value pair plays a crucial role in the OpenAI function calling API by helping the language model determine which tool is most appropriate for a given task. This functionality enhances the model's ability to interact effectively with external tools and APIs. Here are the key points:\n",
        "\n",
        "1. **Contextual Understanding and Disambiguation:**\n",
        "   - The `description` provides the model with a clear understanding of what each function does, enabling it to distinguish between multiple functions that may have similar names or purposes. This helps the model make more accurate decisions when selecting the appropriate function to call.\n",
        "\n",
        "2. **Intent Matching:**\n",
        "   - By using the `description`, the model can better align the user's intent with the correct function. This alignment ensures that the model chooses a function that best fits the user's request, improving the relevance and accuracy of the function call.\n",
        "\n",
        "3. **Enhanced Performance:**\n",
        "   - Descriptions help the model generate structured JSON objects that contain the necessary arguments for calling the functions. This structured approach allows developers to create more reliable and sophisticated applications by ensuring the model's outputs are consistent with the function signatures.\n",
        "\n",
        "4. **Use Cases:**\n",
        "   - Function calling can convert natural language queries into API calls, database queries, or structured data extraction tasks. For example, the model can convert a query like \"What's the weather like in Boston?\" into a function call like `get_current_weather(location: string, unit: 'celsius' | 'fahrenheit')`.\n",
        "\n",
        "These capabilities are part of the updates to the GPT-4 and GPT-3.5-turbo models, which include improved function calling features that make the integration with external tools more efficient and reliable.\n",
        "\n",
        "Reference:\n",
        "\n",
        "- [Function calling](https://platform.openai.com/docs/guides/function-calling)\n",
        "- [How to call functions with chat models](https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models)\n",
        "- [Create chat completion - tool choice](https://platform.openai.com/docs/api-reference/chat/create#chat-create-tool_choice)\n",
        "- [examples/Fine_tuning_for_function_calling.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/Fine_tuning_for_function_calling.ipynb)\n",
        "- [GitHub search on OpenAI Function Calling](https://github.com/search?q=repo%3Aopenai%2Fopenai-cookbook%20Function%20Calling&type=code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Assistant with Function Calling tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "4eFpwi12Mzlg"
      },
      "outputs": [],
      "source": [
        "# confirm ability to set json mode during assistant creation\n",
        "\n",
        "fc_assistant = client.beta.assistants.create(\n",
        "    name=name + \" + Function Calling\",\n",
        "    instructions=instructions,\n",
        "    tools=[\n",
        "        {\"type\": \"function\",\n",
        "         \"function\" : ddg_function\n",
        "        }\n",
        "    ],\n",
        "    model=model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tir4WySGM0x2"
      },
      "source": [
        "Now when we create our Assistant - we'll want to include the function description as a tool using the following format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcoiF_gXksaa"
      },
      "source": [
        "Now we can create our thread, and attach our message to it - just as we've been doing!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "CtELZ5wFjdo_"
      },
      "outputs": [],
      "source": [
        "fc_thread = client.beta.threads.create()\n",
        "fc_message = client.beta.threads.messages.create(\n",
        "  thread_id=fc_thread.id,\n",
        "  role=\"user\",\n",
        "  content=\"Can you describe the Twitter beef between Elon and LeCun? Use JSON as the response format.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KT_dj2YkwyK"
      },
      "source": [
        "Once again, we'll need an `EventHandler` for the streaming response - notice that this time we're utilizing a few new methods:\n",
        "\n",
        "1. `handle_requires_action` - this will handle whatever action needs to take place in *our local environment*.\n",
        "2. `submit_tool_outputs` - this will let us submit the resultant outputs from our local function call back to the Assistant run in the required format.\n",
        "\n",
        "To be very clear and explicit - we'll be following this pattern:\n",
        "\n",
        "1. Make a call to the LLM which will decide if a local function call is required.\n",
        "2. If a local function call is required - send a response that indicates a local function call is required.\n",
        "3. Call the function using the arguments provided by the LLM.\n",
        "4. Return the response from the function call with LLM provided arguments.\n",
        "5. Receive a response based on the output of the functional call from the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "qgXhMEHbizEX"
      },
      "outputs": [],
      "source": [
        "class FCEventHandler(AssistantEventHandler):\n",
        "  @override\n",
        "  def on_event(self, event):\n",
        "    # Retrieve events that are denoted with 'requires_action'\n",
        "    # since these will have our tool_calls\n",
        "    if event.event == 'thread.run.requires_action':\n",
        "      run_id = event.data.id  # Retrieve the run ID from the event data\n",
        "      self.handle_requires_action(event.data, run_id)\n",
        "\n",
        "  def handle_requires_action(self, data, run_id):\n",
        "    tool_outputs = []\n",
        "\n",
        "    for tool in data.required_action.submit_tool_outputs.tool_calls:\n",
        "      print(tool.function.arguments)\n",
        "      if tool.function.name == \"duckduckgo_search\":\n",
        "        tool_outputs.append({\"tool_call_id\": tool.id, \"output\": duckduckgo_search(tool.function.arguments)})\n",
        "\n",
        "    # Submit all tool_outputs at the same time\n",
        "    self.submit_tool_outputs(tool_outputs, run_id)\n",
        "\n",
        "  def submit_tool_outputs(self, tool_outputs, run_id):\n",
        "    # Use the submit_tool_outputs_stream helper\n",
        "    with client.beta.threads.runs.submit_tool_outputs_stream(\n",
        "      thread_id=self.current_run.thread_id,\n",
        "      run_id=self.current_run.id,\n",
        "      tool_outputs=tool_outputs,\n",
        "      event_handler=FCEventHandler(),\n",
        "    ) as stream:\n",
        "      for text in stream.text_deltas:\n",
        "        print(text, end=\"\", flush=True)\n",
        "      print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9Gzz4OZnxCQ"
      },
      "source": [
        "Thanks to our event handler - we can stream this process in our notebook!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HGInY8fjYNa",
        "outputId": "b6fc88c6-4e58-46f2-c21a-0869fc208c46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"query\":\"Twitter beef between Elon Musk and Yann LeCun\"}\n",
            "```json\n",
            "{\n",
            "  \"summary\": \"There have been various exchanges between Elon Musk and Yann LeCun on Twitter. Yann LeCun, Chief AI Scientist at Meta (formerly Facebook), and Elon Musk, CEO of SpaceX and Tesla, have different views on the future and safety of artificial intelligence. Musk has expressed concerns over AI safety and the potential existential threat it poses. In contrast, LeCun has been more optimistic and has criticized Musk's doomsday stance on AI. Their exchanges often involve debating AI's future impacts, safety protocols, and differing perspectives on the responsible development of technology.\",\n",
            "  \"details\": [\n",
            "    {\n",
            "      \"date\": \"January 2020\",\n",
            "      \"content\": \"Yann LeCun criticized Elon Musk for his alarmist views on AI safety, claiming that Musk's perspective spreads unnecessary fear. Musk responded by reiterating his concerns about AI safety.\"\n",
            "    },\n",
            "    {\n",
            "      \"date\": \"July 2021\",\n",
            "      \"content\": \"Musk tweeted about concerns regarding the superintelligent AI and its potential risks, to which LeCun responded by emphasizing the unrealistic nature of such fears based on current AI progress and capabilities.\"\n",
            "    },\n",
            "    {\n",
            "      \"date\": \"May 2022\",\n",
            "      \"content\": \"Yann LeCun pointed out that concerns about AI turning malicious are exaggerated and that current AI systems are not capable of the autonomous decisions that Musk fears.\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "with client.beta.threads.runs.stream(\n",
        "  thread_id=fc_thread.id,\n",
        "  assistant_id=fc_assistant.id,\n",
        "  event_handler=FCEventHandler()\n",
        ") as stream:\n",
        "  stream.until_done()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# How do we diagram this flow?\n",
        "\n",
        "- and how do we\n",
        "  - effectively monitor it?\n",
        "  - implement resiliency?\n",
        "- what Event Handler solutions exist in the market?\n",
        "  - where do solutions such as LangChain / LangGraph and LllamaIndex fit in?\n",
        "  - what about existing Hyperscaler solutions?\n",
        "\n",
        "- need to fix this diagram so it reflects the basics events, maybe change it to a state diagram :)\n",
        "\n",
        "Reiterating some of the key events:\n",
        "1. A `on_tool_call_created` function which tells us which tool is being used.\n",
        "2. A `on_message_done` that includes citations that were used by our File Search tool - this is like return the context *and* the response that we saw in our Pythonic RAG implementation.\n",
        "3. on_tool_call_delta\n",
        "4. `handle_requires_action` - this will handle whatever action needs to take place in *our local environment*.\n",
        "5. `submit_tool_outputs` - this will let us submit the resultant outputs from our local function call back to the Assistant run in the required format.\n",
        "\n",
        "\n",
        "```mermaid\n",
        "sequenceDiagram\n",
        "    participant User\n",
        "    participant ClientApp\n",
        "    participant EventHandler\n",
        "    participant OpenAI\n",
        "    participant FileSearch\n",
        "    participant CodeInterpreter\n",
        "    participant FunctionCalling\n",
        "\n",
        "    User->>ClientApp: Enter query\n",
        "    ClientApp->>EventHandler: Trigger Event (tool_call)\n",
        "    EventHandler->>OpenAI: Send tool call type and arguments\n",
        "\n",
        "    activate OpenAI\n",
        "    OpenAI->>OpenAI: Parse and analyze query\n",
        "    alt File Search\n",
        "        OpenAI->>FileSearch: Direct file search\n",
        "        activate FileSearch\n",
        "        FileSearch-->>OpenAI: Return relevant information\n",
        "        deactivate FileSearch\n",
        "    else Code Interpreter\n",
        "        OpenAI->>CodeInterpreter: Direct code execution\n",
        "        activate CodeInterpreter\n",
        "        CodeInterpreter-->>OpenAI: Return code output\n",
        "        deactivate CodeInterpreter\n",
        "    else Function Calling\n",
        "        OpenAI->>FunctionCalling: Identify function and arguments\n",
        "        activate FunctionCalling\n",
        "        FunctionCalling-->>OpenAI: Return function call requirements\n",
        "        deactivate FunctionCalling\n",
        "    end\n",
        "    OpenAI-->>EventHandler: Return adjusted parameters or tool requirements\n",
        "    deactivate OpenAI\n",
        "\n",
        "    alt File Search\n",
        "        EventHandler->>ClientApp: Display relevant information\n",
        "    else Code Interpreter\n",
        "        EventHandler->>ClientApp: Display code output or handle generated files\n",
        "    else Function Calling\n",
        "        EventHandler->>FunctionCalling: Execute function with arguments\n",
        "        activate FunctionCalling\n",
        "        FunctionCalling-->>EventHandler: Return function output\n",
        "        deactivate FunctionCalling\n",
        "        EventHandler->>OpenAI: Submit function output\n",
        "        activate OpenAI\n",
        "        OpenAI-->>EventHandler: Return response based on function output\n",
        "        deactivate OpenAI\n",
        "        EventHandler->>ClientApp: Display response\n",
        "    end\n",
        "\n",
        "    ClientApp->>User: Present results or response\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
