{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c090265c-992a-42ad-97c0-a0cb4b70d0ac",
   "metadata": {},
   "source": [
    "## Understanding Post-Training Quantization\r\n",
    "\r\n",
    "Post-Training Quantization is a powerful technique designed to reduce the size and computational complexity of deep learning models, making it particularly advantageous for deployment in resource-constrained environments such as mobile devices and embedded systems.\r\n",
    "\r\n",
    "This process involves taking a pre-trained floating-point model and converting its weights and activations to lower-precision formats‚Äîtypically 8-bit integers‚Äîwhile striving to retain as much accuracy as possible.\r\n",
    "\r\n",
    "### Workflow of Post-Training Quantization:\r\n",
    "\r\n",
    "**Pre-trained Model** ‚Üí **Observer** ‚Üí **Calibration (using unlabeled data)** ‚Üí **Quantized Model**\r\n",
    "\r\n",
    "### Detailed Explanation:\r\n",
    "\r\n",
    "1. **Pre-trained Model:** This is a model that has been trained using standard floating-point precision, usually FP32 (32-bit floating point).\r\n",
    "\r\n",
    "2. **Observers:** During the quantization process, observers are integrated into the model. These observers monitor the ranges and distributions of data (both weights and activations) during inference. Their role is crucial as they provide insights that help determine the scaling factors necessary for precision reduction.\r\n",
    "\r\n",
    "3. **Calibration:** In this step, unlabeled data is input into the model to facilitate calibration. The process analyzes the model‚Äôs output ranges to identify the appropriate scale and zero-point needed for the conversion to lower precision.\r\n",
    "\r\n",
    "4. **Quantized Model:** Ultimately, the floating-point model is transformed into a quantized model, typically using int8 (8-bit integer). This conversion leads to a significant reduction in the model's memory footprint and computational demands.\r\n",
    "\r\n",
    "By following these steps, the model is optimized for deployment in environments where memory and processing power are limited, all while aiming to minimize accuracy loss.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22bd3957-99b2-4d3c-9018-f977efce8e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b24e11-4154-4b15-9ebe-cd5e5928ddda",
   "metadata": {},
   "source": [
    "### Loading the Dataset\r\n",
    "\r\n",
    "In this section, we focus on preparing the dataset crucial for training our neural network. We utilize the **MNIST dataset**, a renowned collection of handwritten digits that serves as an ideal benchmark for image classification tasks. \r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "üîç **Reproducibility**: By setting a manual seed, we ensure that our results remain consistent across different runs.\r\n",
    "\r\n",
    "üìä **Data Transformations**: The dataset undergoes a series of transformations to:\r\n",
    "- Convert the images into tensor format\r\n",
    "- Normalize them, which is vital for enhancing the training process and improving model performance.\r\n",
    "\r\n",
    "üöÄ **Data Loaders**: We create data loaders for both the training and testing datasets, facilitating efficient mini-batch processing:\r\n",
    "- **Mini-batch processing** allows our model to learn from smaller subsets of data, speeding up training and improving generalization.\r\n",
    "\r\n",
    "üíª **GPU Configuration**: Lastly, we configure our environment to leverage GPU resources if available, optimizing computational efficiency.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "This foundational step sets us up for the subsequent stages of model training and quantization, enabling us to effectively implement Post-Training Quantization on our trained model.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d35a060f-c80d-4896-9a0c-4c1bc47e5c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = torch.manual_seed(433)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9621d56f-e932-409f-853d-cbd958fc7322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9912422/9912422 [00:01<00:00, 5556356.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28881/28881 [00:00<00:00, 400398.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1648877/1648877 [00:00<00:00, 3320136.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4542/4542 [00:00<00:00, 4554274.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # converting to tensors\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # performing normalization on the data which is optimal in ML or DL\n",
    "])\n",
    "\n",
    "# we would be using the MNIST dataset\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# creating batch norm\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=True)\n",
    "\n",
    "# trying to leverage my baby GPU hahahaha ;)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88781bb7-b846-43cc-9c89-e9ec7ddc6c85",
   "metadata": {},
   "source": [
    "### Simple Neural Network\r\n",
    "\r\n",
    "In this section, we define a **Simple Neural Network** architecture tailored for classifying handwritten digits from the MNIST dataset. This neural network will serve as the foundation for our subsequent quantization process.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "üß† **Neural Network Architecture**:\r\n",
    "- **Input Layer**: Accepts images reshaped to a flat vector of size 28x28 (i.e., 784).\r\n",
    "- **Hidden Layers**: \r\n",
    "  - **First Hidden Layer**: 50 neurons\r\n",
    "  - **Second Hidden Layer**: 80 neurons\r\n",
    "  - **Third Hidden Layer**: 30 neurons\r\n",
    "- **Output Layer**: Outputs 10 classes, corresponding to the digits 0-9.\r\n",
    "\r\n",
    "üîó **Activation Function**: \r\n",
    "- **ReLU (Rectified Linear Unit)** is employed between layers to introduce non-linearity, enhancing the model's ability to learn complex patterns.\r\n",
    "\r\n",
    "üì¶ **Forward Pass**: \r\n",
    "- The input image is flattened, and data flows through the layers, applying the activation function at each hidden layer, culminating in the output layer which predicts the digit class.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "By constructing this neural network, we prepare to train it on the MNIST dataset, laying the groundwork for effective quantization and deployment in resource-constrained environments.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e986788-c7e3-4687-8747-10a71a40d6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, hidden_layer_1 = 50,hidden_layer_2 = 80, hidden_layer_3 = 30):\n",
    "        super(NeuralNetwork,self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, hidden_layer_1)\n",
    "        self.linear2 = nn.Linear(hidden_layer_1, hidden_layer_2)\n",
    "        self.linear3 = nn.Linear(hidden_layer_2, hidden_layer_3)\n",
    "        self.linear4 = nn.Linear(hidden_layer_3, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self,img):\n",
    "        x = img.view(-1, 28*28)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.relu(self.linear3(x))\n",
    "        x = self.linear4(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbea275-b5eb-4029-8293-c8b33681a1f1",
   "metadata": {},
   "source": [
    "### Model Training\r\n",
    "\r\n",
    "In this section, we implement the training routine for our neural network model. This crucial step allows the model to learn from the MNIST dataset and adjust its weights to improve accuracy.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "üîß **Training Parameters**:\r\n",
    "- **Optimizer**: We utilize the **Adam optimizer** for efficient weight updates.\r\n",
    "- **Loss Function**: The **CrossEntropyLoss** is chosen since this is a multi-class classification problem, suitable for our digit recognition task.\r\n",
    "\r\n",
    "üìà **Training Process**:\r\n",
    "- The training occurs over a specified number of **epochs**, where each epoch consists of several iterations over batches of data.\r\n",
    "- **Loss Calculation**: During each iteration, the model computes the loss, which quantifies how well it performs. The average loss for the epoch is tracked and displayed dynamically.\r\n",
    "\r\n",
    "üîÑ **Iterative Updates**:\r\n",
    "- The model's parameters are updated using backpropagation after each batch, allowing it to learn from its mistakes and improve over time.\r\n",
    "\r\n",
    "üîç **Progress Monitoring**:\r\n",
    "- A progress bar (using **tqdm**) provides real-time feedback on the training status, displaying the current epoch and average loss.\r\n",
    "\r\n",
    "By effectively training the model, we lay the groundwork for accurate digit classification, setting the stage for the subsequent quantization process.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16e147cd-0de2-4375-9686-8d22aefee94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, epochs = None, total_iterations_limit = None):\n",
    "    # optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_function = nn.CrossEntropyLoss() # since this is a classification problem.\n",
    "\n",
    "    total_iterations = 0  # Keep track of how many total iterations we've done\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        loss_sum = 0  # Sum of all the losses to calculate the average loss\n",
    "        num_iterations = 0  # Keep track of the iterations in this epoch\n",
    "        data_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
    "\n",
    "        if total_iterations_limit is not None:\n",
    "            data_iterator.total = total_iterations_limit\n",
    "        for data in data_iterator:\n",
    "            num_iterations += 1\n",
    "            total_iterations += 1\n",
    "            x, y = data # 'data' is a batch (x, y), where x is the input (image), and y is the label (digit)\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x.view(-1, 28*28))\n",
    "            loss = loss_function(output, y)\n",
    "            loss_sum += loss.item()\n",
    "            avg_loss = loss_sum / num_iterations\n",
    "            data_iterator.set_postfix(loss=avg_loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # If a total iteration limit is set, stop training once the limit is reached\n",
    "            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n",
    "                return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60057b8-3357-476f-a494-491684113376",
   "metadata": {},
   "source": [
    "### Model Size and Loading\r\n",
    "\r\n",
    "In this section, we focus on understanding the memory footprint of our trained model and managing its persistence.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "üóÇÔ∏è **Model Size Estimation**:\r\n",
    "- We define a function to calculate the size of the model in kilobytes (KB). This gives us insights into the model's complexity and how it might perform in constrained environments.\r\n",
    "- The size is determined by saving the model's state dictionary temporarily and checking the file size.\r\n",
    "\r\n",
    "üíæ **Model Persistence**:\r\n",
    "- The model is saved to disk under the filename **`simpleNN_ptq.pt`**.\r\n",
    "- **Loading Mechanism**: Before training the model, we check if a saved version already exists. If so, we load it from disk to avoid redundant training, ensuring efficient resource utilization.\r\n",
    "\r\n",
    "üîÑ **Training and Saving**:\r\n",
    "- If no pre-existing model is found, the training process commences, followed by saving the trained model for future use. This workflow allows for easy model reuse without the need for retraining, which is particularly beneficial in production settings.\r\n",
    "\r\n",
    "By efficiently managing the model's size and implementing a robust loading mechanism, we prepare for the next steps in the post-training quantization process.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c46c5b0f-bf32-49df-8ebe-40941a6907d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:58<00:00, 101.70it/s, loss=0.135]\n"
     ]
    }
   ],
   "source": [
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp_delme.p\")\n",
    "    print('Size (KB):', os.path.getsize(\"temp_delme.p\")/1e3)\n",
    "    os.remove('temp_delme.p')\n",
    "\n",
    "MODEL_FILENAME = 'simpleNN_ptq.pt'\n",
    "\n",
    "if Path(MODEL_FILENAME).exists():\n",
    "    model.load_state_dict(torch.load(MODEL_FILENAME))\n",
    "    print('Loaded model from disk')\n",
    "else:\n",
    "    train(train_loader, model, epochs=1)\n",
    "    # Save the model to disk\n",
    "    torch.save(model.state_dict(), MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51099455-0543-446e-84f5-5a324d73a43b",
   "metadata": {},
   "source": [
    "### Time to Test Our Neural Network\r\n",
    "\r\n",
    "In this segment, we evaluate the performance of our trained neural network by measuring its accuracy on the test dataset.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "üîç **Testing Mechanism**:\r\n",
    "- We define a function to conduct the testing phase, where the model's predictions are compared against the true labels from the test dataset.\r\n",
    "- The model is set to **evaluation mode**, disabling gradient calculations to optimize performance during inference.\r\n",
    "\r\n",
    "üìä **Accuracy Calculation**:\r\n",
    "- As we iterate through the test data, we keep track of the number of correct predictions versus the total predictions made.\r\n",
    "- For each prediction, if the model's output matches the actual label, we increment our **correct** counter.\r\n",
    "\r\n",
    "üîÑ **Iterative Process**:\r\n",
    "- We also provide a mechanism to limit the number of iterations during testing, allowing for quick checks without the need to evaluate the entire dataset when desired.\r\n",
    "\r\n",
    "‚úÖ **Results Presentation**:\r\n",
    "- Finally, the accuracy of the model is computed and printed, offering insights into its performance and readiness for deployment.\r\n",
    "\r\n",
    "This testing phase is crucial in understanding how well our model generalizes to unseen data, guiding us in subsequent optimization and quantization steps.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b01a44a8-4cd1-4bdc-9fec-cdedc75d6f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, total_iterations):\n",
    "    correct,total, iterations = 0,0,0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader, desc='Testing'):\n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(x.view(-1, 784))\n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == y[idx]:\n",
    "                    correct +=1\n",
    "                total +=1\n",
    "            iterations += 1\n",
    "            if total_iterations is not None and iterations >= total_iterations:\n",
    "                break\n",
    "    print(f'Accuracy: {round(correct/total, 8)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27c7c3a-2687-4574-aa8b-c2e06939f5c3",
   "metadata": {},
   "source": [
    "### Evaluating Model Size and Accuracy Before Quantization\r\n",
    "\r\n",
    "Before we proceed with the quantization process, it‚Äôs essential to assess our model's current performance and resource footprint.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "üìè **Model Size**:\r\n",
    "- We begin by examining the **weights** of the model's first layer to understand its structure.\r\n",
    "- The size of the model is reported, helping us gauge the potential benefits of quantization in reducing memory requirements.\r\n",
    "\r\n",
    "üí° **Weights Overview**:\r\n",
    "- The weights matrix is printed to provide a clear view of the parameters the model has learned during training. Additionally, we check the data type of the weights to confirm that they are in floating-point format.\r\n",
    "\r\n",
    "üß™ **Model Accuracy**:\r\n",
    "- Following the size evaluation, we check the model's accuracy on the test dataset. This metric is crucial as it indicates how well the model generalizes to unseen data.\r\n",
    "- By evaluating the model before quantization, we establish a baseline for comparing post-quantization performance.\r\n",
    "\r\n",
    "üìà **Results Summary**:\r\n",
    "- The accuracy result highlights the model's effectiveness, ensuring that any changes made during quantization will be closely monitored against this benchmark.\r\n",
    "\r\n",
    "This step is vital in understanding the initial state of our model, setting the stage for the enhancements that quantization can bring.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5a0b8c9-af3b-4dde-b100-13217698e792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights before quantization\n",
      "Parameter containing:\n",
      "tensor([[ 2.2669e-02,  2.1956e-02, -2.6446e-03,  ...,  3.9206e-03,\n",
      "          3.7860e-02,  4.6699e-03],\n",
      "        [ 9.6753e-05,  3.8221e-02,  1.9658e-02,  ..., -6.3200e-03,\n",
      "          2.2407e-02, -1.6425e-02],\n",
      "        [ 5.9448e-02,  4.3217e-03,  1.6101e-02,  ..., -7.5925e-03,\n",
      "          7.5968e-03,  2.9488e-02],\n",
      "        ...,\n",
      "        [-1.2917e-02,  1.4802e-02,  6.9440e-03,  ...,  3.6022e-04,\n",
      "         -9.9758e-03,  9.7989e-03],\n",
      "        [-3.5000e-03,  4.5107e-02,  1.5952e-02,  ...,  3.1508e-02,\n",
      "          2.7726e-02,  3.5913e-02],\n",
      "        [-3.3904e-02,  1.7777e-02, -3.4840e-03,  ...,  2.9134e-03,\n",
      "         -1.7864e-03,  1.2128e-02]], requires_grad=True)\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Print the weights matrix of the model before quantization\n",
    "print('Weights before quantization')\n",
    "print(model.linear1.weight) # for the 1st layer. \n",
    "print(model.linear1.weight.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "381ce708-10e4-41c2-a1c3-e1594c0e5ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model before quantization\n",
      "Size (KB): 187.364\n"
     ]
    }
   ],
   "source": [
    "print('Size of the model before quantization')\n",
    "print_size_of_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be51f0ef-641a-4f3d-a929-a06b28b190e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model before quantization: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:03<00:00, 291.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## we also want to check the accuracy of our model \n",
    "print(f'Accuracy of the model before quantization: ')\n",
    "test(model,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c6245a-6e36-4960-b4db-8fe39f439c4d",
   "metadata": {},
   "source": [
    "### Time to Quantize üòâ\r\n",
    "\r\n",
    "As we step into the quantization phase, we begin by creating a **copy of our existing model** to facilitate a smooth transition into lower-precision computations without altering the original architecture.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "üîÑ **Introducing Quantization**:\r\n",
    "- We define a new class, `QuantizeNeuralNetwork`, which inherits from `nn.Module`. This class is designed to incorporate quantization directly into the model structure.\r\n",
    "- By leveraging **QuantStub** and **DeQuantStub**, we seamlessly integrate quantization and dequantization processes into our model's forward pass.\r\n",
    "\r\n",
    "üõ†Ô∏è **Model Architecture**:\r\n",
    "- The architecture remains largely consistent with our previous neural network, featuring:\r\n",
    "  - **Input Layer:** 28x28 input flattened to a single vector.\r\n",
    "  - **Hidden Layers:** Three fully connected layers with ReLU activations.\r\n",
    "  - **Output Layer:** Final layer projecting to 10 classes for digit classification.\r\n",
    "\r\n",
    "üìâ **Precision Optimization**:\r\n",
    "- The use of quantization allows us to transform the floating-point operations into lower-precision computations, which can significantly reduce both the memory footprint and computational load.\r\n",
    "- This step prepares the model for efficient deployment, especially in resource-constrained environments like mobile devices.\r\n",
    "\r\n",
    "With our quantization model defined, we are now poised to further explore the benefits it brings to our neural network's performance and efficiency!\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15f05c55-1114-426c-9763-fc0f9a0f9ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We make a copy of that same model: \n",
    "class QuantizeNeuralNetwork(nn.Module):\n",
    "    def __init__(self, hidden_layer_1 = 50,hidden_layer_2 = 80, hidden_layer_3 = 30):\n",
    "        super(QuantizeNeuralNetwork,self).__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.linear1 = nn.Linear(28*28, hidden_layer_1)\n",
    "        self.linear2 = nn.Linear(hidden_layer_1, hidden_layer_2)\n",
    "        self.linear3 = nn.Linear(hidden_layer_2, hidden_layer_3)\n",
    "        self.linear4 = nn.Linear(hidden_layer_3, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "        \n",
    "    def forward(self,img):\n",
    "        x = img.view(-1, 28*28)\n",
    "        x = self.quant(x)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.relu(self.linear3(x))\n",
    "        x = self.linear4(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "quant_model = QuantizeNeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f528346e-24b1-4eaf-966a-db0b13ec7a15",
   "metadata": {},
   "source": [
    "#### Quantized Model Inference üåü\r\n",
    "\r\n",
    "In this phase, we take a pivotal step by **transferring the weights** from our original model to the quantized version, ensuring that we leverage the knowledge gained from previous training without the need for retraining. \r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "üîÑ **Weight Transfer**:\r\n",
    "- The quantized model, `quant_model`, is initialized by loading the state dictionary from the original model. This allows us to maintain the learned parameters and avoid redundant training.\r\n",
    "\r\n",
    "üí° **Inference Mode**:\r\n",
    "- By setting `quant_model.eval()`, we switch to inference mode, which disables dropout layers and other training-specific behaviors, ensuring accurate predictions during evaluation.\r\n",
    "\r\n",
    "üîç **Quantization Configuration**:\r\n",
    "- We assign the default quantization configuration using `quant_model.qconfig = torch.ao.quantization.default_qconfig` to prepare the model for quantization.\r\n",
    "- The `prepare` method inserts observers into the model, allowing us to collect statistics about the activations during inference.\r\n",
    "\r\n",
    "üìä **Layer Statistics**:\r\n",
    "- After testing the quantized model, we can examine the statistics of each layer, which provides insights into the range of activations processed through the network.\r\n",
    "- This information is invaluable for understanding the behavior of the model and can guide adjustments in quantization strategies.\r\n",
    "\r\n",
    "With the quantized model in place and its statistics reviewed, we are now equipped to assess the benefits of quantization in terms of performance and efficiency!\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b2ea4ec-473d-4c95-b654-7d6b34e96fbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'quant_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m quant_model\u001b[38;5;241m.\u001b[39mload_state_dict(model\u001b[38;5;241m.\u001b[39mstate_dict())\n\u001b[0;32m      2\u001b[0m quant_model\u001b[38;5;241m.\u001b[39meval() \u001b[38;5;66;03m## we are not training but foing inferencing \u001b[39;00m\n\u001b[0;32m      4\u001b[0m quant_model\u001b[38;5;241m.\u001b[39mqconfig \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mao\u001b[38;5;241m.\u001b[39mquantization\u001b[38;5;241m.\u001b[39mdefault_qconfig\n",
      "\u001b[1;31mNameError\u001b[0m: name 'quant_model' is not defined"
     ]
    }
   ],
   "source": [
    "quant_model.load_state_dict(model.state_dict())\n",
    "quant_model.eval() ## we are not training but foing inferencing \n",
    "\n",
    "quant_model.qconfig = torch.ao.quantization.default_qconfig\n",
    "quant_model = torch.ao.quantization.prepare(quant_model) # Insert observers\n",
    "quant_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d12b14fb-e9a7-470d-ba4f-e40e81a35faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:02<00:00, 385.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9646\n",
      "Check statistics of the various layers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantizeNeuralNetwork(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=-0.4242129623889923, max_val=2.821486711502075)\n",
       "  )\n",
       "  (linear1): Linear(\n",
       "    in_features=784, out_features=50, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-58.53604507446289, max_val=43.8294563293457)\n",
       "  )\n",
       "  (linear2): Linear(\n",
       "    in_features=50, out_features=80, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-40.140647888183594, max_val=35.24177551269531)\n",
       "  )\n",
       "  (linear3): Linear(\n",
       "    in_features=80, out_features=30, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-36.355369567871094, max_val=43.074493408203125)\n",
       "  )\n",
       "  (linear4): Linear(\n",
       "    in_features=30, out_features=10, bias=True\n",
       "    (activation_post_process): MinMaxObserver(min_val=-36.072540283203125, max_val=22.88129234313965)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(quant_model,None)\n",
    "print(f'Check statistics of the various layers')\n",
    "quant_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0df630-425a-4ae9-aafc-e1c15ef84ef0",
   "metadata": {},
   "source": [
    "I think this is beautiful as we can see this values of this tensors. It gives us an Idea for how to go about each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c7a335-e26c-4872-bd61-0de9eda33f4b",
   "metadata": {},
   "source": [
    "### Quantization of the Neural Network üéâ\r\n",
    "\r\n",
    "As we move forward with quantization, we utilize the statistics gathered from the observers to optimize our model. This transformation is crucial for reducing the model's memory footprint and enhancing inference speed.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "üîç **Layer Statistics Post-Quantization**:\r\n",
    "- The quantized model reveals each layer's unique **scale** and **zero point**, essential parameters for transforming floating-point values into quantized integers. These statistics give us insights into how each layer processes data, ensuring optimal performance post-quantization.\r\n",
    "\r\n",
    "### Weights Representation\r\n",
    "- After quantization, we examine the weight matrix of the first layer:\r\n",
    "  - **Quantized Weights**: The weights are represented in integer format, showcasing the quantization's effect on the values.\r\n",
    "  \r\n",
    "- **Original vs. Dequantized Weights**:\r\n",
    "  - Comparing the original floating-point weights with their quantized counterparts gives us a deeper understanding of how quantization affects weight precision.\r\n",
    "  - The dequantized weights demonstrate how effectively the quantized representation retains the original values, providing a glimpse into potential performance during inference.\r\n",
    "\r\n",
    "This process not only optimizes our model but also helps us ensure that we maintain a balance between performance and efficiency. Let‚Äôs continue to explore how quantization can impact the overall performance of our neural network!\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29ff3da3-c6c6-46b9-9153-e923ffb4c34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check statistics of the various layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantizeNeuralNetwork(\n",
       "  (quant): Quantize(scale=tensor([0.0256]), zero_point=tensor([17]), dtype=torch.quint8)\n",
       "  (linear1): QuantizedLinear(in_features=784, out_features=50, scale=0.8060275912284851, zero_point=73, qscheme=torch.per_tensor_affine)\n",
       "  (linear2): QuantizedLinear(in_features=50, out_features=80, scale=0.5935623645782471, zero_point=68, qscheme=torch.per_tensor_affine)\n",
       "  (linear3): QuantizedLinear(in_features=80, out_features=30, scale=0.625432014465332, zero_point=58, qscheme=torch.per_tensor_affine)\n",
       "  (linear4): QuantizedLinear(in_features=30, out_features=10, scale=0.464203417301178, zero_point=78, qscheme=torch.per_tensor_affine)\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_model = torch.ao.quantization.convert(quant_model)\n",
    "print(f'Check statistics of the various layers')\n",
    "quant_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4650b39-fd99-4b00-8903-16235684981b",
   "metadata": {},
   "source": [
    "So for each layer it has it's own Scale and zero point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "331e2954-09f2-4abe-afdc-8f1042aba7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after quantization\n",
      "tensor([[ 4,  4,  0,  ...,  1,  6,  1],\n",
      "        [ 0,  6,  3,  ..., -1,  4, -3],\n",
      "        [10,  1,  3,  ..., -1,  1,  5],\n",
      "        ...,\n",
      "        [-2,  2,  1,  ...,  0, -2,  2],\n",
      "        [-1,  7,  3,  ...,  5,  5,  6],\n",
      "        [-6,  3, -1,  ...,  0,  0,  2]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "# Print the weights matrix of the model after quantization\n",
    "print('Weights after quantization')\n",
    "print(torch.int_repr(quant_model.linear1.weight()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "babcd50e-55c5-40b5-97be-b557ba7f8a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weights: \n",
      "Parameter containing:\n",
      "tensor([[ 2.2669e-02,  2.1956e-02, -2.6446e-03,  ...,  3.9206e-03,\n",
      "          3.7860e-02,  4.6699e-03],\n",
      "        [ 9.6753e-05,  3.8221e-02,  1.9658e-02,  ..., -6.3200e-03,\n",
      "          2.2407e-02, -1.6425e-02],\n",
      "        [ 5.9448e-02,  4.3217e-03,  1.6101e-02,  ..., -7.5925e-03,\n",
      "          7.5968e-03,  2.9488e-02],\n",
      "        ...,\n",
      "        [-1.2917e-02,  1.4802e-02,  6.9440e-03,  ...,  3.6022e-04,\n",
      "         -9.9758e-03,  9.7989e-03],\n",
      "        [-3.5000e-03,  4.5107e-02,  1.5952e-02,  ...,  3.1508e-02,\n",
      "          2.7726e-02,  3.5913e-02],\n",
      "        [-3.3904e-02,  1.7777e-02, -3.4840e-03,  ...,  2.9134e-03,\n",
      "         -1.7864e-03,  1.2128e-02]], requires_grad=True)\n",
      "\n",
      "Dequantized weights: \n",
      "tensor([[ 0.0245,  0.0245,  0.0000,  ...,  0.0061,  0.0368,  0.0061],\n",
      "        [ 0.0000,  0.0368,  0.0184,  ..., -0.0061,  0.0245, -0.0184],\n",
      "        [ 0.0613,  0.0061,  0.0184,  ..., -0.0061,  0.0061,  0.0306],\n",
      "        ...,\n",
      "        [-0.0123,  0.0123,  0.0061,  ...,  0.0000, -0.0123,  0.0123],\n",
      "        [-0.0061,  0.0429,  0.0184,  ...,  0.0306,  0.0306,  0.0368],\n",
      "        [-0.0368,  0.0184, -0.0061,  ...,  0.0000,  0.0000,  0.0123]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Original weights: ')\n",
    "print(model.linear1.weight)\n",
    "print('')\n",
    "print(f'Dequantized weights: ')\n",
    "print(torch.dequantize(quant_model.linear1.weight()))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108df3be-822a-4bb2-b85c-5f9e1647f196",
   "metadata": {},
   "source": [
    "#### Lets compare Unquantized and Quantized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9409872c-ae25-4c98-8975-d0cb8c6b9148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model after quantization\n",
      "Size (KB): 52.77\n",
      "Testing the model after quantization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:02<00:00, 403.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Size of the model after quantization')\n",
    "print_size_of_model(quant_model)\n",
    "print('Testing the model after quantization')\n",
    "test(quant_model,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e91029-539d-4927-85dd-7a79e8a42a59",
   "metadata": {},
   "source": [
    "The size has gone down from Size (KB): 187.364 to Size (KB): 52.77 and accuracy has gone from 0.9646 to 0.9626. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e22ca1-eb53-403e-b6bb-6c3b3587bccf",
   "metadata": {},
   "source": [
    "## üìä Comparing Unquantized and Quantized Models\r\n",
    "\r\n",
    "### Size Reduction\r\n",
    "- **Unquantized Model Size**: \r\n",
    "  - **Size (KB)**: 187.364\r\n",
    "- **Quantized Model Size**: \r\n",
    "  - **Size (KB)**: 52.77\r\n",
    "\r\n",
    "### Accuracy Evaluation\r\n",
    "- **Unquantized Model Accuracy**: \r\n",
    "  - **Accuracy**: 0.9646\r\n",
    "- **Quantized Model Accuracy**: \r\n",
    "  - **Accuracy**: 0.9626\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Summary of Findings\r\n",
    "The quantization process has led to a significant reduction in model size from **187.364 KB** to **52.77 KB**. Although there is a slight decrease in accuracy from **0.9646** to **0.9626**, the trade-off between model efficiency and accuracy is evident.\r\n",
    "\r\n",
    "### Conclusion\r\n",
    "Quantization proves to be a powerful technique in optimizing neural networks for deployment, allowing for smaller models that maintain a high level of accuracy. This transformation enhances inference speed and reduces resource consumption, making it an essential step in practical machine learning applications.\r\n",
    "\r\n",
    "Let‚Äôs keep pushing the boundaries of what our models can achieve!\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
