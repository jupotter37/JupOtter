{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "275024d2",
   "metadata": {},
   "source": [
    "# Can we use perplexity as a metric to detect AI generated text?\n",
    "# With language detection\n",
    "\n",
    "### Why this notebook?\n",
    "With the rise of Generative AI in education, there is a growing need for teachers to check whether a text is written by a human or an AI. A possible metric to detect AI generated text is perplexity. In this notebook we will compare 5 AI generated texts with 5 human generated texts, calculate their perplexity and compare results.\n",
    "\n",
    "### TL;DR\n",
    "Yes, this works. (For the small set of given texts that is.)\n",
    "\n",
    "### Contents\n",
    "0. Install packages\n",
    "1. Settings\n",
    "2. Getting perplexity for 10 openAI generated texts\n",
    "3. Getting the perplexity for 10 human written texts\n",
    "4. Comparing results and conclusion\n",
    "\n",
    "### Sources\n",
    "The English human written text is from two articles:\n",
    "- https://www.economist.com/europe/2023/06/09/ukraines-assault-in-zaporizhia-may-be-the-focus-of-its-offensive\n",
    "- https://www.bbc.com/news/world-latin-america-65864158\n",
    "\n",
    "The Dutch human written text is from these two sources:\n",
    "- https://www.nu.nl/binnenland/6267525/hitteprotocol-rijkswaterstaat-geldt-ook-maandag-behalve-in-het-noorden.html\n",
    "- https://nos.nl/artikel/2478434-het-oekraiense-tegenoffensief-is-begonnen-hoe-staat-het-ervoor\n",
    "\n",
    "The AI generated text is from OpenAI GPT-3.5 using their API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c53dc23",
   "metadata": {},
   "source": [
    "## Calculating perplexity\n",
    "\n",
    "Perplexity is a metric that is used to measure the quality of language models. It is reversely related to the entropy of a model. \n",
    "\n",
    "\n",
    "- blog: https://medium.com/@priyankads/perplexity-of-language-models-41160427ed72\n",
    "- wiki: https://en.wikipedia.org/wiki/Perplexity\n",
    "- o'reilly: https://www.oreilly.com/library/view/natural-language-processing/9781787285101/ch21s05.html (to do)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b19a75",
   "metadata": {},
   "source": [
    "## 0. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f80563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install openai\n",
    "#pip install transformers\n",
    "#pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3eee16",
   "metadata": {},
   "source": [
    "## 1. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60d37515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set API keys\n",
    "import os\n",
    "import config\n",
    "os.environ['OPENAI_API_KEY'] = config.openai_key # replace with your own API-key, but don't keep it in your source code :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b902029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OpenAI settings\n",
    "import openai  # for calling the OpenAI API\n",
    "\n",
    "# models\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\" #choose your embeddings model\n",
    "GPT_MODEL = \"gpt-3.5-turbo\" #choose the generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ff6f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c65e987246ba45f88242ee36ef97a79b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a37918d61f644bd93b88d9858d4d8aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Huggingface Transformers settings (to do perplexity)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c1caf9",
   "metadata": {},
   "source": [
    "## 2. Getting perplexity for 10 openAI generated texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b21d9ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wat is een perceptron en wat is een multi layered perceptron?\\n',\n",
       " 'Wat is het verschil tussen machine leren en diep leren?\\n',\n",
       " 'Let het backpropagatie algoritme uit.\\n',\n",
       " 'Wie was Geoff Hinton en wat was zijn rol in de ontwikkeling van neurale netwerken?\\n',\n",
       " 'Hoe kunnen we onderscheid maken tussen een door een KI gegenereerde tekst en een door een mens geschreven tekst?\\n',\n",
       " 'Beschrijf het verschil tussen menselijke neuronen en kunstmatige neuronen\\n',\n",
       " 'Op welke manier zal generatieve KI de wereld gaan veranderen?\\n',\n",
       " 'Wat was er zo belangrijk aan het taalmodel BERT in het vak NLP.\\n',\n",
       " \"Leg uit waarom Meta's LLaMA beter is dan GPT3.5\\n\",\n",
       " 'Wat zijn Gaussian Mixtures?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_list = []\n",
    "with open ('prompts_nl.txt', mode='r') as prompts: #change to 'prompts_nl.txt' for Dutch prompts\n",
    "    for prompt in prompts: #for each row do the following\n",
    "        #query = prompt.split()\n",
    "        query_list.append(prompt)\n",
    "query_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49b99211",
   "metadata": {},
   "outputs": [],
   "source": [
    "oa_result_list =[] \n",
    "for i in range(len(query_list)):\n",
    "    query = query_list[i]\n",
    " \n",
    "    response = openai.ChatCompletion.create(\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': ''},\n",
    "            {'role': 'user', 'content': query},\n",
    "        ],\n",
    "        model=GPT_MODEL,\n",
    "        temperature=0\n",
    "        \n",
    "        , #temperature is a parameter about the entropy of the model (the higher, the greater variation in results)\n",
    "        )\n",
    "        \n",
    "    #print(response['choices'][0]['message']['content'])\n",
    "    openai_result = response['choices'][0]['message']['content']\n",
    "    oa_result_list.append(openai_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac16e98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Een perceptron is een eenvoudig neuraal netwerkmodel dat wordt gebruikt voor binair classificatietaken. Het bestaat uit een enkele laag van inputneuronen die verbonden zijn met een enkel uitvoerneuron. Elk inputneuron heeft een gewicht en een bias, en de uitvoer van het perceptron wordt berekend door de gewogen som van de inputwaarden te nemen en deze door een activatiefunctie te laten gaan.\n",
      "\n",
      "Een multi-layered perceptron (MLP) is een uitbreiding van het perceptron-model. Het bestaat uit meerdere lagen van neuronen, waarbij elke laag verbonden is met de volgende laag. De eerste laag wordt de inputlaag genoemd, de laatste laag wordt de outputlaag genoemd, en de tussenliggende lagen worden verborgen lagen genoemd. Elk neuron in de verborgen lagen en de outputlaag heeft gewichten en biases, en de uitvoer van het MLP wordt berekend door de gewogen som van de inputwaarden van elk neuron te nemen en deze door een activatiefunctie te laten gaan.\n",
      "\n",
      "Het MLP-model is in staat om complexere patronen te leren en kan worden gebruikt voor zowel classificatie- als regressietaken. Het wordt vaak gebruikt in machine learning en deep learning toepassingen vanwege zijn vermogen om niet-lineaire relaties te modelleren.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Machine learning en deep learning zijn beide subvelden van kunstmatige intelligentie, maar er zijn enkele belangrijke verschillen tussen de twee.\n",
      "\n",
      "Machine learning is een benadering waarbij computersystemen worden getraind om taken uit te voeren zonder expliciet geprogrammeerd te worden. In plaats daarvan worden algoritmen gebruikt om patronen en structuren in gegevens te identificeren en te leren van ervaring. Machine learning-algoritmen kunnen worden onderverdeeld in verschillende categorieën, zoals supervised learning (waarbij gelabelde gegevens worden gebruikt om een model te trainen), unsupervised learning (waarbij ongelabelde gegevens worden gebruikt om patronen te ontdekken) en reinforcement learning (waarbij een agent leert door interactie met een omgeving en beloningen te ontvangen).\n",
      "\n",
      "Deep learning is een specifieke vorm van machine learning die gebruikmaakt van kunstmatige neurale netwerken met meerdere lagen. Deze neurale netwerken zijn geïnspireerd op de structuur en werking van het menselijk brein. Deep learning-algoritmen kunnen automatisch abstracte functies en representaties leren van ruwe gegevens, zonder dat er handmatige functies hoeven te worden geëxtraheerd. Hierdoor kunnen ze complexe taken uitvoeren, zoals beeld- en spraakherkenning.\n",
      "\n",
      "Kortom, machine learning is een bredere term die verwijst naar het trainen van computersystemen om taken uit te voeren, terwijl deep learning een specifieke techniek is binnen machine learning die gebruikmaakt van kunstmatige neurale netwerken met meerdere lagen.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Het backpropagatie-algoritme is een algoritme dat wordt gebruikt om de gewichten van een neuraal netwerk aan te passen tijdens het trainingsproces. Het algoritme maakt gebruik van de kettingregel van de afgeleide om de fout van het netwerk terug te propageren van de uitvoerlaag naar de invoerlaag, waarbij de gewichten worden aangepast op basis van de afgeleide van de foutfunctie ten opzichte van de gewichten.\n",
      "\n",
      "Het backpropagatie-algoritme kan worden samengevat in de volgende stappen:\n",
      "\n",
      "1. Initialisatie: Initialiseer de gewichten van het netwerk met willekeurige waarden.\n",
      "\n",
      "2. Voorwaartse propagatie: Voer de invoerwaarden door het netwerk en bereken de uitvoerwaarden van elke laag door de activatiefunctie toe te passen op de gewogen som van de invoerwaarden.\n",
      "\n",
      "3. Foutberekening: Bereken de fout van het netwerk door het verschil te nemen tussen de verwachte uitvoer en de werkelijke uitvoer.\n",
      "\n",
      "4. Achterwaartse propagatie: Bereken de afgeleide van de foutfunctie ten opzichte van de gewichten door de kettingregel toe te passen. Begin bij de uitvoerlaag en werk terug naar de invoerlaag, waarbij de afgeleide van de activatiefunctie wordt vermenigvuldigd met de afgeleide van de gewogen som van de invoerwaarden.\n",
      "\n",
      "5. Gewichtsaanpassing: Pas de gewichten aan door de afgeleide van de foutfunctie ten opzichte van de gewichten te vermenigvuldigen met een leersnelheid en deze af te trekken van de huidige gewichten.\n",
      "\n",
      "6. Herhaal stappen 2-5 voor een vooraf bepaald aantal iteraties of totdat de fout onder een bepaalde drempelwaarde ligt.\n",
      "\n",
      "Het backpropagatie-algoritme wordt vaak gebruikt in combinatie met een optimalisatie-algoritme zoals gradient descent om de gewichten van het netwerk efficiënt aan te passen. Door de fout terug te propageren en de gewichten aan te passen op basis van de afgeleide van de foutfunctie, kan het netwerk leren om de gewenste uitvoer te produceren voor een gegeven invoer.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Geoff Hinton is een gerenommeerde Canadese computerwetenschapper en pionier op het gebied van neurale netwerken en machine learning. Hij wordt beschouwd als een van de grondleggers van het veld van deep learning.\n",
      "\n",
      "Hinton begon zijn onderzoek naar neurale netwerken in de jaren 80 en was een van de eersten die het potentieel van deze technologie erkende. Hij ontwikkelde baanbrekende algoritmen en modellen die de basis legden voor moderne neurale netwerken. Een van zijn belangrijkste bijdragen was de ontwikkeling van de backpropagation-algoritme, dat wordt gebruikt om de gewichten van een neuraal netwerk aan te passen en zo het leren mogelijk te maken.\n",
      "\n",
      "Hinton heeft ook bijgedragen aan de ontwikkeling van convolutionele neurale netwerken (CNN's), die tegenwoordig veel worden gebruikt in beeldherkenningstoepassingen. Hij heeft ook gewerkt aan de ontwikkeling van recurrente neurale netwerken (RNN's), die worden gebruikt voor taken zoals spraakherkenning en natuurlijke taalverwerking.\n",
      "\n",
      "Hinton's werk heeft een enorme impact gehad op het veld van machine learning en heeft geleid tot doorbraken op verschillende gebieden, waaronder computer vision, spraakherkenning en natuurlijke taalverwerking. Hij heeft talloze prijzen en onderscheidingen ontvangen voor zijn bijdragen aan het veld en wordt algemeen erkend als een van de meest invloedrijke figuren in de ontwikkeling van neurale netwerken.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Het kan soms lastig zijn om onderscheid te maken tussen een door een KI gegenereerde tekst en een door een mens geschreven tekst, omdat KI-systemen steeds geavanceerder worden in het nabootsen van menselijke schrijfstijlen. Er zijn echter enkele aanwijzingen die kunnen helpen bij het identificeren van een door een KI gegenereerde tekst:\n",
      "\n",
      "1. Consistentie: KI-systemen zijn vaak consistent in hun schrijfstijl en tonen weinig variatie. Menselijke schrijvers daarentegen kunnen verschillende stijlen en toon gebruiken, afhankelijk van het onderwerp en de context.\n",
      "\n",
      "2. Creativiteit: Hoewel KI-systemen in staat zijn om tekst te genereren, missen ze vaak de creativiteit en originaliteit die menselijke schrijvers kunnen tonen. Als de tekst te perfect of te voorspelbaar lijkt, kan dit wijzen op een door een KI gegenereerde tekst.\n",
      "\n",
      "3. Begrip van context: Menselijke schrijvers hebben vaak een dieper begrip van de context waarin ze schrijven, terwijl KI-systemen vaak afhankelijk zijn van patronen en statistieken. Als de tekst geen rekening houdt met de specifieke context of geen relevante details bevat, kan dit wijzen op een door een KI gegenereerde tekst.\n",
      "\n",
      "4. Fouten en inconsistenties: Menselijke schrijvers maken vaak fouten, zoals grammaticale fouten, spelfouten of inconsistenties in de tekst. KI-systemen zijn over het algemeen nauwkeuriger en maken minder fouten.\n",
      "\n",
      "Het is belangrijk op te merken dat deze aanwijzingen niet altijd betrouwbaar zijn en dat KI-systemen steeds beter worden in het nabootsen van menselijke schrijfstijlen. Het kan dus moeilijk zijn om met zekerheid te zeggen of een tekst door een KI of een mens is geschreven.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Menselijke neuronen zijn de cellen in het zenuwstelsel van mensen die verantwoordelijk zijn voor het doorgeven van elektrische signalen. Ze bestaan uit een cellichaam, dendrieten die signalen ontvangen, een axon dat signalen doorgeeft en synapsen waarmee ze communiceren met andere neuronen. Menselijke neuronen zijn zeer complex en kunnen verschillende soorten signalen verwerken, zoals sensorische input, motorische output en cognitieve functies.\n",
      "\n",
      "Aan de andere kant zijn kunstmatige neuronen onderdeel van kunstmatige neurale netwerken, die worden gebruikt in kunstmatige intelligentie en machine learning. Kunstmatige neuronen zijn ontworpen om de basisfuncties van menselijke neuronen na te bootsen, maar ze zijn veel eenvoudiger van structuur. Ze bestaan meestal uit een inputlaag, een gewichtsfunctie, een activatiefunctie en een outputlaag. Kunstmatige neuronen ontvangen inputsignalen, verwerken deze met behulp van wiskundige operaties en geven vervolgens een outputsignaal.\n",
      "\n",
      "Het belangrijkste verschil tussen menselijke neuronen en kunstmatige neuronen is de complexiteit. Menselijke neuronen hebben een veel grotere variëteit aan functies en kunnen verschillende soorten informatie verwerken. Ze kunnen ook leren en zich aanpassen aan veranderende omstandigheden. Kunstmatige neuronen zijn daarentegen eenvoudiger en hebben een beperktere functionaliteit. Ze zijn ontworpen om specifieke taken uit te voeren en kunnen niet leren of zich aanpassen zoals menselijke neuronen dat kunnen.\n",
      "\n",
      "Daarnaast zijn er ook verschillen in de manier waarop menselijke neuronen en kunstmatige neuronen signalen verwerken. Menselijke neuronen werken op basis van elektrochemische signalen, terwijl kunstmatige neuronen werken met behulp van wiskundige berekeningen. Dit betekent dat menselijke neuronen een hogere mate van parallelle verwerking hebben, terwijl kunstmatige neuronen sequentieel werken.\n",
      "\n",
      "Kortom, menselijke neuronen zijn complexe cellen die verantwoordelijk zijn voor de verwerking van informatie in het menselijk zenuwstelsel, terwijl kunstmatige neuronen eenvoudige eenheden zijn die worden gebruikt in kunstmatige neurale netwerken voor specifieke taken.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Generatieve kunstmatige intelligentie (KI) zal naar verwachting de wereld op verschillende manieren veranderen. Hier zijn enkele mogelijke effecten:\n",
      "\n",
      "1. Creatieve industrieën: Generatieve KI kan worden gebruikt om kunst, muziek, literatuur en andere vormen van creatieve expressie te genereren. Dit kan leiden tot nieuwe vormen van kunst en cultuur, en kan ook de manier waarop we kunst waarderen en consumeren veranderen.\n",
      "\n",
      "2. Personalisatie: Generatieve KI kan worden gebruikt om gepersonaliseerde ervaringen te creëren, zoals gepersonaliseerde advertenties, aanbevelingen en producten. Dit kan leiden tot een meer op maat gemaakte en relevante gebruikerservaring.\n",
      "\n",
      "3. Automatisering: Generatieve KI kan worden gebruikt om taken en processen te automatiseren die voorheen door mensen werden uitgevoerd. Dit kan leiden tot efficiëntere workflows en kostenbesparingen in verschillende sectoren, zoals productie, logistiek en klantenservice.\n",
      "\n",
      "4. Onderzoek en ontwikkeling: Generatieve KI kan worden gebruikt om nieuwe ideeën en concepten te genereren, waardoor het proces van onderzoek en ontwikkeling wordt versneld. Dit kan leiden tot nieuwe ontdekkingen en innovaties in verschillende wetenschappelijke en technologische domeinen.\n",
      "\n",
      "5. Ethiek en privacy: Generatieve KI kan ook uitdagingen met zich meebrengen op het gebied van ethiek en privacy. Het genereren van inhoud en het nemen van beslissingen door machines kan leiden tot vragen over verantwoordelijkheid, transparantie en controle. Het is belangrijk om deze kwesties aan te pakken om ervoor te zorgen dat generatieve KI op een ethische en verantwoorde manier wordt gebruikt.\n",
      "\n",
      "Over het algemeen zal generatieve KI de wereld waarschijnlijk veranderen door nieuwe mogelijkheden te bieden op het gebied van creativiteit, personalisatie, automatisering, onderzoek en ontwikkeling. Het is belangrijk om deze technologie op een verantwoorde en ethische manier te benutten om de voordelen ervan te maximaliseren en de mogelijke nadelen te beperken.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Het taalmodel BERT (Bidirectional Encoder Representations from Transformers) heeft een belangrijke bijdrage geleverd aan het vakgebied Natural Language Processing (NLP). Hier zijn een paar redenen waarom BERT zo belangrijk is:\n",
      "\n",
      "1. Contextuele representatie: BERT maakt gebruik van transformer-architectuur om contextuele representaties van woorden te leren. Dit betekent dat het model begrijpt hoe de betekenis van een woord kan variëren afhankelijk van de context waarin het wordt gebruikt. Dit is een belangrijke stap voorwaarts in het begrijpen van natuurlijke taal.\n",
      "\n",
      "2. Pre-training en fine-tuning: BERT wordt eerst pre-getraind op grote hoeveelheden ongelabelde tekst, waarbij het leert om taalkundige taken uit te voeren, zoals het voorspellen van ontbrekende woorden in een zin. Vervolgens kan het model worden fijnafgestemd op specifieke NLP-taken, zoals tekstclassificatie of vraag-antwoordmodellering. Dit maakt het mogelijk om met relatief weinig gelabelde gegevens goede prestaties te behalen op verschillende NLP-taken.\n",
      "\n",
      "3. State-of-the-art prestaties: BERT heeft state-of-the-art prestaties geleverd op verschillende NLP-taken, zoals vraag-antwoordmodellering, tekstclassificatie en named entity recognition. Het heeft laten zien dat het beter in staat is om de complexiteit van natuurlijke taal te begrijpen en te modelleren dan eerdere modellen.\n",
      "\n",
      "4. Open-source en beschikbaarheid: BERT is open-source en beschikbaar gesteld door Google. Dit heeft geleid tot een brede acceptatie en adoptie van het model in de NLP-gemeenschap. Hierdoor kunnen onderzoekers en ontwikkelaars het model gebruiken en verbeteren voor verschillende toepassingen.\n",
      "\n",
      "Al met al heeft BERT een belangrijke bijdrage geleverd aan het verbeteren van de prestaties van NLP-modellen en heeft het de weg vrijgemaakt voor verdere ontwikkelingen in het begrijpen en genereren van natuurlijke taal.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Er zijn verschillende redenen waarom Meta's LLaMA beter kan zijn dan GPT3.5:\n",
      "\n",
      "1. Schaalbaarheid: LLaMA is ontworpen om op grote schaal te werken en kan gemakkelijk worden opgeschaald om aan de behoeften van verschillende toepassingen te voldoen. Dit betekent dat het beter kan omgaan met grote hoeveelheden gegevens en complexe taken.\n",
      "\n",
      "2. Begrip van context: LLaMA is getraind op een breed scala aan gegevensbronnen, waaronder tekst, afbeeldingen en video's. Hierdoor kan het een dieper begrip van de context hebben en beter reageren op complexe vragen en verzoeken.\n",
      "\n",
      "3. Veelzijdigheid: LLaMA is ontworpen om verschillende soorten taken aan te kunnen, zoals het beantwoorden van vragen, het genereren van tekst, het vertalen van talen en nog veel meer. Dit maakt het een veelzijdigere tool dan GPT3.5, dat voornamelijk gericht is op het genereren van tekst.\n",
      "\n",
      "4. Betere controle: LLaMA biedt meer controle over de gegenereerde output. Gebruikers kunnen specifieke instructies geven en bepalen hoe de gegenereerde tekst eruit moet zien. Dit helpt bij het voorkomen van ongewenste of onnauwkeurige resultaten.\n",
      "\n",
      "5. Verbeterde ethiek en veiligheid: Meta heeft aangegeven dat ze zich inzetten voor het waarborgen van ethiek en veiligheid in hun AI-systemen. Ze hebben richtlijnen en beleid opgesteld om misbruik en schadelijk gedrag te voorkomen. Dit kan een belangrijk voordeel zijn ten opzichte van GPT3.5, waarbij de controle over de gegenereerde output beperkter is.\n",
      "\n",
      "Het is belangrijk op te merken dat de vergelijking tussen LLaMA en GPT3.5 afhangt van de specifieke toepassing en de behoeften van de gebruiker. Beide systemen hebben unieke eigenschappen en kunnen verschillende voordelen bieden, afhankelijk van de context waarin ze worden gebruikt.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Gaussian Mixtures, ook wel Gaussian Mixture Models (GMM) genoemd, zijn statistische modellen die worden gebruikt om complexe datadistributies te modelleren. Ze zijn gebaseerd op de veronderstelling dat de geobserveerde gegevenspunten afkomstig zijn van een combinatie van meerdere Gaussische (normale) verdelingen.\n",
      "\n",
      "Een Gaussian Mixture Model bestaat uit een combinatie van K Gaussische verdelingen, waarbij K het aantal componenten of clusters in het model vertegenwoordigt. Elk van deze componenten wordt gekarakteriseerd door zijn eigen set parameters, waaronder het gemiddelde en de variantie.\n",
      "\n",
      "Het doel van het modelleren van gegevens met een Gaussian Mixture Model is om de onderliggende structuur van de gegevens te begrijpen en te kunnen gebruiken voor verschillende toepassingen, zoals clustering, classificatie en het genereren van nieuwe datapunten.\n",
      "\n",
      "Het schatten van de parameters van een Gaussian Mixture Model kan worden gedaan met behulp van verschillende algoritmen, zoals de Expectation-Maximization (EM) algoritme. Dit algoritme schat de parameters door iteratief de verwachte waarden van de verborgen variabelen te berekenen en vervolgens de maximale waarschijnlijkheidsschatting van de parameters te vinden.\n",
      "\n",
      "Gaussian Mixtures zijn flexibele modellen die kunnen worden gebruikt om verschillende soorten datadistributies te modelleren, inclusief die met meerdere pieken, overlappende clusters en complexe correlaties tussen variabelen. Ze worden veel gebruikt in verschillende domeinen, zoals patroonherkenning, computer vision, spraakherkenning en financiële analyse.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(len(oa_result_list))\n",
    "for i in range(len(oa_result_list)):\n",
    "    print(oa_result_list[i])\n",
    "    print(100*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a297c7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NL', 'NL', 'NL', 'NL', 'NL', 'NL', 'NL', 'NL', 'NL', 'NL']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#detect the language \n",
    "import deepl \n",
    "import config #this is my own config file. You'll need to create your own. See notebook 2.x\n",
    "my_api_key = config.deepl #get your own API Key!\n",
    "translator = deepl.Translator(my_api_key) \n",
    "\n",
    "language_list=[]\n",
    "for i in range(len(oa_result_list)):\n",
    "    text = oa_result_list[i]\n",
    "    result = translator.translate_text(text, target_lang=\"EN-US\")\n",
    "    #print(result.detected_source_lang)\n",
    "    language_list.append(result.detected_source_lang)\n",
    "language_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0bca84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NL_AI',\n",
       " 'NL_AI',\n",
       " 'NL_AI',\n",
       " 'NL_AI',\n",
       " 'NL_AI',\n",
       " 'NL_AI',\n",
       " 'NL_AI',\n",
       " 'NL_AI',\n",
       " 'NL_AI',\n",
       " 'NL_AI']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combining language with AI\n",
    "lang_ai_list=[]\n",
    "for i in range(len(language_list)):\n",
    "    new_str = language_list[i]+\"_AI\"\n",
    "    lang_ai_list.append(new_str)\n",
    "lang_ai_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8cb2d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "095c7edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(60.8375, grad_fn=<ExpBackward0>)\n",
      "tensor(83.1194, grad_fn=<ExpBackward0>)\n",
      "tensor(30.4905, grad_fn=<ExpBackward0>)\n",
      "tensor(63.7182, grad_fn=<ExpBackward0>)\n",
      "tensor(46.2665, grad_fn=<ExpBackward0>)\n",
      "tensor(46.4662, grad_fn=<ExpBackward0>)\n",
      "tensor(57.8467, grad_fn=<ExpBackward0>)\n",
      "tensor(64.7657, grad_fn=<ExpBackward0>)\n",
      "tensor(84.3473, grad_fn=<ExpBackward0>)\n",
      "tensor(101.7445, grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#getting the perplexity tensor for the openai generated text\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "perplexity_list =[]\n",
    "for i in range(len(oa_result_list)):\n",
    "    inputs_text = tokenizer(oa_result_list[i], return_tensors = \"pt\")\n",
    "    loss = model(input_ids = inputs_text[\"input_ids\"], labels = inputs_text[\"input_ids\"]).loss\n",
    "    ppl = torch.exp(loss)\n",
    "    print(ppl)\n",
    "    perplexity_list.append(ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73406aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60.8375, 83.1194, 30.4905, 63.7182, 46.2665, 46.4662, 57.8467, 64.7657, 84.3473, 101.7445]\n"
     ]
    }
   ],
   "source": [
    "#converting tensors to string and getting just the values\n",
    "ai_text_perplexity= []\n",
    "for i in range(len(perplexity_list)):\n",
    "    tensor_string=str(perplexity_list[i])\n",
    "    a,b = tensor_string.split(\",\")\n",
    "    tensor, perplexity = a.split(\"(\")\n",
    "    ai_text_perplexity.append(float(perplexity))\n",
    "print(ai_text_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6ac99e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{60.8375: 'NL_AI',\n",
       " 83.1194: 'NL_AI',\n",
       " 30.4905: 'NL_AI',\n",
       " 63.7182: 'NL_AI',\n",
       " 46.2665: 'NL_AI',\n",
       " 46.4662: 'NL_AI',\n",
       " 57.8467: 'NL_AI',\n",
       " 64.7657: 'NL_AI',\n",
       " 84.3473: 'NL_AI',\n",
       " 101.7445: 'NL_AI'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combining two lists to one dictionary using 'zip'\n",
    "ai_perplexity_dict = {key: value for key, value in zip(ai_text_perplexity, lang_ai_list)}\n",
    "ai_perplexity_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee42485",
   "metadata": {},
   "source": [
    "## 3. Getting the perplexity for 10 human written texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fff8aa45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Het hitteprotocol van Rijkswaterstaat geldt ook maandag. Dat houdt in dat weggebruikers die met pech langs de weg staan, direct worden geholpen. Vandaag is het protocol in heel Nederland van kracht, maandag in alle provincies behalve Friesland, Groningen en Drenthe.\\n',\n",
       " 'Het hitteprotocol geldt maandag van 10.00 tot 22.00 uur. Ook zaterdag was het in de meeste provincies van kracht wegens de tropische temperaturen. Zodra er een grote kans is dat het 30 graden of warmer wordt, zet Rijkswaterstaat het protocol in. Rijkswaterstaat wil niet dat gestrande weggebruikers bij hoge temperaturen langdurig in de zon moeten wachten.\\n',\n",
       " 'Daarom is het de bedoeling dat mensen bij autopech zo snel mogelijk door een berger naar een plek met voorzieningen worden gebracht, zoals een tankstation. Weggebruikers krijgen het advies goed voorbereid op weg te gaan en alert te zijn bij hitte. Zo adviseert Rijkswaterstaat genoeg drinkwater en een paraplu tegen de zon in de auto te leggen.\\n',\n",
       " 'Hoe effectief zal de Russische luchtmacht zijn? En hoe effectief de Oekraïense luchtverdediging? Volgens oud-commandant Landstrijdkrachten Mart de Kruif zijn dat de cruciale kwesties die zullen bepalen hoe het Oekraïense tegenoffensief, dat de afgelopen dagen echt op gang is gekomen, zal uitpakken. \"Het is een onderbuikgevoel, maar ik denk dat we over een week meer weten.\"\\n',\n",
       " 'De Oekraïners zijn deze week op meerdere plaatsen langs het 800 tot 900 kilometer lange front in de aanval. De strijd is intensief en bloedig. Onduidelijk is nog wat het hoofddoel van het offensief wordt, ook voor De Kruif. \"De regio Zaporizja is de meest logische optie. Dan kun je naar de Zee van Azov doorstoten en verbreek je de landverbinding tussen Rusland en de Krim. Maar je kunt ook heel veel terrein terugwinnen in Loehansk en dan naar het noorden toe richting Charkiv.\"\\n',\n",
       " 'De uitkomst van oorlogen en veldslagen wordt vaak bepaald door reserves, omdat dat vaak ervaren en goed getrainde militairen zijn. Rusland zal zijn reserves daar willen inzetten waar het gevaar van een doorbraak het grootst is, denkt De Kruif. Oekraïne zal er alles aan doen om de Russen te misleiden, zodat de kans dat Oekraïners Russische reserves tegenkomen zo klein mogelijk is, of dat die reserves te laat komen.\\n',\n",
       " 'In verband daarmee is de kromming die de frontlijn laat zien in het voordeel van Oekraïne en in het nadeel van de Russen. \"Oekraïne kan eenheden relatief snel over eigen gebied van noord naar zuid en van zuid naar noord brengen. Rusland heeft het probleem van de buitenlijn. Die moeten vanwege de kromming in het front bijna altijd buitenom.\"\\n',\n",
       " 'Op dit moment wordt op verschillende plekken gevochten. \"Het is nog niet duidelijk waar de hoofdaanval komt\", zegt De Kruif. \"Je dwingt zo Rusland om voortdurend de keuze te maken: waar ga ik de reserves opstellen of zelf inzetten om een doorbraak tegen te gaan?\"\\n',\n",
       " 'Een van de grootste zorgen aan Oekraïense kant is dat als ze die doorbraak hebben, de Russische luchtmacht daar massaal op gaat ingrijpen. Dus Oekraïne zal daar ook het gros van zijn luchtverdediging moeten positioneren, inclusief de jachtvliegtuigen die de Russische luchtmacht daar moet weghouden. \"Je moet al je middelen op het goede moment op de juiste plaats hebben staan. Dat doe je niet zomaar, het is een kwestie van dagen en weken om dat te plannen.\"\\n',\n",
       " 'Op sociale media gaan intussen beelden rond van door het Westen geleverde tanks, die door de Russen worden vernietigd. De Kruif zag de beelden ook. \"Ook een Leopard 2 kan stuk. Al eerder hebben Koerden in Noord-Syrië een Turkse Leopard 2 uitgeschakeld. Een voertuig is alleen maar effectief als je het goed gebruikt, op de juiste wijze het terrein gebruikt, anders ben je altijd kwetsbaar.\"']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_list = []\n",
    "with open ('humanwritten_nl.txt', mode='r') as prompts: #change to 'humanwritten_nl.txt' if you prefer Dutch\n",
    "    for prompt in prompts: #for each row do the following\n",
    "        #query = prompt.split()\n",
    "        query_list.append(prompt)\n",
    "query_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ab9bac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Het hitteprotocol van Rijkswaterstaat geldt ook maandag. Dat houdt in dat weggebruikers die met pech langs de weg staan, direct worden geholpen. Vandaag is het protocol in heel Nederland van kracht, maandag in alle provincies behalve Friesland, Groningen en Drenthe.\\n',\n",
       " 'Het hitteprotocol geldt maandag van 10.00 tot 22.00 uur. Ook zaterdag was het in de meeste provincies van kracht wegens de tropische temperaturen. Zodra er een grote kans is dat het 30 graden of warmer wordt, zet Rijkswaterstaat het protocol in. Rijkswaterstaat wil niet dat gestrande weggebruikers bij hoge temperaturen langdurig in de zon moeten wachten.\\n',\n",
       " 'Daarom is het de bedoeling dat mensen bij autopech zo snel mogelijk door een berger naar een plek met voorzieningen worden gebracht, zoals een tankstation. Weggebruikers krijgen het advies goed voorbereid op weg te gaan en alert te zijn bij hitte. Zo adviseert Rijkswaterstaat genoeg drinkwater en een paraplu tegen de zon in de auto te leggen.\\n',\n",
       " 'Hoe effectief zal de Russische luchtmacht zijn? En hoe effectief de Oekraïense luchtverdediging? Volgens oud-commandant Landstrijdkrachten Mart de Kruif zijn dat de cruciale kwesties die zullen bepalen hoe het Oekraïense tegenoffensief, dat de afgelopen dagen echt op gang is gekomen, zal uitpakken. \"Het is een onderbuikgevoel, maar ik denk dat we over een week meer weten.\"\\n',\n",
       " 'De Oekraïners zijn deze week op meerdere plaatsen langs het 800 tot 900 kilometer lange front in de aanval. De strijd is intensief en bloedig. Onduidelijk is nog wat het hoofddoel van het offensief wordt, ook voor De Kruif. \"De regio Zaporizja is de meest logische optie. Dan kun je naar de Zee van Azov doorstoten en verbreek je de landverbinding tussen Rusland en de Krim. Maar je kunt ook heel veel terrein terugwinnen in Loehansk en dan naar het noorden toe richting Charkiv.\"\\n',\n",
       " 'De uitkomst van oorlogen en veldslagen wordt vaak bepaald door reserves, omdat dat vaak ervaren en goed getrainde militairen zijn. Rusland zal zijn reserves daar willen inzetten waar het gevaar van een doorbraak het grootst is, denkt De Kruif. Oekraïne zal er alles aan doen om de Russen te misleiden, zodat de kans dat Oekraïners Russische reserves tegenkomen zo klein mogelijk is, of dat die reserves te laat komen.\\n',\n",
       " 'In verband daarmee is de kromming die de frontlijn laat zien in het voordeel van Oekraïne en in het nadeel van de Russen. \"Oekraïne kan eenheden relatief snel over eigen gebied van noord naar zuid en van zuid naar noord brengen. Rusland heeft het probleem van de buitenlijn. Die moeten vanwege de kromming in het front bijna altijd buitenom.\"\\n',\n",
       " 'Op dit moment wordt op verschillende plekken gevochten. \"Het is nog niet duidelijk waar de hoofdaanval komt\", zegt De Kruif. \"Je dwingt zo Rusland om voortdurend de keuze te maken: waar ga ik de reserves opstellen of zelf inzetten om een doorbraak tegen te gaan?\"\\n',\n",
       " 'Een van de grootste zorgen aan Oekraïense kant is dat als ze die doorbraak hebben, de Russische luchtmacht daar massaal op gaat ingrijpen. Dus Oekraïne zal daar ook het gros van zijn luchtverdediging moeten positioneren, inclusief de jachtvliegtuigen die de Russische luchtmacht daar moet weghouden. \"Je moet al je middelen op het goede moment op de juiste plaats hebben staan. Dat doe je niet zomaar, het is een kwestie van dagen en weken om dat te plannen.\"\\n',\n",
       " 'Op sociale media gaan intussen beelden rond van door het Westen geleverde tanks, die door de Russen worden vernietigd. De Kruif zag de beelden ook. \"Ook een Leopard 2 kan stuk. Al eerder hebben Koerden in Noord-Syrië een Turkse Leopard 2 uitgeschakeld. Een voertuig is alleen maar effectief als je het goed gebruikt, op de juiste wijze het terrein gebruikt, anders ben je altijd kwetsbaar.\"']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37401a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NL', 'NL', 'NL', 'NL', 'NL', 'NL', 'NL', 'NL', 'NL', 'NL']\n"
     ]
    }
   ],
   "source": [
    "#detect the language \n",
    "import deepl\n",
    "import config #this is my own config file. You'll need to create your own. See notebook 2.x\n",
    "my_api_key = config.deepl #get your own API Key!\n",
    "translator = deepl.Translator(my_api_key) \n",
    "\n",
    "\n",
    "hw_language_list = []\n",
    "for i in range(len(query_list)):\n",
    "    text = query_list[i]\n",
    "    result = translator.translate_text(text, target_lang=\"EN-US\")\n",
    "    #print(result.detected_source_lang)\n",
    "    hw_language_list.append(result.detected_source_lang)\n",
    "\n",
    "print(hw_language_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d9a7be30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NL', 'NL', 'NL', 'NL', 'NL', 'NL', 'NL', 'NL', 'NL', 'NL']\n"
     ]
    }
   ],
   "source": [
    "print(hw_language_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8805a32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NL_human', 'NL_human', 'NL_human', 'NL_human', 'NL_human', 'NL_human', 'NL_human', 'NL_human', 'NL_human', 'NL_human']\n"
     ]
    }
   ],
   "source": [
    "#combining language with AI\n",
    "\n",
    "lang_human_list=[]\n",
    "for i in range(len(hw_language_list)):\n",
    "    new_str = hw_language_list[i]+\"_human\"\n",
    "    lang_human_list.append(new_str)\n",
    "\n",
    "print(lang_human_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4a7d8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NL_human', 'NL_human', 'NL_human', 'NL_human', 'NL_human', 'NL_human', 'NL_human', 'NL_human', 'NL_human', 'NL_human']\n"
     ]
    }
   ],
   "source": [
    "print(lang_human_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a3600798",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m         human_text_perplexity\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(perplexity))\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m human_text_perplexity\n\u001b[0;32m---> 14\u001b[0m \u001b[43mget_human_text_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[43], line 10\u001b[0m, in \u001b[0;36mget_human_text_perplexity\u001b[0;34m(query_list, tokenizer, model)\u001b[0m\n\u001b[1;32m      8\u001b[0m ppl \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(loss)\n\u001b[1;32m      9\u001b[0m tensor_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(lang_human_list[i])\n\u001b[0;32m---> 10\u001b[0m a,b \u001b[38;5;241m=\u001b[39m tensor_string\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m tensor, perplexity \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m human_text_perplexity\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(perplexity))\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_human_text_perplexity(query_list, tokenizer, model):\n",
    "    human_text_perplexity= []\n",
    "    for i in range(len(query_list)):\n",
    "        inputs_text = tokenizer(query_list[i], return_tensors=\"pt\")\n",
    "        loss = model(input_ids=inputs_text[\"input_ids\"], labels=inputs_text[\"input_ids\"]).loss\n",
    "        ppl = torch.exp(loss)\n",
    "        tensor_string=str(lang_human_list[i])\n",
    "        a,b = tensor_string.split(\",\")\n",
    "        tensor, perplexity = a.split(\"(\")\n",
    "        human_text_perplexity.append(float(perplexity))\n",
    "    return human_text_perplexity\n",
    "get_human_text_perplexity(query_list, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae2cd88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b237c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining two lists to one dictionary using 'zip'\n",
    "ai_perplexity_dict = {key: value for key, value in zip(human_text_perplexity, lang_human_list)}\n",
    "ai_perplexity_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa398864",
   "metadata": {},
   "source": [
    "## 4. Comparing results and conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create one list\n",
    "results =ai_text_perplexity+human_text_perplexity\n",
    "print(results)\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd533ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using different colors based on labels\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data\n",
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16 ,17 ,18 ,19 ,20]\n",
    "y = results\n",
    "labels = ['AI', 'AI', 'AI', 'AI', 'AI', 'AI','AI','AI','AI','AI', 'human','human','human','human','human','human','human','human','human','human','human']\n",
    "\n",
    "# Define colors for different labels\n",
    "#colors = {'human': 'red', 'AI': 'blue'}\n",
    "colors = ['red' if value > 20 else 'green' for value in y]\n",
    "\n",
    "# Create scatter plot\n",
    "for i in range(len(x)):\n",
    "    #plt.scatter(x[i], y[i], color=colors[labels[i]])\n",
    "    plt.scatter(x[i], y[i], color=colors)\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Sample no.')\n",
    "plt.ylabel('Perplexity score')\n",
    "plt.title('AI generated text (blue) vs Human written text (red)')\n",
    "#plt.legend('red: human text, blue: AI text')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70f6dc0",
   "metadata": {},
   "source": [
    "In this notebook we've compared 10 AI generated texts with 10 texts written by humans and calculated their perplexity. When we plotted the results we see a difference between AI-generated texts (low perplexity) and human written text (high perplexity). \n",
    "\n",
    "Without doubt, more testing is needed, especially with other types of text, to get more reliable results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c629712",
   "metadata": {},
   "source": [
    "## Testing with chunks of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a382bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2check ='Ik hoop dat deze e-mail u goed bereikt. Ik schrijf u met betrekking tot de deadline van vandaag voor Mini technische portfolio. Het spijt me ten zeerste dat ik niet in staat ben om het op tijd in te leveren. Ik begrijp dat tijdige indiening van het grootste belang is en ik bied mijn oprechte verontschuldigingen aan voor het niet nakomen van de deadline. De reden dat ik de opdracht niet op tijd kan voltooien, is omdat ik niet genoeg tijd had. Ik heb alles in het werk gesteld om het tijdig af te ronden, maar ik ben geconfronteerd met onverwachte omstandigheden die mijn voortgang hebben vertraagd. Ik betreur het dat ik niet eerder contact met u heb opgenomen om deze situatie te bespreken, maar ik hoop dat u begrip kunt opbrengen voor mijn situatie.Daarom wil ik graag vragen of het mogelijk is om een verlenging te krijgen en de opdracht uiterlijk aanstaande zondag in te leveren. Ik verzeker u dat ik er alles aan zal doen om de opdracht volledig en naar behoren af te ronden binnen de extra tijd die ik vraag. Ik begrijp dat dit afhankelijk is van uw goedkeuring en ik ben u zeer dankbaar als u mij deze kans wilt geven.Nogmaals, mijn oprechte excuses voor het niet nakomen van de deadline. Ik waardeer uw begrip en ik hoop dat we een oplossing kunnen vinden waarmee ik de opdracht als nog succesvol kan afronden.Ik kijk uit naar jullie reactie!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139e5f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the perplexity tensor for the openai generated text\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "inputs_text = tokenizer(text2check, return_tensors = \"pt\")\n",
    "loss = model(input_ids = inputs_text[\"input_ids\"], labels = inputs_text[\"input_ids\"]).loss\n",
    "ppl = torch.exp(loss)\n",
    "tensor_string=str(ppl)\n",
    "a,b = tensor_string.split(\",\")\n",
    "tensor, perplexity = a.split(\"(\")\n",
    "ppl = float(perplexity)\n",
    "print(ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1192585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b892554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.append(21)\n",
    "y.append(ppl)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7e811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f3ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_scatter(x, y):\n",
    "    # Define colors for different labels\n",
    "    #colors = {'human': 'red', 'AI': 'blue'}\n",
    "    colors = ['red' if value > 100 else 'green' for value in y]\n",
    "    # Create scatter plot\n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i], y[i])\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.xlabel('Sample no.')\n",
    "    plt.ylabel('Perplexity score')\n",
    "    plt.title('AI generated text (blue) vs Human written text (red)')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb47aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36812ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
