{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\"><img src=\"https://www.dropbox.com/s/roovx0dx8mpm4ep/ECE4076_banner.png?dl=1\"></div>\n",
    "\n",
    "\n",
    "<h1 align=\"center\"> ECE4076/5176 - Week 8 </h1>\n",
    "<h1 align=\"center\"> Logistic Regression </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to have the following packages to work with this notebook\n",
    "- [numpy](https://anaconda.org/anaconda/numpy)\n",
    "- [matplotlib](https://anaconda.org/conda-forge/matplotlib)\n",
    "- [sklearn](https://scikit-learn.org/stable/install.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "np.random.seed(2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Sigmoid function\n",
    "\n",
    "We start by remining ourselves about the Sigmoid function $\\sigma:\\mathbb{R} \\to [0,1]$:\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma(x) = \\frac{1}{1+\\exp(-x)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the function to understand it better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the Sigmoid function\n",
    "x = np.arange(-7, 7, 0.1)\n",
    "sigma_x = sigmoid(x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(x, sigma_x, c='purple')\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('$\\sigma (x)$')\n",
    "\n",
    "# y axis ticks and gridline\n",
    "ax.set_yticks([0.0, 0.5, 1.0])\n",
    "ax.yaxis.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "## Task 1: Derivative of the Sigmoid: \n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "Soon we will see that deep learning requires the knowledge of gradient of functions to operate, so how about grabbing a piece of paper and deriving the form of derivative of the Sigmoid function! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "##  Logistic Loss (aka binary cross entropy)\n",
    "\n",
    "</div>\n",
    "\n",
    "The total loss of the logistic regression is defined as\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{L}_\\text{BCE}(\\boldsymbol{w}) = -\\frac{1}{m}\\sum_{i=1}^m \\bigg\\{\n",
    "    y_i \\log(\\sigma\\big(\\boldsymbol{w}^\\top\\boldsymbol{x}_i)\\big) +\n",
    "    (1 - y_i) \\log(1 - \\sigma\\big(\\boldsymbol{w}^\\top\\boldsymbol{x}_i)\\big)\n",
    "    \\bigg\\}.\n",
    "\\end{align*}\n",
    "\n",
    "For a sample $(\\boldsymbol{x},y)$, the loss is \n",
    "\\begin{align*}\n",
    "    \\ell(y,\\hat{y}) = -y \\log(\\sigma\\big(\\boldsymbol{w}^\\top\\boldsymbol{x})\\big) -\n",
    "    (1 - y) \\log(1 - \\sigma\\big(\\boldsymbol{w}^\\top\\boldsymbol{x})\\big)\n",
    "    \\bigg\\}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_loss(y,y_hat):\n",
    "    if y == 1:\n",
    "        return -np.log(y_hat)\n",
    "    else:\n",
    "        return -np.log(1-y_hat)\n",
    "\n",
    "\n",
    "# Let us study the behavior of the loss with respect to teh Sigmoid function.\n",
    "z = np.arange(-20, 20, 0.1)\n",
    "sigma_z_arr = sigmoid(z)\n",
    "\n",
    "c0 = np.empty_like(sigma_z_arr)\n",
    "c1 = np.empty_like(sigma_z_arr)\n",
    "for cntr, sigma_z in enumerate(sigma_z_arr):\n",
    "    c0[cntr] = sample_loss(0,sigma_z)\n",
    "    c1[cntr] = sample_loss(1,sigma_z)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(sigma_z_arr, c0, linestyle='--', label='$\\ell(\\hat{y},y)$ if $y=0$')\n",
    "ax.plot(sigma_z_arr, c1, label='$\\ell(\\hat{y},y)$ if $y=1$')\n",
    "ax.set_xlim(0.0, 1.0)\n",
    "ax.set_ylim(0.0, 10.0)\n",
    "ax.set_xlabel('$\\hat{y}$')\n",
    "ax.set_ylabel('$\\ell(y, \\hat{y})$')\n",
    "ax.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "## Task 2: Behaviour of the loss \n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "Explain the behaviour of the sample loss based on the graph above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the model\n",
    "\n",
    "First, let us define the total loss (loss for all the samples) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(X,Y,w):\n",
    "    m = Y.size #number of samples\n",
    "    total_loss = 0\n",
    "    for x_i,y_i in zip(X,Y):\n",
    "        y_i_hat = sigmoid(np.dot(x_i,w))\n",
    "        total_loss += sample_loss(y_i,y_i_hat)\n",
    "    \n",
    "    return total_loss/m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn the model, we need to obtain the derivative of the total loss wrt parameters of the model. This can be written as \n",
    "\n",
    "\\begin{align}\n",
    "\\Delta = \\frac{1}{m}\\sum_{i=1}^m \\big(z_i - y_i\\big)\\boldsymbol{x}_i\\;.\n",
    "\\end{align}\n",
    "\n",
    "Here, \n",
    "\\begin{align}\n",
    "   z_i = \\sigma(\\boldsymbol{w}^\\top\\boldsymbol{x}_i)\\;.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_delta(X,Y,w):\n",
    "    m,n = X.shape #m = number of samples, n = dim of samples\n",
    "    delta = np.zeros((n))\n",
    "    for x_i,y_i in zip(X,Y):\n",
    "        z_i = sigmoid(np.dot(x_i,w))\n",
    "        delta += (z_i - y_i)*x_i\n",
    "    \n",
    "    return delta/m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it seems we have everything ready for training our model. We just need to loop over according to the algorithm below.\n",
    "\n",
    "\n",
    "![alt text](logistic_regression_pseudo_code.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating some artificial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_trn, y_trn = make_classification(n_features=2, n_redundant=0, \n",
    "                                   n_informative=2, n_clusters_per_class=1)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.scatter(X_trn[:,0], X_trn[:,1], alpha=0.8, c=y_trn, edgecolor='black')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m,n = X_trn.shape\n",
    "max_num_iter = 100\n",
    "loss_arr = np.empty((max_num_iter))\n",
    "eta = 1e-2\n",
    "w = np.random.randn(n)\n",
    "\n",
    "for iter in range(max_num_iter):\n",
    "    loss_arr[iter] = total_loss(X_trn,y_trn, w) \n",
    "    delta = compute_delta(X_trn,y_trn,w)\n",
    "    w -= eta*delta \n",
    "    if (iter % 10 == 0): \n",
    "        print(f\"iter#{iter:3}| loss = {loss_arr[iter]:6.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "## Task 3: Prediction \n",
    "\n",
    "</div>\n",
    "\n",
    "Once we learn the model, how can we use it to predict the class a query sample $\\boldsymbol{x}_q$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a simple thershjolding method for prediction\n",
    "\\begin{align*}\n",
    "    \\hat{y} = \n",
    "    \\begin{cases}\n",
    "    0 &\\text{if } \\sigma(\\boldsymbol{W}^\\top\\boldsymbol{x}_q) \\leq 0.5\\\\\n",
    "    1 &\\text{otherwise} \\\\ \n",
    "    \\end{cases}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "## Putting everything together\n",
    "    \n",
    "</div> \n",
    "\n",
    "Below, we will put everything nicely together in a class. We also use a function for plotting, which is located below the class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's put everything into a class so we can use things easily\n",
    "\n",
    "class LogisticModel:\n",
    "    def __init__(self, lr=1e-2, max_num_iter=100):\n",
    "        self.lr = lr\n",
    "        self.max_num_iter = max_num_iter\n",
    "        self.w_ = None\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(X):\n",
    "        return 1.0 / (1.0 + np.exp(-np.clip(X, -250, 250))) #taking care of overflowing\n",
    "\n",
    "    def total_loss(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        Z = np.matmul(X,self.w_)\n",
    "        y_hat = self.sigmoid(Z)\n",
    "\n",
    "        loss = -np.log(y_hat[y==1]).sum() - np.log(1 - y_hat[y != 1]).sum() \n",
    "        return loss/m\n",
    "    \n",
    "    def compute_delta(self, X, y):\n",
    "        m, n = X.shape #m = number of samples, n = dim of samples\n",
    "        Z = np.matmul(X,self.w_)\n",
    "        y_hat = self.sigmoid(Z)\n",
    "        delta = np.matmul(X.T,(y_hat - y))\n",
    "\n",
    "        return delta/m\n",
    "\n",
    "\n",
    "        \n",
    "    def predict(self, X_q):\n",
    "        \n",
    "        \"\"\"Return prediction of a logistic model\"\"\"\n",
    "        m = X_q.shape[0]\n",
    "        pred = np.zeros(m)\n",
    "        Z = np.matmul(X_q,self.w_)\n",
    "        y_hat = self.sigmoid(Z)\n",
    "        pred[y_hat >= 0.5] = 1\n",
    "        return pred\n",
    "    \n",
    "    def fit(self, X_trn, y_trn): \n",
    "        m,n = X_trn.shape\n",
    "        loss_arr = np.empty((self.max_num_iter))\n",
    "\n",
    "        self.w_ = np.random.random(n)\n",
    "\n",
    "        print_step = np.ceil(self.max_num_iter/10)\n",
    "        for iter in range(self.max_num_iter):\n",
    "            \n",
    "            loss_arr[iter] = self.total_loss(X_trn,y_trn) \n",
    "            delta = self.compute_delta(X_trn,y_trn)\n",
    "            self.w_ -= eta*delta \n",
    "            if (iter % print_step == 0): \n",
    "                print(f\"iter#{iter:3}| loss = {loss_arr[iter]:6.2f}\")\n",
    "            \n",
    "        return self.w_\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "def plot_decision_regions(X, y, classifier, wormhole_flag=False):\n",
    "\n",
    "    resolution=0.01\n",
    "    # setup marker generator and color map\n",
    "    markers = ('o', 's', '^', 'v', '<')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    x2_min, x2_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    \n",
    "    \n",
    "    X_mesh = np.array([xx1.ravel(), xx2.ravel()]).T\n",
    "    \n",
    "    if wormhole_flag:\n",
    "        X_mesh = Phi(X_mesh)\n",
    "    lab = classifier.predict(X_mesh)\n",
    "    \n",
    "    lab = lab.reshape(xx1.shape)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.contourf(xx1, xx2, lab, alpha=0.3, cmap=cmap)\n",
    "    ax.set_xlim(xx1.min(), xx1.max())\n",
    "    ax.set_ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # plot class examples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        ax.scatter(x=X[y == cl, 0], \n",
    "                   y=X[y == cl, 1],\n",
    "                   alpha=0.8, \n",
    "                   c=colors[idx],\n",
    "                   marker=markers[idx], \n",
    "                   label=f'Class {cl}', \n",
    "                   edgecolor='black')\n",
    "    ax.legend(loc='best')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we use the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple = LogisticModel(lr=0.05,max_num_iter=500)\n",
    "model_simple.fit(X_trn, y_trn)\n",
    "\n",
    "plot_decision_regions(X=X_trn, \n",
    "                      y=y_trn,\n",
    "                      classifier=model_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "## Task 4: Explain the result \n",
    "\n",
    "</div>\n",
    "\n",
    "- Explain what you observed. \n",
    "- Try to run the algorithm several times. \n",
    "- Change the learning rate and observe the results (recall $\\eta \\in (0,1]$)\n",
    "- Change the maximum number of iterations and observe the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    \n",
    "We are now ready to take up a more serious challenge, the two circles data!\n",
    "    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "X_circ, y_circ = make_circles(n_samples=1_000, factor=0.3, noise=0.05, random_state=0)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.scatter(X_circ[:,0], X_circ[:,1], c=y_circ)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "## Task 5: Logistic Model and Two Circle Data \n",
    "\n",
    "</div>\n",
    "\n",
    "- Train a new logistic model using the two circle data.  \n",
    "- Discuss whether it is possible to perfectly classify the data or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_circ = LogisticModel(lr=0.05)\n",
    "#model.w_ = w\n",
    "\n",
    "model_circ.fit(X_circ,y_circ)\n",
    "\n",
    "plot_decision_regions(X=X_circ, y=y_circ, classifier=model_circ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> Wormhole </h1>\n",
    "\n",
    "<img src=\"https://scitechdaily.com/images/Wormhole-Spacetime-Shortcut.jpg\"/>\n",
    "\n",
    "\n",
    "\n",
    "Recall in science fiction, you travel via wormholes? A similar concept in machine learning is when you map your data to higher-dimensional spaces, creating path to categorize them via linear models (eg., logistic regression). Below, we use a deterministic function in the form \n",
    "\n",
    "\\begin{align}\n",
    "\\phi:\n",
    "\\begin{pmatrix}\n",
    "x[0]\\\\\n",
    "x[1]\\\\\n",
    "\\end{pmatrix}\n",
    "\\to \n",
    "\\begin{pmatrix}\n",
    "x[0]\\\\\n",
    "x[1]\\\\\n",
    "x^2[0]\\\\\n",
    "x^2[1]\\\\\n",
    "x[0] \\times x[1]\\\\\n",
    "1\\\\\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "to map our 2D data to 6D. If this form seems curious to you, think about the form of ellipces and their dependence on quadratic forms. Let us create this mapping and apply our logistic model to the resulting data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Phi(X):\n",
    "    X_sq = X**2\n",
    "    X_cross = np.expand_dims(X[:,0]*X[:,1],1)\n",
    "    X_ones = np.ones_like(X_cross)\n",
    "    X_aug = np.concatenate((X,X_sq,X_cross,X_ones), axis=1)\n",
    "    return X_aug \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## Task 6: Logistic Model and High-Dimensional Data\n",
    "\n",
    "</div>\n",
    "\n",
    "Now that we have the mapping, let's apply a logistic model to our data. Ideally, we would like to see the following (figure courtesy \"Machine Learning with PyTorch and Scikit-Learn\" by Raschka et al.).\n",
    "\n",
    "\n",
    "![alt text](wormhole_logistic.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wh, y_wh = Phi(X_circ), y_circ\n",
    "model_wh = LogisticModel(lr=0.3, max_num_iter=2000)\n",
    "model_wh.fit(X_wh,y_wh)\n",
    "plot_decision_regions(X=X_wh, y=y_wh, classifier=model_wh, wormhole_flag=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_hat = model_wh.predict(X_wh)\n",
    "acc = np.sum(y_hat == y_wh)/(y_hat.size)\n",
    "print(f\"accuracy: {acc:.2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. S. Raschka, Y. Liu, and V. Mirjalili, \"Machine Learning with PyTorch and Scikit-Learn\", 2022.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9db45ff850ceb96eaf55f40c6ae920139e36b819849b88af3db42b63a9cc6b3f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
