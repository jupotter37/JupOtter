{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REFERENCES\n",
    "\n",
    "**Information Extraction**\n",
    "- https://www.analyticsvidhya.com/blog/2020/06/nlp-project-information-extraction/\n",
    "- https://medium.com/analytics-vidhya/introduction-to-information-extraction-using-python-and-spacy-858f5d6416ca\n",
    "\n",
    "**Chatbot**\n",
    "- https://medium.com/predict/create-your-chatbot-using-python-nltk-761cd0aeaed3\n",
    "- https://medium.com/swlh/a-chatbot-in-python-using-nltk-938a37a9eacc\n",
    "\n",
    "**Intent**\n",
    "- https://medium.com/walmartglobaltech/joint-intent-classification-and-entity-recognition-for-conversational-commerce-35bf69195176\n",
    "- https://medium.com/analytics-vidhya/machine-learning-intent-classification-221ecded7c74\n",
    "- https://colab.research.google.com/github/deepmipt/dp_notebooks/blob/master/DP_autoFAQ.ipynb (!)\n",
    "- https://towardsdatascience.com/a-brief-introduction-to-intent-classification-96fda6b1f557\n",
    "- https://medium.com/artefact-engineering-and-data-science/nlu-benchmark-for-intent-detection-and-named-entity-recognition-in-call-center-conversations-f58e5b4c8d3d\n",
    "- https://medium.com/iambot/ai-assistance-with-pytext-6308d896566d\n",
    "\n",
    "**NER**\n",
    "· Simple Entities\n",
    "· Composite Entities\n",
    "· Entity Roles\n",
    "· Entity Lists\n",
    "· Regular Expressions\n",
    "· Prebuilt Models\n",
    "- https://github.com/DhruvilKarani/NER-Blog/blob/master/analysis.ipynb\n",
    "- https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da\n",
    "- https://towardsdatascience.com/named-entity-recognition-ner-using-keras-bidirectional-lstm-28cd3f301f54\n",
    "- https://towardsdatascience.com/named-entity-recognition-ner-meeting-industrys-requirement-by-applying-state-of-the-art-deep-698d2b3b4ede\n",
    "- https://towardsdatascience.com/deep-learning-for-named-entity-recognition-3-reusing-a-bidirectional-lstm-cnn-on-clinical-text-e84bd28052df\n",
    "- https://medium.com/@b.terryjack/nlp-pretrained-named-entity-recognition-7caa5cd28d7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "\n",
    "from nltk.stem import wordnet, PorterStemmer # to perform lemmitization\n",
    "from nltk.corpus import stopwords # for stop words\n",
    "from nltk import pos_tag # for parts of speech\n",
    "from nltk import word_tokenize # to create tokens\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer # to perform bow\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # to perform tfidf\n",
    "from sklearn.metrics import pairwise_distances # to perfrom cosine similarity\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from sklearn.datasets import fetch_openml\n",
    "# import stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10256)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stanfordnlp.download('en') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intent + entity type\n",
    "intent_sets = ['greet', 'new', 'update', 'finish', 'query', 'no']\n",
    "# possible data types: tabular, image, and text (?) -- training and testing set for each\n",
    "slot_sets = {\n",
    "    \"task\": [], # regression or classification\n",
    "    \"data_source\": [], # upload, url, or built-in\n",
    "    \"target_variable\": [], # specific name or undefined\n",
    "    \"dataset\": [], # dataset name or filepath\n",
    "    \"delivery\": [], # on-web or email\n",
    "}\n",
    "\n",
    "# from user query sentences\n",
    "constructed_pipeline = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible tasks: tabular classification, tabular regression, image classification, image regression, text classification, and text regression\n",
    "states = ['standby', 'inquire', 'inference', 'running', 'deliver'] # possible states of the CA and AutoML Engine\n",
    "user_slot = {\"method\": None, \"task\": None, \"data_source\": None, \"dataset\": None, \"target\": None, \"delivery\": 'chat'}\n",
    "\n",
    "def intent_classification():\n",
    "    return intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {\n",
    "    'start': [],\n",
    "    'ongoing': [],\n",
    "    'interrupt': [],\n",
    "    'end': [],\n",
    "}\n",
    "\n",
    "# error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generatin data set (may use dictionary based synonym replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple keyword matching?\n",
    "# Rule-based grammar matching\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of available Datasets (built-in or .csv, .txt, .xls, folders with .png or .jpg), Algorithms (just simply ML or DL), and Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normalization(text):\n",
    "#     text=str(text).lower() # text to lower case\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('please')\n",
    "    text = re.sub('[^a-zA-z0-9]', ' ', text) # removing special characters\n",
    "    text = nltk.word_tokenize(text) # word tokenizing\n",
    "    lema = wordnet.WordNetLemmatizer() # intializing lemmatization\n",
    "    tags_list = pos_tag(text, tagset=None) # parts of speech\n",
    "    lema_words = []   # empty list \n",
    "    for token, pos_token in tags_list:\n",
    "        if pos_token.startswith('V'):  # Verb\n",
    "            pos_val = 'v'\n",
    "        elif pos_token.startswith('J'): # Adjective\n",
    "            pos_val = 'a'\n",
    "        elif pos_token.startswith('R'): # Adverb\n",
    "            pos_val = 'r'\n",
    "        else:\n",
    "            pos_val = 'n' # Noun\n",
    "        lema_token = lema.lemmatize(token, pos_val) # performing lemmatization\n",
    "        lema_words.append(lema_token) # appending the lemmatized token into a list\n",
    "    \n",
    "    lema_words = pos_tag(lema_words)\n",
    "    text = [item for item in lema_words if item[0].lower() not in stop_words]\n",
    "    return text # returns the lemmatized tokens as a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = 'Hi! Please build an ML model that can classify 10 digits with MNIST data. Thanks~'\n",
    "\n",
    "GREETING_INPUTS = [\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\", \"morning\", \"afternoon\", \"evening\", \"night\"]\n",
    "CONFIRM_WORDS = [\"yes\", \"yep\", \"okay\", \"ok\", \"sure\", \"certainly\", \"definitely\", \"absolutely\", \"go ahead\", \"cool\", \"right\", \"of course\"]\n",
    "DENY_WORDS = [\"no\", \"nope\", \"na\", \"not yet\", \"not sure\", \"more\", \"not\", \"don't\", \"do not\", \"again\"]\n",
    "END_WORDS = [\"goodbye\", \"bye\", \"see you later\", \"thanks\", \"thank\", \"thank you\"]\n",
    "ML_METHODS = ['machine learning', 'ml', 'machine']\n",
    "DL_METHODS = ['deep learning', 'dl', 'neural network', 'nn', 'neural', 'network', 'deep']\n",
    "CLASSIFICATIONS = ['class', 'classify', 'classification', 'classifier', 'discrete output']\n",
    "REGRESSIONS = ['regress', 'regression', 'regressor', 'continuous output']\n",
    "DATA_SOURCES = ['upload', '<url>', 'this data', 'this dataset', 'my data', 'my dataset']\n",
    "IMAGE_TYPES = ['image', 'picture', 'figure', 'art', 'draw', 'photo', 'photograph', 'portrait', 'painting', 'visual', 'illustration', 'symbol', 'view', 'vision', 'sketch', 'icon']\n",
    "TEXT_TYPES = ['text', 'word', 'message', 'writing', 'script', 'content', 'document', 'passage', 'context', 'essay', 'manuscript', 'paper', 'language', 'letter', 'written', 'write', 'character', 'note', 'darft']\n",
    "TABLE_TYPES = ['structured', 'structure', 'tabular', 'table', 'relation', 'database', 'dataframe', 'frame', 'normal', 'excel', 'csv', 'file', 'summary', 'process']\n",
    "# AVAIL_DATASETS = pd.read_csv('openml_datasets.csv')['name'].apply(lambda x: x.lower()).to_list()\n",
    "AVAIL_DATASETS = tfds.list_builders()\n",
    "TARGET_VAR = ['<name> value', 'predict <name>', 'forecast <name>', 'classify <name>'] # regex ?\n",
    "DELIVERY = ['by email', 'by e-mail', 'email', 'e-mail']\n",
    "\n",
    "GREETING_RESPONSES = [\"Yep, It's nice to see you here! 🙌🏻\", \"Hey~\", \"*nods*\", \"Hi there!\", \"Hello\", \"I am glad! You are talking to me~~~\"]\n",
    "END_RESPONSES = [\"See you then! 🙌🏻\", \"Bye~\", \"Goodluck!\", \"Hope to see you again~\", \"Goodbye!~\", \"Thanks~\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_slot = {\"method\": None, \"task\": None, \"data_source\": None, \"data_type\": None, \"dataset\": None, \"target\": None, \"delivery\": 'chat'}\n",
    "\n",
    "def standby_state(user_message):\n",
    "    text = ''\n",
    "    current_state = 'standby'\n",
    "    global user_slot\n",
    "    \n",
    "    for message in user_message:\n",
    "        if message.lower() in GREETING_INPUTS:\n",
    "            text += random.choice(GREETING_RESPONSES) + '\\n'\n",
    "        elif message.lower() in END_WORDS:\n",
    "            current_state = 'end'\n",
    "            text = random.choice(END_RESPONSES)\n",
    "        elif message.lower() in ML_METHODS:\n",
    "            user_slot['method'] = 'ml'\n",
    "        elif message.lower() in DL_METHODS:\n",
    "            user_slot['method'] = 'dl'\n",
    "        elif message.lower() in IMAGE_TYPES:\n",
    "            user_slot['data_type'] = 'image'\n",
    "        elif message.lower() in TEXT_TYPES:\n",
    "            user_slot['data_type'] = 'text'\n",
    "        elif message.lower() in TABLE_TYPES:\n",
    "            user_slot['data_type'] = 'table'\n",
    "        elif message.lower() in CLASSIFICATIONS:\n",
    "            user_slot['task'] = 'cls'\n",
    "        elif message.lower() in REGRESSIONS:\n",
    "            user_slot['task'] = 'reg'\n",
    "        elif message.lower() in AVAIL_DATASETS:\n",
    "            user_slot['dataset'] = message.lower()\n",
    "            user_slot['data_source'] = 'built_in'\n",
    "            user_slot['target'] = 'label'\n",
    "            tfds.load(message.lower())\n",
    "        elif message.lower() in DATA_SOURCES:\n",
    "            user_slot['data_source'] = 'user_define'\n",
    "            user_slot['dataset'] = 'filepath'\n",
    "        elif message.lower() in DELIVERY:\n",
    "            user_slot['delivery'] = 'email'\n",
    "\n",
    "    \n",
    "    if current_state != 'end':\n",
    "        if user_slot['method'] != None and user_slot['task'] != None and user_slot['data_source'] != None and user_slot['dataset'] != None and user_slot['target'] != None and user_slot['delivery'] != None:\n",
    "            text += f\"All you requested are well received! \\n These are your requirements: {user_slot['method']}, {user_slot['task']}, {user_slot['dataset']}, {user_slot['target']}, and {user_slot['delivery']} \\n Do you want to proceed?\"\n",
    "            current_state = 'await'\n",
    "\n",
    "        else:\n",
    "            text += 'I need more things to complete. Please specify the following list:\\n'\n",
    "            for key, value in user_slot.items():\n",
    "                if value == None:\n",
    "                    text += '- ' + re.sub('_', ' ', key) + '\\n'\n",
    "\n",
    "            current_state = 'active'\n",
    "    else:\n",
    "        current_state = 'standby'\n",
    "        \n",
    "    return text, current_state, user_slot\n",
    "\n",
    "\n",
    "def active_state(user_message):\n",
    "    text = ''\n",
    "    current_state = 'active'\n",
    "    global user_slot\n",
    "\n",
    "    for message in user_message:\n",
    "        if message.lower() in GREETING_INPUTS:\n",
    "            text += random.choice([\"Yep! We've already greeted!\", \"I've already known you~\", \"Please go to the next step!\"]) + '\\n'\n",
    "        elif message.lower() in END_WORDS:\n",
    "            current_state = 'standby'\n",
    "            text = random.choice(END_RESPONSES)\n",
    "        elif message.lower() in ML_METHODS:\n",
    "            user_slot['method'] = 'ml'\n",
    "        elif message.lower() in DL_METHODS:\n",
    "            user_slot['method'] = 'dl'\n",
    "        elif message.lower() in IMAGE_TYPES:\n",
    "            user_slot['data_type'] = 'image'\n",
    "        elif message.lower() in TEXT_TYPES:\n",
    "            user_slot['data_type'] = 'text'\n",
    "        elif message.lower() in TABLE_TYPES:\n",
    "            user_slot['data_type'] = 'table'\n",
    "        elif message.lower() in CLASSIFICATIONS:\n",
    "            user_slot['task'] = 'cls'\n",
    "        elif message.lower() in REGRESSIONS:\n",
    "            user_slot['task'] = 'reg'\n",
    "        elif message.lower() in AVAIL_DATASETS:\n",
    "            user_slot['dataset'] = message.lower()\n",
    "            user_slot['data_source'] = 'built_in'\n",
    "            user_slot['target'] = 'label'\n",
    "            tfds.load(message.lower())\n",
    "        elif message.lower() in DATA_SOURCES:\n",
    "            user_slot['data_source'] = 'user_define'\n",
    "            user_slot['dataset'] = 'filepath'\n",
    "        elif message.lower() in DELIVERY:\n",
    "            user_slot['delivery'] = 'email'\n",
    "    \n",
    "    if current_state != 'standby':\n",
    "        if user_slot['method'] != None and user_slot['task'] != None and user_slot['data_source'] != None and user_slot['dataset'] != None and user_slot['target'] != None and user_slot['delivery'] != None:\n",
    "            text += f\"All you requested are well received! \\n These are your requirements: {user_slot['method']}, {user_slot['task']}, {user_slot['dataset']}, {user_slot['target']}, and {user_slot['delivery']} \\n Do you want to proceed?\"\n",
    "            current_state = 'await'\n",
    "        else:\n",
    "            text += 'I need more things to complete. Please specify the following list:\\n'\n",
    "            for key, value in user_slot.items():\n",
    "                if value == None:\n",
    "                    text += '- ' + re.sub('_', ' ', key) + '\\n'\n",
    "            current_state = 'active'\n",
    "\n",
    "    return text, current_state, user_slot\n",
    "\n",
    "def await_state(user_message):\n",
    "    text = ''\n",
    "    current_state = 'await'\n",
    "    global user_slot\n",
    "    \n",
    "    for message in user_message:\n",
    "        if message.lower() in GREETING_INPUTS:\n",
    "            text += random.choice([\"Yep! We've already greeted!\", \"I've already known you~\", \"Please go to the next step!\"]) + '\\n'\n",
    "        elif message.lower() in END_WORDS:\n",
    "            current_state = 'standby'\n",
    "            text = random.choice(END_RESPONSES)\n",
    "        elif message.lower() in CONFIRM_WORDS:\n",
    "            current_state = 'building'\n",
    "        elif message.lower() in DENY_WORDS:\n",
    "            current_state = 'await'\n",
    "            text += 'Umm... Please check your requirement~'\n",
    "        elif message.lower() in ML_METHODS:\n",
    "            user_slot['method'] = 'ml'\n",
    "        elif message.lower() in DL_METHODS:\n",
    "            user_slot['method'] = 'dl'\n",
    "        elif message.lower() in IMAGE_TYPES:\n",
    "            user_slot['data_type'] = 'image'\n",
    "        elif message.lower() in TEXT_TYPES:\n",
    "            user_slot['data_type'] = 'text'\n",
    "        elif message.lower() in TABLE_TYPES:\n",
    "            user_slot['data_type'] = 'table'\n",
    "        elif message.lower() in CLASSIFICATIONS:\n",
    "            user_slot['task'] = 'cls'\n",
    "        elif message.lower() in REGRESSIONS:\n",
    "            user_slot['task'] = 'reg'\n",
    "        elif message.lower() in AVAIL_DATASETS:\n",
    "            user_slot['dataset'] = message.lower()\n",
    "            user_slot['data_source'] = 'built_in'\n",
    "            user_slot['target'] = 'label'\n",
    "            tfds.load(message.lower())\n",
    "        elif message.lower() in DATA_SOURCES:\n",
    "            user_slot['data_source'] = 'user_define'\n",
    "            user_slot['dataset'] = 'filepath'\n",
    "        elif message.lower() in DELIVERY:\n",
    "            user_slot['delivery'] = 'email'\n",
    "    \n",
    "    \n",
    "    if current_state != 'building' and current_state != 'standby':   \n",
    "        if user_slot['method'] != None and user_slot['task'] != None and user_slot['data_source'] != None and user_slot['dataset'] != None and user_slot['target'] != None and user_slot['delivery'] != None:\n",
    "            text += f\"All you requested are well received! \\n These are your requirements: {user_slot['method']}, {user_slot['task']}, {user_slot['dataset']}, {user_slot['target']}, and {user_slot['delivery']} \\n Do you want to proceed?\"\n",
    "            current_state = 'await'\n",
    "    elif current_state == 'building':\n",
    "        \n",
    "        \n",
    "    return text, current_state, user_slot\n",
    "\n",
    "def building_state(user_message):\n",
    "    text = ''\n",
    "    current_state = 'building'\n",
    "    global user_slot\n",
    "\n",
    "    return text, current_state, user_slot\n",
    "\n",
    "def response_text(current_state, user_message):    \n",
    "    filtered_text = [token[0] for token in text_normalization(user_message)]\n",
    "\n",
    "    response = {\n",
    "        'standby': standby_state(filtered_text),\n",
    "        'active': active_state(filtered_text),\n",
    "        'await': await_state(filtered_text),\n",
    "        'building': building_state(filtered_text),\n",
    "    }\n",
    "    \n",
    "    return \"I am sorry. I don't understand you. Please refer to the following examples.\" if response[current_state]  == '' else response[current_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Hi 👋🏻! I'm your model builder🧑🏻‍💻~ Just tell me which model do you want by simply following the examples below👇🏻.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need more things to complete. Please specify the following list:\n",
      "- method\n",
      "- task\n",
      "- data source\n",
      "- data type\n",
      "- dataset\n",
      "- target\n",
      " active\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need more things to complete. Please specify the following list:\n",
      "- method\n",
      "- task\n",
      "- data source\n",
      "- data type\n",
      "- dataset\n",
      "- target\n",
      " active\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need more things to complete. Please specify the following list:\n",
      "- method\n",
      "- task\n",
      "- data source\n",
      "- data type\n",
      "- dataset\n",
      "- target\n",
      " active\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " I want an image classifier with MNIST data set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need more things to complete. Please specify the following list:\n",
      "- method\n",
      " active\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " I want a deep learning-based image classifier with MNIST data set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need more things to complete. Please specify the following list:\n",
      "- method\n",
      " active\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " I want a DL model for image classification with MNIST data set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All you requested are well received! \n",
      " These are your requirements: dl, cls, mnist, label, and chat \n",
      " Do you want to proceed? await\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " building\n"
     ]
    }
   ],
   "source": [
    "current_state = 'standby'\n",
    "print(\"Bot:\", \"Hi 👋🏻! I'm your model builder🧑🏻‍💻~ Just tell me which model do you want by simply following the examples below👇🏻.\")\n",
    "\n",
    "while True:\n",
    "    user_query = input()\n",
    "    response, current_state, user_slots = response_text(current_state, user_query)\n",
    "    print(response, current_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import autokeras as ak\n",
    "import autosklearn as ask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(specs):\n",
    "    model = None\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
