{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Enriching Meal\n",
    "## Machine Learning\n",
    "\n",
    "##### Blended Model\n",
    "- XGBoost (35%)\n",
    "- LightGBM (30%)\n",
    "- Neural Network (30%)\n",
    "- Random Forest (5%)\n",
    "\n",
    "#### Public Leaderboard Test set AUC: 0.83972\n",
    "#### Private Leaderboard Test set AUC: 0.83914"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import training set and create column identifying as train\n",
    "training = pd.read_excel('TrainingSet.xlsx')\n",
    "training['train_test_split']= 'Train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import test set and create column identifying as test\n",
    "test = pd.read_excel('TestSet.xlsx')\n",
    "test['train_test_split']='Test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate Training and Test set for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test = pd.concat([training,test],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "This is where we develop new features based off of the data in the current dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### Compute average competency performance for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building an average competency performance for the year 2009\n",
    "train_test['comp_perf2009'] = train_test[['X_PMB_Engagement_2009','X_PMB_Teamwork_2009','X_PMB_Accountability_2009',\n",
    "                                          'X_PMB_Action_2009','X_PMB_Values_2009']].mean(axis=1)\n",
    "\n",
    "# Building an average competency performance for the year 2008\n",
    "train_test['comp_perf2008'] = train_test[['X_PMB_ModelValues_2008','X_PMB_CreateExternalFocus_2008','X_PMB_AnticipateChange_2008',\n",
    "                                           'X_PMB_ImplementQuality_2008','X_PMB_EvaluateAct_2008',\n",
    "                                           'X_PMB_AchieveResultsPeople_2008','X_PMB_ShareKeyLearning_2008']].mean(axis=1)\n",
    "\n",
    "# Building an average competency performance for the year 2007                               \n",
    "train_test['comp_perf2007'] = train_test[['X_PMB_ModelValues_2007','X_PMB_CreateExternalFocus_2007',\n",
    "                                           'X_PMB_AnticipateChange_2007',\n",
    "                                           'X_PMB_ImplementQuality_2007','X_PMB_EvaluateAct_2007',\n",
    "                                           'X_PMB_AchieveResultsPeople_2007','X_PMB_ShareKeyLearning_2007']].mean(axis=1)\n",
    "\n",
    "# Building an average competency performance for the year 2006                               \n",
    "train_test['comp_perf2006'] = train_test[['X_PMB_ModelValues_2006','X_PMB_CreateExternalFocus_2006','X_PMB_AnticipateChange_2006',\n",
    "                                           'X_PMB_ImplementQuality_2006','X_PMB_EvaluateAct_2006',\n",
    "                                           'X_PMB_AchieveResultsPeople_2006','X_PMB_ShareKeyLearning_2006']].mean(axis=1)\n",
    "\n",
    "# Building an average competency performance for the year 2005                               \n",
    "train_test['comp_perf2005'] = train_test[['X_PMB_ModelValues_2005','X_PMB_CreateExternalFocus_2005','X_PMB_AnticipateChange_2005',\n",
    "                                           'X_PMB_ImplementQuality_2005','X_PMB_EvaluateAct_2005',\n",
    "                                           'X_PMB_AchieveResultsPeople_2005','X_PMB_ShareKeyLearning_2005']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Compute the standard deviation for competency for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building a standard deviation competency performance for the year 2009\n",
    "train_test['std_comp_perf2009'] = train_test[['X_PMB_Engagement_2009','X_PMB_Teamwork_2009','X_PMB_Accountability_2009',\n",
    "                                          'X_PMB_Action_2009','X_PMB_Values_2009']].std(axis=1)\n",
    "\n",
    "# Building a standard deviation competency performance for the year 2008\n",
    "train_test['std_comp_perf2008'] = train_test[['X_PMB_ModelValues_2008','X_PMB_CreateExternalFocus_2008','X_PMB_AnticipateChange_2008',\n",
    "                                           'X_PMB_ImplementQuality_2008','X_PMB_EvaluateAct_2008',\n",
    "                                           'X_PMB_AchieveResultsPeople_2008','X_PMB_ShareKeyLearning_2008']].std(axis=1)\n",
    "\n",
    "# Building a standard deviation competency performance for the year 2007                            \n",
    "train_test['std_comp_perf2007'] = train_test[['X_PMB_ModelValues_2007','X_PMB_CreateExternalFocus_2007',\n",
    "                                           'X_PMB_AnticipateChange_2007',\n",
    "                                           'X_PMB_ImplementQuality_2007','X_PMB_EvaluateAct_2007',\n",
    "                                           'X_PMB_AchieveResultsPeople_2007','X_PMB_ShareKeyLearning_2007']].std(axis=1)\n",
    "\n",
    "# Building a standard deviation competency performance for the year 2006                             \n",
    "train_test['std_comp_perf2006'] = train_test[['X_PMB_ModelValues_2006','X_PMB_CreateExternalFocus_2006','X_PMB_AnticipateChange_2006',\n",
    "                                           'X_PMB_ImplementQuality_2006','X_PMB_EvaluateAct_2006',\n",
    "                                           'X_PMB_AchieveResultsPeople_2006','X_PMB_ShareKeyLearning_2006']].std(axis=1)\n",
    "\n",
    "# Building a standard deviation competency performance for the year 2005                              \n",
    "train_test['std_comp_perf2005'] = train_test[['X_PMB_ModelValues_2005','X_PMB_CreateExternalFocus_2005','X_PMB_AnticipateChange_2005',\n",
    "                                           'X_PMB_ImplementQuality_2005','X_PMB_EvaluateAct_2005',\n",
    "                                           'X_PMB_AchieveResultsPeople_2005','X_PMB_ShareKeyLearning_2005']].std(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the average overall performance, standard deviation of overall performance, overall competency average, and overall competency standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Overall performance and competency based performance average and standard deviation \n",
    "\n",
    "train_test['Overall_Avg_Perf']= train_test[['X_OverallPerformanceRating2009','X_OverallPerformanceRating2008',\n",
    "                                           'X_OverallPerformanceRating2007','X_OverallPerformanceRating2006',\n",
    "                                           'X_OverallPerformanceRating2005','X_OverallPerformanceRating2004']].mean(axis=1)\n",
    "\n",
    "train_test['Overall_STD_Perf']= train_test[['X_OverallPerformanceRating2009','X_OverallPerformanceRating2008',\n",
    "                                           'X_OverallPerformanceRating2007','X_OverallPerformanceRating2006',\n",
    "                                           'X_OverallPerformanceRating2005','X_OverallPerformanceRating2004']].std(axis=1)\n",
    "\n",
    "train_test['Comp_Avg_Perf'] = train_test[['comp_perf2009','comp_perf2008','comp_perf2007','comp_perf2006',\n",
    "                                         'comp_perf2005']].mean(axis=1)\n",
    "\n",
    "train_test['Comp_STD_Perf'] = train_test[['comp_perf2009','comp_perf2008','comp_perf2007','comp_perf2006',\n",
    "                                         'comp_perf2005']].std(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute change over years for both competency and overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate performance change\n",
    "\n",
    "train_test['Comp_PM_Change2006'] = train_test['comp_perf2006']-train_test['comp_perf2005']\n",
    "train_test['Comp_PM_Change2007'] = train_test['comp_perf2007']-train_test['comp_perf2006']\n",
    "train_test['Comp_PM_Change2008'] = train_test['comp_perf2008']-train_test['comp_perf2007']\n",
    "train_test['Comp_PM_Change2009'] = train_test['comp_perf2009']-train_test['comp_perf2008']\n",
    "\n",
    "train_test['Overall_Change2005'] = train_test['X_OverallPerformanceRating2005']-train_test['X_OverallPerformanceRating2004']\n",
    "train_test['Overall_Change2006'] = train_test['X_OverallPerformanceRating2006']-train_test['X_OverallPerformanceRating2005']\n",
    "train_test['Overall_Change2007'] = train_test['X_OverallPerformanceRating2007']-train_test['X_OverallPerformanceRating2006']\n",
    "train_test['Overall_Change2008'] = train_test['X_OverallPerformanceRating2008']-train_test['X_OverallPerformanceRating2007']\n",
    "train_test['Overall_Change2009'] = train_test['X_OverallPerformanceRating2009']-train_test['X_OverallPerformanceRating2008']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the number of unique values for each global ID: \n",
    "- Supervisors\n",
    "- Countries\n",
    "- Cities\n",
    "- job types\n",
    "- sub functions\n",
    "- functions\n",
    "- paygrades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating a column that counts the number of unique supervisors an individual had\n",
    "\n",
    "col_list = train_test[['S_SupervisorGlobal_ID2009', 'S_SupervisorGlobal_ID2008', 'S_SupervisorGlobal_ID2007', 'S_SupervisorGlobal_ID2006',\n",
    "           'S_SupervisorGlobal_ID2005','S_SupervisorGlobal_ID2004']]\n",
    "\n",
    "uniques=col_list.nunique(axis=1)\n",
    "\n",
    "train_test['unique_supervisors']= uniques\n",
    "\n",
    "\n",
    "# creating a column that counts the number of unique countries an individual has worked in\n",
    "\n",
    "country_list = train_test[['X_Country2009', 'X_Country2008', 'X_Country2007', 'X_Country2006',\n",
    "           'X_Country2005','X_Country2004']]\n",
    "\n",
    "countries_unique=country_list.nunique(axis=1)\n",
    "\n",
    "train_test['unique_countries']= countries_unique\n",
    "\n",
    "# creating a column that counts the number of unique cities an individual has worked in\n",
    "\n",
    "city_list = train_test[['X_City2009', 'X_City2008', 'X_City2007', 'X_City2006',\n",
    "           'X_City2005','X_City2004']]\n",
    "\n",
    "city_unique=city_list.nunique(axis=1)\n",
    "\n",
    "train_test['unique_cities']= city_unique\n",
    "\n",
    "# creating a column that counts the number of unique jobtypes an individual has had\n",
    "\n",
    "job_list = train_test[['X_JobType2009', 'X_JobType2008', 'X_JobType2007', 'X_JobType2006',\n",
    "           'X_JobType2005','X_JobType2004']]\n",
    "\n",
    "job_unique=job_list.nunique(axis=1)\n",
    "\n",
    "train_test['unique_jobs']= job_unique\n",
    "\n",
    "# creating a column that counts the number of unique subfunctions an individual has had\n",
    "\n",
    "subfunction_list = train_test[['X_JobSubFunction2009', 'X_JobSubFunction2008', 'X_JobSubFunction2007', 'X_JobSubFunction2006',\n",
    "           'X_JobSubFunction2005','X_JobSubFunction2004']]\n",
    "\n",
    "subfunction_unique=subfunction_list.nunique(axis=1)\n",
    "\n",
    "train_test['unique_subfunctions']= subfunction_unique\n",
    "\n",
    "train_test['unique_subfunctions'].unique()\n",
    "\n",
    "# creating a column that counts the number of unique functions an individual has had\n",
    "\n",
    "function_list = train_test[['X_JobFunction2009', 'X_JobFunction2008', 'X_JobFunction2007', 'X_JobFunction2006',\n",
    "           'X_JobFunction2005','X_JobFunction2004']]\n",
    "\n",
    "function_unique= function_list.nunique(axis=1)\n",
    "\n",
    "train_test['unique_functions']= function_unique\n",
    "\n",
    "\n",
    "# creating a column that counts the number of unique paygrades an individual has had\n",
    "\n",
    "paygrade_list = train_test[['X_PayGradeLevel2009','X_PayGradeLevel2008','X_PayGradeLevel2007','X_PayGradeLevel2006',\n",
    "                           'X_PayGradeLevel2005','X_PayGradeLevel2004']]\n",
    "\n",
    "paygrade_unique = paygrade_list.nunique(axis=1)\n",
    "\n",
    "train_test['paygrade_changes'] = paygrade_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate gender differences amongst supervisor and employee\n",
    "\n",
    "sup_gender_2009 = train_test[['S_Gender2009','X_Gender']]\n",
    "\n",
    "sup_gender_dif_2009 = sup_gender_2009.nunique(axis=1)\n",
    "\n",
    "sup_gender_2008 = train_test[['S_Gender2008','X_Gender']]\n",
    "\n",
    "sup_gender_dif_2008 = sup_gender_2008.nunique(axis=1)\n",
    "\n",
    "sup_gender_2007 = train_test[['S_Gender2007','X_Gender']]\n",
    "\n",
    "sup_gender_dif_2007 = sup_gender_2007.nunique(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map gender differences as categorical\n",
    "\n",
    "train_test['sup_emp_gender_dif_2009'] = sup_gender_dif_2009.apply(lambda x: 1 if x==2 else 0)\n",
    "train_test['sup_emp_gender_dif_2008'] = sup_gender_dif_2008.apply(lambda x: 1 if x==2 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert hire date to datetime format and then extract year into it's own column\n",
    "train_test['X_ServiceEntryDate'] = pd.to_datetime(train_test['X_ServiceEntryDate'],format='%Y%m%d')\n",
    "train_test['hire_year'] = train_test['X_ServiceEntryDate'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a binary feature for those hired before 2005\n",
    "train_test['hire_before_2005'] = train_test['hire_year'].apply(lambda x: 1 if x<2005 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "calculate interactions between number of unique supervisors and interactions and age and tenure interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate supervisor changes by tenure\n",
    "\n",
    "train_test['sup_ten_inter']= train_test['unique_supervisors']*train_test['X_TenureDays2009']\n",
    "\n",
    "# calculate age by tenure interaction\n",
    "\n",
    "train_test['age_tenure_inter']= train_test['X_TenureDays2009']*train_test['X_Age2009']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate whether or not the supervisor had a performance rating in 2009 \n",
    "#proxy for whether or not the supervisor is new\n",
    "\n",
    "train_test['new_sup'] = train_test['S_OverallPerformanceRating2009'].isnull()\n",
    "train_test['S_OPR_N_2009'] = train_test['new_sup'].apply(lambda x: 1 if x==True else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate whether or not the supervisor had a share key learnings rating in 2005\n",
    "train_test['skl_2005'] = train_test['S_PMB_ShareKeyLearning_2005'].isnull()\n",
    "train_test['S_PMB_SHA_N_2005'] = train_test['skl_2005'].apply(lambda x: 1 if x==True else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# z-score transformation for 2005 performance scores\n",
    "# this method was used instead of standardscalar() to avoid imputing data at this point\n",
    "\n",
    "S_OverallPerformanceRating2005_mean = train_test['S_OverallPerformanceRating2005'].mean()\n",
    "S_OverallPerformanceRating2005_STD = train_test['S_OverallPerformanceRating2005'].std()\n",
    "train_test['S_OverallPerformanceRating2005_Z'] = (train_test['S_OverallPerformanceRating2005']-\n",
    "                                    S_OverallPerformanceRating2005_mean)/S_OverallPerformanceRating2005_STD\n",
    "\n",
    "S_PMB_ModelValues_2005_mean = train_test['S_PMB_ModelValues_2005'].mean()\n",
    "S_PMB_ModelValues_2005_STD = train_test['S_PMB_ModelValues_2005'].std()\n",
    "train_test['S_PMB_ModelValues_2005_Z'] = (train_test['S_PMB_ModelValues_2005']-\n",
    "                                    S_PMB_ModelValues_2005_mean)/S_PMB_ModelValues_2005_STD\n",
    "\n",
    "S_PMB_CreateExternalFocus_2005_mean = train_test['S_PMB_CreateExternalFocus_2005'].mean()\n",
    "S_PMB_CreateExternalFocus_2005_STD = train_test['S_PMB_CreateExternalFocus_2005'].std()\n",
    "train_test['S_PMB_CreateExternalFocus_2005_Z'] = (train_test['S_PMB_CreateExternalFocus_2005']-\n",
    "                                    S_PMB_CreateExternalFocus_2005_mean)/S_PMB_CreateExternalFocus_2005_STD\n",
    "\n",
    "S_PMB_AnticipateChange_2005_mean = train_test['S_PMB_AnticipateChange_2005'].mean()\n",
    "S_PMB_AnticipateChange_2005_STD = train_test['S_PMB_AnticipateChange_2005'].std()\n",
    "train_test['S_PMB_AnticipateChange_2005_Z'] = (train_test['S_PMB_AnticipateChange_2005']-\n",
    "                                    S_PMB_AnticipateChange_2005_mean)/S_PMB_AnticipateChange_2005_STD\n",
    "\n",
    "S_PMB_ImplementQuality_2005_mean = train_test['S_PMB_ImplementQuality_2005'].mean()\n",
    "S_PMB_ImplementQuality_2005_STD = train_test['S_PMB_ImplementQuality_2005'].std()\n",
    "train_test['S_PMB_ImplementQuality_2005_Z'] = (train_test['S_PMB_ImplementQuality_2005']-\n",
    "                                    S_PMB_ImplementQuality_2005_mean)/S_PMB_ImplementQuality_2005_STD\n",
    "\n",
    "S_PMB_EvaluateAct_2005_mean = train_test['S_PMB_EvaluateAct_2005'].mean()\n",
    "S_PMB_EvaluateAct_2005_STD = train_test['S_PMB_EvaluateAct_2005'].std()\n",
    "train_test['S_PMB_EvaluateAct_2005_Z'] = (train_test['S_PMB_EvaluateAct_2005']-\n",
    "                                    S_PMB_EvaluateAct_2005_mean)/S_PMB_EvaluateAct_2005_STD\n",
    "\n",
    "S_PMB_AchieveResultsPeople_2005_mean = train_test['S_PMB_AchieveResultsPeople_2005'].mean()\n",
    "S_PMB_AchieveResultsPeople_2005_STD = train_test['S_PMB_AchieveResultsPeople_2005'].std()\n",
    "train_test['S_PMB_AchieveResultsPeople_2005_Z'] = (train_test['S_PMB_AchieveResultsPeople_2005']-\n",
    "                                    S_PMB_AchieveResultsPeople_2005_mean)/S_PMB_AchieveResultsPeople_2005_STD\n",
    "\n",
    "S_PMB_ShareKeyLearning_2005_mean = train_test['S_PMB_ShareKeyLearning_2005'].mean()\n",
    "S_PMB_ShareKeyLearning_2005_STD = train_test['S_PMB_ShareKeyLearning_2005'].std()\n",
    "train_test['S_PMB_ShareKeyLearning_2005_Z'] = (train_test['S_PMB_ShareKeyLearning_2005']-\n",
    "                                    S_PMB_ShareKeyLearning_2005_mean)/S_PMB_ShareKeyLearning_2005_STD\n",
    "\n",
    "train_test['S_OPR_COMP_2005'] = train_test[['S_OverallPerformanceRating2005_Z','S_PMB_ModelValues_2005_Z',\n",
    "                                           'S_PMB_CreateExternalFocus_2005_Z','S_PMB_AnticipateChange_2005_Z',\n",
    "                                          'S_PMB_EvaluateAct_2005_Z', 'S_PMB_AchieveResultsPeople_2005_Z',\n",
    "                                           'S_PMB_ShareKeyLearning_2005_Z']].mean(axis=1)\n",
    "\n",
    "train_test['S_OPR_COMP_2005'].replace(np.nan,0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# z-score transformation for 2006 performance scores\n",
    "# this method was used instead of standardscalar() to avoid imputing data at this point\n",
    "\n",
    "S_OverallPerformanceRating2006_mean = train_test['S_OverallPerformanceRating2006'].mean()\n",
    "S_OverallPerformanceRating2006_STD = train_test['S_OverallPerformanceRating2006'].std()\n",
    "train_test['S_OverallPerformanceRating2006_Z'] = (train_test['S_OverallPerformanceRating2006']-\n",
    "                                    S_OverallPerformanceRating2006_mean)/S_OverallPerformanceRating2006_STD\n",
    "\n",
    "S_PMB_ModelValues_2006_mean = train_test['S_PMB_ModelValues_2006'].mean()\n",
    "S_PMB_ModelValues_2006_STD = train_test['S_PMB_ModelValues_2006'].std()\n",
    "train_test['S_PMB_ModelValues_2006_Z'] = (train_test['S_PMB_ModelValues_2006']-\n",
    "                                    S_PMB_ModelValues_2006_mean)/S_PMB_ModelValues_2006_STD\n",
    "\n",
    "S_PMB_CreateExternalFocus_2006_mean = train_test['S_PMB_CreateExternalFocus_2006'].mean()\n",
    "S_PMB_CreateExternalFocus_2006_STD = train_test['S_PMB_CreateExternalFocus_2006'].std()\n",
    "train_test['S_PMB_CreateExternalFocus_2006_Z'] = (train_test['S_PMB_CreateExternalFocus_2006']-\n",
    "                                    S_PMB_CreateExternalFocus_2006_mean)/S_PMB_CreateExternalFocus_2006_STD\n",
    "\n",
    "S_PMB_AnticipateChange_2006_mean = train_test['S_PMB_AnticipateChange_2006'].mean()\n",
    "S_PMB_AnticipateChange_2006_STD = train_test['S_PMB_AnticipateChange_2006'].std()\n",
    "train_test['S_PMB_AnticipateChange_2006_Z'] = (train_test['S_PMB_AnticipateChange_2006']-\n",
    "                                    S_PMB_AnticipateChange_2006_mean)/S_PMB_AnticipateChange_2006_STD\n",
    "\n",
    "S_PMB_ImplementQuality_2006_mean = train_test['S_PMB_ImplementQuality_2006'].mean()\n",
    "S_PMB_ImplementQuality_2006_STD = train_test['S_PMB_ImplementQuality_2006'].std()\n",
    "train_test['S_PMB_ImplementQuality_2006_Z'] = (train_test['S_PMB_ImplementQuality_2006']-\n",
    "                                    S_PMB_ImplementQuality_2006_mean)/S_PMB_ImplementQuality_2006_STD\n",
    "\n",
    "S_PMB_EvaluateAct_2006_mean = train_test['S_PMB_EvaluateAct_2006'].mean()\n",
    "S_PMB_EvaluateAct_2006_STD = train_test['S_PMB_EvaluateAct_2006'].std()\n",
    "train_test['S_PMB_EvaluateAct_2006_Z'] = (train_test['S_PMB_EvaluateAct_2006']-\n",
    "                                    S_PMB_EvaluateAct_2006_mean)/S_PMB_EvaluateAct_2006_STD\n",
    "\n",
    "S_PMB_AchieveResultsPeople_2006_mean = train_test['S_PMB_AchieveResultsPeople_2006'].mean()\n",
    "S_PMB_AchieveResultsPeople_2006_STD = train_test['S_PMB_AchieveResultsPeople_2006'].std()\n",
    "train_test['S_PMB_AchieveResultsPeople_2006_Z'] = (train_test['S_PMB_AchieveResultsPeople_2006']-\n",
    "                                    S_PMB_AchieveResultsPeople_2006_mean)/S_PMB_AchieveResultsPeople_2006_STD\n",
    "\n",
    "S_PMB_ShareKeyLearning_2006_mean = train_test['S_PMB_ShareKeyLearning_2006'].mean()\n",
    "S_PMB_ShareKeyLearning_2006_STD = train_test['S_PMB_ShareKeyLearning_2006'].std()\n",
    "train_test['S_PMB_ShareKeyLearning_2006_Z'] = (train_test['S_PMB_ShareKeyLearning_2006']-\n",
    "                                    S_PMB_ShareKeyLearning_2006_mean)/S_PMB_ShareKeyLearning_2006_STD\n",
    "\n",
    "train_test['S_OPR_COMP_2006'] = train_test[['S_OverallPerformanceRating2006_Z','S_PMB_ModelValues_2006_Z',\n",
    "                                           'S_PMB_CreateExternalFocus_2006_Z','S_PMB_AnticipateChange_2006_Z',\n",
    "                                          'S_PMB_EvaluateAct_2006_Z', 'S_PMB_AchieveResultsPeople_2006_Z',\n",
    "                                           'S_PMB_ShareKeyLearning_2006_Z']].mean(axis=1)\n",
    "\n",
    "train_test['S_OPR_COMP_2006'].replace(np.nan,0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# z-score transformation for 2007 performance scores\n",
    "# this method was used instead of standardscalar() to avoid imputing data at this point\n",
    "\n",
    "S_OverallPerformanceRating2007_mean = train_test['S_OverallPerformanceRating2007'].mean()\n",
    "S_OverallPerformanceRating2007_STD = train_test['S_OverallPerformanceRating2007'].std()\n",
    "train_test['S_OverallPerformanceRating2007_Z'] = (train_test['S_OverallPerformanceRating2007']-\n",
    "                                    S_OverallPerformanceRating2007_mean)/S_OverallPerformanceRating2007_STD\n",
    "\n",
    "S_PMB_ModelValues_2007_mean = train_test['S_PMB_ModelValues_2007'].mean()\n",
    "S_PMB_ModelValues_2007_STD = train_test['S_PMB_ModelValues_2007'].std()\n",
    "train_test['S_PMB_ModelValues_2007_Z'] = (train_test['S_PMB_ModelValues_2007']-\n",
    "                                    S_PMB_ModelValues_2007_mean)/S_PMB_ModelValues_2007_STD\n",
    "\n",
    "S_PMB_CreateExternalFocus_2007_mean = train_test['S_PMB_CreateExternalFocus_2007'].mean()\n",
    "S_PMB_CreateExternalFocus_2007_STD = train_test['S_PMB_CreateExternalFocus_2007'].std()\n",
    "train_test['S_PMB_CreateExternalFocus_2007_Z'] = (train_test['S_PMB_CreateExternalFocus_2007']-\n",
    "                                    S_PMB_CreateExternalFocus_2007_mean)/S_PMB_CreateExternalFocus_2007_STD\n",
    "\n",
    "S_PMB_AnticipateChange_2007_mean = train_test['S_PMB_AnticipateChange_2007'].mean()\n",
    "S_PMB_AnticipateChange_2007_STD = train_test['S_PMB_AnticipateChange_2007'].std()\n",
    "train_test['S_PMB_AnticipateChange_2007_Z'] = (train_test['S_PMB_AnticipateChange_2007']-\n",
    "                                    S_PMB_AnticipateChange_2007_mean)/S_PMB_AnticipateChange_2007_STD\n",
    "\n",
    "S_PMB_ImplementQuality_2007_mean = train_test['S_PMB_ImplementQuality_2007'].mean()\n",
    "S_PMB_ImplementQuality_2007_STD = train_test['S_PMB_ImplementQuality_2007'].std()\n",
    "train_test['S_PMB_ImplementQuality_2007_Z'] = (train_test['S_PMB_ImplementQuality_2007']-\n",
    "                                    S_PMB_ImplementQuality_2007_mean)/S_PMB_ImplementQuality_2007_STD\n",
    "\n",
    "S_PMB_EvaluateAct_2007_mean = train_test['S_PMB_EvaluateAct_2007'].mean()\n",
    "S_PMB_EvaluateAct_2007_STD = train_test['S_PMB_EvaluateAct_2007'].std()\n",
    "train_test['S_PMB_EvaluateAct_2007_Z'] = (train_test['S_PMB_EvaluateAct_2007']-\n",
    "                                    S_PMB_EvaluateAct_2007_mean)/S_PMB_EvaluateAct_2007_STD\n",
    "\n",
    "S_PMB_AchieveResultsPeople_2007_mean = train_test['S_PMB_AchieveResultsPeople_2007'].mean()\n",
    "S_PMB_AchieveResultsPeople_2007_STD = train_test['S_PMB_AchieveResultsPeople_2007'].std()\n",
    "train_test['S_PMB_AchieveResultsPeople_2007_Z'] = (train_test['S_PMB_AchieveResultsPeople_2007']-\n",
    "                                    S_PMB_AchieveResultsPeople_2007_mean)/S_PMB_AchieveResultsPeople_2007_STD\n",
    "\n",
    "S_PMB_ShareKeyLearning_2007_mean = train_test['S_PMB_ShareKeyLearning_2007'].mean()\n",
    "S_PMB_ShareKeyLearning_2007_STD = train_test['S_PMB_ShareKeyLearning_2007'].std()\n",
    "train_test['S_PMB_ShareKeyLearning_2007_Z'] = (train_test['S_PMB_ShareKeyLearning_2007']-\n",
    "                                    S_PMB_ShareKeyLearning_2007_mean)/S_PMB_ShareKeyLearning_2007_STD\n",
    "\n",
    "train_test['S_OPR_COMP_2007'] = train_test[['S_OverallPerformanceRating2007_Z','S_PMB_ModelValues_2007_Z',\n",
    "                                           'S_PMB_CreateExternalFocus_2007_Z','S_PMB_AnticipateChange_2007_Z',\n",
    "                                          'S_PMB_EvaluateAct_2007_Z', 'S_PMB_AchieveResultsPeople_2007_Z',\n",
    "                                           'S_PMB_ShareKeyLearning_2007_Z']].mean(axis=1)\n",
    "\n",
    "train_test['S_OPR_COMP_2007'].replace(np.nan,0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# z-score transformation for 2008 performance scores\n",
    "# this method was used instead of standardscalar() to avoid imputing data at this point\n",
    "\n",
    "S_OverallPerformanceRating2008_mean = train_test['S_OverallPerformanceRating2008'].mean()\n",
    "S_OverallPerformanceRating2008_STD = train_test['S_OverallPerformanceRating2008'].std()\n",
    "train_test['S_OverallPerformanceRating2008_Z'] = (train_test['S_OverallPerformanceRating2008']-\n",
    "                                    S_OverallPerformanceRating2008_mean)/S_OverallPerformanceRating2008_STD\n",
    "\n",
    "S_PMB_ModelValues_2008_mean = train_test['S_PMB_ModelValues_2008'].mean()\n",
    "S_PMB_ModelValues_2008_STD = train_test['S_PMB_ModelValues_2008'].std()\n",
    "train_test['S_PMB_ModelValues_2008_Z'] = (train_test['S_PMB_ModelValues_2008']-\n",
    "                                    S_PMB_ModelValues_2008_mean)/S_PMB_ModelValues_2008_STD\n",
    "\n",
    "S_PMB_CreateExternalFocus_2008_mean = train_test['S_PMB_CreateExternalFocus_2008'].mean()\n",
    "S_PMB_CreateExternalFocus_2008_STD = train_test['S_PMB_CreateExternalFocus_2008'].std()\n",
    "train_test['S_PMB_CreateExternalFocus_2008_Z'] = (train_test['S_PMB_CreateExternalFocus_2008']-\n",
    "                                    S_PMB_CreateExternalFocus_2008_mean)/S_PMB_CreateExternalFocus_2008_STD\n",
    "\n",
    "S_PMB_AnticipateChange_2008_mean = train_test['S_PMB_AnticipateChange_2008'].mean()\n",
    "S_PMB_AnticipateChange_2008_STD = train_test['S_PMB_AnticipateChange_2008'].std()\n",
    "train_test['S_PMB_AnticipateChange_2008_Z'] = (train_test['S_PMB_AnticipateChange_2008']-\n",
    "                                    S_PMB_AnticipateChange_2008_mean)/S_PMB_AnticipateChange_2008_STD\n",
    "\n",
    "S_PMB_ImplementQuality_2008_mean = train_test['S_PMB_ImplementQuality_2008'].mean()\n",
    "S_PMB_ImplementQuality_2008_STD = train_test['S_PMB_ImplementQuality_2008'].std()\n",
    "train_test['S_PMB_ImplementQuality_2008_Z'] = (train_test['S_PMB_ImplementQuality_2008']-\n",
    "                                    S_PMB_ImplementQuality_2008_mean)/S_PMB_ImplementQuality_2008_STD\n",
    "\n",
    "S_PMB_EvaluateAct_2008_mean = train_test['S_PMB_EvaluateAct_2008'].mean()\n",
    "S_PMB_EvaluateAct_2008_STD = train_test['S_PMB_EvaluateAct_2008'].std()\n",
    "train_test['S_PMB_EvaluateAct_2008_Z'] = (train_test['S_PMB_EvaluateAct_2008']-\n",
    "                                    S_PMB_EvaluateAct_2008_mean)/S_PMB_EvaluateAct_2008_STD\n",
    "\n",
    "S_PMB_AchieveResultsPeople_2008_mean = train_test['S_PMB_AchieveResultsPeople_2008'].mean()\n",
    "S_PMB_AchieveResultsPeople_2008_STD = train_test['S_PMB_AchieveResultsPeople_2008'].std()\n",
    "train_test['S_PMB_AchieveResultsPeople_2008_Z'] = (train_test['S_PMB_AchieveResultsPeople_2008']-\n",
    "                                    S_PMB_AchieveResultsPeople_2008_mean)/S_PMB_AchieveResultsPeople_2008_STD\n",
    "\n",
    "S_PMB_ShareKeyLearning_2008_mean = train_test['S_PMB_ShareKeyLearning_2008'].mean()\n",
    "S_PMB_ShareKeyLearning_2008_STD = train_test['S_PMB_ShareKeyLearning_2008'].std()\n",
    "train_test['S_PMB_ShareKeyLearning_2008_Z'] = (train_test['S_PMB_ShareKeyLearning_2008']-\n",
    "                                    S_PMB_ShareKeyLearning_2008_mean)/S_PMB_ShareKeyLearning_2008_STD\n",
    "\n",
    "train_test['S_OPR_COMP_2008'] = train_test[['S_OverallPerformanceRating2008_Z','S_PMB_ModelValues_2008_Z',\n",
    "                                           'S_PMB_CreateExternalFocus_2008_Z','S_PMB_AnticipateChange_2008_Z',\n",
    "                                          'S_PMB_EvaluateAct_2008_Z', 'S_PMB_AchieveResultsPeople_2008_Z',\n",
    "                                           'S_PMB_ShareKeyLearning_2008_Z']].mean(axis=1)\n",
    "\n",
    "train_test['S_OPR_COMP_2008'].replace(np.nan,0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# z-score transformation for 2009 performance scores\n",
    "# this method was used instead of standardscalar() to avoid imputing data at this point\n",
    "\n",
    "S_OverallPerformanceRating2009_mean = train_test['S_OverallPerformanceRating2009'].mean()\n",
    "S_OverallPerformanceRating2009_STD = train_test['S_OverallPerformanceRating2009'].std()\n",
    "train_test['S_OverallPerformanceRating2009_Z'] = (train_test['S_OverallPerformanceRating2009']-\n",
    "                                    S_OverallPerformanceRating2009_mean)/S_OverallPerformanceRating2009_STD\n",
    "\n",
    "S_PMB_Engagement_2009_mean = train_test['S_PMB_Engagement_2009'].mean()\n",
    "S_PMB_Engagement_2009_STD = train_test['S_PMB_Engagement_2009'].std()\n",
    "train_test['S_PMB_Engagement_2009_Z'] = (train_test['S_PMB_Engagement_2009']-\n",
    "                                    S_PMB_Engagement_2009_mean)/S_PMB_Engagement_2009_STD\n",
    "\n",
    "S_PMB_Teamwork_2009_mean = train_test['S_PMB_Teamwork_2009'].mean()\n",
    "S_PMB_Teamwork_2009_STD = train_test['S_PMB_Teamwork_2009'].std()\n",
    "train_test['S_PMB_Teamwork_2009_Z'] = (train_test['S_PMB_Teamwork_2009']-\n",
    "                                    S_PMB_Teamwork_2009_mean)/S_PMB_Teamwork_2009_STD\n",
    "\n",
    "S_PMB_Accountability_2009_mean = train_test['S_PMB_Accountability_2009'].mean()\n",
    "S_PMB_Accountability_2009_STD = train_test['S_PMB_Accountability_2009'].std()\n",
    "train_test['S_PMB_Accountability_2009_Z'] = (train_test['S_PMB_Accountability_2009']-\n",
    "                                    S_PMB_Accountability_2009_mean)/S_PMB_Accountability_2009_STD\n",
    "\n",
    "S_PMB_Action_2009_mean = train_test['S_PMB_Action_2009'].mean()\n",
    "S_PMB_Action_2009_STD = train_test['S_PMB_Action_2009'].std()\n",
    "train_test['S_PMB_Action_2009_Z'] = (train_test['S_PMB_Action_2009']-\n",
    "                                    S_PMB_Action_2009_mean)/S_PMB_Action_2009_STD\n",
    "\n",
    "S_PMB_Values_2009_mean = train_test['S_PMB_Values_2009'].mean()\n",
    "S_PMB_Values_2009_STD = train_test['S_PMB_Values_2009'].std()\n",
    "train_test['S_PMB_Values_2009_Z'] = (train_test['S_PMB_Values_2009']-\n",
    "                                    S_PMB_Values_2009_mean)/S_PMB_Values_2009_STD\n",
    "\n",
    "train_test['S_OPR_COMP_2009'] = train_test[['S_OverallPerformanceRating2009_Z','S_PMB_Engagement_2009_Z',\n",
    "                                           'S_PMB_Teamwork_2009_Z','S_PMB_Accountability_2009_Z',\n",
    "                                          'S_PMB_Action_2009_Z', 'S_PMB_Values_2009_Z',]].mean(axis=1)\n",
    "\n",
    "train_test['S_OPR_COMP_2009'].replace(np.nan,0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nckoeni/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# standardizing/z-scoring performance\n",
    "#supervisor performance for 2009\n",
    "ov_comp_perf = train_test[['S_OPR_COMP_2005','S_OPR_COMP_2006','S_OPR_COMP_2007','S_OPR_COMP_2008','S_OPR_COMP_2009']]\n",
    "ov_comp_perf.replace(np.nan,0,inplace=True)\n",
    "zov_comp_perf = sc.fit_transform(ov_comp_perf)\n",
    "\n",
    "\n",
    "# assigning the numpy array columns to the appropriate new feature\n",
    "train_test['ZS_OPR_COMP_2005'] = zov_comp_perf[:,[0]]\n",
    "train_test['ZS_OPR_COMP_2006'] = zov_comp_perf[:,[1]]\n",
    "train_test['ZS_OPR_COMP_2007'] = zov_comp_perf[:,[2]]\n",
    "train_test['ZS_OPR_COMP_2008'] = zov_comp_perf[:,[3]]\n",
    "train_test['ZS_OPR_COMP_2009'] = zov_comp_perf[:,[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# looking at the difference in years in z-score\n",
    "#then applying a -1 if the difference is lower and a 1 if the difference is higher\n",
    "#then taking the mean across all of the years the data exists\n",
    "\n",
    "ZS_OPR_COMP_CHANGE_05_TO_06 = train_test['ZS_OPR_COMP_2005']-train_test['ZS_OPR_COMP_2006']\n",
    "ZS_OPR_COMP_CHANGE_06_TO_07 = train_test['ZS_OPR_COMP_2006']-train_test['ZS_OPR_COMP_2007']\n",
    "ZS_OPR_COMP_CHANGE_07_TO_08 = train_test['ZS_OPR_COMP_2007']-train_test['ZS_OPR_COMP_2008']\n",
    "ZS_OPR_COMP_CHANGE_08_TO_09 = train_test['ZS_OPR_COMP_2008']-train_test['ZS_OPR_COMP_2009']\n",
    "\n",
    "train_test['ZS_Change_0506'] = ZS_OPR_COMP_CHANGE_05_TO_06.apply(lambda x: -1 if x<=0 else 1)\n",
    "train_test['ZS_Change_0607'] = ZS_OPR_COMP_CHANGE_06_TO_07.apply(lambda x: -1 if x<=0 else 1)\n",
    "train_test['ZS_Change_0708'] = ZS_OPR_COMP_CHANGE_07_TO_08.apply(lambda x: -1 if x<=0 else 1)\n",
    "train_test['ZS_Change_0809'] = ZS_OPR_COMP_CHANGE_08_TO_09.apply(lambda x: -1 if x<=0 else 1)\n",
    "\n",
    "train_test['S_OPRC_Mean'] = train_test[['ZS_Change_0506','ZS_Change_0607','ZS_Change_0708','ZS_Change_0809']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Code Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test = pd.concat([train_test, pd.get_dummies(train_test['X_Gender'],drop_first=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test = pd.concat([train_test, pd.get_dummies(train_test['X_City2009'])],axis=1)\n",
    "train_test = pd.concat([train_test, pd.get_dummies(train_test['X_Country2009'])],axis=1)\n",
    "train_test = pd.concat([train_test, pd.get_dummies(train_test['X_JobType2009'])],axis=1)\n",
    "train_test = pd.concat([train_test, pd.get_dummies(train_test['X_RaceEthnicity'])],axis=1)\n",
    "train_test = pd.concat([train_test, pd.get_dummies(train_test['X_JobSubFunction2009'])],axis=1)\n",
    "train_test = pd.concat([train_test, pd.get_dummies(train_test['X_ExpOutsideHomeCountry'],drop_first=True)],axis=1)\n",
    "train_test = pd.concat([train_test, pd.get_dummies(train_test['X_CrossFunctionalExperience'],drop_first=True)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Identify the people that are in the cities with the largest # of employees\n",
    "\n",
    "def footprint(col):\n",
    "    city=col[0]\n",
    "    \n",
    "    if city in['INDIANAPOLIS','CARMEL','FISHERS','GREENWOOD','ZIONSVILLE','MADRID',\n",
    "              'AVON','GREENFIELD','BROWNSBURG','WESTFIELD','CO CORK',\"SAO PAULO\",\n",
    "              \"LIVERPOOL\",\"CAROLINA\",\"FIRENZE\",\"PLAINFIELD\",\"MARTINSVILLE\",\"MOORESVILLE\",\n",
    "              \"NEW PALESTINE\",\"NOBLESVILLE\",\"STRASBOURG\",\"SESTO FIORENTINO\",\"FRANKLIN\",\"TERRE HAUTE\",\n",
    "              \"SAN DIEGO\"]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "train_test['footprint'] = train_test[['X_City2009']].apply(footprint, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code Low Engagement for 2009\n",
    "train_test['S_PMB_ENG_L_2009'] = train_test['S_PMB_Engagement_2009'].apply(lambda x: 1 if x<2.9 else 0)\n",
    "\n",
    "# Code Low Teamwork for 2009\n",
    "train_test['S_PMB_TEA_L_2009'] = train_test['S_PMB_Teamwork_2009'].apply(lambda x: 1 if x<2.9 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Mean/Target encode jobfunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mean encode turnover rate for Sub Function 2009\n",
    "jobfunction_turn_2009 = train_test.groupby('X_JobFunction2009')['Y_Exit'].mean()\n",
    "\n",
    "train_test['jobfunction_turn_2009'] = train_test['X_JobFunction2009'].map(jobfunction_turn_2009)\n",
    "\n",
    "# mean encode turnover rate for Sub Function 2008\n",
    "jobfunction_turn_2008 = train_test.groupby('X_JobFunction2008')['Y_Exit'].mean()\n",
    "\n",
    "train_test['jobfunction_turn_2008'] = train_test['X_JobFunction2008'].map(jobfunction_turn_2008)\n",
    "\n",
    "# mean encode turnover rate for Sub Function 2007\n",
    "jobfunction_turn_2007 = train_test.groupby('X_JobFunction2007')['Y_Exit'].mean()\n",
    "\n",
    "train_test['jobfunction_turn_2007'] = train_test['X_JobFunction2007'].map(jobfunction_turn_2007)\n",
    "\n",
    "# mean encode turnover rate for Sub Function 2006\n",
    "jobfunction_turn_2006 = train_test.groupby('X_JobFunction2006')['Y_Exit'].mean()\n",
    "\n",
    "train_test['jobfunction_turn_2006'] = train_test['X_JobFunction2006'].map(jobfunction_turn_2006)\n",
    "\n",
    "# mean encode turnover rate for Sub Function 2005\n",
    "jobfunction_turn_2005 = train_test.groupby('X_JobFunction2005')['Y_Exit'].mean()\n",
    "\n",
    "train_test['jobfunction_turn_2005'] = train_test['X_JobFunction2005'].map(jobfunction_turn_2005)\n",
    "\n",
    "# mean encode turnover rate for Sub Function 2004\n",
    "jobfunction_turn_2004 = train_test.groupby('X_JobFunction2004')['Y_Exit'].mean()\n",
    "\n",
    "train_test['jobfunction_turn_2004'] = train_test['X_JobFunction2004'].map(jobfunction_turn_2004)\n",
    "\n",
    "# mean encode turnover rate overall average jobfunction\n",
    "\n",
    "train_test['jobfunction_mean_turn_overall'] = train_test[['jobfunction_turn_2009','jobfunction_turn_2008',\n",
    "                                   'jobfunction_turn_2007','jobfunction_turn_2006',\n",
    "                                    'jobfunction_turn_2005','jobfunction_turn_2004']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace all countries with fewer than 20 employees as an 'other' country to introduce 'noise' in potentially a high cardinality variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replace_values_2009 = {'Oman':'Other', 'Syria':'Other', 'Bahama':'Other','Jamaica': 'Other','Bahrain': 'Other',\n",
    "                   'Netherlands Antilles': 'Other','El Savador':'Other','Bosnia':'Other',\n",
    "                  'Chile':'Other','New Zealand':'Other','Kazakhstan':'Other','Morocco':'Other',\n",
    "                   'Ecuador':'Other','Indonesia':'Other','Ukraine':'Other','Cyprus':'Other','Estonia':'Other',\n",
    "                   'Tunisia':'Other','Panama':'Other','Guatemala':'Other','Kuwait':'Other','Qatar':'Other',\n",
    "                   'Jordan':'Other','Dominican Republic':'Other','Latvia':'Other',\n",
    "                   'Vietnam':'Other','Honduras':'Other','Trinidad and Tobago':'Other'}\n",
    "\n",
    "train_test = train_test.replace({'X_Country2009': replace_values_2009})\n",
    "\n",
    "replace_values_2008 = {'United Arab Emirates':'Other','Slovakia':'Other','Oman':'Other', 'Syria':'Other',\n",
    "                  'New Zealand':'Other','Kazakhstan':'Other','Costa Rica':'Other','Morocco':'Other',\n",
    "                  'Ukraine':'Other','Indonesia':'Other','Ecuador':'Other','Cyprus':'Other','Estonia':'Other',\n",
    "                   'Tunisia':'Other','Jordan':'Other','Panama':'Other','Kuwait':'Other','Trinidad and Tobago':'Other',\n",
    "                  'Honduras':'Other','Guatemala':'Other','Qatar':'Other','Dominican Republic':'Other','Latvia':'Other',\n",
    "                  'Netherlands Antilles': 'Other','El Savador':'Other','Bosnia':'Other','Bahamas':'Other',\n",
    "                  'Jamaica': 'Other','Uzbekistan':'Other','Vietnam':'Other','Lithuania':'Other',\n",
    "                      'Slovenia':'Other','Chile':'Other'}\n",
    "train_test = train_test.replace({'X_Country2008': replace_values_2008}) \n",
    "\n",
    "replace_values_2007 = {'Bahamas':'Other','Uzbekistan':'Other','Oman':'Other','Jamaica': 'Other',\n",
    "                       'Netherlands Antilles': 'Other','Qatar':'Other', 'Syria':'Other','El Savador':'Other',\n",
    "                       'Bosnia':'Other','Panama':'Other','Vietnam':'Other','Dominican Republic':'Other',\n",
    "                       'Guatemala':'Other','Trinidad and Tobago':'Other','Kuwait':'Other', 'Honduras':'Other',\n",
    "                       'Jordan':'Other','Latvia':'Other','Ecuador':'Other','Cyprus':'Other','Estonia':'Other',\n",
    "                       'Tunisia':'Other','Ukraine':'Other','United Arab Emirates':'Other','Morocco':'Other',\n",
    "                       'Indonesia':'Other','Kazakhstan':'Other','Costa Rica':'Other','Slovakia':'Other',\n",
    "                       'New Zealand':'Other','Lithuania':'Other','Slovenia':'Other','Chile':'Other','Croatia':'Other'}\n",
    "train_test = train_test.replace({'X_Country2007': replace_values_2007}) \n",
    "\n",
    "replace_values_2006 = {'Dominican Republic':'Other','Serbia':'Other','Oman':'Other','Syria':'Other',\n",
    "                       'Trinidad and Tobago':'Other','Bahamas':'Other','Uzbekistan':'Other','Jamaica': 'Other',\n",
    "                       'Netherlands Antilles': 'Other','Qatar':'Other', 'El Savador':'Other',\n",
    "                       'Bosnia':'Other','Guatemala':'Other','Panama':'Other','Jordan':'Other','Indonesia':'Other',\n",
    "                       'Estonia':'Other','Cyprus':'Other','Tunisia':'Other','Kazakhstan':'Other','Ukraine':'Other',\n",
    "                       'United Arab Emirates':'Other','Slovakia':'Other','New Zealand':'Other','Morocco':'Other',\n",
    "                       'Costa Rica':'Other','Vietnam':'Other','Kuwait':'Other','Ecuador':'Other','Honduras':'Other',\n",
    "                      'Latvia':'Other','Lithuania':'Other','Slovenia':'Other','Chile':'Other','Croatia':'Other',\n",
    "                      'Algeria':'Other','Bulgaria':'Other'}\n",
    "\n",
    "train_test = train_test.replace({'X_Country2006': replace_values_2006}) \n",
    "\n",
    "replace_values_2005 = {'Oman':'Other','Syria':'Other','Jamaica': 'Other','Ecuador':'Other','Dominican Republic':'Other',\n",
    "                       'Serbia':'Other','Netherlands Antilles': 'Other','El Savador':'Other','Bosnia':'Other',\n",
    "                       'Kuwait':'Other','Guatemala':'Other','Estonia':'Other','Trinidad and Tobago':'Other',\n",
    "                       'Vietnam':'Other','Panama':'Other','Honduras':'Other','Latvia':'Other','Jordan':'Other',\n",
    "                       'Indonesia':'Other','Cyprus':'Other','Tunisia':'Other','Kazakhstan':'Other','Ukraine':'Other',\n",
    "                       'Costa Rica':'Other','United Arab Emirates':'Other','Slovakia':'Other','New Zealand':'Other',\n",
    "                       'Chile':'Other','Lithuania':'Other','Algeria':'Other','Morocco':'Other','Croatia':'Other',\n",
    "                      'Slovenia':'Other','Bulgaria':'Other'}\n",
    "\n",
    "train_test = train_test.replace({'X_Country2005': replace_values_2005}) \n",
    "\n",
    "replace_values_2004 = {'Oman':'Other','Ecuador':'Other','Dominican Republic':'Other','Syria':'Other',\n",
    "                       'Jamaica': 'Other','Serbia':'Other','Netherlands Antilles': 'Other','Guatemala':'Other',\n",
    "                       'Kuwait':'Other','Vietnam':'Other','Panama':'Other','El Savador':'Other','Bosnia':'Other',\n",
    "                       'Estonia':'Other','Trinidad and Tobago':'Other','Honduras':'Other','Latvia':'Other',\n",
    "                       'Kazakhstan':'Other','Indonesia':'Other','Cyprus':'Other','Jordan':'Other',\n",
    "                       'Tunisia':'Other','United Arab Emirates':'Other','Ukraine':'Other',\n",
    "                       'Costa Rica':'Other','Slovakia':'Other','New Zealand':'Other',\n",
    "                       'Chile':'Other','Lithuania':'Other','Morocco':'Other','Algeria':'Other','Croatia':'Other',\n",
    "                      'Slovenia':'Other','Bulgaria':'Other'}\n",
    "\n",
    "train_test = train_test.replace({'X_Country2004': replace_values_2004}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Mean/Target encode country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mean encode turnover rate for country\n",
    "\n",
    "country_turn_2009 = train_test.groupby('X_Country2009')['Y_Exit'].mean()\n",
    "\n",
    "train_test['country_turn_2009'] = train_test['X_Country2009'].map(country_turn_2009)\n",
    "\n",
    "# mean encode turnover rate for country 2008\n",
    "\n",
    "country_turn_2008 = train_test.groupby('X_Country2008')['Y_Exit'].mean()\n",
    "\n",
    "train_test['country_turn_2008'] = train_test['X_Country2008'].map(country_turn_2008)\n",
    "\n",
    "# mean encode turnover rate for country 2007\n",
    "\n",
    "country_turn_2007 = train_test.groupby('X_Country2007')['Y_Exit'].mean()\n",
    "\n",
    "train_test['country_turn_2007'] = train_test['X_Country2007'].map(country_turn_2007)\n",
    "\n",
    "# mean encode turnover rate for country 2006\n",
    "\n",
    "country_turn_2006 = train_test.groupby('X_Country2006')['Y_Exit'].mean()\n",
    "\n",
    "train_test['country_turn_2006'] = train_test['X_Country2006'].map(country_turn_2006)\n",
    "\n",
    "# mean encode turnover rate for country 2005\n",
    "\n",
    "country_turn_2005 = train_test.groupby('X_Country2005')['Y_Exit'].mean()\n",
    "\n",
    "train_test['country_turn_2005'] = train_test['X_Country2005'].map(country_turn_2005)\n",
    "\n",
    "# mean encode turnover rate for country 2004\n",
    "\n",
    "country_turn_2004 = train_test.groupby('X_Country2004')['Y_Exit'].mean()\n",
    "\n",
    "train_test['country_turn_2004'] = train_test['X_Country2004'].map(country_turn_2004)\n",
    "\n",
    "# mean encode turnover rate overall average country\n",
    "\n",
    "train_test['country_mean_turn_overall'] = train_test[['country_turn_2009','country_turn_2008',\n",
    "                                   'country_turn_2007','country_turn_2006',\n",
    "                                    'country_turn_2005','country_turn_2004']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the end of our feature engineering. At this point we have all the features we will use in our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data on train_test sets to build the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train_test[train_test['train_test_split']=='Train']\n",
    "test = train_test[train_test['train_test_split']=='Test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = train.drop('Y_Exit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Elimination/Selection\n",
    "\n",
    "After creating all of these additional variables (including the dummy variables) you are left with over 5k. In order to identify which are going to be most useful we used a method in the scikit learn feature selection package called Recursive Feature Elimination. \n",
    "\n",
    "Here is information from the documentation: \n",
    "\n",
    "- Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature importances attribute. Then, the least important features are pruned from current set of features.That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n",
    "\n",
    "[Link](http://scikit-learn.org/stable/modules/feature_selection.html) to documentation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### In this example I used LightGBM because it trains significantly faster than XGBoost, but you can replace that model with XGboost, Random Forest, etc. We also only selectively included cities with more than 15 employees in 2009, to reduce the number of features the model had to eliminate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = lgb.LGBMClassifier(n_estimators=150, n_jobs=-1)\n",
    "selector = RFE(estimator,step=1,n_features_to_select=1, verbose=3)\n",
    "selector = selector.fit(X_train,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this will print out the order of importance for all features included\n",
    "lgbm = sorted(zip(map(lambda x: round(x, 4), selector.ranking_), names))\n",
    "lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build X_train, y_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train\n",
    "\n",
    "X_train = train[['X_OverallPerformanceRating2009','X_OverallPerformanceRating2008',\n",
    "                          'X_OverallPerformanceRating2007',\n",
    "                          'comp_perf2009','comp_perf2008','comp_perf2007',\n",
    "                          'Comp_PM_Change2009','Comp_PM_Change2008','Comp_PM_Change2007',\n",
    "                          \"X_TenureDays2009\",'X_ReadyNow','unique_supervisors','sup_ten_inter',\n",
    "                         'unique_countries','unique_cities','unique_jobs', 'Male',\n",
    "                         'Asian','Black/African American','EXP Yes',\n",
    "                         'Hispanic/Latino','Other','White/Caucasian',\n",
    "                         'SALES REPRESENTATIVE','BUSINESS ASSOCIATE',\"Algeria\",\"Argentina\",\"Australia\",\n",
    "                         \"Belgium\",\"Brazil\",\"Canada\",\"China\",\"Czech Rebublic\",\"Egypt\",\"France\",\"Germany\",\n",
    "                         \"Greece\",\"India\",\"Indonesia\",\"Ireland\",\"Italy\",\"Japan\",\"Lebanon\",\"Malaysia\",\"Mexico\",\n",
    "                         \"Morocco\",\"Netherlands\",\"New Zealand\",\"Norway\",\"Pakistan\",\"Peru\",\"Philippines\",\n",
    "                        \"Poland\",\"Portugal\",\"Puerto Rico\",\"Romania\",\"Russia\",\"Saudia Arabia\",\"Singapore\",\"Slovakia\",\n",
    "                        \"South Africa\",\"South Korea\",\"Spain\",\"Thailand\",\"United States of America\",\"Venezuela\",\n",
    "                       'footprint','S_OPR_N_2009','ADMINISTRATIVE ASSISTANT/SECRETARY','ATTORNEY','BIOLOGIST',\n",
    "                       'CHEMIST','CLERICAL ASSISTANT','CRAFTS/TRADES WORKER','MANAGEMENT (G-LEVEL)','OTHER SCIENTIST',\n",
    "                       'OUTCOME/MEDICAL LIAISON','PHYSICIAN','PRODUCTION OPERATOR','SALES MANAGER','STATISTICIAN',\n",
    "                       'TECHNICIAN','ACCOUNTING/REPORTING','AFFILIATE LEAD POSITION - PHARMA','BULK MANUFACTURING OPERATIONS',\n",
    "                       'COMMUNICATIONS/COMMUNITY RELATIONS','COMPENSATION/BENEFITS/RELOCATION','CORPORATE FUNDED MEDICAL',\n",
    "                      'DISCOVERY RESEARCH/RESEARCH TECHNOLOGIES','ENVIRONMENTAL/HEALTH/SAFETY','EXECUTIVE MANAGEMENT ADMIN SUPPORT',\n",
    "                     'FACILITY/CORPORATE SECURITY/ERT','GENERAL LAW','GLOBAL COMPLIANCE AND ETHICS','GLOBAL HEALTH OUTCOMES',\n",
    "                     'GOVERNMENT/PUBLIC/ADVOCACY','IT BUSINESS INTEGRATION/AFF','LRL EXTERNAL R&D','MAINTENANCE',\n",
    "                     'MANUFACTURING TECHNICAL SERVICES','MARKETING-PHARMA','MKT/SALES EXEC/ADMIN SUPPORT-PHARMA',\n",
    "                     'PACKAGING','PATENT LAW','PLANNING/CONTROLLER','PRODUCT/PROCESS DEVELOPMENT','PROJECT MANAGEMENT',\n",
    "                     'QUALITY ASSURANCE','QUALITY CONTROL','REGULATORY','SALES - PHARMA','SALES/MARKETING SUPPORT',\n",
    "                     'SCIENCE/TECHNOLOGY GEN ADMIN/ADMIN SUPPT','TOXICOLOGY/ADME','TRAINING/DEVELOPMENT',\n",
    "                    'sup_emp_gender_dif_2009','sup_emp_gender_dif_2008','paygrade_changes','S_OPRC_Mean',\n",
    "                    'S_PMB_ENG_L_2009','S_PMB_TEA_L_2009','X_FutureGeneration','INDIANAPOLIS','MADRID',\n",
    "                    'S_Age2007','S_OPR_COMP_2006','S_OPR_COMP_2008','S_PMB_SHA_N_2005','S_PayGradeLevel2005',\n",
    "                    'S_PayGradeLevel2009','S_TenureDays2004','S_TenureDays2008','S_OPR_COMP_2007','S_PMB_Teamwork_2009',\n",
    "                    'X_PMB_ModelValues_2008','S_TenureDays2006','X_PMB_Teamwork_2009','S_OPR_COMP_2009',\n",
    "                    'S_Age2008','X_Age2009','X_OverallPerformanceRating2005','S_PayGradeLevel2007','S_Age2009',\n",
    "                'Comp_STD_Perf','Comp_Avg_Perf','age_tenure_inter', 'Overall_STD_Perf',\n",
    "                    'jobfunction_mean_turn_overall','country_mean_turn_overall']]\n",
    "\n",
    "\n",
    "y_train = train['Y_Exit']\n",
    "\n",
    "X_test = test[['X_OverallPerformanceRating2009','X_OverallPerformanceRating2008',\n",
    "                          'X_OverallPerformanceRating2007',\n",
    "                          'comp_perf2009','comp_perf2008','comp_perf2007',\n",
    "                          'Comp_PM_Change2009','Comp_PM_Change2008','Comp_PM_Change2007',\n",
    "                          \"X_TenureDays2009\",'X_ReadyNow','unique_supervisors','sup_ten_inter',\n",
    "                         'unique_countries','unique_cities','unique_jobs', 'Male',\n",
    "                         'Asian','Black/African American','EXP Yes',\n",
    "                         'Hispanic/Latino','Other','White/Caucasian',\n",
    "                         'SALES REPRESENTATIVE','BUSINESS ASSOCIATE',\"Algeria\",\"Argentina\",\"Australia\",\n",
    "                         \"Belgium\",\"Brazil\",\"Canada\",\"China\",\"Czech Rebublic\",\"Egypt\",\"France\",\"Germany\",\n",
    "                         \"Greece\",\"India\",\"Indonesia\",\"Ireland\",\"Italy\",\"Japan\",\"Lebanon\",\"Malaysia\",\"Mexico\",\n",
    "                         \"Morocco\",\"Netherlands\",\"New Zealand\",\"Norway\",\"Pakistan\",\"Peru\",\"Philippines\",\n",
    "                        \"Poland\",\"Portugal\",\"Puerto Rico\",\"Romania\",\"Russia\",\"Saudia Arabia\",\"Singapore\",\"Slovakia\",\n",
    "                        \"South Africa\",\"South Korea\",\"Spain\",\"Thailand\",\"United States of America\",\"Venezuela\",\n",
    "                       'footprint','S_OPR_N_2009','ADMINISTRATIVE ASSISTANT/SECRETARY','ATTORNEY','BIOLOGIST',\n",
    "                       'CHEMIST','CLERICAL ASSISTANT','CRAFTS/TRADES WORKER','MANAGEMENT (G-LEVEL)','OTHER SCIENTIST',\n",
    "                       'OUTCOME/MEDICAL LIAISON','PHYSICIAN','PRODUCTION OPERATOR','SALES MANAGER','STATISTICIAN',\n",
    "                       'TECHNICIAN','ACCOUNTING/REPORTING','AFFILIATE LEAD POSITION - PHARMA','BULK MANUFACTURING OPERATIONS',\n",
    "                       'COMMUNICATIONS/COMMUNITY RELATIONS','COMPENSATION/BENEFITS/RELOCATION','CORPORATE FUNDED MEDICAL',\n",
    "                      'DISCOVERY RESEARCH/RESEARCH TECHNOLOGIES','ENVIRONMENTAL/HEALTH/SAFETY','EXECUTIVE MANAGEMENT ADMIN SUPPORT',\n",
    "                     'FACILITY/CORPORATE SECURITY/ERT','GENERAL LAW','GLOBAL COMPLIANCE AND ETHICS','GLOBAL HEALTH OUTCOMES',\n",
    "                     'GOVERNMENT/PUBLIC/ADVOCACY','IT BUSINESS INTEGRATION/AFF','LRL EXTERNAL R&D','MAINTENANCE',\n",
    "                     'MANUFACTURING TECHNICAL SERVICES','MARKETING-PHARMA','MKT/SALES EXEC/ADMIN SUPPORT-PHARMA',\n",
    "                     'PACKAGING','PATENT LAW','PLANNING/CONTROLLER','PRODUCT/PROCESS DEVELOPMENT','PROJECT MANAGEMENT',\n",
    "                     'QUALITY ASSURANCE','QUALITY CONTROL','REGULATORY','SALES - PHARMA','SALES/MARKETING SUPPORT',\n",
    "                     'SCIENCE/TECHNOLOGY GEN ADMIN/ADMIN SUPPT','TOXICOLOGY/ADME','TRAINING/DEVELOPMENT',\n",
    "                    'sup_emp_gender_dif_2009','sup_emp_gender_dif_2008','paygrade_changes','S_OPRC_Mean',\n",
    "                    'S_PMB_ENG_L_2009','S_PMB_TEA_L_2009','X_FutureGeneration','INDIANAPOLIS','MADRID',\n",
    "                    'S_Age2007','S_OPR_COMP_2006','S_OPR_COMP_2008','S_PMB_SHA_N_2005','S_PayGradeLevel2005',\n",
    "                    'S_PayGradeLevel2009','S_TenureDays2004','S_TenureDays2008','S_OPR_COMP_2007','S_PMB_Teamwork_2009',\n",
    "                    'X_PMB_ModelValues_2008','S_TenureDays2006','X_PMB_Teamwork_2009','S_OPR_COMP_2009',\n",
    "                    'S_Age2008','X_Age2009','X_OverallPerformanceRating2005','S_PayGradeLevel2007','S_Age2009',\n",
    "                'Comp_STD_Perf','Comp_Avg_Perf','age_tenure_inter', 'Overall_STD_Perf',\n",
    "                    'jobfunction_mean_turn_overall','country_mean_turn_overall']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Grid Search for XGBoost\n",
    "\n",
    "In order to identify the best hyper parameters to use for this specific dataset we performed a randomized version of grid search. This utilizes cross-validation and randomly selects a different subset of the hyperparameters for each iteration. We set this to run 200 unique iterations. \n",
    "\n",
    "[Link](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) to documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from time import time\n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify parameters and distributions to sample from\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(n_jobs=-1)\n",
    "param_dist = {\"learning_rate\":np.linspace(.01,.15,20),\n",
    "              \"max_depth\": [1,3,7,12,15],\n",
    "              'min_child_weight': sp_randint(1,7),\n",
    "              \"reg_lambda\":[0.0,0.1,1,5],\n",
    "              \"reg_alpha\":[0,0.1,1,5],\n",
    "              \"n_estimators\":[100,250,500,750]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run randomized search\n",
    "n_iter_search = 200\n",
    "random_search = RandomizedSearchCV(lgbm, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search,scoring='roc_auc',verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a report for the output\n",
    "\n",
    "start = time()\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "report(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After completing the grid search we using a bagging methodology to smooth the outputs and include the hyperparameters from the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=1.5, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=5, missing=None, n_estimators=500,\n",
       "       n_jobs=-1, nthread=None, objective='binary:logistic',\n",
       "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=True, subsample=1)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(gamma=1.5,learning_rate=0.1,max_depth=3,min_child_weight=5,n_estimators=500,n_jobs=-1)\n",
    "xgb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bagged Predictions for XGBoost Model\n",
    "\n",
    "xgb_model = XGBClassifier(gamma=1.5,learning_rate=0.1,max_depth=3,min_child_weight=5,n_estimators=500)\n",
    "bags = 10\n",
    "seed = 1\n",
    "\n",
    "xgb_bagged_prediction = np.zeros(X_test.shape[0])\n",
    "\n",
    "for n in range(0,bags):\n",
    "    xgb_model.set_params(random_state=seed + n)\n",
    "    xgb_model.fit(X_train,y_train)\n",
    "    preds=xgb_model.predict_proba(X_test)[:,1]\n",
    "    xgb_bagged_prediction+=preds\n",
    "\n",
    "xgb_bagged_prediction/=bags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Grid Search for LightGBM\n",
    "\n",
    "In order to identify the best hyper parameters to use for this specific dataset we performed a randomized version of grid search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "lgbm = lgb.LGBMClassifier(n_jobs=-1)\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"learning_rate\":np.linspace(.009,.20,50),\n",
    "              \"max_depth\": [-1,3,5,7,10,12,15],\n",
    "              'min_child_samples': sp_randint(2,20),\n",
    "              \"reg_lambda\":[0.0,0.01,0.1],\n",
    "              \"n_estimators\": [100,250,500,750,1000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run randomized search\n",
    "n_iter_search = 250\n",
    "random_search = RandomizedSearchCV(lgbm, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search,scoring='roc_auc',verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "report(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After completing the grid search we using a bagging methodology to smooth the outputs and include the hyperparameters from the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bagged Predictions for LightGBM Model\n",
    "lgbm_model = lgb.LGBMClassifier(n_estimators=500,min_child_samples=3,max_depth=-1,\n",
    "                         learning_rate=0.013,n_jobs=-1)\n",
    "bags = 10\n",
    "seed = 1\n",
    "\n",
    "lgbm_bagged_prediction = np.zeros(X_test.shape[0])\n",
    "\n",
    "for n in range(0,bags):\n",
    "    lgbm_model.set_params(random_state=seed + n)\n",
    "    lgbm_model.fit(X_train,y_train)\n",
    "    preds=lgbm_model.predict_proba(X_test)[:,1]\n",
    "    lgbm_bagged_prediction+=preds\n",
    "\n",
    "lgbm_bagged_prediction/=bags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests and Neural Networks\n",
    "\n",
    "- Both these models can not work with missing data. For this reason we will replace all missing values with 0 (you could also impute the mean, or median value as well). For the purposes of this particular project we felt imputing missing data wasn't an accurate representation because often times the data was missing because the person actually wasn't an employee at the time the data was collected. We also tried several models early on where we imputed the job performance variables for 2009 (if missing) with the means for the particular job function. It didn't add any real value so we went back to just using zeros. \n",
    "- In addition to this because neural networks compute gradients (which can be computationally complex) they prefer standardized values. So we will standardize the input features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nckoeni/anaconda/lib/python3.6/site-packages/pandas/core/frame.py:2852: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n",
      "/Users/nckoeni/anaconda/lib/python3.6/site-packages/pandas/core/generic.py:3549: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "#standardizing data and replacing all missing values with zeros for Random Forest and Neural Network\n",
    "# hashed out the validation sets, but use them when building a train/test split.\n",
    "\n",
    "X_train.fillna(0,inplace=True)\n",
    "#X_val.fillna(0,inplace=True)\n",
    "X_test.fillna(0,inplace=True)\n",
    "\n",
    "y_train.fillna(0,inplace=True)\n",
    "#y_val.fillna(0,inplace=True)\n",
    "#y_test.fillna(0,inplace=True)\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "#X_val = sc.fit_transform(X_val)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Grid Search for Random Forest\n",
    "\n",
    "In order to identify the best hyper parameters to use for this specific dataset we performed a randomized version of grid search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify parameters and distributions to sample from\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "param_dist = {\"max_features\":['auto','sqrt','log2',None],\n",
    "              \"max_depth\": [None,1,3,7,12,15],\n",
    "              'min_samples_split': sp_randint(2,15),\n",
    "              \"n_estimators\":[50,100,250,500,750]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run randomized search\n",
    "n_iter_search = 200\n",
    "random_search = RandomizedSearchCV(rf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search,scoring='roc_auc',verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "report(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After completing the grid search we using a bagging methodology to smooth the outputs and include the hyperparameters from the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=750,max_depth=15,max_features='sqrt',min_samples_split=11)\n",
    "bags = 10\n",
    "seed = 1\n",
    "\n",
    "rf_bagged_prediction = np.zeros(X_test.shape[0])\n",
    "\n",
    "for n in range(0,bags):\n",
    "    rf_model.set_params(random_state=seed + n)\n",
    "    rf_model.fit(X_train,y_train)\n",
    "    preds=rf_model.predict_proba(X_test)[:,1]\n",
    "    rf_bagged_prediction+=preds\n",
    "\n",
    "rf_bagged_prediction/=bags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Model\n",
    "\n",
    "\n",
    "I tested several different hyperparamaters (# of neurons, activation functions, dropout %, # of hidden layers). I settled on a model with 3 hidden layers and cascading neurons in each layer. I also found that using dropout (at 40% for each layer) resulted in a good amount of regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24205, 149)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "# print shape to get the number of features for the input layer\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "# Adding the input layer and the first hidden layer\n",
    "# input_dim needs to match the shape of the data, hence input_dim=149\n",
    "classifier.add(Dense(units = 25, kernel_initializer = 'uniform', activation = 'tanh', input_dim = 149))\n",
    "classifier.add(Dropout(0.40))\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 10, kernel_initializer = 'uniform', activation = 'tanh'))\n",
    "classifier.add(Dropout(0.40))\n",
    "# Adding the third hidden layer\n",
    "classifier.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'tanh'))\n",
    "classifier.add(Dropout(0.40))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24205/24205 [==============================] - 2s - loss: 0.5234 - acc: 0.7923     \n",
      "Epoch 2/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.4163 - acc: 0.8266     \n",
      "Epoch 3/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.4071 - acc: 0.8289     \n",
      "Epoch 4/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.4003 - acc: 0.8268     \n",
      "Epoch 5/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3987 - acc: 0.8254     \n",
      "Epoch 6/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3964 - acc: 0.8266     \n",
      "Epoch 7/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3947 - acc: 0.8270     \n",
      "Epoch 8/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3905 - acc: 0.8271     \n",
      "Epoch 9/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3897 - acc: 0.8250     \n",
      "Epoch 10/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3891 - acc: 0.8255     \n",
      "Epoch 11/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3889 - acc: 0.8264     \n",
      "Epoch 12/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3876 - acc: 0.8276     \n",
      "Epoch 13/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3867 - acc: 0.8274     \n",
      "Epoch 14/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3867 - acc: 0.8273     \n",
      "Epoch 15/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3884 - acc: 0.8254     \n",
      "Epoch 16/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3855 - acc: 0.8254     \n",
      "Epoch 17/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3857 - acc: 0.8259     \n",
      "Epoch 18/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3856 - acc: 0.8257     \n",
      "Epoch 19/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3837 - acc: 0.8267     \n",
      "Epoch 20/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3810 - acc: 0.8290     \n",
      "Epoch 21/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3849 - acc: 0.8271     \n",
      "Epoch 22/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3851 - acc: 0.8245     \n",
      "Epoch 23/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3832 - acc: 0.8265     \n",
      "Epoch 24/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3851 - acc: 0.8236     \n",
      "Epoch 25/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3813 - acc: 0.8276     \n",
      "Epoch 26/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3825 - acc: 0.8269     \n",
      "Epoch 27/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3815 - acc: 0.8273     \n",
      "Epoch 28/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3787 - acc: 0.8285     \n",
      "Epoch 29/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3808 - acc: 0.8284     \n",
      "Epoch 30/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3807 - acc: 0.8276     \n",
      "Epoch 31/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3793 - acc: 0.8272     \n",
      "Epoch 32/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3791 - acc: 0.8288     \n",
      "Epoch 33/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3792 - acc: 0.8282     \n",
      "Epoch 34/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3800 - acc: 0.8277     \n",
      "Epoch 35/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3783 - acc: 0.8279     \n",
      "Epoch 36/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3767 - acc: 0.8304     \n",
      "Epoch 37/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3789 - acc: 0.8274     \n",
      "Epoch 38/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3774 - acc: 0.8284     \n",
      "Epoch 39/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3760 - acc: 0.8290     \n",
      "Epoch 40/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3773 - acc: 0.8284     \n",
      "Epoch 41/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3756 - acc: 0.8288     \n",
      "Epoch 42/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3782 - acc: 0.8282     \n",
      "Epoch 43/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3759 - acc: 0.8298     \n",
      "Epoch 44/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3735 - acc: 0.8301     \n",
      "Epoch 45/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3768 - acc: 0.8262     \n",
      "Epoch 46/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3760 - acc: 0.8295     \n",
      "Epoch 47/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3756 - acc: 0.8280     \n",
      "Epoch 48/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3737 - acc: 0.8307     \n",
      "Epoch 49/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3753 - acc: 0.8310     \n",
      "Epoch 50/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3766 - acc: 0.8279     \n",
      "Epoch 51/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3731 - acc: 0.8322     \n",
      "Epoch 52/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3760 - acc: 0.8280     \n",
      "Epoch 53/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3729 - acc: 0.8299     \n",
      "Epoch 54/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3755 - acc: 0.8307     \n",
      "Epoch 55/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3719 - acc: 0.8327     \n",
      "Epoch 56/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3733 - acc: 0.8330     \n",
      "Epoch 57/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3743 - acc: 0.8297     \n",
      "Epoch 58/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3719 - acc: 0.8266     \n",
      "Epoch 59/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3731 - acc: 0.8297     \n",
      "Epoch 60/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3724 - acc: 0.8309     \n",
      "Epoch 61/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3726 - acc: 0.8307     \n",
      "Epoch 62/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3709 - acc: 0.8333     \n",
      "Epoch 63/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3707 - acc: 0.8307     \n",
      "Epoch 64/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3720 - acc: 0.8302     \n",
      "Epoch 65/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3715 - acc: 0.8322     \n",
      "Epoch 66/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3727 - acc: 0.8300     \n",
      "Epoch 67/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3704 - acc: 0.8316     \n",
      "Epoch 68/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3699 - acc: 0.8326     \n",
      "Epoch 69/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3729 - acc: 0.8311     \n",
      "Epoch 70/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3732 - acc: 0.8299     \n",
      "Epoch 71/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3691 - acc: 0.8331     \n",
      "Epoch 72/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3698 - acc: 0.8306     \n",
      "Epoch 73/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3682 - acc: 0.8332     \n",
      "Epoch 74/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3700 - acc: 0.8316     \n",
      "Epoch 75/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3672 - acc: 0.8338     \n",
      "Epoch 76/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3689 - acc: 0.8314     \n",
      "Epoch 77/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3690 - acc: 0.8310     \n",
      "Epoch 78/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3691 - acc: 0.8323     \n",
      "Epoch 79/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3660 - acc: 0.8332     \n",
      "Epoch 80/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3688 - acc: 0.8325     \n",
      "Epoch 81/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3688 - acc: 0.8325     \n",
      "Epoch 82/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3705 - acc: 0.8319     \n",
      "Epoch 83/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3664 - acc: 0.8324     \n",
      "Epoch 84/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3663 - acc: 0.8345     \n",
      "Epoch 85/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3672 - acc: 0.8333     \n",
      "Epoch 86/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3701 - acc: 0.8323     \n",
      "Epoch 87/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3671 - acc: 0.8325     \n",
      "Epoch 88/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3656 - acc: 0.8320     \n",
      "Epoch 89/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3666 - acc: 0.8343     \n",
      "Epoch 90/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3654 - acc: 0.8328     \n",
      "Epoch 91/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3664 - acc: 0.8342     \n",
      "Epoch 92/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3634 - acc: 0.8370     \n",
      "Epoch 93/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3670 - acc: 0.8340     \n",
      "Epoch 94/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3694 - acc: 0.8326     \n",
      "Epoch 95/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3688 - acc: 0.8289     \n",
      "Epoch 96/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3662 - acc: 0.8328     \n",
      "Epoch 97/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3670 - acc: 0.8340     \n",
      "Epoch 98/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3659 - acc: 0.8338     \n",
      "Epoch 99/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3667 - acc: 0.8344     \n",
      "Epoch 100/100\n",
      "24205/24205 [==============================] - 1s - loss: 0.3659 - acc: 0.8338     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x128c5b390>"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size = 64, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ANN_test = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape the ANN to be the same shape as the other models\n",
    "ANN_test = np.reshape(ANN_test,(8091,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importances\n",
    "\n",
    "Specific to XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats = {} # a dict to hold feature_name: feature_importance\n",
    "for feature, importance in zip(X_train.columns, xgb.feature_importances_):\n",
    "    feats[feature] = importance #add the name/value pair \n",
    "\n",
    "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize']=24,10\n",
    "figure = importances.sort_values(by='Gini-importance').iloc[-15:,:].plot(kind='bar', rot=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put the features together in one numpy array and then turn that numpy array into a dataframe with the column names for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked_test_features = np.column_stack((xgb_bagged_prediction,rf_bagged_prediction,lgbm_bagged_prediction,ANN_test))\n",
    "stacked_test_df = pd.DataFrame(stacked_test_features, columns=['XGB','RF','LGBM', 'ANN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked_test_df['Y_ExitProbability'] = ((stacked_test_df['XGB']*.35)+(stacked_test_df['LGBM']*.3)+\n",
    "                                        (stacked_test_df['RF']*.05)+\n",
    "                                        (stacked_test_df['ANN']*.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_stack = np.column_stack((test['Global_ID'],stacked_test_df['Y_ExitProbability']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to get an idea of the distribution of the predictions\n",
    "\n",
    "- The mean probability was just over 18.5% which is in line with the turnover in the training set.\n",
    "- The distribution has a lot near zero and cascades downwards, which would we expect. \n",
    "- If we assign a cutoff of 0.5 and examine the number of people with probabilities above that we see 782, which is a little low, but still a decent amount of people. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Global_ID</th>\n",
       "      <th>Y_ExitProbability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.091000e+03</td>\n",
       "      <td>8091.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.088606e+06</td>\n",
       "      <td>0.185228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.004817e+06</td>\n",
       "      <td>0.183492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.061000e+03</td>\n",
       "      <td>0.011227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.096690e+05</td>\n",
       "      <td>0.041824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.025910e+05</td>\n",
       "      <td>0.110269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.058672e+06</td>\n",
       "      <td>0.279188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.506159e+06</td>\n",
       "      <td>0.858831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Global_ID  Y_ExitProbability\n",
       "count  8.091000e+03        8091.000000\n",
       "mean   1.088606e+06           0.185228\n",
       "std    1.004817e+06           0.183492\n",
       "min    7.061000e+03           0.011227\n",
       "25%    1.096690e+05           0.041824\n",
       "50%    8.025910e+05           0.110269\n",
       "75%    2.058672e+06           0.279188\n",
       "max    8.506159e+06           0.858831"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_stack_df = pd.DataFrame(final_stack, columns = ['Global_ID','Y_ExitProbability'])\n",
    "final_stack_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Submission_30 = final_stack_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x127615898>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAFXCAYAAABtOQ2RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x127612f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(Submission_30['Y_ExitProbability'], bins=20, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people with predicted probabilities above 0.5: 782\n"
     ]
    }
   ],
   "source": [
    "print('Number of people with predicted probabilities above 0.5:',\n",
    "      np.count_nonzero(Submission_30['Y_ExitProbability'][Submission_30['Y_ExitProbability']>=0.50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Submission_30.to_csv('AnEnrichingMeal022618_Entry30.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix of the probabilities from each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAckAAAFJCAYAAADwj9apAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERGxoSApIiJiQ0FSRETEhoKkiIiIjXf7W3HosVb8/AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12764aef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame({'XGBoost': xgb_bagged_prediction,'Rand_For':rf_bagged_prediction,'LGBM':lgbm_bagged_prediction,\n",
    "                   'ANN':ANN_test})\n",
    "\n",
    "sns.heatmap(df.corr(),annot=True);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
