{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4f0b42b-10f9-4a4e-bb69-2c16b6432ca8",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "## Transformer\n",
    "\n",
    "Author: Binghen Wang\n",
    "\n",
    "Last Updated: 6 Feb, 2023\n",
    "\n",
    "<nav>\n",
    "    <b>Deep learning navigation:</b> <a href=\"./Deep Learning Basics.ipynb\">Deep Learning Basics</a> |\n",
    "    <a href=\"./Deep Learning Optimization.ipynb\">Optimization</a> |\n",
    "    <a href=\"./Convolutional Neural Networks.ipynb\">Convolutional Neural Networks</a>\n",
    "    <br>\n",
    "    <b>RNN navigation:</b> <a href=\"./Recurrent Neural Networks.ipynb\">Basics</a> |\n",
    "    <a href=\"./Natural Language Processing.ipynb\">Natural Language Processing</a>\n",
    "</nav>\n",
    "\n",
    "---\n",
    "<nav>\n",
    "    <a href=\"../Machine%20Learning.ipynb\">Machine Learning</a> |\n",
    "    <a href=\"../Supervised Learning/Supervised%20Learning.ipynb\">Supervised Learning</a>\n",
    "</nav>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77d76c8-aab1-4e0d-a8fb-5d84e157c8b1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Original paper:\n",
    "<ul>\n",
    "    <li><a href = \"https://arxiv.org/pdf/1706.03762.pdf\">Attention Is All You Need. (Vaswani et al., 2017)</a>\n",
    "</ul>\n",
    "    \n",
    "Nice explanations:\n",
    "<ul>\n",
    "    <li><a href = \"https://gb.coursera.org/learn/nlp-sequence-models\">Sequential Models, Course 5. (DeepLearning.AI)</a>\n",
    "    <li><a href= \"https://people.tamu.edu/~sji/classes/attn.pdf\">A Mathematical View of Attention Models in Deep Learning. (Ji, Gao & Xie, 2019)</a>\n",
    "</ul>\n",
    "This note consolidates the information based on the above sources.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df16612e-2d6d-4703-bd12-e1a1e995bfde",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- Attention Mechanism\n",
    "- Multi-Headed Attention\n",
    "- The Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d1f1fc-b8a5-4cba-9e1f-d991132d1c1a",
   "metadata": {},
   "source": [
    "## Attention Mechanism\n",
    "<blockquote>\n",
    "    Given a set of $n$ query vectors $q_1,q_2,\\dots,q_n \\in \\mathbb{R}^d$, $m$ key vectors $k_1,k_2,\\dots,k_m \\in \\mathbb{R}^d$, and $m$ value vectors $v_1,v_2,\\dots,v_m \\in \\mathbb{R}^p$,the <b>attention mechanism</b> computes a set of output vectors $o_1,o_2,\\dots,o_n \\in \\mathbb{R}^q$ by linearly combining the $g$-transformed value vectors $g(v_i) \\in \\mathbb{R}^q$ using the relations between the corresponding query vector and each key vector as coefficients. Formally,\n",
    "    $$\n",
    "    o_j = \\frac{1}{C}\\sum_{i=1}^{m} f(q_j,k_i) g(v_i),\n",
    "    $$\n",
    "where $f(q_j,k_i)$ charaterizes the relation (e.g., similarity) between $q_j$ and $k_i$, $g(\\cdot)$ is commonly a linear transformation as $g(v_i) = W_v v_i \\in \\mathbb{R}^q$, where $W_v \\in \\mathbb{R}^{q\\times p}$, and $C = \\sum_{i=1}^{m} f(q_j,k_i) $ is a normalization factor. A commonly used similarity function is the embedded Gaussian, defined as $ f(q_j,k_i) = \\exp\\left(\\theta(q_j)^{\\mathsf{T}}\\phi(k_i)\\right)$, where $\\theta(\\cdot)$ and $\\phi(\\cdot)$ are commonly linear transformations as $\\theta(q_j)= W_q q_j$ and $\\phi (k_i) = W_k k_i$.\n",
    "    <div style =\"text-align:right\">â€” Ji, Gao & Xie, 2019</div>\n",
    "</blockquote>\n",
    "\n",
    "Here for **notational consistency** with Vaswani et al. (2017), we **redefine** the linear transformations with their transformed versions such that $W_v \\in \\mathbb{R}^{p\\times q}$, $W_q \\in \\mathbb{R}^{d\\times d}$ and $W_k \\in \\mathbb{R}^{d\\times d}$.\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/attention mechanism.png\" style=\"width:80%;\" >\n",
    "</div>\n",
    "\n",
    "The dimension of the output matrix $O$ is governed by the number of queries $n$ and the dimension of linearly transformed value vectors $q$.\n",
    "\n",
    "### Self-Attention\n",
    "In **self-attention**, $Q=K=V=X$, i.e. inputs are all from the same matrix embeddings.\n",
    "$$\n",
    "O = \\text{softmax}\\left((QW_q){(KW_k)}^T\\right)VW_v\n",
    "$$\n",
    "### Scaled Dot-Product Attention\n",
    "While dot product attention mechanism is **much faster and more space-efficient** than additive attention (via a feed-forward neural network), it does not work as well for larger values of $d$, potentially due to large magnitudes of the dot products (Vaswani et al., 2017). Recall the vanishing gradient problem. In response, Vaswani et al. (2017) propose **scaled dot-product attention**.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "O =& \\text{softmax}\\left(\\frac{\\tilde{Q}\\tilde{K}^{T}}{\\sqrt{d}}\\right)\\tilde{V} \\\\\n",
    "=& \\text{softmax}\\left(\\frac{(QW_q){(KW_k)}^{T}}{\\sqrt{d}}\\right)VW_v\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Attention with Learnable Query\n",
    "As a common variant of the self-attention, **attention with learnable query** does not require an input for $Q$ and learns it entirely as trainable variables:\n",
    "$$\n",
    "O = \\text{softmax}\\left(\\tilde{Q}_{\\text{trainable}}{(KW_k)}^T\\right)VW_v\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bce550-87bf-4b92-b8e9-2a5bd0f3927e",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "**Multi-head attention** consists of multiple attention operators with different queries, keys and values, allowing for specialization in each head (attention computation). Given a M-head attention, the output of the $i$-th attention is computed (using scaled dot-product attention) as:\n",
    "$$\n",
    "H_i = \\text{softmax}\\left(\\frac{(QW_q^{(i)}){(KW_k^{(i)})}^{T}}{\\sqrt{d}}\\right)VW_v^{(i)} \\in \\mathbb{R}^{n \\times q_i}\n",
    "$$\n",
    "Technically speaking, heads could differ in the lengths of output vectors $q_i$.\n",
    "\n",
    "The final output is computed by concatenating the heads and then matrix-multiply it by a weight matrix $W_o$ (linear transformation):\n",
    "$$\n",
    "O=\\underbrace{\\left[\\begin{array}{ccccc} H_1 & H_2 & \\cdots & H_{M-1} & H_M\\end{array}\\right]}_{n\\times\\sum_i q_i} W_o\n",
    "$$\n",
    "where $W_o \\in \\mathbb{R}^{\\sum_i q_i \\times q}$ projects the concatenated heads into the desired dimensionality $q$.\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/multi-head attention.png\" style=\"width:80%;\" >\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6209a160-2aba-43f7-8f06-b644b74204ee",
   "metadata": {},
   "source": [
    "## The Transformer\n",
    "\n",
    "The Transformer relies on **self-attention** and **point-wise feed-forward networks** to compute representations of its input and output **without using sequence-aligned RNNs or convolution**.\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/Transformer.png\" style=\"width:50%;\" ><br>\n",
    "    Source: <a href = \"https://arxiv.org/pdf/1706.03762.pdf\">Attention Is All You Need. (Vaswani et al., 2017)</a>\n",
    "</div>\n",
    "\n",
    "### Positional Encoding\n",
    "Since the Transformer does not use recurrence nor convolution, additional **positional encoding** is incorporated into the model architecture so that the model could take into account information about the relative and absolute positions of the tokens in the sequence. \n",
    "\n",
    "Two main types of positional encodings are: 1) learned 2) fixed. Vaswani et al. (2017) use sinusoidal functions of different frequences to encode positional information:\n",
    "$$\n",
    "\\begin{align}\n",
    "PE_{(pos, 2i)} = & \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right) \\\\\n",
    "PE_{(pos, 2i+1)} = & \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d}}}\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/positional encoding.png\" style=\"width:70%;\" >\n",
    "</div>\n",
    "\n",
    "**Property 1: Each positional encoding vector has a constant norm.**\n",
    "\n",
    "For an even number of dimensions $d$, the resulting position encoding vectors have a **constant norm** regardless of the position:\n",
    "$$\n",
    "\\Vert PE_{(pos)} \\Vert = \\sqrt{\\frac{d}{2}}\n",
    "$$\n",
    "due to the fact that \n",
    "$$\\sin^2\\theta + \\cos^2 \\theta = 1.$$\n",
    "\n",
    "**Property 2: The distance between two positional encoding vectors depends on the relative positions of words irrespective of the absolute positions of each word.**\n",
    "\n",
    "To see this, consider\n",
    "$$\n",
    "\\begin{align}\n",
    "&{\\left[\\sin{(\\theta+ k)} - \\sin{\\theta}\\right]}^2 + {\\left[\\cos{(\\theta+ k)} - \\cos{\\theta}\\right]}^2 \\\\\n",
    "= & \\;2 - 2\\left[\\sin{(\\theta+ k)}\\sin{\\theta}+\\cos{(\\theta+ k)}\\cos{\\theta}\\right] \\\\\n",
    "= & \\;2- 2\\cos{(\\theta + k - \\theta)} \\\\\n",
    "= & \\;2- 2\\cos{k} \\left( = 4\\sin\\left(\\frac{k}{2}\\right) \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "which does not depend on $\\theta$.\n",
    "\n",
    "Link: <a href = \"https://en.wikipedia.org/wiki/List_of_trigonometric_identities\">List of trigonometric identities </a>\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/relative positions_positional encoding.png\" style=\"width:100%;\" >\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datascience)",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
