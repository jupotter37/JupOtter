{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal concepts, preamble & glossary\n",
    "\n",
    "## Types of Machine Learning\n",
    "There are three fundamental types of problems that machine learning algorithms are trying to solve.\n",
    "\n",
    "### Supervised\n",
    "This is where we have some examples of a given input and its output and we are trying to model that function based on the data.\n",
    "\n",
    "####  &nbsp;&nbsp;&nbsp;&nbsp; Regression\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;The output of our model is a number or vector of numbers which can take any real value. An example is trying to predict the price of a house based on a number of features such as number of rooms, number of windows, etc.\n",
    "\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;Classification\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;The output of our model, based on the inputs, is the probability of the input belonging to a class. An example is having an input image and outputting if it is a hotdog or not.\n",
    "\n",
    "### Unsupervised\n",
    "There is no input-output relationship which we are trying to model. Rather, we want to find some hidden structure in the data. One type is clustering in which you are trying to group your data. A use case could be segmenting your customers by their interests so you can target them with different material.\n",
    "\n",
    "### Reinforcement Learning\n",
    "In reinforcement learning, we have an agent, which takes in observations from an environment and takes actions based on those observations to maximize a reward function. This is inspired by pavlovian conditioning which has been shown to be the method that mammals learn by."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datapoint\n",
    "\n",
    "Here a single data point, $x^{(1)}$ is represented as a row vector where each column is a different **feature**. \n",
    "\n",
    "For example, if each training example is a house, then its vector of features may include elements for its price, no. rooms, no. windows etc.\n",
    "\n",
    "### $X^{(1)} =  \\begin{bmatrix} x^{(1)}_1 & x^{(1)}_2 & \\dots & x^{(1)}_{n-1}& x^{(1)}_n \\end{bmatrix}$\n",
    "\n",
    "## Design Matrix\n",
    "The **design matrix**, **X** contains all of our training data. Each row represents a certain example. There are $m$ training examples. Each column represents a different feature. There are $n$ features. Hence the design matrix has dimensions of $n$ by $m$.\n",
    "\n",
    "### $Design \\ matrix,\\ X = \\begin{bmatrix} \\dots & x^{(1)} &\\dots \\\\  & \\vdots &  \\\\ \\dots & x^{(m)} & \\dots \\end{bmatrix} = \\begin{bmatrix} x_{11} \\dots x_{1n} \\\\ \\vdots \\ddots \\vdots \\\\ x_{m1} \\dots x_{mn} \\end{bmatrix} \\in m \\times n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "The hypothesis, $h$ is the output of your model. It is your current prediction of the mapping from input to output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss/cost function\n",
    "\n",
    "\n",
    "For our algorithms to learn, we need a way to evaluate their current performance, so that we can determine how to improve. We can mathematically define when our algorithm is performing well by evaluating an appropriate objective function. We usually try to minimise a function which indicates the error in our hypothesis (how bad our model is). We will represent the loss of our models with the symbol $J$. The cost function is dependent on as many dimensions as we have parameters (which are relevant to that loss function). Changing these parameters moves us around parameter space, in which the cost varies. Varying different parameters will have varying influence on how the cost changes - as such, some are more important to optimise.\n",
    "\n",
    "#### Mean Squared Error Loss\n",
    "MSE loss is the average over all training points of the squared error between your hypothesis and the label. The factor of $\\frac{1}{2}$ is often included to cancel with the power of 2 when differentiated so that no constants are present.\n",
    "\n",
    "### $ J =\\frac{1}{2m} \\sum_{i=1}^{m}(h^{(i)} - y^{(i)})^2$\n",
    "\n",
    "#### Binary Cross Entropy loss\n",
    "BCE loss is used to calculate error for classification tasks. \n",
    "\n",
    "### $ J = \\sum_{i=1}^{m} - y^{(i)} \\cdot \\text{log}(h^{(i)}) + (1-y^{(i)}) \\cdot \\text{log}(1-h^{(i)})$\n",
    "\n",
    "In classification tasks, for each class the label of a datapoint can only take binary values of 0 or 1; i.e. it *is* a member of that class or it *is not* a member of that class, and the output is usually a *confidence* value $\\in [0, 1]$.\n",
    "When , $y, = 0$ the first term is 'turned off' and the second term \n",
    "\n",
    "#### Kullback-Leibler Divergence\n",
    "The KL divergence is a metric that quantifies the difference between two probability distributions, $p$ & $q$. It is used frequently in machine learning to measure the information lost when we try to represent a probability distribution in a different way (e.g. after reconstructing it from an encoding).\n",
    "\n",
    "### $D_{KL}(p||q) = \\sum_{i=1}^{m} p(x_i)\\cdot (\\text{log }p(x_i) - \\text{log }q(x_i)) = \\sum_{i=1}^{m} p(x_i)\\cdot \\text{log } \\frac{p(x_i)}{q(x_i)}$\n",
    "\n",
    "##### Practically, for a normal distribution, the KL-divergence can be evaluated as : $D_{KL} = \\sum_i (\\sigma^2 + $\n",
    "\n",
    "For a single datapoint, $x$, the KL divergence tests how similar the log probabilities of that value are and weights that difference by the value of the probability of sampling that $x$ from $p(x)$. The weighting $p(x)$ of the log difference makes the KL divergence different depending on which arrangement you compare the probability distributions in.\n",
    "\n",
    "Consider:\n",
    "- It takes large values when the sampled probabilities for the same values are more different, and the weighting probability distribution $p(x)$ is larger. \n",
    "- It takes a value of zero where the weighting probability distribution is zero.\n",
    "- Where the \n",
    "- The aim is often to minimise the KL divergence (the information difference between two probability distributions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent and the learning rate\n",
    "Gradient descent is the most popular optimization strategy currently used in machine learning. It has proven to be very effective even when there are millions of parameters to optimize as in the case of deep learning.<br>\n",
    "Lets say we have a function\n",
    "### $J = f(x, y;\\theta)$\n",
    "which we are trying to minimize by finding optimal values of $\\theta$.<br>\n",
    "\n",
    "Gradient descent works by moving the weights that control a model in a direction that most decreases the cost. What is this direction? The gradient of a function at a point is a vector pointing in the direction which it increases fastest locally. So the direction which most *decreases* the cost function, is the negative gradient - in this case, it will be an $n+1$ (features and bias) dimensional vector containing the partial derivatives of the cost with respect to each of these model parameters.\n",
    "\n",
    "The negative gradient tells us the correct direction to move each weight in, but not the ideal size of the step.\n",
    "\n",
    "If we move the parameters by the value of the negative gradient, there is a chance that they may jump straight over the minima, perhaps to a point where the gradient is even higher! This can happen because the gradient can be greater than the distance of the parameter from its optimal position. This causes divergence, of the model parameters, instead of convergence. \n",
    "So, in gradient descent, we iteratively update the weights *proportionally* to the negative gradient at their local position. This proportionality constant, which the gradient is multiplied by to get the step size, is called the **learning rate**. The learning rate should be large enough so that the algorithm converges at a suitable rate, but small enough enough to ensure that it does not diverge.\n",
    "\n",
    "At a minima, the parameter should stabilise because the step size is proportional to the gradient, which will be zero.\n",
    "\n",
    "We can utilize the gradient descent strategy only if $J$ is a differentiable function.<br>\n",
    "We first start by initializing $\\theta$ randomly. We then calculate $J$ and the derivative of $J$ w.r.t $\\theta$. \n",
    "Once we have $\\frac{\\partial J}{\\partial\\theta}$, we update $\\theta$ using the following update rule: \n",
    "### $\\theta := \\theta - \\alpha\\frac{\\partial J}{\\partial\\theta}$\n",
    "\n",
    "While it is important to understand the equations, it is equally important to have an intuitive understanding of what is going on. What we are doing when we calulate the partial derivative of $J$ w.r.t $\\theta_i$ is we are finding out how a small increase in $\\theta_i$ affects $J$. If this leads to an increase in $J$ we decrease our $\\theta_i$ as we are trying to reduce $J$. If it leads to a decrease in $J$, we increase our $\\theta_i$. This explains the negative sign in the update rule. <br>\n",
    "\n",
    "This can easily be visualised in the case where we have two parameters. We have a surface and we are trying to find the lowest point on this surface which corresponds to the lowest value of $J$. We start at a random point on the surface and interatively calculate the direction of greatest ascent ($\\frac{\\partial J}{\\partial\\theta}$) before taking a step in the opposite direction. We scale our steps by a factor of $\\alpha$. We can set different values for $\\alpha$ at runtime and we will get different results. If our $\\alpha$ is too high, we will overshoot the minima and if it is too low, we will take too long to get to the minima.\n",
    "![](gradientdescent.png)\n",
    "\n",
    "\n",
    "\n",
    "### Pseudo code for SGD\n",
    "#### Randomly select a batch of datapoints from the training set to train on.\n",
    "#### Make a prediction of the output for those \n",
    "#### Evaluate the model's cost for these predictions\n",
    "#### Find rate of change of cost wrt model parameters\n",
    "#### Update the model parameters proportionately to the negative gradients found above, according to:\n",
    "\n",
    "### $ \\theta\\  \\dot{=}\\  \\theta - \\alpha \\frac{\\partial J}{\\partial \\theta}$\n",
    "\n",
    "#### Repeat for defined number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "#### One-hot\n",
    "\n",
    "If we have a classification problem, instead of having a word as a label, we can have a $K$-dimensional vector, where $K$ is the number of classes, and each element of that vector is zero except for one element that represents the true class label. This is a one-hot encoding. This is a way of representing a label numberically, using the same number of elements as there are classes.\n",
    "\n",
    "#### Embedding\n",
    "\n",
    "For one-hot encoding, each different possible label is a mutually orthogonal unit vector. All possible class labels make up a $K$-dimensional basis of vectors. This means that to be able to reach the whole range of our output space, we need a $K$- dimensional  output from our model. \n",
    "Alternatively, class vectors can be embedded into a lower dimensional subspace, where less than $K$-dimensional vectors can be the output of our model. This embedding is not binary (discrete), and now has a continuous range not limited within the range $[0, 1]$. \n",
    "\n",
    "Imagine you are training a model to predict the next word in a sentance. You don't want your ouput space to have as many dimensions as you have predictable words in your corpus. So instead, you can embed these words into a low dimensional subspace where each of them is a vector. Similar words will be closer to each other and vector algebra can be done on these vectors.\n",
    "\n",
    "\n",
    "![title](embedding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
