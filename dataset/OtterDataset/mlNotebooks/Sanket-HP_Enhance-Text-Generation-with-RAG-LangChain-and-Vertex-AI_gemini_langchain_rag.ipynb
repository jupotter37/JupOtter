{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2df5d726-4dd0-4158-bee0-f0abaa1bd6cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip -q install langchain_experimental langchain_core\n",
    "!pip -q install google-generativeai==0.3.1\n",
    "!pip -q install google-ai-generativelanguage==0.4.0\n",
    "!pip -q install langchain-google-genai\n",
    "!pip -q install wikipedia\n",
    "!pip -q install docarray\n",
    "!pip -q install --upgrade protobuf google.protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c42420d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Before proceeding with the next cells restart the kernel by clicking the refresh icon on the top toolbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "931ae5e6-7580-4d67-b7fd-3a875b8cf649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "from google.protobuf.empty_pb2 import Empty\n",
    "\n",
    "key_name = !gcloud services api-keys list --filter=\"gemini-api-key\" --format=\"value(name)\"\n",
    "key_name = key_name[0]\n",
    "\n",
    "api_key = !gcloud services api-keys get-key-string $key_name --location=\"us-central1\" --format=\"value(keyString)\"\n",
    "api_key = api_key[0]\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0538d742-6944-4041-9dde-073385a87b3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(name='models/chat-bison-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='PaLM 2 Chat (Legacy)',\n",
       "       description='A legacy text-only model optimized for chat conversations',\n",
       "       input_token_limit=4096,\n",
       "       output_token_limit=1024,\n",
       "       supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
       "       temperature=0.25,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/text-bison-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='PaLM 2 (Legacy)',\n",
       "       description='A legacy model that understands text and generates text as an output',\n",
       "       input_token_limit=8196,\n",
       "       output_token_limit=1024,\n",
       "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
       "       temperature=0.7,\n",
       "       max_temperature=None,\n",
       "       top_p=0.95,\n",
       "       top_k=40),\n",
       " Model(name='models/embedding-gecko-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Embedding Gecko',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=1024,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-1.0-pro-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro Latest',\n",
       "       description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
       "                    'model.'),\n",
       "       input_token_limit=30720,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.9,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-1.0-pro',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro',\n",
       "       description='The best model for scaling across a wide range of tasks',\n",
       "       input_token_limit=30720,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.9,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-pro',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro',\n",
       "       description='The best model for scaling across a wide range of tasks',\n",
       "       input_token_limit=30720,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.9,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-1.0-pro-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
       "       description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
       "                    'model that supports tuning.'),\n",
       "       input_token_limit=30720,\n",
       "       output_token_limit=2048,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
       "       temperature=0.9,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=None),\n",
       " Model(name='models/gemini-1.0-pro-vision-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro Vision',\n",
       "       description='The best image understanding model to handle a broad range of applications',\n",
       "       input_token_limit=12288,\n",
       "       output_token_limit=4096,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.4,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=32),\n",
       " Model(name='models/gemini-pro-vision',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.0 Pro Vision',\n",
       "       description='The best image understanding model to handle a broad range of applications',\n",
       "       input_token_limit=12288,\n",
       "       output_token_limit=4096,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=0.4,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=32),\n",
       " Model(name='models/gemini-1.5-pro-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Pro Latest',\n",
       "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
       "       input_token_limit=2097152,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-pro-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Pro 001',\n",
       "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
       "       input_token_limit=2097152,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-pro',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Pro',\n",
       "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
       "       input_token_limit=2097152,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-pro-exp-0801',\n",
       "       base_model_id='',\n",
       "       version='exp-0801',\n",
       "       display_name='Gemini 1.5 Pro Experimental 0801',\n",
       "       description='Mid-size multimodal model that supports up to 2 million tokens',\n",
       "       input_token_limit=2097152,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-flash-latest',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash Latest',\n",
       "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-flash-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash 001',\n",
       "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/gemini-1.5-flash',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Gemini 1.5 Flash',\n",
       "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
       "       input_token_limit=1048576,\n",
       "       output_token_limit=8192,\n",
       "       supported_generation_methods=['generateContent', 'countTokens'],\n",
       "       temperature=1.0,\n",
       "       max_temperature=2.0,\n",
       "       top_p=0.95,\n",
       "       top_k=64),\n",
       " Model(name='models/embedding-001',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Embedding 001',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/text-embedding-004',\n",
       "       base_model_id='',\n",
       "       version='004',\n",
       "       display_name='Text Embedding 004',\n",
       "       description='Obtain a distributed representation of a text.',\n",
       "       input_token_limit=2048,\n",
       "       output_token_limit=1,\n",
       "       supported_generation_methods=['embedContent'],\n",
       "       temperature=None,\n",
       "       max_temperature=None,\n",
       "       top_p=None,\n",
       "       top_k=None),\n",
       " Model(name='models/aqa',\n",
       "       base_model_id='',\n",
       "       version='001',\n",
       "       display_name='Model that performs Attributed Question Answering.',\n",
       "       description=('Model trained to return answers to questions that are grounded in provided '\n",
       "                    'sources, along with estimating answerable probability.'),\n",
       "       input_token_limit=7168,\n",
       "       output_token_limit=1024,\n",
       "       supported_generation_methods=['generateAnswer'],\n",
       "       temperature=0.2,\n",
       "       max_temperature=None,\n",
       "       top_p=1.0,\n",
       "       top_k=40)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [m for m in genai.list_models()]\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36455709-7797-4a28-a9a9-89a038e96b26",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using Gemini directly with Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d08290d-120a-489e-a4e3-46bfbd619bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I am Gemini, a multimodal AI model, developed by Google. I am designed to provide information and assist with a wide range of language-based tasks, such as answering questions, generating text, translating languages, and writing different kinds of creative content.\n",
       "\n",
       "Here are some of my capabilities:\n",
       "\n",
       "**Information Retrieval:**\n",
       "* Answering factual questions on a diverse range of topics\n",
       "* Providing summaries of complex information\n",
       "* Translating text between over 100 languages\n",
       "\n",
       "**Text Generation:**\n",
       "* Generating creative text, such as stories, poems, and songs\n",
       "* Writing different types of content, including articles, blog posts, and marketing copy\n",
       "* Summarizing long pieces of text\n",
       "\n",
       "**Language Understanding:**\n",
       "* Understanding the meaning and context of text\n",
       "* Identifying and extracting key information\n",
       "* Recognizing and responding to different types of user intent\n",
       "\n",
       "**Communication:**\n",
       "* Engaging in natural language conversations\n",
       "* Answering follow-up questions\n",
       "* Providing clear and concise explanations\n",
       "\n",
       "**Other Capabilities:**\n",
       "* Code generation\n",
       "* Math problem solving\n",
       "* Fact-checking\n",
       "* Sentiment analysis\n",
       "\n",
       "I am constantly learning and improving, and my capabilities are expanding all the time. I am not yet able to perform all tasks perfectly, but I am always striving to provide the best possible assistance.\n",
       "\n",
       "Please let me know if you have any other questions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate text\n",
    "prompt = 'Who are you and what can you do?'\n",
    "\n",
    "model = genai.GenerativeModel('gemini-pro')\n",
    "\n",
    "response = model.generate_content(prompt)\n",
    "\n",
    "Markdown(response.candidates[0].content.parts[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d214aef1-1aad-4469-a216-9d4a9b2cb278",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using Gemini with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b7f1ae-13f2-4a28-bd09-237eacea1106",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basic LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef52da8e-0cee-409b-8a37-5c2ddca42aa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LLM stands for Large Language Model. It is a type of artificial intelligence (AI) that is trained on a massive dataset of text and code. LLMs are able to understand and generate human-like text, translate languages, write different kinds of creative content, and even write different kinds of code. They are also able to answer questions and perform other tasks that require a deep understanding of language.\n",
       "\n",
       "LLMs are still under development, but they have already shown great promise for a wide range of applications, including customer service, education, and entertainment. As LLMs continue to improve, they are likely to play an increasingly important role in our lives.\n",
       "\n",
       "Here are some of the things that LLMs are capable of:\n",
       "* **Understanding and generating human-like text:** LLMs can understand the meaning of text and generate new text that is fluent, coherent, and informative. This makes them ideal for tasks such as writing articles, stories, and marketing copy.\n",
       "* **Translating languages:** LLMs can translate text between different languages with a high degree of accuracy. This makes them ideal for tasks such as customer service, travel, and education.\n",
       "* **Writing different kinds of creative content:** LLMs can write different kinds of creative content, such as poems, songs, and screenplays. This makes them ideal for tasks such as entertainment, education, and marketing.\n",
       "* **Answering questions:** LLMs can answer questions on a wide range of topics, including history, science, and current events. This makes them ideal for tasks such as customer service, education, and research.\n",
       "* **Performing other tasks that require a deep understanding of language:** LLMs can perform a variety of other tasks that require a deep understanding of language, such as summarizing text, identifying sentiment, and detecting plagiarism. This makes them ideal for tasks such as research, data analysis, and content moderation.\n",
       "\n",
       "LLMs are a powerful tool that can be used for a wide range of applications. As they continue to improve, they are likely to play an increasingly important role in our lives."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
    "                             temperature=0.7)\n",
    "\n",
    "\n",
    "result = llm.invoke(\"What is a LLM?\")\n",
    "\n",
    "Markdown(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "202f5831-31e8-429a-9a79-e26fb096f9d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words dance in the void,\n",
      "AI's boundless language stream,\n",
      "Thoughts\n",
      "---\n",
      " take flight anew.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(\"Write a haiku about LLMs.\"):\n",
    "    print(chunk.content)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1410317-db72-4c2a-99dc-279938a07074",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basic Multi Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b29cd459-e106-4407-8561-253d8b55ccf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e53795e-8b12-4166-82a9-e242f6cf80c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"tell me a short joke about {topic}\"\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38cc1c15-a8b9-48fc-a7cc-3cb9666fb822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "677fb13f-b6b1-407e-a4e0-16d16e31f636",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the machine learning algorithm get a speeding ticket?\\n\\nBecause it was caught accelerating too fast.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"machine learning\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0802e31-78d7-42fa-a830-7a5daeed7872",
   "metadata": {
    "tags": []
   },
   "source": [
    "## A more complicated Chain - RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3086e684-8311-484a-84c4-d58a415c3c36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e30de0-7dda-4997-9c54-c01d028e1c99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
    "                             temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba2d34a-2a31-48c5-9ce1-4940a731b340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39116cea-1261-4eeb-a69a-9acc5b0104b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "# use Wikipedia loader to create some docs to use..\n",
    "docs = WikipediaLoader(query=\"Machine Learning\", load_max_docs=10).load()\n",
    "docs += WikipediaLoader(query=\"Deep Learning\", load_max_docs=10).load() \n",
    "docs += WikipediaLoader(query=\"Neural Networks\", load_max_docs=10).load() \n",
    "\n",
    "# Take a look at a single document\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6bfe25-419d-4caa-80d0-4878fb67cdc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorstore = DocArrayInMemorySearch.from_documents(\n",
    "    docs,\n",
    "    embedding=embeddings # passing in the model to embed documents..\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2451a12-1e00-414a-a187-ab9675edb752",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(\"what is machine learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ec6628-e399-4d0a-bc71-b74eac511648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(\"what is gemini pro?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773eb751-451d-429a-ae3e-75287baea2f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question a a full sentence, based only on the following context:\n",
    "{context}\n",
    "\n",
    "Return you answer in three back ticks\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469a5efb-90ac-4b51-b323-cd5dceb95984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac53ade5-9662-4f5a-a033-99622809163f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever.get_relevant_documents(\"What is a graident boosted tree?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ec2b44-1798-4695-936c-7026b00117d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "}) | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b7b8f4-5426-4e87-8ac4-b58159cfa4b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain.invoke({\"question\": \"What is machine learning?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7751c12f-922e-4d75-8ce4-8533dc0c024b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain.invoke({\"question\": \"When was the transformer invented?\"})"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-16.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-16:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
