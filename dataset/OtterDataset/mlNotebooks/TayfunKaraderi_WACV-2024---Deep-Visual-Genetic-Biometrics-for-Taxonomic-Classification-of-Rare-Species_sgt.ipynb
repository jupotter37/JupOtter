{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TD_ouVtxyxgH"
   },
   "source": [
    "\n",
    "# Sequence Graph Transform (SGT) &mdash; Sequence Embedding for Clustering, Classification, and Search\n",
    "\n",
    "\n",
    "The following will cover,\n",
    "\n",
    "1. SGT Class Definition\n",
    "2. Installation\n",
    "3. Test Examples\n",
    "4. Sequence Clustering Example\n",
    "5. Sequence Classification Example\n",
    "6. Sequence Search Example\n",
    "7. SGT - Spark implementation for distributed computing\n",
    "8. Datasets\n",
    "\n",
    "\n",
    "## Definition\n",
    "\n",
    "Sequence Graph Transform (SGT) is a sequence embedding function. SGT extracts the short- and long-term sequence features and embeds them in a finite-dimensional feature space. The long and short term patterns embedded in SGT can be tuned without any increase in the computation.\"\n",
    "\n",
    "\n",
    "```class SGT():\n",
    "    '''\n",
    "    Compute embedding of a single or a collection of discrete item \n",
    "    sequences. A discrete item sequence is a sequence made from a set\n",
    "    discrete elements, also known as alphabet set. For example,\n",
    "    suppose the alphabet set is the set of roman letters, \n",
    "    {A, B, ..., Z}. This set is made of discrete elements. Examples of\n",
    "    sequences from such a set are AABADDSA, UADSFJPFFFOIHOUGD, etc.\n",
    "    Such sequence datasets are commonly found in online industry,\n",
    "    for example, item purchase history, where the alphabet set is\n",
    "    the set of all product items. Sequence datasets are abundant in\n",
    "    bioinformatics as protein sequences.\n",
    "    Using the embeddings created here, classification and clustering\n",
    "    models can be built for sequence datasets.\n",
    "    Read more in https://arxiv.org/pdf/1608.03533.pdf\n",
    "    '''\n",
    "```\n",
    "    Parameters\n",
    "    ----------\n",
    "    Input:\n",
    "\n",
    "    alphabets       Optional, except if mode is Spark. \n",
    "                    The set of alphabets that make up all \n",
    "                    the sequences in the dataset. If not passed, the\n",
    "                    alphabet set is automatically computed as the \n",
    "                    unique set of elements that make all the sequences.\n",
    "                    A list or 1d-array of the set of elements that make up the      \n",
    "                    sequences. For example, np.array([\"A\", \"B\", \"C\"].\n",
    "                    If mode is 'spark', the alphabets are necessary.\n",
    "\n",
    "    kappa           Tuning parameter, kappa > 0, to change the extraction of \n",
    "                    long-term dependency. Higher the value the lesser\n",
    "                    the long-term dependency captured in the embedding.\n",
    "                    Typical values for kappa are 1, 5, 10.\n",
    "\n",
    "    lengthsensitive Default false. This is set to true if the embedding of\n",
    "                    should have the information of the length of the sequence.\n",
    "                    If set to false then the embedding of two sequences with\n",
    "                    similar pattern but different lengths will be the same.\n",
    "                    lengthsensitive = false is similar to length-normalization.\n",
    "                    \n",
    "    flatten         Default True. If True the SGT embedding is flattened and returned as\n",
    "                    a vector. Otherwise, it is returned as a matrix with the row and col\n",
    "                    names same as the alphabets. The matrix form is used for            \n",
    "                    interpretation purposes. Especially, to understand how the alphabets\n",
    "                    are \"related\". Otherwise, for applying machine learning or deep\n",
    "                    learning algorithms, the embedding vectors are required.\n",
    "    \n",
    "    mode            Choices in {'default', 'multiprocessing'}. Note: 'multiprocessing' \n",
    "                    mode requires pandas==1.0.3+ and pandarallel libraries.\n",
    "    \n",
    "    processors      Used if mode is 'multiprocessing'. By default, the \n",
    "                    number of processors used in multiprocessing is\n",
    "                    number of available - 1.\n",
    "    '''\n",
    "\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    def fit(sequence)\n",
    "    \n",
    "    Extract Sequence Graph Transform features using Algorithm-2 in https://arxiv.org/abs/1608.03533.\n",
    "    Input:\n",
    "    sequence        An array of discrete elements. For example,\n",
    "                    np.array([\"B\",\"B\",\"A\",\"C\",\"A\",\"C\",\"A\",\"A\",\"B\",\"A\"].\n",
    "                    \n",
    "    Output: \n",
    "    sgt embedding   sgt matrix or vector (depending on Flatten==False or True) of the sequence\n",
    "    \n",
    "    \n",
    "    --\n",
    "    def fit_transform(corpus)\n",
    "    \n",
    "    Extract SGT embeddings for all sequences in a corpus. It finds\n",
    "    the alphabets encompassing all the sequences in the corpus, if not inputted. \n",
    "    However, if the mode is 'spark', then the alphabets list has to be\n",
    "    explicitly given in Sgt object declaration.\n",
    "    \n",
    "    Input:\n",
    "    corpus          A list of sequences. Each sequence is a list of alphabets.\n",
    "    \n",
    "    Output:\n",
    "    sgt embedding of all sequences in the corpus.\n",
    "    \n",
    "    \n",
    "    --\n",
    "    def transform(corpus)\n",
    "    \n",
    "    Find SGT embeddings of a new data sample belonging to the same population\n",
    "    of the corpus that was fitted initially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install SGT\n",
    "\n",
    "Install SGT in Python by running,\n",
    "\n",
    "```pip install sgt```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sgt\n",
    "sgt.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sgt import SGT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LGHFu5XMyxf-"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Authors: Chitta Ranjan <cran2367@gmail.com>\n",
    "#\n",
    "# License: BSD 3 clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ArnMbN8WyxgJ"
   },
   "source": [
    "## Illustrative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2766,
     "status": "ok",
     "timestamp": 1586797328329,
     "user": {
      "displayName": "Chitta Ranjan",
      "photoUrl": "",
      "userId": "03244148267376825421"
     },
     "user_tz": 240
    },
    "id": "ijpSrJdHyxgK",
    "outputId": "c6a7a965-cb65-40f4-e461-1bdfef415724"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from itertools import product as iterproduct\n",
    "import warnings\n",
    "\n",
    "import pickle\n",
    "\n",
    "########\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import sklearn.metrics\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(7) # fix random seed for reproducibility\n",
    "\n",
    "# from sgt import Sgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35655,
     "status": "ok",
     "timestamp": 1586957385910,
     "user": {
      "displayName": "Chitta Ranjan",
      "photoUrl": "",
      "userId": "03244148267376825421"
     },
     "user_tz": 240
    },
    "id": "rkUTErOrBs16",
    "outputId": "3cb322c9-697f-4629-93bf-e773663f409c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "\n",
    "root_path = 'gdrive/My Drive/Work/Research/Sequence Clustering/sgt'\n",
    "import os\n",
    "os.chdir(root_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P3MHa5AeyxgS"
   },
   "source": [
    "## Installation Test Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rcm7t0hVCgzD"
   },
   "source": [
    "In the following, there are a few test examples to verify the installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 295,
     "status": "ok",
     "timestamp": 1586632282382,
     "user": {
      "displayName": "Chitta Ranjan",
      "photoUrl": "",
      "userId": "03244148267376825421"
     },
     "user_tz": 240
    },
    "id": "iZM6-HVEyxgT",
    "outputId": "861aa835-fd79-40fd-a0fb-fc1085c7bd85"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.090616</td>\n",
       "      <td>0.131002</td>\n",
       "      <td>0.261849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.086569</td>\n",
       "      <td>0.123042</td>\n",
       "      <td>0.052544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.137142</td>\n",
       "      <td>0.028263</td>\n",
       "      <td>0.135335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B         C\n",
       "A  0.090616  0.131002  0.261849\n",
       "B  0.086569  0.123042  0.052544\n",
       "C  0.137142  0.028263  0.135335"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Learning a sgt embedding as a matrix with \n",
    "# rows and columns as the sequence alphabets. \n",
    "# This embedding shows the relationship between \n",
    "# the alphabets. The higher the value the \n",
    "# stronger the relationship.\n",
    "\n",
    "sgt = SGT(flatten=False)\n",
    "sequence = np.array([\"B\",\"B\",\"A\",\"C\",\"A\",\"C\",\"A\",\"A\",\"B\",\"A\"])\n",
    "sgt.fit(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1586632282975,
     "user": {
      "displayName": "Chitta Ranjan",
      "photoUrl": "",
      "userId": "03244148267376825421"
     },
     "user_tz": 240
    },
    "id": "luUMNGLDRhba",
    "outputId": "8824f3d5-fbf5-403d-c2a0-f2782af99388"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(A, A)    0.090616\n",
       "(A, B)    0.131002\n",
       "(A, C)    0.261849\n",
       "(B, A)    0.086569\n",
       "(B, B)    0.123042\n",
       "(B, C)    0.052544\n",
       "(C, A)    0.137142\n",
       "(C, B)    0.028263\n",
       "(C, C)    0.135335\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SGT embedding to a vector. The vector\n",
    "# embedding is useful for directly applying\n",
    "# a machine learning algorithm.\n",
    "\n",
    "sgt = SGT(flatten=True)\n",
    "sequence = np.array([\"B\",\"B\",\"A\",\"C\",\"A\",\"C\",\"A\",\"A\",\"B\",\"A\"])\n",
    "sgt.fit(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 155,
     "status": "ok",
     "timestamp": 1586632283669,
     "user": {
      "displayName": "Chitta Ranjan",
      "photoUrl": "",
      "userId": "03244148267376825421"
     },
     "user_tz": 240
    },
    "id": "1kLt8JqpyxgX",
    "outputId": "4f9da432-814d-45ea-9911-27115e488266"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[B, B, A, C, A, C, A, A, B, A]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[C, Z, Z, Z, D]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                        sequence\n",
       "0   1  [B, B, A, C, A, C, A, A, B, A]\n",
       "1   2                 [C, Z, Z, Z, D]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "SGT embedding on a corpus of sequences.\n",
    "Test the two processing modes within the\n",
    "SGT class: 'default', 'multiprocessing'.\n",
    "\n",
    "'''\n",
    "\n",
    "# A sample corpus of two sequences.\n",
    "corpus = pd.DataFrame([[1, [\"B\",\"B\",\"A\",\"C\",\"A\",\"C\",\"A\",\"A\",\"B\",\"A\"]], \n",
    "                       [2, [\"C\", \"Z\", \"Z\", \"Z\", \"D\"]]], \n",
    "                      columns=['id', 'sequence'])\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 322,
     "status": "ok",
     "timestamp": 1586632284532,
     "user": {
      "displayName": "Chitta Ranjan",
      "photoUrl": "",
      "userId": "03244148267376825421"
     },
     "user_tz": 240
    },
    "id": "62v1uFkiYXmo",
    "outputId": "0d47ba98-aab6-4be2-e50d-3ce9c41bdb71"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>(A, A)</th>\n",
       "      <th>(A, B)</th>\n",
       "      <th>(A, C)</th>\n",
       "      <th>(A, D)</th>\n",
       "      <th>(A, Z)</th>\n",
       "      <th>(B, A)</th>\n",
       "      <th>(B, B)</th>\n",
       "      <th>(B, C)</th>\n",
       "      <th>(B, D)</th>\n",
       "      <th>...</th>\n",
       "      <th>(D, A)</th>\n",
       "      <th>(D, B)</th>\n",
       "      <th>(D, C)</th>\n",
       "      <th>(D, D)</th>\n",
       "      <th>(D, Z)</th>\n",
       "      <th>(Z, A)</th>\n",
       "      <th>(Z, B)</th>\n",
       "      <th>(Z, C)</th>\n",
       "      <th>(Z, D)</th>\n",
       "      <th>(Z, Z)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.090616</td>\n",
       "      <td>0.131002</td>\n",
       "      <td>0.261849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086569</td>\n",
       "      <td>0.123042</td>\n",
       "      <td>0.052544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.184334</td>\n",
       "      <td>0.290365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id    (A, A)    (A, B)    (A, C)  (A, D)  (A, Z)    (B, A)    (B, B)  \\\n",
       "0  1.0  0.090616  0.131002  0.261849     0.0     0.0  0.086569  0.123042   \n",
       "1  2.0  0.000000  0.000000  0.000000     0.0     0.0  0.000000  0.000000   \n",
       "\n",
       "     (B, C)  (B, D)  ...  (D, A)  (D, B)  (D, C)  (D, D)  (D, Z)  (Z, A)  \\\n",
       "0  0.052544     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1  0.000000     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   (Z, B)  (Z, C)    (Z, D)    (Z, Z)  \n",
       "0     0.0     0.0  0.000000  0.000000  \n",
       "1     0.0     0.0  0.184334  0.290365  \n",
       "\n",
       "[2 rows x 26 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Learning the sgt embeddings as vector for\n",
    "# all sequences in a corpus.\n",
    "# mode: 'default'\n",
    "sgt = SGT(kappa=1, \n",
    "          flatten=True, \n",
    "          lengthsensitive=False, \n",
    "          mode='default')\n",
    "sgt.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 348,
     "status": "ok",
     "timestamp": 1586632299011,
     "user": {
      "displayName": "Chitta Ranjan",
      "photoUrl": "",
      "userId": "03244148267376825421"
     },
     "user_tz": 240
    },
    "id": "nzigpRL95TMC",
    "outputId": "94d34790-e361-4fa8-c516-c39f27198e9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 7 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>(A, A)</th>\n",
       "      <th>(A, B)</th>\n",
       "      <th>(A, C)</th>\n",
       "      <th>(A, D)</th>\n",
       "      <th>(A, Z)</th>\n",
       "      <th>(B, A)</th>\n",
       "      <th>(B, B)</th>\n",
       "      <th>(B, C)</th>\n",
       "      <th>(B, D)</th>\n",
       "      <th>...</th>\n",
       "      <th>(D, A)</th>\n",
       "      <th>(D, B)</th>\n",
       "      <th>(D, C)</th>\n",
       "      <th>(D, D)</th>\n",
       "      <th>(D, Z)</th>\n",
       "      <th>(Z, A)</th>\n",
       "      <th>(Z, B)</th>\n",
       "      <th>(Z, C)</th>\n",
       "      <th>(Z, D)</th>\n",
       "      <th>(Z, Z)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.090616</td>\n",
       "      <td>0.131002</td>\n",
       "      <td>0.261849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.086569</td>\n",
       "      <td>0.123042</td>\n",
       "      <td>0.052544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.184334</td>\n",
       "      <td>0.290365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id    (A, A)    (A, B)    (A, C)  (A, D)  (A, Z)    (B, A)    (B, B)  \\\n",
       "0  1.0  0.090616  0.131002  0.261849     0.0     0.0  0.086569  0.123042   \n",
       "1  2.0  0.000000  0.000000  0.000000     0.0     0.0  0.000000  0.000000   \n",
       "\n",
       "     (B, C)  (B, D)  ...  (D, A)  (D, B)  (D, C)  (D, D)  (D, Z)  (Z, A)  \\\n",
       "0  0.052544     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1  0.000000     0.0  ...     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   (Z, B)  (Z, C)    (Z, D)    (Z, Z)  \n",
       "0     0.0     0.0  0.000000  0.000000  \n",
       "1     0.0     0.0  0.184334  0.290365  \n",
       "\n",
       "[2 rows x 26 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Learning the sgt embeddings as vector for\n",
    "# all sequences in a corpus.\n",
    "# mode: 'multiprocessing'\n",
    "\n",
    "import pandarallel  # required library for multiprocessing\n",
    "\n",
    "sgt = SGT(kappa=1, \n",
    "          flatten=True, \n",
    "          lengthsensitive=False,\n",
    "          mode='multiprocessing')\n",
    "\n",
    "sgt.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Clustering\n",
    "\n",
    "A form of unsupervised learning from sequences is clustering. For example, in \n",
    "\n",
    "- user weblogs sequences: clustering the weblogs segments users into groups with similar browsing behavior. This helps in targeted marketing, anomaly detection, and other web customizations.\n",
    "\n",
    "- protein sequences: clustering proteins with similar structures help researchers study the commonalities between species. It also helps in faster search in some search algorithms.\n",
    "\n",
    "In the following, clustering on a protein sequence dataset will be shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g0HwFzOjyxgj"
   },
   "source": [
    "### Protein Sequence Clustering\n",
    "\n",
    "The data used here is taken from www.uniprot.org. This is a public database for proteins. The data contains the protein sequences and their functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "48hrShv7yxgk",
    "outputId": "89e5e349-1158-4dfe-bc89-b7ec92f8f768"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M7MCX3</td>\n",
       "      <td>[M, E, I, E, K, T, N, R, M, N, A, L, F, E, F, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K6PL84</td>\n",
       "      <td>[M, E, I, E, K, N, Y, R, M, N, S, L, F, E, F, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R4W5V3</td>\n",
       "      <td>[M, E, I, E, K, T, N, R, M, N, A, L, F, E, F, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T2A126</td>\n",
       "      <td>[M, E, I, E, K, T, N, R, M, N, A, L, F, E, F, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L0SHD5</td>\n",
       "      <td>[M, E, I, E, K, T, N, R, M, N, A, L, F, E, F, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2107</th>\n",
       "      <td>A0A081R612</td>\n",
       "      <td>[M, M, N, M, Q, N, M, M, R, Q, A, Q, K, L, Q, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2108</th>\n",
       "      <td>A0A081QQM2</td>\n",
       "      <td>[M, M, N, M, Q, N, M, M, R, Q, A, Q, K, L, Q, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2109</th>\n",
       "      <td>J1A517</td>\n",
       "      <td>[M, M, R, Q, A, Q, K, L, Q, K, Q, M, E, Q, S, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2110</th>\n",
       "      <td>F5U1T6</td>\n",
       "      <td>[M, M, N, M, Q, S, M, M, K, Q, A, Q, K, L, Q, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2111</th>\n",
       "      <td>J3A2T7</td>\n",
       "      <td>[M, M, N, M, Q, N, M, M, K, Q, A, Q, K, L, Q, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2112 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           sequence\n",
       "0         M7MCX3  [M, E, I, E, K, T, N, R, M, N, A, L, F, E, F, ...\n",
       "1         K6PL84  [M, E, I, E, K, N, Y, R, M, N, S, L, F, E, F, ...\n",
       "2         R4W5V3  [M, E, I, E, K, T, N, R, M, N, A, L, F, E, F, ...\n",
       "3         T2A126  [M, E, I, E, K, T, N, R, M, N, A, L, F, E, F, ...\n",
       "4         L0SHD5  [M, E, I, E, K, T, N, R, M, N, A, L, F, E, F, ...\n",
       "...          ...                                                ...\n",
       "2107  A0A081R612  [M, M, N, M, Q, N, M, M, R, Q, A, Q, K, L, Q, ...\n",
       "2108  A0A081QQM2  [M, M, N, M, Q, N, M, M, R, Q, A, Q, K, L, Q, ...\n",
       "2109      J1A517  [M, M, R, Q, A, Q, K, L, Q, K, Q, M, E, Q, S, ...\n",
       "2110      F5U1T6  [M, M, N, M, Q, S, M, M, K, Q, A, Q, K, L, Q, ...\n",
       "2111      J3A2T7  [M, M, N, M, Q, N, M, M, K, Q, A, Q, K, L, Q, ...\n",
       "\n",
       "[2112 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data\n",
    "corpus = pd.read_csv('data/protein_classification.csv')\n",
    "\n",
    "# Data preprocessing\n",
    "corpus = corpus.loc[:,['Entry','Sequence']]\n",
    "corpus.columns = ['id', 'sequence']\n",
    "corpus['sequence'] = corpus['sequence'].map(list)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 7 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "CPU times: user 324 ms, sys: 68 ms, total: 392 ms\n",
      "Wall time: 9.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute SGT embeddings\n",
    "sgt_ = SGT(kappa=1, \n",
    "           lengthsensitive=False, \n",
    "           mode='multiprocessing')\n",
    "sgtembedding_df = sgt_.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>(A, A)</th>\n",
       "      <th>(A, C)</th>\n",
       "      <th>(A, D)</th>\n",
       "      <th>(A, E)</th>\n",
       "      <th>(A, F)</th>\n",
       "      <th>(A, G)</th>\n",
       "      <th>(A, H)</th>\n",
       "      <th>(A, I)</th>\n",
       "      <th>(A, K)</th>\n",
       "      <th>...</th>\n",
       "      <th>(Y, M)</th>\n",
       "      <th>(Y, N)</th>\n",
       "      <th>(Y, P)</th>\n",
       "      <th>(Y, Q)</th>\n",
       "      <th>(Y, R)</th>\n",
       "      <th>(Y, S)</th>\n",
       "      <th>(Y, T)</th>\n",
       "      <th>(Y, V)</th>\n",
       "      <th>(Y, W)</th>\n",
       "      <th>(Y, Y)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M7MCX3</td>\n",
       "      <td>0.020180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009635</td>\n",
       "      <td>0.013529</td>\n",
       "      <td>0.009360</td>\n",
       "      <td>0.003205</td>\n",
       "      <td>2.944887e-10</td>\n",
       "      <td>0.002226</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009196</td>\n",
       "      <td>0.007964</td>\n",
       "      <td>0.036788</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>0.020665</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.007479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K6PL84</td>\n",
       "      <td>0.001604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012637</td>\n",
       "      <td>0.006323</td>\n",
       "      <td>0.006224</td>\n",
       "      <td>0.004819</td>\n",
       "      <td>3.560677e-03</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.012136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135335</td>\n",
       "      <td>0.006568</td>\n",
       "      <td>0.038901</td>\n",
       "      <td>0.011298</td>\n",
       "      <td>0.012578</td>\n",
       "      <td>0.009913</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R4W5V3</td>\n",
       "      <td>0.012448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008408</td>\n",
       "      <td>0.016363</td>\n",
       "      <td>0.027469</td>\n",
       "      <td>0.003205</td>\n",
       "      <td>2.944887e-10</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008114</td>\n",
       "      <td>0.007128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.001757</td>\n",
       "      <td>0.022736</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.012652</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T2A126</td>\n",
       "      <td>0.010545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012560</td>\n",
       "      <td>0.014212</td>\n",
       "      <td>0.013728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.944887e-10</td>\n",
       "      <td>0.007223</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.009669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003182</td>\n",
       "      <td>0.001904</td>\n",
       "      <td>0.015607</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.007479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L0SHD5</td>\n",
       "      <td>0.020180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008628</td>\n",
       "      <td>0.015033</td>\n",
       "      <td>0.009360</td>\n",
       "      <td>0.003205</td>\n",
       "      <td>2.944887e-10</td>\n",
       "      <td>0.002226</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009196</td>\n",
       "      <td>0.007964</td>\n",
       "      <td>0.036788</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>0.020665</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.007479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2107</th>\n",
       "      <td>A0A081R612</td>\n",
       "      <td>0.014805</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004159</td>\n",
       "      <td>0.017541</td>\n",
       "      <td>0.012701</td>\n",
       "      <td>0.013099</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.017043</td>\n",
       "      <td>0.004732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2108</th>\n",
       "      <td>A0A081QQM2</td>\n",
       "      <td>0.010774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004283</td>\n",
       "      <td>0.014732</td>\n",
       "      <td>0.014340</td>\n",
       "      <td>0.014846</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.016806</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2109</th>\n",
       "      <td>J1A517</td>\n",
       "      <td>0.010774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004283</td>\n",
       "      <td>0.014732</td>\n",
       "      <td>0.014340</td>\n",
       "      <td>0.014846</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2110</th>\n",
       "      <td>F5U1T6</td>\n",
       "      <td>0.015209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005175</td>\n",
       "      <td>0.023888</td>\n",
       "      <td>0.011410</td>\n",
       "      <td>0.011510</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.021145</td>\n",
       "      <td>0.009280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2111</th>\n",
       "      <td>J3A2T7</td>\n",
       "      <td>0.005240</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012301</td>\n",
       "      <td>0.013178</td>\n",
       "      <td>0.014744</td>\n",
       "      <td>0.014705</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.007957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2112 rows × 401 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id    (A, A)  (A, C)    (A, D)    (A, E)    (A, F)    (A, G)  \\\n",
       "0         M7MCX3  0.020180     0.0  0.009635  0.013529  0.009360  0.003205   \n",
       "1         K6PL84  0.001604     0.0  0.012637  0.006323  0.006224  0.004819   \n",
       "2         R4W5V3  0.012448     0.0  0.008408  0.016363  0.027469  0.003205   \n",
       "3         T2A126  0.010545     0.0  0.012560  0.014212  0.013728  0.000000   \n",
       "4         L0SHD5  0.020180     0.0  0.008628  0.015033  0.009360  0.003205   \n",
       "...          ...       ...     ...       ...       ...       ...       ...   \n",
       "2107  A0A081R612  0.014805     0.0  0.004159  0.017541  0.012701  0.013099   \n",
       "2108  A0A081QQM2  0.010774     0.0  0.004283  0.014732  0.014340  0.014846   \n",
       "2109      J1A517  0.010774     0.0  0.004283  0.014732  0.014340  0.014846   \n",
       "2110      F5U1T6  0.015209     0.0  0.005175  0.023888  0.011410  0.011510   \n",
       "2111      J3A2T7  0.005240     0.0  0.012301  0.013178  0.014744  0.014705   \n",
       "\n",
       "            (A, H)    (A, I)    (A, K)  ...    (Y, M)    (Y, N)    (Y, P)  \\\n",
       "0     2.944887e-10  0.002226  0.000379  ...  0.009196  0.007964  0.036788   \n",
       "1     3.560677e-03  0.001124  0.012136  ...  0.135335  0.006568  0.038901   \n",
       "2     2.944887e-10  0.004249  0.013013  ...  0.008114  0.007128  0.000000   \n",
       "3     2.944887e-10  0.007223  0.000309  ...  0.000325  0.009669  0.000000   \n",
       "4     2.944887e-10  0.002226  0.000379  ...  0.009196  0.007964  0.036788   \n",
       "...            ...       ...       ...  ...       ...       ...       ...   \n",
       "2107  0.000000e+00  0.017043  0.004732  ...  0.000000  0.000000  0.000000   \n",
       "2108  0.000000e+00  0.016806  0.005406  ...  0.000000  0.000000  0.000000   \n",
       "2109  0.000000e+00  0.014500  0.005406  ...  0.000000  0.000000  0.000000   \n",
       "2110  0.000000e+00  0.021145  0.009280  ...  0.000000  0.000000  0.000000   \n",
       "2111  0.000000e+00  0.000981  0.007957  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "        (Y, Q)    (Y, R)    (Y, S)    (Y, T)    (Y, V)  (Y, W)    (Y, Y)  \n",
       "0     0.000195  0.001513  0.020665  0.000542  0.007479     0.0  0.010419  \n",
       "1     0.011298  0.012578  0.009913  0.001079  0.000023     0.0  0.007728  \n",
       "2     0.000203  0.001757  0.022736  0.000249  0.012652     0.0  0.008533  \n",
       "3     0.003182  0.001904  0.015607  0.000577  0.007479     0.0  0.008648  \n",
       "4     0.000195  0.001513  0.020665  0.000542  0.007479     0.0  0.010419  \n",
       "...        ...       ...       ...       ...       ...     ...       ...  \n",
       "2107  0.000000  0.000000  0.000000  0.000000  0.000000     0.0  0.000000  \n",
       "2108  0.000000  0.000000  0.000000  0.000000  0.000000     0.0  0.000000  \n",
       "2109  0.000000  0.000000  0.000000  0.000000  0.000000     0.0  0.000000  \n",
       "2110  0.000000  0.000000  0.000000  0.000000  0.000000     0.0  0.000000  \n",
       "2111  0.000000  0.000000  0.000000  0.000000  0.000000     0.0  0.000000  \n",
       "\n",
       "[2112 rows x 401 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgtembedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(A, A)</th>\n",
       "      <th>(A, C)</th>\n",
       "      <th>(A, D)</th>\n",
       "      <th>(A, E)</th>\n",
       "      <th>(A, F)</th>\n",
       "      <th>(A, G)</th>\n",
       "      <th>(A, H)</th>\n",
       "      <th>(A, I)</th>\n",
       "      <th>(A, K)</th>\n",
       "      <th>(A, L)</th>\n",
       "      <th>...</th>\n",
       "      <th>(Y, M)</th>\n",
       "      <th>(Y, N)</th>\n",
       "      <th>(Y, P)</th>\n",
       "      <th>(Y, Q)</th>\n",
       "      <th>(Y, R)</th>\n",
       "      <th>(Y, S)</th>\n",
       "      <th>(Y, T)</th>\n",
       "      <th>(Y, V)</th>\n",
       "      <th>(Y, W)</th>\n",
       "      <th>(Y, Y)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>M7MCX3</th>\n",
       "      <td>0.020180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009635</td>\n",
       "      <td>0.013529</td>\n",
       "      <td>0.009360</td>\n",
       "      <td>0.003205</td>\n",
       "      <td>2.944887e-10</td>\n",
       "      <td>0.002226</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.021703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009196</td>\n",
       "      <td>0.007964</td>\n",
       "      <td>0.036788</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>0.020665</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.007479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K6PL84</th>\n",
       "      <td>0.001604</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012637</td>\n",
       "      <td>0.006323</td>\n",
       "      <td>0.006224</td>\n",
       "      <td>0.004819</td>\n",
       "      <td>3.560677e-03</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.012136</td>\n",
       "      <td>0.018427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135335</td>\n",
       "      <td>0.006568</td>\n",
       "      <td>0.038901</td>\n",
       "      <td>0.011298</td>\n",
       "      <td>0.012578</td>\n",
       "      <td>0.009913</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R4W5V3</th>\n",
       "      <td>0.012448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008408</td>\n",
       "      <td>0.016363</td>\n",
       "      <td>0.027469</td>\n",
       "      <td>0.003205</td>\n",
       "      <td>2.944887e-10</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.013013</td>\n",
       "      <td>0.031118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008114</td>\n",
       "      <td>0.007128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.001757</td>\n",
       "      <td>0.022736</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.012652</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T2A126</th>\n",
       "      <td>0.010545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012560</td>\n",
       "      <td>0.014212</td>\n",
       "      <td>0.013728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.944887e-10</td>\n",
       "      <td>0.007223</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.028531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.009669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003182</td>\n",
       "      <td>0.001904</td>\n",
       "      <td>0.015607</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.007479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L0SHD5</th>\n",
       "      <td>0.020180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008628</td>\n",
       "      <td>0.015033</td>\n",
       "      <td>0.009360</td>\n",
       "      <td>0.003205</td>\n",
       "      <td>2.944887e-10</td>\n",
       "      <td>0.002226</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.021703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009196</td>\n",
       "      <td>0.007964</td>\n",
       "      <td>0.036788</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>0.020665</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.007479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0A081R612</th>\n",
       "      <td>0.014805</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004159</td>\n",
       "      <td>0.017541</td>\n",
       "      <td>0.012701</td>\n",
       "      <td>0.013099</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.017043</td>\n",
       "      <td>0.004732</td>\n",
       "      <td>0.014904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0A081QQM2</th>\n",
       "      <td>0.010774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004283</td>\n",
       "      <td>0.014732</td>\n",
       "      <td>0.014340</td>\n",
       "      <td>0.014846</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.016806</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>0.014083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>J1A517</th>\n",
       "      <td>0.010774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004283</td>\n",
       "      <td>0.014732</td>\n",
       "      <td>0.014340</td>\n",
       "      <td>0.014846</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>0.014083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F5U1T6</th>\n",
       "      <td>0.015209</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005175</td>\n",
       "      <td>0.023888</td>\n",
       "      <td>0.011410</td>\n",
       "      <td>0.011510</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.021145</td>\n",
       "      <td>0.009280</td>\n",
       "      <td>0.017466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>J3A2T7</th>\n",
       "      <td>0.005240</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012301</td>\n",
       "      <td>0.013178</td>\n",
       "      <td>0.014744</td>\n",
       "      <td>0.014705</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.007957</td>\n",
       "      <td>0.017112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2112 rows × 400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              (A, A)  (A, C)    (A, D)    (A, E)    (A, F)    (A, G)  \\\n",
       "id                                                                     \n",
       "M7MCX3      0.020180     0.0  0.009635  0.013529  0.009360  0.003205   \n",
       "K6PL84      0.001604     0.0  0.012637  0.006323  0.006224  0.004819   \n",
       "R4W5V3      0.012448     0.0  0.008408  0.016363  0.027469  0.003205   \n",
       "T2A126      0.010545     0.0  0.012560  0.014212  0.013728  0.000000   \n",
       "L0SHD5      0.020180     0.0  0.008628  0.015033  0.009360  0.003205   \n",
       "...              ...     ...       ...       ...       ...       ...   \n",
       "A0A081R612  0.014805     0.0  0.004159  0.017541  0.012701  0.013099   \n",
       "A0A081QQM2  0.010774     0.0  0.004283  0.014732  0.014340  0.014846   \n",
       "J1A517      0.010774     0.0  0.004283  0.014732  0.014340  0.014846   \n",
       "F5U1T6      0.015209     0.0  0.005175  0.023888  0.011410  0.011510   \n",
       "J3A2T7      0.005240     0.0  0.012301  0.013178  0.014744  0.014705   \n",
       "\n",
       "                  (A, H)    (A, I)    (A, K)    (A, L)  ...    (Y, M)  \\\n",
       "id                                                      ...             \n",
       "M7MCX3      2.944887e-10  0.002226  0.000379  0.021703  ...  0.009196   \n",
       "K6PL84      3.560677e-03  0.001124  0.012136  0.018427  ...  0.135335   \n",
       "R4W5V3      2.944887e-10  0.004249  0.013013  0.031118  ...  0.008114   \n",
       "T2A126      2.944887e-10  0.007223  0.000309  0.028531  ...  0.000325   \n",
       "L0SHD5      2.944887e-10  0.002226  0.000379  0.021703  ...  0.009196   \n",
       "...                  ...       ...       ...       ...  ...       ...   \n",
       "A0A081R612  0.000000e+00  0.017043  0.004732  0.014904  ...  0.000000   \n",
       "A0A081QQM2  0.000000e+00  0.016806  0.005406  0.014083  ...  0.000000   \n",
       "J1A517      0.000000e+00  0.014500  0.005406  0.014083  ...  0.000000   \n",
       "F5U1T6      0.000000e+00  0.021145  0.009280  0.017466  ...  0.000000   \n",
       "J3A2T7      0.000000e+00  0.000981  0.007957  0.017112  ...  0.000000   \n",
       "\n",
       "              (Y, N)    (Y, P)    (Y, Q)    (Y, R)    (Y, S)    (Y, T)  \\\n",
       "id                                                                       \n",
       "M7MCX3      0.007964  0.036788  0.000195  0.001513  0.020665  0.000542   \n",
       "K6PL84      0.006568  0.038901  0.011298  0.012578  0.009913  0.001079   \n",
       "R4W5V3      0.007128  0.000000  0.000203  0.001757  0.022736  0.000249   \n",
       "T2A126      0.009669  0.000000  0.003182  0.001904  0.015607  0.000577   \n",
       "L0SHD5      0.007964  0.036788  0.000195  0.001513  0.020665  0.000542   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "A0A081R612  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "A0A081QQM2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "J1A517      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "F5U1T6      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "J3A2T7      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "              (Y, V)  (Y, W)    (Y, Y)  \n",
       "id                                      \n",
       "M7MCX3      0.007479     0.0  0.010419  \n",
       "K6PL84      0.000023     0.0  0.007728  \n",
       "R4W5V3      0.012652     0.0  0.008533  \n",
       "T2A126      0.007479     0.0  0.008648  \n",
       "L0SHD5      0.007479     0.0  0.010419  \n",
       "...              ...     ...       ...  \n",
       "A0A081R612  0.000000     0.0  0.000000  \n",
       "A0A081QQM2  0.000000     0.0  0.000000  \n",
       "J1A517      0.000000     0.0  0.000000  \n",
       "F5U1T6      0.000000     0.0  0.000000  \n",
       "J3A2T7      0.000000     0.0  0.000000  \n",
       "\n",
       "[2112 rows x 400 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the id column as the dataframe index\n",
    "sgtembedding_df = sgtembedding_df.set_index('id')\n",
    "sgtembedding_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kyfezp5Ryxgt"
   },
   "source": [
    "We perform PCA on the sequence embeddings and then do kmeans clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "REP48saLyxgu",
    "outputId": "b7dd52c2-a3d4-4e04-afba-a711ad1e32f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6432744907364981\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.384913</td>\n",
       "      <td>-0.269873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.022764</td>\n",
       "      <td>0.135995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.177792</td>\n",
       "      <td>-0.172454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.168074</td>\n",
       "      <td>-0.147334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.383616</td>\n",
       "      <td>-0.271163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2\n",
       "0  0.384913 -0.269873\n",
       "1  0.022764  0.135995\n",
       "2  0.177792 -0.172454\n",
       "3  0.168074 -0.147334\n",
       "4  0.383616 -0.271163"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(sgtembedding_df)\n",
    "\n",
    "X=pca.transform(sgtembedding_df)\n",
    "\n",
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "df = pd.DataFrame(data=X, columns=['x1', 'x2'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zByYgY5Tyxgw",
    "outputId": "84b6c07c-6c9f-4032-b86e-c732c0c9ca9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x14c5a77f0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=3, max_iter =300)\n",
    "kmeans.fit(df)\n",
    "\n",
    "labels = kmeans.predict(df)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "colmap = {1: 'r', 2: 'g', 3: 'b'}\n",
    "colors = list(map(lambda x: colmap[x+1], labels))\n",
    "plt.scatter(df['x1'], df['x2'], color=colors, alpha=0.5, edgecolor=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "xUWyENrayxgx"
   },
   "source": [
    "## Sequence Classification using Deep Learning in TensorFlow\n",
    "\n",
    "The protein data set used above is also labeled. The labels represent the protein functions. Similarly, there are other labeled sequence data sets. For example, DARPA shared an intrusion weblog data set. It contains weblog sequences with positive labels if the log represents a network intrusion.\n",
    "\n",
    "In such problems supervised learning is employed. Classification is a supervised learning we will demonstrate here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protein Sequence Classification\n",
    "\n",
    "The data set is taken from https://www.uniprot.org . The protein sequences in the data set have one of the two functions,\n",
    "- Binds to DNA and alters its conformation. May be involved in regulation of gene expression, nucleoid organization and DNA protection.\n",
    "- Might take part in the signal recognition particle (SRP) pathway. This is inferred from the conservation of its genetic proximity to ftsY/ffh. May be a regulatory protein.\n",
    "\n",
    "There are a total of 2113 samples. The sequence lengths vary between 80-130."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PxPU6NhVyxgy"
   },
   "outputs": [],
   "source": [
    "# Loading data\n",
    "data = pd.read_csv('data/protein_classification.csv')\n",
    "\n",
    "\n",
    "# Data preprocessing\n",
    "y = data['Function [CC]']\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y = encoder.transform(y)\n",
    "\n",
    "corpus = data.loc[:,['Entry','Sequence']]\n",
    "corpus.columns = ['id', 'sequence']\n",
    "corpus['sequence'] = corpus['sequence'].map(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PxPU6NhVyxgy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 7 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "# Sequence embedding\n",
    "sgt_ = SGT(kappa=1, \n",
    "           lengthsensitive=False, \n",
    "           mode='multiprocessing')\n",
    "sgtembedding_df = sgt_.fit_transform(corpus)\n",
    "X = sgtembedding_df.set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y8nle1_wyxgz"
   },
   "source": [
    "We will perform a 10-fold cross-validation to measure the performance of the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bdZiJxZEyxgz",
    "outputId": "f2413970-908d-45b7-d18f-4eb7754595fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average f1 score 1.0\n"
     ]
    }
   ],
   "source": [
    "kfold = 10\n",
    "X = X\n",
    "y = encoded_y\n",
    "\n",
    "random_state = 1\n",
    "\n",
    "test_F1 = np.zeros(kfold)\n",
    "skf = KFold(n_splits = kfold, shuffle = True, random_state = random_state)\n",
    "k = 0\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_shape = (X_train.shape[1],))) \n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train ,batch_size=batch_size, epochs=epochs, verbose=0)\n",
    "    \n",
    "    y_pred = model.predict_proba(X_test).round().astype(int)\n",
    "    y_train_pred = model.predict_proba(X_train).round().astype(int)\n",
    "\n",
    "    test_F1[k] = sklearn.metrics.f1_score(y_test, y_pred)\n",
    "    k+=1\n",
    "    \n",
    "print ('Average f1 score', np.mean(test_F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weblog Classification for Intrusion Detection\n",
    "\n",
    "This data sample is taken from https://www.ll.mit.edu/r-d/datasets/1998-darpa-intrusion-detection-evaluation-dataset. \n",
    "This is a network intrusion data containing audit logs and any attack as a positive label. Since, network intrusion is a rare event, the data is unbalanced. Here we will,\n",
    "- build a sequence classification model to predict a network intrusion.\n",
    "\n",
    "Each sequence contains in the data is a series of activity, for example, {login, password}. The _alphabets_ in the input data sequences are already encoded into integers. The original sequences data file is also present in the `/data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IEbbCqTIyxg1",
    "outputId": "f8607d28-b9ae-4082-dc96-3ef5c50b55a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timeduration', 'seqlen', 'seq', 'class'], dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data\n",
    "data = pd.read_csv('data/darpa_data.csv')\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['id'] = data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "y = data['class']\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y = encoder.transform(y)\n",
    "\n",
    "corpus = data.loc[:,['id','seq']]\n",
    "corpus.columns = ['id', 'sequence']\n",
    "corpus['sequence'] = corpus['sequence'].map(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2bGYejSIyxg6"
   },
   "source": [
    "#### Sequence embeddings\n",
    "In this data, the sequence embeddings should be **length-sensitive**. \n",
    "\n",
    "The lengths are important here because sequences with similar patterns but different lengths can have different labels. Consider a simple example of two sessions: `{login, pswd, login, pswd,...}` and `{login, pswd,...(repeated several times)..., login, pswd}`. \n",
    "\n",
    "While the first session can be a regular user mistyping the password once, the other session is possibly an attack to guess the password. Thus, the sequence lengths are as important as the patterns.\n",
    "\n",
    "Therefore, `lengthsensitive=True` is used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 7 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(0, 0)</th>\n",
       "      <th>(0, 1)</th>\n",
       "      <th>(0, 2)</th>\n",
       "      <th>(0, 3)</th>\n",
       "      <th>(0, 4)</th>\n",
       "      <th>(0, 5)</th>\n",
       "      <th>(0, 6)</th>\n",
       "      <th>(0, 7)</th>\n",
       "      <th>(0, 8)</th>\n",
       "      <th>(0, 9)</th>\n",
       "      <th>...</th>\n",
       "      <th>(~, 1)</th>\n",
       "      <th>(~, 2)</th>\n",
       "      <th>(~, 3)</th>\n",
       "      <th>(~, 4)</th>\n",
       "      <th>(~, 5)</th>\n",
       "      <th>(~, 6)</th>\n",
       "      <th>(~, 7)</th>\n",
       "      <th>(~, 8)</th>\n",
       "      <th>(~, 9)</th>\n",
       "      <th>(~, ~)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.485034</td>\n",
       "      <td>0.486999</td>\n",
       "      <td>0.485802</td>\n",
       "      <td>0.483097</td>\n",
       "      <td>0.483956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025622</td>\n",
       "      <td>0.228156</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.310714e-09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447620</td>\n",
       "      <td>0.452097</td>\n",
       "      <td>0.464568</td>\n",
       "      <td>0.367296</td>\n",
       "      <td>0.525141</td>\n",
       "      <td>0.455018</td>\n",
       "      <td>0.374364</td>\n",
       "      <td>0.414081</td>\n",
       "      <td>0.549981</td>\n",
       "      <td>0.172479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.193359</td>\n",
       "      <td>0.071469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>0.077999</td>\n",
       "      <td>0.208974</td>\n",
       "      <td>0.230338</td>\n",
       "      <td>1.830519e-01</td>\n",
       "      <td>1.200926e-17</td>\n",
       "      <td>1.696880e-01</td>\n",
       "      <td>0.093646</td>\n",
       "      <td>7.985870e-02</td>\n",
       "      <td>2.896813e-05</td>\n",
       "      <td>3.701710e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474072</td>\n",
       "      <td>0.468353</td>\n",
       "      <td>0.463594</td>\n",
       "      <td>0.177507</td>\n",
       "      <td>0.551270</td>\n",
       "      <td>0.418652</td>\n",
       "      <td>0.309652</td>\n",
       "      <td>0.384657</td>\n",
       "      <td>0.378225</td>\n",
       "      <td>0.170362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023695</td>\n",
       "      <td>0.217819</td>\n",
       "      <td>2.188276e-33</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.075992e-11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.681668e-39</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.464120</td>\n",
       "      <td>0.468229</td>\n",
       "      <td>0.452170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300534</td>\n",
       "      <td>0.161961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.167082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106.0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024495</td>\n",
       "      <td>0.219929</td>\n",
       "      <td>2.035190e-17</td>\n",
       "      <td>1.073271e-18</td>\n",
       "      <td>5.656994e-11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.047380e-29</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.502213</td>\n",
       "      <td>0.544343</td>\n",
       "      <td>0.477281</td>\n",
       "      <td>0.175901</td>\n",
       "      <td>0.461103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.162796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.167687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107.0</th>\n",
       "      <td>0.110422</td>\n",
       "      <td>0.227478</td>\n",
       "      <td>0.217549</td>\n",
       "      <td>1.723963e-01</td>\n",
       "      <td>1.033292e-14</td>\n",
       "      <td>3.896725e-07</td>\n",
       "      <td>0.083685</td>\n",
       "      <td>2.940589e-08</td>\n",
       "      <td>8.864072e-02</td>\n",
       "      <td>4.813990e-29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.490398</td>\n",
       "      <td>0.522016</td>\n",
       "      <td>0.466808</td>\n",
       "      <td>0.470603</td>\n",
       "      <td>0.479795</td>\n",
       "      <td>0.480057</td>\n",
       "      <td>0.194888</td>\n",
       "      <td>0.172397</td>\n",
       "      <td>0.164873</td>\n",
       "      <td>0.172271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108.0</th>\n",
       "      <td>0.005646</td>\n",
       "      <td>0.202424</td>\n",
       "      <td>0.196786</td>\n",
       "      <td>2.281242e-01</td>\n",
       "      <td>1.133936e-01</td>\n",
       "      <td>1.862098e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.212869e-01</td>\n",
       "      <td>9.180520e-08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.432834</td>\n",
       "      <td>0.434953</td>\n",
       "      <td>0.439615</td>\n",
       "      <td>0.390864</td>\n",
       "      <td>0.481764</td>\n",
       "      <td>0.600875</td>\n",
       "      <td>0.166766</td>\n",
       "      <td>0.165368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109.0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025616</td>\n",
       "      <td>0.238176</td>\n",
       "      <td>3.889176e-55</td>\n",
       "      <td>1.332427e-60</td>\n",
       "      <td>1.408003e-09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.845377e-60</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.421318</td>\n",
       "      <td>0.439985</td>\n",
       "      <td>0.467953</td>\n",
       "      <td>0.440951</td>\n",
       "      <td>0.527165</td>\n",
       "      <td>0.864717</td>\n",
       "      <td>0.407155</td>\n",
       "      <td>0.399335</td>\n",
       "      <td>0.251304</td>\n",
       "      <td>0.171885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110.0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022868</td>\n",
       "      <td>0.203513</td>\n",
       "      <td>9.273472e-64</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.240870e-09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.478090</td>\n",
       "      <td>0.454871</td>\n",
       "      <td>0.459109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.490534</td>\n",
       "      <td>0.370357</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.162997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.162089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         (0, 0)    (0, 1)    (0, 2)        (0, 3)        (0, 4)        (0, 5)  \\\n",
       "id                                                                              \n",
       "0.0    0.000000  0.000000  0.000000  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "1.0    0.000000  0.025622  0.228156  0.000000e+00  0.000000e+00  1.310714e-09   \n",
       "2.0    0.000000  0.000000  0.000000  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "3.0    0.077999  0.208974  0.230338  1.830519e-01  1.200926e-17  1.696880e-01   \n",
       "4.0    0.000000  0.023695  0.217819  2.188276e-33  0.000000e+00  6.075992e-11   \n",
       "...         ...       ...       ...           ...           ...           ...   \n",
       "106.0  0.000000  0.024495  0.219929  2.035190e-17  1.073271e-18  5.656994e-11   \n",
       "107.0  0.110422  0.227478  0.217549  1.723963e-01  1.033292e-14  3.896725e-07   \n",
       "108.0  0.005646  0.202424  0.196786  2.281242e-01  1.133936e-01  1.862098e-01   \n",
       "109.0  0.000000  0.025616  0.238176  3.889176e-55  1.332427e-60  1.408003e-09   \n",
       "110.0  0.000000  0.022868  0.203513  9.273472e-64  0.000000e+00  1.240870e-09   \n",
       "\n",
       "         (0, 6)        (0, 7)        (0, 8)        (0, 9)  ...    (~, 1)  \\\n",
       "id                                                         ...             \n",
       "0.0    0.000000  0.000000e+00  0.000000e+00  0.000000e+00  ...  0.485034   \n",
       "1.0    0.000000  0.000000e+00  0.000000e+00  0.000000e+00  ...  0.447620   \n",
       "2.0    0.000000  0.000000e+00  0.000000e+00  0.000000e+00  ...  0.525605   \n",
       "3.0    0.093646  7.985870e-02  2.896813e-05  3.701710e-05  ...  0.474072   \n",
       "4.0    0.000000  0.000000e+00  5.681668e-39  0.000000e+00  ...  0.464120   \n",
       "...         ...           ...           ...           ...  ...       ...   \n",
       "106.0  0.000000  0.000000e+00  5.047380e-29  0.000000e+00  ...  0.502213   \n",
       "107.0  0.083685  2.940589e-08  8.864072e-02  4.813990e-29  ...  0.490398   \n",
       "108.0  0.000000  1.212869e-01  9.180520e-08  0.000000e+00  ...  0.432834   \n",
       "109.0  0.000000  9.845377e-60  0.000000e+00  0.000000e+00  ...  0.421318   \n",
       "110.0  0.000000  0.000000e+00  0.000000e+00  0.000000e+00  ...  0.478090   \n",
       "\n",
       "         (~, 2)    (~, 3)    (~, 4)    (~, 5)    (~, 6)    (~, 7)    (~, 8)  \\\n",
       "id                                                                            \n",
       "0.0    0.486999  0.485802  0.483097  0.483956  0.000000  0.000000  0.000000   \n",
       "1.0    0.452097  0.464568  0.367296  0.525141  0.455018  0.374364  0.414081   \n",
       "2.0    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3.0    0.468353  0.463594  0.177507  0.551270  0.418652  0.309652  0.384657   \n",
       "4.0    0.468229  0.452170  0.000000  0.501242  0.000000  0.300534  0.161961   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "106.0  0.544343  0.477281  0.175901  0.461103  0.000000  0.000000  0.162796   \n",
       "107.0  0.522016  0.466808  0.470603  0.479795  0.480057  0.194888  0.172397   \n",
       "108.0  0.434953  0.439615  0.390864  0.481764  0.600875  0.166766  0.165368   \n",
       "109.0  0.439985  0.467953  0.440951  0.527165  0.864717  0.407155  0.399335   \n",
       "110.0  0.454871  0.459109  0.000000  0.490534  0.370357  0.000000  0.162997   \n",
       "\n",
       "         (~, 9)    (~, ~)  \n",
       "id                         \n",
       "0.0    0.000000  0.178609  \n",
       "1.0    0.549981  0.172479  \n",
       "2.0    0.193359  0.071469  \n",
       "3.0    0.378225  0.170362  \n",
       "4.0    0.000000  0.167082  \n",
       "...         ...       ...  \n",
       "106.0  0.000000  0.167687  \n",
       "107.0  0.164873  0.172271  \n",
       "108.0  0.000000  0.171729  \n",
       "109.0  0.251304  0.171885  \n",
       "110.0  0.000000  0.162089  \n",
       "\n",
       "[111 rows x 121 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequence embedding\n",
    "sgt_ = SGT(kappa=5, \n",
    "           lengthsensitive=True, \n",
    "           mode='multiprocessing')\n",
    "sgtembedding_df = sgt_.fit_transform(corpus)\n",
    "sgtembedding_df = sgtembedding_df.set_index('id')\n",
    "sgtembedding_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "icj-ffi-yxg_"
   },
   "source": [
    "#### Applying PCA on the embeddings\n",
    "The embeddings are sparse and high-dimensional. PCA is, therefore, applied for dimension reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6J_4MLbYyxg_",
    "outputId": "1e9eefd8-51fb-4471-f2ef-50220b779304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9962446146783123\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=35)\n",
    "pca.fit(sgtembedding_df)\n",
    "X = pca.transform(sgtembedding_df)\n",
    "print(np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R9F287J9yxhB"
   },
   "source": [
    "#### Building a Multi-Layer Perceptron Classifier\n",
    "The PCA transforms of the embeddings are used directly as inputs to an MLP classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y3ugVbjHyxhB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 128)               4608      \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,737\n",
      "Trainable params: 4,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 74 samples\n",
      "Epoch 1/300\n",
      "74/74 [==============================] - 0s 7ms/sample - loss: 0.1487 - accuracy: 0.5270\n",
      "Epoch 2/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.1421 - accuracy: 0.5000\n",
      "Epoch 3/300\n",
      "74/74 [==============================] - 0s 154us/sample - loss: 0.1440 - accuracy: 0.5541\n",
      "Epoch 4/300\n",
      "74/74 [==============================] - 0s 184us/sample - loss: 0.1378 - accuracy: 0.6216\n",
      "Epoch 5/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.1309 - accuracy: 0.6892\n",
      "Epoch 6/300\n",
      "74/74 [==============================] - 0s 137us/sample - loss: 0.1235 - accuracy: 0.7973\n",
      "Epoch 7/300\n",
      "74/74 [==============================] - 0s 158us/sample - loss: 0.1217 - accuracy: 0.7838\n",
      "Epoch 8/300\n",
      "74/74 [==============================] - 0s 159us/sample - loss: 0.1212 - accuracy: 0.8378\n",
      "Epoch 9/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.1118 - accuracy: 0.8108\n",
      "Epoch 10/300\n",
      "74/74 [==============================] - 0s 145us/sample - loss: 0.1108 - accuracy: 0.8378\n",
      "Epoch 11/300\n",
      "74/74 [==============================] - 0s 156us/sample - loss: 0.1083 - accuracy: 0.8243\n",
      "Epoch 12/300\n",
      "74/74 [==============================] - 0s 150us/sample - loss: 0.1042 - accuracy: 0.8378\n",
      "Epoch 13/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0996 - accuracy: 0.8514\n",
      "Epoch 14/300\n",
      "74/74 [==============================] - 0s 153us/sample - loss: 0.0996 - accuracy: 0.8649\n",
      "Epoch 15/300\n",
      "74/74 [==============================] - 0s 141us/sample - loss: 0.0960 - accuracy: 0.8514\n",
      "Epoch 16/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0907 - accuracy: 0.8649\n",
      "Epoch 17/300\n",
      "74/74 [==============================] - 0s 154us/sample - loss: 0.0926 - accuracy: 0.8378\n",
      "Epoch 18/300\n",
      "74/74 [==============================] - 0s 174us/sample - loss: 0.0887 - accuracy: 0.8514\n",
      "Epoch 19/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0899 - accuracy: 0.8649\n",
      "Epoch 20/300\n",
      "74/74 [==============================] - 0s 174us/sample - loss: 0.0825 - accuracy: 0.8649\n",
      "Epoch 21/300\n",
      "74/74 [==============================] - 0s 151us/sample - loss: 0.0767 - accuracy: 0.8649\n",
      "Epoch 22/300\n",
      "74/74 [==============================] - 0s 150us/sample - loss: 0.0755 - accuracy: 0.8649\n",
      "Epoch 23/300\n",
      "74/74 [==============================] - 0s 142us/sample - loss: 0.0749 - accuracy: 0.8649\n",
      "Epoch 24/300\n",
      "74/74 [==============================] - 0s 144us/sample - loss: 0.0680 - accuracy: 0.8514\n",
      "Epoch 25/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0674 - accuracy: 0.8649\n",
      "Epoch 26/300\n",
      "74/74 [==============================] - 0s 141us/sample - loss: 0.0669 - accuracy: 0.8649\n",
      "Epoch 27/300\n",
      "74/74 [==============================] - 0s 171us/sample - loss: 0.0667 - accuracy: 0.8649\n",
      "Epoch 28/300\n",
      "74/74 [==============================] - 0s 157us/sample - loss: 0.0647 - accuracy: 0.8649\n",
      "Epoch 29/300\n",
      "74/74 [==============================] - 0s 157us/sample - loss: 0.0665 - accuracy: 0.8649\n",
      "Epoch 30/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0599 - accuracy: 0.8649\n",
      "Epoch 31/300\n",
      "74/74 [==============================] - 0s 146us/sample - loss: 0.0592 - accuracy: 0.8649\n",
      "Epoch 32/300\n",
      "74/74 [==============================] - 0s 168us/sample - loss: 0.0585 - accuracy: 0.8649\n",
      "Epoch 33/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0555 - accuracy: 0.8649\n",
      "Epoch 34/300\n",
      "74/74 [==============================] - 0s 141us/sample - loss: 0.0546 - accuracy: 0.8649\n",
      "Epoch 35/300\n",
      "74/74 [==============================] - 0s 157us/sample - loss: 0.0508 - accuracy: 0.8649\n",
      "Epoch 36/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0557 - accuracy: 0.8649\n",
      "Epoch 37/300\n",
      "74/74 [==============================] - 0s 140us/sample - loss: 0.0506 - accuracy: 0.8649\n",
      "Epoch 38/300\n",
      "74/74 [==============================] - 0s 160us/sample - loss: 0.0508 - accuracy: 0.8649\n",
      "Epoch 39/300\n",
      "74/74 [==============================] - 0s 157us/sample - loss: 0.0523 - accuracy: 0.8649\n",
      "Epoch 40/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0507 - accuracy: 0.8649\n",
      "Epoch 41/300\n",
      "74/74 [==============================] - 0s 151us/sample - loss: 0.0501 - accuracy: 0.8649\n",
      "Epoch 42/300\n",
      "74/74 [==============================] - 0s 153us/sample - loss: 0.0483 - accuracy: 0.8649\n",
      "Epoch 43/300\n",
      "74/74 [==============================] - 0s 154us/sample - loss: 0.0491 - accuracy: 0.8649\n",
      "Epoch 44/300\n",
      "74/74 [==============================] - 0s 142us/sample - loss: 0.0411 - accuracy: 0.8649\n",
      "Epoch 45/300\n",
      "74/74 [==============================] - 0s 155us/sample - loss: 0.0487 - accuracy: 0.8649\n",
      "Epoch 46/300\n",
      "74/74 [==============================] - 0s 171us/sample - loss: 0.0435 - accuracy: 0.8649\n",
      "Epoch 47/300\n",
      "74/74 [==============================] - 0s 137us/sample - loss: 0.0432 - accuracy: 0.8649\n",
      "Epoch 48/300\n",
      "74/74 [==============================] - 0s 137us/sample - loss: 0.0435 - accuracy: 0.8649\n",
      "Epoch 49/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0438 - accuracy: 0.8649\n",
      "Epoch 50/300\n",
      "74/74 [==============================] - 0s 164us/sample - loss: 0.0435 - accuracy: 0.8649\n",
      "Epoch 51/300\n",
      "74/74 [==============================] - 0s 149us/sample - loss: 0.0417 - accuracy: 0.8649\n",
      "Epoch 52/300\n",
      "74/74 [==============================] - 0s 190us/sample - loss: 0.0447 - accuracy: 0.8649\n",
      "Epoch 53/300\n",
      "74/74 [==============================] - 0s 160us/sample - loss: 0.0430 - accuracy: 0.8649\n",
      "Epoch 54/300\n",
      "74/74 [==============================] - 0s 165us/sample - loss: 0.0418 - accuracy: 0.8784\n",
      "Epoch 55/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0447 - accuracy: 0.8784\n",
      "Epoch 56/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0420 - accuracy: 0.8649\n",
      "Epoch 57/300\n",
      "74/74 [==============================] - 0s 154us/sample - loss: 0.0380 - accuracy: 0.8649\n",
      "Epoch 58/300\n",
      "74/74 [==============================] - 0s 137us/sample - loss: 0.0397 - accuracy: 0.8649\n",
      "Epoch 59/300\n",
      "74/74 [==============================] - 0s 157us/sample - loss: 0.0408 - accuracy: 0.8649\n",
      "Epoch 60/300\n",
      "74/74 [==============================] - 0s 161us/sample - loss: 0.0357 - accuracy: 0.8649\n",
      "Epoch 61/300\n",
      "74/74 [==============================] - 0s 185us/sample - loss: 0.0375 - accuracy: 0.8649\n",
      "Epoch 62/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0403 - accuracy: 0.8649\n",
      "Epoch 63/300\n",
      "74/74 [==============================] - 0s 149us/sample - loss: 0.0413 - accuracy: 0.8649\n",
      "Epoch 64/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0356 - accuracy: 0.8649\n",
      "Epoch 65/300\n",
      "74/74 [==============================] - 0s 137us/sample - loss: 0.0419 - accuracy: 0.8784\n",
      "Epoch 66/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0353 - accuracy: 0.8784\n",
      "Epoch 67/300\n",
      "74/74 [==============================] - 0s 147us/sample - loss: 0.0384 - accuracy: 0.8649\n",
      "Epoch 68/300\n",
      "74/74 [==============================] - 0s 141us/sample - loss: 0.0383 - accuracy: 0.8784\n",
      "Epoch 69/300\n",
      "74/74 [==============================] - 0s 157us/sample - loss: 0.0359 - accuracy: 0.8649\n",
      "Epoch 70/300\n",
      "74/74 [==============================] - 0s 156us/sample - loss: 0.0364 - accuracy: 0.8649\n",
      "Epoch 71/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0351 - accuracy: 0.8649\n",
      "Epoch 72/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0355 - accuracy: 0.8784\n",
      "Epoch 73/300\n",
      "74/74 [==============================] - 0s 166us/sample - loss: 0.0343 - accuracy: 0.8919\n",
      "Epoch 74/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0362 - accuracy: 0.8649\n",
      "Epoch 75/300\n",
      "74/74 [==============================] - 0s 140us/sample - loss: 0.0353 - accuracy: 0.8784\n",
      "Epoch 76/300\n",
      "74/74 [==============================] - 0s 156us/sample - loss: 0.0377 - accuracy: 0.8649\n",
      "Epoch 77/300\n",
      "74/74 [==============================] - 0s 197us/sample - loss: 0.0379 - accuracy: 0.8649\n",
      "Epoch 78/300\n",
      "74/74 [==============================] - 0s 151us/sample - loss: 0.0378 - accuracy: 0.8649\n",
      "Epoch 79/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0367 - accuracy: 0.8784\n",
      "Epoch 80/300\n",
      "74/74 [==============================] - 0s 166us/sample - loss: 0.0369 - accuracy: 0.8649\n",
      "Epoch 81/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0343 - accuracy: 0.8784\n",
      "Epoch 82/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0319 - accuracy: 0.8649\n",
      "Epoch 83/300\n",
      "74/74 [==============================] - 0s 165us/sample - loss: 0.0344 - accuracy: 0.8649\n",
      "Epoch 84/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0374 - accuracy: 0.8649\n",
      "Epoch 85/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0334 - accuracy: 0.8649\n",
      "Epoch 86/300\n",
      "74/74 [==============================] - 0s 158us/sample - loss: 0.0319 - accuracy: 0.8649\n",
      "Epoch 87/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0364 - accuracy: 0.8649\n",
      "Epoch 88/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0324 - accuracy: 0.8784\n",
      "Epoch 89/300\n",
      "74/74 [==============================] - 0s 186us/sample - loss: 0.0360 - accuracy: 0.8649\n",
      "Epoch 90/300\n",
      "74/74 [==============================] - 0s 159us/sample - loss: 0.0361 - accuracy: 0.8649\n",
      "Epoch 91/300\n",
      "74/74 [==============================] - 0s 163us/sample - loss: 0.0362 - accuracy: 0.8649\n",
      "Epoch 92/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0336 - accuracy: 0.8919\n",
      "Epoch 93/300\n",
      "74/74 [==============================] - 0s 167us/sample - loss: 0.0342 - accuracy: 0.8784\n",
      "Epoch 94/300\n",
      "74/74 [==============================] - 0s 181us/sample - loss: 0.0338 - accuracy: 0.8649\n",
      "Epoch 95/300\n",
      "74/74 [==============================] - 0s 156us/sample - loss: 0.0373 - accuracy: 0.8649\n",
      "Epoch 96/300\n",
      "74/74 [==============================] - 0s 140us/sample - loss: 0.0350 - accuracy: 0.8649\n",
      "Epoch 97/300\n",
      "74/74 [==============================] - 0s 152us/sample - loss: 0.0343 - accuracy: 0.8784\n",
      "Epoch 98/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0329 - accuracy: 0.8649\n",
      "Epoch 99/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0333 - accuracy: 0.8649\n",
      "Epoch 100/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0323 - accuracy: 0.8784\n",
      "Epoch 101/300\n",
      "74/74 [==============================] - 0s 145us/sample - loss: 0.0329 - accuracy: 0.8649\n",
      "Epoch 102/300\n",
      "74/74 [==============================] - 0s 140us/sample - loss: 0.0312 - accuracy: 0.8649\n",
      "Epoch 103/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0311 - accuracy: 0.8649\n",
      "Epoch 104/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0357 - accuracy: 0.8784\n",
      "Epoch 105/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0314 - accuracy: 0.8649\n",
      "Epoch 106/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0383 - accuracy: 0.8649\n",
      "Epoch 107/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0355 - accuracy: 0.8919\n",
      "Epoch 108/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0337 - accuracy: 0.8649\n",
      "Epoch 109/300\n",
      "74/74 [==============================] - 0s 141us/sample - loss: 0.0375 - accuracy: 0.8649\n",
      "Epoch 110/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0365 - accuracy: 0.8649\n",
      "Epoch 111/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0331 - accuracy: 0.8784\n",
      "Epoch 112/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0321 - accuracy: 0.8649\n",
      "Epoch 113/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0391 - accuracy: 0.8649\n",
      "Epoch 114/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0372 - accuracy: 0.8649\n",
      "Epoch 115/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0347 - accuracy: 0.8649\n",
      "Epoch 116/300\n",
      "74/74 [==============================] - 0s 147us/sample - loss: 0.0308 - accuracy: 0.8649\n",
      "Epoch 117/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0313 - accuracy: 0.8649\n",
      "Epoch 118/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0335 - accuracy: 0.8649\n",
      "Epoch 119/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0313 - accuracy: 0.8784\n",
      "Epoch 120/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0352 - accuracy: 0.8649\n",
      "Epoch 121/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0318 - accuracy: 0.8649\n",
      "Epoch 122/300\n",
      "74/74 [==============================] - 0s 142us/sample - loss: 0.0360 - accuracy: 0.8784\n",
      "Epoch 123/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0311 - accuracy: 0.8649\n",
      "Epoch 124/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0299 - accuracy: 0.8784\n",
      "Epoch 125/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0348 - accuracy: 0.8649\n",
      "Epoch 126/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0294 - accuracy: 0.8649\n",
      "Epoch 127/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0349 - accuracy: 0.8649\n",
      "Epoch 128/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0311 - accuracy: 0.8649\n",
      "Epoch 129/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0307 - accuracy: 0.8649\n",
      "Epoch 130/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0348 - accuracy: 0.8649\n",
      "Epoch 131/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0294 - accuracy: 0.8649\n",
      "Epoch 132/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0320 - accuracy: 0.8784\n",
      "Epoch 133/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0306 - accuracy: 0.8649\n",
      "Epoch 134/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0274 - accuracy: 0.8919\n",
      "Epoch 135/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0335 - accuracy: 0.8649\n",
      "Epoch 136/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0363 - accuracy: 0.8514\n",
      "Epoch 137/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0357 - accuracy: 0.8514\n",
      "Epoch 138/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0293 - accuracy: 0.8649\n",
      "Epoch 139/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0308 - accuracy: 0.8784\n",
      "Epoch 140/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0304 - accuracy: 0.8919\n",
      "Epoch 141/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0326 - accuracy: 0.8919\n",
      "Epoch 142/300\n",
      "74/74 [==============================] - 0s 154us/sample - loss: 0.0332 - accuracy: 0.8649\n",
      "Epoch 143/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0344 - accuracy: 0.8649\n",
      "Epoch 144/300\n",
      "74/74 [==============================] - 0s 145us/sample - loss: 0.0326 - accuracy: 0.8784\n",
      "Epoch 145/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0342 - accuracy: 0.8649\n",
      "Epoch 146/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0333 - accuracy: 0.8649\n",
      "Epoch 147/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 0s 140us/sample - loss: 0.0317 - accuracy: 0.8784\n",
      "Epoch 148/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0343 - accuracy: 0.8649\n",
      "Epoch 149/300\n",
      "74/74 [==============================] - 0s 140us/sample - loss: 0.0343 - accuracy: 0.8649\n",
      "Epoch 150/300\n",
      "74/74 [==============================] - 0s 140us/sample - loss: 0.0335 - accuracy: 0.8649\n",
      "Epoch 151/300\n",
      "74/74 [==============================] - 0s 159us/sample - loss: 0.0344 - accuracy: 0.8649\n",
      "Epoch 152/300\n",
      "74/74 [==============================] - 0s 141us/sample - loss: 0.0331 - accuracy: 0.8919\n",
      "Epoch 153/300\n",
      "74/74 [==============================] - 0s 172us/sample - loss: 0.0317 - accuracy: 0.8649\n",
      "Epoch 154/300\n",
      "74/74 [==============================] - 0s 169us/sample - loss: 0.0321 - accuracy: 0.8784\n",
      "Epoch 155/300\n",
      "74/74 [==============================] - 0s 153us/sample - loss: 0.0305 - accuracy: 0.8649\n",
      "Epoch 156/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0314 - accuracy: 0.8919\n",
      "Epoch 157/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0344 - accuracy: 0.8784\n",
      "Epoch 158/300\n",
      "74/74 [==============================] - 0s 143us/sample - loss: 0.0339 - accuracy: 0.8649\n",
      "Epoch 159/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0322 - accuracy: 0.8649\n",
      "Epoch 160/300\n",
      "74/74 [==============================] - 0s 143us/sample - loss: 0.0359 - accuracy: 0.8649\n",
      "Epoch 161/300\n",
      "74/74 [==============================] - 0s 155us/sample - loss: 0.0268 - accuracy: 0.8649\n",
      "Epoch 162/300\n",
      "74/74 [==============================] - 0s 149us/sample - loss: 0.0309 - accuracy: 0.8784\n",
      "Epoch 163/300\n",
      "74/74 [==============================] - 0s 145us/sample - loss: 0.0338 - accuracy: 0.8784\n",
      "Epoch 164/300\n",
      "74/74 [==============================] - 0s 154us/sample - loss: 0.0306 - accuracy: 0.8649\n",
      "Epoch 165/300\n",
      "74/74 [==============================] - 0s 152us/sample - loss: 0.0297 - accuracy: 0.8784\n",
      "Epoch 166/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0323 - accuracy: 0.8919\n",
      "Epoch 167/300\n",
      "74/74 [==============================] - 0s 143us/sample - loss: 0.0331 - accuracy: 0.8919\n",
      "Epoch 168/300\n",
      "74/74 [==============================] - 0s 170us/sample - loss: 0.0383 - accuracy: 0.8649\n",
      "Epoch 169/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0342 - accuracy: 0.8649\n",
      "Epoch 170/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0325 - accuracy: 0.8649\n",
      "Epoch 171/300\n",
      "74/74 [==============================] - 0s 147us/sample - loss: 0.0309 - accuracy: 0.8649\n",
      "Epoch 172/300\n",
      "74/74 [==============================] - 0s 153us/sample - loss: 0.0351 - accuracy: 0.8784\n",
      "Epoch 173/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0317 - accuracy: 0.8784\n",
      "Epoch 174/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0314 - accuracy: 0.8649\n",
      "Epoch 175/300\n",
      "74/74 [==============================] - 0s 151us/sample - loss: 0.0300 - accuracy: 0.8649\n",
      "Epoch 176/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0343 - accuracy: 0.8649\n",
      "Epoch 177/300\n",
      "74/74 [==============================] - 0s 145us/sample - loss: 0.0330 - accuracy: 0.8649\n",
      "Epoch 178/300\n",
      "74/74 [==============================] - 0s 141us/sample - loss: 0.0349 - accuracy: 0.8649\n",
      "Epoch 179/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0309 - accuracy: 0.8784\n",
      "Epoch 180/300\n",
      "74/74 [==============================] - 0s 152us/sample - loss: 0.0331 - accuracy: 0.8784\n",
      "Epoch 181/300\n",
      "74/74 [==============================] - 0s 197us/sample - loss: 0.0340 - accuracy: 0.8649\n",
      "Epoch 182/300\n",
      "74/74 [==============================] - 0s 137us/sample - loss: 0.0282 - accuracy: 0.8919\n",
      "Epoch 183/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0294 - accuracy: 0.8649\n",
      "Epoch 184/300\n",
      "74/74 [==============================] - 0s 147us/sample - loss: 0.0318 - accuracy: 0.8649\n",
      "Epoch 185/300\n",
      "74/74 [==============================] - 0s 275us/sample - loss: 0.0319 - accuracy: 0.8649\n",
      "Epoch 186/300\n",
      "74/74 [==============================] - 0s 160us/sample - loss: 0.0323 - accuracy: 0.8649\n",
      "Epoch 187/300\n",
      "74/74 [==============================] - 0s 166us/sample - loss: 0.0309 - accuracy: 0.8649\n",
      "Epoch 188/300\n",
      "74/74 [==============================] - 0s 176us/sample - loss: 0.0349 - accuracy: 0.8649\n",
      "Epoch 189/300\n",
      "74/74 [==============================] - 0s 176us/sample - loss: 0.0340 - accuracy: 0.8649\n",
      "Epoch 190/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0331 - accuracy: 0.8649\n",
      "Epoch 191/300\n",
      "74/74 [==============================] - 0s 153us/sample - loss: 0.0322 - accuracy: 0.8649\n",
      "Epoch 192/300\n",
      "74/74 [==============================] - 0s 149us/sample - loss: 0.0328 - accuracy: 0.8784\n",
      "Epoch 193/300\n",
      "74/74 [==============================] - 0s 137us/sample - loss: 0.0306 - accuracy: 0.8919\n",
      "Epoch 194/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0337 - accuracy: 0.8649\n",
      "Epoch 195/300\n",
      "74/74 [==============================] - 0s 144us/sample - loss: 0.0352 - accuracy: 0.8649\n",
      "Epoch 196/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0326 - accuracy: 0.8649\n",
      "Epoch 197/300\n",
      "74/74 [==============================] - 0s 147us/sample - loss: 0.0325 - accuracy: 0.8784\n",
      "Epoch 198/300\n",
      "74/74 [==============================] - 0s 148us/sample - loss: 0.0330 - accuracy: 0.8649\n",
      "Epoch 199/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0364 - accuracy: 0.8514\n",
      "Epoch 200/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0330 - accuracy: 0.8649\n",
      "Epoch 201/300\n",
      "74/74 [==============================] - 0s 149us/sample - loss: 0.0283 - accuracy: 0.8649\n",
      "Epoch 202/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0323 - accuracy: 0.8514\n",
      "Epoch 203/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0320 - accuracy: 0.8649\n",
      "Epoch 204/300\n",
      "74/74 [==============================] - 0s 161us/sample - loss: 0.0306 - accuracy: 0.8649\n",
      "Epoch 205/300\n",
      "74/74 [==============================] - 0s 144us/sample - loss: 0.0316 - accuracy: 0.8784\n",
      "Epoch 206/300\n",
      "74/74 [==============================] - 0s 157us/sample - loss: 0.0294 - accuracy: 0.8784\n",
      "Epoch 207/300\n",
      "74/74 [==============================] - 0s 149us/sample - loss: 0.0311 - accuracy: 0.8784\n",
      "Epoch 208/300\n",
      "74/74 [==============================] - 0s 179us/sample - loss: 0.0315 - accuracy: 0.8649\n",
      "Epoch 209/300\n",
      "74/74 [==============================] - 0s 187us/sample - loss: 0.0344 - accuracy: 0.8649\n",
      "Epoch 210/300\n",
      "74/74 [==============================] - 0s 155us/sample - loss: 0.0316 - accuracy: 0.8649\n",
      "Epoch 211/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0340 - accuracy: 0.8649\n",
      "Epoch 212/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0282 - accuracy: 0.8649\n",
      "Epoch 213/300\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.86 - 0s 135us/sample - loss: 0.0322 - accuracy: 0.8919\n",
      "Epoch 214/300\n",
      "74/74 [==============================] - 0s 141us/sample - loss: 0.0380 - accuracy: 0.8649\n",
      "Epoch 215/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0305 - accuracy: 0.8649\n",
      "Epoch 216/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0336 - accuracy: 0.8649\n",
      "Epoch 217/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0307 - accuracy: 0.8649\n",
      "Epoch 218/300\n",
      "74/74 [==============================] - 0s 140us/sample - loss: 0.0334 - accuracy: 0.8649\n",
      "Epoch 219/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0305 - accuracy: 0.8649\n",
      "Epoch 220/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0337 - accuracy: 0.8649\n",
      "Epoch 221/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0306 - accuracy: 0.8649\n",
      "Epoch 222/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0347 - accuracy: 0.8649\n",
      "Epoch 223/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0294 - accuracy: 0.8649\n",
      "Epoch 224/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0344 - accuracy: 0.8649\n",
      "Epoch 225/300\n",
      "74/74 [==============================] - 0s 141us/sample - loss: 0.0312 - accuracy: 0.8649\n",
      "Epoch 226/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0313 - accuracy: 0.8649\n",
      "Epoch 227/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0321 - accuracy: 0.8649\n",
      "Epoch 228/300\n",
      "74/74 [==============================] - 0s 153us/sample - loss: 0.0322 - accuracy: 0.8649\n",
      "Epoch 229/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0317 - accuracy: 0.8649\n",
      "Epoch 230/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0275 - accuracy: 0.8649\n",
      "Epoch 231/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0333 - accuracy: 0.8784\n",
      "Epoch 232/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0300 - accuracy: 0.8649\n",
      "Epoch 233/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0315 - accuracy: 0.8784\n",
      "Epoch 234/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0345 - accuracy: 0.8919\n",
      "Epoch 235/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0320 - accuracy: 0.8649\n",
      "Epoch 236/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0284 - accuracy: 0.8784\n",
      "Epoch 237/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0326 - accuracy: 0.8649\n",
      "Epoch 238/300\n",
      "74/74 [==============================] - 0s 159us/sample - loss: 0.0332 - accuracy: 0.8649\n",
      "Epoch 239/300\n",
      "74/74 [==============================] - 0s 163us/sample - loss: 0.0317 - accuracy: 0.8649\n",
      "Epoch 240/300\n",
      "74/74 [==============================] - 0s 140us/sample - loss: 0.0283 - accuracy: 0.8649\n",
      "Epoch 241/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0352 - accuracy: 0.8649\n",
      "Epoch 242/300\n",
      "74/74 [==============================] - 0s 157us/sample - loss: 0.0313 - accuracy: 0.8649\n",
      "Epoch 243/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0332 - accuracy: 0.8649\n",
      "Epoch 244/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0315 - accuracy: 0.8784\n",
      "Epoch 245/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0367 - accuracy: 0.8649\n",
      "Epoch 246/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0328 - accuracy: 0.8649\n",
      "Epoch 247/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0313 - accuracy: 0.8649\n",
      "Epoch 248/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0278 - accuracy: 0.8784\n",
      "Epoch 249/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0335 - accuracy: 0.8649\n",
      "Epoch 250/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0310 - accuracy: 0.8784\n",
      "Epoch 251/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0328 - accuracy: 0.8649\n",
      "Epoch 252/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0297 - accuracy: 0.8649\n",
      "Epoch 253/300\n",
      "74/74 [==============================] - 0s 148us/sample - loss: 0.0289 - accuracy: 0.8784\n",
      "Epoch 254/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0292 - accuracy: 0.8649\n",
      "Epoch 255/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0297 - accuracy: 0.8784\n",
      "Epoch 256/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0313 - accuracy: 0.8784\n",
      "Epoch 257/300\n",
      "74/74 [==============================] - 0s 144us/sample - loss: 0.0324 - accuracy: 0.8649\n",
      "Epoch 258/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0351 - accuracy: 0.8649\n",
      "Epoch 259/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0295 - accuracy: 0.8784\n",
      "Epoch 260/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0341 - accuracy: 0.8649\n",
      "Epoch 261/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0302 - accuracy: 0.8649\n",
      "Epoch 262/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0318 - accuracy: 0.8649\n",
      "Epoch 263/300\n",
      "74/74 [==============================] - 0s 141us/sample - loss: 0.0325 - accuracy: 0.8649\n",
      "Epoch 264/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0309 - accuracy: 0.8784\n",
      "Epoch 265/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0357 - accuracy: 0.8514\n",
      "Epoch 266/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0359 - accuracy: 0.8784\n",
      "Epoch 267/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0312 - accuracy: 0.8784\n",
      "Epoch 268/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0323 - accuracy: 0.8649\n",
      "Epoch 269/300\n",
      "74/74 [==============================] - 0s 110us/sample - loss: 0.0317 - accuracy: 0.8649\n",
      "Epoch 270/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0280 - accuracy: 0.8784\n",
      "Epoch 271/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0347 - accuracy: 0.8649\n",
      "Epoch 272/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0352 - accuracy: 0.8649\n",
      "Epoch 273/300\n",
      "74/74 [==============================] - 0s 154us/sample - loss: 0.0317 - accuracy: 0.8784\n",
      "Epoch 274/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0346 - accuracy: 0.8514\n",
      "Epoch 275/300\n",
      "74/74 [==============================] - 0s 148us/sample - loss: 0.0312 - accuracy: 0.8649\n",
      "Epoch 276/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0316 - accuracy: 0.8649\n",
      "Epoch 277/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0294 - accuracy: 0.8649\n",
      "Epoch 278/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0265 - accuracy: 0.8649\n",
      "Epoch 279/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0299 - accuracy: 0.8649\n",
      "Epoch 280/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0319 - accuracy: 0.8649\n",
      "Epoch 281/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0324 - accuracy: 0.8649\n",
      "Epoch 282/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0290 - accuracy: 0.8649\n",
      "Epoch 283/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0290 - accuracy: 0.8649\n",
      "Epoch 284/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0283 - accuracy: 0.8649\n",
      "Epoch 285/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0324 - accuracy: 0.8649\n",
      "Epoch 286/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0334 - accuracy: 0.8919\n",
      "Epoch 287/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0316 - accuracy: 0.8649\n",
      "Epoch 288/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0310 - accuracy: 0.8784\n",
      "Epoch 289/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0321 - accuracy: 0.8649\n",
      "Epoch 290/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0294 - accuracy: 0.8649\n",
      "Epoch 291/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0323 - accuracy: 0.8784\n",
      "Epoch 292/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0308 - accuracy: 0.8649\n",
      "Epoch 293/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0339 - accuracy: 0.8649\n",
      "Epoch 294/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0286 - accuracy: 0.8784\n",
      "Epoch 295/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0313 - accuracy: 0.8649\n",
      "Epoch 296/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0296 - accuracy: 0.8784\n",
      "Epoch 297/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0344 - accuracy: 0.8649\n",
      "Epoch 298/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0350 - accuracy: 0.8649\n",
      "Epoch 299/300\n",
      "74/74 [==============================] - 0s 159us/sample - loss: 0.0287 - accuracy: 0.8784\n",
      "Epoch 300/300\n",
      "74/74 [==============================] - 0s 147us/sample - loss: 0.0297 - accuracy: 0.8919\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_32 (Dense)             (None, 128)               4608      \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,737\n",
      "Trainable params: 4,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 74 samples\n",
      "Epoch 1/300\n",
      "74/74 [==============================] - 0s 7ms/sample - loss: 0.1450 - accuracy: 0.5270\n",
      "Epoch 2/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.1398 - accuracy: 0.6216\n",
      "Epoch 3/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.1370 - accuracy: 0.7162\n",
      "Epoch 4/300\n",
      "74/74 [==============================] - 0s 149us/sample - loss: 0.1344 - accuracy: 0.6757\n",
      "Epoch 5/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.1283 - accuracy: 0.7297\n",
      "Epoch 6/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.1210 - accuracy: 0.8108\n",
      "Epoch 7/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.1156 - accuracy: 0.8108\n",
      "Epoch 8/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.1159 - accuracy: 0.8108\n",
      "Epoch 9/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.1104 - accuracy: 0.8514\n",
      "Epoch 10/300\n",
      "74/74 [==============================] - 0s 149us/sample - loss: 0.1116 - accuracy: 0.8378\n",
      "Epoch 11/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.1070 - accuracy: 0.8514\n",
      "Epoch 12/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.1041 - accuracy: 0.8649\n",
      "Epoch 13/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.1013 - accuracy: 0.9054\n",
      "Epoch 14/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0959 - accuracy: 0.8919\n",
      "Epoch 15/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0922 - accuracy: 0.8919\n",
      "Epoch 16/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0905 - accuracy: 0.8919\n",
      "Epoch 17/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0891 - accuracy: 0.9054\n",
      "Epoch 18/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0845 - accuracy: 0.9054\n",
      "Epoch 19/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0810 - accuracy: 0.9054\n",
      "Epoch 20/300\n",
      "74/74 [==============================] - 0s 144us/sample - loss: 0.0826 - accuracy: 0.9189\n",
      "Epoch 21/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0779 - accuracy: 0.8919\n",
      "Epoch 22/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0740 - accuracy: 0.9189\n",
      "Epoch 23/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0717 - accuracy: 0.8919\n",
      "Epoch 24/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0735 - accuracy: 0.9189\n",
      "Epoch 25/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0672 - accuracy: 0.9054\n",
      "Epoch 26/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0676 - accuracy: 0.9189\n",
      "Epoch 27/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0616 - accuracy: 0.9054\n",
      "Epoch 28/300\n",
      "74/74 [==============================] - 0s 142us/sample - loss: 0.0604 - accuracy: 0.9054\n",
      "Epoch 29/300\n",
      "74/74 [==============================] - 0s 142us/sample - loss: 0.0620 - accuracy: 0.9054\n",
      "Epoch 30/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0572 - accuracy: 0.9054\n",
      "Epoch 31/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0568 - accuracy: 0.9054\n",
      "Epoch 32/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0594 - accuracy: 0.9189\n",
      "Epoch 33/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0537 - accuracy: 0.9189\n",
      "Epoch 34/300\n",
      "74/74 [==============================] - 0s 147us/sample - loss: 0.0549 - accuracy: 0.9054\n",
      "Epoch 35/300\n",
      "74/74 [==============================] - 0s 181us/sample - loss: 0.0497 - accuracy: 0.9189\n",
      "Epoch 36/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0471 - accuracy: 0.9189\n",
      "Epoch 37/300\n",
      "74/74 [==============================] - 0s 168us/sample - loss: 0.0488 - accuracy: 0.9189\n",
      "Epoch 38/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0489 - accuracy: 0.9054\n",
      "Epoch 39/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0475 - accuracy: 0.9054\n",
      "Epoch 40/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0511 - accuracy: 0.9189\n",
      "Epoch 41/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0407 - accuracy: 0.9189\n",
      "Epoch 42/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0468 - accuracy: 0.9189\n",
      "Epoch 43/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0472 - accuracy: 0.9189\n",
      "Epoch 44/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0413 - accuracy: 0.9189\n",
      "Epoch 45/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0420 - accuracy: 0.9189\n",
      "Epoch 46/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0439 - accuracy: 0.9189\n",
      "Epoch 47/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0451 - accuracy: 0.9189\n",
      "Epoch 48/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0423 - accuracy: 0.9189\n",
      "Epoch 49/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0395 - accuracy: 0.9189\n",
      "Epoch 50/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0415 - accuracy: 0.9189\n",
      "Epoch 51/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0373 - accuracy: 0.9189\n",
      "Epoch 52/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0430 - accuracy: 0.9189\n",
      "Epoch 53/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0393 - accuracy: 0.9189\n",
      "Epoch 54/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0416 - accuracy: 0.9189\n",
      "Epoch 55/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0371 - accuracy: 0.9189\n",
      "Epoch 56/300\n",
      "74/74 [==============================] - 0s 137us/sample - loss: 0.0410 - accuracy: 0.9189\n",
      "Epoch 57/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0360 - accuracy: 0.9189\n",
      "Epoch 58/300\n",
      "74/74 [==============================] - 0s 148us/sample - loss: 0.0415 - accuracy: 0.9189\n",
      "Epoch 59/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0354 - accuracy: 0.9189\n",
      "Epoch 60/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0341 - accuracy: 0.9189\n",
      "Epoch 61/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0359 - accuracy: 0.9189\n",
      "Epoch 62/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0342 - accuracy: 0.9189\n",
      "Epoch 63/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0348 - accuracy: 0.9189\n",
      "Epoch 64/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0362 - accuracy: 0.9189\n",
      "Epoch 65/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0310 - accuracy: 0.9189\n",
      "Epoch 66/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0377 - accuracy: 0.9189\n",
      "Epoch 67/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0377 - accuracy: 0.9189\n",
      "Epoch 68/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0340 - accuracy: 0.9189\n",
      "Epoch 69/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0334 - accuracy: 0.9189\n",
      "Epoch 70/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0314 - accuracy: 0.9189\n",
      "Epoch 71/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0379 - accuracy: 0.9189\n",
      "Epoch 72/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0344 - accuracy: 0.9189\n",
      "Epoch 73/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0298 - accuracy: 0.9189\n",
      "Epoch 74/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0351 - accuracy: 0.9189\n",
      "Epoch 75/300\n",
      "74/74 [==============================] - 0s 145us/sample - loss: 0.0300 - accuracy: 0.9189\n",
      "Epoch 76/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0315 - accuracy: 0.9189\n",
      "Epoch 77/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0390 - accuracy: 0.9189\n",
      "Epoch 78/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0309 - accuracy: 0.9189\n",
      "Epoch 79/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0281 - accuracy: 0.9189\n",
      "Epoch 80/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0349 - accuracy: 0.9189\n",
      "Epoch 81/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0320 - accuracy: 0.9189\n",
      "Epoch 82/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0327 - accuracy: 0.9189\n",
      "Epoch 83/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0335 - accuracy: 0.9189\n",
      "Epoch 84/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0304 - accuracy: 0.9189\n",
      "Epoch 85/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0349 - accuracy: 0.9189\n",
      "Epoch 86/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0335 - accuracy: 0.9189\n",
      "Epoch 87/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0290 - accuracy: 0.9189\n",
      "Epoch 88/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0340 - accuracy: 0.9189\n",
      "Epoch 89/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0293 - accuracy: 0.9189\n",
      "Epoch 90/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0298 - accuracy: 0.9189\n",
      "Epoch 91/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0285 - accuracy: 0.9189\n",
      "Epoch 92/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0313 - accuracy: 0.9189\n",
      "Epoch 93/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0311 - accuracy: 0.9189\n",
      "Epoch 94/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0320 - accuracy: 0.9189\n",
      "Epoch 95/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0299 - accuracy: 0.9189\n",
      "Epoch 96/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0321 - accuracy: 0.9189\n",
      "Epoch 97/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0302 - accuracy: 0.9189\n",
      "Epoch 98/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0300 - accuracy: 0.9189\n",
      "Epoch 99/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0312 - accuracy: 0.9189\n",
      "Epoch 100/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0322 - accuracy: 0.9189\n",
      "Epoch 101/300\n",
      "74/74 [==============================] - 0s 142us/sample - loss: 0.0293 - accuracy: 0.9189\n",
      "Epoch 102/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0270 - accuracy: 0.9189\n",
      "Epoch 103/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0324 - accuracy: 0.9189\n",
      "Epoch 104/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0315 - accuracy: 0.9189\n",
      "Epoch 105/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0301 - accuracy: 0.9189\n",
      "Epoch 106/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0288 - accuracy: 0.9189\n",
      "Epoch 107/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0301 - accuracy: 0.9189\n",
      "Epoch 108/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0310 - accuracy: 0.9189\n",
      "Epoch 109/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0329 - accuracy: 0.9189\n",
      "Epoch 110/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0300 - accuracy: 0.9189\n",
      "Epoch 111/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0283 - accuracy: 0.9189\n",
      "Epoch 112/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0303 - accuracy: 0.9189\n",
      "Epoch 113/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0291 - accuracy: 0.9189\n",
      "Epoch 114/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0281 - accuracy: 0.9189\n",
      "Epoch 115/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0293 - accuracy: 0.9189\n",
      "Epoch 116/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0314 - accuracy: 0.9189\n",
      "Epoch 117/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0297 - accuracy: 0.9189\n",
      "Epoch 118/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0324 - accuracy: 0.9189\n",
      "Epoch 119/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0303 - accuracy: 0.9189\n",
      "Epoch 120/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0313 - accuracy: 0.9189\n",
      "Epoch 121/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0305 - accuracy: 0.9189\n",
      "Epoch 122/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0282 - accuracy: 0.9189\n",
      "Epoch 123/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0267 - accuracy: 0.9189\n",
      "Epoch 124/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0300 - accuracy: 0.9189\n",
      "Epoch 125/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0292 - accuracy: 0.9189\n",
      "Epoch 126/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0313 - accuracy: 0.9189\n",
      "Epoch 127/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0309 - accuracy: 0.9189\n",
      "Epoch 128/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0310 - accuracy: 0.9189\n",
      "Epoch 129/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0294 - accuracy: 0.9189\n",
      "Epoch 130/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0288 - accuracy: 0.9189\n",
      "Epoch 131/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0274 - accuracy: 0.9189\n",
      "Epoch 132/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0251 - accuracy: 0.9189\n",
      "Epoch 133/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0287 - accuracy: 0.9189\n",
      "Epoch 134/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0277 - accuracy: 0.9189\n",
      "Epoch 135/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0291 - accuracy: 0.9189\n",
      "Epoch 136/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0313 - accuracy: 0.9189\n",
      "Epoch 137/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0290 - accuracy: 0.9189\n",
      "Epoch 138/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0287 - accuracy: 0.9189\n",
      "Epoch 139/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0279 - accuracy: 0.9189\n",
      "Epoch 140/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0288 - accuracy: 0.9189\n",
      "Epoch 141/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0265 - accuracy: 0.9189\n",
      "Epoch 142/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0263 - accuracy: 0.9189\n",
      "Epoch 143/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0266 - accuracy: 0.9189\n",
      "Epoch 144/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0283 - accuracy: 0.9189\n",
      "Epoch 145/300\n",
      "74/74 [==============================] - 0s 145us/sample - loss: 0.0280 - accuracy: 0.9189\n",
      "Epoch 146/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0284 - accuracy: 0.9189\n",
      "Epoch 147/300\n",
      "74/74 [==============================] - 0s 137us/sample - loss: 0.0259 - accuracy: 0.9189\n",
      "Epoch 148/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0303 - accuracy: 0.9189\n",
      "Epoch 149/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0268 - accuracy: 0.9189\n",
      "Epoch 150/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0289 - accuracy: 0.9189\n",
      "Epoch 151/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0282 - accuracy: 0.9189\n",
      "Epoch 152/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0268 - accuracy: 0.9189\n",
      "Epoch 153/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0259 - accuracy: 0.9189\n",
      "Epoch 154/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0304 - accuracy: 0.9189\n",
      "Epoch 155/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0281 - accuracy: 0.9189\n",
      "Epoch 156/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0280 - accuracy: 0.9189\n",
      "Epoch 157/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0281 - accuracy: 0.9189\n",
      "Epoch 158/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0273 - accuracy: 0.9189\n",
      "Epoch 159/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0271 - accuracy: 0.9189\n",
      "Epoch 160/300\n",
      "74/74 [==============================] - 0s 137us/sample - loss: 0.0249 - accuracy: 0.9189\n",
      "Epoch 161/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0293 - accuracy: 0.9189\n",
      "Epoch 162/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0280 - accuracy: 0.9189\n",
      "Epoch 163/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0286 - accuracy: 0.9189\n",
      "Epoch 164/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0292 - accuracy: 0.9189\n",
      "Epoch 165/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0250 - accuracy: 0.9189\n",
      "Epoch 166/300\n",
      "74/74 [==============================] - 0s 143us/sample - loss: 0.0254 - accuracy: 0.9189\n",
      "Epoch 167/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0285 - accuracy: 0.9189\n",
      "Epoch 168/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0268 - accuracy: 0.9189\n",
      "Epoch 169/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0293 - accuracy: 0.9189\n",
      "Epoch 170/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0248 - accuracy: 0.9189\n",
      "Epoch 171/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0234 - accuracy: 0.9189\n",
      "Epoch 172/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0264 - accuracy: 0.9189\n",
      "Epoch 173/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0286 - accuracy: 0.9189\n",
      "Epoch 174/300\n",
      "74/74 [==============================] - 0s 142us/sample - loss: 0.0300 - accuracy: 0.9189\n",
      "Epoch 175/300\n",
      "74/74 [==============================] - 0s 172us/sample - loss: 0.0229 - accuracy: 0.9189\n",
      "Epoch 176/300\n",
      "74/74 [==============================] - 0s 142us/sample - loss: 0.0252 - accuracy: 0.9189\n",
      "Epoch 177/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0308 - accuracy: 0.9189\n",
      "Epoch 178/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0283 - accuracy: 0.9189\n",
      "Epoch 179/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0291 - accuracy: 0.9189\n",
      "Epoch 180/300\n",
      "74/74 [==============================] - 0s 141us/sample - loss: 0.0258 - accuracy: 0.9189\n",
      "Epoch 181/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0266 - accuracy: 0.9189\n",
      "Epoch 182/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0287 - accuracy: 0.9189\n",
      "Epoch 183/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0278 - accuracy: 0.9189\n",
      "Epoch 184/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0275 - accuracy: 0.9189\n",
      "Epoch 185/300\n",
      "74/74 [==============================] - 0s 166us/sample - loss: 0.0286 - accuracy: 0.9189\n",
      "Epoch 186/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0247 - accuracy: 0.9189\n",
      "Epoch 187/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0293 - accuracy: 0.9189\n",
      "Epoch 188/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0272 - accuracy: 0.9189\n",
      "Epoch 189/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0317 - accuracy: 0.9189\n",
      "Epoch 190/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0289 - accuracy: 0.9189\n",
      "Epoch 191/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0270 - accuracy: 0.9189\n",
      "Epoch 192/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0322 - accuracy: 0.9189\n",
      "Epoch 193/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0294 - accuracy: 0.9189\n",
      "Epoch 194/300\n",
      "74/74 [==============================] - 0s 170us/sample - loss: 0.0258 - accuracy: 0.9189\n",
      "Epoch 195/300\n",
      "74/74 [==============================] - 0s 137us/sample - loss: 0.0285 - accuracy: 0.9189\n",
      "Epoch 196/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0265 - accuracy: 0.9189\n",
      "Epoch 197/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0256 - accuracy: 0.9189\n",
      "Epoch 198/300\n",
      "74/74 [==============================] - 0s 141us/sample - loss: 0.0293 - accuracy: 0.9189\n",
      "Epoch 199/300\n",
      "74/74 [==============================] - 0s 146us/sample - loss: 0.0281 - accuracy: 0.9189\n",
      "Epoch 200/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0247 - accuracy: 0.9189\n",
      "Epoch 201/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0268 - accuracy: 0.9189\n",
      "Epoch 202/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0262 - accuracy: 0.9189\n",
      "Epoch 203/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0272 - accuracy: 0.9189\n",
      "Epoch 204/300\n",
      "74/74 [==============================] - 0s 143us/sample - loss: 0.0261 - accuracy: 0.9189\n",
      "Epoch 205/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0293 - accuracy: 0.9189\n",
      "Epoch 206/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0245 - accuracy: 0.9189\n",
      "Epoch 207/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0275 - accuracy: 0.9189\n",
      "Epoch 208/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0290 - accuracy: 0.9189\n",
      "Epoch 209/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0278 - accuracy: 0.9189\n",
      "Epoch 210/300\n",
      "74/74 [==============================] - 0s 137us/sample - loss: 0.0268 - accuracy: 0.9189\n",
      "Epoch 211/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0265 - accuracy: 0.9189\n",
      "Epoch 212/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0269 - accuracy: 0.9189\n",
      "Epoch 213/300\n",
      "74/74 [==============================] - 0s 150us/sample - loss: 0.0275 - accuracy: 0.9189\n",
      "Epoch 214/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0251 - accuracy: 0.9189\n",
      "Epoch 215/300\n",
      "74/74 [==============================] - 0s 141us/sample - loss: 0.0266 - accuracy: 0.9189\n",
      "Epoch 216/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0281 - accuracy: 0.9189\n",
      "Epoch 217/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0276 - accuracy: 0.9189\n",
      "Epoch 218/300\n",
      "74/74 [==============================] - 0s 141us/sample - loss: 0.0325 - accuracy: 0.9189\n",
      "Epoch 219/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0322 - accuracy: 0.9189\n",
      "Epoch 220/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0281 - accuracy: 0.9189\n",
      "Epoch 221/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0241 - accuracy: 0.9189\n",
      "Epoch 222/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0266 - accuracy: 0.9189\n",
      "Epoch 223/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0280 - accuracy: 0.9189\n",
      "Epoch 224/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0285 - accuracy: 0.9189\n",
      "Epoch 225/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0263 - accuracy: 0.9189\n",
      "Epoch 226/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0281 - accuracy: 0.9189\n",
      "Epoch 227/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0235 - accuracy: 0.9189\n",
      "Epoch 228/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0249 - accuracy: 0.9189\n",
      "Epoch 229/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0268 - accuracy: 0.9189\n",
      "Epoch 230/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0267 - accuracy: 0.9189\n",
      "Epoch 231/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0301 - accuracy: 0.9189\n",
      "Epoch 232/300\n",
      "74/74 [==============================] - 0s 144us/sample - loss: 0.0297 - accuracy: 0.9189\n",
      "Epoch 233/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0263 - accuracy: 0.9189\n",
      "Epoch 234/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0258 - accuracy: 0.9189\n",
      "Epoch 235/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0268 - accuracy: 0.9189\n",
      "Epoch 236/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0270 - accuracy: 0.9189\n",
      "Epoch 237/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0271 - accuracy: 0.9189\n",
      "Epoch 238/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0262 - accuracy: 0.9189\n",
      "Epoch 239/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0313 - accuracy: 0.9189\n",
      "Epoch 240/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0247 - accuracy: 0.9189\n",
      "Epoch 241/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0250 - accuracy: 0.9189\n",
      "Epoch 242/300\n",
      "74/74 [==============================] - 0s 150us/sample - loss: 0.0250 - accuracy: 0.9189\n",
      "Epoch 243/300\n",
      "74/74 [==============================] - 0s 146us/sample - loss: 0.0260 - accuracy: 0.9189\n",
      "Epoch 244/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0245 - accuracy: 0.9189\n",
      "Epoch 245/300\n",
      "74/74 [==============================] - 0s 137us/sample - loss: 0.0255 - accuracy: 0.9189\n",
      "Epoch 246/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0274 - accuracy: 0.9189\n",
      "Epoch 247/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0258 - accuracy: 0.9189\n",
      "Epoch 248/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0257 - accuracy: 0.9189\n",
      "Epoch 249/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0275 - accuracy: 0.9189\n",
      "Epoch 250/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0283 - accuracy: 0.9189\n",
      "Epoch 251/300\n",
      "74/74 [==============================] - 0s 153us/sample - loss: 0.0254 - accuracy: 0.9189\n",
      "Epoch 252/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0271 - accuracy: 0.9189\n",
      "Epoch 253/300\n",
      "74/74 [==============================] - 0s 169us/sample - loss: 0.0319 - accuracy: 0.9189\n",
      "Epoch 254/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0269 - accuracy: 0.9189\n",
      "Epoch 255/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0291 - accuracy: 0.9189\n",
      "Epoch 256/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0280 - accuracy: 0.9189\n",
      "Epoch 257/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0258 - accuracy: 0.9189\n",
      "Epoch 258/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0281 - accuracy: 0.9189\n",
      "Epoch 259/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0284 - accuracy: 0.9189\n",
      "Epoch 260/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0241 - accuracy: 0.9189\n",
      "Epoch 261/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0276 - accuracy: 0.9189\n",
      "Epoch 262/300\n",
      "74/74 [==============================] - 0s 144us/sample - loss: 0.0297 - accuracy: 0.9189\n",
      "Epoch 263/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0249 - accuracy: 0.9189\n",
      "Epoch 264/300\n",
      "74/74 [==============================] - 0s 143us/sample - loss: 0.0241 - accuracy: 0.9189\n",
      "Epoch 265/300\n",
      "74/74 [==============================] - 0s 148us/sample - loss: 0.0280 - accuracy: 0.9189\n",
      "Epoch 266/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0236 - accuracy: 0.9189\n",
      "Epoch 267/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0266 - accuracy: 0.9189\n",
      "Epoch 268/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0239 - accuracy: 0.9189\n",
      "Epoch 269/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0265 - accuracy: 0.9189\n",
      "Epoch 270/300\n",
      "74/74 [==============================] - 0s 172us/sample - loss: 0.0259 - accuracy: 0.9189\n",
      "Epoch 271/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0273 - accuracy: 0.9189\n",
      "Epoch 272/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0283 - accuracy: 0.9189\n",
      "Epoch 273/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0279 - accuracy: 0.9189\n",
      "Epoch 274/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0272 - accuracy: 0.9189\n",
      "Epoch 275/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0265 - accuracy: 0.9189\n",
      "Epoch 276/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0296 - accuracy: 0.9189\n",
      "Epoch 277/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0284 - accuracy: 0.9189\n",
      "Epoch 278/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0279 - accuracy: 0.9189\n",
      "Epoch 279/300\n",
      "74/74 [==============================] - 0s 176us/sample - loss: 0.0291 - accuracy: 0.9189\n",
      "Epoch 280/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0235 - accuracy: 0.9189\n",
      "Epoch 281/300\n",
      "74/74 [==============================] - 0s 141us/sample - loss: 0.0259 - accuracy: 0.9189\n",
      "Epoch 282/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0280 - accuracy: 0.9189\n",
      "Epoch 283/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0223 - accuracy: 0.9189\n",
      "Epoch 284/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0269 - accuracy: 0.9189\n",
      "Epoch 285/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0260 - accuracy: 0.9189\n",
      "Epoch 286/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0249 - accuracy: 0.9189\n",
      "Epoch 287/300\n",
      "74/74 [==============================] - 0s 143us/sample - loss: 0.0255 - accuracy: 0.9189\n",
      "Epoch 288/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0264 - accuracy: 0.9189\n",
      "Epoch 289/300\n",
      "74/74 [==============================] - 0s 145us/sample - loss: 0.0276 - accuracy: 0.9189\n",
      "Epoch 290/300\n",
      "74/74 [==============================] - 0s 143us/sample - loss: 0.0256 - accuracy: 0.9189\n",
      "Epoch 291/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0277 - accuracy: 0.9189\n",
      "Epoch 292/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0262 - accuracy: 0.9189\n",
      "Epoch 293/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0270 - accuracy: 0.9189\n",
      "Epoch 294/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0287 - accuracy: 0.9189\n",
      "Epoch 295/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0247 - accuracy: 0.9189\n",
      "Epoch 296/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0288 - accuracy: 0.9189\n",
      "Epoch 297/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0277 - accuracy: 0.9189\n",
      "Epoch 298/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0277 - accuracy: 0.9189\n",
      "Epoch 299/300\n",
      "74/74 [==============================] - 0s 160us/sample - loss: 0.0279 - accuracy: 0.9189\n",
      "Epoch 300/300\n",
      "74/74 [==============================] - 0s 155us/sample - loss: 0.0266 - accuracy: 0.9189\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_34 (Dense)             (None, 128)               4608      \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,737\n",
      "Trainable params: 4,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 74 samples\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 1s 14ms/sample - loss: 0.1403 - accuracy: 0.2703\n",
      "Epoch 2/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.1384 - accuracy: 0.3378\n",
      "Epoch 3/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.1339 - accuracy: 0.3378\n",
      "Epoch 4/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.1323 - accuracy: 0.4730\n",
      "Epoch 5/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.1256 - accuracy: 0.5405\n",
      "Epoch 6/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.1223 - accuracy: 0.6216\n",
      "Epoch 7/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.1219 - accuracy: 0.6216\n",
      "Epoch 8/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.1219 - accuracy: 0.6351\n",
      "Epoch 9/300\n",
      "74/74 [==============================] - 0s 137us/sample - loss: 0.1109 - accuracy: 0.7027\n",
      "Epoch 10/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.1088 - accuracy: 0.7297\n",
      "Epoch 11/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.1094 - accuracy: 0.7568\n",
      "Epoch 12/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.1017 - accuracy: 0.7973\n",
      "Epoch 13/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0982 - accuracy: 0.7838\n",
      "Epoch 14/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0971 - accuracy: 0.8108\n",
      "Epoch 15/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0979 - accuracy: 0.7838\n",
      "Epoch 16/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0900 - accuracy: 0.8108\n",
      "Epoch 17/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0862 - accuracy: 0.8243\n",
      "Epoch 18/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0899 - accuracy: 0.8243\n",
      "Epoch 19/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0846 - accuracy: 0.8243\n",
      "Epoch 20/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0826 - accuracy: 0.8514\n",
      "Epoch 21/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0812 - accuracy: 0.8243\n",
      "Epoch 22/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0807 - accuracy: 0.8378\n",
      "Epoch 23/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0770 - accuracy: 0.8649\n",
      "Epoch 24/300\n",
      "74/74 [==============================] - 0s 137us/sample - loss: 0.0759 - accuracy: 0.8514\n",
      "Epoch 25/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0713 - accuracy: 0.8514\n",
      "Epoch 26/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0703 - accuracy: 0.8649\n",
      "Epoch 27/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0647 - accuracy: 0.8649\n",
      "Epoch 28/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0692 - accuracy: 0.8649\n",
      "Epoch 29/300\n",
      "74/74 [==============================] - 0s 156us/sample - loss: 0.0633 - accuracy: 0.8649\n",
      "Epoch 30/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0690 - accuracy: 0.8649\n",
      "Epoch 31/300\n",
      "74/74 [==============================] - 0s 142us/sample - loss: 0.0633 - accuracy: 0.8514\n",
      "Epoch 32/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0608 - accuracy: 0.8649\n",
      "Epoch 33/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0560 - accuracy: 0.8649\n",
      "Epoch 34/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0551 - accuracy: 0.8649\n",
      "Epoch 35/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0583 - accuracy: 0.8649\n",
      "Epoch 36/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0554 - accuracy: 0.8649\n",
      "Epoch 37/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0529 - accuracy: 0.8649\n",
      "Epoch 38/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0581 - accuracy: 0.8649\n",
      "Epoch 39/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0520 - accuracy: 0.8649\n",
      "Epoch 40/300\n",
      "74/74 [==============================] - 0s 163us/sample - loss: 0.0495 - accuracy: 0.8649\n",
      "Epoch 41/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0524 - accuracy: 0.8649\n",
      "Epoch 42/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0535 - accuracy: 0.8649\n",
      "Epoch 43/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0435 - accuracy: 0.8649\n",
      "Epoch 44/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0461 - accuracy: 0.8649\n",
      "Epoch 45/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0482 - accuracy: 0.8649\n",
      "Epoch 46/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0484 - accuracy: 0.8784\n",
      "Epoch 47/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0460 - accuracy: 0.8784\n",
      "Epoch 48/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0440 - accuracy: 0.8649\n",
      "Epoch 49/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0462 - accuracy: 0.8649\n",
      "Epoch 50/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0454 - accuracy: 0.8649\n",
      "Epoch 51/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0456 - accuracy: 0.8649\n",
      "Epoch 52/300\n",
      "74/74 [==============================] - 0s 150us/sample - loss: 0.0406 - accuracy: 0.8649\n",
      "Epoch 53/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0496 - accuracy: 0.8649\n",
      "Epoch 54/300\n",
      "74/74 [==============================] - 0s 145us/sample - loss: 0.0412 - accuracy: 0.8784\n",
      "Epoch 55/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0412 - accuracy: 0.8784\n",
      "Epoch 56/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0440 - accuracy: 0.8649\n",
      "Epoch 57/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0380 - accuracy: 0.8649\n",
      "Epoch 58/300\n",
      "74/74 [==============================] - 0s 150us/sample - loss: 0.0436 - accuracy: 0.8649\n",
      "Epoch 59/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0408 - accuracy: 0.8649\n",
      "Epoch 60/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0362 - accuracy: 0.8649\n",
      "Epoch 61/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0380 - accuracy: 0.8649\n",
      "Epoch 62/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0404 - accuracy: 0.8649\n",
      "Epoch 63/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0364 - accuracy: 0.8649\n",
      "Epoch 64/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0410 - accuracy: 0.8649\n",
      "Epoch 65/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0379 - accuracy: 0.8649\n",
      "Epoch 66/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0372 - accuracy: 0.8784\n",
      "Epoch 67/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0392 - accuracy: 0.8649\n",
      "Epoch 68/300\n",
      "74/74 [==============================] - 0s 157us/sample - loss: 0.0396 - accuracy: 0.8784\n",
      "Epoch 69/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0365 - accuracy: 0.8649\n",
      "Epoch 70/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0392 - accuracy: 0.8649\n",
      "Epoch 71/300\n",
      "74/74 [==============================] - 0s 150us/sample - loss: 0.0375 - accuracy: 0.8784\n",
      "Epoch 72/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0347 - accuracy: 0.8919\n",
      "Epoch 73/300\n",
      "74/74 [==============================] - 0s 137us/sample - loss: 0.0393 - accuracy: 0.8649\n",
      "Epoch 74/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0389 - accuracy: 0.8649\n",
      "Epoch 75/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0382 - accuracy: 0.8649\n",
      "Epoch 76/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0386 - accuracy: 0.8649\n",
      "Epoch 77/300\n",
      "74/74 [==============================] - 0s 183us/sample - loss: 0.0361 - accuracy: 0.8514\n",
      "Epoch 78/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0367 - accuracy: 0.8649\n",
      "Epoch 79/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0367 - accuracy: 0.8649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0389 - accuracy: 0.8649\n",
      "Epoch 81/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0373 - accuracy: 0.8649\n",
      "Epoch 82/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0366 - accuracy: 0.8649\n",
      "Epoch 83/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0340 - accuracy: 0.8649\n",
      "Epoch 84/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0358 - accuracy: 0.8784\n",
      "Epoch 85/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0341 - accuracy: 0.8649\n",
      "Epoch 86/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0371 - accuracy: 0.8784\n",
      "Epoch 87/300\n",
      "74/74 [==============================] - 0s 145us/sample - loss: 0.0335 - accuracy: 0.8784\n",
      "Epoch 88/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0351 - accuracy: 0.8649\n",
      "Epoch 89/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0325 - accuracy: 0.8649\n",
      "Epoch 90/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0334 - accuracy: 0.8649\n",
      "Epoch 91/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0328 - accuracy: 0.8919\n",
      "Epoch 92/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0324 - accuracy: 0.8649\n",
      "Epoch 93/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0356 - accuracy: 0.8649\n",
      "Epoch 94/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0357 - accuracy: 0.8784\n",
      "Epoch 95/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0360 - accuracy: 0.8784\n",
      "Epoch 96/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0323 - accuracy: 0.8919\n",
      "Epoch 97/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0367 - accuracy: 0.8649\n",
      "Epoch 98/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0338 - accuracy: 0.8649\n",
      "Epoch 99/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0381 - accuracy: 0.8649\n",
      "Epoch 100/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0378 - accuracy: 0.8784\n",
      "Epoch 101/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0376 - accuracy: 0.8784\n",
      "Epoch 102/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0312 - accuracy: 0.8649\n",
      "Epoch 103/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0326 - accuracy: 0.8649\n",
      "Epoch 104/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0304 - accuracy: 0.8649\n",
      "Epoch 105/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0336 - accuracy: 0.8649\n",
      "Epoch 106/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0294 - accuracy: 0.8649\n",
      "Epoch 107/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0363 - accuracy: 0.8649\n",
      "Epoch 108/300\n",
      "74/74 [==============================] - 0s 147us/sample - loss: 0.0338 - accuracy: 0.8649\n",
      "Epoch 109/300\n",
      "74/74 [==============================] - 0s 141us/sample - loss: 0.0360 - accuracy: 0.8784\n",
      "Epoch 110/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0331 - accuracy: 0.8649\n",
      "Epoch 111/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0326 - accuracy: 0.8784\n",
      "Epoch 112/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0327 - accuracy: 0.8784\n",
      "Epoch 113/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0383 - accuracy: 0.8514\n",
      "Epoch 114/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0336 - accuracy: 0.8784\n",
      "Epoch 115/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0346 - accuracy: 0.8649\n",
      "Epoch 116/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0351 - accuracy: 0.8649\n",
      "Epoch 117/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0327 - accuracy: 0.8649\n",
      "Epoch 118/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0360 - accuracy: 0.8649\n",
      "Epoch 119/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0317 - accuracy: 0.8784\n",
      "Epoch 120/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0378 - accuracy: 0.8649\n",
      "Epoch 121/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0369 - accuracy: 0.8514\n",
      "Epoch 122/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0332 - accuracy: 0.8649\n",
      "Epoch 123/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0359 - accuracy: 0.8784\n",
      "Epoch 124/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0331 - accuracy: 0.8649\n",
      "Epoch 125/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0346 - accuracy: 0.8649\n",
      "Epoch 126/300\n",
      "74/74 [==============================] - 0s 137us/sample - loss: 0.0325 - accuracy: 0.8649\n",
      "Epoch 127/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0343 - accuracy: 0.8649\n",
      "Epoch 128/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0335 - accuracy: 0.8649\n",
      "Epoch 129/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0331 - accuracy: 0.8649\n",
      "Epoch 130/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0362 - accuracy: 0.8649\n",
      "Epoch 131/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0328 - accuracy: 0.8649\n",
      "Epoch 132/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0349 - accuracy: 0.8649\n",
      "Epoch 133/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0332 - accuracy: 0.8784\n",
      "Epoch 134/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0337 - accuracy: 0.8649\n",
      "Epoch 135/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0315 - accuracy: 0.8784\n",
      "Epoch 136/300\n",
      "74/74 [==============================] - 0s 151us/sample - loss: 0.0346 - accuracy: 0.8649\n",
      "Epoch 137/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0369 - accuracy: 0.8514\n",
      "Epoch 138/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0336 - accuracy: 0.8784\n",
      "Epoch 139/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0312 - accuracy: 0.8649\n",
      "Epoch 140/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0334 - accuracy: 0.8649\n",
      "Epoch 141/300\n",
      "74/74 [==============================] - 0s 148us/sample - loss: 0.0343 - accuracy: 0.8784\n",
      "Epoch 142/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0321 - accuracy: 0.8784\n",
      "Epoch 143/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0314 - accuracy: 0.8784\n",
      "Epoch 144/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0280 - accuracy: 0.8649\n",
      "Epoch 145/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0319 - accuracy: 0.8649\n",
      "Epoch 146/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0324 - accuracy: 0.8649\n",
      "Epoch 147/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0336 - accuracy: 0.8649\n",
      "Epoch 148/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0388 - accuracy: 0.8514\n",
      "Epoch 149/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0303 - accuracy: 0.8784\n",
      "Epoch 150/300\n",
      "74/74 [==============================] - 0s 152us/sample - loss: 0.0358 - accuracy: 0.8649\n",
      "Epoch 151/300\n",
      "74/74 [==============================] - 0s 149us/sample - loss: 0.0327 - accuracy: 0.8649\n",
      "Epoch 152/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0285 - accuracy: 0.8649\n",
      "Epoch 153/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0332 - accuracy: 0.8649\n",
      "Epoch 154/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0285 - accuracy: 0.8649\n",
      "Epoch 155/300\n",
      "74/74 [==============================] - 0s 190us/sample - loss: 0.0282 - accuracy: 0.8649\n",
      "Epoch 156/300\n",
      "74/74 [==============================] - 0s 141us/sample - loss: 0.0299 - accuracy: 0.8649\n",
      "Epoch 157/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0308 - accuracy: 0.8919\n",
      "Epoch 158/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0332 - accuracy: 0.8784\n",
      "Epoch 159/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0383 - accuracy: 0.8514\n",
      "Epoch 160/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0386 - accuracy: 0.8784\n",
      "Epoch 161/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0328 - accuracy: 0.8784\n",
      "Epoch 162/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0295 - accuracy: 0.8649\n",
      "Epoch 163/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0365 - accuracy: 0.8649\n",
      "Epoch 164/300\n",
      "74/74 [==============================] - 0s 148us/sample - loss: 0.0307 - accuracy: 0.8784\n",
      "Epoch 165/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0366 - accuracy: 0.8649\n",
      "Epoch 166/300\n",
      "74/74 [==============================] - 0s 144us/sample - loss: 0.0332 - accuracy: 0.8649\n",
      "Epoch 167/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0304 - accuracy: 0.8649\n",
      "Epoch 168/300\n",
      "74/74 [==============================] - 0s 159us/sample - loss: 0.0301 - accuracy: 0.8649\n",
      "Epoch 169/300\n",
      "74/74 [==============================] - 0s 151us/sample - loss: 0.0287 - accuracy: 0.8784\n",
      "Epoch 170/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0283 - accuracy: 0.8649\n",
      "Epoch 171/300\n",
      "74/74 [==============================] - 0s 143us/sample - loss: 0.0301 - accuracy: 0.8649\n",
      "Epoch 172/300\n",
      "74/74 [==============================] - 0s 144us/sample - loss: 0.0301 - accuracy: 0.8649\n",
      "Epoch 173/300\n",
      "74/74 [==============================] - 0s 167us/sample - loss: 0.0328 - accuracy: 0.8649\n",
      "Epoch 174/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0289 - accuracy: 0.8784\n",
      "Epoch 175/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0331 - accuracy: 0.8649\n",
      "Epoch 176/300\n",
      "74/74 [==============================] - 0s 150us/sample - loss: 0.0338 - accuracy: 0.8649\n",
      "Epoch 177/300\n",
      "74/74 [==============================] - 0s 137us/sample - loss: 0.0295 - accuracy: 0.8784\n",
      "Epoch 178/300\n",
      "74/74 [==============================] - 0s 140us/sample - loss: 0.0297 - accuracy: 0.8649\n",
      "Epoch 179/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0276 - accuracy: 0.8649\n",
      "Epoch 180/300\n",
      "74/74 [==============================] - 0s 149us/sample - loss: 0.0310 - accuracy: 0.8784\n",
      "Epoch 181/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0317 - accuracy: 0.8649\n",
      "Epoch 182/300\n",
      "74/74 [==============================] - 0s 148us/sample - loss: 0.0301 - accuracy: 0.8784\n",
      "Epoch 183/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0318 - accuracy: 0.8784\n",
      "Epoch 184/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0325 - accuracy: 0.8784\n",
      "Epoch 185/300\n",
      "74/74 [==============================] - 0s 142us/sample - loss: 0.0343 - accuracy: 0.8514\n",
      "Epoch 186/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0313 - accuracy: 0.8784\n",
      "Epoch 187/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0331 - accuracy: 0.8784\n",
      "Epoch 188/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0299 - accuracy: 0.8919\n",
      "Epoch 189/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0303 - accuracy: 0.8649\n",
      "Epoch 190/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0310 - accuracy: 0.8784\n",
      "Epoch 191/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0374 - accuracy: 0.8514\n",
      "Epoch 192/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0324 - accuracy: 0.8649\n",
      "Epoch 193/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0309 - accuracy: 0.8649\n",
      "Epoch 194/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0306 - accuracy: 0.8649\n",
      "Epoch 195/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0363 - accuracy: 0.8649\n",
      "Epoch 196/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0300 - accuracy: 0.8649\n",
      "Epoch 197/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0365 - accuracy: 0.8784\n",
      "Epoch 198/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0316 - accuracy: 0.8784\n",
      "Epoch 199/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0279 - accuracy: 0.8649\n",
      "Epoch 200/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0300 - accuracy: 0.8649\n",
      "Epoch 201/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0344 - accuracy: 0.8649\n",
      "Epoch 202/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0326 - accuracy: 0.8919\n",
      "Epoch 203/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0291 - accuracy: 0.8784\n",
      "Epoch 204/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0334 - accuracy: 0.8649\n",
      "Epoch 205/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0297 - accuracy: 0.8649\n",
      "Epoch 206/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0305 - accuracy: 0.8649\n",
      "Epoch 207/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0341 - accuracy: 0.8649\n",
      "Epoch 208/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0288 - accuracy: 0.8784\n",
      "Epoch 209/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0315 - accuracy: 0.8919\n",
      "Epoch 210/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0330 - accuracy: 0.8784\n",
      "Epoch 211/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0371 - accuracy: 0.8514\n",
      "Epoch 212/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0278 - accuracy: 0.8784\n",
      "Epoch 213/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0352 - accuracy: 0.8649\n",
      "Epoch 214/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0304 - accuracy: 0.8784\n",
      "Epoch 215/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0292 - accuracy: 0.8649\n",
      "Epoch 216/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0294 - accuracy: 0.8919\n",
      "Epoch 217/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0368 - accuracy: 0.8649\n",
      "Epoch 218/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0318 - accuracy: 0.8649\n",
      "Epoch 219/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0290 - accuracy: 0.9054\n",
      "Epoch 220/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0359 - accuracy: 0.8649\n",
      "Epoch 221/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0327 - accuracy: 0.8649\n",
      "Epoch 222/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0332 - accuracy: 0.8784\n",
      "Epoch 223/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0303 - accuracy: 0.8784\n",
      "Epoch 224/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0273 - accuracy: 0.8649\n",
      "Epoch 225/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0296 - accuracy: 0.8649\n",
      "Epoch 226/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0292 - accuracy: 0.8649\n",
      "Epoch 227/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0289 - accuracy: 0.8784\n",
      "Epoch 228/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0295 - accuracy: 0.8919\n",
      "Epoch 229/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0330 - accuracy: 0.8649\n",
      "Epoch 230/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0323 - accuracy: 0.8784\n",
      "Epoch 231/300\n",
      "74/74 [==============================] - 0s 146us/sample - loss: 0.0334 - accuracy: 0.8649\n",
      "Epoch 232/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0314 - accuracy: 0.8649\n",
      "Epoch 233/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0324 - accuracy: 0.8649\n",
      "Epoch 234/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0294 - accuracy: 0.8649\n",
      "Epoch 235/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0294 - accuracy: 0.8919\n",
      "Epoch 236/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0347 - accuracy: 0.8514\n",
      "Epoch 237/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0325 - accuracy: 0.8649\n",
      "Epoch 238/300\n",
      "74/74 [==============================] - 0s 140us/sample - loss: 0.0348 - accuracy: 0.8649\n",
      "Epoch 239/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0321 - accuracy: 0.8649\n",
      "Epoch 240/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0310 - accuracy: 0.8784\n",
      "Epoch 241/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0336 - accuracy: 0.8649\n",
      "Epoch 242/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0290 - accuracy: 0.8649\n",
      "Epoch 243/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0296 - accuracy: 0.8649\n",
      "Epoch 244/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0322 - accuracy: 0.8514\n",
      "Epoch 245/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0353 - accuracy: 0.8649\n",
      "Epoch 246/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0299 - accuracy: 0.8784\n",
      "Epoch 247/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0352 - accuracy: 0.8649\n",
      "Epoch 248/300\n",
      "74/74 [==============================] - 0s 152us/sample - loss: 0.0355 - accuracy: 0.8514\n",
      "Epoch 249/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0319 - accuracy: 0.8649\n",
      "Epoch 250/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0326 - accuracy: 0.8784\n",
      "Epoch 251/300\n",
      "74/74 [==============================] - 0s 149us/sample - loss: 0.0324 - accuracy: 0.8784\n",
      "Epoch 252/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0341 - accuracy: 0.8649\n",
      "Epoch 253/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0329 - accuracy: 0.8649\n",
      "Epoch 254/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0278 - accuracy: 0.8649\n",
      "Epoch 255/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0312 - accuracy: 0.8649\n",
      "Epoch 256/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0310 - accuracy: 0.8649\n",
      "Epoch 257/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0344 - accuracy: 0.8649\n",
      "Epoch 258/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0329 - accuracy: 0.8649\n",
      "Epoch 259/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0328 - accuracy: 0.8649\n",
      "Epoch 260/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0300 - accuracy: 0.8784\n",
      "Epoch 261/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0335 - accuracy: 0.8649\n",
      "Epoch 262/300\n",
      "74/74 [==============================] - 0s 145us/sample - loss: 0.0283 - accuracy: 0.8649\n",
      "Epoch 263/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0316 - accuracy: 0.8649\n",
      "Epoch 264/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0280 - accuracy: 0.8649\n",
      "Epoch 265/300\n",
      "74/74 [==============================] - 0s 149us/sample - loss: 0.0290 - accuracy: 0.8649\n",
      "Epoch 266/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0337 - accuracy: 0.8784\n",
      "Epoch 267/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0318 - accuracy: 0.8649\n",
      "Epoch 268/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0302 - accuracy: 0.8649\n",
      "Epoch 269/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0316 - accuracy: 0.8649\n",
      "Epoch 270/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0395 - accuracy: 0.8649\n",
      "Epoch 271/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0371 - accuracy: 0.8514\n",
      "Epoch 272/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0310 - accuracy: 0.8649\n",
      "Epoch 273/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0342 - accuracy: 0.8784\n",
      "Epoch 274/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0311 - accuracy: 0.8784\n",
      "Epoch 275/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0306 - accuracy: 0.8649\n",
      "Epoch 276/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0305 - accuracy: 0.8649\n",
      "Epoch 277/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0309 - accuracy: 0.8784\n",
      "Epoch 278/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0308 - accuracy: 0.8784\n",
      "Epoch 279/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0335 - accuracy: 0.8784\n",
      "Epoch 280/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0305 - accuracy: 0.8784\n",
      "Epoch 281/300\n",
      "74/74 [==============================] - 0s 148us/sample - loss: 0.0316 - accuracy: 0.8649\n",
      "Epoch 282/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0332 - accuracy: 0.8649\n",
      "Epoch 283/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0318 - accuracy: 0.8784\n",
      "Epoch 284/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0322 - accuracy: 0.8649\n",
      "Epoch 285/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0332 - accuracy: 0.8649\n",
      "Epoch 286/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0328 - accuracy: 0.8649\n",
      "Epoch 287/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0320 - accuracy: 0.8649\n",
      "Epoch 288/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0328 - accuracy: 0.8649\n",
      "Epoch 289/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0347 - accuracy: 0.8649\n",
      "Epoch 290/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0304 - accuracy: 0.8784\n",
      "Epoch 291/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0297 - accuracy: 0.8649\n",
      "Epoch 292/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0350 - accuracy: 0.8649\n",
      "Epoch 293/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0288 - accuracy: 0.8649\n",
      "Epoch 294/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0301 - accuracy: 0.8649\n",
      "Epoch 295/300\n",
      "74/74 [==============================] - 0s 143us/sample - loss: 0.0365 - accuracy: 0.8649\n",
      "Epoch 296/300\n",
      "74/74 [==============================] - 0s 108us/sample - loss: 0.0327 - accuracy: 0.8649\n",
      "Epoch 297/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0336 - accuracy: 0.8649\n",
      "Epoch 298/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0317 - accuracy: 0.8649\n",
      "Epoch 299/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0299 - accuracy: 0.8784\n",
      "Epoch 300/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0296 - accuracy: 0.8649\n"
     ]
    }
   ],
   "source": [
    "kfold = 3\n",
    "random_state = 11\n",
    "\n",
    "X = X\n",
    "y = encoded_y\n",
    "\n",
    "test_F1 = np.zeros(kfold)\n",
    "time_k = np.zeros(kfold)\n",
    "skf = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=random_state)\n",
    "k = 0\n",
    "epochs = 300\n",
    "batch_size = 15\n",
    "\n",
    "# class_weight = {0 : 1., 1: 1.,}  # The weights can be changed and made inversely proportional to the class size to improve the accuracy.\n",
    "class_weight = {0 : 0.12, 1: 0.88,}\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(X_train.shape[1],))) \n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train ,batch_size=batch_size, epochs=epochs, verbose=1, class_weight=class_weight)\n",
    "    end_time = time.time()\n",
    "    time_k[k] = end_time-start_time\n",
    "\n",
    "    y_pred = model.predict_proba(X_test).round().astype(int)\n",
    "    y_train_pred = model.predict_proba(X_train).round().astype(int)\n",
    "    test_F1[k] = sklearn.metrics.f1_score(y_test, y_pred)\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0A2o8M47yxhD",
    "outputId": "cc7d5c34-467c-4581-f19d-da623009e7d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average f1 score 0.6341880341880342\n",
      "Average Run time 3.880180994669596\n"
     ]
    }
   ],
   "source": [
    "print ('Average f1 score', np.mean(test_F1))\n",
    "print ('Average Run time', np.mean(time_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A9Ak6HiSyxhE"
   },
   "source": [
    "#### Building an LSTM Classifier on the sequences for comparison\n",
    "We built an LSTM Classifier on the sequences to compare the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VtHG2V_syxhE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1~2~3~3~3~3~3~3~1~4~5~1~2~3~3~3~3~3~3~1~4~5~1~...\n",
       "1      6~5~5~6~5~6~5~2~5~5~5~5~5~5~5~5~5~5~5~5~5~5~5~...\n",
       "2      19~19~19~19~19~19~19~19~19~19~19~19~19~19~19~1...\n",
       "3      6~5~5~6~5~6~5~2~5~5~5~5~5~5~5~5~5~5~5~5~5~5~5~...\n",
       "4      5~5~17~5~5~5~5~5~10~2~11~2~11~11~12~11~11~5~2~...\n",
       "                             ...                        \n",
       "106    10~2~11~2~11~11~12~11~11~5~2~11~5~2~5~2~3~14~3...\n",
       "107    5~5~2~5~17~6~5~6~5~5~2~6~17~3~2~2~3~5~2~3~5~6~...\n",
       "108    6~5~6~5~5~6~5~5~6~6~6~6~6~6~6~6~6~6~6~6~6~6~6~...\n",
       "109    6~5~5~6~5~6~5~2~38~2~3~5~22~39~5~5~5~5~5~5~5~5...\n",
       "110    5~6~5~5~10~2~11~2~11~11~12~11~5~2~11~11~12~11~...\n",
       "Name: seq, Length: 111, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data['seq']\n",
    "encoded_X = np.ndarray(shape=(len(X),), dtype=list)\n",
    "for i in range(0,len(X)):\n",
    "    encoded_X[i]=X.iloc[i].split(\"~\")\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7BIzZAtHyxhG"
   },
   "outputs": [],
   "source": [
    "max_seq_length = np.max(data['seqlen'])\n",
    "encoded_X = tf.keras.preprocessing.sequence.pad_sequences(encoded_X, maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yL6q9OlmyxhH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1773, 32)          1600      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 9,953\n",
      "Trainable params: 9,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 74 samples\n",
      "Epoch 1/50\n",
      "74/74 [==============================] - 5s 72ms/sample - loss: 0.6894 - accuracy: 0.5676\n",
      "Epoch 2/50\n",
      "74/74 [==============================] - 4s 48ms/sample - loss: 0.6590 - accuracy: 0.8784\n",
      "Epoch 3/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.6212 - accuracy: 0.8784\n",
      "Epoch 4/50\n",
      "74/74 [==============================] - 3s 47ms/sample - loss: 0.5569 - accuracy: 0.8784\n",
      "Epoch 5/50\n",
      "74/74 [==============================] - 3s 47ms/sample - loss: 0.4582 - accuracy: 0.8784\n",
      "Epoch 6/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.3787 - accuracy: 0.8784\n",
      "Epoch 7/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.3741 - accuracy: 0.8784\n",
      "Epoch 8/50\n",
      "74/74 [==============================] - 4s 54ms/sample - loss: 0.3789 - accuracy: 0.8784\n",
      "Epoch 9/50\n",
      "74/74 [==============================] - 4s 56ms/sample - loss: 0.3716 - accuracy: 0.8784\n",
      "Epoch 10/50\n",
      "74/74 [==============================] - 4s 51ms/sample - loss: 0.3728 - accuracy: 0.8784\n",
      "Epoch 11/50\n",
      "74/74 [==============================] - 4s 51ms/sample - loss: 0.3698 - accuracy: 0.8784\n",
      "Epoch 12/50\n",
      "74/74 [==============================] - 4s 61ms/sample - loss: 0.3693 - accuracy: 0.8784\n",
      "Epoch 13/50\n",
      "74/74 [==============================] - 4s 54ms/sample - loss: 0.3683 - accuracy: 0.8784\n",
      "Epoch 14/50\n",
      "74/74 [==============================] - 4s 54ms/sample - loss: 0.3686 - accuracy: 0.8784\n",
      "Epoch 15/50\n",
      "74/74 [==============================] - 4s 51ms/sample - loss: 0.3681 - accuracy: 0.8784\n",
      "Epoch 16/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.3678 - accuracy: 0.8784\n",
      "Epoch 17/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.3666 - accuracy: 0.8784\n",
      "Epoch 18/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.3643 - accuracy: 0.8784\n",
      "Epoch 19/50\n",
      "74/74 [==============================] - 4s 52ms/sample - loss: 0.3656 - accuracy: 0.8784\n",
      "Epoch 20/50\n",
      "74/74 [==============================] - 4s 51ms/sample - loss: 0.3625 - accuracy: 0.8784\n",
      "Epoch 21/50\n",
      "74/74 [==============================] - 4s 51ms/sample - loss: 0.3636 - accuracy: 0.8784\n",
      "Epoch 22/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.3569 - accuracy: 0.8784\n",
      "Epoch 23/50\n",
      "74/74 [==============================] - 4s 52ms/sample - loss: 0.3497 - accuracy: 0.8784\n",
      "Epoch 24/50\n",
      "74/74 [==============================] - 4s 54ms/sample - loss: 0.3429 - accuracy: 0.8784\n",
      "Epoch 25/50\n",
      "74/74 [==============================] - 4s 54ms/sample - loss: 0.3329 - accuracy: 0.8784\n",
      "Epoch 26/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.3122 - accuracy: 0.8784\n",
      "Epoch 27/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.2849 - accuracy: 0.8784\n",
      "Epoch 28/50\n",
      "74/74 [==============================] - 4s 51ms/sample - loss: 0.2611 - accuracy: 0.8784\n",
      "Epoch 29/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.2358 - accuracy: 0.8784\n",
      "Epoch 30/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.2257 - accuracy: 0.8784\n",
      "Epoch 31/50\n",
      "74/74 [==============================] - 4s 55ms/sample - loss: 0.2220 - accuracy: 0.8919\n",
      "Epoch 32/50\n",
      "74/74 [==============================] - 4s 53ms/sample - loss: 0.2553 - accuracy: 0.8919\n",
      "Epoch 33/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.1990 - accuracy: 0.8919\n",
      "Epoch 34/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.1951 - accuracy: 0.9054\n",
      "Epoch 35/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.1800 - accuracy: 0.9189\n",
      "Epoch 36/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.2065 - accuracy: 0.9054\n",
      "Epoch 37/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.1721 - accuracy: 0.9324\n",
      "Epoch 38/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.1895 - accuracy: 0.9324\n",
      "Epoch 39/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.1795 - accuracy: 0.9324\n",
      "Epoch 40/50\n",
      "74/74 [==============================] - 4s 48ms/sample - loss: 0.1627 - accuracy: 0.9324\n",
      "Epoch 41/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.1614 - accuracy: 0.9459\n",
      "Epoch 42/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.1568 - accuracy: 0.9595\n",
      "Epoch 43/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.1550 - accuracy: 0.9595\n",
      "Epoch 44/50\n",
      "74/74 [==============================] - 4s 53ms/sample - loss: 0.1494 - accuracy: 0.9595\n",
      "Epoch 45/50\n",
      "74/74 [==============================] - 4s 53ms/sample - loss: 0.1506 - accuracy: 0.9595\n",
      "Epoch 46/50\n",
      "74/74 [==============================] - 4s 52ms/sample - loss: 0.1518 - accuracy: 0.9595\n",
      "Epoch 47/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.1420 - accuracy: 0.9595\n",
      "Epoch 48/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.1514 - accuracy: 0.9595\n",
      "Epoch 49/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.1399 - accuracy: 0.9595\n",
      "Epoch 50/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.1333 - accuracy: 0.9595\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1773, 32)          1600      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 9,953\n",
      "Trainable params: 9,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 74 samples\n",
      "Epoch 1/50\n",
      "74/74 [==============================] - 5s 70ms/sample - loss: 0.6908 - accuracy: 0.5811\n",
      "Epoch 2/50\n",
      "74/74 [==============================] - 4s 48ms/sample - loss: 0.6588 - accuracy: 0.8784\n",
      "Epoch 3/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.6177 - accuracy: 0.8784\n",
      "Epoch 4/50\n",
      "74/74 [==============================] - 4s 52ms/sample - loss: 0.5426 - accuracy: 0.8784\n",
      "Epoch 5/50\n",
      "74/74 [==============================] - 4s 52ms/sample - loss: 0.4332 - accuracy: 0.8784\n",
      "Epoch 6/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.3692 - accuracy: 0.8784\n",
      "Epoch 7/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.3841 - accuracy: 0.8784\n",
      "Epoch 8/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.3805 - accuracy: 0.8784\n",
      "Epoch 9/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.3757 - accuracy: 0.8784\n",
      "Epoch 10/50\n",
      "74/74 [==============================] - 4s 51ms/sample - loss: 0.3703 - accuracy: 0.8784\n",
      "Epoch 11/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.3704 - accuracy: 0.8784\n",
      "Epoch 12/50\n",
      "74/74 [==============================] - 4s 51ms/sample - loss: 0.3706 - accuracy: 0.8784\n",
      "Epoch 13/50\n",
      "74/74 [==============================] - 4s 58ms/sample - loss: 0.3710 - accuracy: 0.8784\n",
      "Epoch 14/50\n",
      "74/74 [==============================] - 4s 53ms/sample - loss: 0.3696 - accuracy: 0.8784\n",
      "Epoch 15/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.3695 - accuracy: 0.8784\n",
      "Epoch 16/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.3690 - accuracy: 0.8784\n",
      "Epoch 17/50\n",
      "74/74 [==============================] - 4s 51ms/sample - loss: 0.3693 - accuracy: 0.8784\n",
      "Epoch 18/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.3696 - accuracy: 0.8784\n",
      "Epoch 19/50\n",
      "74/74 [==============================] - 4s 51ms/sample - loss: 0.3685 - accuracy: 0.8784\n",
      "Epoch 20/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.3681 - accuracy: 0.8784\n",
      "Epoch 21/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.3683 - accuracy: 0.8784\n",
      "Epoch 22/50\n",
      "74/74 [==============================] - 4s 51ms/sample - loss: 0.3666 - accuracy: 0.8784\n",
      "Epoch 23/50\n",
      "74/74 [==============================] - 4s 51ms/sample - loss: 0.3662 - accuracy: 0.8784\n",
      "Epoch 24/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.3652 - accuracy: 0.8784\n",
      "Epoch 25/50\n",
      "74/74 [==============================] - 4s 51ms/sample - loss: 0.3643 - accuracy: 0.8784\n",
      "Epoch 26/50\n",
      "74/74 [==============================] - 4s 53ms/sample - loss: 0.3625 - accuracy: 0.8784\n",
      "Epoch 27/50\n",
      "74/74 [==============================] - 4s 52ms/sample - loss: 0.3606 - accuracy: 0.8784\n",
      "Epoch 28/50\n",
      "74/74 [==============================] - 4s 51ms/sample - loss: 0.3572 - accuracy: 0.8784\n",
      "Epoch 29/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.3586 - accuracy: 0.8784\n",
      "Epoch 30/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.3476 - accuracy: 0.8784\n",
      "Epoch 31/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.3418 - accuracy: 0.8784\n",
      "Epoch 32/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.3332 - accuracy: 0.8784\n",
      "Epoch 33/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.3170 - accuracy: 0.8784\n",
      "Epoch 34/50\n",
      "74/74 [==============================] - 4s 51ms/sample - loss: 0.2989 - accuracy: 0.8784\n",
      "Epoch 35/50\n",
      "74/74 [==============================] - 4s 52ms/sample - loss: 0.2768 - accuracy: 0.8784\n",
      "Epoch 36/50\n",
      "74/74 [==============================] - 4s 53ms/sample - loss: 0.2722 - accuracy: 0.8784\n",
      "Epoch 37/50\n",
      "74/74 [==============================] - 4s 52ms/sample - loss: 0.2409 - accuracy: 0.9054\n",
      "Epoch 38/50\n",
      "74/74 [==============================] - 4s 52ms/sample - loss: 0.2363 - accuracy: 0.8919\n",
      "Epoch 39/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.2539 - accuracy: 0.8919\n",
      "Epoch 40/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.2227 - accuracy: 0.9189\n",
      "Epoch 41/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.2165 - accuracy: 0.9189\n",
      "Epoch 42/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.2155 - accuracy: 0.9189\n",
      "Epoch 43/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.2259 - accuracy: 0.9324\n",
      "Epoch 44/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.2115 - accuracy: 0.9324\n",
      "Epoch 45/50\n",
      "74/74 [==============================] - 4s 51ms/sample - loss: 0.2086 - accuracy: 0.9324\n",
      "Epoch 46/50\n",
      "74/74 [==============================] - 4s 52ms/sample - loss: 0.2296 - accuracy: 0.9189\n",
      "Epoch 47/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.2217 - accuracy: 0.9189\n",
      "Epoch 48/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.2061 - accuracy: 0.9189\n",
      "Epoch 49/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.1819 - accuracy: 0.9324\n",
      "Epoch 50/50\n",
      "74/74 [==============================] - 4s 51ms/sample - loss: 0.2094 - accuracy: 0.8919\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1773, 32)          1600      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 9,953\n",
      "Trainable params: 9,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 74 samples\n",
      "Epoch 1/50\n",
      "74/74 [==============================] - 5s 70ms/sample - loss: 0.6802 - accuracy: 0.7297\n",
      "Epoch 2/50\n",
      "74/74 [==============================] - 4s 48ms/sample - loss: 0.6386 - accuracy: 0.8919\n",
      "Epoch 3/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.5920 - accuracy: 0.8919\n",
      "Epoch 4/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.5131 - accuracy: 0.8919\n",
      "Epoch 5/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.4075 - accuracy: 0.8919\n",
      "Epoch 6/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.3678 - accuracy: 0.8919\n",
      "Epoch 7/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.3742 - accuracy: 0.8919\n",
      "Epoch 8/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.3646 - accuracy: 0.8919\n",
      "Epoch 9/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.3464 - accuracy: 0.8919\n",
      "Epoch 10/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.3420 - accuracy: 0.8919\n",
      "Epoch 11/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.3457 - accuracy: 0.8919\n",
      "Epoch 12/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.3444 - accuracy: 0.8919\n",
      "Epoch 13/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.3428 - accuracy: 0.8919\n",
      "Epoch 14/50\n",
      "74/74 [==============================] - 4s 50ms/sample - loss: 0.3428 - accuracy: 0.8919\n",
      "Epoch 15/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.3462 - accuracy: 0.8919\n",
      "Epoch 16/50\n",
      "74/74 [==============================] - 4s 51ms/sample - loss: 0.3406 - accuracy: 0.8919\n",
      "Epoch 17/50\n",
      "74/74 [==============================] - 5s 62ms/sample - loss: 0.3405 - accuracy: 0.8919\n",
      "Epoch 18/50\n",
      "74/74 [==============================] - 4s 53ms/sample - loss: 0.3396 - accuracy: 0.8919\n",
      "Epoch 19/50\n",
      "74/74 [==============================] - 4s 57ms/sample - loss: 0.3399 - accuracy: 0.8919\n",
      "Epoch 20/50\n",
      "74/74 [==============================] - 4s 57ms/sample - loss: 0.3371 - accuracy: 0.8919\n",
      "Epoch 21/50\n",
      "74/74 [==============================] - 4s 51ms/sample - loss: 0.3355 - accuracy: 0.8919\n",
      "Epoch 22/50\n",
      "74/74 [==============================] - 4s 59ms/sample - loss: 0.3356 - accuracy: 0.8919\n",
      "Epoch 23/50\n",
      "74/74 [==============================] - 4s 58ms/sample - loss: 0.3301 - accuracy: 0.8919\n",
      "Epoch 24/50\n",
      "74/74 [==============================] - 4s 54ms/sample - loss: 0.3278 - accuracy: 0.8919\n",
      "Epoch 25/50\n",
      "74/74 [==============================] - 4s 52ms/sample - loss: 0.3221 - accuracy: 0.8919\n",
      "Epoch 26/50\n",
      "74/74 [==============================] - 4s 53ms/sample - loss: 0.3096 - accuracy: 0.8919\n",
      "Epoch 27/50\n",
      "74/74 [==============================] - 4s 56ms/sample - loss: 0.3019 - accuracy: 0.8919\n",
      "Epoch 28/50\n",
      "74/74 [==============================] - 4s 56ms/sample - loss: 0.2991 - accuracy: 0.8919\n",
      "Epoch 29/50\n",
      "74/74 [==============================] - 4s 59ms/sample - loss: 0.2763 - accuracy: 0.8919\n",
      "Epoch 30/50\n",
      "74/74 [==============================] - 4s 57ms/sample - loss: 0.2710 - accuracy: 0.8919\n",
      "Epoch 31/50\n",
      "74/74 [==============================] - 4s 52ms/sample - loss: 0.2501 - accuracy: 0.8919\n",
      "Epoch 32/50\n",
      "74/74 [==============================] - 4s 54ms/sample - loss: 0.2418 - accuracy: 0.9054\n",
      "Epoch 33/50\n",
      "74/74 [==============================] - 4s 57ms/sample - loss: 0.2353 - accuracy: 0.9054\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 4s 56ms/sample - loss: 0.2374 - accuracy: 0.9054\n",
      "Epoch 35/50\n",
      "74/74 [==============================] - 4s 55ms/sample - loss: 0.2372 - accuracy: 0.9189\n",
      "Epoch 36/50\n",
      "74/74 [==============================] - 4s 56ms/sample - loss: 0.2120 - accuracy: 0.9054\n",
      "Epoch 37/50\n",
      "74/74 [==============================] - 4s 60ms/sample - loss: 0.2243 - accuracy: 0.8919\n",
      "Epoch 38/50\n",
      "74/74 [==============================] - 4s 56ms/sample - loss: 0.2125 - accuracy: 0.8919\n",
      "Epoch 39/50\n",
      "74/74 [==============================] - 4s 55ms/sample - loss: 0.2108 - accuracy: 0.9054\n",
      "Epoch 40/50\n",
      "74/74 [==============================] - 4s 56ms/sample - loss: 0.2002 - accuracy: 0.9189\n",
      "Epoch 41/50\n",
      "74/74 [==============================] - 4s 56ms/sample - loss: 0.2103 - accuracy: 0.9459\n",
      "Epoch 42/50\n",
      "74/74 [==============================] - 4s 52ms/sample - loss: 0.1961 - accuracy: 0.9459\n",
      "Epoch 43/50\n",
      "74/74 [==============================] - 4s 57ms/sample - loss: 0.1837 - accuracy: 0.9324\n",
      "Epoch 44/50\n",
      "74/74 [==============================] - 4s 52ms/sample - loss: 0.1771 - accuracy: 0.9324\n",
      "Epoch 45/50\n",
      "74/74 [==============================] - 4s 51ms/sample - loss: 0.1774 - accuracy: 0.9324\n",
      "Epoch 46/50\n",
      "74/74 [==============================] - 4s 52ms/sample - loss: 0.1693 - accuracy: 0.9324\n",
      "Epoch 47/50\n",
      "74/74 [==============================] - 4s 53ms/sample - loss: 0.1640 - accuracy: 0.9189\n",
      "Epoch 48/50\n",
      "74/74 [==============================] - 4s 57ms/sample - loss: 0.1606 - accuracy: 0.9459\n",
      "Epoch 49/50\n",
      "74/74 [==============================] - 4s 51ms/sample - loss: 0.1638 - accuracy: 0.9459\n",
      "Epoch 50/50\n",
      "74/74 [==============================] - 4s 51ms/sample - loss: 0.1596 - accuracy: 0.9324\n"
     ]
    }
   ],
   "source": [
    "kfold = 3\n",
    "random_state = 11\n",
    "\n",
    "test_F1 = np.zeros(kfold)\n",
    "time_k = np.zeros(kfold)\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 15\n",
    "skf = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=random_state)\n",
    "k = 0\n",
    "\n",
    "for train_index, test_index in skf.split(encoded_X, y):\n",
    "    X_train, X_test = encoded_X[train_index], encoded_X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    embedding_vecor_length = 32\n",
    "    top_words=50\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embedding_vecor_length, input_length=max_seq_length))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    end_time=time.time()\n",
    "    time_k[k]=end_time-start_time\n",
    "\n",
    "    y_pred = model.predict_proba(X_test).round().astype(int)\n",
    "    y_train_pred=model.predict_proba(X_train).round().astype(int)\n",
    "    test_F1[k]=sklearn.metrics.f1_score(y_test, y_pred)\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YMr6LQfKyxhI",
    "outputId": "868779ed-15ab-4d68-bb15-413278056e6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average f1 score 0.36111111111111116\n",
      "Average Run time 192.46954011917114\n"
     ]
    }
   ],
   "source": [
    "print ('Average f1 score', np.mean(test_F1))\n",
    "print ('Average Run time', np.mean(time_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xf1aQbDVyxhJ"
   },
   "source": [
    "We find that the LSTM classifier gives a significantly lower F1 score. This may be improved by changing the model. However, we find that the SGT embedding could work with a small and unbalanced data without the need of a complicated classifier model.\n",
    "\n",
    "LSTM models typically require more data for training and also has significantly more computation time. The LSTM model above took 425.6 secs while the MLP model took just 9.1 secs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8MovyTNkyxhK"
   },
   "source": [
    "## Sequence Search\n",
    "\n",
    "Sequence data sets are generally large. For example, sequences of listening history in music streaming services, such as Pandora, for more than 70M users are huge. In protein data bases there could be even larger size. For instance, the Uniprot data repository has more than 177M sequences.\n",
    "\n",
    "Searching for similar sequences in such large data bases is challenging. SGT embedding provides a simple solution. In the following it will be shown on a protein data set that SGT embedding can be used to compute similarity between a query sequence and the sequence corpus using a dot product. The sequences with the highest dot product are returned as the most similar sequence to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Protein Sequence Search\n",
    "\n",
    "In the following, a sample of 10k protein sequences are used for illustration. The data is taken from https://www.uniprot.org ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I2WKR6</td>\n",
       "      <td>[M, V, H, K, S, D, S, D, E, L, A, A, L, R, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A2A6M8K9</td>\n",
       "      <td>[M, Q, E, S, L, V, V, R, R, E, T, H, I, A, A, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A3G5KEC3</td>\n",
       "      <td>[M, A, S, G, A, Y, S, K, Y, L, F, Q, I, I, G, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                           sequence\n",
       "0      I2WKR6  [M, V, H, K, S, D, S, D, E, L, A, A, L, R, A, ...\n",
       "1  A0A2A6M8K9  [M, Q, E, S, L, V, V, R, R, E, T, H, I, A, A, ...\n",
       "2  A0A3G5KEC3  [M, A, S, G, A, Y, S, K, Y, L, F, Q, I, I, G, ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data\n",
    "data = pd.read_csv('data/protein-uniprot-reviewed-Ano-10k.tab', sep='\\t')\n",
    "\n",
    "# Data preprocessing\n",
    "corpus = data.loc[:,['Entry','Sequence']]\n",
    "corpus.columns = ['id', 'sequence']\n",
    "corpus['sequence'] = corpus['sequence'].map(list)\n",
    "corpus.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 7 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "# Protein sequence alphabets\n",
    "alphabets = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', \n",
    "             'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', \n",
    "             'W', 'X', 'Y', 'U', 'O']  # List of amino acids\n",
    "\n",
    "# Alphabets are known and inputted \n",
    "# as arguments for faster computation\n",
    "sgt_ = SGT(alphabets=alphabets, \n",
    "           lengthsensitive=True, \n",
    "           kappa=1, \n",
    "           flatten=True, \n",
    "           mode='multiprocessing')\n",
    "\n",
    "sgtembedding_df = sgt_.fit_transform(corpus)\n",
    "sgtembedding_df = sgtembedding_df.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "K0ZGN5        2773.749663\n",
       "A0A0Y1CPH7    1617.451379\n",
       "A0A5R8LCJ1    1566.833152\n",
       "A0A290WY40    1448.772820\n",
       "A0A073K6N6    1392.267250\n",
       "                 ...     \n",
       "A0A1S7UBK4     160.074989\n",
       "A0A2S7T1R9     156.580584\n",
       "A0A0E0UQV6     155.834932\n",
       "A0A1Y5Y0S0     148.862049\n",
       "B0NRP3         117.656497\n",
       "Length: 10063, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Search proteins similar to a query protein.\n",
    "The approach is to find the SGT embedding of the\n",
    "query protein and find its similarity with the\n",
    "embeddings of the protein database.\n",
    "'''\n",
    "\n",
    "query_protein = 'MSHVFPIVIDDNFLSPQDLVSAARSGCSLRLHTGVVDKIDRAHRFVLEIAGAEALHYGINTGFGSLCTTHIDPADLSTLQHNLLKSHACGVGPTVSEEVSRVVTLIKLLTFRTGNSGVSLSTVNRIIDLWNHGVVGAIAQKGTVGASGDLAPLAHLFLPLIGLGQVWHRGVLRPSREVMDELKLAPLTLQPKDGLCLTNGVQYLNAWGALSTVRAKRLVALADLCAAMSMMGFSAARSFIEAQIHQTSLHPERGHVALHLRTLTHGSNHADLPHCNPAMEDPYSFRCAPQVHGAARQVVGYLETVIGNECNSVSDNPLVFPDTRQILTCGNLHGQSTAFALDFAAIGITDLSNISERRTYQLLSGQNGLPGFLVAKPGLNSGFMVVQYTSAALLNENKVLSNPASVDTIPTCHLQEDHVSMGGTSAYKLQTILDNCETILAIELMTACQAIDMNPGLQLSERGRAIYEAVREEIPFVKEDHLMAGLISKSRDLCQHSTVIAQQLAEMQAQ'\n",
    "\n",
    "# Step 1. Compute sgt embedding for the query protein.\n",
    "query_protein_sgt_embedding = sgt_.fit(list(query_protein))\n",
    "\n",
    "# Step 2. Compute the dot product of query embedding \n",
    "# with the protein embedding database.\n",
    "similarity = sgtembedding_df.dot(query_protein_sgt_embedding)\n",
    "\n",
    "# Step 3. Return the top k protein names based on similarity.\n",
    "similarity.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGT - Spark implementation for faster embedding\n",
    "\n",
    "As mentioned in the previous section, sequence data sets can be large. SGT complexity is linear with the number of sequences in a data set. Still if the data size is large the computation becomes high. For example, for a set of 1M protein sequences the default SGT mode takes over 24 hours.\n",
    "\n",
    "Using distributed computing with Spark the runtime can be significantly reduced. For instance, SGT-Spark on the same 1M protein data set took less than 29 minutes.\n",
    "\n",
    "In the following, Spark implementation for SGT is shown. First, it is applied on a smaller 10k data set for comparison. Then it is applied on 1M data set without any syntactical change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load the data and remove header.\n",
    "'''\n",
    "data = sc.textFile('data/protein-uniprot-reviewed-Ano-10k.tab')\n",
    " \n",
    "header = data.first() #extract header\n",
    "data = data.filter(lambda row: row != header)   #filter out header\n",
    "data.take(1)  # See one sample\n",
    "\n",
    "# ['I2WKR6\\tI2WKR6_ECOLX\\tunreviewed\\tType III restriction enzyme, res subunit (EC 3.1.21.5)\\tEC90111_4246\\tEscherichia coli 9.0111\\t786\\tMVHKSDSDELAALRAENVRLVSLLEAHGIEWRRKPQSPVPRVSVLSTNEKVALFRRLFRGRDDVWALRWESKTSGKSGYSPACANEWQLGICGKPRIKCGDCAHRQLIPVSDLVIYHHLAGTHTAGMYPLLEDDSCYFLAVDFDEAEWQKDASAFMRSCDELGVPAALEISRSRQGAHVWIFFASRVSAREARRLGTAIISYTCSRTRQLRLGSYDRLFPNQDTMPKGGFGNLIALPLQKRPRELGGSVFVDMNLQPYPDQWAFLVSVIPMNVQDIEPTILRATGSIHPLDVNFINEEDLGTPWEEKKSSGNRLNIAVTEPLIITLANQIYFEKAQLPQALVNRLIRLAAFPNPEFYKAQAMRMSVWNKPRVIGCAENYPQHIALPRGCLDSALSFLRYNNIAAELIDKRFAGTECNAVFTGNLRAEQEEAVSALLRYDTGVLCAPTAFGKTVTAAAVIARRKVNTLILVHRTELLKQWQERLAVFLQVGDSIGIIGGGKHKPCGNIDIAVVQSISRHGEVEPLVRNYGQIIVDECHHIGAVSFSAILKETNARYLLGLTATPIRRDGLHPIIFMYCGAIRHTAARPKESLHNLEVLTRSRFTSGHLPSDARIQDIFREIALDHDRTVAIAEEAMKAFGQGRKVLVLTERTDHLDDIASVMNTLKLSPFVLHSRLSKKKRTMLISGLNALPPDSPRILLSTGRLIGEGFDHPPLDTLILAMPVSWKGTLQQYAGRLHREHTGKSDVRIIDFVDTAYPVLLRMWDKRQRGYKAMGYRIVADGEGLSF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition for increasing the parallel processes\n",
    "data = data.repartition(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(line):\n",
    "    '''\n",
    "    Original data are lines where each line has \\t\n",
    "    separated values. We are interested in preserving\n",
    "    the first value (entry id), tmp[0], and the last value\n",
    "    (the sequence), tmp[-1].\n",
    "    '''\n",
    "    tmp = line.split('\\t')\n",
    "    id = tmp[0]\n",
    "    sequence = list(tmp[-1])\n",
    "    return (id, sequence)\n",
    "\n",
    "processeddata = data.map(lambda line: preprocessing(line))\n",
    "processeddata.take(1)  # See one sample\n",
    "\n",
    "# [('A0A2E9WIJ1',\n",
    "#   ['M','Y','I','F','L','T','L','A','L','F','S',...,'F','S','I','F','A','K','L','D','K','N','D'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protein sequence alphabets\n",
    "alphabets = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', \n",
    "             'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', \n",
    "             'W', 'X', 'Y', 'U', 'O']  # List of amino acids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Spark approach.\n",
    "In this approach the alphabets argument has to\n",
    "be passed to the SGT class definition.\n",
    "The SGT.fit() is then called in parallel.\n",
    "'''\n",
    "sgt_ = sgt.SGT(alphabets=alphabets, \n",
    "               kappa=1, \n",
    "               lengthsensitive=True, \n",
    "               flatten=True)\n",
    "rdd = processeddata.map(lambda x: (x[0], list(sgt_.fit(x[1]))))\n",
    "sgtembeddings = rdd.collect()\n",
    "# Command took 29.66 seconds -- by cranjan@processminer.com at 4/22/2020, 12:31:23 PM on databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with the default SGT mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "data = pd.read_csv('data/protein-uniprot-reviewed-Ano-10k.tab', sep='\\t')\n",
    "\n",
    "# Data preprocessing\n",
    "corpus = data.loc[:,['Entry','Sequence']]\n",
    "corpus.columns = ['id', 'sequence']\n",
    "corpus['sequence'] = corpus['sequence'].map(list)\n",
    "corpus.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgt_ = sgt.SGT(alphabets=alphabets, \n",
    "               lengthsensitive=True, \n",
    "               kappa=1, \n",
    "               flatten=True, \n",
    "               mode='default')\n",
    "\n",
    "sgtembedding_df = sgt_.fit_transform(corpus)\n",
    "# Command took 13.08 minutes -- by cranjan@processminer.com at 4/22/2020, 1:48:02 PM on databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1M Protein Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load the data and remove header.\n",
    "'''\n",
    "data = sc.textFile('data/protein-uniprot-reviewed-Ano-1M.tab')\n",
    " \n",
    "header = data.first() #extract header\n",
    "data = data.filter(lambda row: row != header)   #filter out header\n",
    "data.take(1)  # See one sample\n",
    "\n",
    "# ['I2WKR6\\tI2WKR6_ECOLX\\tunreviewed\\tType III restriction enzyme, res subunit (EC 3.1.21.5)\\tEC90111_4246\\tEscherichia coli 9.0111\\t786\\tMVHKSDSDELAALRAENVRLVSLLEAHGIEWRRKPQSPVPRVSVLSTNEKVALFRRLFRGRDDVWALRWESKTSGKSGYSPACANEWQLGICGKPRIKCGDCAHRQLIPVSDLVIYHHLAGTHTAGMYPLLEDDSCYFLAVDFDEAEWQKDASAFMRSCDELGVPAALEISRSRQGAHVWIFFASRVSAREARRLGTAIISYTCSRTRQLRLGSYDRLFPNQDTMPKGGFGNLIALPLQKRPRELGGSVFVDMNLQPYPDQWAFLVSVIPMNVQDIEPTILRATGSIHPLDVNFINEEDLGTPWEEKKSSGNRLNIAVTEPLIITLANQIYFEKAQLPQALVNRLIRLAAFPNPEFYKAQAMRMSVWNKPRVIGCAENYPQHIALPRGCLDSALSFLRYNNIAAELIDKRFAGTECNAVFTGNLRAEQEEAVSALLRYDTGVLCAPTAFGKTVTAAAVIARRKVNTLILVHRTELLKQWQERLAVFLQVGDSIGIIGGGKHKPCGNIDIAVVQSISRHGEVEPLVRNYGQIIVDECHHIGAVSFSAILKETNARYLLGLTATPIRRDGLHPIIFMYCGAIRHTAARPKESLHNLEVLTRSRFTSGHLPSDARIQDIFREIALDHDRTVAIAEEAMKAFGQGRKVLVLTERTDHLDDIASVMNTLKLSPFVLHSRLSKKKRTMLISGLNALPPDSPRILLSTGRLIGEGFDHPPLDTLILAMPVSWKGTLQQYAGRLHREHTGKSDVRIIDFVDTAYPVLLRMWDKRQRGYKAMGYRIVADGEGLSF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition for increasing the parallel processes\n",
    "data = data.repartition(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processeddata = data.map(lambda line: preprocessing(line))\n",
    "processeddata.take(1)  # See one sample\n",
    "\n",
    "# [('A0A2E9WIJ1',\n",
    "#   ['M','Y','I','F','L','T','L','A','L','F','S',...,'F','S','I','F','A','K','L','D','K','N','D'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protein sequence alphabets\n",
    "alphabets = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', \n",
    "             'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', \n",
    "             'W', 'X', 'Y', 'U', 'O']  # List of amino acids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Spark approach.\n",
    "In this approach the alphabets argument has to\n",
    "be passed to the SGT class definition.\n",
    "The SGT.fit() is then called in parallel.\n",
    "'''\n",
    "sgt_ = sgt.SGT(alphabets=alphabets, \n",
    "               kappa=1, \n",
    "               lengthsensitive=True, \n",
    "               flatten=True)\n",
    "rdd = processeddata.map(lambda x: (x[0], list(sgt_.fit(x[1]))))\n",
    "sgtembeddings = rdd.collect()\n",
    "# Command took 28.98 minutes -- by cranjan@processminer.com at 4/22/2020, 3:16:41 PM on databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''OPTIONAL.\n",
    "Save the embeddings for future use or \n",
    "production deployment.'''\n",
    "# Save for deployment\n",
    "# pickle.dump(sgtembeddings, \n",
    "#             open(\"data/protein-sgt-embeddings-1M.pkl\", \"wb\"))\n",
    "# The pickle dump is shared at https://mega.nz/file/hiAxAAoI#SStAIn_FZjAHvXSpXfdy8VpISG6rusHRf9HlUSqwcsw\n",
    "# sgtembeddings = pickle.load(open(\"data/protein-sgt-embeddings-1M.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Search using SGT - Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `sgtembeddings` on the 1M data set is large it is recommended to use distributed computing to find similar proteins during a search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgtembeddings_rdd = sc.parallelize(list(dict(sgtembeddings).items()))\n",
    "sgtembeddings_rdd = sgtembeddings_rdd.repartition(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Search proteins similar to a query protein.\n",
    "The approach is to find the SGT embedding of the\n",
    "query protein and find its similarity with the\n",
    "embeddings of the protein database.\n",
    "'''\n",
    "\n",
    "query_protein = 'MSHVFPIVIDDNFLSPQDLVSAARSGCSLRLHTGVVDKIDRAHRFVLEIAGAEALHYGINTGFGSLCTTHIDPADLSTLQHNLLKSHACGVGPTVSEEVSRVVTLIKLLTFRTGNSGVSLSTVNRIIDLWNHGVVGAIAQKGTVGASGDLAPLAHLFLPLIGLGQVWHRGVLRPSREVMDELKLAPLTLQPKDGLCLTNGVQYLNAWGALSTVRAKRLVALADLCAAMSMMGFSAARSFIEAQIHQTSLHPERGHVALHLRTLTHGSNHADLPHCNPAMEDPYSFRCAPQVHGAARQVVGYLETVIGNECNSVSDNPLVFPDTRQILTCGNLHGQSTAFALDFAAIGITDLSNISERRTYQLLSGQNGLPGFLVAKPGLNSGFMVVQYTSAALLNENKVLSNPASVDTIPTCHLQEDHVSMGGTSAYKLQTILDNCETILAIELMTACQAIDMNPGLQLSERGRAIYEAVREEIPFVKEDHLMAGLISKSRDLCQHSTVIAQQLAEMQAQ'\n",
    "\n",
    "# Step 1. Compute sgt embedding for the query protein.\n",
    "query_protein_sgt_embedding = sgt_.fit(list(query_protein))\n",
    "\n",
    "# Step 2. Broadcast the embedding to the cluster.\n",
    "query_protein_sgt_embedding_broadcasted = sc.broadcast(list(query_protein_sgt_embedding))\n",
    "\n",
    "# Step 3. Compute similarity between each sequence embedding and the query.\n",
    "similarity = sgtembeddings_rdd.map(lambda x: (x[0], \n",
    "                                              np.dot(query_protein_sgt_embedding_broadcasted.value, \n",
    "                                                     x[1]))).collect()\n",
    "\n",
    "# Step 4. Show the most similar sequences with the query.\n",
    "pd.DataFrame(similarity).sort_values(by=1, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Data sets provided with this release are,\n",
    "\n",
    "### Simulated Sequence Dataset\n",
    "\n",
    "A benchmark simulated sequence data set with labels are provided. There are 5 labels and a total of 300 samples. The sequence lengths range from 50-800.\n",
    "\n",
    "Location:\n",
    "\n",
    "`data/simulated-sequence-dataset.csv`\n",
    "\n",
    "### Protein Dataset - 2k\n",
    "\n",
    "Protein sequences data set taken from https://www.uniprot.org. The data set has reviewed and annotated proteins. The fields in the data set are,\n",
    "\n",
    "- Entry\n",
    "- Entry name\t\n",
    "- Status\t\n",
    "- Protein names\t\n",
    "- Gene names\t\n",
    "- Organism\t\n",
    "- Length\t\n",
    "- Sequence\t\n",
    "- Function [CC]\t\n",
    "- Features\t\n",
    "- Taxonomic lineage (all)\n",
    "- Protein families\n",
    "\n",
    "There are a total of 2113 samples (protein sequences). The proteins have one of the following two functions,\n",
    "\n",
    "- Binds to DNA and alters its conformation. May be involved in regulation of gene expression, nucleoid organization and DNA protection.\n",
    "- Might take part in the signal recognition particle (SRP) pathway. This is inferred from the conservation of its genetic proximity to ftsY/ffh. May be a regulatory protein.\n",
    "\n",
    "\n",
    "The data set has about 40:60 class distribution.\n",
    "\n",
    "Location:\n",
    "`data/protein_classification.csv`\n",
    "\n",
    "\n",
    "### Darpa Weblog Network Intrusion Dataset\n",
    "\n",
    "This is a processed weblog data provided by DARPA in: DARPA INTRUSION DETECTION EVALUATION DATASET. The link to it is shared by MIT at https://www.ll.mit.edu/r-d/datasets/1998-darpa-intrusion-detection-evaluation-dataset .\n",
    "\n",
    "The available data set is a weblog dump with timestamps. They are converted to sequences and shared here. A sequence is labeled as 1 if it was a potential intrusion, otherwise 0. \n",
    "\n",
    "The data has 112 samples with imbalanced class distribution of about 10% positive labeled samples.\n",
    "\n",
    "The available fields are,\n",
    "\n",
    "- timeduration\n",
    "- seqlen\n",
    "- seq\n",
    "- class\n",
    "\n",
    "Location:\n",
    "\n",
    "`data/darpa_data.csv`\n",
    "\n",
    "\n",
    "### Protein Sequence - 10k, 1M and 3M\n",
    "\n",
    "Three protein sequence data sets of size 10k, 1 Million, and 3 Million are provided. The 10k data set is available in GitHub, while the latter two are available publicly [here](https://mega.nz/folder/MqAzmKqS#2jqJKJifOgnFACP9GqX6QQ) https://mega.nz/folder/MqAzmKqS#2jqJKJifOgnFACP9GqX6QQ .\n",
    "\n",
    "The fields in these data sets are,\n",
    "\n",
    "- Entry\t\n",
    "- Entry name\t\n",
    "- Protein names\t\n",
    "- Gene names\t\n",
    "- Organism\n",
    "- Length\n",
    "- Sequence\n",
    "\n",
    "The source of these data are https://www.uniprot.org .\n",
    "\n",
    "Location:\n",
    "\n",
    "10k: `data/protein-uniprot-reviewed-Ano-10k.tab`\n",
    "\n",
    "1M: `https://mega.nz/folder/MqAzmKqS#2jqJKJifOgnFACP9GqX6QQ/file/t7YlUQTK`\n",
    "\n",
    "3M: `https://mega.nz/folder/MqAzmKqS#2jqJKJifOgnFACP9GqX6QQ/file/InAzwYDa`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sgt.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
