{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0gQXaOBBo1oW"
   },
   "source": [
    "![](images/pytorch-logo-dark.png)\n",
    "\n",
    "# PyTorch 101: Building a Model Step-by-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QEp40UvmAmPK"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kyE2ltG3Auin"
   },
   "source": [
    "**PyTorch** is the **fastest growing** Deep Learning framework and it is also used by **Fast.ai** in its MOOC, [Deep Learning for Coders](https://course.fast.ai/) and its [library](https://docs.fast.ai/).\n",
    "\n",
    "PyTorch is also very *pythonic*, meaning, it feels more natural to use it if you already are a Python developer.\n",
    "\n",
    "Besides, using PyTorch may even improve your health, according to [Andrej Karpathy](https://twitter.com/karpathy/status/868178954032513024) :-)\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/tweet_karpathy.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YSt6oP0rA_FG"
   },
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PPZG7RmuBCv0"
   },
   "source": [
    "There are *many many* PyTorch tutorials around and its documentation is quite complete and extensive. So, **why** should you keep reading this step-by-step tutorial?\n",
    "\n",
    "Well, even though one can find information on pretty much anything PyTorch can do, I missed having a **structured, incremental and from first principles** approach to it.\n",
    "\n",
    "In this tutorial, I will guide you through the *main reasons* why PyTorch makes it much **easier** and more **intuitive** to build a Deep Learning model in Python — **autograd, dynamic computation graph, model classes** and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "<h3>\n",
    "<ul>\n",
    "    <li>A Simple Problem - Linear Regression</li>\n",
    "</ul>\n",
    "<ul>\n",
    "    <li>PyTorch: tensors, tensors, tensors</li>\n",
    "</ul>\n",
    "<ul>\n",
    "    <li>Gradient Descent in 5 easy steps!</li>\n",
    "</ul>\n",
    "<ul>\n",
    "    <li>Autograd, your companion for all your gradient needs!</li>\n",
    "</ul>\n",
    "<ul>\n",
    "    <li>Dynamic Computation Graph: what is that?</li>\n",
    "</ul>\n",
    "<ul>\n",
    "    <li>Optimizer: learning the parameters step-by-step</li>\n",
    "</ul>\n",
    "<ul>\n",
    "    <li>Loss: aggregating erros into a single value</li>\n",
    "</ul>\n",
    "<ul>\n",
    "    <li>Model: making predictions</li>\n",
    "</ul>\n",
    "<ul>\n",
    "    <li>Dataset</li>\n",
    "</ul>\n",
    "<ul>\n",
    "    <li>DataLoader, splitting your data into mini-batches</li>\n",
    "</ul>\n",
    "<ul>\n",
    "    <li>Evaluation: does it generalize?</li>\n",
    "</ul>\n",
    "<ul>\n",
    "    <li>Saving (and loading) models: taking a break</li>\n",
    "</ul>\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Y3FcRg0qJ0s"
   },
   "source": [
    "## A Simple Problem - Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vlYSOAy8smmJ"
   },
   "source": [
    "Most tutorials start with some nice and pretty *image classification problem* to illustrate how to use PyTorch. It may seem cool, but I believe it **distracts** you from the **main goal: how PyTorch works**?\n",
    "\n",
    "For this reason, in this tutorial, I will stick with a **simple** and **familiar** problem: a **linear regression with a single feature x**! It doesn’t get much simpler than that…\n",
    "\n",
    "$$\n",
    "\\large y = a + b x + \\epsilon\n",
    "$$\n",
    "\n",
    "We can also think of it as the **simplest neural network**: one node, one input, one output, linear activation function.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/NNs_bias_2.png\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "Adapted from <a href=\"http://jalammar.github.io/visual-interactive-guide-basics-neural-networks/\">Source</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oHiXYWCMrGEr"
   },
   "source": [
    "### Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v7gu_5iGsVrA"
   },
   "source": [
    "Let’s start **generating** some synthetic data: we start with a vector of 100 points for our **feature x** and create our **labels** using **a = 1, b = 2** and some Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-MRtiMeAsos"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l36meXzoop0U"
   },
   "outputs": [],
   "source": [
    "true_a = 1\n",
    "true_b = 2\n",
    "N = 100\n",
    "\n",
    "# Data Generation\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(N, 1)\n",
    "y = true_a + true_b * x + .1 * np.random.randn(N, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LGWRIovYrsxp"
   },
   "source": [
    "### Train / Validation Split\n",
    "\n",
    "Next, let’s **split** our synthetic data into **train** and **validation** sets, shuffling the array of indices and using the first 80 shuffled points for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6itdbvQXp8Xx"
   },
   "outputs": [],
   "source": [
    "# Shuffles the indices\n",
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Uses first 80 random indices for train\n",
    "train_idx = idx[:int(N*.8)]\n",
    "# Uses the remaining indices for validation\n",
    "val_idx = idx[int(N*.8):]\n",
    "\n",
    "# Generates train and validation sets\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "colab_type": "code",
    "id": "D4E8Dqaar1bn",
    "outputId": "8ee22423-5276-4f2d-b2a9-e7a425278224"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax[0].scatter(x_train, y_train)\n",
    "ax[0].set_xlabel('x')\n",
    "ax[0].set_ylabel('y')\n",
    "ax[0].set_ylim([1, 3])\n",
    "ax[0].set_title('Generated Data - Train')\n",
    "ax[1].scatter(x_val, y_val, c='r')\n",
    "ax[1].set_xlabel('x')\n",
    "ax[1].set_ylabel('y')\n",
    "ax[1].set_ylim([1, 3])\n",
    "ax[1].set_title('Generated Data - Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-p8xcQKo9pDQ"
   },
   "source": [
    "## PyTorch: tensors, tensors, tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2bKKlGN9BaBa"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet torchviz\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2usggtDH9r2e"
   },
   "source": [
    "First, we need to cover a **few basic concepts** that may throw you off-balance if you don’t grasp them well enough before going full-force on modeling.\n",
    "\n",
    "In Deep Learning, we see **tensors** everywhere. Well, Google’s framework is called *TensorFlow* for a reason! *What is a tensor, anyway*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vwyU03EC-zPj"
   },
   "source": [
    "### Tensors\n",
    "\n",
    "In *Numpy*, you may have an **array** that has **three dimensions**, right? That is, technically speaking, a **tensor**.\n",
    "\n",
    "A **scalar** (a single number) has **zero** dimensions, a **vector has one** dimension, a **matrix has two** dimensions and a **tensor has three or more dimensions**. That’s it!\n",
    "\n",
    "But, to keep things simple, it is commonplace to call vectors and matrices tensors as well — so, from now on, **everything is either a scalar or a tensor**.\n",
    "\n",
    "![alt text](images/linear_dogs.jpg)\n",
    "Tensors are just higher-dimensional matrices :-) [Source](http://karlstratos.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rPdIb6pIXrwW"
   },
   "source": [
    "You can create **tensors** in PyTorch pretty much the same way you create **arrays** in Numpy. Using [**tensor()**](https://pytorch.org/docs/stable/torch.html#torch.tensor) you can create either a scalar or a tensor.\n",
    "\n",
    "PyTorch's tensors have equivalent functions as its Numpy counterparts, like: [**ones()**](https://pytorch.org/docs/stable/torch.html#torch.ones), [**zeros()**](https://pytorch.org/docs/stable/torch.html#torch.zeros), [**rand()**](https://pytorch.org/docs/stable/torch.html#torch.rand), [**randn()**](https://pytorch.org/docs/stable/torch.html#torch.randn) and many more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "KeV7KOyBUi2S",
    "outputId": "87c42cb8-2e8b-4113-856b-d8ab0bce28d9"
   },
   "outputs": [],
   "source": [
    "scalar = torch.tensor(3.14159)\n",
    "vector = torch.tensor([1, 2, 3])\n",
    "matrix = torch.ones((2, 3), dtype=torch.float)\n",
    "tensor = torch.randn((2, 3, 4), dtype=torch.float)\n",
    "\n",
    "print(scalar)\n",
    "print(vector)\n",
    "print(matrix)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wUp_UkLOcApC"
   },
   "source": [
    "You can get the shape of a tensor using its [**size()**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size) method or its **shape** attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pumy8ocYbpcC",
    "outputId": "de6392ac-100c-4935-85c7-aa4095347c17"
   },
   "outputs": [],
   "source": [
    "print(tensor.size(), tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-9UB154cIsr"
   },
   "source": [
    "You can also reshape a tensor using its [**reshape()**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape) or [**view()**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view) methods.\n",
    "\n",
    "Beware: these methods create a new tensor with the desired shape that **shares the underlying data** with the original tensor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dd5eSRi-XObM",
    "outputId": "d1a67c41-e177-4280-a280-202491c93135"
   },
   "outputs": [],
   "source": [
    "new_tensor1 = tensor.reshape(2, -1)\n",
    "new_tensor2 = tensor.view(2, -1)\n",
    "print(new_tensor1.shape, new_tensor2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cCt37k99g2nm"
   },
   "source": [
    "If you want to copy all data for real, that is, duplicate it in memory, you should use either its [**copy_()**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.copy_) or [**clone()**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.clone) methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q67_KSCSAAT1"
   },
   "source": [
    "### Loading Data, Devices and CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OfRQIc19AEYA"
   },
   "source": [
    "”*How do we go from Numpy’s arrays to PyTorch’s tensors*”, you ask? \n",
    "\n",
    "That’s what [**from_numpy()**](https://pytorch.org/docs/stable/torch.html#torch.from_numpy) is good for. It returns a **CPU tensor**, though.\n",
    "\n",
    "You can also easily **cast** it to a lower precision (32-bit float) using [**float()**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.float)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fgIjFB6gBUuz",
    "outputId": "24ef9c69-e450-4d3b-f6a1-9334627cee3c"
   },
   "outputs": [],
   "source": [
    "# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n",
    "x_train_tensor = torch.from_numpy(x_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "\n",
    "print(type(x_train), type(x_train_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hux68h4oCFt4"
   },
   "source": [
    "“*But I want to use my fancy GPU…*”, you say.\n",
    "\n",
    "No worries, that’s what [**to()**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.to) is good for. It sends your tensor to whatever **device** you specify, including your **GPU** (referred to as `cuda` or `cuda:0`).\n",
    "\n",
    "“*What if I want my code to fallback to CPU if no GPU is available?*”, you may be wondering… \n",
    "\n",
    "PyTorch got your back once more — you can use [**cuda.is_available()**](https://pytorch.org/docs/stable/cuda.html?highlight=is_available#torch.cuda.is_available) to find out if you have a GPU at your disposal and set your device accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Q5qlPSxvCGKf",
    "outputId": "cdc313ee-ec97-46b7-99bc-0c757973beb0"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\n",
    "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "\n",
    "print(type(x_train), type(x_train_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4lsYIvjQDE7O"
   },
   "source": [
    "If you compare the **types** of both variables, you’ll get what you’d expect: `numpy.ndarray` for the first one and `torch.Tensor` for the second one.\n",
    "\n",
    "But where does your nice tensor “live”? In your CPU or your GPU? You can’t say… but if you use PyTorch’s **type()**, it will reveal its **location** — `torch.cuda.FloatTensor` — a GPU tensor in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QbECiJmaC8nt",
    "outputId": "957b59fe-5c8d-4cdc-f5b8-f1f871c27c87"
   },
   "outputs": [],
   "source": [
    "print(x_train_tensor.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XF0MnXHYDRF4"
   },
   "source": [
    "We can also go the other way around, turning tensors back into Numpy arrays, using [**numpy()**](https://pytorch.org/docs/stable/tensors.html?highlight=numpy#torch.Tensor.numpy). It should be easy as `x_train_tensor.numpy()` but…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "colab_type": "code",
    "id": "x1uCOtZvDdT0",
    "outputId": "cd027659-6872-420b-cf5c-b7125f727edd"
   },
   "outputs": [],
   "source": [
    "x_train_tensor.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dNKoo-n0DlUP"
   },
   "source": [
    "Unfortunately, Numpy **cannot** handle GPU tensors… you need to make them CPU tensors first using [**cpu()**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cpu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ouRBUpRaDfUG",
    "outputId": "25379c5c-f648-4bfa-de50-e0ab21dbd938"
   },
   "outputs": [],
   "source": [
    "x_train_tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1pYm2AqND-_T"
   },
   "source": [
    "### Creating Tensor for Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fCT2YOYjEGJ_"
   },
   "source": [
    "What distinguishes a *tensor* used for *data* — like the ones we’ve just created — from a **tensor** used as a (*trainable*) **parameter/weight**?\n",
    "\n",
    "The latter tensors require the **computation of its gradients**, so we can **update** their values (the parameters’ values, that is). That’s what the **`requires_grad=True`** argument is good for. It tells PyTorch we want it to compute gradients for us.\n",
    "\n",
    "---\n",
    "\n",
    "<h2><b><i>A tensor for a learnable parameter requires gradient!</i></b></h2>\n",
    "\n",
    "---\n",
    "\n",
    "You may be tempted to create a simple tensor for a parameter and, later on, send it to your chosen device, as we did with our data, right?\n",
    "\n",
    "Actually, you should **assign** tensors to a **device** at the moment of their **creation** to avoid unexpected behaviors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vb866lTxiTiA",
    "outputId": "82d08dde-6858-4f8e-a9ea-4556488e0a94"
   },
   "outputs": [],
   "source": [
    "# We can specify the device at the moment of creation - RECOMMENDED!\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DGp27LxNi5ZL"
   },
   "source": [
    "Now that we know how to create tensors that require gradients, let’s see how PyTorch handles them — that’s the role of the…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i2SePFR0syOS"
   },
   "source": [
    "## Gradient Descent in 5 easy steps!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aI0IWVbPtGzw"
   },
   "source": [
    "Gradient descent is the most common **optimization algorithm** in Machine Learning and Deep Learning.\n",
    "\n",
    "The purpose of using gradient descent is **to minimize the loss**, that is, **minimize the errors between predictions and actual values** (and sometimes some other term as well).\n",
    "\n",
    "It goes beyond the scope of this tutorial to fully explain how gradient descent works, but I'll cover the **five basic steps** you'd need to go through to compute it, namely:\n",
    "\n",
    "- Step 0: Random initialize parameters / weights\n",
    "- Step 1: Compute model's predictions - forward pass\n",
    "- Step 2: Compute loss\n",
    "- Step 3: Compute the gradients\n",
    "- Step 4: Update the parameters\n",
    "- Step 5: Rinse and repeat!\n",
    "\n",
    "---\n",
    "\n",
    "If you want to learn more about gradient descent, check the following resources:\n",
    "- [**Linear Regression Simulator**](https://www.mladdict.com/linear-regression-simulator), which goes through the very same steps listed here\n",
    "- [**A Visual and Interactive Guide to the Basics of Neuran Networks**](http://jalammar.github.io/visual-interactive-guide-basics-neural-networks/)\n",
    "- [**Gradient Descent Algorithms and Its Variants**](https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R04jMPtj2wtg"
   },
   "source": [
    "### Step 0: Initialization\n",
    "\n",
    "Technically, this step is not part of gradient descent, but it is an important step nonetheless.\n",
    "\n",
    "For training a model, you need to **randomly initialize the parameters/weights** (we have only two, **a** and **b**).\n",
    "\n",
    "Make sure to *always initialize your random seed* to ensure **reproducibility** of your results. As usual, the random seed is [42](https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker%27s_Guide_to_the_Galaxy#Answer_to_the_Ultimate_Question_of_Life,_the_Universe,_and_Everything_(42)), the *least random* of all random seeds one could possibly choose :-)\n",
    "\n",
    "**BTW: we are back to Numpy for a little while!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nMIZZu9o4JNs",
    "outputId": "18a125b4-e501-47ab-c365-b3571e9071da"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "a = np.random.randn(1)\n",
    "b = np.random.randn(1)\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TO1oNfOq5_Oi"
   },
   "source": [
    "### Step 1: Compute Model's Predictions\n",
    "\n",
    "This is the **forward pass** - it simply *computes the model's predictions using the current values of the parameters/weights*. At the very beginning, we will be producing really bad predictions, as we started with random values from Step 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iAIfJb-v6eo0"
   },
   "outputs": [],
   "source": [
    "# Computes our model's predicted output\n",
    "yhat = a + b * x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AAJChW8Gs2vB"
   },
   "source": [
    "### Step 2: Compute Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qFkXT7untW14"
   },
   "source": [
    "There is a subtle but fundamental difference between **error** and **loss**. \n",
    "\n",
    "The **error** is the difference between **actual** and **predicted** computed for a single data point.\n",
    "\n",
    "$$\n",
    "\\Large error_i = y_i - \\hat{y_i}\n",
    "$$\n",
    "\n",
    "The **loss**, on the other hand, is some sort of **aggregation of errors for a set of data points**.\n",
    "\n",
    "For a regression problem, the **loss** is given by the **Mean Square Error (MSE)**, that is, the average of all squared differences between **actual values** (y) and **predictions** (a + bx).\n",
    "\n",
    "$$\n",
    "\\large MSE = \\frac{1}{N} \\sum_{i=1}^N{error_i}^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\large MSE = \\frac{1}{N} \\sum_{i=1}^N{(y_i - \\hat{y_i})}^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\large MSE = \\frac{1}{N} \\sum_{i=1}^N{(y_i - a - b x_i)}^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "It is worth mentioning that, if we **compute the loss** using:\n",
    "- **all points** in the training set (N), we are performing a **batch** gradient descent\n",
    "- a **single point** at each time, it would be a **stochastic** gradient descent\n",
    "- anything else (n) **in-between 1 and N** characterizes a **mini-batch** gradient descent\n",
    "\n",
    "---\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/batch_vs_stochastic.png\">\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "<a href=\"https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3\">Source</a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vF-utTp22es0",
    "outputId": "abba8a14-3810-4529-b2a1-556558d1ced2"
   },
   "outputs": [],
   "source": [
    "# How wrong is our model? That's the error! \n",
    "error = (y_train - yhat)\n",
    "\n",
    "# It is a regression, so it computes mean squared error (MSE)\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bO8K6rePs48M"
   },
   "source": [
    "### Step 3: Compute the Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m-RZUTYvzDGB"
   },
   "source": [
    "A **gradient** is a **partial derivative** — *why partial*? Because one computes it with respect to (w.r.t.) a **single parameter**. We have two parameters, **a** and **b**, so we must compute two partial derivatives.\n",
    "\n",
    "A **derivative** tells you *how much* **a given quantity changes** when you *slightly* vary some **other quantity**. In our case, how much does our **MSE** **loss** change when we vary **each one of our two parameters**?\n",
    "\n",
    "The *right-most* part of the equations below is what you usually see in implementations of gradient descent for a simple linear regression. In the **intermediate step**, I show you **all elements** that pop-up from the application of the [chain rule](https://en.wikipedia.org/wiki/Chain_rule), so you know how the final expression came to be.\n",
    "\n",
    "---\n",
    "\n",
    "<h3><i><b>Gradient = how much the LOSS changes if ONE parameter changes a little bit!</b></i></h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UaSuPRY5zxqb"
   },
   "source": [
    "**Gradients**:\n",
    "\n",
    "$$\n",
    "\\large \\frac{\\partial{MSE}}{\\partial{a}} = \\frac{\\partial{MSE}}{\\partial{\\hat{y_i}}} \\cdot \\frac{\\partial{\\hat{y_i}}}{\\partial{a}} = \\frac{1}{N} \\sum_{i=1}^N{2(y_i - a - b x_i) \\cdot (-1)} = -2 \\frac{1}{N} \\sum_{i=1}^N{(y_i - \\hat{y_i})}\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\large \\frac{\\partial{MSE}}{\\partial{b}} = \\frac{\\partial{MSE}}{\\partial{\\hat{y_i}}} \\cdot \\frac{\\partial{\\hat{y_i}}}{\\partial{b}} = \\frac{1}{N} \\sum_{i=1}^N{2(y_i - a - b x_i) \\cdot (-x_i)} = -2 \\frac{1}{N} \\sum_{i=1}^N{x_i (y_i - \\hat{y_i})}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6coSXbUz4h1F",
    "outputId": "ef8f1c8a-fdf9-4e90-e43d-1ca61d9a00bd"
   },
   "outputs": [],
   "source": [
    "# Computes gradients for both \"a\" and \"b\" parameters\n",
    "a_grad = -2 * error.mean()\n",
    "b_grad = -2 * (x_train * error).mean()\n",
    "print(a_grad, b_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yz6DZyBCs9sX"
   },
   "source": [
    "### Step 4: Update the Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UzaeN06rz5Ok"
   },
   "source": [
    "In the final step, we **use the gradients to update** the parameters. Since we are trying to **minimize** our **losses**, we **reverse the sign** of the gradient for the update.\n",
    "\n",
    "There is still another parameter to consider: the **learning rate**, denoted by the *Greek letter* **eta** (that looks like the letter **n**), which is the **multiplicative factor** that we need to apply to the gradient for the parameter update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dmBroXjJ0L4I"
   },
   "source": [
    "**Parameters**:\n",
    "\n",
    "$$\n",
    "\\large a = a - \\eta \\frac{\\partial{MSE}}{\\partial{a}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\large b = b - \\eta \\frac{\\partial{MSE}}{\\partial{b}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tU6sjDLx0YUq"
   },
   "source": [
    "Let's start with a value of **0.1** (which is a relatively *big value*, as far as learning rates are concerned!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "XSEigxDH5LJh",
    "outputId": "edc03ab4-27ed-420c-8a15-632a8bda615e"
   },
   "outputs": [],
   "source": [
    "# Sets learning rate\n",
    "lr = 1e-1\n",
    "print(a, b)\n",
    "\n",
    "# Updates parameters using gradients and the learning rate\n",
    "a = a - lr * a_grad\n",
    "b = b - lr * b_grad\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RKF3qNFkIXIi"
   },
   "source": [
    "---\n",
    "\n",
    "<h2><b><i>\"Choose your learning rate wisely...\"</b></i></h2>\n",
    "\n",
    "<h3><i><b>The learning rate is the single most important hyper-parameter to tune when you are using Deep Learning models!</b></i></h3>\n",
    "\n",
    "What happens if I choose the learning rate **poorly**? Your model may **take too long to train** or **get stuck with a high loss** or, even worse, **diverge into an exploding loss**!\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/learningrates.jpeg\">\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "<a href=\"http://cs231n.github.io/neural-networks-3/\">Source</a>\n",
    "</p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with Learning Rates\n",
    "\n",
    "Let's work through **an interactive example**!\n",
    "\n",
    "We start at a (not so) **random initial value** of our **feature**, say, -1.5. It has a corresponding **loss** of 2.25.\n",
    "\n",
    "You can choose between **two functions**:\n",
    "- **convex**, meaning, its **loss is well-behaved** and **gradient descent is guaranteed to converge**\n",
    "- **non-convex**, meaning, **all bets are off**!\n",
    "\n",
    "Every time you **take a step**, the plot gets updated:\n",
    "\n",
    "- The **red vector** is our update to the **weight**, that is, **learning rate times gradient**.\n",
    "\n",
    "- The **gray vecto**r shows **how much the cost changes** given our update.\n",
    "\n",
    "- If you divide their lengths, **gray over red**, it will give you the **approximate gradient**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run once if you're in Google Colab\n",
    "#!curl https://raw.githubusercontent.com/dvgodoy/PyTorch101_ODSC_London2019/master/gradient_descent.py --output gradient_descent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import iplot, init_notebook_mode\n",
    "from ipywidgets import VBox, IntSlider, FloatSlider, Dropdown\n",
    "from gradient_descent import *\n",
    "\n",
    "init_notebook_mode(connected=False)\n",
    "\n",
    "w0 = FloatSlider(description='Start', value=-1.5, min=-2, max=2, step=.05)\n",
    "functype = Dropdown(description='Function', options=['Convex', 'Non-Convex'], value='Convex')\n",
    "lrate = FloatSlider(description='Learning Rate', value=.05, min=.05, max=1.1, step=.05)\n",
    "n_steps = IntSlider(description='# updates', value=10, min=10, max=20, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you're in Google Colab\n",
    "#configure_plotly_browser_state()\n",
    "VBox((w0, functype, lrate, n_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you're in Google Colab\n",
    "#configure_plotly_browser_state()\n",
    "fig = build_fig(functype.value, lrate.value, w0.value, n_steps.value)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BIUmWPfKtAm1"
   },
   "source": [
    "### Step 5: Rinse and Repeat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qvwtAKpd0frZ"
   },
   "source": [
    "Now we use the **updated parameters** to go back to **Step 1** and restart the process.\n",
    "\n",
    "Repeating this process over and over, for **many epochs**, is, in a nutshell, **training** a model.\n",
    "\n",
    "---\n",
    "\n",
    "An **epoch** is complete whenever **every point has been already used once for computing the loss**: \n",
    "- **batch** gradient descent: this is trivial, as it uses all points for computing the loss — **one epoch** is the same as **one update**\n",
    "- **stochastic** gradient descent: **one epoch** means **N updates**\n",
    "- **mini-batch** (of size n): **one epoch** has **N/n updates**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Let's put the previous pieces of code together and loop over many epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZyLGYSSLtCeX"
   },
   "outputs": [],
   "source": [
    "# Defines number of epochs\n",
    "n_epochs = 1000\n",
    "\n",
    "# Step 0\n",
    "np.random.seed(42)\n",
    "a = np.random.randn(1)\n",
    "b = np.random.randn(1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1\n",
    "    # Computes our model's predicted output\n",
    "    yhat = a + b * x_train\n",
    "    \n",
    "    # Step 2\n",
    "    # How wrong is our model? That's the error! \n",
    "    error = (y_train - yhat)\n",
    "    # It is a regression, so it computes mean squared error (MSE)\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # Step 3    \n",
    "    # Computes gradients for both \"a\" and \"b\" parameters\n",
    "    a_grad = -2 * error.mean()\n",
    "    b_grad = -2 * (x_train * error).mean()\n",
    "    \n",
    "    # Step 4\n",
    "    # Updates parameters using gradients and the learning rate\n",
    "    a -= lr * a_grad\n",
    "    b -= lr * b_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BwwqUJWv5X0s",
    "outputId": "468da19d-a165-4d4e-806c-f033272bdab7"
   },
   "outputs": [],
   "source": [
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RtH7USnV76Mz"
   },
   "source": [
    "Just keep in mind that, if you **don’t** use batch gradient descent (our example does),you’ll have to write an **inner loop** to perform the **five training steps** for either each **individual point** (**stochastic**) or **n points** (**mini-batch**). We’ll see a mini-batch example later down the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ATtSr6pR8Oed"
   },
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ke4Bw5wI8Q5R"
   },
   "source": [
    "Just to make sure we haven’t done any mistakes in our code, we can use *Scikit-Learn’s Linear Regression* to fit the model and compare the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XUsUvW808QLv",
    "outputId": "27903aa6-3d39-41a1-f4fa-6558b30eb0f9"
   },
   "outputs": [],
   "source": [
    "# Sanity Check: do we get the same results as our gradient descent?\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linr = LinearRegression()\n",
    "linr.fit(x_train, y_train)\n",
    "print(linr.intercept_, linr.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jh8Sc2ff8k7M"
   },
   "source": [
    "They **match** up to 6 decimal places — we have a *fully working implementation of linear regression* using Numpy.\n",
    "\n",
    "**Numpy?! Wait a minute… I thought this tutorial was about PyTorch!**\n",
    "\n",
    "Yes, it is, but this served **two purposes**: *first*, to introduce the **structure** of our task, which will remain largely the same and, *second*, to show you the main **pain points** so you can fully appreciate how much PyTorch makes your life easier :-)\n",
    "\n",
    "<h2><b><i>Numpy?! TORCH IT!</b></i></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "68GypByGi8RO"
   },
   "source": [
    "## Autograd, your companion for all your gradient needs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sPV5It5YjAvL"
   },
   "source": [
    "Autograd is PyTorch’s *automatic differentiation package*. Thanks to it, we **don’t need to worry** about partial derivatives, chain rule or anything like it.\n",
    "\n",
    "<h2><b><i>Computing gradients manually?! No way! Backward!</b></i></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FLIcMHgVo4CU"
   },
   "source": [
    "### backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "irQuMOgJo6FU"
   },
   "source": [
    "So, how do we tell PyTorch to do its thing and **compute all gradients**? That’s what [**backward()**](https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward) is good for.\n",
    "\n",
    "Do you remember the **starting point** for **computing the gradients**? It was the **loss**, as we computed its partial derivatives w.r.t. our parameters. Hence, we need to invoke the `backward()` method from the corresponding Python variable, like, `loss.backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q7Bi9KWgmuXI"
   },
   "outputs": [],
   "source": [
    "# Step 0\n",
    "torch.manual_seed(42)\n",
    "\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S9OilmlLh9_d"
   },
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# Computes our model's predicted output\n",
    "yhat = a + b * x_train_tensor\n",
    "\n",
    "# Step 2    \n",
    "# How wrong is our model? That's the error! \n",
    "error = (y_train_tensor - yhat)\n",
    "# It is a regression, so it computes mean squared error (MSE)\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "# Step 3    \n",
    "# No more manual computation of gradients! \n",
    "loss.backward()\n",
    "\n",
    "# Computes gradients for both \"a\" and \"b\" parameters\n",
    "# a_grad = -2 * error.mean()\n",
    "# b_grad = -2 * (x_train_tensor * error).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "youD0_1Do9_E"
   },
   "source": [
    "### grad / zero_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z2JJ_osngW5H"
   },
   "source": [
    "\n",
    "What about the **actual values** of the **gradients**? We can inspect them by looking at the [**grad**](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.grad) **attribute** of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SM1hYBxlhMoZ",
    "outputId": "06042d01-680b-4746-a270-b31e66e0bc59"
   },
   "outputs": [],
   "source": [
    "print(a.grad, b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qJL6O06ChLio"
   },
   "source": [
    "If you check the method’s documentation, it clearly states that **gradients are accumulated**. \n",
    "\n",
    "You can check this out by running the two code cells above again.\n",
    "\n",
    "So, every time we use the **gradients** to **update** the parameters, we need to **zero the gradients afterwards**. And that’s what [**zero_()**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.zero_) is good for.\n",
    "\n",
    "---\n",
    "\n",
    "*In PyTorch, every method that **ends** with an **underscore (_)** makes changes **in-place**, meaning, they will **modify** the underlying variable.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3wh62U94i7-o",
    "outputId": "23c746f0-fce0-49ec-f45f-8555b2ac97f5"
   },
   "outputs": [],
   "source": [
    "a.grad.zero_(), b.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5dy5vKAOi8J1"
   },
   "source": [
    "So, let’s **ditch** the **manual computation of gradients** and use both `backward()` and `zero_()` methods instead.\n",
    "\n",
    "And, we are still missing **Step 4**, that is, **updating the parameters**. Let's include it as well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "2GAkYJYniX42",
    "outputId": "b0dd52a6-125a-4287-82ec-f16e382fe59a"
   },
   "outputs": [],
   "source": [
    "# Step 0\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "# Step 1\n",
    "# Computes our model's predicted output\n",
    "yhat = a + b * x_train_tensor\n",
    "\n",
    "# Step 2    \n",
    "# How wrong is our model? That's the error! \n",
    "error = (y_train_tensor - yhat)\n",
    "# It is a regression, so it computes mean squared error (MSE)\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "# Step 3    \n",
    "# No more manual computation of gradients! \n",
    "loss.backward()\n",
    "# Computes gradients for both \"a\" and \"b\" parameters\n",
    "# a_grad = -2 * error.mean()\n",
    "# b_grad = -2 * (x_train_tensor * error).mean()\n",
    "print(a.grad, b.grad)\n",
    "\n",
    "# Step 4\n",
    "# Updates parameters using gradients and the learning rate\n",
    "with torch.no_grad(): # what is that?!\n",
    "    a -= lr * a.grad\n",
    "    b -= lr * b.grad\n",
    "\n",
    "# PyTorch is \"clingy\" to its computed gradients, we need to tell it to let it go...\n",
    "a.grad.zero_()\n",
    "b.grad.zero_()\n",
    "\n",
    "print(a.grad, b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M-spqiSkpG3_"
   },
   "source": [
    "### no_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SIhiT37ApJy3"
   },
   "source": [
    "<h2><b><i>\"One does not simply update parameters without no_grad\"</b></i></h2>\n",
    "\n",
    "Why do we need to use [**no_grad()**](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad) to **update the parameters**?\n",
    "\n",
    "The culprit is PyTorch’s ability to build a **dynamic computation graph** from every **Python operation** that involves any **gradient-computing tensor** or its **dependencies**.\n",
    "\n",
    "---\n",
    "\n",
    "**What is a dynamic computation graph?**\n",
    "\n",
    "Don't worry, we’ll go deeper into the inner workings of the dynamic computation graph in the next section.\n",
    "\n",
    "---\n",
    "\n",
    "So, how do we tell PyTorch to “**back off**” and let us **update our parameters** without messing up with its **fancy dynamic computation graph**? \n",
    "\n",
    "\n",
    "That is the purpose of **no_grad()**: it allows us to **perform regular Python operations on tensors, independent of PyTorch’s computation graph**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rxkh-X1ui9qJ",
    "outputId": "c5d3cc14-a26e-4bcc-b6d1-edde36a28765"
   },
   "outputs": [],
   "source": [
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "# Step 0\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1\n",
    "    # Computes our model's predicted output\n",
    "    yhat = a + b * x_train_tensor\n",
    "\n",
    "    # Step 2    \n",
    "    # How wrong is our model? That's the error! \n",
    "    error = (y_train_tensor - yhat)\n",
    "    # It is a regression, so it computes mean squared error (MSE)\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # Step 3    \n",
    "    # No more manual computation of gradients! \n",
    "    loss.backward()\n",
    "\n",
    "    # Step 4\n",
    "    # Updates parameters using gradients and the learning rate\n",
    "    with torch.no_grad():\n",
    "        a -= lr * a.grad\n",
    "        b -= lr * b.grad\n",
    "\n",
    "    # PyTorch is \"clingy\" to its computed gradients, we need to tell it to let it go...\n",
    "    a.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cfde7qBr-dLd"
   },
   "source": [
    "Finally, we managed to successfully run our model and get the **resulting parameters**. Surely enough, they **match** the ones we got in our *Numpy*-only implementation.\n",
    "\n",
    "Let's take a look at the **loss** at the end of the training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qyrLZ7bm-fu3",
    "outputId": "6e292fe9-409b-4bad-e05e-7308b1385acb"
   },
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "shzUgjPH_hqq"
   },
   "source": [
    "What if we wanted to have it as a *Numpy* array? I guess we could just use **numpy()** again, right? (and **cpu()** as well, since our *loss* is in the `cuda` device..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "colab_type": "code",
    "id": "c-fSotmZ_s-9",
    "outputId": "1b52b633-68ab-4077-a4f5-53122695136e"
   },
   "outputs": [],
   "source": [
    "loss.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Uwg76k-_6CH"
   },
   "source": [
    "What happened here? Unlike our *data tensors*, the **loss tensor** is actually computing gradients - and in order to use **numpy**, we need to [**detach()**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.detach_) that tensor from the computation graph first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qWBiGTaLAVfj",
    "outputId": "5372bb61-7f59-4f68-e4bd-f9fdeffd68ef"
   },
   "outputs": [],
   "source": [
    "loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mVvbZu_6AZZU"
   },
   "source": [
    "This seems like **a lot of work**, there must be an easier way! And there is one indeed: we can use [**item()**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.item), for **tensors with a single element** or [**tolist()**](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.tolist) otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PCKOS0RlAx-U",
    "outputId": "fd2bee76-87d3-4b08-d575-d2cb009e4fe5"
   },
   "outputs": [],
   "source": [
    "print(loss.item(), loss.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mL4F2K3KpadZ"
   },
   "source": [
    "## Dynamic Computation Graph: what is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EamDiupxpjSp"
   },
   "source": [
    "<h2><b><i>\"No one can be told what the dynamic computation graph is - you have to see it for yourself\"</b></i></h2>\n",
    "\n",
    "Jokes aside, I want **you** to **see the graph for yourself** too!\n",
    "\n",
    "The [PyTorchViz](https://github.com/szagoruyko/pytorchviz) package and its `make_dot(variable)` method allows us to easily visualize a graph associated with a given Python variable.\n",
    "\n",
    "So, let’s stick with the **bare minimum**: two (gradient computing) **tensors** for our parameters, predictions, errors and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "va5ZPnwdpg0W"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "yhat = a + b * x_train_tensor\n",
    "error = y_train_tensor - yhat\n",
    "loss = (error ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ERIK1XVWqemS"
   },
   "source": [
    "Now let's plot the **computation graph** for the **yhat** variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "colab_type": "code",
    "id": "eft1ShqTp8_k",
    "outputId": "7c169a26-e2d8-446f-d904-7453bc42c783"
   },
   "outputs": [],
   "source": [
    "make_dot(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "We7c0NLkq07K"
   },
   "source": [
    "Let’s take a closer look at its components:\n",
    "\n",
    "* **blue boxes**: these correspond to the **tensors** we use as **parameters**, the ones we’re asking PyTorch to **compute gradients** for;\n",
    "\n",
    "* **gray box**: a **Python operation** that involves a **gradient-computing tensor or its dependencies**;\n",
    "\n",
    "* **green box**: the same as the gray box, except it is the **starting point for the computation** of gradients (assuming the `**backward()**` method is called from the **variable used to visualize** the graph)— they are computed from the **bottom-up** in a graph.\n",
    "\n",
    "Now, take a closer look at the **green box**: there are **two arrows** pointing to it, since it is **adding up two variables**, `a` and `b*x`. Seems obvious, right?\n",
    "\n",
    "Then, look at the **gray box** of the same graph: it is performing a **multiplication**, namely, `b*x`. But there is only **one arrow** pointing to it! The arrow comes from the **blue box** that corresponds to our parameter `b`.\n",
    "\n",
    "Why don’t we have a box for our **data x**? The answer is: **we do not compute gradients for it**! So, even though there are *more* tensors involved in the operations performed by the computation graph, it **only** shows **gradient-computing tensors and its dependencies**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZGrlUBcMriPJ"
   },
   "source": [
    "Try using the `make_dot` method to plot the **computation graph** of other variables, like `error` or `loss`.\n",
    "\n",
    "The **only difference** between them and the first one is the number of **intermediate steps (gray boxes)**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "colab_type": "code",
    "id": "Z3T9goqOqIt3",
    "outputId": "5ca6ca7e-31d9-4ceb-8a0e-c417f1820c36"
   },
   "outputs": [],
   "source": [
    "make_dot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VPZuuf-wt5Yk"
   },
   "source": [
    "What would happen to the computation graph if we set **`requires_grad`** to **`False`** for our parameter **`a`**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BnsSXOMctokE"
   },
   "outputs": [],
   "source": [
    "a_nograd = torch.randn(1, requires_grad=False, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "yhat = a_nograd + b * x_train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "zzActDPeuLjb",
    "outputId": "b5345cf7-07c2-413e-e604-c2468c960d2a"
   },
   "outputs": [],
   "source": [
    "make_dot(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fxojZtzpuQ6W"
   },
   "source": [
    "Unsurprisingly, the **blue box** corresponding to the **parameter a** is no more! \n",
    "\n",
    "Simple enough: **no gradients, no graph**.\n",
    "\n",
    "The **best thing** about the *dynamic computing graph* is the fact that you can make it **as complex as you want it**. You can even use *control flow statements* (e.g., if statements) to **control the flow of the gradients** (obviously!) :-)\n",
    "\n",
    "Let's build a nonsensical, yet complex, computation graph just to make a point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wddV-foIuOcO"
   },
   "outputs": [],
   "source": [
    "yhat = a + b * x_train_tensor\n",
    "error = y_train_tensor - yhat\n",
    "\n",
    "loss = (error ** 2).mean()\n",
    "\n",
    "if loss > 0:\n",
    "    yhat2 = b * x_train_tensor\n",
    "    error2 = y_train_tensor - yhat2\n",
    "\n",
    "loss += error2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "colab_type": "code",
    "id": "J6kP2dA-u6w4",
    "outputId": "986fbd2f-48dc-4918-c1db-22cf75d77728"
   },
   "outputs": [],
   "source": [
    "make_dot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LB6C0xscvB3o"
   },
   "source": [
    "## Optimizer:  learning the parameters step-by-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cwemv-t3vIbJ"
   },
   "source": [
    "So far, we’ve been **manually** updating the parameters using the computed gradients. That’s probably fine for *two parameters*… but what if we had a **whole lot of them**?! We use one of PyTorch’s **optimizers**, like [SGD](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) or [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam).\n",
    "\n",
    "---\n",
    "\n",
    "There are **many** optimizers, **SGD** is the most basic of them and **Adam** is one of the most popular. They achieve the same goal through, literally, **different paths**.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/opt2.gif\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "<a href=\"http://cs231n.github.io/neural-networks-3/\">Source</a>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "In the code below, we create a *Stochastic Gradient Descent* (SGD) optimizer to update our parameters **a** and **b**.\n",
    "\n",
    "---\n",
    "\n",
    "Don’t be fooled by the **optimizer’s** name: if we use **all training data** at once for the update — as we are actually doing in the code — the optimizer is performing a **batch** gradient descent, despite of its name.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gOkDgwawBUR"
   },
   "outputs": [],
   "source": [
    "# Our parameters\n",
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "# Learning rate\n",
    "lr = 1e-1\n",
    "\n",
    "# Defines a SGD optimizer to update the parameters\n",
    "optimizer = optim.SGD([a, b], lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lw8Sku2ev4ML"
   },
   "source": [
    "### step / zero_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i3ljYgxLv52K"
   },
   "source": [
    "An optimizer takes the **parameters** we want to update, the **learning rate** we want to use (and possibly many other hyper-parameters as well!) and **performs the updates** through its [**`step()`**](https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer.step) method.\n",
    "\n",
    "Besides, we also don’t need to zero the gradients one by one anymore. We just invoke the optimizer’s [**`zero_grad()`**](https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer.zero_grad) method and that’s it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YcNkxhfEvDlT",
    "outputId": "2d6ae35e-44d3-45b6-a89a-9f9c0202b006"
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1\n",
    "    yhat = a + b * x_train_tensor\n",
    "\n",
    "    # Step 2\n",
    "    error = y_train_tensor - yhat\n",
    "    loss = (error ** 2).mean()\n",
    "\n",
    "    # Step 3\n",
    "    loss.backward()    \n",
    "    \n",
    "    # Step 4\n",
    "    # No more manual update!\n",
    "    # with torch.no_grad():\n",
    "    #     a -= lr * a.grad\n",
    "    #     b -= lr * b.grad\n",
    "    optimizer.step()\n",
    "    \n",
    "    # No more telling PyTorch to let gradients go!\n",
    "    # a.grad.zero_()\n",
    "    # b.grad.zero_()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zHwVdA0HxCON"
   },
   "source": [
    "Cool! We’ve *optimized* the **optimization** process :-) What’s left?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wAq8ytYfxK7w"
   },
   "source": [
    "## Loss: aggregating erros into a single value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h7f_eHQOxN5b"
   },
   "source": [
    "We now tackle the **loss computation**. As expected, PyTorch got us covered once again. There are many [loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions) to choose from, depending on the task at hand. Since ours is a regression, we are using the [Mean Square Error (MSE)](https://pytorch.org/docs/stable/nn.html#torch.nn.MSELoss) loss.\n",
    "\n",
    "---\n",
    "\n",
    "Notice that `nn.MSELoss` actually **creates a loss function** for us — **it is NOT the loss function itself**. Moreover, you can specify a **reduction method** to be applied, that is, **how do you want to aggregate the results for individual points** — you can average them (reduction=’mean’) or simply sum them up (reduction=’sum’).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eC4EH3rKxL1J",
    "outputId": "fad967b0-1bda-4dc2-8799-ba998d310fd7"
   },
   "outputs": [],
   "source": [
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Pq5dPX8zBZk4",
    "outputId": "cbfba17a-003e-4f46-a501-21a02334d564"
   },
   "outputs": [],
   "source": [
    "fake_labels = torch.tensor([1., 2., 3.])\n",
    "fake_preds = torch.tensor([1., 3., 5.])\n",
    "\n",
    "loss_fn(fake_labels, fake_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kcS4vSiTyG7U"
   },
   "source": [
    "We then **use** the created loss function to compute the loss given our **predictions** and our **labels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SxmHENASx3-A",
    "outputId": "ff4998c9-ba0c-4c7c-fc50-f806ae1b5519"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "# Defines a MSE loss function\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "optimizer = optim.SGD([a, b], lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Step 1\n",
    "    yhat = a + b * x_train_tensor\n",
    "    \n",
    "    # Step 2\n",
    "    # No more manual loss!\n",
    "    # error = y_tensor - yhat\n",
    "    # loss = (error ** 2).mean()\n",
    "    loss = loss_fn(y_train_tensor, yhat)\n",
    "\n",
    "    # Step 3\n",
    "    loss.backward() \n",
    "\n",
    "    # Step 4\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ey-gt8k4yUX6"
   },
   "source": [
    "At this point, there’s only one piece of code left to change: the **predictions**. It is then time to introduce PyTorch’s way of implementing a…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Fh8LOI0yYT7"
   },
   "source": [
    "## Model: making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A03THT7YybBn"
   },
   "source": [
    "In PyTorch, a **model** is represented by a regular **Python class** that inherits from the [**Module**](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) class.\n",
    "\n",
    "The most fundamental methods it needs to implement are:\n",
    "\n",
    "* **`__init__(self)`**: **it defines the parts that make up the model** —in our case, two parameters, **a** and **b**.\n",
    "\n",
    "* **`forward(self, x)`**: it performs the **actual computation**, that is, it **outputs a prediction**, given the input **x**.\n",
    "\n",
    "Let’s build a proper (yet simple) model for our regression task. It should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SNMj2ScHyoXU"
   },
   "outputs": [],
   "source": [
    "class ManualLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        a = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "        b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "\n",
    "        # To make \"a\" and \"b\" real parameters of the model, we need to wrap them with nn.Parameter\n",
    "        self.a = nn.Parameter(a)\n",
    "        self.b = nn.Parameter(b)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Computes the outputs / predictions\n",
    "        return self.a + self.b * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a4CN2oGK0G7z"
   },
   "source": [
    "### Parameter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FBc6HURx0F2i"
   },
   "source": [
    "In the **\\__init__** method, we define our **two parameters**, **a** and **b**, using the [**Parameter()**](https://pytorch.org/docs/stable/nn.html#torch.nn.Parameter) class, to tell PyTorch these **tensors should be considered parameters of the model they are an attribute of**.\n",
    "\n",
    "Why should we care about that? By doing so, we can use our model’s [**parameters()**](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.parameters) method to retrieve **an iterator over all model’s parameters**, even those parameters of **nested models**, that we can use to feed our optimizer (instead of building a list of parameters ourselves!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "Nwf_kZPhz0Qw",
    "outputId": "902fcc06-2fb3-452c-c929-f1303080a894"
   },
   "outputs": [],
   "source": [
    "dummy = ManualLinearRegression()\n",
    "\n",
    "list(dummy.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K-5BiAQs08Tn"
   },
   "source": [
    "Moreover, we can get the **current values for all parameters** using our model’s [**state_dict()**](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.state_dict) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "htdYIsSD0q2G",
    "outputId": "5ce48458-e93d-414f-d75f-cd7d4989da5e"
   },
   "outputs": [],
   "source": [
    "dummy.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zIWl41iKdJdP"
   },
   "source": [
    "### state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WU5er3Y4MdO0"
   },
   "source": [
    "The **state_dict()** of a given model is simply a Python dictionary that **maps each layer / parameter to its corresponding tensor**. But only **learnable** parameters are included, as its purpose is to keep track of parameters that are going to be updated by the **optimizer**.\n",
    "\n",
    "The **optimizer** itself also has a **state_dict()**, which contains its internal state, as well as the hyperparameters used.\n",
    "\n",
    "---\n",
    "\n",
    "It turns out **state_dicts** can also be used for **checkpointing** a model, as we will see later down the line.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "fPlsgkPUemlh",
    "outputId": "b0f29d45-bb84-4fd4-9c69-9a7e17669450"
   },
   "outputs": [],
   "source": [
    "optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hJk570c41MlG"
   },
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OlTBJ4H21WSj"
   },
   "source": [
    "**IMPORTANT**: we need to **send our model to the same device where the data is**. If our data is made of GPU tensors, our model must “live” inside the GPU as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nYJYMvYE1N-Q",
    "outputId": "a318eea6-eaa7-4625-c094-a3324d4fdfe9"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Now we can create a model and send it at once to the device\n",
    "model = ManualLinearRegression().to(device)\n",
    "\n",
    "# We can also inspect its parameters using its state_dict\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sxg4yCviqhJT"
   },
   "source": [
    "### Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6NAAng-mqmdP"
   },
   "source": [
    "The **forward pass** is the moment when the model **makes predictions**.\n",
    "\n",
    "---\n",
    "\n",
    "You should **NOT call the `forward(x)`** method, though. You should **call the whole model itself**, as in **`model(x)`** to perform a forward pass and output predictions.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tuXlvnf-qlw_"
   },
   "outputs": [],
   "source": [
    "yhat = model(x_train_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QG2VxJhf1v_a"
   },
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tbHFyO-22Amu"
   },
   "source": [
    "<h2><b><i>\"What does train() do? It only sets the mode!\"</b></i></h2>\n",
    "\n",
    "In PyTorch, models have a [**train()**](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.train) method which, somewhat disappointingly, **does NOT perform a training step**. Its only purpose is to **set the model to training mode**. \n",
    "\n",
    "Why is this important? Some models may use mechanisms like [**Dropout**](https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout), for instance, which have **distinct behaviors in training and evaluation phases**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Sl-Sd_1f1k--",
    "outputId": "311151e2-4017-4043-ae98-e8bb0c79f2a3"
   },
   "outputs": [],
   "source": [
    "lr = 1e-1\n",
    "n_epochs = 1000\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "# Now the optimizers uses the parameters from the model\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Sets model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Step 1\n",
    "    # No more manual prediction!\n",
    "    # yhat = a + b * x_tensor\n",
    "    yhat = model(x_train_tensor)\n",
    "    \n",
    "    # Step 2\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    "    # Step 3\n",
    "    loss.backward()\n",
    "    # Step 4\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cD_yXamT2V7r"
   },
   "source": [
    "Now, the printed statements will look like this — final values for parameters **a** and **b** are still the same, so everything is ok :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kGrzI0tw2pUX"
   },
   "source": [
    "### Nested Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hpp-gTuB23xK"
   },
   "source": [
    "In our model, we manually created two parameters to perform a linear regression. \n",
    "\n",
    "---\n",
    "\n",
    "You are **not** limited to defining parameters, though… **models can contain other models as its attributes** as well, so you can easily nest them. We’ll see an example of this shortly as well.\n",
    "\n",
    "---\n",
    "\n",
    "Let’s use PyTorch’s [**Linear**](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear) model as an attribute of our own, thus creating a nested model.\n",
    "\n",
    "Even though this clearly is a contrived example, as we are pretty much wrapping the underlying model without adding anything useful (or, at all!) to it, it illustrates well the concept.\n",
    "\n",
    "In the **`__init__`** method, we created an attribute that contains our **nested `Linear` model**.\n",
    "\n",
    "In the **`forward()`** method, we **call the nested model itself** to perform the forward pass (notice, we are **not** calling `self.linear.forward(x)`!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "amK4WCg72rVB"
   },
   "outputs": [],
   "source": [
    "class LayerLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Instead of our custom parameters, we use a Linear layer with single input and single output\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        # Now it only takes a call to the layer to make predictions\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cTaDWLJq3djj"
   },
   "source": [
    "Now, if we call the **parameters()** method of this model, **PyTorch will figure the parameters of its attributes in a recursive way**.\n",
    "\n",
    "You can also add new `Linear` attributes and, even if you don’t use them at all in the forward pass, they will **still** be listed under `parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "396v3N5A3ONO",
    "outputId": "b72889c5-4eef-422e-f56a-7ef2b44d477e"
   },
   "outputs": [],
   "source": [
    "dummy = LayerLinearRegression()\n",
    "\n",
    "list(dummy.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "lRDodDDg3XEs",
    "outputId": "cca27cc1-6e98-4037-9208-affec81e220a"
   },
   "outputs": [],
   "source": [
    "dummy.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1KcS2nS2LxpI"
   },
   "source": [
    "### Layers\n",
    "\n",
    "A **Linear** model can be seen as a **layer** in a neural network.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/layer.png\">\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "<a href=\"https://www.kdnuggets.com/2017/09/neural-network-foundations-explained-activation-function.html\">Source</a>\n",
    "</p>\n",
    "\n",
    "In the example above, the **hidden layer** would be `nn.Linear(3, 4)` and the **output layer** would be `nn.Linear(4, 1)`.\n",
    "\n",
    "\n",
    "There are **MANY** different layers that can be uses in PyTorch:\n",
    "- [Convolution Layers](https://pytorch.org/docs/stable/nn.html#convolution-layers)\n",
    "- [Pooling Layers](https://pytorch.org/docs/stable/nn.html#pooling-layers)\n",
    "- [Padding Layers](https://pytorch.org/docs/stable/nn.html#padding-layers)\n",
    "- [Non-linear Activations](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n",
    "- [Normalization Layers](https://pytorch.org/docs/stable/nn.html#normalization-layers)\n",
    "- [Recurrent Layers](https://pytorch.org/docs/stable/nn.html#recurrent-layers)\n",
    "- [Transformer Layers](https://pytorch.org/docs/stable/nn.html#transformer-layers)\n",
    "- [Linear Layers](https://pytorch.org/docs/stable/nn.html#linear-layers)\n",
    "- [Dropout Layers](https://pytorch.org/docs/stable/nn.html#dropout-layers)\n",
    "- [Sparse Layers (embbedings)](https://pytorch.org/docs/stable/nn.html#sparse-layers)\n",
    "- [Vision Layers](https://pytorch.org/docs/stable/nn.html#vision-layers)\n",
    "- [DataParallel Layers (multi-GPU)](https://pytorch.org/docs/stable/nn.html#dataparallel-layers-multi-gpu-distributed)\n",
    "- [Flatten Layer](https://pytorch.org/docs/stable/nn.html#flatten)\n",
    "\n",
    "We have just used a **Linear** layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X7G3-9GX2sRe"
   },
   "source": [
    "### Sequential Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8G3evlAo37Gj"
   },
   "source": [
    "<h2><b><i>Run-of-the-mill layers? Sequential model!</b></i></h2>\n",
    "\n",
    "Our model was simple enough… You may be thinking: “*why even bother to build a class for it?!*” Well, you have a point…\n",
    "\n",
    "For **straightforward models**, that use **run-of-the-mill layers**, where the output of a layer is sequentially fed as an input to the next, we can use a, er… [**Sequential**](https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential) model :-)\n",
    "\n",
    "In our case, we would build a Sequential model with a single argument, that is, the Linear layer we used to train our linear regression. The model would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mt9ssm5w2vQa"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WZt_4Z3U4KpE"
   },
   "source": [
    "Simple enough, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AgJ1Oi1R4RAF"
   },
   "source": [
    "### Training Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uZ_wUvVA4TEP"
   },
   "source": [
    "So far, we’ve defined:\n",
    "* an **optimizer**\n",
    "\n",
    "* a **loss function**\n",
    "\n",
    "* a **model**\n",
    "\n",
    "Scroll up a bit and take a quick look at the code inside the loop. Would it **change** if we were using a **different optimizer**, or **loss**, or even **model**? If not, how can we make it more generic?\n",
    "\n",
    "Well, I guess we could say all these lines of code **perform a training step**, given those **three elements** (optimizer, loss and model),the **features** and the **labels**.\n",
    "\n",
    "So, how about **writing a function that takes those three elements** and **returns another function that performs a training step**, taking a set of features and labels as arguments and returning the corresponding loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sH-nhgSY4SSC"
   },
   "outputs": [],
   "source": [
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    # Builds function that performs a step in the train loop\n",
    "    def train_step(x, y):\n",
    "        # Sets model to TRAIN mode\n",
    "        model.train()\n",
    "        # Step 1: Makes predictions\n",
    "        yhat = model(x)\n",
    "        # Step 2: Computes loss\n",
    "        loss = loss_fn(yhat, y)\n",
    "        # Step 3: Computes gradients\n",
    "        loss.backward()\n",
    "        # Step 4: Updates parameters and zeroes gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "    \n",
    "    # Returns the function that will be called inside the train loop\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q1z-JIko4ysl"
   },
   "source": [
    "Then we can use this general-purpose function to build a **train_step()** function to be called inside our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4F60DNI64zBk",
    "outputId": "8d5eb83c-e276-4fc1-d840-b68f7cd5dfe9"
   },
   "outputs": [],
   "source": [
    "lr = 1e-1\n",
    "\n",
    "# Create a MODEL, a LOSS FUNCTION and an OPTIMIZER\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Creates the train_step function for our model, loss function and optimizer\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "train_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EkqKUEaj5EOi"
   },
   "source": [
    "Now our code should look like this… see how **tiny** the training loop is now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "p2ysa76u44nX",
    "outputId": "e5d8df37-4578-4373-fd7b-771d531182b5"
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "\n",
    "losses = []\n",
    "# For each epoch...\n",
    "for epoch in range(n_epochs):\n",
    "    # Performs one train step and returns the corresponding loss\n",
    "    loss = train_step(x_train_tensor, y_train_tensor)\n",
    "    losses.append(loss)\n",
    "    \n",
    "# Checks model's parameters\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "colab_type": "code",
    "id": "jOnHnxJQDU5C",
    "outputId": "20b12834-b138-4fe5-fdcf-7c609a29745f"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses[:200])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LWRFuU3R5zqs"
   },
   "source": [
    "Let’s give our training loop a rest and focus on our **data** for a while… so far, we’ve simply used our *Numpy arrays* turned **PyTorch tensors**. But we can do better, we can build a…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6VcHv7EZ56PR"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GUdT1l_Q6Dx_"
   },
   "source": [
    "In PyTorch, a **dataset** is represented by a regular **Python class** that inherits from the [**Dataset**](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class. You can think of it as a kind of a Python **list of tuples**, each tuple corresponding to **one point (features, label)**.\n",
    "\n",
    "The most fundamental methods it needs to implement are:\n",
    "\n",
    "* **`__init__(self)`**: it takes **whatever arguments** needed to build a **list of tuples** — it may be the name of a CSV file that will be loaded and processed; it may be two tensors, one for features, another one for labels; or anything else, depending on the task at hand.\n",
    "\n",
    "* **`__get_item__(self, index)`**: it allows the dataset to be **indexed**, so it can work like a list (`dataset[i]`) — it must **return a tuple (features, label)** corresponding to the requested data point. We can either return the **corresponding slices** of our **pre-loaded** dataset or tensors or, as mentioned above, **load them on demand** (like in this [example](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class)).\n",
    "\n",
    "* **`__len__(self)`**: it should simply return the **size** of the whole dataset so, whenever it is sampled, its indexing is limited to the actual size.\n",
    "\n",
    "---\n",
    "\n",
    "There is **no need to load the whole dataset in the constructor method** (`__init__`). If your **dataset is big** (tens of thousands of image files, for instance), loading it at once would not be memory efficient. It is recommended to **load them on demand** (whenever `__get_item__` is called).\n",
    "\n",
    "---\n",
    "\n",
    "Let’s build a simple custom dataset that takes two tensors as arguments: one for the features, one for the labels. For any given index, our dataset class will return the corresponding slice of each of those tensors. It should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eKu-E5xQ57pE"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "T2Lpj43e7Gpt",
    "outputId": "e73b01f6-84bc-4603-e0b4-c0c0ba51a0f7"
   },
   "outputs": [],
   "source": [
    "# Wait, is this a CPU tensor now? Why? Where is .to(device)?\n",
    "x_train_tensor = torch.from_numpy(x_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "\n",
    "train_data = CustomDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zW1bTD4l7VAE"
   },
   "source": [
    "---\n",
    "\n",
    "Did you notice we built our **training tensors** out of Numpy arrays but we **did not send them to a device**? So, they are **CPU** tensors now! **Why**?\n",
    "\n",
    "We **don’t want our whole training data to be loaded into GPU tensors**, as we have been doing in our example so far, because **it takes up space** in our precious **graphics card’s RAM**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EUwVUhKP75zt"
   },
   "source": [
    "### TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a0-lPsQf770A"
   },
   "source": [
    "Besides, you may be thinking “*why go through all this trouble to wrap a couple of tensors in a class?*”. And, once again, you do have a point… if a dataset is nothing else but a **couple of tensors**, we can use PyTorch’s [**TensorDataset**](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset) class, which will do pretty much what we did in our custom dataset above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wIjLRR_666H2",
    "outputId": "08e88416-cf08-41b3-d297-b65acc07f5bc"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9bZZI1U37ylE"
   },
   "source": [
    "OK, fine, but then again, **why** are we building a dataset anyway? We’re doing it because we want to use a…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FLvd96o77-Rj"
   },
   "source": [
    "## DataLoader, splitting your data into mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JH_FSh4G8CM-"
   },
   "source": [
    "<h2><b><i>- Let's split data into mini-batches<br>- Use DataLoaders!</b></i></h2>\n",
    "\n",
    "Until now, we have used the **whole training data** at every training step. It has been **batch gradient descent** all along. This is fine for our *ridiculously small dataset*, sure, but if we want to go serious about all this, we **must use mini-batch** gradient descent. Thus, we need mini-batches. Thus, we need to **slice** our dataset accordingly. \n",
    "\n",
    "Do you want to do it *manually*?! Me neither!\n",
    "\n",
    "So we use PyTorch’s [**DataLoader**](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class for this job. We tell it which **dataset** to use (the one we just built in the previous section), the desired **mini-batch size** and if we’d like to **shuffle** it or not. That’s it!\n",
    "\n",
    "Our **loader** will behave like an **iterator**, so we can **loop over it** and **fetch a different mini-batch** every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ana8r6AN7_hv"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RHHtcorn8dMq"
   },
   "source": [
    "To retrieve a sample mini-batch, one can simply run the command below — it will return a list containing two tensors, one for the features, another one for the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "colab_type": "code",
    "id": "sPX5ggEC8bXU",
    "outputId": "50146492-76fe-49a7-cf3d-bc7bd3ca43a5"
   },
   "outputs": [],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wrhy1id98iVj"
   },
   "source": [
    "How does this change our training loop? Let’s check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "madfwBfC8fjs",
    "outputId": "027fa3a5-ba20-4f4f-c124-bbf741edc321"
   },
   "outputs": [],
   "source": [
    "lr = 1e-1\n",
    "\n",
    "# Create a MODEL, a LOSS FUNCTION and an OPTIMIZER\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Creates the train_step function for our model, loss function and optimizer\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "n_epochs = 1000\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # inner loop\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # the dataset \"lives\" in the CPU, so do our mini-batches\n",
    "        # therefore, we need to send those mini-batches to the\n",
    "        # device where the model \"lives\"\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        losses.append(loss)\n",
    "        \n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "colab_type": "code",
    "id": "0_QL4Q2aEGR7",
    "outputId": "c1d7d27d-793f-439e-9437-2511f2818d76"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs (?)')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6mc06yWq85XN"
   },
   "source": [
    "Did you notice it is taking **longer** to train now? Can you guess **why**?\n",
    "\n",
    "Two things are different now: not only we have an **inner loop** to load each and every **mini-batch** from our **DataLoader** but, more importantly, we are now **sending only one mini-batch to the device**.\n",
    "\n",
    "---\n",
    "\n",
    "For bigger datasets, **loading data sample by sample** (into a **CPU** tensor) using **Dataset’s \\__get_item__** and then **sending all samples** that belong to the **same mini-batch at once to your GPU** (device) is the way to go in order to make the **best use of your graphics card’s RAM**.\n",
    "\n",
    "Moreover, if you have **many GPUs** to train your model on, it is best to keep your dataset “agnostic” and assign the batches to different GPUs during training.\n",
    "\n",
    "---\n",
    "\n",
    "So far, we’ve focused on the **training data** only. We built a *dataset* and a *data loader* for it. We could do the same for the **validation** data, using the **split** we performed at the beginning of this post… or we could use **random_split** instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JuUK7LBS9u8L"
   },
   "source": [
    "### random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6qndQid95al"
   },
   "source": [
    "<h2><b><i>- How did you split your data?<br>- I didn't...<br>- WHAT?</b></i></h2>\n",
    "\n",
    "PyTorch’s [**random_split()**](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split) method is an easy and familiar way of performing a **training-validation split**. Just keep in mind that, in our example, we need to apply it to the **whole dataset** (not the *training* dataset we built in couple of sections ago).\n",
    "\n",
    "Then, for each subset of data, we build a corresponding DataLoader, so our code looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U_g3S4hx83pW"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "# builds tensors from numpy arrays BEFORE split\n",
    "x_tensor = torch.from_numpy(x).float()\n",
    "y_tensor = torch.from_numpy(y).float()\n",
    "\n",
    "# builds dataset containing ALL data points\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "# performs the split\n",
    "train_dataset, val_dataset = random_split(dataset, [80, 20])\n",
    "\n",
    "# builds a loader of each set\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8pD8ItwP-TMC"
   },
   "source": [
    "Now we have a **data loader** for our **validation** set, so, it makes sense to use it for the…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d6iCpQ1B-YfS"
   },
   "source": [
    "## Evaluation: does it generalize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aQCWnko4-az9"
   },
   "source": [
    "Now, we need to change the training loop to include the **evaluation of our model**, that is, computing the **validation loss**. The first step is to include another inner loop to handle the *mini-batches* that come from the *validation loader* , sending them to the same *device* as our model. Next, we make **predictions** using our model and compute the corresponding **loss**.\n",
    "\n",
    "That’s pretty much it, but there are **two small, yet important**, things to consider:\n",
    "\n",
    "* [**torch.no_grad()**](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad): even though it won’t make a difference in our simple model, it is a **good practice to wrap the validation inner loop with this context manager to disable any gradient calculation** that you may inadvertently trigger — **gradients belong in training**, not in validation steps;\n",
    "    \n",
    "* [**eval()**](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.eval): the only thing it does is **setting the model to evaluation mode** (just like its `train()` counterpart did), so the model can adjust its behavior regarding some operations, like [**Dropout**](https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout).\n",
    "\n",
    "Now, our training loop should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4bgvUJLI-Z_D",
    "outputId": "492f2366-293c-423a-b130-e59f0fda8a1c"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# builds tensors from numpy arrays BEFORE split\n",
    "x_tensor = torch.from_numpy(x).float()\n",
    "y_tensor = torch.from_numpy(y).float()\n",
    "\n",
    "# builds dataset containing ALL data points\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "# performs the split\n",
    "train_dataset, val_dataset = random_split(dataset, [80, 20])\n",
    "\n",
    "# builds a loader of each set\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=20)\n",
    "\n",
    "# defines learning rate\n",
    "lr = 1e-1\n",
    "\n",
    "# Create a MODEL, a LOSS FUNCTION and an OPTIMIZER\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Creates the train_step function for our model, loss function and optimizer\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "n_epochs = 1000\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Looping through epochs...\n",
    "for epoch in range(n_epochs):\n",
    "    # TRAINING\n",
    "    batch_losses = []\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        batch_losses.append(loss)\n",
    "\n",
    "    losses.append(np.mean(batch_losses))\n",
    "        \n",
    "    # VALIDATION\n",
    "    # no gradients in validation!\n",
    "    with torch.no_grad():\n",
    "        val_batch_losses = []\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            \n",
    "            # sets model to EVAL mode\n",
    "            model.eval()\n",
    "\n",
    "            # make predictions\n",
    "            yhat = model(x_val)\n",
    "            val_loss = loss_fn(yhat, y_val)\n",
    "            val_batch_losses.append(val_loss.item())\n",
    "\n",
    "        val_losses.append(np.mean(val_batch_losses))\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "colab_type": "code",
    "id": "HQsR61egsV5P",
    "outputId": "fcbd0e9a-de35-4dd2-ba27-24043b085c49"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rmmux1GVseDH"
   },
   "source": [
    "\"*Wait, there is something weird with this plot...*\", you say. You're right, the **validation loss** is **smaller** than the **training loss**. Shouldn't it be the other way around?! Well, generally speaking, *YES*, it should... but you can learn more about situations where this *swap* happens at this great [post](pyimg.co/kku35)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YW8xqQSifxv9"
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wbeACyd5mYbP"
   },
   "source": [
    "The training loop should be a stable structure, so we can organize it into functions as well...\n",
    "Let's build a function for **validation** and another one for the **training loop** itself, training step and all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i2ITAYVZf69n"
   },
   "outputs": [],
   "source": [
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    # Builds function that performs a step in the train loop\n",
    "    def train_step(x, y):\n",
    "        # Sets model to TRAIN mode\n",
    "        model.train()\n",
    "        # Step 1: Makes predictions\n",
    "        yhat = model(x)\n",
    "        # Step 2: Computes loss\n",
    "        loss = loss_fn(yhat, y)\n",
    "        # Step 3: Computes gradients\n",
    "        loss.backward()\n",
    "        # Step 4: Updates parameters and zeroes gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "    \n",
    "    # Returns the function that will be called inside the train loop\n",
    "    return train_step\n",
    "\n",
    "def validation(model, loss_fn, val_loader):\n",
    "    # Figures device from where the model parameters (hence, the model) are\n",
    "    device = next(model.parameters()).device.type\n",
    "\n",
    "    # no gradients in validation!\n",
    "    with torch.no_grad():\n",
    "        val_batch_losses = []\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            \n",
    "            # sets model to EVAL mode\n",
    "            model.eval()\n",
    "\n",
    "            # make predictions\n",
    "            yhat = model(x_val)\n",
    "            val_loss = loss_fn(yhat, y_val)\n",
    "            val_batch_losses.append(val_loss.item())\n",
    "\n",
    "        val_losses = np.mean(val_batch_losses)\n",
    "\n",
    "    return val_losses\n",
    "\n",
    "\n",
    "def train_loop(model, loss_fn, optimizer, n_epochs, train_loader, val_loader=None):\n",
    "    # Figures device from where the model parameters (hence, the model) are\n",
    "    device = next(model.parameters()).device.type\n",
    "    # Creates the train_step function for our model, loss function and optimizer\n",
    "    train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # TRAINING\n",
    "        batch_losses = []\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            loss = train_step(x_batch, y_batch)\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "        losses.append(np.mean(batch_losses))\n",
    "\n",
    "        # VALIDATION\n",
    "        if val_loader is not None:\n",
    "            val_loss = validation(model, loss_fn, val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "        print(\"Epoch {} complete...\".format(epoch))\n",
    "\n",
    "    return losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6LFO4qDTtD43"
   },
   "source": [
    "## Final Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ez-hqXKrs-yC"
   },
   "source": [
    "We finally have an organized version of our code, consisting of the following steps:\n",
    "- building a **Dataset**\n",
    "- performing a **random split** into **train** and **validation** datasets\n",
    "- building **DataLoaders**\n",
    "- building a **model**\n",
    "- defining a **loss function**\n",
    "- specifying a **learning rate**\n",
    "- defining an **optimizer**\n",
    "- specifying the **number of epochs**\n",
    "\n",
    "All nitty-gritty details of performing the actual training is encapsulated inside the **`train_loop`** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ztnViVdOiNUS",
    "outputId": "f0676564-7a80-4e17-e001-ce55b29ccd10"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# builds tensors from numpy arrays BEFORE split\n",
    "x_tensor = torch.from_numpy(x).float()\n",
    "y_tensor = torch.from_numpy(y).float()\n",
    "\n",
    "# builds dataset containing ALL data points\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "# performs the split\n",
    "train_dataset, val_dataset = random_split(dataset, [80, 20])\n",
    "\n",
    "# builds a loader of each set\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=20)\n",
    "\n",
    "# defines learning rate\n",
    "lr = 1e-1\n",
    "\n",
    "# Create a MODEL, a LOSS FUNCTION and an OPTIMIZER\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "n_epochs = 1000\n",
    "\n",
    "losses, val_losses = train_loop(model, loss_fn, optimizer, n_epochs, train_loader, val_loader)\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "colab_type": "code",
    "id": "YiAyyGoPEp1l",
    "outputId": "2d823dca-e37a-4e21-e752-48b543f2b6e2"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ysxlX3K69G9G"
   },
   "source": [
    "## Saving (and Loading) Models: taking a break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6j9bvL8K94JF"
   },
   "source": [
    "<h2><b><i>\"That would be great, to restart training later\"</b></i></h2>\n",
    "\n",
    "So, it is important to be able to **checkpoint** our model, in case we'd like to **restart training later**.\n",
    "\n",
    "To checkpoint a model, we basically have to **save its state** into a file, to **load** it back later - nothing special, actually.\n",
    "\n",
    "What defines the **state of a model**?\n",
    "- **model.state_dict()**: kinda obvious, right?\n",
    "- **optimizer.state_dict()**: remember optimizers had the `state_dict` as well?\n",
    "- **loss**: after all, you should keep track of its evolution\n",
    "- **epoch**: it is just a number, so why not? :-)\n",
    "- **anything else you'd like to have restored**\n",
    "\n",
    "Then, **wrap everything into a Python dictionary** and use [**torch.save()**](https://pytorch.org/docs/stable/torch.html?highlight=save#torch.save) to dump it all into a file! Easy peasy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8gbfTwXn97m9"
   },
   "outputs": [],
   "source": [
    "checkpoint = {'epoch': n_epochs,\n",
    "              'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'loss': losses,\n",
    "              'val_loss': val_losses}\n",
    "\n",
    "torch.save(checkpoint, 'model_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pan0KRCt9_4s"
   },
   "source": [
    "How would you **load** it back? Easy as well:\n",
    "- load the dictionary back using [**torch.load()**](https://pytorch.org/docs/stable/torch.html?highlight=torch%20load#torch.load)\n",
    "- load **model** and **optimizer** state dictionaries back using its methods [**load_state_dict()**](https://pytorch.org/docs/stable/nn.html?highlight=load_state_dict#torch.nn.Module.load_state_dict)\n",
    "- load everything else into their corresponding variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LWdi-mOf9_K7"
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('model_checkpoint.pth')\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "epoch = checkpoint['epoch']\n",
    "losses = checkpoint['loss']\n",
    "val_losses = checkpoint['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AvwaHaAP-Mgk"
   },
   "source": [
    "You may save a model for **checkpointing**, like we have just done, or for **making predictions**, assuming training is finished.\n",
    "\n",
    "After loading the model, **DO NOT FORGET**:\n",
    "\n",
    "---\n",
    "\n",
    "**SET THE MODE** (not the mood!):\n",
    "- **checkpointing: model.train()**\n",
    "- **predicting: model.eval()**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s1JGN8FIgKSE"
   },
   "source": [
    "## BONUS: Further Improvements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZXy6L02A_idH"
   },
   "source": [
    "Is there **anything else** we can improve or change? Sure, there is **always something else** to add to your model — using a [**learning rate scheduler**](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate), for instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the \"Playing with the Learning Rate\" section, we observed how different **learning rates** may be more useful at different moments of the optimization process.\n",
    "\n",
    "PyTorch offers a long list of **learning rate schedulers** for all your learning rate needs:\n",
    "- [**StepLR**](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.StepLR)\n",
    "- [**MultiStepLR**](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.MultiStepLR)\n",
    "- [**ReduceLROnPlateau**](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.ReduceLROnPlateau)\n",
    "- [**LambdaLR**](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.LambdaLR)\n",
    "- [**ExponentialLR**](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.ExponentialLR)\n",
    "- [**CosineAnnealingLR**](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.CosineAnnealingLR)\n",
    "- [**CyclicLR**](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.CyclicLR)\n",
    "- [**OneCycleLR**](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.OneCycleLR)\n",
    "- [**CosineAnnealingWarmRestarts**](https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts)\n",
    "\n",
    "To include a scheduler into our workflow, we need to take two steps:\n",
    "- create a **scheduler** and pass our **optimizer as argument**\n",
    "- use our scheduler's **step()** method\n",
    "    - **after the validation**, that is, **last thing before finishing an epoch**, for the first 6 schedulers on the list\n",
    "    - **after every batch update** for the last 3 schedulers on the list\n",
    "    \n",
    "We also need to **pass an argument** to **step()** if we're using **ReduceLROnPlateau**: the **validation loss**, which is the quantity we're using to **control the effectiveness of the current learning rate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, MultiStepLR\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "#scheduler = StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "#scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are focusing only on **ReduceLROnPlateau**, **StepLR** and **MultiStepLR** on this tutorial, so we'll change our training loop accordingly: adding the **scheduler's step()** as **last thing before finishing an epoch**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_with_scheduler(model, loss_fn, optimizer, scheduler, n_epochs, train_loader, val_loader=None):\n",
    "    # Figures device from where the model parameters (hence, the model) are\n",
    "    device = next(model.parameters()).device.type\n",
    "    # Creates the train_step function for our model, loss function and optimizer\n",
    "    train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    learning_rates = []\n",
    "\n",
    "    for epoch in range(n_epochs):        \n",
    "        # TRAINING\n",
    "        batch_losses = []\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            loss = train_step(x_batch, y_batch)\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "        losses.append(np.mean(batch_losses))\n",
    "\n",
    "        # VALIDATION\n",
    "        if val_loader is not None:\n",
    "            val_loss = validation(model, loss_fn, val_loader)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "        print(\"Epoch {} complete...\".format(epoch))\n",
    "\n",
    "        # SCHEDULER\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "            \n",
    "        learning_rates.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "        \n",
    "    return losses, val_losses, learning_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the whole thing once again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# builds tensors from numpy arrays BEFORE split\n",
    "x_tensor = torch.from_numpy(x).float()\n",
    "y_tensor = torch.from_numpy(y).float()\n",
    "\n",
    "# builds dataset containing ALL data points\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "# performs the split\n",
    "train_dataset, val_dataset = random_split(dataset, [80, 20])\n",
    "\n",
    "# builds a loader of each set\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=20)\n",
    "\n",
    "# defines learning rate\n",
    "lr = 1e-1\n",
    "\n",
    "# Create a MODEL, a LOSS FUNCTION and an OPTIMIZER (and SCHEDULER)\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "#scheduler = StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "#scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
    "\n",
    "n_epochs = 1000\n",
    "\n",
    "losses, val_losses, l_rates = train_loop_with_scheduler(model, loss_fn, optimizer, scheduler, n_epochs, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(l_rates)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Learning Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the learning rate is progressively reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7v_kLDpO_1kn"
   },
   "source": [
    "## Final Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_6_ZPJMa_5DI"
   },
   "source": [
    "I believe this tutorial has **most of the necessary steps** one needs go to trough in order to **learn**, in a **structured** and **incremental** way, how to **develop Deep Learning models using PyTorch**.\n",
    "\n",
    "Hopefully, after finishing working through all code in this post, you’ll be able to better appreciate and more easily work your way through PyTorch’s official [tutorials](https://pytorch.org/tutorials/).\n",
    "\n",
    "If you have any thoughts, comments or questions, please leave a comment below or contact me on [LinkedIn](https://br.linkedin.com/in/dvgodoy) or [Twitter](https://twitter.com/dvgodoy)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PyTorch 101.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
