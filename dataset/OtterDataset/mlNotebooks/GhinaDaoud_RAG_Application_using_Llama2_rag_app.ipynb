{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import dotenv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model and embedding to be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "MODEL = \"llama2\"\n",
    "model = Ollama(model=MODEL)\n",
    "embeddings = OllamaEmbeddings(model=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nArtificial intelligence (AI) is a branch of computer science that focuses on creating intelligent machines capable of performing tasks that typically require human intelligence, such as learning, problem-solving, decision-making, and perception. AI systems use algorithms and statistical models to analyze data, learn from it, and make decisions or take actions based on that analysis.\\n\\nThere are several types of AI, including:\\n\\n1. Narrow or weak AI: This type of AI is designed to perform a specific task, such as image recognition, speech recognition, or language translation. Examples include Siri, Alexa, and Google Assistant.\\n2. General or strong AI: This type of AI is designed to perform any intellectual task that a human can do. It is still in the realm of science fiction, but researchers are working towards developing a general AI that can learn, reason, and make decisions like a human.\\n3. Superintelligence: This type of AI is significantly more intelligent than the best human minds. It could potentially solve complex problems that are unsolvable for humans alone, such as curing diseases or solving global climate change. However, it also raises concerns about safety and control.\\n4. Artificial general intelligence (AGI): This type of AI is a hypothetical AI that possesses the ability to understand, learn, and apply knowledge across a wide range of tasks, similar to human intelligence.\\n5. Cognitive computing: This type of AI uses machine learning and natural language processing to analyze and understand human emotions, preferences, and behaviors. Examples include virtual assistants and sentiment analysis tools.\\n6. Robotic process automation (RPA): This type of AI uses software bots to automate repetitive, rule-based tasks, freeing up human time for more strategic work.\\n7. Machine learning: This is a subset of AI that involves training algorithms to learn from data and improve their performance over time. Examples include image recognition, natural language processing, and predictive analytics.\\n8. Natural language processing (NLP): This type of AI enables computers to understand, interpret, and generate human-like language, allowing for applications such as chatbots, voice assistants, and language translation tools.\\n9. Computer vision: This type of AI enables computers to interpret and analyze visual data from the world around us, such as images and videos. Examples include self-driving cars, facial recognition systems, and medical imaging analysis tools.\\n10. Neural networks: These are a type of machine learning algorithm inspired by the structure and function of the human brain. They can be used for a variety of applications, including image recognition, natural language processing, and predictive analytics.\\n\\nThese are just a few examples of the many types of AI that exist or are being developed. As AI technology continues to advance, we can expect to see more innovative applications and subfields emerge in the future.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the model\n",
    "model.invoke(\"briefly, what's AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nArtificial intelligence (AI) is:\\n\\n1. A machine that can think and learn like a human.\\n2. A technology that enables computers to perform tasks that typically require human intelligence, such as understanding natural language or recognizing images.\\n3. A system that can process and analyze vast amounts of data quickly and accurately.\\n4. A tool that can help automate repetitive tasks and make decisions based on patterns and insights.\\n5. A revolutionary technology that is transforming industries and changing the way we live and work.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a parser to convert the model output to a string\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# chain the model with output parser\n",
    "chain = model | parser\n",
    "chain.invoke(\"in short sentences, what's AI?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a prompt template to answer questions based on context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nAnswer the question based on the context given below, \\nif you don\\'t know the answer, just say \"I don\\'t know\".\\n\\nContext: Here is some context\\n\\nquestion: Here is the question\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\" \n",
    "Answer the question based on the context given below, \n",
    "if you don't know the answer, just say \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "question: {question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt.format(\n",
    "    context = \"Here is some context\",\n",
    "    question = \"Here is the question\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a chain that combines the prompt, model and output parser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure! Based on the context provided, the types of machine learning are:\\n\\n1. Supervised learning\\n2. Unsupervised learning\\n3. Reinforcement learning'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | parser\n",
    "chain.invoke({\n",
    "    \"context\": \"There are three types of machine learning: supervised, unsupervised, and reinforcement learning.\",\n",
    "    \"question\": \"What are the types of machine learning?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    page_content: str\n",
    "    metadata: dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load PDF files and convert them into Document objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Chapter 1  I  Introducing deep learning 4\\nWhy you should learn deep learning\\nIt’s a powerful tool for the incremental automation of intelligence.\\nFrom the beginning of time, humans have been building better and better tools to \\nunderstand and control the environment around us. Deep learning is today’s chapter in this story of innovation. \\nPerhaps what makes this chapter so compelling is that this field is more of a mental  \\ninnovation than a mechanical one . Much like its sister fields in machine learning, deep \\nlearning seeks to automate intelligence  bit by bit. In the past few years, it has achieved \\nenormous success and progress in this endeavor, exceeding previous records in computer \\nvision, speech recognition, machine translation, and many other tasks. \\nThis is particularly extraordinary given that deep learning seems to use largely the same \\nbrain-inspired algorithm (neural networks) for achieving these accomplishments across a vast number of fields. Even though deep learning is still an actively developing field with many challenges, recent developments have lead to tremendous excitement: perhaps we’ve discovered not just a great tool, but a window into our own minds.\\nDeep learning has the potential for significant automation  \\nof skilled labor.\\nThere’s a substantial amount of hype around the potential impacts of deep learning if \\nthe current trend of progress is extrapolated at varying speeds. Although many of these predictions are overzealous, I believe one merits your consideration: job displacement. I think this claim stands out from the rest because even if deep learning’s innovations stopped today , there would already be an incredible impact on skilled labor around the globe. Call- \\ncenter operators, taxi drivers, and low-level business analysts are compelling examples where deep learning can provide a low-cost alternative. \\nFortunately, the economy doesn’t turn on a dime; but in many ways we’re already past \\nthe point of concern, given the current power of the technology. It’s my hope that you (and people you know) will be enabled by this book to transition from perhaps one of the industries facing disruption into an industry ripe with growth and prosperity: deep learning.\\nIt’s fun and creative. Y ou’ll discover much about what it is to be \\nhuman by trying to simulate intelligence and creativity.\\nPersonally, I got into deep learning because it’s fascinating. It’s an amazing intersection \\nbetween human and machine. Unpacking exactly what it means to think, to reason, and to create is enlightening, engaging, and, for me, inspiring. Consider having a dataset filled with every painting ever painted, and then using that to teach a machine how to paint like Monet. Insanely, it’s possible, and it’s mind-bogglingly cool to see how it works.', metadata={'source': 'dl_pdf_files/chapter1.pdf', 'page': 0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load PDF using pypdf into list of documents.\n",
    "# Loader chunks by page and stores page numbers in metadata.\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "directory_path = \"dl_pdf_files/\"\n",
    "loaders = []\n",
    "pages = []\n",
    "files = os.listdir(directory_path)\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for file in files:\n",
    "    # Only process PDF files\n",
    "    if file.endswith(\".pdf\"):\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        loaders.append(loader)\n",
    "        # Load and split the PDF into pages\n",
    "        raw_pages = loader.load_and_split()\n",
    "        for raw_page in raw_pages:\n",
    "            # Convert raw page to Document\n",
    "            document = Document(page_content=raw_page.page_content, metadata=raw_page.metadata)\n",
    "            pages.append(document)\n",
    "\n",
    "pages[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a vector store from the loaded documents using the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pydantic\\_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(pages, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'dl_pdf_files/chapter4.pdf', 'page': 1}, page_content='Chapter 4  I  Introduction to neural learning 48\\nPredict, compare, and learn\\nIn chapter 3, you learned about the paradigm “predict, compare, learn, ” and we dove \\ndeep into the first step: predict . In the process, you learned a myriad of things, including \\nthe major parts of neural networks (nodes and weights), how datasets fit into networks (matching the number of datapoints coming in at one time), and how to use a neural network to make a prediction. \\nPerhaps this process begged the question, “How do we set weight values so the network \\npredicts accurately?” Answering this question is the main focus of this chapter, as we cover the next two steps of the paradigm: compare  and learn .\\nCompare\\nComparing gives a measurement of how much a prediction \\n“missed” by.\\nOnce you’ve made a prediction, the next step is to evaluate how well you did. This may \\nseem like a simple concept, but you’ll find that coming up with a good way to measure error is one of the most important and complicated subjects of deep learning.\\nThere are many properties of measuring error that you’ve likely been doing your whole \\nlife without realizing it. Perhaps you (or someone you know) amplify bigger errors while ignoring very small ones. In this chapter, you’ll learn how to mathematically teach a network to do this. Y ou’ll also learn that error is always positive! We’ll consider the analogy of an archer hitting a target: whether the shot is too low by an inch or too high by an inch, the error is still just 1 inch. In the neural network compare  step, you need to consider these \\nkinds of properties when measuring error.\\nAs a heads-up, in this chapter we evaluate only one simple way of measuring error: mean \\nsquared error . It’s but one of many ways to evaluate the accuracy of a neural network.\\nThis step will give you a sense for how much you missed, but that isn’t enough to be able to learn. The output of the compare  logic is a “hot or cold” type signal. Given some prediction, \\nyou’ll calculate an error measure that says either “a lot” or “a little. ” It won’t tell you why you missed, what direction you missed, or what you should do to fix the error. It more or less says “big miss, ” “little miss, ” or “perfect prediction. ” What to do about the error is captured in the next step, learn .'),\n",
       " Document(metadata={'source': 'dl_pdf_files/chapter1.pdf', 'page': 3}, page_content='What you need to get started 7\\nWhat you need to get started\\nInstall Jupyter Notebook and the NumPy Python library.\\nMy absolute favorite place to work is in Jupyter Notebook. One of the most important parts \\nof learning deep learning (for me) is the ability to stop a network while it’s training and tear apart absolutely every piece to see what it looks like. This is something Jupyter Notebook is incredibly useful for. \\nAs for NumPy, perhaps the most compelling case for why this book leaves nothing out is \\nthat we’ll be using only a single matrix library. In this way, you’ll understand how  everything \\nworks, not just how to call a framework. This book teaches deep learning from absolute scratch, soup to nuts. \\nInstallation instructions for these two tools can be found at http://jupyter.org for Jupyter  \\nand http://numpy.org for NumPy. I’ll build the examples in Python 2.7, but I’ve tested them for Python 3 as well. For easy installation, I also recommend the Anaconda framework: https://docs.continuum.io/anaconda/install .\\nPass high school mathematics.\\nSome mathematical assumptions are out of depth for this book, but my goal is to teach deep learning assuming that you understand only basic algebra.\\nFind a personal problem you’re interested in.\\nThis might seem like an optional “need” to get started. I guess it could be, but seriously, I highly, highly recommend finding one. Everyone I know who has become successful at this stuff had some sort of problem they were trying to solve. Learning deep learning was just a “dependency” to solving some other interesting task. \\nFor me, it was using Twitter to predict the stock market. It’s just something I thought was \\nreally fascinating. It’s what drove me to sit down and read the next chapter and build the next prototype. \\nAnd as it turns out, this field is so new , and is changing so fast , that if you spend the next \\ncouple of years chasing one project with these tools, you’ll find yourself becoming one of \\nthe leading experts in that particular problem  faster than you might think. For me, chasing \\nthis idea took me from barely knowing anything about programming to a research grant at a hedge fund applying what I learned, in around 18 months! For deep learning, having a problem you’re fascinated with that involves using one dataset to predict another is the key catalyst! Go find one!'),\n",
       " Document(metadata={'source': 'dl_pdf_files/chapter4.pdf', 'page': 0}, page_content='47\\nIn this chapter\\n• Do neural networks make accurate predictions?\\n• Why measure error?\\n• Hot and cold learning\\n• Calculating both direction and amount from error\\n• Gradient descent\\n• Learning is just reducing error\\n• Derivatives and how to use them to learn\\n• Divergence and alphaintroduction to neural learning:\\ngradient descent 4\\nThe only relevant test of the validity of a hypothesis is \\ncomparison of its predictions with experience.\\n— Milton Friedman, Essays in P ositive Economics   \\n(University of Chicago Press, 1953)'),\n",
       " Document(metadata={'source': 'dl_pdf_files/chapter4.pdf', 'page': 26}, page_content='Visualizing the overcorrections 73\\nVisualizing the overcorrections\\nerror = 0.04\\nweight = 0.5b\\n2.0 .04.5\\n0.2\\nweight_delta = -0.28\\n(Raw error modified for scaling, \\nnegative reversal, and stopping per this weight and input)0.41.0delta  (raw error)A big weight increase \\nerror = 0.36\\nweight = 0.1\\n2.0 .36.1\\n–.6–1.20.2cOvershot a bit; let’s go back the other way. \\nerror = 3.24\\nweight = 1.3\\n2.0 3.241.3\\n1.83.62.6dOvershot again! Let’s go back, but only a  little.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the vectorstore to a retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "retriever.invoke(\"What is a neural network?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a chain to process questions using context retrieval and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt | model | parser \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the purpose of the book?\n",
      "Answer:  Based on the given context, the purpose of the book appears to be teaching deep learning from scratch, assuming that the reader has a basic understanding of algebra. The author highlights the importance of finding a personal problem to work on and using deep learning to solve it. The book covers installation of Jupyter Notebook and NumPy, as well as mathematical assumptions and how to measure error in the \"compare\" step of the paradigm \"predict, compare, learn\".\n",
      "\n",
      "Question: How many chapters there is?\n",
      "Answer: Based on the provided PDF documents, there are 4 chapters in total.\n",
      "\n",
      "Chapter 1: Introduction to neural learning\n",
      "Chapter 2: Predict, compare, and learn\n",
      "Chapter 3: Predict with a neural network\n",
      "Chapter 4: Introduction to neural prediction: forward propagation\n",
      "\n",
      "Question: How long it will take to read the whole book?\n",
      "Answer: I don't know how long it will take to read the whole book. The length of time required to read a book depends on various factors such as reading speed, comprehension level, and individual reading habits. However, based on the page count provided in the question, the book appears to be around 200-250 pages long, which would typically take around 6-8 hours to read, depending on your reading pace.\n",
      "\n",
      "Question: How to check if the network is making good predictions?\n",
      "Answer: I don't know.\n",
      "\n",
      "Question: What's the difference between generalization and overfitting?\n",
      "Answer: I don't know.\n",
      "\n",
      "Question: What is a deep learning framework?\n",
      "Answer:  Based on the given context, a deep learning framework is Anaconda.\n",
      "\n",
      "Question: At the end, give me a brief guide please.\n",
      "Answer: Sure! Here's the answer to your question based on the provided context:\n",
      "\n",
      "In this case, the weight update formula is `weight = weight - (input * (pred - goal_pred))`. When the input is large, the weight update can be significant even when the error is small. This can cause the network to overcorrect, leading to divergence. To avoid divergence, the author suggests making the input smaller to reduce the sensitivity of the prediction to changes in the weight.\n",
      "\n",
      "In summary, the key takeaway from this section is that when dealing with large inputs, it's essential to be mindful of the weight update formula and how it can cause overcorrection, leading to divergence. To avoid this, reducing the input size can help mitigate the issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is the purpose of the book?\",\n",
    "    \"How many chapters there is?\",\n",
    "    \"How long it will take to read the whole book?\",\n",
    "    \"How to check if the network is making good predictions?\",\n",
    "    \"What's the difference between generalization and overfitting?\",\n",
    "    \"What is a deep learning framework?\",\n",
    "    \"At the end, give me a brief guide please.\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {chain.invoke({'question': question})}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Based on the given context, the purpose of the book appears to be teaching deep learning from scratch, assuming the reader has a basic understanding of algebra. The author wants the reader to find a personal problem they are interested in and use deep learning to solve it. The book covers installation of Jupyter Notebook and NumPy, and assumes the reader will have a basic understanding of mathematical concepts. The author also mentions that having a problem you're fascinated with that involves using one dataset to predict another is key to becoming an expert in the field.\",\n",
       " 'Based on the provided PDF documents, there are 4 chapters in the book.',\n",
       " \"I don't know exactly how long it will take to read the whole book based on the provided context, as it depends on various factors such as reading speed, comprehension level, and personal interest in the subject matter. However, based on the length of the book and the amount of information presented, it could take anywhere from a few days to a week or more to finish reading the whole book.\",\n",
       " ' Based on the content provided in the chapter, there are several ways to evaluate if a neural network is making good predictions:\\n\\n1. Mean Squared Error (MSE): This is one of the simplest ways to measure the accuracy of a neural network. The MSE measures the average squared difference between the predicted and actual values.\\n2. Hot and Cold Learning: This approach evaluates the accuracy of the network by comparing the predicted output to the actual output. If the predicted output is close to the actual output, the network is said to be \"hot,\" otherwise it is \"cold.\"\\n3. Calculating both direction and amount from error: This approach involves calculating the direction and amount of error between the predicted and actual values. The direction tells you the direction in which the network needs to improve, while the amount gives you an idea of how much improvement is needed.\\n4. Gradient Descent: This is an optimization algorithm used to learn the parameters of a neural network. By minimizing the error between the predicted and actual values, the network learns to make better predictions over time.\\n5. Learning is just reducing error: This statement suggests that the main goal of a neural network is to reduce the error between the predicted and actual values. Therefore, if the network is making good predictions, it means that it has learned well.\\n\\nBased on these methods, you can check if the network is making good predictions by evaluating the MSE, hot and cold learning, calculating both direction and amount from error, gradient descent, and learning is just reducing error. However, if you don\\'t know the answer to this question, please let me know and I will do my best to help.',\n",
       " \"I don't know.\",\n",
       " ' Based on the context provided, a deep learning framework is a software library or toolkit that provides tools and resources for building and training deep neural networks. The two frameworks mentioned in the text are Anaconda and Jupyter Notebook.',\n",
       " \"I don't know the answer to your question. However, based on the context provided, I can tell you that the passage discusses the concept of learning in deep neural networks and how it is related to the error attribution problem. The author explains that learning tells each weight how it can change to reduce the error, and that learning is all about error attribution, or figuring out how each weight played its part in creating the error. The passage also touches upon the topic of overfitting, which is the greatest weakness of deep learning, and how it can be avoided by convincing the neural network to generalize instead of memorizing.\\n\\nAs for a brief guide, here are some key takeaways from the passage:\\n\\n1. Learning tells each weight how it can change to reduce the error.\\n2. Learning is all about error attribution, or figuring out how each weight played its part in creating the error.\\n3. Overfitting is the greatest weakness of deep learning, and it occurs when a particular configuration of weights accidentally creates perfect correlation between the prediction and the output dataset without giving the heaviest weight to the best inputs.\\n4. To avoid overfitting, it's important to convince the neural network to generalize instead of memorizing.\\n5. The dot product operation is an important part of truly understanding how neural networks make predictions, and it can cause the network to overcorrect if the input is sufficiently large.\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process questions in batch\n",
    "chain.batch([{ \"question\": question } for question in questions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the provided context, the purpose of the book appears to be teaching deep learning from scratch, assuming the reader has a basic understanding of algebra. The author emphasizes the importance of finding a personal problem to work on, as it can help drive interest and motivation in the subject. The book covers installation of Jupyter Notebook and NumPy, as well as mathematical assumptions and concepts. Additionally, the author discusses the importance of measuring error and learning from it, using examples from real-life situations. Overall, the purpose of the book seems to be providing a comprehensive introduction to deep learning for beginners."
     ]
    }
   ],
   "source": [
    "# Stream the answer to a specific question\n",
    "for s in chain.stream({\"question\": \"What is the purpose of the book?\"}):\n",
    "    print(s, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
