{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [ (\"big data\", 100, 15), (\"Hadoop\", 95, 25), (\"Python\", 75, 50),  \n",
    "        (\"R\", 50, 40), (\"machine learning\", 80, 20), (\"statistics\", 20, 60),   \n",
    "        (\"data science\", 60, 70), (\"analytics\", 90, 3),      \n",
    "        (\"team player\", 85, 85), (\"dynamic\", 2, 90), (\"synergies\", 70, 0),   \n",
    "        (\"actionable insights\", 40, 30), (\"think out of the box\", 45, 10),   \n",
    "        (\"self-starter\", 30, 50), (\"customer focus\", 65, 15),     \n",
    "        (\"thought leadership\", 35, 35)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get data\n",
    "\n",
    "def fix_unicode(text):\n",
    "    return text.replace(u\"\\u2019\", \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "url = \"http://radar.oreilly.com/2010/06/what-is-data-science.html\"\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content = soup.find(\"div\", \"a-body\")\n",
    "regex = r\"[\\w']+|[\\.]\"\n",
    "\n",
    "document = []\n",
    "\n",
    "for paragraph in content('p'):\n",
    "    words = re.findall(regex, fix_unicode(paragraph.text))\n",
    "    document.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigrams = zip(document, document[1:])\n",
    "transitions = defaultdict(list)\n",
    "for prev,current in bigrams:\n",
    "    transitions[prev].append(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_using_bigrams():\n",
    "    current = \".\"\n",
    "    result = []\n",
    "    while True:\n",
    "        next_word_candidates = transitions[current] # all bigrams for current\n",
    "        current = random.choice(next_word_candidates) # pick one\n",
    "        result.append(current)\n",
    "        if current == '.':\n",
    "            return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"There's a problem that you'd otherwise have to the next sexy job data platform though not be data to Mike Driscoll dataspora statistics is a solution .\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_using_bigrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigrams = zip(document, document[1:], document[2:])\n",
    "trigram_transitions = defaultdict(list)\n",
    "starts = []\n",
    "\n",
    "for prev, current, next in trigrams:\n",
    "    if prev == \".\":\n",
    "        starts.append(current)\n",
    "        \n",
    "    trigram_transitions[(prev, current)].append(next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_using_trigrams():\n",
    "    current = random.choice(starts)\n",
    "    prev = \".\"\n",
    "    result = [current]\n",
    "    \n",
    "    while True:\n",
    "        next_word_candidates = trigram_transitions[(prev, current)]\n",
    "        next_word = random.choice(next_word_candidates)\n",
    "        \n",
    "        prev, current = current, next_word\n",
    "        result.append(current)\n",
    "        \n",
    "        if current == \".\":\n",
    "            return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"The part of the earlier data products that are easily described you can do something with it and that's where Moore's Law applied to data conditioning to drawing conclusions .\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_using_trigrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grammar = {\n",
    "    \"_S\" : [\"_NP _VP\"],\n",
    "    \"_NP\" : [\"_N\",\n",
    "            \"_A _NP _P _A _N\"],\n",
    "    \"_VP\" : [\"_V\",\n",
    "            \"_V _NP\"],\n",
    "    \"_N\" : [\"data science\", \"Python\", \"regression\"],\n",
    "    \"_A\" : [\"big\", \"linear\", \"logistic\"],\n",
    "    \"_P\" : [\"about\", \"near\"],\n",
    "    \"_V\" : [\"learns\", \"trains\", \"tests\", \"is\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_terminal(token):\n",
    "    return token[0] != \"_\"\n",
    "\n",
    "def expand(grammar, tokens):\n",
    "    for i, token in enumerate(tokens):\n",
    "        \n",
    "        # skip over terminals\n",
    "        if is_terminal(token):\n",
    "            continue\n",
    "            \n",
    "        # if non-terminal choose replacement at random\n",
    "        replacement = random.choice(grammar[token])\n",
    "        \n",
    "        if is_terminal(replacement):\n",
    "            tokens[i] = replacement\n",
    "        else:\n",
    "            tokens = tokens[:i] + replacement.split() + tokens[(i+1):]\n",
    "            \n",
    "        return expand(grammar, tokens)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def generate_sentence(grammar):\n",
    "    return expand(grammar, [\"_S\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['linear',\n",
       " 'linear',\n",
       " 'Python',\n",
       " 'near',\n",
       " 'logistic',\n",
       " 'regression',\n",
       " 'about',\n",
       " 'big',\n",
       " 'regression',\n",
       " 'trains']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gibbs Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def roll_a_die():\n",
    "    return random.choice([1,2,3,4,5,6])\n",
    "\n",
    "def direct_sample():\n",
    "    d1 = roll_a_die()\n",
    "    d2 = roll_a_die()\n",
    "    return d1, d1 + d2\n",
    "\n",
    "def random_y_given_x(x):\n",
    "    return x + roll_a_die()\n",
    "\n",
    "def random_x_given_y(y):\n",
    "    if y <= 7:\n",
    "        return random.randrange(1,y)\n",
    "    else:\n",
    "        return random.randrange(y - 6, 7)\n",
    "    \n",
    "def gibbs_sample(num_iters = 100):\n",
    "    x, y = 1, 2 # arbitrary\n",
    "    for _ in range(num_iters):\n",
    "        x = random_x_given_y(y)\n",
    "        y = random_y_given_x(x)\n",
    "    return x, y\n",
    "\n",
    "def compare_distributions(num_samples = 1000):\n",
    "    counts = defaultdict(lambda: [0,0])\n",
    "    for _ in range(num_samples):\n",
    "        counts[gibbs_sample()][0] += 1\n",
    "        counts[gibbs_sample()][1] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function <lambda> at 0x105125ed8>, {(5, 9): [31, 38], (6, 9): [19, 19], (1, 3): [27, 34], (4, 8): [30, 31], (5, 6): [28, 19], (2, 8): [23, 30], (4, 7): [23, 29], (1, 6): [30, 30], (3, 7): [25, 35], (2, 5): [34, 26], (5, 8): [25, 20], (1, 2): [28, 29], (4, 9): [28, 28], (6, 10): [33, 27], (1, 5): [28, 37], (3, 6): [36, 23], (4, 5): [27, 23], (4, 10): [29, 30], (2, 6): [26, 26], (5, 11): [21, 21], (6, 11): [27, 26], (1, 4): [29, 35], (6, 7): [29, 30], (3, 9): [29, 29], (2, 3): [32, 28], (6, 8): [21, 21], (6, 12): [30, 28], (3, 5): [21, 25], (2, 7): [27, 24], (5, 10): [33, 20], (4, 6): [25, 27], (5, 7): [29, 32], (3, 8): [33, 39], (1, 7): [37, 26], (3, 4): [28, 33], (2, 4): [19, 22]})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_distributions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "        [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "        [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "        [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "        [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "        [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "        [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "        [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "        [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "        [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "        [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "        [\"statistics\", \"R\", \"statsmodels\"],\n",
    "        [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "        [\"pandas\", \"R\", \"Python\"],\n",
    "        [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "        [\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# works by working backwards through the cumulative dist\n",
    "def sample_from(weights):\n",
    "    \"\"\"returns i with probability weights[i] / sum(weights)\"\"\"\n",
    "    total = sum(weights)\n",
    "    rnd = total*random.random() # uniform between 0 and total\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w # find the smallest i\n",
    "        if rnd <= 0: # s.t. weights[0] + ... + weights[i] >= rnd\n",
    "            return i\n",
    "        \n",
    "def p_topic_given_document(topic, d, alpha = 0.1):\n",
    "    \"\"\"fraction of words in document _d_\n",
    "    that are assigned to _topic_ (plus some bit)\"\"\"\n",
    "    \n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K*alpha))\n",
    "\n",
    "def p_word_given_topic(word, topic, beta = 0.1):\n",
    "    \"\"\"fraction of words assigned to _topic_\n",
    "    that equal _word_\"\"\"\n",
    "    \n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + W*beta))\n",
    "\n",
    "def topic_weight(d, word, k):\n",
    "    \"\"\"given a document and a word in that doc,\n",
    "    return the weight for the kth topic\"\"\"\n",
    "    \n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k)\n",
    "                        for k in range(K)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 4\n",
    "    \n",
    "# one counter per doc\n",
    "document_topic_counts = [Counter() for _ in documents]\n",
    "\n",
    "# one counter per topic\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "\n",
    "# list of numbers, one per topic\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "\n",
    "# list of numbers, one per doc\n",
    "document_lengths = map(len, documents)\n",
    "\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "W = len(distinct_words)\n",
    "\n",
    "D = len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "# start with random assignment\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                   for document in documents]\n",
    "\n",
    "# initialize the counts from the random assignment above\n",
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for iteration in range(1000):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(documents[d],\n",
    "                                              document_topics[d])):\n",
    "            \n",
    "            # remove this word / topic from the counts\n",
    "            # or else the weighting doens't matter\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "            \n",
    "            # choose a new topic based on the weights\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "            \n",
    "            # and add back to counts\n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 neural networks 3\n",
      "0 artificial intelligence 3\n",
      "0 machine learning 3\n",
      "0 Hadoop 2\n",
      "0 Mahout 2\n",
      "0 deep learning 2\n",
      "0 mathematics 2\n",
      "0 C++ 2\n",
      "0 decision trees 2\n",
      "0 Big Data 2\n",
      "0 probability 1\n",
      "0 regression 1\n",
      "1 R 5\n",
      "1 regression 4\n",
      "1 libsvm 4\n",
      "1 Big Data 4\n",
      "1 NoSQL 2\n",
      "1 Storm 2\n",
      "1 deep learning 2\n",
      "1 Spark 2\n",
      "1 numpy 2\n",
      "1 C++ 2\n",
      "1 HBase 2\n",
      "1 MongoDB 2\n",
      "1 scikit-learn 2\n",
      "1 Haskell 2\n",
      "1 artificial intelligence 1\n",
      "1 machine learning 1\n",
      "1 support vector machines 1\n",
      "2 Postgres 4\n",
      "2 probability 3\n",
      "2 Java 2\n",
      "2 Hadoop 2\n",
      "2 pandas 2\n",
      "2 theory 2\n",
      "2 MongoDB 2\n",
      "2 MapReduce 2\n",
      "2 MySQL 1\n",
      "2 Python 1\n",
      "2 R 1\n",
      "2 databases 1\n",
      "2 support vector machines 1\n",
      "2 Cassandra 1\n",
      "3 Python 7\n",
      "3 statistics 6\n",
      "3 Java 4\n",
      "3 HBase 4\n",
      "3 statsmodels 4\n",
      "3 Cassandra 3\n",
      "3 probability 2\n",
      "3 R 2\n",
      "3 programming languages 2\n",
      "3 pandas 2\n",
      "3 scipy 2\n",
      "3 scikit-learn 2\n",
      "3 neural networks 1\n",
      "3 MySQL 1\n",
      "3 databases 1\n"
     ]
    }
   ],
   "source": [
    "for k, word_counts in enumerate(topic_word_counts):\n",
    "    for word, count in word_counts.most_common():\n",
    "        if count > 0:\n",
    "            print k, word, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
