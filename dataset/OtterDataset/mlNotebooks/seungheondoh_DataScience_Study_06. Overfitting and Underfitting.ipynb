{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정규화\n",
    "\n",
    "정규화는 학습이 아니라 무언가 선험 지식을 모델에 넣어주는 행위에 가깝다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting과 Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "overfitting과 underfitting은 모형에 Just-right하지않은 모형의 복잡도 혹은 모형의 학습횟수에 따라서 결정이 된다.\n",
    "\n",
    "\n",
    "### 2 Case of Overfitting\n",
    "\n",
    "- 애초에 모형이 너무 복잡하게 설계 되었기 떄문에 발생한 것이다? -> Machine Learning, Deep Learning\n",
    "- Epoch가 증가에 따라서 판별함수가 너무 training set에 fit해졌다? -> Deep Learning\n",
    "- hideen neruons 과 layer에 수에 따라서 복잡해 지는것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Complexity?\n",
    "\n",
    "- 각 뉴런은 layer-by-layer 그리고 feed-forward 관계로 directed acyclic graph 형태입니다.\n",
    "- 각 뉴런들의 activation 과 output은 non-linear transform of weighted sum of inputs 입니다.\n",
    "- Model complexity는 데이터의 feautures 개수에 따라 결정이 됩니다.\n",
    "    - 파라미터 layer의 갯수 or Parameter의 갯수는 영향을 끼치지 않는가요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch?\n",
    "\n",
    "- Epoch\n",
    "한번의 forward pass와 한번의 backward pass가 모든 training examples에 대해서 진행되었다는 소리입니다.\n",
    "\n",
    "- batch size\n",
    "한번의 forward pass와 한번의 backward pass 사이에 사용된 data의 갯수를 사용합니다. 만약 batch size가 크다는 소리는, 더 많은 memory space가 필요하다는 소리입니다.\n",
    "\n",
    "- number of Iteration\n",
    "흔히 passes 숫자라고도 합니다. batchsize가 몇번 forward 와 backward를 반복했는지를 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How we detact Over & Underfitting\n",
    "\n",
    "전체 데이터 셋을 Test, Train, Validation set으로 나누어서 cross validation을 진행한다.\n",
    "\n",
    "- 3개의 데이터셋 분포를 파악해야한다. Sampling에 따른 모집단의 추종여부를 확인할 것!\n",
    "- High Bias : Underfitting, 모델의 퍼포먼스가 좋지않다.\n",
    "    - Sol : 모델의 복잡도를 높이거나 Epoch를 증가시킨다.\n",
    "- High Variance : Overfitting, Train set에 fit하다.\n",
    "    - Overfitting이 발생하는 곳에서 Early stopping을 한다.\n",
    "    - Sol : 데이터의 갯수를 늘리거나, Regulariazation을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![of1](img/06.of1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch 증가의 경우\n",
    "\n",
    "- Underfitting : Training Error, Test Error의 차이가 거의 적고, Error의 크기가 매우 크다. (High Bias)\n",
    "- Just right : Training error 는 작고, test error 역시 작다 매우 이상적인 상태이다. \n",
    "- Overfitting : Training error는 작고, test error 는 큰 상태이다. (High Variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![of2](img/06.of2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More about Error\n",
    "\n",
    "- Noise : 관측에서 오는 에러 (통제 불가), 확률프로세스이거나, 관측이 안된 무언가가 있거나\n",
    "- Bias : 타겟 함수($y$) - 추정함수($\\hat{y}$)의 평균의 차이이다.\n",
    "- Variance : 추정 평균과 추정값의 차이이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Weight & Overfitting\n",
    "\n",
    "- Large Weight 가 overfitting을 만든다.\n",
    "- Regularization을 해주자!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/06.RG.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How we solve Over & Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regluarlization\n",
    "\n",
    "Regularization는 학습이 아니라 무언가 선험 지식을 모델에 넣어주는 행위에 가깝다. \n",
    "\n",
    "Regularization은 일반화의 성능을 올려주는 방법입니다. 이는 우리가 일반적으로 Overfitting이 감지 되었을때 사용하는 방법입니다. 뉴럴넷의 weight값을 0에 가깝게 만들어주는 효과가 있습니다.\n",
    "\n",
    "$\\lambda$ 가 커지면 Activate function에 들어가는 input값이 0에 가까워지게 됩니다. 그러면서 activation함수의 linear한 영역을 통과하게 됩니다.\n",
    "\n",
    "- L2 norm\n",
    "    - 모든 Parameters의 영향력을 정규화합니다.\n",
    "    - 뉴럴넷에서는 일반적으로 Frobenius norm이라고 부릅니다.\n",
    "    - Weight decay의 효과도 있습니다.\n",
    "    - Map Prior을 Normal Distribution로 가정합니다.\n",
    "    \n",
    "- L1 norm\n",
    "    - 특정 Parameters의 영향력을 정규화합니다. (sparse solutions이 됩니다.)\n",
    "    - 모델을 압축하기 위한 목적이 없다면 딱히 사용하진 않습니다.\n",
    "    - 모델이 너무 복잡하다면 L1 norm을 사용합니다.\n",
    "    - Map Prior을 Laplace prior로 가정합니다.\n",
    "\n",
    "- L0 norm\n",
    "    - Test performance를 위해서 이다.\n",
    "    - 0이 아닌 파라미터의 갯수를 세줍니다. \n",
    "    - Sparsity를 직접적으로 measure해줍니다.\n",
    "    - 쉽게 최적화 되기 힘들기 때문에 많이 쓰이지 않습니다.\n",
    "- Louizos et al: L^{0} Regularization\n",
    "    - Discrete optimization을 Continuous하게 해봅시다.\n",
    "        - 베르누이에서 가우시안으로 보내준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directly approximately regulaizing complexity(DARC)\n",
    "Measure to Complexity를 위해서 도입된 계념은 Rademecher complexity\n",
    "\n",
    "우리가 뉴럴넷 아키텍쳐를 고정시킨다면, 모든 함수들은 파라미터에 의해서 정의 됩니다. Rademecher complexity은 뉴럴넷의 output을 정규화합니다. Weight를 정규화하는가 Output을 정규화 하는가의 차이입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "더 작은 뉴럴넷 모형을 상용해서, 정규화 효과를 줍니다. 또한 Dropout으로 인해서 각 노드들은 이제 어떠한 feature에 의존하는 경향을 줄어들게 됩니다. 때문에 가중치를 줄이게 되는 효과를 줍니다. 각 레이어들 마다 keep_prob 의 값을 바꾸어줄수 있다.\n",
    "\n",
    "feedforward pass에서, hidden layer unit들을 랜덤하게 끊어버린다. 이는 hidden unit들의 co-adaptiation을 방지한다. 이던 feature의 reduendency를 만들며, waste of resource가 발생한다.\n",
    "- Co-adaptation : 두개나 더 많은 hidden이 같은 feature를 학습하는 것\n",
    "\n",
    "또한 Dropout은 큰 ensemble 모델처럼 사용이 가능하다. 앙상블 모형은 모델은 variance를 줄여준다. Fully connected layer의 1024 유닛은 2^1024은 masks combination을 만든다. 그들은 파라미터를 공유한다.\n",
    "\n",
    "- Inverted dropout\n",
    "\n",
    "### Variational dropout (Kingma)\n",
    "Dropout rate를 학습한다면 어떨까? VD는 Re-parametrization trick을 사용해서,\n",
    "\n",
    "### Dropout in Convolution\n",
    "Convolutional layer의 목적은 translate invariance를 보존하는 것이다. 만약 Dropout을 사용하게 된다면 translate invariance가 깨지게 됩니다. 즉 CNN의 원래 목적을 달성하기 힘들어지게 됩니다. 그렇기 때문에 대부분의 논문에서 BatchNorm을 사용하게 됩니다.\n",
    "\n",
    "### Shake-Shake Regularization\n",
    "Residual block에서 Gating을 해주는 마지막 레이어를 도입합니다. 그다음 Forward, Backward, Test때의 Gating 파라미터를 다르게 학습하게 도와줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "problem은 zero mean과 unit variance에 있다.\n",
    "- input normalization\n",
    "    - Standadization : 평균으로 빼고, std로 나누어줍니다. 문제는 Std가 0이 되면 큰일납니다.\n",
    "    - PCA : orthogonal 한 latent axis로 projection 시켜줍니다. 일반적으로 뉴럴넷에서 학습해주기 때문에, reduendency를 만드는 경향이 있습니다.\n",
    "- Weight Initalization\n",
    "    - Gaussian random number\n",
    "        - 깁은 뉴럴넷 레이어의 경우에는, 문제가 생기는 경우가 많습니다.\n",
    "    - Xavier initalization\n",
    "        - hanh function에 잘 작동합니다.\n",
    "    - He initalization\n",
    "        - ReLU에 잘 됩니다.\n",
    "\n",
    "- Batch normalization\n",
    "    - Gradient Vanishing, Gradient Exploding 문제\n",
    "    - Internal covariance shift\n",
    "        - Training 과정에서 Optimization Smooth하게 해준다.\n",
    "    - 새로운 layer를 FC, Conv 와 non-linear function 사이에 넣어줍니다.\n",
    "    - activation function의 linear 구간을 scaling 해주고 shift 해주면서 더 좋은 nonlinearity position을 잡아냅니다.\n",
    "    \n",
    "- Layer normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "데이터의 variance를 주면서 데이터의 양을 증가시킵니다. 오디오 데이터의 경우에는 다음과 같은 방식이 있습니다.\n",
    "- Pitch Shifting\n",
    "- Resampling\n",
    "- Time-stretching\n",
    "- Equalization\n",
    "- Adding noises\n",
    "\n",
    "### CutOut (Local - Masking)\n",
    "pixel을 버리는 것입니다. Local한 직사각형을 잘라내 버립니다. Dropout과 매우 유사하지만, Uniform random하게 버리는 것이 아니라 데이터를 생성하는 단계에서 masking을 통해 생성하게 됩니다.\n",
    "\n",
    "### Mixup\n",
    "Linear-Interpolation을 통해서 두개 다른 데이터에 대해서 혼합된 어떤 데이터를 생성하게 됩니다.\n",
    "\n",
    "- 두개를 섞어서 내면 둘중 하나로 Prediction을 해야한다.\n",
    "- 만약에 그 외의 Class로 분류하게 된다면 그 부분을 Mix-up으로 잡아내 보자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
