{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control Using DDPG\n",
    "---\n",
    "\n",
    "\n",
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shallow and deep copy operations\n",
    "# Documentation: https://docs.python.org/3/library/copy.html\n",
    "import copy\n",
    "\n",
    "# Generate pseudo-random numbers\n",
    "# Documentation: https://docs.python.org/3/library/random.html\n",
    "import random\n",
    "\n",
    "# namedtuple: factory function for creating tuple subclasses with named fields\n",
    "# deque: list-like container with fast appends and pops on either end\n",
    "# Documentation: https://docs.python.org/3/library/collections.html\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# Return the time in seconds since the epoch as a floating point number\n",
    "# Documentation: https://docs.python.org/3/library/time.html#time.time\n",
    "from time import time\n",
    "\n",
    "# Basic date and time types\n",
    "# Documentation: https://docs.python.org/3/library/datetime.html\n",
    "import datetime\n",
    "\n",
    "# Unity Machine Learning Agents Environment\n",
    "# Documentation: https://github.com/Unity-Technologies/ml-agents/tree/master/docs\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "# torch: Optimized tensor library for deep learning using GPUs and CPUs\n",
    "# torch.nn: Contains various components for constructing neural networks\n",
    "# torch.nn.functional: Contains various functions for neural networks\n",
    "# torch.optim: A package implementing various optimization algorithms\n",
    "# Documentation: https://pytorch.org/docs/stable/index.html\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Package for scientific computing with a powerful N-dimensional array object\n",
    "# Documentation: https://docs.scipy.org/doc/numpy/reference/routines.html\n",
    "import numpy as np\n",
    "\n",
    "# Provides a MATLAB-like way of plotting\n",
    "# Documentation: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.html\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the backend of matplotlib to the 'inline' backend\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor & Critic Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    # Actor (Policy) Model\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc_units=256):\n",
    "        # Parameters are initialized and model is built.\n",
    "        # state_size (int): Dimension of each state\n",
    "        # action_size (int): Dimension of each action\n",
    "        # seed (int): Random seed\n",
    "        # fc_units (int): Number of nodes in first hidden layer\n",
    "        \n",
    "        super(Actor, self).__init__()\n",
    "        # random seed for pytorch number generators\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        # Network layers\n",
    "        self.fc1 = nn.Linear(state_size, fc_units)\n",
    "        self.fc2 = nn.Linear(fc_units, action_size)\n",
    "        \n",
    "        # Layer weights are initialized\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        return torch.tanh(self.fc2(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    # Critic (Value) Model\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=256, fc2_units=256, fc3_units=128):\n",
    "        # Parameters are initialized and model is built.\n",
    "        # state_size (int): Dimension of each state\n",
    "        # action_size (int): Dimension of each action\n",
    "        # seed (int): Random seed\n",
    "        # fcs1_units (int): Number of nodes in the first hidden layer\n",
    "        # fc2_units (int): Number of nodes in the second hidden layer\n",
    "        # fc3_units (int): Number of nodes in the second hidden layer\n",
    "        \n",
    "        super(Critic, self).__init__()\n",
    "        # random seed for pytorch number generators\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        # Network layers\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, fc3_units)\n",
    "        self.fc4 = nn.Linear(fc3_units, 1)\n",
    "        \n",
    "        # Layer weights are initialized\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(*hidden_init(self.fc3))\n",
    "        self.fc4.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        xs = F.leaky_relu(self.fcs1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        return self.fc4(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unity Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# Set the file_name for the unity environment\n",
    "file_name= './env/Reacher_Linux/Reacher.x86_64'\n",
    "\n",
    "# Load the unity environment\n",
    "env = UnityEnvironment(file_name=file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------- Computing Device ---------------------------- #\n",
    "# Use a CUDA GPU for training, if available. Otherwise, use CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------- Environment Variables ---------------------------- #\n",
    "# brain_name: The name for the environment brain\n",
    "# brain: The environment brain\n",
    "# env_info: # The environment is reset and the new environment information is stored\n",
    "# state: Current environment state\n",
    "# state_size: The environment state size\n",
    "# action_size: The action size\n",
    "\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "state = env_info.vector_observations\n",
    "state_size = state.shape[1]\n",
    "action_size = brain.vector_action_space_size\n",
    "num_agents = len(env_info.agents)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------- RNG Variables ---------------------------- #\n",
    "# random_seed: The random seed\n",
    "# seed: Initializes random number generator\n",
    "\n",
    "random_seed = 10\n",
    "seed = random.seed(random_seed)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------- Actor Networks ---------------------------- #\n",
    "# lr_actor: Learning rate for the Actor Optimizer\n",
    "# actor_local: The Local Actor Network\n",
    "# actor_target: The Target Actor Network\n",
    "# actor_optimizer: Optimizer for Actor Network\n",
    "\n",
    "lr_actor = 1e-4\n",
    "actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "actor_optimizer = optim.Adam(actor_local.parameters(), lr=lr_actor)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------- Critic Networks ---------------------------- #\n",
    "# lr_critic: Learning rate for the Critic Optimizer\n",
    "# w_decay: L2 weight decay for Critic Optimizer\n",
    "# critic_local: The Local Critic Network\n",
    "# critic_target: The Target Critic Network\n",
    "# critic_optimizer: Optimizer for Critic Network\n",
    "\n",
    "lr_critic = 3e-4\n",
    "w_decay = 1e-5\n",
    "critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "critic_optimizer = optim.Adam(critic_local.parameters(), lr=lr_critic, weight_decay=w_decay)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------- Ornstein-Uhlenbeck Process Variables ---------------------------- #\n",
    "# adds time-correlated noise to the actions taken by the policy\n",
    "\n",
    "noise = 2\n",
    "mu = 0.0 * np.ones((num_agents, action_size))\n",
    "theta = 0.15\n",
    "sigma = 0.2\n",
    "noise_reduction = 0.9999\n",
    "noise_state = copy.copy(mu)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------- Replay Memory Variables ---------------------------- #\n",
    "# buffer_size: The replay memory buffer size\n",
    "# memory: The memory buffer\n",
    "# experience: A tuple subclass to store individual experiences\n",
    "\n",
    "buffer_size = int(1e6)\n",
    "memory = deque(maxlen=buffer_size)\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------- Training Parameters ---------------------------- #\n",
    "# checkpoint_folder: path to checkpoint files\n",
    "# batch_size: Minibatch size\n",
    "# gamma: Reward discount factor\n",
    "# tau: For soft update of target parameters\n",
    "# add_noise: Add noise to action space during training\n",
    "# t_step: Initial time step (for updating every update_every steps)\n",
    "# update_every: Time step interval for learning from experiences\n",
    "# target_avg: Average score required to consider environment solved\n",
    "# n_episodes: Maximum number of episodes to train\n",
    "# scores_deque: Stores scores from last 100 episodes for calculating current average score\n",
    "# scores: Stores scores for all episode to plot after training\n",
    "\n",
    "checkpoint_folder = './checkpoints/'\n",
    "batch_size = 256\n",
    "gamma = 0.99\n",
    "tau = 1e-3\n",
    "add_noise = True\n",
    "t_step = 0\n",
    "update_every = 4\n",
    "target_avg = 30.0\n",
    "n_episodes = 2000\n",
    "scores_deque = deque(maxlen=100)\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watching an Untrained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "# get the current state\n",
    "states = env_info.vector_observations\n",
    "\n",
    "# Agents interact with environment for 150 time steps\n",
    "for i in range(150):\n",
    "    states = torch.from_numpy(states).float().to(device)\n",
    "    \n",
    "    # actor_local is set to evaluation mode\n",
    "    actor_local.eval()\n",
    "\n",
    "    # Gradient calculation is disabled for inference\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        actions = np.zeros((num_agents, action_size))\n",
    "\n",
    "        for agent_num, state in enumerate(states):\n",
    "            action = actor_local(state).cpu().data.numpy()\n",
    "            actions[agent_num, :] = action\n",
    "\n",
    "    # actor_local is set back to training mode\n",
    "    actor_local.train()\n",
    "    \n",
    "    # Clip the values in action to the range [-1, 1]\n",
    "    actions = np.clip(actions, -1, 1)\n",
    "    \n",
    "    # send the action to the environment\n",
    "    env_info = env.step(actions)[brain_name]\n",
    "\n",
    "    # get the next state\n",
    "    next_states = env_info.vector_observations\n",
    "\n",
    "    # get the reward\n",
    "    rewards = env_info.rewards\n",
    "\n",
    "    # see if episode has finished\n",
    "    dones = env_info.local_done\n",
    "\n",
    "    # state is updated for next time step\n",
    "    states = next_states\n",
    "\n",
    "    # exit loop if episode finished\n",
    "    if np.any(dones):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tScore: 9.69\tAverage Score: 3.85\n",
      "Episode 200\tScore: 18.08\tAverage Score: 13.22\n",
      "Episode 300\tScore: 25.73\tAverage Score: 19.85\n",
      "Episode 400\tScore: 29.46\tAverage Score: 25.00\n",
      "Episode 460\tScore: 33.49\tAverage Score: 30.09\n",
      "Environment solved in 360 episodes!\tAverage Score: 30.1\n",
      "\n",
      "Actor checkpoint saved to checkpoint_actor_2018-12-11_16-55-57.pth\n",
      "Critic checkpoint saved to checkpoint_critic_2018-12-11_16-55-57.pth\n",
      "\n",
      "Trained for 1 hours 30 minutes 19 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Time training started\n",
    "start_time = time()\n",
    "\n",
    "# Train for n_episodes or until the target_avg score is achieved\n",
    "for i_episode in range(1, n_episodes+1):\n",
    "    \n",
    "    # The starting score for the current episode\n",
    "    score = 0\n",
    "    \n",
    "    # The environment is reset and the new environment information is stored\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    \n",
    "    # The first state observation of the episode\n",
    "    states = env_info.vector_observations\n",
    "    \n",
    "    # ---------------------------- Iterate through episode time steps ---------------------------- #\n",
    "    while True:\n",
    "        # ---------------------------- Choose action ---------------------------- #\n",
    "        # Converts the state observation to a torch.FloatTensor and \n",
    "        # moves it to the specified computing device\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        \n",
    "        # actor_local is set to evaluation mode\n",
    "        actor_local.eval()\n",
    "        \n",
    "        # Gradient calculation is disabled for inference\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            actions = np.zeros((num_agents, action_size))\n",
    "            \n",
    "            for agent_num, state in enumerate(states):\n",
    "                action = actor_local(state).cpu().data.numpy()\n",
    "                actions[agent_num, :] = action\n",
    "            \n",
    "        # actor_local is set back to training mode\n",
    "        actor_local.train()\n",
    "        \n",
    "        # Add noise to the action space\n",
    "        if add_noise:\n",
    "            x = noise_state\n",
    "            dx = theta * (mu - x) + sigma * np.random.standard_normal((num_agents, action_size))\n",
    "            noise_state = x + dx\n",
    "            actions += noise*noise_state\n",
    "            noise *= noise_reduction\n",
    "        \n",
    "        # Clip the values in action to the range [-1, 1]\n",
    "        actions = np.clip(actions, -1, 1)\n",
    "        \n",
    "        # ---------------------------- Take action and get new environment info ---------------------------- #\n",
    "        # Sends all actions to the environment\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        \n",
    "        # Get next state (for each agent)\n",
    "        next_states = env_info.vector_observations\n",
    "        \n",
    "        # Rewards gained this time step (for each agent)\n",
    "        rewards = env_info.rewards\n",
    "        \n",
    "        # Stores whether episode finished\n",
    "        dones = env_info.local_done\n",
    "\n",
    "        # The experience from this time step is saved in the memory buffer\n",
    "        for i in range(num_agents):\n",
    "            e = experience(states[i], actions[i], rewards[i], next_states[i], dones[i])\n",
    "            memory.append(e)\n",
    "        \n",
    "        t_step = (t_step + 1) % update_every\n",
    "        \n",
    "        # Learning is performed every update_every time steps when enough experiences are available in memory\n",
    "        if t_step == 0 and len(memory) > batch_size:\n",
    "            \n",
    "            # Random sample of experiences from memory buffer\n",
    "            experiences = random.sample(memory, k=batch_size)\n",
    "            \n",
    "            # Parses the Experience Sample:\n",
    "            # Extracts the states, actions, rewards, next_states, and dones from \n",
    "            # the sampled experience tuples into separate lists.  Converts the lists\n",
    "            # to vertical NumPy arrays.  Converts the NumPy arrays into torch Tensors\n",
    "            # and moves them to the specified computing device\n",
    "            \n",
    "            s_states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "            s_actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "            s_rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "            s_next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "            s_dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "            # Policy and value parameters are updated using the sample of experiences\n",
    "            # ---------------------------- Critic is Updated ---------------------------- #\n",
    "            # Predicted next-state actions from target actor\n",
    "            actions_next = actor_target(s_next_states)\n",
    "            \n",
    "            # Q values for next state/action from target critic\n",
    "            Q_targets_next = critic_target(s_next_states, actions_next)\n",
    "            \n",
    "            # Q targets for current state(s)/action(s)\n",
    "            Q_targets = s_rewards + (gamma * Q_targets_next * (1 - s_dones))\n",
    "            \n",
    "            # Expected Q value from local critic\n",
    "            Q_expected = critic_local(s_states, s_actions)\n",
    "            \n",
    "            # The element-wise mean squared error between expected Q value and actual Q value\n",
    "            critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "            \n",
    "            # Clears the gradients of all optimized Tensors\n",
    "            critic_optimizer.zero_grad()\n",
    "            \n",
    "            # Computes the gradient\n",
    "            critic_loss.backward()\n",
    "            \n",
    "            # Clips the gradient for critic_local\n",
    "            torch.nn.utils.clip_grad_norm_(critic_local.parameters(), 1)\n",
    "            \n",
    "            # Performs a single optimization step to improve the local critic's expected Q value prediction\n",
    "            critic_optimizer.step()\n",
    "\n",
    "            # ---------------------------- Actor is Updated ---------------------------- #\n",
    "            # The predicted current-state actions from local actor\n",
    "            actions_pred = actor_local(s_states)\n",
    "            \n",
    "            # Actor loss\n",
    "            actor_loss = -critic_local(s_states, actions_pred).mean()\n",
    "            \n",
    "            # Clears the gradients of all optimized Tensors\n",
    "            actor_optimizer.zero_grad()\n",
    "            \n",
    "            # Computes the gradient\n",
    "            actor_loss.backward()\n",
    "            \n",
    "            # Performs a single optimization step to minimize actor loss\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            # ----------------------- Soft updates on target networks ----------------------- #\n",
    "            for target_param, local_param in zip(critic_target.parameters(), critic_local.parameters()):\n",
    "                # θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "                target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "            for target_param, local_param in zip(actor_target.parameters(), actor_local.parameters()):\n",
    "                # θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "                target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "        \n",
    "        # state is updated for next time step\n",
    "        states = next_states\n",
    "        \n",
    "        # episode score is updated\n",
    "        score += np.mean(rewards)\n",
    "        \n",
    "        # Check if episode has ended\n",
    "        if np.any(dones):\n",
    "            break\n",
    "    \n",
    "    scores_deque.append(score)\n",
    "    scores.append(score)\n",
    "    \n",
    "    print('\\rEpisode {}\\tScore: {:.2f}\\tAverage Score: {:.2f}'.format(i_episode, score, np.mean(scores_deque)), end=\"\")\n",
    "    \n",
    "    # Save checkpoints every 100 episodes\n",
    "    if i_episode % 100 == 0:\n",
    "        torch.save(actor_local.state_dict(), checkpoint_folder + 'checkpoint_actor.pth')\n",
    "        torch.save(critic_local.state_dict(), checkpoint_folder + 'checkpoint_critic.pth')\n",
    "        print('\\rEpisode {}\\tScore: {:.2f}\\tAverage Score: {:.2f}'.format(i_episode, score, np.mean(scores_deque)))\n",
    "\n",
    "    # If the target average score has been achieved, the model checkpoints are saved\n",
    "    # with timestamps and the training loop is exited\n",
    "    if np.average(scores_deque)>=target_avg:\n",
    "        timestamp = '{:%Y-%m-%d_%H-%M-%S}'.format(datetime.datetime.now())\n",
    "        actor_checkpoint_name = \"checkpoint_actor_{}.pth\".format(timestamp)\n",
    "        critic_checkpoint_name = \"checkpoint_critic_{}.pth\".format(timestamp)\n",
    "        \n",
    "        torch.save(actor_local.state_dict(), checkpoint_folder + actor_checkpoint_name)\n",
    "        torch.save(critic_local.state_dict(), checkpoint_folder + critic_checkpoint_name)\n",
    "        \n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.1f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "        print(\"\\nActor checkpoint saved to {}\".format(actor_checkpoint_name))\n",
    "        print(\"Critic checkpoint saved to {}\".format(critic_checkpoint_name))\n",
    "        break\n",
    "\n",
    "# Time training ended\n",
    "end_time = time()\n",
    "\n",
    "# Stores training time\n",
    "tot_time = end_time - start_time\n",
    "print(\"\\nTrained for\", int((tot_time / 3600)), \"hours\", int(((tot_time % 3600) / 60)), \"minutes\", \n",
    "      int(((tot_time % 3600) % 60)), \"seconds\")\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Load Model Weights from a Previous Training Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The checkpoint file names\n",
    "actor_checkpoint = checkpoint_folder + 'checkpoint_actor_2018-12-11_16-55-57.pth'\n",
    "critic_checkpoint = checkpoint_folder + 'checkpoint_critic_2018-12-11_16-55-57.pth'\n",
    "\n",
    "# Checkpoints weights are loaded into the models\n",
    "actor_local.load_state_dict(torch.load(actor_checkpoint))\n",
    "critic_local.load_state_dict(torch.load(critic_checkpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watching a Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 33.25, Timestamp: 0:0:0\n",
      "Score: 32.79, Timestamp: 0:1:20\n",
      "Score: 33.79, Timestamp: 0:2:40\n",
      "Average Score: 33.27\n"
     ]
    }
   ],
   "source": [
    "# Total points from all episodes\n",
    "total_score = 0\n",
    "\n",
    "# Number of episodes for agent to interact with environment\n",
    "num_episodes = 3\n",
    "\n",
    "# Starting time for generating timestamps for each episode\n",
    "base_time = time()\n",
    "\n",
    "# Agent will interact with environment for specified number of episodes\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    # Beginning of episode timestamp\n",
    "    start_time = time()\n",
    "    \n",
    "    # Time passed since the beginning of first episode\n",
    "    tot_time = start_time - base_time\n",
    "    time_stamp = \"{}:{}:{}\".format(int((tot_time / 3600)), int(((tot_time % 3600) / 60)), int(((tot_time % 3600) % 60)))\n",
    "    \n",
    "    # reset the environment\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "    # get the current state\n",
    "    states = env_info.vector_observations\n",
    "    \n",
    "    # The starting score for the current episode\n",
    "    score = 0\n",
    "\n",
    "    # Agents interact with environment for 150 time steps\n",
    "    while True:\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "\n",
    "        # actor_local is set to evaluation mode\n",
    "        actor_local.eval()\n",
    "\n",
    "        # Gradient calculation is disabled for inference\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Get action Tensor\n",
    "            # Move action Tensor to CPU and convert to NumPy array\n",
    "            actions = np.zeros((num_agents, action_size))\n",
    "\n",
    "            for agent_num, state in enumerate(states):\n",
    "                action = actor_local(state).cpu().data.numpy()\n",
    "                actions[agent_num, :] = action\n",
    "\n",
    "        # actor_local is set back to training mode\n",
    "        actor_local.train()\n",
    "\n",
    "        # Clip the values in action to the range [-1, 1]\n",
    "        actions = np.clip(actions, -1, 1)\n",
    "\n",
    "        # send the action to the environment\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "\n",
    "        # get the next state\n",
    "        next_states = env_info.vector_observations\n",
    "\n",
    "        # get the reward\n",
    "        rewards = env_info.rewards\n",
    "\n",
    "        # see if episode has finished\n",
    "        dones = env_info.local_done\n",
    "\n",
    "        # state is updated for next time step\n",
    "        states = next_states\n",
    "        \n",
    "        # episode score is updated\n",
    "        score += np.mean(rewards)\n",
    "\n",
    "        # exit loop if episode finished\n",
    "        if np.any(dones):\n",
    "            break\n",
    "    \n",
    "    total_score += score\n",
    "    print(\"Score: {:.2f}, Timestamp: {}\".format(score, time_stamp))\n",
    "\n",
    "print(\"Average Score: {:.2f}\".format(total_score/num_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
