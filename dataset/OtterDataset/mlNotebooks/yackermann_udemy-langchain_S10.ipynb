{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path='.env')\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulding Smart Voice Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "import elevenlabs\n",
    "\n",
    "elevenlabs.set_api_key(os.getenv('ELEVEN_API_KEY'))\n",
    "\n",
    "class AudioInterface:\n",
    "    def listen(self) -> str:\n",
    "        recognizer = sr.Recognizer()\n",
    "\n",
    "        with sr.Microphone() as source:\n",
    "            print(\"Say something...\")\n",
    "            audio = recognizer.listen(source)\n",
    "\n",
    "        text = recognizer.recognize_whisper_api(\n",
    "            audio,\n",
    "            api_key=os.getenv('OPENAI_API_KEY'),\n",
    "        )\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def speak(self, text: str) -> None:\n",
    "        audio = elevenlabs.generate(\n",
    "            text=text,\n",
    "            model=\"eleven_monolingual_v1\"\n",
    "        )\n",
    "\n",
    "        elevenlabs.play(audio)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something...\n",
      "My name is Yuri.\n"
     ]
    }
   ],
   "source": [
    "interface = AudioInterface()\n",
    "text = interface.listen()\n",
    "print(text)\n",
    "\n",
    "interface.speak(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building conversation agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "class ConversationAgent:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI()\n",
    "        self.chain = ConversationChain(llm=self.llm)\n",
    "    \n",
    "    def run(self, text):\n",
    "        return self.chain.run(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something...\n",
      "ME: Hey, how's it going?\n",
      "AI: Hello! I'm doing well, thank you for asking. I'm an AI designed to assist with various tasks and engage in conversation. How can I help you today?\n",
      "Say something...\n",
      "ME: What's the weather today in Chicago?\n",
      "AI: I'm sorry, but I don't have access to real-time information. However, you can easily check the weather in Chicago by searching for it online or using a weather app on your phone.\n",
      "Say something...\n",
      "ME: What's the typical winter like in Chicago?\n",
      "AI: Chicago experiences cold and snowy winters. The average temperature in January, the coldest month, is around 30°F (-1°C). However, temperatures can drop below freezing, and wind chills can make it feel even colder. Chicago receives an average of 36 inches (91 cm) of snowfall during the winter season. It's important to bundle up and be prepared for winter weather conditions if you're planning to visit or live in Chicago during the winter months.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m response \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mrun(text)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m response)\n\u001b[0;32m---> 10\u001b[0m \u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeak\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 27\u001b[0m, in \u001b[0;36mAudioInterface.speak\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspeak\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     audio \u001b[38;5;241m=\u001b[39m elevenlabs\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     23\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m     24\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meleven_monolingual_v1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m     )\n\u001b[0;32m---> 27\u001b[0m     \u001b[43melevenlabs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/learning/langchain-udemy/venv/lib/python3.10/site-packages/elevenlabs/utils.py:33\u001b[0m, in \u001b[0;36mplay\u001b[0;34m(audio, notebook, use_ffmpeg)\u001b[0m\n\u001b[1;32m     26\u001b[0m     args \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mffplay\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-autoexit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-nodisp\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     27\u001b[0m     proc \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPopen(\n\u001b[1;32m     28\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m     29\u001b[0m         stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[1;32m     30\u001b[0m         stdin\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[1;32m     31\u001b[0m         stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[1;32m     32\u001b[0m     )\n\u001b[0;32m---> 33\u001b[0m     out, err \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     proc\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/subprocess.py:1154\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1151\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1154\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/subprocess.py:2021\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2014\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2015\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2016\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2017\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2018\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2019\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2021\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2022\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2024\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2025\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = ConversationAgent()\n",
    "\n",
    "interface = AudioInterface()\n",
    "\n",
    "while True:\n",
    "    text = interface.listen()\n",
    "    print(\"ME: \" + text)\n",
    "    response = agent.run(text)\n",
    "    print(\"AI: \" + response)\n",
    "    interface.speak(response)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenting the Agent with Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType, load_tools\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "class SmartChatAgent:\n",
    "    def __init__(self):\n",
    "        self.memory = ConversationBufferMemory(\n",
    "            memory_key=\"chat_history\",\n",
    "            return_messages=True,\n",
    "        )\n",
    "\n",
    "        self.llm = ChatOpenAI()\n",
    "        self.tools = load_tools([\"google-search\"])\n",
    "\n",
    "        self.agent = initialize_agent(\n",
    "            llm=self.llm,\n",
    "            tools=self.tools,\n",
    "            agent_type=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "            memory=self.memory,\n",
    "            verbose=True,\n",
    "            handle_parsing_errors=True\n",
    "        )\n",
    "    \n",
    "    def run(self, text):\n",
    "        return self.agent.run(text, callbacks=[StdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something...\n",
      "ME: Hi, how are you? My name is Yuri.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "google_search: A wrapper around Google Search. Useful for when you need to answer questions about current events. Input should be a search query.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [google_search]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: Hi, how are you? My name is Yuri.\n",
      "Thought:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThis question does not require any external tools to answer.\u001b[0m\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "google_search: A wrapper around Google Search. Useful for when you need to answer questions about current events. Input should be a search query.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [google_search]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: Hi, how are you? My name is Yuri.\n",
      "Thought:This question does not require any external tools to answer.\n",
      "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
      "Thought:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI don't need to use any external tools to answer this question.\n",
      "Final Answer: I'm sorry, I cannot answer this question as it is not a valid format.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI: I'm sorry, I cannot answer this question as it is not a valid format.\n",
      "Say something...\n",
      "ME: What are the latest developments in artificial intelligence?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "google_search: A wrapper around Google Search. Useful for when you need to answer questions about current events. Input should be a search query.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [google_search]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: What are the latest developments in artificial intelligence?\n",
      "Thought:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use the google_search tool to find the latest news on artificial intelligence.\n",
      "Action: google_search\n",
      "Action Input: \"latest developments in artificial intelligence\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mMar 13, 2023 ... AI has significantly improved medical imaging algorithms, surpassing human capabilities in diagnosing diseases from X-rays, MRIs, and CT scans. Jan. 8, 2024 — Researchers have developed a new soft robot design that engages in three simultaneous behaviors: rolling forward, spinning like a record, and ... 5 days ago ... AI Development Trends · AI Robots Learning Through Observation · AI Robot Caregivers Are Filling a Shortfall · AI Beer Brewers · AI-Based ... Sep 20, 2022 ... 3 Latest Developments in Artificial Intelligence · 1. Deep Learning Chips. The first development is the creation of new chips that help run deep ... Jul 22, 2022 ... Latest Developments of Artificial Intelligence: 10 AI Trends to Follow · #1: Artificial Intelligence Trends in Robotics · #2: Machine Learning ... Jan 4, 2024 ... 1. Multimodal AI. Multimodal AI goes beyond traditional single-mode data processing to encompass multiple input types, such as text, images and ... Jan 7, 2023 ... 1. Voice and language-driven intelligence · 2. Ethical and Explainable AI · 3. AI-powered cybersecurity · 4. Generative AI · 5. Sustainable AI · 6. Sep 24, 2021 ... We have recently seen the release of GPT-3 by OpenAI, the most advanced (and largest) language model ever created, consisting of around 175 ... The core technology behind most of the most visible advances is machine learning, especially deep learning (including generative adversarial networks or GANs) ... Aug 3, 2022 ... The implementation of DP will revolutionize current practice by providing pathologists with additional tools and algorithms to improve ...\u001b[0m\n",
      "Thought:\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnswer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "google_search: A wrapper around Google Search. Useful for when you need to answer questions about current events. Input should be a search query.\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [google_search]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: What are the latest developments in artificial intelligence?\n",
      "Thought:I should use the google_search tool to find the latest news on artificial intelligence.\n",
      "Action: google_search\n",
      "Action Input: \"latest developments in artificial intelligence\"\n",
      "Observation: Mar 13, 2023 ... AI has significantly improved medical imaging algorithms, surpassing human capabilities in diagnosing diseases from X-rays, MRIs, and CT scans. Jan. 8, 2024 — Researchers have developed a new soft robot design that engages in three simultaneous behaviors: rolling forward, spinning like a record, and ... 5 days ago ... AI Development Trends · AI Robots Learning Through Observation · AI Robot Caregivers Are Filling a Shortfall · AI Beer Brewers · AI-Based ... Sep 20, 2022 ... 3 Latest Developments in Artificial Intelligence · 1. Deep Learning Chips. The first development is the creation of new chips that help run deep ... Jul 22, 2022 ... Latest Developments of Artificial Intelligence: 10 AI Trends to Follow · #1: Artificial Intelligence Trends in Robotics · #2: Machine Learning ... Jan 4, 2024 ... 1. Multimodal AI. Multimodal AI goes beyond traditional single-mode data processing to encompass multiple input types, such as text, images and ... Jan 7, 2023 ... 1. Voice and language-driven intelligence · 2. Ethical and Explainable AI · 3. AI-powered cybersecurity · 4. Generative AI · 5. Sustainable AI · 6. Sep 24, 2021 ... We have recently seen the release of GPT-3 by OpenAI, the most advanced (and largest) language model ever created, consisting of around 175 ... The core technology behind most of the most visible advances is machine learning, especially deep learning (including generative adversarial networks or GANs) ... Aug 3, 2022 ... The implementation of DP will revolutionize current practice by providing pathologists with additional tools and algorithms to improve ...\n",
      "Thought:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThere have been several recent developments in artificial intelligence, including improvements in medical imaging algorithms, the development of new soft robot designs, the creation of deep learning chips, and advancements in voice and language-driven intelligence. There is also ongoing research in ethical and explainable AI, AI-powered cybersecurity, generative AI, and sustainable AI. Additionally, the release of GPT-3 by OpenAI has been a significant milestone in language modeling. Overall, there are various areas of progress in artificial intelligence.\n",
      "Final Answer: Recent developments in artificial intelligence include improvements in medical imaging algorithms, the development of new soft robot designs, the creation of deep learning chips, advancements in voice and language-driven intelligence, and the release of GPT-3 by OpenAI.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI: Recent developments in artificial intelligence include improvements in medical imaging algorithms, the development of new soft robot designs, the creation of deep learning chips, advancements in voice and language-driven intelligence, and the release of GPT-3 by OpenAI.\n",
      "Say something...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m interface \u001b[38;5;241m=\u001b[39m AudioInterface()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlisten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mME: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m text)\n\u001b[1;32m      8\u001b[0m     response \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mrun(text)\n",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m, in \u001b[0;36mAudioInterface.listen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sr\u001b[38;5;241m.\u001b[39mMicrophone() \u001b[38;5;28;01mas\u001b[39;00m source:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSay something...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     audio \u001b[38;5;241m=\u001b[39m \u001b[43mrecognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlisten\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m text \u001b[38;5;241m=\u001b[39m recognizer\u001b[38;5;241m.\u001b[39mrecognize_whisper_api(\n\u001b[1;32m     15\u001b[0m     audio,\n\u001b[1;32m     16\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "File \u001b[0;32m~/Github/learning/langchain-udemy/venv/lib/python3.10/site-packages/speech_recognition/__init__.py:491\u001b[0m, in \u001b[0;36mRecognizer.listen\u001b[0;34m(self, source, timeout, phrase_time_limit, snowboy_configuration)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mand\u001b[39;00m elapsed_time \u001b[38;5;241m>\u001b[39m timeout:\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m WaitTimeoutError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlistening timed out while waiting for phrase to start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 491\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# reached end of the stream\u001b[39;00m\n\u001b[1;32m    493\u001b[0m frames\u001b[38;5;241m.\u001b[39mappend(buffer)\n",
      "File \u001b[0;32m~/Github/learning/langchain-udemy/venv/lib/python3.10/site-packages/speech_recognition/__init__.py:199\u001b[0m, in \u001b[0;36mMicrophone.MicrophoneStream.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, size):\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyaudio_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/learning/langchain-udemy/venv/lib/python3.10/site-packages/pyaudio/__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[0;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input:\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[0;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = SmartChatAgent()\n",
    "\n",
    "interface = AudioInterface()\n",
    "\n",
    "while True:\n",
    "    text = interface.listen()\n",
    "    print(\"ME: \" + text)\n",
    "    response = agent.run(text)\n",
    "    print(\"AI: \" + response)\n",
    "    interface.speak(response)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
