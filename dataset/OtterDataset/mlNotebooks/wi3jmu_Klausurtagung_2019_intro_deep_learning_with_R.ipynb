{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*Klausurtagung 2019*\n",
    "\n",
    "# Deep Learning with R\n",
    "\n",
    "Matthias Griebel<br>\n",
    "Lehrstuhl f√ºr Wirtschaftsinformatik und Informationsmanagement\n",
    "\n",
    "*Rothenburg ob der Tauber, August 7-9, 2019*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__WIRED: The three breakthroughs that have finally unleashed A.I. on the world!__\n",
    "<img src=\"https://github.com/matjesg/AIS_2019/raw/master/notebooks/images/08/accelerators.png\" style='width:80%'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "toc": true
   },
   "source": [
    "<h1>Agenda<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Datasets-for-Deep-Learning\" data-toc-modified-id=\"Datasets-for-Deep-Learning-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Datasets for Deep Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Image-Data-Sets\" data-toc-modified-id=\"Image-Data-Sets-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Image Data Sets</a></span></li><li><span><a href=\"#Natural-Language-Processing-Data-Sets\" data-toc-modified-id=\"Natural-Language-Processing-Data-Sets-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Natural Language Processing Data Sets</a></span></li><li><span><a href=\"#Audio/Speech-Datasets\" data-toc-modified-id=\"Audio/Speech-Datasets-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Audio/Speech Datasets</a></span></li></ul></li><li><span><a href=\"#New-Algorithmic-Components\" data-toc-modified-id=\"New-Algorithmic-Components-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>New Algorithmic Components</a></span><ul class=\"toc-item\"><li><span><a href=\"#Architectures-&amp;-Node-Types\" data-toc-modified-id=\"Architectures-&amp;-Node-Types-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Architectures &amp; Node Types</a></span></li><li><span><a href=\"#Deep-Learning-Frameworks\" data-toc-modified-id=\"Deep-Learning-Frameworks-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Deep Learning Frameworks</a></span></li></ul></li><li><span><a href=\"#Fine-Tuned-Hardware\" data-toc-modified-id=\"Fine-Tuned-Hardware-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Fine-Tuned Hardware</a></span></li><li><span><a href=\"#Basic-Classification-with-Keras-in-R\" data-toc-modified-id=\"Basic-Classification-with-Keras-in-R-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Basic Classification with Keras in R</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-the-Fashion-MNIST-Data-Set\" data-toc-modified-id=\"Import-the-Fashion-MNIST-Data-Set-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Import the Fashion MNIST Data Set</a></span></li><li><span><a href=\"#Explore-the-Data\" data-toc-modified-id=\"Explore-the-Data-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Explore the Data</a></span></li><li><span><a href=\"#Preprocess-the-Data\" data-toc-modified-id=\"Preprocess-the-Data-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Preprocess the Data</a></span></li><li><span><a href=\"#Build-the-Model\" data-toc-modified-id=\"Build-the-Model-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Build the Model</a></span></li><li><span><a href=\"#Train-the-Model\" data-toc-modified-id=\"Train-the-Model-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Train the Model</a></span></li><li><span><a href=\"#Evaluate-the-Model\" data-toc-modified-id=\"Evaluate-the-Model-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Evaluate the Model</a></span></li><li><span><a href=\"#Make-Predictions\" data-toc-modified-id=\"Make-Predictions-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>Make Predictions</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Datasets for Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Image Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "[__MNIST__](http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "- Large database of handwritten digits\n",
    "- 70,000 images in 10 classes\n",
    "    - 60k training images\n",
    "    - 10k testing images\n",
    "- Size: ~50 MB\n",
    "    \n",
    "It is a good database for trying learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\" style='width:100%'>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[__ImageNet__](http://www.image-net.org)\n",
    "\n",
    "- Designed for use in visual object recognition software research\n",
    "- ~1,500,000 images, each with multiple __bounding boxes__ and respective class labels\n",
    "- Size: ~150GB\n",
    "\n",
    "Since 2010, the ImageNet project runs an annual software contest, the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). \n",
    "\n",
    "Modern deep learning was born on 30 September 2012, when a convolutional neural network (CNN) called AlexNet achieved a top-5 error of 15.3% in the ImageNet 2012 Challenge.\n",
    "\n",
    "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/03/ImageNet.png\" style='width:100%'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[__MS-COCO__](http://cocodataset.org/#home)\n",
    "\n",
    "- Designed for object detection, segmentation and captioning\n",
    "- ~ 30K images\n",
    "    - 80 object categories, \n",
    "    - 5 captions per image, \n",
    "    - 250,000 people with key points\n",
    "- Size: ~25 GB (Compressed)\n",
    "\n",
    "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/03/coco.jpg\" style='width:100%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Natural Language Processing Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[__IMDB Reviews__](http://cocodataset.org/#home)\n",
    "\n",
    "- Contains movie reviews along with their associated binary sentiment polarity labels\n",
    "- 50,000 reviews \n",
    "    - 25k highly polar movie reviews for training\n",
    "    - 25k for testing\n",
    "- Size: ~80 MB\n",
    "\n",
    "It is intended to serve as a benchmark for sentiment classification. \n",
    "\n",
    "More NLP data sets: [Twenty Newsgroups](https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups), [Sentiment140](http://help.sentiment140.com/for-students/), [WordNet](https://wordnet.princeton.edu/), [Yelp Reviews](https://www.yelp.com/dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Audio/Speech Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[__Free Spoken Digit Dataset__](https://github.com/Jakobovski/free-spoken-digit-dataset)\n",
    "\n",
    "- Created to solve the task of identifying spoken digits in audio samples\n",
    "- 1,500 recordings (50 of each digit per speaker)\n",
    "    - 3 speakers\n",
    "- English pronunciations\n",
    "- Size: ~10 MB\n",
    "\n",
    "It is intended to serve as a benchmark for sentiment classification. \n",
    "\n",
    "Other data sets\n",
    "- [Free Music Archive (FMA)](https://github.com/mdeff/fma)\n",
    "- [Ballroom](http://mtg.upf.edu/ismir2004/contest/tempoContest/node5.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## New Algorithmic Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Architectures & Node Types\n",
    "\n",
    "- Convolutional Neural Networks (CNN)\n",
    "- Long Short Term Memory (LSTM)\n",
    "- Drop-out layers\n",
    "- Auto encoder hierarchies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__CNN Architecture Comparison on ImageNet__\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*ZqkLRkMU2ObOQWIHLBg8sw.png\" style='width:100%'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Deep Learning Frameworks\n",
    "\n",
    "- Deep learning frameworks offer building blocks for designing, training and validating deep neural networks\n",
    "- Allows for easy and fast prototyping\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*s_BwkYxpGv34vjOHi8tDzg.png\" style='width:70%'>\n",
    "\n",
    "[Source](https://towardsdatascience.com/deep-learning-framework-power-scores-2018-23607ddf297a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Deep learning frameworks are changing rapidly__\n",
    "\n",
    "<img src=\"https://github.com/matjesg/AIS_2019/raw/master/notebooks/images/08/dl_trends.png\" style='width:100%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__ImageNet Pretrained Models on [Keras](https://keras.io/applications/)__\n",
    "\n",
    "\n",
    "- Xception\n",
    "- VGG16\n",
    "- VGG19\n",
    "- ResNet, ResNetV2, ResNeXt\n",
    "- InceptionV3\n",
    "- InceptionResNetV2\n",
    "- MobileNet\n",
    "- MobileNetV2\n",
    "- DenseNet\n",
    "- NASNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fine-Tuned Hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Hardware for Deep Learning__\n",
    "\n",
    "- __GPU__ (Graphics processing unit) are well suited for Deep Learning \n",
    "     - Efficient for matrix multiplication and convolution (parallelized operations)\n",
    "     - Sequential code is still faster on CPUs\n",
    "     \n",
    "- __TPUs__ (tensor processing units) are specifically designed for neural network machine learning\n",
    "\n",
    "<img src=\"https://cloud.google.com/images/products/tpu/machine-learning-performance.png\" style='width:50%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic Classification with Keras in R "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__We will train a neural network model to classify images of clothing, like sneakers and shirts__\n",
    "\n",
    "The code is adopted from https://keras.rstudio.com/articles/tutorial_basic_classification.html.\n",
    "\n",
    "First, we need to install and load the required R packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "install.packages('keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(keras)\n",
    "library(tidyverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Import the Fashion MNIST Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We will use the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) data set\n",
    "- 70,000 grayscale images in 10 categories\n",
    "    - 60k training images\n",
    "    - 10k testing images\n",
    "- The images show individual articles of clothing at low resolution (28 by 28 pixels)\n",
    "\n",
    "\n",
    "__Why Fashion MNIST?__\n",
    "\n",
    "Fashion MNIST is a slightly more challenging problem than regular MNIST. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src='https://keras.rstudio.com/articles/images/fashion_mnist.png' style='width:80%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can access the Fashion MNIST directly from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist <- dataset_fashion_mnist()\n",
    "c(train_images, train_labels) %<-% fashion_mnist$train\n",
    "c(test_images, test_labels) %<-% fashion_mnist$test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "At this point we have four arrays: \n",
    "- `train_image` and `train_label` arrays are the training set\n",
    "- `test_image` and `test_labels array` are the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The images each are 28 x 28 arrays, with pixel values ranging between 0 and 255. The labels are arrays of integers, ranging from 0 to 9. These correspond to the class of clothing the image represents:\n",
    "\n",
    "Digit  | Class\n",
    "-------| -------------\n",
    "0 | T-shirt/top\n",
    "1 | Trouser\n",
    "2 | Pullover\n",
    "3 | Dress\n",
    "4 | Coat\n",
    "5 | Sandal\n",
    "6 | Shirt\n",
    "7 | Sneaker\n",
    "8 | Bag\n",
    "9 | Ankle boot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each image is mapped to a single label. Since the class names are not included with the dataset, we'll store them in a vector to use later when plotting the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = c('T-shirt/top',\n",
    "                'Trouser',\n",
    "                'Pullover',\n",
    "                'Dress',\n",
    "                'Coat', \n",
    "                'Sandal',\n",
    "                'Shirt',\n",
    "                'Sneaker',\n",
    "                'Bag',\n",
    "                'Ankle boot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Explore the Data\n",
    "\n",
    "Let's explore the format of the dataset before training the model. The following shows there are 60,000 images in the training set, with each image represented as 28 x 28 pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(train_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, there are 60,000 labels in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are 10,000 images in the test set. Again, each image is represented as 28 x 28 pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the test set contains 10,000 images labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Preprocess the Data\n",
    "\n",
    "The data must be preprocessed before training the network. If you inspect the first image in the training set, you will see that the pixel values fall in the range of 0 to 255:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=3, repr.plot.height=3) \n",
    "image_1 <- as.data.frame(train_images[1, , ])\n",
    "colnames(image_1) <- seq_len(ncol(image_1))\n",
    "image_1$y <- seq_len(nrow(image_1))\n",
    "image_1 <- gather(image_1, \"x\", \"value\", -y)\n",
    "image_1$x <- as.integer(image_1$x)\n",
    "ggplot(image_1, aes(x = x, y = y, fill = value)) +\n",
    "    geom_tile() +\n",
    "    scale_fill_gradient(low = \"white\", high = \"black\", na.value = NA) +\n",
    "    scale_y_reverse() +\n",
    "    theme_minimal() +\n",
    "    theme(panel.grid = element_blank())   +\n",
    "    theme(aspect.ratio = 1) +\n",
    "    xlab(\"\") +\n",
    "    ylab(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We scale these values to a range of 0 to 1 before feeding to the neural network model. For this, we simply divide by 255. \n",
    "\n",
    "It's important that the training set and the testing set are preprocessed in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images <- train_images / 255\n",
    "test_images <- test_images / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Display the first 25 images from the training set and display the class name below each image. \n",
    "Verify that the data is in the correct format and we're ready to build and train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=5, repr.plot.height=5) \n",
    "par(mfcol=c(5,5))\n",
    "par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')\n",
    "for (i in 1:25) { \n",
    "  img <- train_images[i, , ]\n",
    "  img <- t(apply(img, 2, rev)) \n",
    "  image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',\n",
    "        main = paste(class_names[train_labels[i] + 1]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For our CNN, we need to add one channel at the end of our images. The shape is usually defined as __(features, height, width, channels)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train <- train_images %>%\n",
    "    array_reshape(c(60000, 28, 28, 1))\n",
    "x_test <- test_images %>%\n",
    "    array_reshape(c(10000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(train_images)\n",
    "dim(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Build the Model \n",
    "\n",
    "Building the neural network in Keras requires configuring the layers of the model, then compiling the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Setup the layers__\n",
    "\n",
    "The basic building block of a neural network is the layer. Layers extract representations from the data fed into them. And, hopefully, these representations are more meaningful for the problem at hand.\n",
    "\n",
    "Most of deep learning consists of chaining together simple layers. Most layers, like `layer_dense` and `conv_layers`, have parameters that are learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model <- keras_model_sequential()\n",
    "model %>%\n",
    "    layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu', input_shape = c(28, 28, 1)) %>%\n",
    "    layer_max_pooling_2d(pool_size = c(2,2)) %>%\n",
    "    layer_flatten() %>%\n",
    "    layer_dense(units = 128, activation = 'relu') %>%\n",
    "    layer_dropout(rate = 0.5) %>%\n",
    "    layer_dense(units = 10, activation = 'softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Compile the model__\n",
    "\n",
    "Before the model is ready for training, it needs a few more settings. These are added during the model's compile step:\n",
    "\n",
    "- __Loss function__ - This measures how accurate the model is during training. We want to minimize this function to \"steer\" the model in the right direction.\n",
    "- __Optimizer__ - This is how the model is updated based on the data it sees and its loss function.\n",
    "- __Metrics__ - Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model %>% compile(\n",
    "    loss = 'sparse_categorical_crossentropy',\n",
    "    optimizer = 'adam', \n",
    "    metrics = c('accuracy')\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the neural network model requires the following steps:\n",
    "\n",
    " - Feed the training data to the model ‚Äî in this example, the `train_images` and `train_labels` arrays.\n",
    " - The model learns to associate images and labels.\n",
    " - We ask the model to make predictions about a test set ‚Äî in this example, the test_images array. We verify that the predictions match the labels from the test_labels array.\n",
    "\n",
    "To start training, call the `fit` method ‚Äî the model is \"fit\" to the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history <- model %>% fit(x_train, train_labels, epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, compare how the model performs on the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score <- model %>% evaluate(x_test, test_labels)\n",
    "cat('Test loss:', score$loss, \"\\n\")\n",
    "cat('Test accuracy:', score$acc, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out, the accuracy on the test dataset is a little less than the accuracy on the training dataset. This gap between training accuracy and test accuracy is an example of overfitting. Overfitting is when a machine learning model performs worse on new data than on their training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Make Predictions\n",
    "\n",
    "With the model trained, we can use it to make predictions about some images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions <- model %>% predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here, the model has predicted the label for each image in the testing set. Let's take a look at the first prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[1, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prediction is an array of 10 numbers. These describe the \"confidence\" of the model that the image corresponds to each of the 10 different articles of clothing. We can see which label has the highest confidence value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which.max(predictions[1, ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Alternatively, we can also directly get the class prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_pred <- model %>% predict_classes(x_test)\n",
    "class_pred[1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the labels are 0-based, this actually means a predicted label of 9 (to be found in class_names[9]). So the model is most confident that this image is an ankle boot.\n",
    "And we can check the test label to see this is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's plot several images with their predictions. Correct prediction labels are green and incorrect prediction labels are red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=7, repr.plot.height=7) \n",
    "par(mfcol=c(5,5))\n",
    "par(mar=c(0, 0, 1.5, 0), xaxs='i', yaxs='i')\n",
    "for (i in 1:25) { \n",
    "  img <- test_images[i, , ]\n",
    "  img <- t(apply(img, 2, rev)) \n",
    "  # subtract 1 as labels go from 0 to 9\n",
    "  predicted_label <- which.max(predictions[i, ]) - 1\n",
    "  true_label <- test_labels[i]\n",
    "  if (predicted_label == true_label) {\n",
    "    color <- '#008800' \n",
    "  } else {\n",
    "    color <- '#bb0000'\n",
    "  }\n",
    "  image(1:28, 1:28, img, col = gray((0:255)/255), xaxt = 'n', yaxt = 'n',\n",
    "        main = paste0(class_names[predicted_label + 1], \" (\",\n",
    "                      class_names[true_label + 1], \")\"),\n",
    "        col.main = color)\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  },
  "rise": {
   "enable_chalkboard": false,
   "overlay": "<div class='logo'><img src='images/uniwue4c.png'></div>",
   "scroll": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Agenda",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
