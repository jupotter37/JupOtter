{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "tf_config = tf.compat.v1.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "tf_config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "tf_config.allow_soft_placement = True\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def printValues(array, scale=\"prob\", sample=None):\n",
    "    n = 2 if type(sample) is np.ndarray else 1\n",
    "    fig, axs = plt.subplots(1, n, figsize=(6*n, 6))\n",
    "    x = np.arange(0, 10)\n",
    "    ax = axs if n == 1 else axs[0]\n",
    "    ax.bar(x, array[0])\n",
    "    if scale != \"logit\":\n",
    "        ax.set(ylim=[0, 1])\n",
    "    ax.set_xticks(x)\n",
    "    if n == 2:\n",
    "        axs[1].matshow(sample.reshape((28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow und Keras\n",
    "\n",
    "[Keras](https://keras.io) ist eine Open-Source-Bibliothek für Deep Learning, die in Python geschrieben wurde.\n",
    "Keras unterstützt verschiedene Machine Learning Backends, unter anderem auch [Tensorflow](https://www.tensorflow.org/). Der Vorteil von Keras ist, ist dass die API sehr sauber und intuitiv ist.\n",
    "\n",
    "Ab 2017 hat Google begonnen die Keras-API in Tensorflow zu integrieren, daher kommen die Module im Namespace `tf.keras`.\n",
    "\n",
    "Seit 2019 werden andere Backends als Tensorflow (sprich Theano oder CNTK) von Keras nicht mehr unterstützt.\n",
    "\n",
    "Seit Dezember 2019 empfiehlt das Keras Team auf Tensorflow 2 mit tf.keras zu setzen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klassifikation der MNIST Ziffern\n",
    "\n",
    "Wir laden uns die 60.000 Trainings- und 10.000 Test-Bilder im Format 28x28.\n",
    "\n",
    "**Beachte:** Die Daten werden einmalig heruntergeladen und lokal persistiert - das erste Mal kann abhängig von Bandbreite etwas länger dauern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Construct a tf.data.Dataset\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "ds_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten vorbereiten\n",
    "\n",
    "Ein TensorFlow **`tf.data.Dataset`** dient der Verwaltung von potentiell gewaltigen Datenmengen.\n",
    "\n",
    "Die `tf.data.Dataset` API unterstützt das Schreiben sprechender und effizienter Eingabe-Pipelines. Die Dataset-Verwendung folgt einem gemeinsamen Muster:\n",
    "\n",
    "-  Quelldatensatz aus den Eingabedaten erstellen\n",
    "-  Ggf. Dataset-Transformationen zur Vorverarbeitung der Daten anwenden\n",
    "-  Generator über den Datensatz erzeugen\n",
    "-  Iterieren mittels des Generators\n",
    "\n",
    "Die Iteration erfolgt in einem Streaming-Verfahren, so dass der vollständige Datensatz nicht in den Speicher passen muss.\n",
    "\n",
    "**Einige Methoden:**\n",
    "- `repeat(count)`: Wiederholt diesen Datensatz, so dass jeder Originalwert `count`-mal gesehen wird. Default: unendlich\n",
    "- `shuffle(buffer_size)`: Mischt die Elemente dieses Datensatzes nach dem Zufallsprinzip um.\n",
    "\n",
    "    Dieser Datensatz füllt einen Puffer mit buffer_size-Elementen, nimmt dann nach dem Zufallsprinzip Stichproben von Elementen aus diesem Puffer und ersetzt die ausgewählten Elemente durch neue Elemente. Für ein perfektes Mischen ist eine Puffergröße erforderlich, die größer oder gleich der vollen Größe des Datensatzes ist.\n",
    "\n",
    "    Wenn Ihr Datensatz beispielsweise 10.000 Elemente enthält, buffer_size jedoch auf 1.000 eingestellt ist, wählt die Zufallsmischung zunächst ein Zufallselement aus nur den ersten 1.000 Elementen im Puffer aus. Sobald ein Element ausgewählt ist, wird sein Platz im Puffer durch das nächste (d. h. das 1.001-st.) Element ersetzt, wobei der Puffer mit 1.000 Elementen beibehalten wird.\n",
    "\n",
    "- `batch(batch_size)`: Sammelt aufeinander folgende Elemente dieses Datensatzes zu Batch-Stapeln.\n",
    "- `prefetch(buffer_size)`: Erstellt ein Dataset, das Elemente aus diesem Dataset vorzeitig abruft.\n",
    "\n",
    "    Die meisten Dataset-Eingabe-Pipelines sollten mit einem Aufruf von `prefetch` enden. Auf diese Weise können spätere Elemente vorbereitet werden, während das aktuelle Element verarbeitet wird. Dies verbessert oft die Latenzzeit und den Durchsatz auf Kosten der Verwendung von zusätzlichem Speicher zum Speichern vorgeladener Elemente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Grauwerte müssen wir zunächst von [0 ... 255] zu [0.0 ... 1.0] konvertieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "ds_train_scaled = ds_train.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test_scaled = ds_test.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes erzeugen wir unseren Datengenerator, über den das Training nachher iteriert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ds_train_scaled.shuffle(1024, reshuffle_each_iteration=True).batch(64).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ebenso erzeugen wir unseren Test-Generator, allerdings hier ohne Shufflen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ds_test_scaled.batch(64).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell vorbereiten\n",
    "\n",
    "Ein Keras-Modell hat einen Lebenszyklus, und dieses Wissen bildet das Rückgrat für das Verständnis der tf.keras-API.\n",
    "\n",
    "Die fünf Schritte im Lebenszyklus sind wie folgt:\n",
    "\n",
    "-  Definieren:\n",
    "   Netzarchitektur entwerfen\n",
    "-  Kompilieren: Interne Datenstrukturen werden für die effiziente Verwendung vorbereitet\n",
    "-  Fitten: Der rechenintensive Teil: das Trainieren des Modells\n",
    "-  Evaluieren: Validierung des Modells gegen das Dev-Set, Optimierung der Hyperparameter, Test\n",
    "-  Verwenden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition des neuronalen Netzes\n",
    "\n",
    "Für unser erstes Modell verwenden wir die \"Sequential API\" - dies ist die einfachste und intuitivste Methode: unser Modell konstruieren wir aus einer Sequenz von Layern.\n",
    "\n",
    "Im **Inputlayer** wandeln wir die 28x28 Pixel in einen 784-Vektor um.\n",
    "\n",
    "Der **erste hidden Layer** ist ein voll verbundener Layer mit 128 Nodes und ReLU-Aktivierung. \"input_shape\" muss nur beim Input-Layer angegeben werden, danach weiß Tensorflow ja selbst, was die Ausgabe des vorigen und damit die Eingabe des Folgelayers ist.\n",
    "\n",
    "Der **zweite hidden Layer** ist ein **Dropout**-Layer. Der Parameter 0.2 besagt, dass das Netz in jeder Iteration zufällig 20% der Weights vergisst. Dies ist eine Form der Regularisierung (reduziert das Auswendiglernen von Trainingsdaten).\n",
    "\n",
    "Der **Output-Layer** schließlich ist ein voll verbundener Layer mit 10 Nodes für unsere 10 Klassen. Eine Aktivierungsfunktion ist an diesem Layer nicht angegeben, daher wird die Default-Aktivierung verwendet, das ist schlicht die Identity-Funktion $f(x)=x$.\n",
    "\n",
    "D.h. unser Outputlayer bildet einfach eine gewichtete Summen über die 128 Nodes des vorherigen Layers.\n",
    "\n",
    "Mit den Bezeichnungen unserer Folien ergibt sich daher (wenn wir die Ausgabe des Flatten-Layer als Inputlayer betrachten):\n",
    "\n",
    "$$a^{[1]}=ReLU({w^{[1]}}^Tx+b^{[1]})$$\n",
    "$$a^{[2]}=Dropout(a^{[1]})$$\n",
    "$$a^{[3]}={w^{[3]}}^Ta^{[2]}+b^{[3]}=\\hat{y}$$\n",
    "\n",
    "mit $x \\in \\mathbb{R}^{128}$ und $\\hat{y} \\in \\mathbb{R}^{10}$.\n",
    "\n",
    "**Dropout** führt dazu, dass 20% der $a^{[1]}$ auf 0 gesetzt und die anderen proportional erhöht werden, so dass $\\sum a^{[1]}=\\sum a^{[2]}$ gilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "print(model.summary())\n",
    "print(f'Anzahl Parameters Layer Dense 128: #a * #w + #b = 784 * 128 + 128 = {784 * 128 + 128}')\n",
    "print(f'Anzahl Parameters Layer Dense 10:  #a * #w + #b = 128 * 10 + 10 = {128 * 10 + 10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ds_train_scaled.as_numpy_iterator()\n",
    "[sample, label] = next(samples)\n",
    "sample.shape, label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für jedes Sample gibt das Modell einen Vektor von \"Logits\" oder \"Log-Odds\" zurück, einen für jede Klasse.\n",
    "\n",
    "Tensorflow rechnet mit sogenannten **\"Tensoren\"**. Dies sind schlicht Datenstrukturen, die sowohl Skalare, als auch Vektoren und multidimensionale Matrizen abbilden können. Die Dimension eines Tensors heißt \"Rank\". D.h. ein Skalar ist ein Tensor von Rank=0, ein Vektor hat Rank=1, u.s.w.\n",
    "\n",
    "Wenn wir uns den Wert eines Tensors anschauen wollen, müssen wir die Methode `.numpy()` auf dem Tensor aufrufen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_prediction = model(tf.convert_to_tensor([sample]).numpy())\n",
    "print('Logit-Werte des untrainierten Modells für das erste Bild:')\n",
    "printValues(untrained_prediction, scale=\"logit\", sample=sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die **Softmax Funktion** für einen n-dimensionalen Vektor $z$ ist definiert als:\n",
    "$$\\sigma(z)_j=\\frac{e^{z_i}}{\\sum\\limits_{i=0}^n e^{z_i}}$$\n",
    "\n",
    "Die Werte der $\\sigma(z)_j$ für jedes $j$ liegen dabei zwischen 0 und 1, und die Summe der $\\sigma(z)_j$ addiert sich zu 1.\n",
    "\n",
    "Der Wert kann daher als Wahrscheinlichkeit für die jeweilige Klasse $j$ interpretiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Untrainierte Wahrscheinlichkeit für die Klassen bzgl des ersten Bildes:')\n",
    "printValues(tf.nn.softmax(untrained_prediction).numpy(), sample=sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der **SparseCategoricalCrossentropy**-Verlust nimmt einen Vektor von Logits und einen True-Index und gibt für jedes Sample den skalaren Verlust aus.\n",
    "\n",
    "Dieser Verlust ist gleich der negativen Log-Wahrscheinlichkeit der wahren Klasse: Sie ist Null, wenn das Modell sicher ist, dass es die richtige Klasse hat.\n",
    "\n",
    "Das verallgemeinert die Loss-Funktion, die wir aus der logistischen Regression kennen\n",
    "$$-y log(\\hat{y})-(1-y) log(1-\\hat{y})$$\n",
    "auf den Multi-Label Fall.\n",
    "\n",
    "**Beachte:** SparseCategoricalCrossentropy ist für den Fall, wo die Multi-Label direkt über ihren Index angesprochen werden, CategoricalCrossentropy leistet dasselbe für One-hot encoded Labels, also statt `5` im ersten Fall, `[0, 0, 0, 0, 0, 1, 0,..., 0]` im letzteren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unser **untrainiertes Modell** gibt Wahrscheinlichkeiten nahe dem Zufallsprinzip an (1/10 für jede Klasse), so dass der Anfangsverlust nahe bei $-log(1/10) \\approx 2.3$ liegen sollte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Für den True-Index {label} erhalten wir den Loss {loss_fn(label, untrained_prediction).numpy()} für das untrainierte Modell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilation des Modells\n",
    "\n",
    "Der `compile`-Step konfiguriert das Modell für das Training: in diesem Schritt werden die Loss-Funktion, der Optimizer sowie eine auszuweisende Metrik festgelegt.\n",
    "\n",
    "\"adam\" ist ein optimiertes Gradient Descent-Verfahren, wo die Learning Rate im Laufe der Iterationen angepasst wird.\n",
    "\n",
    "*Beachte:* 'accuracy' im Zusammenspiel mit dem Loss `SparseCategoricalCrossentropy` ist `tf.keras.metrics.SparseCategoricalAccuracy` nicht `tf.metrics.Accuracy`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training des Modells\n",
    "\n",
    "Die Methode `Model.fit` passt die Modellparameter an, um den Verlust zu minimieren.\n",
    "\n",
    "Es werden je Epoche Zwischenergebnisse ausgegeben. Eine **Epoche** ist ein Durchlauf durch die Trainingsdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "hist = model.fit(train, epochs=epochs, verbose=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unter den Epochen zeigt Tensorflow die Anzahl der Iterationsschritte pro Epoche an. Da wir eine\n",
    "Batch-Size von 64 festgelegt hatten, bekommen wir 938 Schritte pro Epoche: wir benötigen 938\n",
    "Minibatch Schritte bei einer Batchsize von 64, um die 60.000 Fälle abzudecken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20,8))\n",
    "x = np.arange(0, epochs)\n",
    "ax[0].plot(x, hist.history['loss'], c='r')\n",
    "ax[0].scatter(0,0, s=0.1)\n",
    "ax[0].set_title('Loss', fontsize=18)\n",
    "ax[0].set_xlabel('Epoche')\n",
    "ax[1].plot(x, hist.history['accuracy'], c='g')\n",
    "ax[1].scatter(0,0, s=0.1)\n",
    "ax[1].set_title('Accuracy', fontsize=18)\n",
    "ax[1].set_xlabel('Epoche');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation des Modells\n",
    "\n",
    "Mit `Model.evaluate` überprüfen wir jetzt die Performanz des Modells mit unserem Test-Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(loss, accuracy) = model.evaluate(test, verbose=1)\n",
    "print(f'\\nLoss: {loss:.4f}, Accuracy: {accuracy*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "Wir könnten den Softmax-Layer auch direkt in unser Modell integrieren, um als Ausgabe direkt eine Wahrscheinlichkeit für die jeweilige Klasse zu erhalten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Softmax()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ds_test_scaled.batch(1)\n",
    "for idx, sample in enumerate(samples):\n",
    "    if idx > 10:\n",
    "        break    \n",
    "    printValues(probability_model(sample), sample=sample[0].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('dhbw20': conda)",
   "language": "python",
   "name": "python37664bitdhbw20conda4e9f90a409f24fc4a1b2fdf8774acce3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
