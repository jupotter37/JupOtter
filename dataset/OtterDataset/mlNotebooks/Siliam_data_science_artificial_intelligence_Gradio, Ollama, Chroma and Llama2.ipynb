{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc352bf4-bfe2-461c-a668-aec6160a94d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "     ---------------------------------------- 0.0/86.0 kB ? eta -:--:--\n",
      "     ---- ----------------------------------- 10.2/86.0 kB ? eta -:--:--\n",
      "     ------------- ------------------------ 30.7/86.0 kB 330.3 kB/s eta 0:00:01\n",
      "     ------------------ ------------------- 41.0/86.0 kB 326.8 kB/s eta 0:00:01\n",
      "     ------------------------------- ------ 71.7/86.0 kB 393.8 kB/s eta 0:00:01\n",
      "     -------------------------------------- 86.0/86.0 kB 440.2 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.36.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\harri\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.1.1+cu121)\n",
      "Requirement already satisfied: torchvision in c:\\users\\harri\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.16.1+cu121)\n",
      "Requirement already satisfied: numpy in c:\\users\\harri\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.26.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\harri\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\harri\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\harri\\anaconda3\\lib\\site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\harri\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.19.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\harri\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.9.2)\n",
      "Requirement already satisfied: requests in c:\\users\\harri\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\harri\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\harri\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\harri\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.2)\n",
      "Requirement already satisfied: click in c:\\users\\harri\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\harri\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers) (10.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\harri\\anaconda3\\lib\\site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125960 sha256=e92a1c4ced1d64b34d5b0b481c059f693262546f610be565bbfed791b7df0b99\n",
      "  Stored in directory: c:\\users\\harri\\appdata\\local\\pip\\cache\\wheels\\ff\\27\\bf\\ffba8b318b02d7f691a57084ee154e26ed24d012b0c7805881\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-2.2.2\n"
     ]
    }
   ],
   "source": [
    "# ! pip3 install langchain\n",
    "# ! pip3 install pypdf\n",
    "# ! pip3 install cohere\n",
    "# ! pip3 install chromadb\n",
    "# ! pip3 install typing_extensions\n",
    "# ! pip3 install fsspec==2023.9.2\n",
    "# ! pip3 install llama_index\n",
    "# ! pip3 install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f1304e6-81f2-463e-af8a-672a60e0ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dd7b455-7b25-4c9f-8852-4603bf680495",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'llama2:latest' #You can replace the model name if needed\n",
    "context = [] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc3eda2-f8e5-469b-b4ee-566419ad8021",
   "metadata": {},
   "source": [
    "# Through the Ollama API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3d55ada-6bdc-40e3-9b43-73232b7278c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, context, top_k, top_p, temp):\n",
    "    r = requests.post('http://localhost:11434/api/generate',\n",
    "                     json={\n",
    "                         'model': model,\n",
    "                         'prompt': prompt,\n",
    "                         'context': context,\n",
    "                         'options':{\n",
    "                             'top_k': top_k,\n",
    "                             'temperature':top_p,\n",
    "                             'top_p': temp\n",
    "                         }\n",
    "                     },\n",
    "                     stream=False)\n",
    "    r.raise_for_status()\n",
    "\n",
    " \n",
    "    response = \"\"  \n",
    "\n",
    "    for line in r.iter_lines():\n",
    "        body = json.loads(line)\n",
    "        response_part = body.get('response', '')\n",
    "        print(response_part)\n",
    "        if 'error' in body:\n",
    "            raise Exception(body['error'])\n",
    "\n",
    "        response += response_part\n",
    "\n",
    "        if body.get('done', False):\n",
    "            context = body.get('context', [])\n",
    "            return response, context\n",
    "\n",
    "\n",
    "\n",
    "def chat(input, chat_history, top_k, top_p, temp):\n",
    "\n",
    "    chat_history = chat_history or []\n",
    "\n",
    "    global context\n",
    "    output, context = generate(input, context, top_k, top_p, temp)\n",
    "\n",
    "    chat_history.append((input, output))\n",
    "\n",
    "    return chat_history, chat_history\n",
    "  #the first history in return history, history is meant to update the \n",
    "  #chatbot widget, and the second history is meant to update the state \n",
    "  #(which is used to maintain conversation history across interactions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97a1e1ea-a502-4314-9f88-19ca16a80968",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "!\n",
      " It\n",
      "'\n",
      "s\n",
      " nice\n",
      " to\n",
      " meet\n",
      " you\n",
      ".\n",
      " Is\n",
      " there\n",
      " something\n",
      " I\n",
      " can\n",
      " help\n",
      " you\n",
      " with\n",
      " or\n",
      " would\n",
      " you\n",
      " like\n",
      " to\n",
      " chat\n",
      "?\n",
      "\n",
      "Like\n",
      "wise\n",
      "!\n",
      " It\n",
      "'\n",
      "s\n",
      " always\n",
      " great\n",
      " to\n",
      " connect\n",
      " with\n",
      " new\n",
      " people\n",
      ".\n",
      " How\n",
      " are\n",
      " you\n",
      " today\n",
      "?\n",
      " Is\n",
      " there\n",
      " anything\n",
      " on\n",
      " your\n",
      " mind\n",
      " that\n",
      " you\n",
      "'\n",
      "d\n",
      " like\n",
      " to\n",
      " talk\n",
      " about\n",
      " or\n",
      " ask\n",
      "?\n",
      " I\n",
      "'\n",
      "m\n",
      " here\n",
      " to\n",
      " listen\n",
      " and\n",
      " help\n",
      " if\n",
      " I\n",
      " can\n",
      ".\n",
      "\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = gr.Blocks()\n",
    "\n",
    "\n",
    "with block:\n",
    "\n",
    "    gr.Markdown(\"\"\"<h1><center> Jarvis </center></h1>\n",
    "    \"\"\")\n",
    "\n",
    "    chatbot = gr.Chatbot()\n",
    "    message = gr.Textbox(placeholder=\"Type here\")\n",
    "\n",
    "    state = gr.State()\n",
    "    with gr.Row():\n",
    "        top_k = gr.Slider(0.0,100.0, label=\"top_k\", value=40, info=\"Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)\")\n",
    "        top_p = gr.Slider(0.0,1.0, label=\"top_p\", value=0.9, info=\" Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)\")\n",
    "        temp = gr.Slider(0.0,2.0, label=\"temperature\", value=0.8, info=\"The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8)\")\n",
    "\n",
    "\n",
    "    submit = gr.Button(\"SEND\")\n",
    "\n",
    "    submit.click(chat, inputs=[message, state, top_k, top_p, temp], outputs=[chatbot, state])\n",
    "\n",
    "\n",
    "block.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5472afe0-2e3c-4327-80b1-ef68483d0d76",
   "metadata": {},
   "source": [
    "# Through LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b471d4d5-8977-491d-90fb-184e076d15f6",
   "metadata": {},
   "source": [
    "### Loading a document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbef62a-8944-4ef5-9ae8-1c4a846f957e",
   "metadata": {},
   "source": [
    "### Web page"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f8b7388-ac24-449c-9cbe-a1414e64e09c",
   "metadata": {},
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://www.gutenberg.org/files/1727/1727-h/1727-h.htm\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1c2cb7-8baf-401b-bf48-4a6ef0399afd",
   "metadata": {},
   "source": [
    "### PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80a17c2a-7490-4c69-b689-d63c382f0380",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('falcon-paper.pdf')\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0845c153-449b-454c-803b-1e17c656c2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c19de77-9d2f-436e-81d2-feb2e81eb4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = pages[:39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a475b8f-e954-4feb-9dda-004597cbf9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PHD THESIS\\nIn Partial Fulﬁlment of the Requirements for the\\nDegree of Doctor of Philosophy from Sorb'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].page_content[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "572b847a-9590-468a-9b63-730e6af086a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb7b2ba9-7905-4756-a0ea-2dc68d367e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=0, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9dabd70b-c974-4418-bc3d-b71e3509253c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PHD THESIS\n",
      "In Partial Fulﬁlment of the Requirements for the\n",
      "Degree of Doctor of Philosophy from Sorb\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents.base import Document\n",
    "# docs = r_splitter.split_documents(pages)\n",
    "paper_content = ' '.join([p.page_content for p in pages])\n",
    "print(paper_content[:100])\n",
    "docs = r_splitter.split_documents([Document(page_content=paper_content)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47693cd9-645e-40fa-a07d-626d0a2076c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='PHD THESIS\\nIn Partial Fulﬁlment of the Requirements for the\\nDegree of Doctor of Philosophy from Sorbonne University\\nSpecialization: Data Science\\nRepresentation, Information Extraction, and\\nSummarization for Automatic Multimedia\\nUnderstanding\\nIsmail HARRANDO\\nDefended on XX/XX/2022 before a committee composed of:\\nReviewer Johanna BJÖRKLUND , Umeå University, Umeå Sweden\\nReviewer Andreas Lothe OPDAHL , University of Bergen, Bergen, Norway\\nExaminer Paolo PAPOTTI , EURECOM, Sophia Antipolis, France'\n",
      "page_content='we also deﬁne 3 new classes and 10 new properties1. The MeMAD ontology provides mappings\\nbetween the legacy metadata models of INA and Yle with the standard EBUCore data model\\nand could therefore be used by those industries to improve their metadata interoperability\\nsystems. The labels of classes and properties are provided in both English and French.\\n1The list can be accessed through the following link: https://data.memad.eu/ontology.\\n21'\n"
     ]
    }
   ],
   "source": [
    "print(docs[0])\n",
    "print(docs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822019fd-1f55-43b8-886e-f2593cfe140d",
   "metadata": {},
   "source": [
    "### embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab6d01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings import LangchainEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "315cdc37-e546-4ae6-8b3a-6f4a244ef198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings model: sentence-transformers/all-MiniLM-L12-v2 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07cb14e3a2ee4080929ab8841e84ba45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d45767bc574ba3ae21cb01c06ea21e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a08233cf1f41af88364459ce9ebf50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009d223e285e43fb963c9deef912da7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/573 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5814d6ca687447f4a72a69b7542ea7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f1063ce5f94258aacbc61307b4683c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e700dad7ddf3454aa50336578cea9f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2185243dd374766954093ebb3fb2e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f52cffdcc1548788cf3e8e1000e84ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8171c8e87b5e4a72943e55e89dac474f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cedef34fe6664fb59c76b3b09a47dca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/352 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5468dff56e4e0f9c1c63474759e142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300b8ced21bc45a2bb38e99f576bbe23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15ea5967cd14a78924f4bc34ac3ac10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EMBEDDING_MODEL_NAME = 'sentence-transformers/all-MiniLM-L12-v2'\n",
    "# EMBEDDING_MODEL_NAME = 'OrdalieTech/Solon-embeddings-large-0.1'\n",
    "\n",
    "print(f\"Loading embeddings model: {EMBEDDING_MODEL_NAME} ...\")\n",
    "\n",
    "embedding_model = LangchainEmbedding(HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    encode_kwargs = {\"normalize_embeddings\": False}\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a6e449-12ec-4b02-9b2f-2cdd60d1ecfc",
   "metadata": {},
   "source": [
    "### llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71db6446-3cf5-4bff-a784-ca11b713664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama2\")\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama2\",\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "073d953f-cae5-4f9e-925c-703ba1406629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Artificial intelligence (AI) has a rich and varied history that spans several decades. Here is a brief overview of some of the key milestones in the development of AI:\n",
      "\n",
      "1. 1950s: The Dartmouth Conference - The field of AI was founded at a conference held at Dartmouth College in 1956. Attendees included computer scientists, mathematicians, and cognitive scientists who shared an interest in exploring the possibilities of creating machines that could simulate human intelligence.\n",
      "2. 1951: The Turing Test - British mathematician Alan Turing proposed a test to measure a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human. The Turing Test has since become a benchmark for measuring the success of AI systems.\n",
      "3. 1956: The First AI Program - Computer scientist John McCarthy created the first AI program, called the Logical Theorist, which was designed to reason and solve problems using logical deduction.\n",
      "4. 1960s: Rule-Based Expert Systems - The development of rule-based expert systems, which used a set of rules to reason and make decisions, marked a significant milestone in the history of AI. These systems were widely used in industries such as banking and healthcare.\n",
      "5. 1970s: Machine Learning - Machine learning, which enables machines to learn from data without being explicitly programmed, emerged as a major area of research in AI. This led to the development of algorithms such as decision trees and neural networks.\n",
      "6. 1980s: Expert Systems - The development of expert systems, which were designed to mimic the decision-making abilities of human experts, reached its peak in the 1980s. These systems were widely used in industries such as banking and healthcare.\n",
      "7. 1990s: AI Winter - Despite the progress that had been made in AI research, the field experienced a decline in funding and interest in the 1990s, which became known as the \"AI winter.\"\n",
      "8. 2000s: Reemergence of AI - The reemergence of AI in the 2000s was driven by advances in computing power, data storage, and machine learning algorithms. This led to the development of applications such as speech recognition, image recognition, and natural language processing.\n",
      "9. 2010s: Deep Learning - The emergence of deep learning, a subset of machine learning that uses neural networks with multiple layers, has led to significant advances in AI research. Applications include image and speech recognition, natural language processing, and autonomous vehicles.\n",
      "10. Present Day: AI for Social Good - Today, there is a growing interest in using AI for social good, such as improving healthcare outcomes, reducing carbon emissions, and promoting economic development.\n",
      "\n",
      "This is just a brief overview of the history of AI, and there are many other important milestones and developments that have shaped the field."
     ]
    }
   ],
   "source": [
    "_ = llm(\"Tell me about the history of AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0514a-bba8-4eb4-8229-af0249ff5a7d",
   "metadata": {},
   "source": [
    "### llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37b6c1e4-60fd-479c-b792-15358612ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "\n",
    "document = Document(text=paper_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ef7f51a-4f50-44d8-ad6b-2fc3af1a8bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "from llama_index import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, embed_model=embedding_model\n",
    ")\n",
    "index = VectorStoreIndex.from_documents([document],\n",
    "                                        service_context=service_context)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ec67320-1ec2-410a-9bac-6d50944a5700",
   "metadata": {},
   "source": [
    "from llama_index import ServiceContext, VectorStoreIndex, StorageContext\n",
    "from llama_index.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor\n",
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "\n",
    "def build_sentence_window_index(\n",
    "    document, llm, vector_store, embed_model=\"local:BAAI/bge-small-en-v1.5\"\n",
    "):\n",
    "    node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "        window_size=3,\n",
    "        window_metadata_key=\"window\",\n",
    "        original_text_metadata_key=\"original_text\",\n",
    "    )\n",
    "    sentence_context = ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "        node_parser=node_parser\n",
    "    )\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    sentence_index = VectorStoreIndex.from_documents(\n",
    "        [document], service_context=sentence_context, storage_context=storage_context\n",
    "    )\n",
    "\n",
    "    return sentence_index\n",
    "\n",
    "def get_sentence_window_query_engine(\n",
    "    sentence_index,\n",
    "    similarity_top_k=6,\n",
    "    rerank_top_n=2,\n",
    "):\n",
    "    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\"\n",
    "    )\n",
    "\n",
    "    sentence_window_engine = sentence_index.as_query_engine(\n",
    "        similarity_top_k=similarity_top_k, node_postprocessors=[postproc, rerank]\n",
    "    )\n",
    "    return sentence_window_engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d33cecd-b78b-4fa7-988a-31877866c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2969e49-1acd-49ab-934a-8109911566cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harri\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardware scalability is an important aspect of building a large-scale knowledge graph. As the size of the graph grows, it becomes increasingly challenging to scale the system using traditional hardware. Here are some strategies that can help improve hardware scalability:\n",
      "\n",
      "1. Distributed architecture: Design the system to be distributed across multiple nodes, each node handling a subset of the graph. This allows for better scalability and fault tolerance.\n",
      "2. Parallel processing: Use parallel processing techniques such as map-reduce or multi-threading to process the large amount of data in parallel.\n",
      "3. Memory optimization: Optimize the use of memory by using compression techniques, reducing the number of redundant data, and using an efficient data structure for storing the graph.\n",
      "4. Data partitioning: Partition the graph into smaller sub-graphs that can be stored and processed independently, reducing the computational complexity and memory requirements.\n",
      "5. Graph reduction: Use graph reduction techniques such as graph clustering or graph embedding to reduce the size of the graph while preserving its important properties.\n",
      "6. Efficient algorithms: Use efficient algorithms for graph traversal and querying, such as Breadth-First Search (BFS) or Depth-First Search (DFS), to minimize the computational complexity.\n",
      "7. GPU acceleration: Utilize Graphics Processing Units (GPUs) to accelerate the computationally intensive tasks in the system, such as graph traversal and querying.\n",
      "8. Cloud computing: Use cloud computing services such as Amazon Web Services (AWS) or Google Cloud Platform (GCP) to scale the system horizontally by adding more nodes to handle the increased workload.\n",
      "9. Distributed indexing: Use a distributed indexing mechanism, such as a distributed hash table (DHT), to efficiently store and query the graph.\n",
      "10. Data pruning: Prune the graph to remove redundant or unnecessary data, reducing the size of the graph and improving its scalability.\n",
      "\n",
      "By applying these strategies, it is possible to build a large-scale knowledge graph that can handle the complexity and volume of data required for effective AI applications."
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What about Hardware scalability?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "25cceead-bb42-43dc-be94-7fdb06ea9e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents.base import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "docs = r_splitter.split_documents([Document(page_content=paper_content)])\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7efca580-f292-4dfd-8cf4-f57611d6ec3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ruled research landscape, it is also a matter of what is our current technology is allowing us\n",
      "to do: because of the need to parallelize (to somewhat extreme degrees5) all the processing\n",
      "needed for the backpropagation-fueled deep learning, the Transformers seem to be the perfect\n",
      "conduit for such convergence.\n",
      "5Megatron-Turing NLG 530B, the World’s Largest and Most Powerful Generative Language Model\n",
      "17  Chapter 3\n",
      "Multimedia Content Representation\n"
     ]
    }
   ],
   "source": [
    "query = \"What about Hardware scalability?\"\n",
    "docs = vectorstore.similarity_search(query)\n",
    "\n",
    "# print results\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4be9fddf-34a5-46b2-8e65-3cbabb40ab15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harri\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "Hardware scalability refers to the ability of a system or device to handle increased workload or processing power without significant degradation in performance. In the context of deep learning and specifically Transformers, hardware scalability is crucial for two reasons:\n",
      "\n",
      "1. Parallelization: Transformers require a large amount of computational resources to parallelize the processing needed for backpropagation-fueled training. As the size of the model increases, so does the need for more powerful hardware to keep up with the processing demands.\n",
      "2. Scalability of attention: Attention mechanisms in Transformers are shown to be Turing Complete [170], meaning they can handle any computational problem that can be solved by a Turing machine. This means that as the size of the model and the amount of data being processed increases, the hardware scalability of the system becomes even more critical.\n",
      "\n",
      "In summary, hardware scalability is essential for Transformers to handle the increasing complexity of deep learning models and the large amounts of data being processed in various applications, including multimedia content representation.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What about Hardware scalability?',\n",
       " 'result': 'Hardware scalability refers to the ability of a system or device to handle increased workload or processing power without significant degradation in performance. In the context of deep learning and specifically Transformers, hardware scalability is crucial for two reasons:\\n\\n1. Parallelization: Transformers require a large amount of computational resources to parallelize the processing needed for backpropagation-fueled training. As the size of the model increases, so does the need for more powerful hardware to keep up with the processing demands.\\n2. Scalability of attention: Attention mechanisms in Transformers are shown to be Turing Complete [170], meaning they can handle any computational problem that can be solved by a Turing machine. This means that as the size of the model and the amount of data being processed increases, the hardware scalability of the system becomes even more critical.\\n\\nIn summary, hardware scalability is essential for Transformers to handle the increasing complexity of deep learning models and the large amounts of data being processed in various applications, including multimedia content representation.'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qachain=RetrievalQA.from_chain_type(llm, retriever=vectorstore.as_retriever(), verbose=True)\n",
    "qachain({\"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3078ad05-7720-4526-a344-74c4daeaeb43",
   "metadata": {},
   "source": [
    "# Chain it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df785e3-8dc4-491e-b30d-e6144be4ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@shrinath.suresh/implementing-streaming-chatbot-with-langchain-callbacks-a-step-by-step-guide-a527a7d65b8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c2b83cc4-df1f-483f-8dfe-9468030b7b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from typing import Any\n",
    "from queue import Queue, Empty\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b5def3bc-0158-4b7a-a961-e52ceb7f04a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Queue()\n",
    "job_done = object()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cba1faa7-3f68-4f0f-ab32-876b5baac865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueueCallback(BaseCallbackHandler):\n",
    "    \"\"\"Callback handler for streaming LLM responses to a queue.\"\"\"\n",
    "\n",
    "    def __init__(self, q):\n",
    "        self.q = q\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n",
    "        self.q.put(token)\n",
    "\n",
    "    def on_llm_end(self, *args, **kwargs: Any) -> None:\n",
    "        return self.q.empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1e83285-79e1-4fd3-b5d3-12d5eba96352",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [QueueCallback(q)]\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a8eb2265-513e-4679-bec3-55234bf49c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(question):\n",
    "    def task():\n",
    "        response = llm(question)\n",
    "        q.put(job_done)\n",
    "\n",
    "    t = Thread(target=task)\n",
    "    t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf6a8741-9a19-4cec-b21c-1a5ff45e4fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.callbacks = callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "76fd82a7-099d-496c-8ab0-4934933c5467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Hello\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def user(user_message, history):\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    def bot(history):\n",
    "        question = history[-1][0]\n",
    "        print(\"Question: \", question)\n",
    "        history[-1][1] = \"\"\n",
    "        answer(question=question)\n",
    "        while True:\n",
    "            try:\n",
    "                next_token = q.get(True, timeout=1)\n",
    "                if next_token is job_done:\n",
    "                    break\n",
    "                history[-1][1] += next_token\n",
    "                yield history\n",
    "            except Empty:\n",
    "                continue\n",
    "\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(bot, chatbot, chatbot)\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.queue()\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf56c66-6e27-413f-8f93-41fd4de1ca5c",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7a5f8508-a1b5-4c0a-b387-b1db64310049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "62a19b39-ee80-4944-8fbf-42587a53dd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an smart and helpful assistant of an AI researcher. Given the following context, answer the question:\n",
    "Context:{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f346c209-4513-47ee-a023-220508e45769",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5539d757-8dea-47f3-a039-4e53c0cd751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": vectorstore.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a47c3b99-849e-4e43-b9ba-266c6220f53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_rag(question):\n",
    "    def task():\n",
    "        response = chain.invoke(question)\n",
    "        q.put(job_done)\n",
    "  \n",
    "    t = Thread(target=task)\n",
    "    t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7d5b0495-2fb6-4605-ac68-7b7ecf1292a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What is hardware scalability?\n",
      "Question:  what is special about the Falcon models?\n",
      "Question:  which embeddings are used?\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot()\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def user(user_message, history):\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    def bot(history):\n",
    "        question = history[-1][0]\n",
    "        print(\"Question: \", question)\n",
    "        history[-1][1] = \"\"\n",
    "        answer_rag(question=question)\n",
    "        while True:\n",
    "            try:\n",
    "                next_token = q.get(True, timeout=1)\n",
    "                if next_token is job_done:\n",
    "                    break\n",
    "                history[-1][1] += next_token\n",
    "                yield history\n",
    "            except Empty:\n",
    "                continue\n",
    "\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(bot, chatbot, chatbot)\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.queue()\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff7487-8ce4-4a57-acb7-e53ff55819e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
