{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 삼성전자 첨기연 시각 심화\n",
    "\n",
    "- **Instructor**: Jongwoo Lim / Jiun Bae\n",
    "- **Email**: [jlim@hanyang.ac.kr](mailto:jlim@hanyang.ac.kr) / [jiun.maydev@gmail.com](mailto:jiun.maydev@gmail.com)\n",
    "\n",
    "## NeuralNetwork Example\n",
    "\n",
    "In this example you will practice a simple neural network written by only [Numpy](https://www.numpy.org) which is fundamental package for scientific computing with Python. The goals of this example are as follows:\n",
    "\n",
    "- Understand **Neural Networks** and how they work.\n",
    "- Learn basically how to **write and use code**(*Numpy*).\n",
    "\n",
    "*If you are more familiar with PyTorch and TensorFlow(or Keras), You might be wondering why to write from the ground up with numpy instead of the built-in framework. This process is essential for understanding how a neural network works, and if you understand it, will not be too difficult to write in code.*\n",
    "\n",
    "And this example also is written in [IPython Notebook](https://ipython.org/notebook.html), an interactive computational environment, in which you can run code directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environments\n",
    "\n",
    "In this assignment, we assume the follows environments. \n",
    "\n",
    "The [Python](https://www.python.org) is a programming language that lets you work quickly and integrate systems more effectively. It is widely used in various fields, and also used in machine learning.\n",
    "\n",
    "The [Pytorch](https://pytorch.org) is an open source deep learning platform, provides a seamless path from research to production.\n",
    "\n",
    "The [Tensorflow](https://www.tensorflow.org) is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML powered applications.\n",
    "\n",
    "The [CUDA®](https://developer.nvidia.com/cuda-zone) Toolkit provides high-performance GPU-accelerated computation. In deep learning, the model takes an age to train without GPU-acceleration. ~~even with the GPU, it still takes a lot of time~~.\n",
    "\n",
    "\n",
    "- [Python3](https://www.python.org/downloads/) (recommend 3.6 or above)\n",
    "- [PyTorch](https://pytorch.org) (recommend 1.0)\n",
    "- [Tensorflow](https://tensorflow.org) (recommend above 1.13.0, but under 2.0 *There are huge difference between 2.0 and below*)\n",
    "- [NumPy](http://www.numpy.org) the fundamental package for scientific computing with Python\n",
    "\n",
    "\n",
    "- (Optional) [Anaconda](https://www.anaconda.com/distribution/#download-section), *popular Python Data Science Platform*\n",
    "- (Optional) [Jupyter](https://jupyter.org/) (Notebook or Lab)\n",
    "- (Optional) [CUDA](https://developer.nvidia.com/cuda-downloads) support GPU\n",
    "\n",
    "\n",
    "Python packages can install by `pip install [package name]` or using **Anaconda** by `conda install [package name]`.\n",
    "\n",
    "*If you are having trouble installing or something else, please contact TA or jiun.maydev@gmail.com.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages\n",
    "\n",
    "Numpy the basic scientific computing package used in customary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST dataset\n",
    "\n",
    "Tesnorflow provide mnist dataset as binary archive file [link](https://chromium.googlesource.com/external/github.com/tensorflow/tensorflow/+/r0.7/tensorflow/g3doc/tutorials/mnist/download/index.md).\n",
    "In this exampe, we already downloaded datafile in `./data` directory. So just laod dataset from `./data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.python import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data('mnist.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "def show(ary):\n",
    "    display(Image.fromarray(ary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAElEQVR4nGNgGMyAWUhIqK5jvdSy/9/rGRgYGFhgEnJsVjYCwQwMDAxPJgV+vniQgYGBgREqZ7iXH8r6l/SV4dn7m8gmCt3++/fv37/Htn3/iMW+gDnZf/+e5WbQnoXNNXyMs/5GoQoxwVmf/n9kSGFiwAW49/11wynJoPzx4YIcRlyygR/+/i2XxCWru+vv32nSuGQFYv/83Y3b4p9/fzpAmSyoMnohpiwM1w5h06Q+5enfv39/bcMiJVF09+/fv39P+mFKiTtd/fv3799jgZiBJLT69t+/f/8eDuDEkDJf8+jv379/v7Ryo4qzMDAwMAQGMjBc3/y35wM2V1IfAABFF16Aa0wAOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x62AA6B9E8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA/0lEQVR4nGNgGHhgPP/vfCMccgbv/vz58xa7nNnjv3/ev/xjyYYpxWXz4M/fP6dC/vytgggwIUnOPCDDwMBgxHOQQRdD0tibkfFQKeOL85OYGLG5ZTOPd6UoA8Pfz2gOVlv69+WFEAj775+lKHLsm/58cBeWgUkeRpG0/PPHHs5Blzz2dx+C8//vEWTX+hj834SQ/Pf/ArLG0D/PJOHWt//dxYMqeR8u1/znoTsDquREKMtg6Z+1DKgg7O9DCKPo3d9FaHIMoX9+TjKQDd308O/95RaYkn/+PL3+58+fI03oUgwMMsf//Pn758/LiZhSDAwMkg1//v7pVcUqR1cAAKxwbkTVIzd2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x62AA6B748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1ElEQVR4nGNgGArA+YU6AwMDAwMTAwMDg10gqqTpGQaEpEMQihyTohwjgndnMYqk9L9FSDqZUE2dw3AbIaknjirJz7AbIenFiSInrsjwFCGpznAVWbJH/NZnCIuFgYGBgeE0XIbPI8aNofkDsqQQAwODPpOzDFs00/eTP1nOQlUyMjAwTEv/8IiBQY/xz7drJ88cfPlEkI0BoTProRUDA8OjjddOMDAwMKSJ3mPACVb+64QxmbBIb8AnyYBHklEVj+R/JjySDJb4jMVj5/b/OB1IJQAAg3ksR3QPgSAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x62AA6BD30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAnElEQVR4nGNgGPyg5u9/e1xyCV9+/7WDMJkwJOXZcRvq8ub3ZXkO7HI2T37/jsOlcfbfv3txyYn8/f3aCYecwtm/v+twacz4/XcHPw65gA+/D4rjMvTv37/zcRk6/ffv3+o45Azu/v69BpfGV79/H+HBJfn39+9IXHLz///9K4/Lxid/v/fgCHAGh99/76CLYcYnNskbx/ApoyoAAGeYO0QsY6cRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x62AA6BD68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1ElEQVR4nN3QPwtBYRQG8EMU0e0uZLIw+QKXRZlMGC0GX8CglE0pk0VxPwQmE5YrJYPVIjYMlImSwXNiMOi97319AM/6O6fzh+g/Y5hr5mrRNByseAZba4D7EnlSN8wy3uAYXJOwDEw0ohKwD9mtxehqRLQBCnZr8GPkJ/Ll79y0m37GiIjiK2AQsGMYiIbryyvjmZO20U9gAIcjTg43GhfethOROToO+En6xRUlZhnSjd+I6BY7xVIRY79w4XapR9IOSTWWYSWUqE0xlH771R7UrULefm5U2pxVCt0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x62AA6B748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABD0lEQVR4nGNgGGSAEY3Py+Mt1vsTq1LF6Rf+/PkzCZuUxowvf/4+uPznhQaGFP+M93/+/Lkhr/rnjw2GZMKfP3/+3JRlQJJkgkuGMjA8WO36mAHJTBY4KzVt151XDAwM4ti9BQFzEcayoEjkcTP+12U4dhxTC5fp5r9////9+0QZQ4rV7PGfz09Wffrz53kpG5ocm9+fP7XWDEIX/vz58yecHVVf+58/WwQYRE///d649s+fHU6GhnA55o4/H7MEGUxP/LnhyMDnsfjjnz/34ZKZfz5FCHmu+vKnTpaBgYGBIXLLFlW45PM/X8/e+PPnTw0zFo+f//Pnz59NJSqovoZGNm+A0at5739h0Ta4AABroXIjERrLHgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x62AA6BD30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAh0lEQVR4nGNgGGAw8f9leVxyCm///nFHFmBCYr8+hKYaWfLrQzySAvp4JLnkGBhMcbqo9u+fPzm4JBnQJJlQJJkYGZG5LCiS//7jdBAGIEGSiZHRDqfSv3/+/NHCpXMGAwNDGi7JG/hcwHDr79//yjh0Mlz9//8fLmMZZqHw0CSvXcdrKx0AAOciI63Ko1kqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x62AA6BD68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABEklEQVR4nM2RMS+DURiFn/ullKXC1KWJyVId2ARBQpqUHyBRC0NjsPsPNktj0F9QEgYiIvEDJG3CYhEpMTBI2qEk5+YzfP1uuD6bwVnum3ve877n5IV/jLH8Vmittfao36fyuw8tWUmSahmPPJEUk5oGIOXIixIvNRMyNZewMZXLZQEyLame9pR6jN7iMDx9JFtevZTk+4mwdtuVdD2IN3Z0fRFmQmjvnHY9TeE+jnLs/gJXGWOMCYwxKyUXIC5u5svn78DmdrJRAIYkpwx8svizv2+5536j/UUZYfZMOYCR8pvUWXAeAWiOU+0AS5MhV9XD78pm71Kyz/sD/sqJA0nSXWOvkBAgXXlVvZL9Jd4f4xPJmHJ5CeNkqwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x62AA6B748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAc0lEQVR4nGNgGMyA1f4obkmRf88kkPlMqNIS+CQZGfBI/ufEI8lgjFPyz0cGZZySHw6jGoNuLF5JYXySfrgl9+Mz9hEDqzxOyT8MjOy43Xft3zTckhM+cuA0loHh/y88knwBuI199l0Dt85Dt77j1kktAADVQhZzhi0BcQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x62AA6BD68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA7klEQVR4nM3QsUtCURiG8QdRFIKEoCHIGtouSM4SRn9CixE0REtjS4tu0tLm4tIS4tLeFqE0FNjukIqLDrchCBq85H1Pt6Gl7vGs4bed78cDHwcWZtodgMRcqxeHzu4y+Cg78UH39rJ0twJw+NbftvHF7AD0ov2f95+DplEGKGx8ZezwIuytwtKNnlKW5V6DXeBKY7vLD1UHzj91GqfksYlMt5pee55dW92RZPpSdyLfsoMw8PcKbckonGzFsDM6AbxHGakVL89yAKV3lT1v2T4WyDbMYC4AVOSvu2xzFNac4UDN2ObXxze5dYb/N9+FeFNxEamP7gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x62AA6BCF8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for image, label, _ in zip(train_images, train_labels, range(10)):\n",
    "    print(label)\n",
    "    show(image.reshape((28, 28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "The data must be preprocessed before training the network. If you inspect the first image in the training set, you will see that the pixel values fall in the range of 0 to 255. We scale these values to a range of 0 to 1 before feeding to the neural network model. For this, we divide the values by 255. It's important that the training set and the testing set are preprocessed in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.expand_dims(train_images, -1)\n",
    "test_images = np.expand_dims(test_images, -1)\n",
    "\n",
    "train_images = train_images / 255.\n",
    "test_images = test_images / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils import to_categorical\n",
    "num_classes = 10\n",
    "train_labels = to_categorical(train_labels, num_classes)\n",
    "test_labels = to_categorical(test_labels, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "\n",
    "This is a simple two dense(fully connected) layer network. The code is quite easy.\n",
    "\n",
    "So, whole network architecture as follow:\n",
    "\n",
    "- Dense\n",
    "- Sigmoid\n",
    "- Dense\n",
    "- Sigmoid\n",
    "- Dense\n",
    "- Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    pass\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, input_units, output_units):\n",
    "        self.weights = np.random.randn(output_units, input_units) * .01\n",
    "        self.biases = np.random.randn(output_units, 1) * .1\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        return np.dot(self.weights, inputs) + self.biases\n",
    "      \n",
    "    def backward(self, grads):\n",
    "        size = np.size(grads, -1)\n",
    "\n",
    "        self.grad_weights = np.dot(grads, self.inputs.T) / size\n",
    "        self.grad_biases = np.sum(grads, axis=1, keepdims=True) / size\n",
    "\n",
    "        return np.dot(self.weights.T, grads)\n",
    "\n",
    "    def update(self, lr: float = .01):\n",
    "        # Here we perform a stochastic gradient descent step.\n",
    "        self.weights = self.weights - lr * self.grad_weights\n",
    "        self.biases = self.biases - lr * self.grad_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        return 1. / (1. + np.exp(-inputs))\n",
    "\n",
    "    def backward(self, grads):\n",
    "        r = self.forward(self.inputs)\n",
    "        return grads * r * (1. - r)\n",
    "    \n",
    "    def update(self, lr):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "def fit(networks: List[Layer], X, y, train=True, epsilon=1e-7):\n",
    "    X = np.reshape(X, (X.shape[0], -1))\n",
    "    \n",
    "    # Forward\n",
    "    preds = reduce(lambda inputs, layer: layer.forward(inputs), [X.T, *networks]).T\n",
    "    \n",
    "    # Compute Loss\n",
    "    loss = -np.sum(y * np.log(np.clip(preds, epsilon, 1. - epsilon) + epsilon)) / np.size(preds, 0)\n",
    "    \n",
    "    if train:\n",
    "        # Backward\n",
    "        grads = -(np.divide(y, preds) - np.divide(1 - y, 1 - preds))\n",
    "        grads = reduce(lambda grads, layer: layer.backward(grads), [grads.T, *reversed(networks)])\n",
    "    \n",
    "        # Update\n",
    "        for layer in networks:\n",
    "            layer.update(lr)\n",
    "    \n",
    "    return loss.mean(), preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(datasets, batch):\n",
    "    image, label = None, None\n",
    "    images, labels = datasets\n",
    "    for b, (i, l) in enumerate(zip(*datasets)):\n",
    "        if not (b % batch):\n",
    "            if image is not None and label is not None:\n",
    "                yield image, label\n",
    "            image = np.empty((batch, 28, 28, 1), dtype=np.float32)\n",
    "            label = np.empty((batch, 10), dtype=np.uint8)\n",
    "            \n",
    "        image[b % batch] = i\n",
    "        label[b % batch] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = .1\n",
    "batch = 128\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = [\n",
    "    Dense(28*28, 100),\n",
    "    Sigmoid(),\n",
    "    Dense(100, 200),\n",
    "    Sigmoid(),\n",
    "    Dense(200, 10),\n",
    "    Sigmoid(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "\tTrain Loss: 2.305125801856038\n",
      "\tTest Loss: 2.2979551816602273\n",
      "\tTest Acc: 0.103\n",
      "Epoch: 1\n",
      "\tTrain Loss: 2.3044144317430537\n",
      "\tTest Loss: 2.297525418632348\n",
      "\tTest Acc: 0.103\n",
      "Epoch: 2\n",
      "\tTrain Loss: 2.3033433392527867\n",
      "\tTest Loss: 2.2963360867401703\n",
      "\tTest Acc: 0.103\n",
      "Epoch: 3\n",
      "\tTrain Loss: 2.299853396264826\n",
      "\tTest Loss: 2.288329348381132\n",
      "\tTest Acc: 0.1902\n",
      "Epoch: 4\n",
      "\tTrain Loss: 2.219474797138108\n",
      "\tTest Loss: 1.9916116627237839\n",
      "\tTest Acc: 0.3034\n",
      "Epoch: 5\n",
      "\tTrain Loss: 1.750021704120589\n",
      "\tTest Loss: 1.458148136785472\n",
      "\tTest Acc: 0.472\n",
      "Epoch: 6\n",
      "\tTrain Loss: 1.3727509055175149\n",
      "\tTest Loss: 1.2419250145996898\n",
      "\tTest Acc: 0.6406\n",
      "Epoch: 7\n",
      "\tTrain Loss: 1.082631134529613\n",
      "\tTest Loss: 1.0283236629227681\n",
      "\tTest Acc: 0.7076\n",
      "Epoch: 8\n",
      "\tTrain Loss: 0.9477122327513593\n",
      "\tTest Loss: 0.9220482727452977\n",
      "\tTest Acc: 0.7624\n",
      "Epoch: 9\n",
      "\tTrain Loss: 0.825922282929919\n",
      "\tTest Loss: 0.7835602915349938\n",
      "\tTest Acc: 0.8152\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Train scope\n",
    "    train_loss, test_loss, test_acc = 0, 0, 0\n",
    "    for images, labels in get_batch((train_images, train_labels), batch):\n",
    "        loss, _ = fit(network, images, labels)\n",
    "        train_loss += loss\n",
    "        \n",
    "    for images, labels in get_batch((test_images, test_labels), batch):\n",
    "        loss, preds = fit(network, images, labels, train=False)\n",
    "\n",
    "        test_loss += loss\n",
    "        test_acc += (preds.argmax(axis=-1) == labels.argmax(axis=-1)).mean()\n",
    "    print(f'Epoch: {epoch}')\n",
    "    print(f'\\tTrain Loss: {train_loss / (len(train_images) / batch)}')\n",
    "    print(f'\\tTest Loss: {test_loss / (len(test_images) / batch)}')\n",
    "    print(f'\\tTest Acc: {test_acc / (len(test_images) / batch)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAxUlEQVR4nGNgGDaAEUKFpD77sfTFHeyS9xQYGBg+X4UKPuk6w8DAwMDAAuGm6l/TMnSweCzLwPDntSTDozPIOhkYGBgYBA3PmDIw/Lh1XShnGi5nBP+9KIRLTuzl/2AokwlDMlv0/U1cGq1//rPDJcfQ+m83Ky45zrM/rHBqrPu3Daec9+8PlrjkhO/+W4ZLjvn0v9vKuCTV/v3zxSUn/+BfMSMuydZ//0xwydl+QpdEClsbHoa7X1AkWZA5F53f4TIWEwAAaRE8kJuHrgAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x62AA6B710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 7, Prediction: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6ElEQVR4nGNgoAlgRDBLOPVCGKYfX4xN2cq/f//+/fv3lhwOuat9G/7+rcKUM/n195ICDwPbub89mJK+vy9JMjAwVP3464jFWHkhBgYGhot/sUoyMDAwMJR+/3uMC4ecz/e/z+2R+EwormJjWHkQh8YN3/7O58EhJ/nq70tlXK459vdvLy45vx9/9+IyVPgEHo1tf/+uxaWR4cffv5LoYixIbKHfDAwMH3+z8jMIFjIw/C3/hix5iYGBgWH1c/FwCPdFKzwlrPNHqPrzj2HTGYYjxxHJpIyVgUE7nIFh3gOGdddxuWyAAQCfcVM+FkfDOQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x62AA6B710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 2, Prediction: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAiElEQVR4nGNgGARA7V8unM2ELmn47ylune0fccvpfpmG4KAbq861ErfOU/e5ccop/LuBxEMz1p7hNW5JXYYunKZavj3LgVOns9CNHzgl9f+vwWmqxIvrKHwUnQliJ3BLyjO8x2kqw5N/Tjh12orj1sfQ++8sMy6dXF4Ma/7i0sh6bAMXHnPpBAAPgx/ARH1j7wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x62AA6B710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1, Prediction: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA70lEQVR4nMXQsWoCQRQF0JvBNNrGdjcgIR8gJJVrqRZCfkMkgn+QlEIIAcHaHzClVWySJkmXSgxqIbKCbcrLxRSbNe7M2uqr7syZefAecOTK9fTp78MLUs2ds9nJ+b71OPMfWzdXAALz9ZrSVCQpclp0bbiRpPVckmPlmUh268Ed2bDsfEVx2skCfsif9qkzxcsZAOCWYsHGDy+K/nuM2zmNuV5E6cQYc5/4+UDG0W07iTFfXlGhl45PJGelKGeQrOElgPFb8vJbqtWW0kYpG2qT8W7ZtdEP/zAcFbI2IniMsOkIAKD6zEGl6qXjweoXXfV/5XmKZEMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x62AA6B710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0, Prediction: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA2klEQVR4nGNgGLzA6AGU4SYLZTAhJN3ZoQy/bgxJFi8Y64wWN7qko+V8KEtIiwvNRt03N3mgzAN/RNEkV3w3hWn8/xdNMuTTZRiz9+9eVlTJlX+yoCyFF7+cUOX4H/6BMdv+wM2AupZdegVMRJnhCppzOM9cFIKwxP7+zYaJskCo73eDt/YxMDDoKMv/Z/iPppNBc9XXP3/+/Hnx/PefP5wwQUa4tKEyAwPDGoaF0TDTsID6P3900exEAEZGhss4Jf8jOYcJXZKD4QdOKxlevMnHLbnZCbcclQAA/k48Hcv/z+EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x62AA6B710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 4, Prediction: 4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAnElEQVR4nGNgGOzA9182My454cf//nHikgz8928pIw459tP//nni0mj6798vBI8JzVQGhl24NDIc+ffDAJec1b9/b5G4qMaaMjBMx2nq4n/vZHDJ2fz5dx+Zj2KsMBPDbnymmuKSk/nz7xKKALKxVkwMG3GamvnvlQhOnW4Mjz7ikmRVYfjxG5fkv9MMd1DtYUEw/9b8P4fTPdQEAJbDL46GK5NFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x62AA6B710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1, Prediction: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA10lEQVR4nGNgGLRA7ECbAozN78uKIif4+tdKuNydj6rIciJ7/06Gc7r/pqBodPv7VxTG1v63lhfFwpl/E+Byz//FoGhc/P8MN4yd8W8eqlMX/d0EdR9n89u/MFEWGMN714fpDAwM9g4WDGtQNTIYP/n799/fv3///vv797Yyms6zugYepa8XMjAsvshw7C4DDqD075woLjmGBX9dccqF/vtohFNy3r+lOOUYnn/BrTHj3wvcGi/8ncvAKwfnMqFJ/43e34xT57+/s2RxSNruaxBnw20rlQAAKNJLfTqR0FsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x62AA6B710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 4, Prediction: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1klEQVR4nGNgGKqAb+pBVlxy0Q/+/hXGISfz+t/fv8uEsEtO+Pvv79+/74rZsMjJf/x7Yeffv3+fS2CR9P93kIEj6fa//ycRJjPBGOz/+xl+zLv9//+3X5iSkQzeDAwMJgwMJ75gGhv294JG6LLfb/+90cKUFHr399/fvztVbvydgcVFLh/+/5vIwdD2774yNtl5fTwMDJzr/y7EIgkFEX8f4QgmBgYGpmV/63BrNfj6Vw23bPG/NZw4JUVv/dPDrVXu31Lckgy7vmAJJhjgu++HRysRAAA+/lIBnbxrFgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x62AA6B710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 9, Prediction: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6ElEQVR4nGNgoD9gxBARlGNgeFh45dZFTMXeM2/+/fv3+re/f9FllPu+/PkLAwwMDCzIkjL5EPrGVQgNlxQpOLLj18ev3LuunDz//Suqidzn/voxMCgwyDFhuoNt498WLhwe4mn9+5IfhxxDzN/7MliEIVZYMZx/gksjw6u/3+oNMYUhwff/HwPDvxkn5O5cZdA+jm5G918EeLECTZLZ9Na93zDZPzWYFjh7nIDKrsfmrIq/P2cYL8EhafT37989f/7+nYJNknP5379///5az41NkkF8y/O/dxuwSjEwMDDEThXDKUcfAAAG83bQTLLiMgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x62AA6B710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5, Prediction: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABBElEQVR4nN3QsS8DcRjG8UcOPUFiINLNYGgj2M7QpTGISSIxWRkMFomIGESCwWJ1XTtYDP4Ag60xoqNBqjW0IWlPQ/Tc9wyWa/P7/QOe7c3nfYb3lf5N+rqm6fHVPH7pybA46zcA6JQvBntortCE6uVpeMdrZavbCg24OXel20zp8zmaSJB7GFE/Gpakx5klIInLH9Q8SXKmdisBcXFAkvr/0IkULqxl9JXNvk1K9ZMw0Ry6bkcxhAD8XKV77hzbz72/pOY9Sf5B0/iTIrQ2HCNprwPrZtJmAOWU2bwWBDlL8RjaeYuNfoNvsZEa3LsWXIlh0WJ6gDObqRrX01bcibetZsovhERycinB3ycAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x62AA6B7B8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 9, Prediction: 9\n"
     ]
    }
   ],
   "source": [
    "for image, label, _ in zip(test_images, test_labels, range(10)):\n",
    "    show((image[:, :, 0] * 255.).astype(np.uint8))\n",
    "    prediction = reduce(lambda inputs, layer: layer.forward(inputs), [np.reshape(np.array(image), -1)[None, :].T, *network])\n",
    "    print (f'Label: {label.argmax(-1)}, Prediction: {prediction.argmax()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
