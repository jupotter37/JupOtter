{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harsh0487/Harsh_INFO5731_fall2024/blob/main/harsh_patel_exercise_3(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of Friday, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting **text classification or text mining task** and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features. **Your dataset must be text.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Text Features:-\n",
        "\n",
        "# 1.Bag-of-Words (BoW):-\n",
        "#     -> Represents the frequency of each word in the review.\n",
        "#     -> Helpful because it captures the overall sentiment of the review.\n",
        "\n",
        "# 2.Term Frequency-Inverse Document Frequency (TF-IDF):-\n",
        "#     -> An extension of BoW that considers the importance of each word in the dataset.\n",
        "#     -> Adjusts for the frequency of words appearing in general, providing a better sense of word importance.\n",
        "\n",
        "# 3.Sentiment-bearing Words:\n",
        "#     -> Extracts words with strong sentiment, such as \"love\", \"hate\", \"amazing\", etc.\n",
        "#     -> Captures the emotional tone of the review.\n",
        "\n",
        "\n",
        "\n",
        "# Structural Features:-\n",
        "\n",
        "# 1.Length of the Review:-\n",
        "#     -> Represents the number of words or characters in the review.\n",
        "#     -> Longer reviews might be more informative or more negative.\n",
        "\n",
        "# 2.Number of Sentences:-\n",
        "#     -> Represents the number of sentences in the review.\n",
        "#     -> Reviews with more sentences might be more detailed or more positive.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Semantic Features\n",
        "\n",
        "# 1.Named Entities:\n",
        "#     -> Extracts named entities such as movie titles, actor names, or director names.\n",
        "#     -> Captures the context of the review.\n",
        "\n",
        "# 2.Part-of-Speech (POS) Tags:\n",
        "#     -> Extracts POS tags for each word, such as noun, verb, adjective, etc.\n",
        "#     -> Captures the grammatical structure of the review."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c08f2b4-436f-4c60-b804-7583ee814968"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words Features:\n",
            "    absolutely  again  amazing  and  but  buy  disappointed  does  ever  \\\n",
            "0           1      0        0    1    0    0             0     0     0   \n",
            "1           0      0        0    0    0    0             1     0     1   \n",
            "2           0      0        0    0    1    0             0     1     0   \n",
            "3           0      0        1    0    0    0             0     0     0   \n",
            "4           0      1        0    0    0    1             0     0     0   \n",
            "\n",
            "   everyone  ...  recommend  terrible  the  this  to  very  what  will  works  \\\n",
            "0         0  ...          0         0    0     1   0     0     0     0      1   \n",
            "1         0  ...          0         0    1     1   0     1     0     0      0   \n",
            "2         0  ...          0         0    1     0   0     0     1     0      0   \n",
            "3         1  ...          1         0    0     0   1     0     0     0      0   \n",
            "4         0  ...          0         1    0     0   0     0     0     1      0   \n",
            "\n",
            "   worst  \n",
            "0      0  \n",
            "1      1  \n",
            "2      0  \n",
            "3      0  \n",
            "4      0  \n",
            "\n",
            "[5 rows x 36 columns]\n",
            "\n",
            "TF-IDF Features:\n",
            "    absolutely     again   amazing       and       but       buy  disappointed  \\\n",
            "0    0.347067  0.000000  0.000000  0.347067  0.000000  0.000000      0.000000   \n",
            "1    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000      0.327881   \n",
            "2    0.000000  0.000000  0.000000  0.000000  0.317733  0.000000      0.000000   \n",
            "3    0.000000  0.000000  0.408248  0.000000  0.000000  0.000000      0.000000   \n",
            "4    0.000000  0.420669  0.000000  0.000000  0.000000  0.420669      0.000000   \n",
            "\n",
            "       does      ever  everyone  ...  recommend  terrible       the      this  \\\n",
            "0  0.000000  0.000000  0.000000  ...   0.000000  0.000000  0.000000  0.280011   \n",
            "1  0.000000  0.327881  0.000000  ...   0.000000  0.000000  0.264532  0.264532   \n",
            "2  0.317733  0.000000  0.000000  ...   0.000000  0.000000  0.256345  0.000000   \n",
            "3  0.000000  0.000000  0.408248  ...   0.408248  0.000000  0.000000  0.000000   \n",
            "4  0.000000  0.000000  0.000000  ...   0.000000  0.420669  0.000000  0.000000   \n",
            "\n",
            "         to      very      what      will     works     worst  \n",
            "0  0.000000  0.000000  0.000000  0.000000  0.347067  0.000000  \n",
            "1  0.000000  0.327881  0.000000  0.000000  0.000000  0.327881  \n",
            "2  0.000000  0.000000  0.317733  0.000000  0.000000  0.000000  \n",
            "3  0.408248  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "4  0.000000  0.000000  0.000000  0.420669  0.000000  0.000000  \n",
            "\n",
            "[5 rows x 36 columns]\n",
            "\n",
            "Part-of-Speech Tags:\n",
            " 0    [(I, PRP), (absolutely, RB), (love, VBP), (thi...\n",
            "1    [(This, DT), (is, VBZ), (the, DT), (worst, JJS...\n",
            "2    [(It, PRP), ('s, VBZ), (okay, JJ), (,, ,), (no...\n",
            "3    [(Amazing, JJ), (quality, NN), (!, .), (Highly...\n",
            "4    [(Terrible, JJ), (experience, NN), (., .), (Wi...\n",
            "Name: reviews, dtype: object\n",
            "\n",
            "Sentiment Scores:\n",
            "      neg    neu    pos  compound\n",
            "0  0.000  0.325  0.675    0.9258\n",
            "1  0.454  0.546  0.000   -0.8173\n",
            "2  0.000  0.861  0.139    0.1154\n",
            "3  0.000  0.367  0.633    0.7836\n",
            "4  0.443  0.557  0.000   -0.6093\n",
            "\n",
            "N-grams Features:\n",
            "    absolutely love  amazing quality  and works  but it  buy again  does the  \\\n",
            "0                1                0          1       0          0         0   \n",
            "1                0                0          0       0          0         0   \n",
            "2                0                0          0       1          0         1   \n",
            "3                0                1          0       0          0         0   \n",
            "4                0                0          0       0          1         0   \n",
            "\n",
            "   ever made  expected but  experience will  fantastic and  ...  the job  \\\n",
            "0          0             0                0              1  ...        0   \n",
            "1          1             0                0              0  ...        0   \n",
            "2          0             1                0              0  ...        1   \n",
            "3          0             0                0              0  ...        0   \n",
            "4          0             0                1              0  ...        0   \n",
            "\n",
            "   the worst  this is  this product  to everyone  very disappointed  \\\n",
            "0          0        0             1            0                  0   \n",
            "1          1        1             0            0                  1   \n",
            "2          0        0             0            0                  0   \n",
            "3          0        0             0            1                  0   \n",
            "4          0        0             0            0                  0   \n",
            "\n",
            "   what expected  will not  works great  worst purchase  \n",
            "0              0         0            1               0  \n",
            "1              0         0            0               1  \n",
            "2              1         0            0               0  \n",
            "3              0         0            0               0  \n",
            "4              0         1            0               0  \n",
            "\n",
            "[5 rows x 36 columns]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "!pip install pandas scikit-learn nltk\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "data = {\n",
        "    'reviews': [\n",
        "        \"I absolutely love this product! It's fantastic and works great.\",\n",
        "        \"This is the worst purchase I have ever made. I'm very disappointed.\",\n",
        "        \"It's okay, not what I expected but it does the job.\",\n",
        "        \"Amazing quality! Highly recommend to everyone.\",\n",
        "        \"Terrible experience. Will not buy again!!!\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "def extract_bow(df):\n",
        "    vectorizer = CountVectorizer()\n",
        "    bow_features = vectorizer.fit_transform(df['reviews']).toarray()\n",
        "    return pd.DataFrame(bow_features, columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "def extract_tfidf(df):\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_features = tfidf_vectorizer.fit_transform(df['reviews']).toarray()\n",
        "    return pd.DataFrame(tfidf_features, columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "def extract_pos_tags(df):\n",
        "    pos_tags = df['reviews'].apply(lambda x: pos_tag(word_tokenize(x)))\n",
        "    return pos_tags\n",
        "\n",
        "def extract_sentiment_scores(df):\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    sentiment_scores = df['reviews'].apply(lambda x: sia.polarity_scores(x))\n",
        "    return pd.DataFrame(sentiment_scores.tolist())\n",
        "\n",
        "def extract_ngrams(df):\n",
        "    vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "    ngram_features = vectorizer.fit_transform(df['reviews']).toarray()\n",
        "    return pd.DataFrame(ngram_features, columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "bow_features = extract_bow(df)\n",
        "tfidf_features = extract_tfidf(df)\n",
        "pos_tags = extract_pos_tags(df)\n",
        "sentiment_scores = extract_sentiment_scores(df)\n",
        "ngram_features = extract_ngrams(df)\n",
        "\n",
        "print(\"Bag of Words Features:\\n\", bow_features)\n",
        "print(\"\\nTF-IDF Features:\\n\", tfidf_features)\n",
        "print(\"\\nPart-of-Speech Tags:\\n\", pos_tags)\n",
        "print(\"\\nSentiment Scores:\\n\", sentiment_scores)\n",
        "print(\"\\nN-grams Features:\\n\", ngram_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bace34c-1fcc-4fa4-9781-c647f647770b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Bag of Words Ranked Features (IG):\n",
            "amazing: 0.4583\n",
            "disappointed: 0.4583\n",
            "the: 0.4583\n",
            "does: 0.3333\n",
            "okay: 0.3333\n",
            "what: 0.3333\n",
            "everyone: 0.2083\n",
            "experience: 0.2083\n",
            "is: 0.2083\n",
            "it: 0.2083\n",
            "purchase: 0.2083\n",
            "quality: 0.2083\n",
            "works: 0.2083\n",
            "great: 0.0833\n",
            "absolutely: 0.0000\n",
            "again: 0.0000\n",
            "and: 0.0000\n",
            "but: 0.0000\n",
            "buy: 0.0000\n",
            "ever: 0.0000\n",
            "expected: 0.0000\n",
            "fantastic: 0.0000\n",
            "have: 0.0000\n",
            "highly: 0.0000\n",
            "job: 0.0000\n",
            "love: 0.0000\n",
            "made: 0.0000\n",
            "not: 0.0000\n",
            "product: 0.0000\n",
            "recommend: 0.0000\n",
            "terrible: 0.0000\n",
            "this: 0.0000\n",
            "to: 0.0000\n",
            "very: 0.0000\n",
            "will: 0.0000\n",
            "worst: 0.0000\n",
            "\n",
            "TF-IDF Ranked Features (CHI):\n",
            "but: 1.2709\n",
            "does: 1.2709\n",
            "expected: 1.2709\n",
            "job: 1.2709\n",
            "okay: 1.2709\n",
            "what: 1.2709\n",
            "it: 1.1125\n",
            "again: 0.6310\n",
            "buy: 0.6310\n",
            "experience: 0.6310\n",
            "terrible: 0.6310\n",
            "will: 0.6310\n",
            "amazing: 0.6124\n",
            "everyone: 0.6124\n",
            "highly: 0.6124\n",
            "quality: 0.6124\n",
            "recommend: 0.6124\n",
            "to: 0.6124\n",
            "absolutely: 0.5206\n",
            "and: 0.5206\n",
            "fantastic: 0.5206\n",
            "great: 0.5206\n",
            "love: 0.5206\n",
            "product: 0.5206\n",
            "works: 0.5206\n",
            "disappointed: 0.4918\n",
            "ever: 0.4918\n",
            "have: 0.4918\n",
            "is: 0.4918\n",
            "made: 0.4918\n",
            "purchase: 0.4918\n",
            "very: 0.4918\n",
            "worst: 0.4918\n",
            "the: 0.4458\n",
            "not: 0.4392\n",
            "this: 0.1367\n",
            "\n",
            "Sentiment Scores Ranked Features (Correlation):\n",
            "compound: 0.9933\n",
            "pos: 0.9676\n",
            "neg: -0.9128\n",
            "neu: -0.4855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "!pip install pandas scikit-learn nltk\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_selection import mutual_info_classif, chi2\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "data = {\n",
        "    'reviews': [\n",
        "        \"I absolutely love this product! It's fantastic and works great.\",\n",
        "        \"This is the worst purchase I have ever made. I'm very disappointed.\",\n",
        "        \"It's okay, not what I expected but it does the job.\",\n",
        "        \"Amazing quality! Highly recommend to everyone.\",\n",
        "        \"Terrible experience. Will not buy again!!!\"\n",
        "    ],\n",
        "    'label': ['positive', 'negative', 'neutral', 'positive', 'negative']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['label'])\n",
        "\n",
        "def extract_bow(df):\n",
        "    vectorizer = CountVectorizer()\n",
        "    bow_features = vectorizer.fit_transform(df['reviews']).toarray()\n",
        "    return pd.DataFrame(bow_features, columns=vectorizer.get_feature_names_out()), vectorizer\n",
        "\n",
        "bow_features, bow_vectorizer = extract_bow(df)\n",
        "\n",
        "def extract_tfidf(df):\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_features = tfidf_vectorizer.fit_transform(df['reviews']).toarray()\n",
        "    return pd.DataFrame(tfidf_features, columns=tfidf_vectorizer.get_feature_names_out()), tfidf_vectorizer\n",
        "\n",
        "tfidf_features, tfidf_vectorizer = extract_tfidf(df)\n",
        "\n",
        "def extract_pos_tags(df):\n",
        "    pos_tags = df['reviews'].apply(lambda x: pos_tag(word_tokenize(x)))\n",
        "    return pos_tags\n",
        "\n",
        "pos_tags = extract_pos_tags(df)\n",
        "\n",
        "def extract_sentiment_scores(df):\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    sentiment_scores = df['reviews'].apply(lambda x: sia.polarity_scores(x))\n",
        "    return pd.DataFrame(sentiment_scores.tolist())\n",
        "\n",
        "sentiment_scores = extract_sentiment_scores(df)\n",
        "\n",
        "def extract_ngrams(df):\n",
        "    vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "    ngram_features = vectorizer.fit_transform(df['reviews']).toarray()\n",
        "    return pd.DataFrame(ngram_features, columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "ngram_features = extract_ngrams(df)\n",
        "\n",
        "ig_scores = mutual_info_classif(bow_features, df['label'])\n",
        "bow_ranked_features = sorted(zip(bow_vectorizer.get_feature_names_out(), ig_scores), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "chi2_scores, _ = chi2(tfidf_features, df['label'])\n",
        "tfidf_ranked_features = sorted(zip(tfidf_vectorizer.get_feature_names_out(), chi2_scores), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "corr_scores = [np.corrcoef(sentiment_scores[col], df['label'])[0][1] for col in sentiment_scores.columns]\n",
        "sentiment_ranked_features = sorted(zip(sentiment_scores.columns, corr_scores), key=lambda x: abs(x[1]), reverse=True)\n",
        "\n",
        "print(\"Bag of Words Ranked Features (IG):\")\n",
        "for feature, score in bow_ranked_features:\n",
        "    print(f\"{feature}: {score:.4f}\")\n",
        "\n",
        "print(\"\\nTF-IDF Ranked Features (CHI):\")\n",
        "for feature, score in tfidf_ranked_features:\n",
        "    print(f\"{feature}: {score:.4f}\")\n",
        "\n",
        "print(\"\\nSentiment Scores Ranked Features (Correlation):\")\n",
        "for feature, score in sentiment_ranked_features:\n",
        "    print(f\"{feature}: {score:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6209bce7-97f2-4e7b-e7ba-cd90bf08c4b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Ranking of documents based on similarity to query:\n",
            "Similarity: 0.4238 - Text: I absolutely love this product! It's fantastic and works great.\n",
            "Similarity: 0.1762 - Text: This is the worst purchase I have ever made. I'm very disappointed.\n",
            "Similarity: 0.1283 - Text: Amazing quality! Highly recommend to everyone.\n",
            "Similarity: 0.0000 - Text: Terrible experience. Will not buy again!!!\n",
            "Similarity: 0.0000 - Text: It's okay, not what I expected but it does the job.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "!pip install scikit-learn nltk\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "data = {\n",
        "    'reviews': [\n",
        "        \"I absolutely love this product! It's fantastic and works great.\",\n",
        "        \"This is the worst purchase I have ever made. I'm very disappointed.\",\n",
        "        \"It's okay, not what I expected but it does the job.\",\n",
        "        \"Amazing quality! Highly recommend to everyone.\",\n",
        "        \"Terrible experience. Will not buy again!!!\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "query = \"This product is amazing and works perfectly.\"\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "documents = [query] + data['reviews']\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()\n",
        "\n",
        "ranked_indices = np.argsort(similarities)[::-1]\n",
        "\n",
        "print(\"Ranking of documents based on similarity to query:\")\n",
        "for i in ranked_indices:\n",
        "    print(f\"Similarity: {similarities[i]:.4f} - Text: {data['reviews'][i]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Learning Experience:-\n",
        "\n",
        "# 1.Feature Extraction Techniques:-\n",
        "#     -> Gained a deeper understanding of various methods such as Bag of Words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), and sentiment analysis using lexicons. Each method offers unique advantages and drawbacks, and recognizing these helps in choosing the appropriate technique for effective text classification.\n",
        "\n",
        "# 2.Dimensionality Reduction and Feature Selection:-\n",
        "#     -> Learned about feature selection techniques like Information Gain (IG) and Chi-Square tests, which are crucial for enhancing model performance by identifying relevant features. This knowledge is essential for creating models that generalize well to new data.\n",
        "\n",
        "# 3.Cosine Similarity for Text Ranking:-\n",
        "#     -> Acquired skills in calculating cosine similarity between text vectors, a fundamental technique for measuring similarity in high-dimensional spaces. This is valuable for document retrieval and recommendation systems.\n",
        "\n",
        "# 4.Practical Application of Libraries:-\n",
        "#     -> Gained hands-on experience with scikit-learn and NLTK, providing practical exposure to industry-standard tools for working with text data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Challenges Encountered\n",
        "\n",
        "# 1.Understanding BERT and Deep Learning Models:-\n",
        "#     -> Initially found it challenging to grasp how BERT operates, particularly its embeddings and the significance of the pooled output (CLS token). Transitioning from traditional methods like TF-IDF to deep learning models required a shift in perspective on text representation.\n",
        "\n",
        "# 2.Library Dependencies:-\n",
        "#     -> Encountered difficulties setting up the environment and ensuring that all necessary libraries were installed correctly. Compatibility issues with specific versions of dependencies, especially with the transformers library, were a concern.\n",
        "\n",
        "# 3.Performance Considerations:-\n",
        "#     -> Realized that computational efficiency becomes crucial with larger datasets or more complex models. Learning how to optimize code for performance while maintaining accuracy is an ongoing process.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Relevance to Your Field of Study\n",
        "\n",
        "# 1.Sentiment Analysis:-\n",
        "#     -> The skills acquired in sentiment classification help businesses understand customer feedback and improve products and services.\n",
        "\n",
        "# 2.Information Retrieval:-\n",
        "#     -> Ranking documents based on relevance to a query is essential for search engines and recommendation systems.\n",
        "\n",
        "# 3.Text Classification:-\n",
        "#     -> Effective feature extraction directly impacts the performance of machine learning models used for categorizing text into predefined classes.\n",
        "\n"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}