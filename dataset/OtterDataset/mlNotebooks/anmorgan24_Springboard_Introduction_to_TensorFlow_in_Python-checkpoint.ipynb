{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4acf1e4f",
   "metadata": {},
   "source": [
    "# Introduction to TensorFlow in Python\n",
    "Not long ago, cutting-edge computer vision algorithms couldn’t differentiate between images of cats and dogs. Today, a skilled data scientist equipped with nothing more than a laptop can classify tens of thousands of objects with greater accuracy than the human eye. In this course, you will use TensorFlow 2.6 to develop, train, and make predictions with the models that have powered major advances in recommendation systems, image classification, and FinTech. You will learn both high-level APIs, which will enable you to design and train deep learning models in 15 lines of code, and low-level APIs, which will allow you to move beyond off-the-shelf routines. You will also learn to accurately predict housing prices, credit card borrower defaults, and images of sign language gestures.\n",
    "\n",
    "**Instructor:** Isaiah Hull, senior economist at Sweden's Central Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce90705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import constant, add, ones, matmul, multiply, reduce_sum, Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "414b1d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "524c41d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x0):\n",
    "  \t# Define x as a variable with an initial value of x0\n",
    "\tx = Variable(x0)\n",
    "\twith GradientTape() as tape:\n",
    "\t\ttape.watch(x)\n",
    "        # Define y using the multiply operation\n",
    "\t\ty = multiply(x,x)\n",
    "    # Return the gradient of y with respect to x\n",
    "\treturn tape.gradient(y, x).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e78ed269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a linear regression model\n",
    "def linear_regression(intercept, slope, features):\n",
    "    return intercept + features*slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d14b3646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function to compute the MSE\n",
    "def loss_function(intercept, slope, targets, features):\n",
    "    # Compute the predictions for the linear model\n",
    "    predictions = linear_regression(intercept, slope)\n",
    "    \n",
    "    # Return the loss\n",
    "    return tf.keras.losses.mse(targets, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd49be",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 1: Introduction to TensorFlow\n",
    "Before you can build advanced models in TensorFlow 2, you will first need to understand the basics. In this chapter, you’ll learn how to define constants and variables, perform tensor addition and multiplication, and compute derivatives. Knowledge of linear algebra will be helpful, but not necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a215fba4",
   "metadata": {},
   "source": [
    "### Constants and variables\n",
    "* TensorFlow's two basic objects of computation are: **constants** and **variables**\n",
    "\n",
    "#### What is TensorFlow?\n",
    "* An open-source library for graph-based numerical computation\n",
    "    * Developed by the Google Brain Team\n",
    "* Low- and high-level APIs\n",
    "    * Addition, multiplication, differentiation\n",
    "    * Design and train machine learning models\n",
    "* Important changes in TensorFlow 2.0\n",
    "    * Eager execution enabled by default\n",
    "        * Allows users to write simpler and more intuitive code\n",
    "        * Model building with Keras and Estimators (high-level APIs)\n",
    "        \n",
    "#### What is a tensor?\n",
    "* The TensorFlow documentation describes a **tensor** as \"generalization of vectors and matrices to potentially higher dimensions.\"\n",
    "* If you're not familiar with linear algebra, think of a tensor as **a collection of numbers, which is arranged into a particular shape**.\n",
    "    * 0-dimensional: point\n",
    "    * 1-dimensional: line\n",
    "    * etc\n",
    "    \n",
    "### Defining tensors in TensorFlow\n",
    "* Each object defined below will be a `tf.Tensor object`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "842151bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# 0D Tensor\n",
    "d0 = tf.ones((1,))\n",
    "\n",
    "# 1D Tensor\n",
    "d1 = tf.ones((2,))\n",
    "\n",
    "# 2D Tensor\n",
    "d2 = tf.ones((2, 2))\n",
    "\n",
    "# 3D Tensor\n",
    "d3 = tf.ones((2, 2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d6eb35",
   "metadata": {},
   "source": [
    "If we want to print the array contained in that object, we can apply the `.numpy()` method and pass the resulting object to the print function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e192c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 1.]\n",
      "  [1. 1.]]\n",
      "\n",
      " [[1. 1.]\n",
      "  [1. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "# Print the 3D tensor\n",
    "print(d3.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a0e15a",
   "metadata": {},
   "source": [
    "### Defining constants in TensorFlow\n",
    "* A **constant** the simplest category of tensor\n",
    "* A constant does not change and cannot be trained\n",
    "    * Immutable\n",
    "    * Untrainable\n",
    "* A constant can have any dimension\n",
    "* In the code below, we've defined two constants:\n",
    "    * `a` is a 2x3 tensor of 3s\n",
    "    * `b` is a 2x2 tensor which is constructed from the 1-dimensional tensor: 1, 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5863563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import constant\n",
    "\n",
    "# Define a 2x3 constant\n",
    "a = constant(3, shape=[2, 3])\n",
    "\n",
    "# Define a 2x2 constant\n",
    "b = constant([1, 2, 3, 4], shape=[2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f1b5306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 3 3]\n",
      " [3 3 3]]\n"
     ]
    }
   ],
   "source": [
    "print(a.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f378d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "print(b.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9339337",
   "metadata": {},
   "source": [
    "* Above we worked exclusively with the constant operation\n",
    "* However, in some cases, there are more convenient options for defining certain types of special tensors\n",
    "<img src='data/convenience_functions.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdab92f",
   "metadata": {},
   "source": [
    "* Use the `.zeros` or `.ones` operations to generate a tensor of arbitrary (but defined) dimension, that is populated entirely with zeros or ones\n",
    "* Use the `zeros_like` or `ones_like` operations to populate tensors with zeros and ones, copying the dimensions of some input tensor passed to it.\n",
    "* Use the `.fill` operation to populate a tensor of arbitrary dimension with the same scalar value in each element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faf56c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_ex = tf.fill([3, 3],7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "654cab84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 7 7]\n",
      " [7 7 7]\n",
      " [7 7 7]]\n"
     ]
    }
   ],
   "source": [
    "print(fill_ex.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fe6455",
   "metadata": {},
   "source": [
    "### Defining and initializing variables\n",
    "* Unlike a constant, a variable's value can change during computation\n",
    "* The value of a variable is **shared**, **persistent**, and **modifiable**.\n",
    "* A variable's **data type and shape are fixed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf368771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# Define a variable\n",
    "a0 = tf.Variable([1, 2, 3, 4, 5, 6], dtype=tf.float32)\n",
    "a1 = tf.Variable([1, 2, 3, 4, 5, 6], dtype=tf.int16)\n",
    "\n",
    "# Define a constant\n",
    "b = tf.constant(2, tf.float32)\n",
    "\n",
    "# Compute their product\n",
    "c0 = tf.multiply(a0, b)\n",
    "c1 = a0 * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "caf9924f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  4.  6.  8. 10. 12.]\n",
      "[ 2.  4.  6.  8. 10. 12.]\n"
     ]
    }
   ],
   "source": [
    "print(c0.numpy())\n",
    "print(c1.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0177e3",
   "metadata": {},
   "source": [
    "* Note that certain TensorFlow operations, such as `tf.multiply` are overloaded, which allows us to use the simpler `a0*b` expression instead.\n",
    "\n",
    "#### Exercises: Defining data as constants\n",
    "Throughout this course, we will use `tensorflow` version 2.6.0 and will exclusively import the submodules needed to complete each exercise. This will usually be done for you, but you will do it in this exercise by importing `constant` from `tensorflow`.\n",
    "\n",
    "After you have imported `constant`, you will use it to transform a `numpy` array, `credit_numpy`, into a `tensorflow` constant, `credit_constant`. This array contains feature columns from a dataset on credit card holders and is previewed in the image below. We will return to this dataset in later chapters.\n",
    "\n",
    "Note that `tensorflow` 2 allows you to use data as either a `numpy` array or a `tensorflow` `constant` object. Using a constant will ensure that any operations performed with that object are done in `tensorflow`.\n",
    "\n",
    "```\n",
    "# Import constant from TensorFlow\n",
    "from tensorflow import constant\n",
    "\n",
    "# Convert the credit_numpy array into a tensorflow constant\n",
    "credit_constant = constant(credit_numpy)\n",
    "\n",
    "# Print constant datatype\n",
    "print('\\n The datatype is:', credit_constant.dtype)\n",
    "\n",
    "# Print constant shape\n",
    "print('\\n The shape is:', credit_constant.shape)\n",
    "```\n",
    "\n",
    "#### Exercises: Defining variables\n",
    "Unlike a constant, a variable's value can be modified. This will be useful when we want to train a model by updating its parameters.\n",
    "\n",
    "Let's try defining and printing a variable. We'll then convert the variable to a `numpy` array, print again, and check for differences. Note that `Variable()`, which is used to create a variable tensor, has been imported from `tensorflow` and is available to use in the exercise.\n",
    "\n",
    "```\n",
    "# Define the 1-dimensional variable A1\n",
    "A1 = Variable([1, 2, 3, 4])\n",
    "\n",
    "# Print the variable A1\n",
    "print('\\n A1: ', A1)\n",
    "\n",
    "# Convert A1 to a numpy array and assign it to B1\n",
    "B1 = A1.numpy()\n",
    "\n",
    "# Print B1\n",
    "print('\\n B1: ', B1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4560a9a1",
   "metadata": {},
   "source": [
    "### Basic operations\n",
    "* TensorFlow has a model of computation that revolves around the use of graphs\n",
    "* A TensorFlow graph contains edges and nodes, where the edges are tensors and the nodes are operations\n",
    "\n",
    "<img src='data/tf_operation_flow.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "### Applying the addition operator\n",
    "* We first import the constant and add operations so that we may now define 0-, 1-, and 2-dimensional tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15f927f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import constant and add from tensorflow\n",
    "# from tensorflow import constant, add\n",
    "\n",
    "# Define 0-dimensional tensors\n",
    "A0 = constant([1])\n",
    "B0 = constant([2])\n",
    "\n",
    "# Define 1-dimensional tensors\n",
    "A1 = constant([1, 2])\n",
    "B1 = constant([3, 4])\n",
    "\n",
    "# Define 2-dimensional tensors\n",
    "A2 = constant([[1, 2], [3, 4]])\n",
    "B2 = constant([[5, 6], [7, 8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9a6e7b",
   "metadata": {},
   "source": [
    "### Applying the addition operator\n",
    "* Finally, let's add them together using the operation for tensor addition\n",
    "* Note that we can perform scalar addition with `A0` and `B0`, vector addition with `A1` and `B1`, and matrix addition with `A2` and `B2`\n",
    "* The `add()` operation performs **element-wise addition** with two tensors\n",
    "* **Element-wise addition requires that both tensors have the same shape:**\n",
    "    * Scalar addition: 1 + 2 = 3\n",
    "    * Vector addition: [1, 2] + [3, 4] = [4, 6]\n",
    "    * Matrix addition:\n",
    "\n",
    "```\n",
    "A = [[1, 2],\n",
    "     [3, 4]]\n",
    "B = [[5, 6], \n",
    "     [7, 8]]\n",
    "A + B = [[6, 8],\n",
    "         [10,12]]\n",
    "```\n",
    "* Furthermore, the `add()` operator is **overloaded**\n",
    "    * We can also perform addition using the plus symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d64cdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform tensor addition with add()\n",
    "C0 = add(A0, B0)\n",
    "C1 = add(A1, B1)\n",
    "C2 = add(A2, B2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87f2cf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "[4 6]\n",
      "[[ 6  8]\n",
      " [10 12]]\n"
     ]
    }
   ],
   "source": [
    "print(C0.numpy())\n",
    "print(C1.numpy())\n",
    "print(C2.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d636bc",
   "metadata": {},
   "source": [
    "### How to perform multiplication in TensorFlow\n",
    "* We will consider both element-wise and matrix multiplication\n",
    "* **Element-wise multiplication** performed using the `multiply()` operation\n",
    "    * Tensors involved **must have the same shape**\n",
    "* **Matrix multiplication** performed with `matmul()` operator \n",
    "    * The `matmul(A, B)` operation multiplies `A` by `B`\n",
    "    * **Note** that number of columns of `A` must equal the number of rows of `B`\n",
    "    \n",
    "#### Applying the multiplication operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e61c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import operators from tensorflow\n",
    "# from tensorflow import ones, matmul, multiply\n",
    "\n",
    "# Define tensors\n",
    "A0 = ones(1)\n",
    "A31 = ones([3, 1])\n",
    "A34 = ones([3, 4])\n",
    "A43 = ones([4, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a365a521",
   "metadata": {},
   "source": [
    "* What types of operations are valid on these tensors of ones?\n",
    "    * We can perform element-wise multiplication of any element by itself\n",
    "        * `multiply(A0, A0)`, `multiply(A31, A31)`, and `multiply(A34, A34)`\n",
    "    * We can perform matrix multiplication on `matmul(A43, A34)`\n",
    "        * but **not** matmul(A43, A43)\n",
    "        \n",
    "### Summing over tensor dimensions\n",
    "* The `reduce_sum()` operator sums over the dimensions of a tensor\n",
    "* This can be used to sum over all dimensions of a tensor or just one.\n",
    "* The `reduce_sum()` operator sums over th dimensions of a tensor\n",
    "    * `reduce_sum(A)` sums over all dimensions of A\n",
    "    * `reduce_sum(A, i)` sums over dimension i "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03206e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import operations from tensorflow\n",
    "# from tensorflow import ones, reduce_sum\n",
    "\n",
    "# Define a 2x3x4 tensor of ones\n",
    "F = ones([2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cafa18",
   "metadata": {},
   "source": [
    "* If we sum over all elements of A, we get 24, since the tensor contains 24 elements, all of which are 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d647d83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum over all dimensions\n",
    "D = reduce_sum(F)\n",
    "\n",
    "# Sum over dimensions 0, 1, and 2\n",
    "D0 = reduce_sum(F, 0)\n",
    "D1 = reduce_sum(F, 1)\n",
    "D2 = reduce_sum(F, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c496d1",
   "metadata": {},
   "source": [
    "* If we sum over dimension 0, we get a 3 x 4 matrix of 2s\n",
    "* If we sum over 1, we get a 2 by 4 matrix of 3s\n",
    "* If we sum over 2, we get a 2 x3 matrix of 4s\n",
    "* In each case, we reduce the size of the tensor by summing over one of its dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "072be60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(24.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c825d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2. 2. 2. 2.]\n",
      " [2. 2. 2. 2.]\n",
      " [2. 2. 2. 2.]], shape=(3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(D0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6c4d8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[3. 3. 3. 3.]\n",
      " [3. 3. 3. 3.]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(D1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "056dd109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[4. 4. 4.]\n",
      " [4. 4. 4.]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(D2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4196715a",
   "metadata": {},
   "source": [
    "#### Exercises Performing element-wise multiplication\n",
    "Element-wise multiplication in TensorFlow is performed using two tensors with identical shapes. This is because the operation multiplies elements in corresponding positions in the two tensors. An example of an element-wise multiplication, denoted by the $\\odot$ symbol, is shown below:\n",
    "\n",
    "<img src='data/ex1_matmul.png' width=\"200\" height=\"100\" align=\"center\"/>\n",
    "\n",
    "In this exercise, you will perform element-wise multiplication, paying careful attention to the shape of the tensors you multiply. Note that `multiply()`, `constant()`, and `ones_like()` have been imported for you.\n",
    "\n",
    "```\n",
    "# Define tensors A1 and A23 as constants\n",
    "A1 = constant([1, 2, 3, 4])\n",
    "A23 = constant([[1, 2, 3], [1, 6, 4]])\n",
    "\n",
    "# Define B1 and B23 to have the correct shape\n",
    "B1 = ones_like(A1)\n",
    "B23 = ones_like(A23)\n",
    "\n",
    "# Perform element-wise multiplication\n",
    "C1 = A1 * B1\n",
    "C23 = A23 * B23\n",
    "\n",
    "# Print the tensors C1 and C23\n",
    "print('\\n C1: {}'.format(C1.numpy()))\n",
    "print('\\n C23: {}'.format(C23.numpy()))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11241a1",
   "metadata": {},
   "source": [
    "#### Exercises: Making predictions with matrix multiplication\n",
    "In later chapters, you will learn to train linear regression models. This process will yield a vector of parameters that can be multiplied by the input data to generate predictions. In this exercise, you will use input data, `features`, and a target vector, `bill`, which are taken from a credit card dataset we will use later in the course.\n",
    "\n",
    "<img src='data/mat_mult.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "The matrix of input data, `features`, contains two columns: education level and age. The target vector, `bill`, is the size of the credit card borrower's bill.\n",
    "\n",
    "Since we have not trained the model, you will enter a guess for the values of the parameter vector, `params`. You will then use `matmul()` to perform matrix multiplication of `features` by `params` to generate predictions, `billpred`, which you will compare with `bill`. Note that we have imported `matmul()` and `constant()`.\n",
    "\n",
    "```\n",
    "# Define features, params, and bill as constants\n",
    "features = constant([[2, 24], [2, 26], [2, 57], [1, 37]])\n",
    "params = constant([[1000], [150]])\n",
    "bill = constant([[3913], [2682], [8617], [64400]])\n",
    "\n",
    "# Compute billpred using features and params\n",
    "billpred = matmul(features, params)\n",
    "\n",
    "# Compute and print the error\n",
    "error = bill-billpred\n",
    "print(error.numpy())\n",
    "```\n",
    "\n",
    "### Advanced Operations\n",
    "* In this lesson, we explore advanced operations:\n",
    "    * `gradient()`\n",
    "    * `reshape()`\n",
    "    * `random()`\n",
    "    \n",
    "<img src='data/adv_ops.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* **`gradient()`:** \n",
    "    * We will use this function in conjuction with gradient tape\n",
    "    * Computes the slope of a function at a point\n",
    "* **`reshape()`:**\n",
    "    * Changes the shape of a tensor (e.g. 10x10 to 100x1)\n",
    "* **`random()`:**\n",
    "    * Generates a tensor out of randomly-drawn values\n",
    "\n",
    "#### Finding the optimum \n",
    "* In many ML problems, we will need to find the optimum (minimum or maximum) of a function \n",
    "    * **Minimum:** Lowest value of a loss function\n",
    "    * **Maximum:** Highest value of objective function\n",
    "* We can do this using the `gradient()` operation, which tells us the slope of a function at a point\n",
    "    * We start this process by passing points to the gradient operation until we find one where the gradient is zero\n",
    "    * **Optimum:** Find a point where gradient = 0\n",
    "    * **Minimum:** Change in gradient > 0 (if it is increasing, we have a minimum)\n",
    "    * **Maximum:** Change in gradient < 0 (if it is decreasing, we have a maximum)\n",
    "  \n",
    "<img src='data/fixed_gradient.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* The plot above shows the function `y = x`; notice that the gradient (the slope at a given point) is constant\n",
    "* This is not true is we instead consider the function `y = x**2` ($y=x^2$)\n",
    "    * When `x` is less than 0, `y` decreases when `x` increases\n",
    "    * When `x` is greater than 0, `y` increases when `x` increases\n",
    "    * Thus, the gradient is initially negative, but becomes positive for `x` larger than 0.\n",
    "    * This means that `x = 0` **minimizes** `y`\n",
    "\n",
    "<img src='data/varying_gradient.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecc6b49",
   "metadata": {},
   "source": [
    "### Gradients in TensorFlow\n",
    "* We define `x` as `-1.0`\n",
    "* We then define `y` as `x**2` *within an instance of gradient tape*.\n",
    "* **Note** that we apply the `watch()` method to an instance of gradient tape and then pass the variable `x`.\n",
    "* This will allow us to compute the rate of change of `y` with respect to `x`\n",
    "* Next, we compute the gradient of `y` with respect to `x` using the tape instance of gradient tape\n",
    "* **Note that y is the first argument and x is the second**\n",
    "* As written, the operation computes the slope of `y` at a point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e38e3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0\n"
     ]
    }
   ],
   "source": [
    "# Import tensorflow under the alias tf\n",
    "# import tensorflow as tf\n",
    "\n",
    "# Define x\n",
    "x = tf.Variable(-1.0)\n",
    "\n",
    "# Define y within instance of GradientTape\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = tf.multiply(x, x)\n",
    "    \n",
    "# Evaluate the gradient of y at x = -1\n",
    "g = tape.gradient(y, x)\n",
    "print(g.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39502ee3",
   "metadata": {},
   "source": [
    "* Running the code and printing we find that the slope is -2 at `x = -1`, which means that `y` is initially decreasing in `x`, as seen in the graph above. \n",
    "* Much of the differentiation you do in deep learning models will be handled by high level APIs\n",
    "* However, **gradient tape remains an invaluable tool for building advanced and custom models.**\n",
    "\n",
    "### Reshaping images as tensors\n",
    "* A tool that is particularly usseful for image classification problems: **reshaping**\n",
    "* While some algorithms allow you to exploit the shape of the original image, other require you to `reshape` matrices into vectors before using them as inputs, as shown in the diagram\n",
    "\n",
    "#### Reshaping a grayscale image\n",
    "* Below we create a random grayscale image by drawing numbers from the set of integers between 0 and 255 (grayscale pixel scale) and use these to populate a 2x2 matrix\n",
    "* We can then reshape this into a 4x1 vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c91a5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow as alias tf\n",
    "# import tensorflow as tf\n",
    "\n",
    "# Generate grayscale image\n",
    "gray = tf.random.uniform([2, 2], maxval=255, dtype='int32')\n",
    "\n",
    "# Reshape grayscale image\n",
    "gray = tf.reshape(gray, [2*2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb75094",
   "metadata": {},
   "source": [
    "<img src='data/reshape_grayscale.png' width=\"200\" height=\"100\"/>\n",
    "\n",
    "#### How to reshape a color image\n",
    "* For color images, we generate 3 such matrices to form a 2x2x3 tensor\n",
    "* We could then reshape the image into a 4x3 tensor, as shown in the diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c88dd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow as alias tf\n",
    "# import tensorflow as tf\n",
    "\n",
    "# Generate color image\n",
    "color = tf.random.uniform([2, 2, 3], maxval= 255, dtype='int32')\n",
    "\n",
    "# Reshape color image\n",
    "color = tf.reshape(color, [2*2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f13971f",
   "metadata": {},
   "source": [
    "#### Exercises: Reshaping tensors\n",
    "Later in the course, you will classify images of sign language letters using a neural network. In some cases, the network will take 1-dimensional tensors as inputs, but your data will come in the form of images, which will either be either 2- or 3-dimensional tensors, depending on whether they are grayscale or color images.\n",
    "\n",
    "The figure below shows grayscale and color images of the sign language letter A. The two images have been imported for you and converted to the numpy arrays `gray_tensor` and `color_tensor`. Reshape these arrays into 1-dimensional vectors using the `reshape` operation, which has been imported for you from `tensorflow`. Note that the shape of `gray_tensor` is 28x28 and the shape of `color_tensor` is 28x28x3.\n",
    "\n",
    "<img src='data/asl_a.png' width=\"200\" height=\"100\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe5655a",
   "metadata": {},
   "source": [
    "```\n",
    "# Reshape the grayscale image tensor into a vector\n",
    "gray_vector = reshape(gray_tensor, (28*28, 1))\n",
    "\n",
    "# Reshape the color image tensor into a vector\n",
    "color_vector = reshape(color_tensor, (28*28*3, 1))\n",
    "```\n",
    "\n",
    "#### Exercises: Optimizing with gradients\n",
    "You are given a loss function, $y = x^2$, which you want to minimize. You can do this by computing the slope using the `GradientTape()` operation at different values of `x`. If the slope is positive, you can decrease the loss by lowering `x`. If it is negative, you can decrease it by increasing `x`. This is how gradient descent works.\n",
    "\n",
    "<img src='data/varying_gradient.png' width=\"300\" height=\"150\" align=\"center\"/>\n",
    "\n",
    "In practice, you will use a high level `tensorflow` operation to perform gradient descent automatically. In this exercise, however, you will compute the slope at `x` values of -1, 1, and 0. The following operations are available: `GradientTape()`, `multiply()`, and `Variable()`.\n",
    "\n",
    "```\n",
    "def compute_gradient(x0):\n",
    "  \t# Define x as a variable with an initial value of x0\n",
    "\tx = Variable(x0)\n",
    "\twith GradientTape() as tape:\n",
    "\t\ttape.watch(x)\n",
    "        # Define y using the multiply operation\n",
    "\t\ty = multiply(x,x)\n",
    "    # Return the gradient of y with respect to x\n",
    "\treturn tape.gradient(y, x).numpy()\n",
    "\n",
    "# Compute and print gradients at x = -1, 1, and 0\n",
    "print(compute_gradient(-1.0))\n",
    "print(compute_gradient(1.0))\n",
    "print(compute_gradient(0.0))\n",
    "```\n",
    "\n",
    "#### Exercises: Working with image data\n",
    "You are given a black-and-white image of a `letter`, which has been encoded as a tensor, `letter`. You want to determine whether the letter is an X or a K. You don't have a trained neural network, but you do have a simple model, `model`, which can be used to classify `letter`.\n",
    "\n",
    "The 3x3 tensor, `letter`, and the 1x3 tensor, `model`, are available in the Python shell. You can determine whether `letter` is a K by multiplying `letter` by `model`, summing over the result, and then checking if it is equal to 1. As with more complicated models, such as neural networks, `model` is a collection of weights, arranged in a tensor.\n",
    "\n",
    "Note that the functions `reshape()`, `matmul()`, and `reduce_sum()` have been imported from `tensorflow` and are available for use.\n",
    "\n",
    "```\n",
    "# Reshape model from a 1x3 to a 3x1 tensor\n",
    "model = reshape(model, (3, 1))\n",
    "\n",
    "# Multiply letter by model\n",
    "output = matmul(letter, model)\n",
    "\n",
    "# Sum over output and print prediction using the numpy method\n",
    "prediction = reduce_sum(output)\n",
    "print(prediction.numpy())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597eaf62",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 2: Linear models\n",
    "In this chapter, you will learn how to build, solve, and make predictions with models in TensorFlow 2. You will focus on a simple class of models – the linear regression model – and will try to predict housing prices. By the end of the chapter, you will know how to load and manipulate data, construct loss functions, perform minimization, make predictions, and reduce resource use with batch training.\n",
    "\n",
    "### Input data\n",
    "In the previous chapter, we focused on how to perform core TensorFlow operations. In this chapter, we will work towards training a linear model with TensorFlow. So far we've only generated data using functions like `ones` and `random_uniform`, however when we train a machine learning model, we will (obviously) want to import data from an external source (whether numeric, image, text, or other data). Beyond simply importing the data, **numeric data will need to be assigned a type, and text and image data will need to be converted to a usable format**. While this is useful for complex data pipelines, it will be unnecessarily complicated for what we do in this chapter. \n",
    "\n",
    "#### Importing data for use in TensorFlow\n",
    "* **Data can be imported using `tensorflow`\n",
    "    * Useful for managing complex pipelines \n",
    "    * Not necessary for this chapter\n",
    "* **Simpler option used in this chapter**\n",
    "    * Import data using `pandas`\n",
    "    * Convert data to `numpy` array\n",
    "    * Use in `tensorflow` without modification\n",
    "    \n",
    "#### How to import and convert data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dabdb4c",
   "metadata": {},
   "source": [
    "```\n",
    "# Import numpy and pandas\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# Load data from csv\n",
    "housing = pd.read_csv('kc_housing.csv')\n",
    "\n",
    "# Convert to numpy array \n",
    "housing = np.array(housing)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a61a279",
   "metadata": {},
   "source": [
    "* We will focus on data stored in csv format in this chapter\n",
    "* pandas also has methods for handling data in other formats\n",
    "    * e.g. `read_json()`, `read_html()`, `read_excel()`\n",
    "    \n",
    "<img src='data/read_csv_params.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* The only required parameter to `read_csv` is the filepath or buffer\n",
    "    * **Note:** Instead of a filepath, you can also provide a URL to load data.\n",
    "* `sep` = delimiter; default is comma\n",
    "    * **Note that if you do use whitespace as a delimiter, you will also need to set the `delim_whitespace` parameter to `True`** (default is `False`).\n",
    "* Finally, if you are working with datasets that contain non-ASCII characters, you can specify the appropriate choice of encoding, so that your characters are correctly parsed.\n",
    "\n",
    "#### Using mixed type datasets\n",
    "* How to transform imported data for use in TensorFlow\n",
    "\n",
    "<img src='data/mixed_type_dfs.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "#### Setting the data type\n",
    "* Let's say we want to perform TensorFlow operations that require `price` to be a 32-bit floating point number and `waterfront` to be a boolean\n",
    "* We can do this in two ways:\n",
    "* 1.\n",
    "    * We select the relevant column in the DataFrame\n",
    "    * Provide relevant column as first argument to `np.array`\n",
    "    * Provide datatype as second argument\n",
    "\n",
    "```\n",
    "# Load KC dataset\n",
    "housing = pd.read_csv('kc_housing.csv')\n",
    "\n",
    "# Convert price column to float32\n",
    "price = np.array(housing['price'], np.float32)\n",
    "\n",
    "# Convert waterfront column to Boolean \n",
    "waterfront = np.array(housing['waterfront'], np.bool)\n",
    "```    \n",
    "    \n",
    "* 2. Cast operation from TensorFlow\n",
    "\n",
    "```\n",
    "# Load KC dataset\n",
    "housing = pd.read_csv('kc_housing.csv')\n",
    "\n",
    "# Convert price column to float32\n",
    "price = tf.cast(housing['price'], tf.float32)\n",
    "\n",
    "# Convert waterfront column to Boolean\n",
    "waterfront = tf.cast(housing['waterfront'], tf.bool)\n",
    "```\n",
    "  \n",
    "* While either `tf.cast` or `np.array` will work, `waterfront` will be a `tf.tensor` type under the former option, and a numpy array under the latter. \n",
    "\n",
    "#### Load data using pandas\n",
    "Before you can train a machine learning model, you must first import data. There are several valid ways to do this, but for now, we will use a simple one-liner from `pandas`: `pd.read_csv()`. Recall from the video that the first argument specifies the path or URL. All other arguments are optional.\n",
    "\n",
    "In this exercise, you will import the King County housing dataset, which we will use to train a linear model later in the chapter.\n",
    "\n",
    "```\n",
    "# Import pandas under the alias pd\n",
    "import pandas as pd\n",
    "\n",
    "# Assign the path to a string variable named data_path\n",
    "data_path = 'kc_house_data.csv'\n",
    "\n",
    "housing = pd.read_csv(data_path)\n",
    "\n",
    "# Print the price column of housing\n",
    "print(housing.price)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aebc01e",
   "metadata": {},
   "source": [
    "### Loss functions\n",
    "* Loss functions play a fundamental role in ML and are a fundamental `tensorflow` operation\n",
    "    * Used to train a model\n",
    "    * Measure of model fit\n",
    "* **Higher value $\\Rightarrow$ worse fit**\n",
    "    * Minimize the loss function (usually)\n",
    "        * But in some cases we may also want to maximize a loss function instead (much less common)\n",
    "        * If this is the case, we can always place a minus sign before the function we want to maximize and minimize it instead\n",
    "        * For this reason, we will always talk about **loss functions and minimization** (because even in the off-change we want to maximize a function, we can always just use the trick mentioned above)\n",
    "        \n",
    "#### Common loss functions in TensorFlow\n",
    "* TensorFlow operations for common loss functions include:\n",
    "    * Mean squared error (MSE)\n",
    "    * Mean absolute error (MAE)\n",
    "    * Huber error\n",
    "* **Loss functions are accessible from `tf.keras.losses()`**\n",
    "    * `tf.keras.losses.mse()`\n",
    "    * `tf.keras.losses.mae()`\n",
    "    * `tf.keras.losses.Huber()`\n",
    "* The loss tells us to what degree our predictions are accurate\n",
    "* Below we plot the MSE, MAE, and Huber loss for error values between -2 and 2\n",
    "\n",
    "<img src='data/common_loss_funcs.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "#### MSE\n",
    "* Strongly penalizes outliers\n",
    "* High (gradient) sensitivity near minimum \n",
    "\n",
    "#### MAE\n",
    "* Scales linearly with size of error\n",
    "* Low sensitivity near minimum \n",
    "\n",
    "#### Huber\n",
    "* Similar to MSE near minimum\n",
    "* Similar to MAE away from minimum\n",
    "\n",
    "\n",
    "* **For greater sensitivity near the minimum, you will want to use the MSE or Huber loss.**\n",
    "* **To minimize the impact of outliers, you will want to use the MAE or Huber loss.**\n",
    "\n",
    "#### Defining a loss function\n",
    "* Let's say we decide to use the MSE loss:\n",
    "\n",
    "```\n",
    "import tensorflow as tf\n",
    "\n",
    "# Compute the MSE loss\n",
    "loss = tf.keras.losses.mse(targets, predictions)\n",
    "```\n",
    "* In many cases, the training process will require us to supply a function that accepts our model's variables and data and returns a loss\n",
    "* Here we'll first define a model, \"linear_regression,\" which takes the intercept, slope, and features as arguments and returns the model's predictions\n",
    "\n",
    "```\n",
    "# Define a linear regression model\n",
    "def linear_regression(intercept, slope = slope, features = features):\n",
    "    return intercept + features*slope\n",
    "```\n",
    "\n",
    "* Next we'll define a loss function called `loss_function` that accepts the slope and intercept of a linear model-- the variables-- and the input data, the targets and the features.\n",
    "* It then makes a prediction and computes and returns the associated MSE loss\n",
    "\n",
    "```\n",
    "# Define a loss function to compute the MSE\n",
    "def loss_function(intercept, slope, targets = targets, features = features):\n",
    "    # Compute the predictions for the linear model\n",
    "    predictions = linear_regression(intercept, slope)\n",
    "    \n",
    "    # Return the loss\n",
    "    return tf.keras.losses.mse(targets, predictions)\n",
    "```\n",
    "\n",
    "* **Note that we've defined both functions to use default argument values for features and targets.**\n",
    "* We will do this whenever we train on the full sample to simplify the code\n",
    "* Also notice that we've nested TensorFlow's MSE loss function within a function that first uses the model to make predictions and then uses those predictions as an input to the MSE loss function\n",
    "* We can then evaluate this function for a given set of parameter values and input data\n",
    "\n",
    "```\n",
    "# Compute the loss for test data inputs\n",
    "loss_function(intercept, slope, test_targets, test_features)\n",
    "```\n",
    "* Note that if we had omitted the data argumetns, test_targets, and test_features, the loss function would have instead used the default targets and features argumetns we set to evaluate model performance.\n",
    "\n",
    "#### Exercises: Loss functions in TensorFlow\n",
    "\n",
    "Compute the loss using data from the King County housing dataset. You are given a target, price, which is a tensor of house prices, and predictions, which is a tensor of predicted house prices. You will evaluate the loss function and print out the value of the loss.\n",
    "\n",
    "```\n",
    "# Import the keras module from tensorflow\n",
    "from tensorflow import keras\n",
    "\n",
    "# Compute the mean absolute error (mae)\n",
    "loss = keras.losses.mse(price, predictions)\n",
    "\n",
    "# Print the mean absolute error (mae)\n",
    "print(loss.numpy())\n",
    "```\n",
    "\n",
    "#### Exercises: Modifying the loss function\n",
    "In the previous exercise, you defined a `tensorflow` loss function and then evaluated it once for a set of actual and predicted values. In this exercise, you will compute the loss within another function called `loss_function()`, which first generates predicted values from the data and variables. The purpose of this is to construct a function of the trainable model variables that returns the loss. You can then repeatedly evaluate this function for different variable values until you find the minimum. In practice, you will pass this function to an optimizer in `tensorflow`. Note that `features` and `targets` have been defined and are available. Additionally, `Variable`, `float32`, and `keras` are available.\n",
    "\n",
    "```\n",
    "# Initialize a variable named scalar\n",
    "scalar = Variable(1.0, float32)\n",
    "\n",
    "# Define the model\n",
    "def model(scalar, features = features):\n",
    "  \treturn scalar * features\n",
    "\n",
    "# Define a loss function\n",
    "def loss_function(scalar, features = features, targets = targets):\n",
    "\t# Compute the predicted values\n",
    "\tpredictions = model(scalar, features)\n",
    "    \n",
    "\t# Return the mean absolute error loss\n",
    "\treturn keras.losses.mae(targets, predictions)\n",
    "\n",
    "# Evaluate the loss function and print the loss\n",
    "print(loss_function(scalar).numpy())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7c40c4",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "* A **linear regression model assumes a linear relationship:**\n",
    "    * $price = intercept + size * slope + error$\n",
    "* **This is an example of a univariate regression.**\n",
    "    * There is only one feature, `size`.\n",
    "* **Multiple regression models have more than one feature**\n",
    "    * e.g. `size` and `location`\n",
    "    \n",
    "```\n",
    "# Define the targets and features \n",
    "price = np.array(housing['price'], np.float32)\n",
    "size = np.array(housing['sqft_living'], np.float32)\n",
    "\n",
    "# Define the intercept and slope\n",
    "intercept = tf.Variable(0.1, np.float32)\n",
    "slope = tf.Variable(0.1, np.float32)\n",
    "```\n",
    "* A univariate linear regression identifies the relationship between a single feature and the target tensor.\n",
    "\n",
    "#### Exercises: Multiple linear regression\n",
    "In most cases, performing a univariate linear regression will not yield a model that is useful for making accurate predictions. In this exercise, you will perform a multiple regression, which uses more than one feature.\n",
    "\n",
    "You will use `price_log` as your target and `size_log` and `bedrooms` as your features. Each of these tensors has been defined and is available. You will also switch from using the the mean squared error loss to the mean absolute error loss: `keras.losses.mae()`. Finally, the predicted values are computed as follows: `params[0] + feature1*params[1] + feature2*params[2]`. Note that we've defined a vector of parameters, `params`, as a variable, rather than using three variables. Here, `params[0]` is the intercept and `params[1]` and `params[2]` are the slopes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1dbcd4",
   "metadata": {},
   "source": [
    "### Batch training\n",
    "* We've now learned how to train a linear model to predict house prices.\n",
    "* Now we will use batch training to handle large datasets\n",
    "\n",
    "#### What is batch training?\n",
    "* Let's pretend the KC Housing dataset is much larger, and we want to perform the training on a GPU, which has only a small amount of memory\n",
    "* **Since you can't fit the entire dataset in memory, you will instead divide it into batches and then train on those batches sequentially.**\n",
    "\n",
    "<img src='data/batch_training.png' width=\"600\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "* A single pass over all of the batches is called **epoch** and the process itself is called **batch training**.\n",
    "* **Batch training** is extremely useful when working with large image datasets\n",
    "* **Batch training will also allow you to update model weights and optimizer parameters after each batch, rather than at the end of an epoch.**\n",
    "\n",
    "#### The chunksize parameter\n",
    "* `pd.read_csv()` allows us to load data in batches\n",
    "    * To avoid loading the entire dataset at once, **`chunksize`** parameter specifies batch size.\n",
    "    \n",
    "* Below, instead of loading the data in a single one-liner, we'll write a for loop that iterates through the data in steps of 100 examples\n",
    "* Each 100 will be available as a batch, which we can use to extract columns, such as `price` and `size` in the housing dataset\n",
    "* We can then convert these to numpy arrays and use them to train\n",
    "* Being able to load data from csv filed in fixed-sized batches using pandas allows us to handle datasets of tens or even hundreds of gigabytes without excedding the memory constraints of our system.\n",
    "\n",
    "```\n",
    "#Import pandas and numpy\n",
    "import pandas as pd\n",
    "import numypy as np\n",
    "\n",
    "# Load data in batches\n",
    "for batch in pd.read_csv('kc_housing.csv', chunksize=100):\n",
    "    # Extract price column\n",
    "    price = np.array(batch['price'], np.float32)\n",
    "    \n",
    "    # Extract size column\n",
    "    size = np.array(batch['size'], np.float32)\n",
    "```\n",
    "\n",
    "### Training a linear model in batches\n",
    "\n",
    "```\n",
    "# Define trainable variables\n",
    "intercept = tf.Variable(0.1, tf.float32)\n",
    "slope = tf.Variable(0.1, tf.float32)\n",
    "\n",
    "# Define the model\n",
    "def linear_regression(intercept, slope, features):\n",
    "    return intercept + features * slope\n",
    "    \n",
    "# Compute predicted values and return loss function\n",
    "def loss_function(intercept, slope, targets, features):\n",
    "    predictions = linear_regression(intercept, slope, features)\n",
    "    return tf.keras.losses.mse(targets, predictions)\n",
    "    \n",
    "# Define optimization operation\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "```\n",
    "#### The next step is to train the model in batches\n",
    "* We do this, once again, by using a for loop and supplying a chunksize to the read csv function\n",
    "* Note that we take each batch,\n",
    "    * separate it into features and a target, \n",
    "    * convert those into numpy arrays,\n",
    "    * and then pass them to the minimize operation\n",
    "* Within the minimize operation, we pass the loss function as a lambda function and we supply a variable list that contains only the trainable parameters, (intercept, and slope).\n",
    "* This loop will continue until we have stepped through all of the examples in `read_csv`\n",
    "* **Importantly, we did not ever need to have more than 100 examples in memory during the entire process.**\n",
    "* Finally, we print our trained intercept and slope\n",
    "\n",
    "```\n",
    "# Load the data in batches from pandas\n",
    "for batch in pd.read_csv('kc_housing.csv', chunksize=100):\n",
    "    # Extract the target and feature columns\n",
    "    price_batch = np.array(batch['prce'], np.float32)\n",
    "    size_batch = np.array(batch['lot_size'], np.float32)\n",
    "    \n",
    "    # Minimize the loss function\n",
    "    opt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch), var_list=[intercept, slope])\n",
    "\n",
    "# Print parameter values\n",
    "print(intercept.numpy(), slope.numpy())\n",
    "```\n",
    "\n",
    "* **Note that we did not use default argument values for input data. This is because our input data was generated in batches during the training process.**\n",
    "\n",
    "<img src='data/fullsample_vs_batch.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "* In later chapters, we'll automate batch training by using high-level APIs\n",
    "* Importantly, however, high-level APIs will not typically load the sample in batches by default, as we have done here (above).\n",
    "\n",
    "#### Exercises: Preparing to batch train\n",
    "Before we can train a linear model in batches, we must first define variables, a loss function, and an optimization operation. In this exercise, we will prepare to train a model that will predict `price_batch`, a batch of house prices, using `size_batch`, a batch of lot sizes in square feet. In contrast to the previous lesson, we will do this by loading batches of data using `pandas`, converting it to `numpy` arrays, and then using it to minimize the loss function in steps.\n",
    "\n",
    "Note that you should not set default argument values for either the model or loss function, since we will generate the data in batches during the training process.\n",
    "\n",
    "```\n",
    "# Define the intercept and slope\n",
    "intercept = Variable(10.0, float32)\n",
    "slope = Variable(0.5, float32)\n",
    "\n",
    "# Define the model\n",
    "def linear_regression(intercept, slope, features):\n",
    "\t# Define the predicted values\n",
    "\treturn intercept + slope * features\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(intercept, slope, targets, features):\n",
    "\t# Define the predicted values\n",
    "\tpredictions = linear_regression(intercept, slope, features)\n",
    "    \n",
    " \t# Define the MSE loss\n",
    "\treturn keras.losses.mse(targets, predictions)\n",
    "```\n",
    "\n",
    "#### Exercises: Training a linear model in batches\n",
    "In this exercise, we will train a linear regression model in batches, starting where we left off in the previous exercise. We will do this by stepping through the dataset in batches and updating the model's variables, `intercept` and `slope`, after each step. This approach will allow us to train with datasets that are otherwise too large to hold in memory.\n",
    "\n",
    "Note that the loss function, `loss_function(intercept, slope, targets, features)`, has been defined for you. Additionally, `keras` has been imported for you and `numpy` is available as `np`. The trainable variables should be entered into `var_list` in the order in which they appear as loss function arguments.\n",
    "\n",
    "```\n",
    "# Initialize Adam optimizer\n",
    "opt = keras.optimizers.Adam()\n",
    "\n",
    "# Load data in batches\n",
    "for batch in pd.read_csv('kc_house_data.csv', chunksize=100):\n",
    "\tsize_batch = np.array(batch['sqft_lot'], np.float32)\n",
    "\n",
    "\t# Extract the price values for the current batch\n",
    "\tprice_batch = np.array(batch['price'], np.float32)\n",
    "\n",
    "\t# Complete the loss, fill in the variable list, and minimize\n",
    "\topt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch), var_list=[intercept, slope])\n",
    "\n",
    "# Print trained parameters\n",
    "print(intercept.numpy(), slope.numpy())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f16986f",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 3: Neural Networks\n",
    "The previous chapters taught you how to build models in TensorFlow 2. In this chapter, you will apply those same tools to build, train, and make predictions with neural networks. You will learn how to define dense layers, apply activation functions, select an optimizer, and apply regularization to reduce overfitting. You will take advantage of TensorFlow's flexibility by using both low-level linear algebra and high-level Keras API operations to define and train models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a245c517",
   "metadata": {},
   "source": [
    "### Dense layers\n",
    "* The dense layer is a frequently used component of neural networks\n",
    "* UCI credit card defaults\n",
    "\n",
    "<img src='data/uci_linreg.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff530464",
   "metadata": {},
   "source": [
    "#### Neural networks\n",
    "* So how do we get from linear regression to a neural network?\n",
    "\n",
    "<img src='data/uci_nn_map.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a64fbf3",
   "metadata": {},
   "source": [
    "* Each hidden layer node takes our two inputs, multiplies them by their respective weights, and sums them together\n",
    "* We also typically pass the hidden layer output to an activation function.\n",
    "* Finally, we sum together the outputs of the hidden layers to compute our prediction for credit card default\n",
    "* The entire process of generating a prediction is referred to as **forward propagation**\n",
    "* In this chapter we will construct NNs with only 3 types of layers: an input later, some number of hidden (dense) layers, and an output layer\n",
    "\n",
    "<img src='data/simple_nn.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf0da63",
   "metadata": {},
   "source": [
    "* Input layer: features\n",
    "* Output layer: prediction\n",
    "* Each hidden layer takes inputs from the previous layer, applies numerical weights to them, sums them together, and then applies an activation function\n",
    "* In the NN map above, each hidden layer is a dense, or fully-connected layer\n",
    "    * **A dense layer applies weights to *all* nodes from the previous layer.**\n",
    "\n",
    "#### A simple dense layer\n",
    "* We'll first define a constant tensor that contains the marital status and age data as the input layer\n",
    "* We then initialize weights as a variable, since we will train those weights to predict the output from the inputs\n",
    "* We also define a bias, which will play a similar role to the intercept in the linear regression model.\n",
    "* Finally, we define a dense layer; note that we first perform a matrix multiplication of the inputs by the weights and assign that to the tensor named product\n",
    "* We then add product to the bias and apply a non-linear transformation, in this case the sigmoid function\n",
    "    * This is called the **activation function**\n",
    "* Note that the bias is not associated with a feature and is analogous to the intercept in a linear regression\n",
    "* Note that TensorFlow also comes with higher level operations, such as `tf.keras.layers.Dense`, which allows us to skip the linear algebra\n",
    "\n",
    "```\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define inputs (features)\n",
    "inputs = tf.constant([[1, 35]])\n",
    "\n",
    "# Define weights\n",
    "weights = tf.Variable([[-0.05], [-0.01]])\n",
    "\n",
    "# Define the bias\n",
    "bias = tf.Variable([0.5])\n",
    "\n",
    "# Multiply inputs (features) by the weights\n",
    "product = tf.matmul(inputs, weights)\n",
    "\n",
    "# Define dense layer\n",
    "dense = tf.keras.activations.sigmoid(product + bias)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835236f3",
   "metadata": {},
   "source": [
    "#### Defining a complete model\n",
    "* **Note that, by default, a bias will be included.**\n",
    "* Note that we've also passed inputs as an argument to the first dense layer\n",
    "* Note that the second dense layer takes the first dense layer as an argument and also reduces the number of nodes\n",
    "* The outputs reduces this again to one.\n",
    "\n",
    "```\n",
    "import tensorflow as tf\n",
    "\n",
    "#Define input (features) layer\n",
    "input = tf.constant(data, tf.float32)\n",
    "\n",
    "# Define first dense layer\n",
    "dense1 = tf.keras.layers.Dense(10, activation='sigmoid')(inputs)\n",
    "\n",
    "# Define second dense layer\n",
    "dense2 = tf.keras.layers.Dense(5, activation='sigmoid')(dense1)\n",
    "\n",
    "# Define output (predictions) layer\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dense2)\n",
    "```\n",
    "\n",
    "#### High-level vs. low-level approach\n",
    "\n",
    "* **High-level approach:**\n",
    "    * Complex operations in high-level API operations\n",
    "    * High-level APIs such as `Keras` and `Estimators`\n",
    "        * reduces the amount of code needed\n",
    "    * weights and the mathematical operations will typically be hidden by the layer constructor \n",
    "    \n",
    "* **Low-level approach \n",
    "    * Linear-algebraic operations\n",
    "        * Allows for the construction of any model\n",
    "    \n",
    "**TensorFlow allows us to use either approach or even combine them.**\n",
    "\n",
    "<img src='data/hi_lo_approach.png' width=\"600\" height=\"300\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e39681",
   "metadata": {},
   "source": [
    "#### Exercises: The linear algebra of dense layers\n",
    "There are two ways to define a dense layer in `tensorflow`. The first involves the use of low-level, linear algebraic operations. The second makes use of high-level `keras` operations. In this exercise, we will use the first method to construct the network shown in the image below.\n",
    "\n",
    "<img src='data/dense_im.png' width=\"300\" height=\"150\" align=\"center\"/>\n",
    "\n",
    "The input layer contains 3 features -- education, marital status, and age -- which are available as `borrower_features`. The hidden layer contains 2 nodes and the output layer contains a single node.\n",
    "\n",
    "For each layer, you will take the previous layer as an input, initialize a set of weights, compute the product of the inputs and weights, and then apply an activation function. Note that `Variable()`, `ones()`, `matmul()`, and `keras()` have been imported from `tensorflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8937195",
   "metadata": {},
   "outputs": [],
   "source": [
    "borrower_features = np.array([[2., 2., 43.]], np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5cf6f97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " dense1's output shape: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# Initialize bias1\n",
    "bias1 = Variable(1.0)\n",
    "\n",
    "# Initialize weights1 as 3x2 variable of ones\n",
    "weights1 = tf.Variable(ones((3, 2)))\n",
    "\n",
    "# Perform matrix multiplication of borrower_features and weights1\n",
    "product1 = matmul(borrower_features, weights1)\n",
    "\n",
    "# Apply sigmoid activation function to product1 + bias1\n",
    "dense1 = tf.keras.activations.sigmoid(product1 + bias1)\n",
    "\n",
    "# Print shape of dense1\n",
    "print(\"\\n dense1's output shape: {}\".format(dense1.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3cbef37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " prediction: 0.9525741338729858\n",
      "\n",
      " actual: 1\n"
     ]
    }
   ],
   "source": [
    "# Initialize bias2 and weights2\n",
    "bias2 = Variable(1.0)\n",
    "weights2 = Variable(ones((2, 1)))\n",
    "\n",
    "# Perform matrix multiplication of dense1 and weights2\n",
    "product2 = matmul(dense1, weights2)\n",
    "\n",
    "# Apply activation to product2 + bias2 and print the prediction\n",
    "prediction = tf.keras.activations.sigmoid(product2 + bias2)\n",
    "print('\\n prediction: {}'.format(prediction.numpy()[0,0]))\n",
    "print('\\n actual: 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed935d2",
   "metadata": {},
   "source": [
    "#### Exercises: The low-level approach with multiple examples\n",
    "In this exercise, we'll build further intuition for the low-level approach by constructing the first dense hidden layer for the case where we have multiple examples. We'll assume the model is trained and the first layer weights, `weights1`, and bias, `bias1`, are available. We'll then perform matrix multiplication of the `borrower_features` tensor by the `weights1` variable. Recall that the `borrower_features` tensor includes education, marital status, and age. Finally, we'll apply the sigmoid function to the elements of `products1 + bias1`, yielding `dense1`.\n",
    "\n",
    "<img src='data/prod1.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "Note that `matmul()` and `keras()` have been imported from tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ec9dcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " shape of borrower_features:  (1, 3)\n",
      "\n",
      " shape of weights1:  (3, 2)\n",
      "\n",
      " shape of bias1:  ()\n",
      "\n",
      " shape of dense1:  (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# Compute the product of borrower_features and weights1\n",
    "products1 = matmul(borrower_features, weights1)\n",
    "\n",
    "# Apply a sigmoid activation function to products1 + bias1\n",
    "dense1 = tf.keras.activations.sigmoid(products1 + bias1)\n",
    "\n",
    "# Print the shapes of borrower_features, weights1, bias1, and dense1\n",
    "print('\\n shape of borrower_features: ', borrower_features.shape)\n",
    "print('\\n shape of weights1: ', weights1.shape)\n",
    "print('\\n shape of bias1: ', bias1.shape)\n",
    "print('\\n shape of dense1: ', dense1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b4d156",
   "metadata": {},
   "source": [
    "#### Exercises: Using the dense layer operation\n",
    "We've now seen how to define dense layers in `tensorflow` using linear algebra. In this exercise, we'll skip the linear algebra and let `keras` work out the details. This will allow us to construct the network below, which has 2 hidden layers and 10 features, using less code than we needed for the network with 1 hidden layer and 3 features.\n",
    "\n",
    "<img src='data/ex_dense.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "To construct this network, we'll need to define three dense layers, each of which takes the previous layer as an input, multiplies it by weights, and applies an activation function. Note that input data has been defined and is available as a 100x10 tensor: `borrower_features`. Additionally, the `keras.layers` module is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66946988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " shape of dense1:  (1, 7)\n",
      "\n",
      " shape of dense2:  (1, 3)\n",
      "\n",
      " shape of predictions:  (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Define the first dense layer\n",
    "dense1 = tf.keras.layers.Dense(7, activation='sigmoid')(borrower_features)\n",
    "\n",
    "# Define a dense layer with 3 output nodes\n",
    "dense2 = tf.keras.layers.Dense(3, activation='sigmoid')(dense1)\n",
    "\n",
    "# Define a dense layer with 1 output node\n",
    "predictions = tf.keras.layers.Dense(1, activation='sigmoid')(dense2)\n",
    "\n",
    "# Print the shapes of dense1, dense2, and predictions\n",
    "print('\\n shape of dense1: ', dense1.shape)\n",
    "print('\\n shape of dense2: ', dense2.shape)\n",
    "print('\\n shape of predictions: ', predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3420af8d",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "* A typical hidden layer consists of two operations:\n",
    "    * 1) **Linear:** Matrix multiplication\n",
    "    * 2) **Non-linear:** Activation function\n",
    "    \n",
    "#### Why nonlinearities are important: a simple example:\n",
    "* Below we assume that the weight on age is 1 and the weight on bill amount is 2\n",
    "* Note that ages are divided by 100 and the bill's amount is divided by 10,000\n",
    "* We then perform the matrix mutltiplication step for all combinations of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd3b0fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define example borrower features\n",
    "young, old = 0.3, 0.6\n",
    "low_bill, high_bill = 0.1, 0.5\n",
    "\n",
    "# Apply matrix multiplication step for all feature combinations\n",
    "young_high = 1.0*young + 2.0*high_bill\n",
    "young_low = 1.0 * young + 2.0* low_bill\n",
    "old_high = 1.0 * old +2.0*high_bill\n",
    "old_low = 1.0*old + 2.0*low_bill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ecb69e",
   "metadata": {},
   "source": [
    "* If we don't apply an activation function and we assume the bias is zero, we find that the impact of bill size on default does not depend on age:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "54cd11ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "# Difference in default predictions for young\n",
    "print(young_high-young_low)\n",
    "\n",
    "# Difference in default predictions for old\n",
    "print(old_high-old_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c8025a",
   "metadata": {},
   "source": [
    "* Note that our target is a binary variable that is equal to 1 when the borrower defaults, however predictions will be real numbers between 0 an 1, where values over 0.5 will be treated as predicting default.\n",
    "* But what if we applied a sigmoid activation function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9c4eb390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16337562\n",
      "0.14204395\n"
     ]
    }
   ],
   "source": [
    "# Difference in default predictions for young\n",
    "print(tf.keras.activations.sigmoid(young_high).numpy() - tf.keras.activations.sigmoid(young_low).numpy())\n",
    "\n",
    "# Difference in default predictions for old\n",
    "print(tf.keras.activations.sigmoid(old_high).numpy()-tf.keras.activations.sigmoid(old_low).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25888059",
   "metadata": {},
   "source": [
    "In the above example, the impact of bill amount on default now depends on the borrower's age. In particular, we can see that the change in the predicted value for default is larger for young borrowers than it is for old borrowers.\n",
    "\n",
    "* In this course, we'll use the three most common activation functions: **sigmoid**, **relu**, and **softmax**\n",
    "\n",
    "#### Sigmoid activation function\n",
    "   * Used primarily in the output layer of binary classification problems\n",
    "   * Low-level: `tf.keras.activations.sigmoid()`\n",
    "   * High-level: `sigmoid`\n",
    "   \n",
    "#### ReLU activation function\n",
    "   * Rectified Activation Function\n",
    "   * Typically used in all layers other than the output layer \n",
    "   * Low-level: `tf.keras.activations.relu()`\n",
    "   * High-level: `relu`\n",
    "   \n",
    "#### Softmax activation function\n",
    "   * Output layer (classification problems with >2 classes)\n",
    "   * Ouputs can be interpreted as predicted class probabilities in multiclass classification problems\n",
    "   * Low-level: `tf.keras.activations.softmax()`\n",
    "   * High-level: `softmax`\n",
    "\n",
    "#### Activation functions in neural networks\n",
    "\n",
    "```\n",
    "# Define input layer\n",
    "inputs = tf.constant(borrower_features, tf.float32)\n",
    "\n",
    "# Define dense layer 1\n",
    "dense1 = tf.keras.layers.Dense(16, activation='relu')(inputs)\n",
    "\n",
    "# Define dense layer 2\n",
    "dense2 = tf.keras.layers.Dense(8, activation='sigmoid')(dense1)\n",
    "\n",
    "# Define output layer\n",
    "outputs = tf.keras.layers.Dense(4, activation='softmax')(dense2)\n",
    "```\n",
    "\n",
    "#### Binary classification problems\n",
    "In this exercise, you will again make use of credit card data. The target variable, `default`, indicates whether a credit card holder defaults on his or her payment in the following period. Since there are only two options--default or not--this is a binary classification problem. While the dataset has many features, you will focus on just three: the size of the three latest credit card bills. Finally, you will compute predictions from your untrained network, `outputs`, and compare those the target variable, `default`.\n",
    "\n",
    "The tensor of features has been loaded and is available as `bill_amounts`. Additionally, the `constant()`, `float32`, and `keras.layers.Dense()` operations are available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e38d430",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "* **Stochastic Gradient Descent** or **SGD** is an improved version of gradient descent that is less likely to get stuck in local minima.\n",
    "* For simple problems, the SGD algorithm performs well\n",
    "* (Below) Adam and RMS Prop require 10x as many iterations to achieve a similar loss\n",
    "\n",
    "<img src='data/opt_performance.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "#### Stochastic gradient descent optimizer\n",
    "   * `tf.keras.optimizers.SGD()`\n",
    "   * `learning_rate`: typically between 0.5 and 0.001, which will determine how quickly the model parameters adjust during training\n",
    "   * **Simple and easy to interpret** (more so than most modern optimization algorithms)\n",
    "   \n",
    "#### RMS propagation optimizer \n",
    "   * **Root mean squared (RMS) propagation optimizer**\n",
    "       * `tf.keras.optimizers.RMSprop()\n",
    "       * Applies different learning rates to each feature, which can be useful for high dimensional problems\n",
    "       * **`momentum`:** allows it to build momentum\n",
    "       * **`decay`**: setting a low value for the decay parameter will prevent momentum for accumumlating over long periods during the training process\n",
    "       \n",
    "#### The adam optimizer\n",
    "   * **Adaptive moment (adam) optimizer**\n",
    "       * Generally a good first choice\n",
    "       * Provides further improvements\n",
    "       * `learning_rate`\n",
    "       * `beta1`: similar to RMS prop, you can set the momentum to decay faster by lowering the `beta1` parameter\n",
    "   * **Performs well with default parameter values** (especially in comparison with RMS prop)\n",
    "   \n",
    "```\n",
    "# Define the model function\n",
    "def model(bias, weights, features = borrower_features):\n",
    "    product = tf.matmul(features, weights)\n",
    "    return tf.keras.activations.sigmoid(product+bias)\n",
    "    \n",
    "# Compute the predicted values and loss\n",
    "def loss_function(bias, weights, targets = default, features = borrower_features):\n",
    "    predictions = model(bias, weights)\n",
    "    return tf.keras.losses.binary_crossentropy(target, predictions)\n",
    "    \n",
    "# Minimize the loss function with RMS propagation\n",
    "opt = tf.keras.omptimizers.RMSprop(learning_rate=0.01, momentum=0.9)\n",
    "opt.minimize(lambda: loss_function(bias, weights), var_list=[bias, weights])\n",
    "```\n",
    "\n",
    "\n",
    "#### Exercises: The dangers of local minima\n",
    "Consider the plot of the following loss function, `loss_function()`, which contains a global minimum, marked by the dot on the right, and several local minima, including the one marked by the dot on the left.\n",
    "\n",
    "<img src='data/local_minima.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "In this exercise, you will try to find the global minimum of `loss_function()` using `keras.optimizers.SGD()`. You will do this twice, each time with a different initial value of the input to `loss_function()`. First, you will use `x_1`, which is a variable with an initial value of 6.0. Second, you will use `x_2`, which is a variable with an initial value of 0.3. Note that `loss_function()` has been defined and is available.\n",
    "\n",
    "```\n",
    "# Initialize x_1 and x_2\n",
    "x_1 = Variable(6.0,float32)\n",
    "x_2 = Variable(0.3,float32)\n",
    "\n",
    "# Define the optimization operation\n",
    "opt = keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "for j in range(100):\n",
    "\t# Perform minimization using the loss function and x_1\n",
    "\topt.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
    "\t# Perform minimization using the loss function and x_2\n",
    "\topt.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
    "\n",
    "# Print x_1 and x_2 as numpy arrays\n",
    "print(x_1.numpy(), x_2.numpy())\n",
    "```\n",
    "\n",
    "```\n",
    "# Initialize x_1 and x_2\n",
    "x_1 = Variable(0.05,float32)\n",
    "x_2 = Variable(0.05,float32)\n",
    "\n",
    "# Define the optimization operation for opt_1 and opt_2\n",
    "opt_1 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.99)\n",
    "opt_2 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.0)\n",
    "\n",
    "for j in range(100):\n",
    "\topt_1.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
    "    # Define the minimization operation for opt_2\n",
    "\topt_2.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
    "\n",
    "# Print x_1 and x_2 as numpy arrays\n",
    "print(x_1.numpy(), x_2.numpy())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d37386",
   "metadata": {},
   "source": [
    "### Training a network in TensorFlow\n",
    "* Finding the global minimum can be difficult, even when we're minimizing a simple loss function\n",
    "* We also saw that we could improve our chances by selecting better initial values for variables\n",
    "* But what can we do for more challenging problems with many variables?\n",
    "* The eggholder function, for example, has many local minima:\n",
    "\n",
    "<img src='data/eggholder_function.png' width=\"700\" height=\"350\" align=\"center\"/>\n",
    "\n",
    "* It may be difficult to see a global minimum on the plot above, but it does have one.\n",
    "* How can we select initial values for `x` and `y`, the two inputs to the eggholder function?\n",
    "* Even worse, what if we have a loss function that depends on hundreds of variables?\n",
    "\n",
    "#### Random initializers \n",
    "* We often need to initialize hundreds or thousands of variables \n",
    "    * `tf.ones()` may perform poorly (and simply will not work for many datasets)\n",
    "    * Tedious and difficult (and infeasible) to initialzie variables individually\n",
    "* **Alternatively, draw initial values from distribution**\n",
    "    * Random or algorithmic generation of initial values\n",
    "    * We can draw them from a probability distribution\n",
    "        * Normal\n",
    "        * Uniform \n",
    "        * Glorot initializers (designed for ML algorithms)\n",
    "        \n",
    "#### Initializing variables in TensorFlow: low-level approach\n",
    "* Alternatively, we can also use the **truncated random normal distribution, which discards very large and very small draws**\n",
    "\n",
    "```\n",
    "# Define 500x500 random normal variable\n",
    "weights = tf.Variable(tf.random.normal([500,500]))\n",
    "\n",
    "# Define 500x500 truncated random normal variable\n",
    "weights = tf.Variable(tf.random.truncated_normal([500,500]))\n",
    "```\n",
    "\n",
    "#### Initializing variables in TensorFlow: high-level approach\n",
    "* We can also use the high-level approach by initializing a dense layer using the default keras option, currently the glorot uniform initializer, as we've done in all exercises thus far.\n",
    "* If we instead wish to initialize values to zero, we can do this using the `kernel_initializer` parameter\n",
    "\n",
    "```\n",
    "# Define a dense layer with the default initializer\n",
    "dense = tf.keras.layers.Dense(32, activation='relu')\n",
    "\n",
    "# Define a dense layer with the zeros intitializer\n",
    "dense = tf.keras.layers.Dense(32, activation='relu', kernel_initializer='zeros')\n",
    "```\n",
    "\n",
    "#### Neural networks and overfitting\n",
    "* Overfitting is especially problematic for neural networks, which contain many parameters and are quite good at memorization\n",
    "* A simple solution is to use **dropout**\n",
    "    * This will force your network to develop more robust rules for classification, since it cannot rely on any particular nodes being passed to an activation function\n",
    "    * This will tend to improve out-of-sample performance\n",
    "    \n",
    "```\n",
    "# Define input data\n",
    "inputs = np.array(borrower_features, np.float32)\n",
    "\n",
    "# Define dense layer 1\n",
    "dense1 = tf.keras.layers.Dense(32, activation='relu')(inputs)\n",
    "\n",
    "# Define dense layer 2\n",
    "dense2 = tf.keras.layers.Dense(16, activation='relu')(dense1)\n",
    "\n",
    "# Apply a dropout operation\n",
    "dropout1 = tf.keras.layers.Dropout(0.25)(dense2)\n",
    "\n",
    "# Define output later\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dropout1)\n",
    "```\n",
    "\n",
    "#### Exercises: Initialization in TensorFlow\n",
    "A good initialization can reduce the amount of time needed to find the global minimum. In this exercise, we will initialize weights and biases for a neural network that will be used to predict credit card default decisions. To build intuition, we will use the low-level, linear algebraic approach, rather than making use of convenience functions and high-level `keras` operations. We will also expand the set of input features from 3 to 23. Several operations have been imported from `tensorflow`: `Variable()`, `random()`, and `ones()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9ec16072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "345f7463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the layer 1 weights\n",
    "w1 = tf.Variable(tf.random.normal([23, 7]))\n",
    "\n",
    "# Initialize the layer 1 bias\n",
    "b1 = tf.Variable(ones([7]))\n",
    "\n",
    "# Define the layer 2 weights\n",
    "w2 = tf.Variable(random.normal([7, 1]))\n",
    "\n",
    "# Define the layer 2 bias\n",
    "b2 = tf.Variable(0.0, np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0ef0ab",
   "metadata": {},
   "source": [
    "#### Exercises: Defining the model and loss function\n",
    "In this exercise, you will train a neural network to predict whether a credit card holder will default. The features and targets you will use to train your network are available in the Python shell as `borrower_features` and `default`. You defined the weights and biases in the previous exercise.\n",
    "\n",
    "Note that the `predictions` layer is defined as $\\sigma(layer1 * w^2 + b^2)$ , where $\\sigma$ is the sigmoid activation, `layer1` is a tensor of nodes for the first hidden dense layer, `w2` is a tensor of weights, and `b2` is the bias tensor.\n",
    "\n",
    "The trainable variables are `w1`, `b1`, `w2`, and `b2`. Additionally, the following operations have been imported for you: `keras.activations.relu()` and `keras.layers.Dropout()`.\n",
    "\n",
    "```\n",
    "# Define the model\n",
    "def model(w1, b1, w2, b2, features = borrower_features):\n",
    "\t# Apply relu activation functions to layer 1\n",
    "\tlayer1 = keras.activations.relu(matmul(features, w1) + b1)\n",
    "    # Apply dropout rate of 0.25\n",
    "\tdropout = keras.layers.Dropout(0.25)(layer1)\n",
    "\treturn keras.activations.sigmoid(matmul(dropout, w2) + b2)\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(w1, b1, w2, b2, features = borrower_features, targets = default):\n",
    "\tpredictions = model(w1, b1, w2, b2)\n",
    "\t# Pass targets and predictions to the cross entropy loss\n",
    "\treturn keras.losses.binary_crossentropy(targets, predictions)\n",
    "```\n",
    "\n",
    "#### Exercises: Training neural networks with TensorFlow\n",
    "In the previous exercise, you defined a model, `model(w1, b1, w2, b2, features)`, and a loss function, `loss_function(w1, b1, w2, b2, features, targets)`, both of which are available to you in this exercise. You will now train the model and then evaluate its performance by predicting default outcomes in a test set, which consists of `test_features` and `test_targets` and is available to you. The trainable variables are `w1`, `b1`, `w2`, and `b2`. Additionally, the following operations have been imported for you: `keras.activations.relu()` and `keras.layers.Dropout()`.\n",
    "\n",
    "```\n",
    "# Train the model\n",
    "for j in range(100):\n",
    "    # Complete the optimizer\n",
    "\topt.minimize(lambda: loss_function(w1, b1, w2, b2), \n",
    "                 var_list=[w1, b1, w2, b2])\n",
    "\n",
    "# Make predictions with model using test features\n",
    "model_predictions = model(w1, b1, w2, b2, test_features)\n",
    "\n",
    "# Construct the confusion matrix\n",
    "confusion_matrix(test_targets, model_predictions)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a50d237",
   "metadata": {},
   "source": [
    "# $\\star$ Chapter 4: High Level APIs\n",
    "In the final chapter, you'll use high-level APIs in TensorFlow 2 to train a sign language letter classifier. You will use both the sequential and functional Keras APIs to train, validate, make predictions with, and evaluate models. You will also learn how to use the Estimators API to streamline the model definition and training process, and to avoid errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450d96f6",
   "metadata": {},
   "source": [
    "## Defining neural networks with Keras\n",
    "* In this lesson, we will introduce the Keras sequential API and expand on our introduction of the Keras functional API\n",
    "* A good way to construct a model in Keras is to use the sequential API\n",
    "\n",
    "### The sequential API\n",
    "* This API is simpler and makes strong assumptions about how you will construct your model \n",
    "* It assumes you have:\n",
    "    * An input layer\n",
    "    * Some number of hidden layers\n",
    "    * An output layer\n",
    "* All of these layers are ordered one after the other in a sequence\n",
    "* Once we have defined the model object, we can simply \"stack\" layers on top of it sequentially using the `add` method\n",
    "* If we want to check our model's architecture, we can use the `.summary` method\n",
    "* **The model has now been defined, but it is not yet ready to be trained.**\n",
    "* We must first perform a **compilation step**, where we specify the optimizer and loss function.\n",
    "\n",
    "```\n",
    "# Import tensorflow\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define a sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define first hidden layer\n",
    "model.add(keras.layers.Dense(16, activation='relu', input_shape=(28*28,)))\n",
    "\n",
    "# Define second hidden layer\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "\n",
    "# Define output layer\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile('adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Summarize the model\n",
    "print(model.summary())\n",
    "```\n",
    "\n",
    "* **Categorical crossentropy:** for use with binary classification problems\n",
    "\n",
    "<img src='data/functional_API.png' width=\"700\" height=\"350\" align=\"center\"/>\n",
    "\n",
    "* **But what if we want to train two models jointly to predict the same target?**\n",
    "    * Answer: The functional API is for that\n",
    "* For example, say we have a set of 28x28 images and a set of 10 features of metadata.\n",
    "* We want to use both to predict the image's class, but restrict how they interact in our model\n",
    "* We'll start by using the Keras inputs operation to define the input shapes for model 1 and model 2\n",
    "* Next we define layer 1 and layer and layer 2 as dense layers for model 1\n",
    "    * **Note that we have to pass the previous layer as an argument if we use the functional API, but did not with the sequential**\n",
    "* We then define layers 1 and 2 for model 2\n",
    "* Then use the add layer in keras to combine the outputs in a layer that merges the two models\n",
    "* **Finally we define a functional model.**\n",
    "    * **As inputs, it takes both the model 1 and model 2 inputs**\n",
    "    * **As outputs it takes the merged layer**\n",
    "    * The only thing left to do is compile the model and train    \n",
    "\n",
    "```\n",
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define model 1 input layer shape\n",
    "model1_inputs = tf.keras.Input(shape=(28*28,))\n",
    "\n",
    "# Define model 2 input layer shape\n",
    "model2_inputs = tf.keras.Input(shape=(10,))\n",
    "\n",
    "# Define layer 1 for model 1\n",
    "model1_layer1 = tf.keras.layers.Dense(12, activation='relu')(model1_inputs)\n",
    "\n",
    "# Define layer 2 for model 1\n",
    "model1_layer2 = tf.keras.layers.Dense(4, activation='softmax')(model1_layer1)\n",
    "\n",
    "# Define layer 1 for model 2\n",
    "model2_layer1 = tf.keras.layers.Dense(8, activation='relu')(model2_inputs)\n",
    "\n",
    "# Define layer 2 for model 2\n",
    "model2_layer2 = tf.keras.layers.Dense(4, activation='softmax')(model2_layer1)\n",
    "\n",
    "# Merge model 1 and model 2\n",
    "merged = tf.keras.layers.add([model1_layer2, model2_layer2])\n",
    "\n",
    "# Define a functional model\n",
    "model = tf.keras.Model(inputs=[model1_inputs, model2_inputs], outputs=merged)\n",
    "\n",
    "# Compile the model\n",
    "model.compile('adam', loss='categorical_crossentropy')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dc8967",
   "metadata": {},
   "source": [
    "#### Exercises: The sequential model in Keras\n",
    "In chapter 3, we used components of the `keras` API in `tensorflow` to define a neural network, but we stopped short of using its full capabilities to streamline model definition and training. In this exercise, you will use the `keras` sequential model API to define a neural network that can be used to classify images of sign language letters. You will also use the `.summary()` method to print the model's architecture, including the shape and number of parameters associated with each layer.\n",
    "\n",
    "Note that the images were reshaped from (28, 28) to (784,), so that they could be used as inputs to a dense layer. Additionally, note that `keras` has been imported from `tensorflow` for you.\n",
    "\n",
    "```\n",
    "# Define a Keras sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define the first dense layer\n",
    "model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Define the second dense layer\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Print the model architecture\n",
    "print(model.summary())\n",
    "```\n",
    "\n",
    "#### Exercises: Compiling a sequential model\n",
    "In this exercise, you will work towards classifying letters from the Sign Language MNIST dataset; however, you will adopt a different network architecture than what you used in the previous exercise. There will be fewer layers, but more nodes. You will also apply dropout to prevent overfitting. Finally, you will compile the model to use the `adam` optimizer and the `categorical_crossentropy` loss. You will also use a method in `keras` to summarize your model's architecture. Note that `keras` has been imported from `tensorflow` for you and a sequential `keras` model has been defined as `model`.\n",
    "\n",
    "```\n",
    "# Define the first dense layer\n",
    "model.add(keras.layers.Dense(16, activation='sigmoid', input_shape=(784,)))\n",
    "\n",
    "# Apply dropout to the first layer's output\n",
    "model.add(keras.layers.Dropout(0.25))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile('adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Print a model summary\n",
    "print(model.summary())\n",
    "```\n",
    "\n",
    "#### Exercises: Defining a multiple input model\n",
    "In some cases, the sequential API will not be sufficiently flexible to accommodate your desired model architecture and you will need to use the functional API instead. If, for instance, you want to train two models with different architectures jointly, you will need to use the functional API to do this. In this exercise, we will see how to do this. We will also use the `.summary()` method to examine the joint model's architecture.\n",
    "\n",
    "Note that `keras` has been imported from `tensorflow` for you. Additionally, the input layers of the first and second models have been defined as `m1_inputs` and `m2_inputs`, respectively. Note that the two models have the same architecture, but one of them uses a `sigmoid` activation in the first layer and the other uses a `relu`.\n",
    "\n",
    "```\n",
    "# For model 1, pass the input layer to layer 1 and layer 1 to layer 2\n",
    "m1_layer1 = keras.layers.Dense(12, activation='sigmoid')(m1_inputs)\n",
    "m1_layer2 = keras.layers.Dense(4, activation='softmax')(m1_layer1)\n",
    "\n",
    "# For model 2, pass the input layer to layer 1 and layer 1 to layer 2\n",
    "m2_layer1 = keras.layers.Dense(12, activation='relu')(m2_inputs)\n",
    "m2_layer2 = keras.layers.Dense(4, activation='softmax')(m2_layer1)\n",
    "\n",
    "# Merge model outputs and define a functional model\n",
    "merged = keras.layers.add([m1_layer2, m2_layer2])\n",
    "model = keras.Model(inputs=[m1_inputs, m2_inputs], outputs=merged)\n",
    "\n",
    "# Print a model summary\n",
    "print(model.summary())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9f7cdb",
   "metadata": {},
   "source": [
    "### Training and validation with Keras\n",
    "* Whenever we train and evaluate a model in tensorflow, we typically use the same set of steps\n",
    "\n",
    "#### Overview of training and evaluation\n",
    "   1. Load and clean data\n",
    "   2. Define model\n",
    "   3. Train and validate model\n",
    "   4. Evaluate model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bf83c1",
   "metadata": {},
   "source": [
    "#### How to train a model\n",
    "\n",
    "```\n",
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a sequential model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Define the hidden layer\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile('adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Train model\n",
    "model.fit(image_features, image_labels)\n",
    "```\n",
    "\n",
    "#### The fit() operation\n",
    "* Notice that we only supplied two arguments to fit: features and labels\n",
    "* Only required arguments:\n",
    "    * `features`\n",
    "    * `labels`\n",
    "* However, there are also many optional arguments, including:\n",
    "    * `batch_size`\n",
    "    * `epochs`\n",
    "    * `validation_split`\n",
    "    \n",
    "### Batch size and epochs\n",
    "\n",
    "<img src='data/batch_vs_epoch.png' width=\"700\" height=\"350\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c95831b",
   "metadata": {},
   "source": [
    "* **Batch size:** the number of examples in each batch; `32` by default\n",
    "* **Epochs:** The number of times you train on the **full** set of batches is referred to as the number of epochs.\n",
    "* In the image above, the batch size is 5 and the number of epochs is 2.\n",
    "* **Using multiple epochs allows the model to revisit the same batches, but with different model weights and possibly optimizer parameters, since they are updated after each batch.**\n",
    "\n",
    "#### Validation split\n",
    "<img src='data/val_split.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b522e648",
   "metadata": {},
   "source": [
    "* **`validation_split`:** divides the dataset into two parts, as shown above\n",
    "* The first part is the training set and the second part is the validation set\n",
    "* For example, setting a value of `0.2` will put 20% of the data in the validation set\n",
    "\n",
    "```\n",
    "# Train model with validation split\n",
    "model.fit(features, labels, epochs=10, validation_split=0.20)\n",
    "```\n",
    "* The benefit of using a validation split is that you can see how your model performs on both the data it was trained on, *the training set*, **and** a separate dataset it was not trained on, *the validation set*. \n",
    "* Below, we can see the first 10 epochs of training:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20a5064",
   "metadata": {},
   "source": [
    "<img src='data/valsplit_perf.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bced90",
   "metadata": {},
   "source": [
    "* Notice that we can see the training loss and validation loss separately\n",
    "* Recall that if the training loss becomes substantially lower than the validation loss, this is an indication that we're overfitting\n",
    "* **We should either terminate the training process before that point or add regularization or dropout.**\n",
    "\n",
    "### Changing the metric\n",
    "* Another benefit of the high-level keras API is that we can swap less informative metrics, such as the loss, for ones that are more easily interpretable, such as the share of accurately classified examples. \n",
    "* We then apply fit to the model again with the same settings\n",
    "\n",
    "```\n",
    "# Recompile the model with the accuracy metric\n",
    "model.compile('adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model with validation split\n",
    "model.fit(features, labels, epochs-10, validation_split=0.20)\n",
    "```\n",
    "\n",
    "#### The evaluation() operation\n",
    "* Finally, it's a good idea to split off a test set before you begin to train and validate\n",
    "\n",
    "<img src='data/train_test_val.png' width=\"400\" height=\"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96c1f11",
   "metadata": {},
   "source": [
    "* You can then use the `evaluate` operation to check performance on the test set at the end of the training process\n",
    "\n",
    "```\n",
    "# Evaluate the test set\n",
    "model.evaluate(test)\n",
    "```\n",
    "* Since you may tune model parameters in response to validation set performance, using a separate test set will provide you with further assurance that you have not overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd8e706",
   "metadata": {},
   "source": [
    "## Training models with the Estimators API\n",
    "* The high-level **Estimators API** was elevated in importance in TensorFlow 2.0\n",
    "* The **Estimators API** is a **high-level TensorFlow submodule**.\n",
    "* Relative to the core, lower-level TensorFlow APIs and the high-level Kera API, model building in the Estimator API is **less flexible**.\n",
    "    * This is because **it enforces a set of best practices by placing restrictions on model architecture and training.**\n",
    "* The upside of using the Estimators API is that is allows for **faster deployment.**\n",
    "* Models can be specified, trained, evaluated, and deployed with **less code.**\n",
    "* Also, there are many **pre-made models** that can be instantiated by setting a handful of model parameters\n",
    "\n",
    "### Model specification and training with the Estimators API\n",
    "   1. **Define feature columns**, which specify the shape and type of your data\n",
    "   2. **Load and transform your data** within a function\n",
    "       * The output of this function will be a dictionary object of features and your labels\n",
    "   3. **Define an estimator**\n",
    "       * Here, we'll use premade estimators, but you can also define custom estimators with different architectures\n",
    "   4. Apply train operation\n",
    "       * Note that all model objects created through the Estimators API have `train`, `evaluate`, and `predict` operations.\n",
    "       \n",
    "       \n",
    "#### Defining feature columns\n",
    "* Note that when defining the numeric feature column, we supplied the dictionary key, **`size`**; we will do this for each feature column we create.\n",
    "* For example, you may also want a *categorical* feature column for the number of rooms using `feature_column.categorical_column_with_vocabulary_list`.\n",
    "* We can then merge these into a list of feature columns\n",
    "\n",
    "#### Loading and transforming\n",
    "\n",
    "```\n",
    "# Import tensorflow under its standard alias\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a numeric feature column\n",
    "size = tf.feature_column.numeric_column(\"size\")\n",
    "\n",
    "# Define a categorical feature column\n",
    "rooms = tf.feature_column.categorical_column_with_vocabulary_list(\"rooms\", [\"1\", \"2\", \"3\", \"4\", \"5\"])\n",
    "\n",
    "# Create feature column list\n",
    "features_list = [size, rooms]\n",
    "```\n",
    "\n",
    "* **Note:** Alternatively, if we were using the sign language MNIST dataset, we'd define a list containing a single vector of features:\n",
    "\n",
    "```\n",
    "# Define a matrix feature column\n",
    "features_list = [tf.feature_column.numeric_column('image', shape=(784,))]\n",
    "```\n",
    "\n",
    "* We next need to define a function that transforms our data, puts the features in a dictionary, and returns both the features and labels.\n",
    "\n",
    "```\n",
    "# Define input data function\n",
    "def input_fn():\n",
    "    # Define feature dictionary\n",
    "    features = {\"size\":[1340,1690,2720], \"rooms\":[1, 3, 4]}\n",
    "    # Define labels\n",
    "    labels= [221900, 538000, 180000\n",
    "    return features, labels\n",
    "```\n",
    "* Note that we've simply taken three examples from the housing dataset for the sake of illustration\n",
    "* Using them, we've defined a dictionary with the keys \"size\" and \"rooms\" which maps to the feature columns we've defined\n",
    "* Next we define a list or array of labels, which give the price of the house in this case, and then return the features and labels\n",
    "\n",
    "#### Define and train a regression estimator\n",
    "* We can now define and train the estimator, but before we do that, we have to define what estimator we actually want to train\n",
    "* If we're predicting house prices, we may want to use a deep neural network with a regression head using **`estimator.DNNRegressor`**.\n",
    "* This allows us to **predict a continous target.**\n",
    "\n",
    "```\n",
    "# Define a deep neural network regression\n",
    "model0 = tf.estimator.DHHRegressor(feature_columns=feature_list, hidden_units=[10,6,6,3])\n",
    "\n",
    "# Train the regression model\n",
    "model0.train(input_fn, steps = 20)\n",
    "```\n",
    "* Note that all we had to supply was the list of feature columns and the number of nodes in each hidden layer; the rest is handled automatically\n",
    "* We then apply the train function, supply our input function, and train for 20 steps\n",
    "\n",
    "#### Define and train a deep neural network (classifier)\n",
    "* Alternatively, if we want to instead perform a classification task with a deep neural network, we just need to change the estimator to **`estimator.DNNClassifier`**, add the number of classes, **`n_classes`**, and then train again\n",
    "\n",
    "```\n",
    "# Define a deep neural network classifier\n",
    "model1 = tf.estimator.DNNClassifier(feature_Columns=feature_list, hidden_units=[32, 16, 8], n_classes=4)\n",
    "\n",
    "# Train the classifier\n",
    "model1.train(input_fn, steps=20)\n",
    "```\n",
    "* You can also use:\n",
    "    * Linear classifiers\n",
    "    * Boosted trees\n",
    "    * [Other common options](https://www.tensorflow.org/guide/estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59624ef9",
   "metadata": {},
   "source": [
    "```\n",
    "# Define feature columns for bedrooms and bathrooms\n",
    "bedrooms = feature_column.numeric_column(\"bedrooms\")\n",
    "bathrooms = feature_column.numeric_column(\"bathrooms\")\n",
    "\n",
    "# Define the list of feature columns\n",
    "feature_list = [bedrooms, bathrooms]\n",
    "\n",
    "def input_fn():\n",
    "\t# Define the labels\n",
    "\tlabels = np.array(housing.price)\n",
    "\t# Define the features\n",
    "\tfeatures = {'bedrooms':np.array(housing['bedrooms']), \n",
    "                'bathrooms':np.array(housing['bathrooms'])}\n",
    "\treturn features, labels\n",
    "    \n",
    "# Define the model and set the number of steps\n",
    "model = estimator.DNNRegressor(feature_columns=feature_list, hidden_units=[2,2])\n",
    "model.train(input_fn, steps=1)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
