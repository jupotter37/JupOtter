{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fa3fed2-3bc1-40e8-a043-6c9ab39a5f42",
   "metadata": {},
   "source": [
    "### Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?\n",
    "#### Overfitting\n",
    "Overfitting occurs when our machine learning model tries to cover all the data points or more than the required data points present in the given dataset. Because of this, the model starts caching noise and inaccurate values present in the dataset, and all these factors reduce the efficiency and accuracy of the model. The overfitted model has low bias and high variance.\n",
    "#### Techniques to Reduce Overfitting\n",
    "1. Increase training data.\n",
    "2. Reduce model complexity.\n",
    "3. Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).\n",
    "4. Ridge Regularization and Lasso Regularization.\n",
    "5. Use dropout for neural networks to tackle overfitting.\n",
    "#### Underfitting\n",
    "Underfitting occurs when our machine learning model is not able to capture the underlying trend of the data. To avoid the overfitting in the model, the fed of training data can be stopped at an early stage, due to which the model may not learn enough from the training data. As a result, it may fail to find the best fit of the dominant trend in the data.\n",
    "#### Techniques to Reduce Underfitting\n",
    "1. Increase model complexity.\n",
    "2. Increase the number of features, performing feature engineering.\n",
    "3. Remove noise from the data.\n",
    "4. Increase the number of epochs or increase the duration of training to get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a8d35a-9f74-4202-ac8a-d02dc6063a2e",
   "metadata": {},
   "source": [
    "### Q2: How can we reduce overfitting? Explain in brief. \n",
    "1. Increase training data.\n",
    "2. Reduce model complexity.\n",
    "3. Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).\n",
    "4. Ridge Regularization and Lasso Regularization.\n",
    "5. Use dropout for neural networks to tackle overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eaee06-9939-49e2-91d9-417e53aaf48c",
   "metadata": {},
   "source": [
    "### Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "#### Underfitting\n",
    "Underfitting occurs when our machine learning model is not able to capture the underlying trend of the data. To avoid the overfitting in the model, the fed of training data can be stopped at an early stage, due to which the model may not learn enough from the training data. As a result, it may fail to find the best fit of the dominant trend in the data.\n",
    "#### scenarios where underfitting can occur in ML.\n",
    "1. High bias and low variance.\n",
    "2. The size of the training dataset used is not enough.\n",
    "3. The model is too simple.\n",
    "4. Training data is not cleaned and also contains noise in it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cfd1c4-5824-465b-9e04-7410daf13752",
   "metadata": {},
   "source": [
    "### Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?\n",
    "If the algorithm is too simple (hypothesis with linear equation) then it may be on high bias and low variance condition and thus is error-prone. If algorithms fit too complex (hypothesis with high degree equation) then it may be on high variance and low bias. In the latter condition, the new entries will not perform well. Well, there is something between both of these conditions, known as a Trade-off or Bias Variance Trade-off\n",
    "\n",
    "Bias and variance are inversely connected. It is impossible to have an ML model with a low bias and a low variance.\n",
    "When a data engineer modifies the ML algorithm to better fit a given data set, it will lead to low bias—but it will increase variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a698ca-14b3-4166-80b6-d9ee22575326",
   "metadata": {},
   "source": [
    "### Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting\n",
    "We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data.\n",
    "\n",
    "our model is underfitting the training data when the model performs poorly on the training data. This is because the model is unable to capture the relationship between the input examples (often called X) and the target values (often called Y). Your model is overfitting your training data when you see that the model performs well on the training data but does not perform well on the evaluation data. This is because the model is memorizing the data it has seen and is unable to generalize to unseen examples.\n",
    "Poor performance on the training data could be because the model is too simple (the input features are not expressive enough) to describe the target well. Performance can be improved by increasing model flexibility. To increase model flexibility, try the following:\n",
    "1. Add new domain-specific features and more feature Cartesian products, and change the types of feature processing used (e.g., increasing n-grams size)\n",
    "2. Decrease the amount of regularization used\n",
    "\n",
    "If your model is overfitting the training data, it makes sense to take actions that reduce model flexibility. To reduce model flexibility, try the following:\n",
    "\n",
    "1. Feature selection: consider using fewer feature combinations, decrease n-grams size, and decrease the number of numeric attribute bins.\n",
    "2. Increase the amount of regularization used.\n",
    "\n",
    "Accuracy on training and test data could be poor because the learning algorithm did not have enough data to learn from. You could improve performance by doing the following:\n",
    "1. Increase the amount of training data examples.\n",
    "2. Increase the number of passes on the existing training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fff473-76ae-40dd-9546-76e7a4ab8816",
   "metadata": {},
   "source": [
    "### Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?\n",
    "When an algorithm is employed in a machine learning model and it does not fit well, a phenomenon known as bias can develop. Bias arises in several situations while The term \"variance\" refers to the degree of change that may be expected in the estimation of the target function as a result of using multiple sets of training data.\n",
    "\n",
    "The disparity between the values that were predicted and the values that were actually observed is referred to as bias while A random variable's variance is a measure of how much it varies from the value that was predicted for it.\n",
    "\n",
    "Examples of high-bias machine learning algorithms include: Linear Regression, Linear Discriminant Analysis and Logistic Regression.\n",
    "\n",
    "Examples of high-variance machine learning algorithms include: Decision Trees, k-Nearest Neighbors and Support Vector Machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f6d16a-f627-4530-b791-62838ead4db7",
   "metadata": {},
   "source": [
    "### Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques\n",
    "#### regularization\n",
    "Regularization in machine learning is the process of regularizing the parameters that constrain, regularizes, or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, avoiding the risk of Overfitting.\n",
    "#### how can it be used to prevent overfitting\n",
    "Let’s consider a neural network which is overfitting on the training data as shown in the image below.\n",
    "\n",
    "If you have studied the concept of regularization in machine learning, you will have a fair idea that regularization penalizes the coefficients. In deep learning, it actually penalizes the weight matrices of the nodes.\n",
    "\n",
    "Assume that our regularization coefficient is so high that some of the weight matrices are nearly equal to zero.\n",
    "This will result in a much simpler linear network and slight underfitting of the training data.\n",
    "\n",
    "Such a large value of the regularization coefficient is not that useful. We need to optimize the value of regularization coefficient in order to obtain a well-fitted model.\n",
    "#### some common regularization techniques\n",
    "1. L2 & L1 regularization\n",
    "2. Dropout\n",
    "3. Data Augmentation\n",
    "4. Early stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8471d127-0878-4506-9929-e41165ebdafc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
