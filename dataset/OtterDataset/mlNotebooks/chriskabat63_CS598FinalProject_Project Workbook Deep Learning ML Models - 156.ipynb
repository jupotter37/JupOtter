{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "3d1ehXFWM5aE"
   },
   "source": [
    "Title:  Project Workbook Deep Learning ML Models\n",
    "\n",
    "Authors:  Matthew Lopes and Chris Kabat\n",
    "\n",
    "This notebook was created to train the Deep Learning Models to support our CS 598 DLH project. The paper we have chosen for the reproducibility project is:\n",
    "***Ensembling Classical Machine Learning and Deep Learning Approaches for Morbidity Identification from Clinical Notes ***\n",
    "\n",
    "Abstract:  The main goal of the paper is to extract Morbidity from clinical notes.  The idea was to use a combination of classical and deep learning methods to determine the best approach for classifying these notes in one or more of 16 morbidity conditions.  These models used a combination of NLP techniques including embeddings and bag of words implementations.  It also measured the effect including of stop words.  Lastly, it used ensemble techniques to tie together a number of the classical and deep learning models to provide the most accurate results.\n",
    "\n",
    "The data cannot be shared publicly due to the agreements required to obtain the data so we are storing the data locally and not putting in GitHub.\n",
    "\n",
    "We are only training models using data that includes stop words.  \n",
    "\n",
    "In this workbook, we are taking the following steps:\n",
    "\n",
    "* Run Bag of Word Hyper Parameter Tuning\n",
    "* Run Bag of Word DL Models across all diseasese\n",
    "* Run Embedding Hyper Parameter Tuning\n",
    "* Run Embedding DL Models across all diseasese\n",
    "\n",
    "Note,  it was very difficult to get CUDA working with the torchtext library as it depends on different versions.   To do this, you need to install in the following way (assuming CUDA 11.7):\n",
    "\n",
    "```\n",
    "python -m pip uninstall torch\n",
    "\n",
    "python -m pip install torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 torchtext==0.14.1  --index-url https://download.pytorch.org/whl/cu117\n",
    "\n",
    "python -m pip install torchdata==0.5.1\n",
    "```\n",
    "\n",
    " First we load the required libraries and retrieve our data.  Note, this can take a really long time to run, if you execute hyper parameter tuning and training.  The lines of code that execute these have been commented out so the user can selectively choose what to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1250,
     "status": "ok",
     "timestamp": 1679769727418,
     "user": {
      "displayName": "Matthew Lopes",
      "userId": "01980291092524472313"
     },
     "user_tz": 240
    },
    "id": "zyXrAo2dsJqf",
    "outputId": "0cc91c5b-f4de-41e2-f2bd-dd9ca70041a3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "import torchtext\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from sklearn.model_selection import KFold,train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import tensorflow_hub as hub\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# set seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "# define data path\n",
    "DATA_PATH = './obesity_data/'\n",
    "RESULTS_PATH = './results/'\n",
    "MODELS_PATH = './models/'\n",
    "AOAI_PATH = './aoai/'\n",
    "\n",
    "if os.path.exists(RESULTS_PATH) == False:\n",
    "    os.mkdir(RESULTS_PATH)\n",
    "if os.path.exists(MODELS_PATH) == False:\n",
    "    os.mkdir(MODELS_PATH)\n",
    "\n",
    "\n",
    "all_df = pd.read_pickle(DATA_PATH + '/all_df.pkl') \n",
    "all_df_expanded = pd.read_pickle(DATA_PATH + '/all_df_expanded.pkl')\n",
    "allannot_df= pd.read_pickle(DATA_PATH + '/allannot_df.pkl')\n",
    "alldocs_df_aoai = pd.read_pickle(AOAI_PATH + '/alldocs_df_aoai.pkl') \n",
    "voc = torch.load(DATA_PATH + '/voc.obj')\n",
    "\n",
    "#This is created in the embeddings file\n",
    "#max_tokens = 1416\n",
    "#max_sentences = 380\n",
    "(max_tokens, max_sentences) = torch.load(DATA_PATH + '/counts.obj')\n",
    "max_sentences_aoai = 381\n",
    "oai_col = 'ada_v2_sent'\n",
    "\n",
    "#Download info for embeddings\n",
    "word_embedding_size = 300\n",
    "sentence_embedding_size = 512\n",
    "aoai_embedding_size = 1536 #2048\n",
    "use_embeddings = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "fasttext_embeddings = torchtext.vocab.FastText()\n",
    "glove_embeddings = torchtext.vocab.GloVe(name='6B', dim=word_embedding_size)    \n",
    "\n",
    "disease_list = all_df['disease'].unique().tolist()\n",
    "embedding_list = ['GloVe', \"FastText\",'USE','AOAI']\n",
    "result_cols = ['Batch','Disease','Embedding','AUROC','F1','F1_MACRO', 'F1_MICRO', 'Exec Time', 'Total Run (secs)','Epochs', 'Dropout', 'BatchSize', 'Hidden', 'LR', 'CV']\n",
    "result_loss_cols = ['Loss','Epoch','Batch','Disease','Embedding','Epochs', 'Dropout', 'BatchSize', 'Hidden', 'LR']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Common training and evaluation code***\n",
    "\n",
    "Note, we did test some learning rate decay techniques, but we did not implement this in final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps=1e-10\n",
    "\n",
    "def train_model(tmodel, train_dataloader, n_epoch=5, lr=0.003, device=None, model_name='unk', use_decay=False):\n",
    "    import torch.optim as optim\n",
    "    \n",
    "    device = device or torch.device('cpu')\n",
    "\n",
    "    tmodel.train()\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    # your code here\n",
    "    optimizer = optim.Adam(tmodel.parameters(), lr=lr)\n",
    "    # want to decay the learning rate as teh number of epochs get larger\n",
    "    #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma = 0.1)\n",
    "    if use_decay:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "            factor=0.1, patience=10, threshold=0.0001, threshold_mode='abs')\n",
    "\n",
    "    #loss_func = nn.BCELoss()\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    #loss_func = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        epoch = epoch+1\n",
    "        curr_epoch_loss = []\n",
    "        start = time.time()\n",
    "\n",
    "        bs = train_dataloader.batch_size\n",
    "        hs = tmodel.hidden_size\n",
    "        do = tmodel.dropout\n",
    "\n",
    "        for X, Y in tqdm(train_dataloader,desc=f\"Training {model_name}-Lr{str(lr)}-Epoch{epoch}of{n_epoch}-BatchSize{bs}-HiddenState{hs}-Dropout{do}...\"):\n",
    "            # your code here\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_hat = tmodel(X.to(device))\n",
    "\n",
    "            loss = loss_func(y_hat, Y.to(device))\n",
    "            #loss = loss_func(torch.log(y_hat+ eps), Y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if use_decay:\n",
    "                scheduler.step(loss)\n",
    "            \n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "\n",
    "\n",
    "        end = time.time()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"epoch{epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)},execution_time={str(datetime.timedelta(seconds = (end-start)))},lr={optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "        #scheduler.step()\n",
    "        loss_history += curr_epoch_loss\n",
    "    return tmodel, loss_history\n",
    "\n",
    "def eval_model(emodel, dataloader, device=None, model_name='unk'):\n",
    "    \"\"\"\n",
    "    :return:\n",
    "        pred_all: prediction of model on the dataloder.\n",
    "        Y_test: truth labels. Should be an numpy array of ints\n",
    "    TODO:\n",
    "        evaluate the model using on the data in the dataloder.\n",
    "        Add all the prediction and truth to the corresponding list\n",
    "        Convert pred_all and Y_test to numpy arrays \n",
    "    \"\"\"\n",
    "    device = device or torch.device('cpu')\n",
    "    emodel.eval()\n",
    "    pred_all = []\n",
    "    Y_test = []\n",
    "    for X, Y in tqdm(dataloader, desc=f\"Evaluating {model_name}...\"):\n",
    "        # your code here\n",
    "        y_hat = emodel(X.to(device))\n",
    "        \n",
    "        pred_all.append(y_hat.cpu().data.numpy())\n",
    "        Y_test.append(Y.cpu().data.numpy())\n",
    "        \n",
    "    pred_all = np.concatenate(pred_all, axis=0)\n",
    "    Y_test = np.concatenate(Y_test, axis=0)\n",
    "\n",
    "    return pred_all, Y_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Common prediction code***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(truth, pred):\n",
    "    \"\"\"\n",
    "    TODO: Evaluate the performance of the predictoin via AUROC, and F1 score\n",
    "    each prediction in pred is a vector representing [p_0, p_1].\n",
    "    When defining the scores we are interesed in detecting class 1 only\n",
    "    (Hint: use roc_auc_score and f1_score from sklearn.metrics, be sure to read their documentation)\n",
    "    return: auroc, f1\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "    # your code here\n",
    "    auroc = roc_auc_score(truth, pred[:,1])\n",
    "    f1 = f1_score(truth, np.argmax(pred,axis=1))\n",
    "    f1_macro = f1_score(truth, np.argmax(pred,axis=1),average='macro')\n",
    "    f1_micro = f1_score(truth, np.argmax(pred,axis=1),average='micro')\n",
    "\n",
    "    return auroc, f1, f1_macro, f1_micro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Common training code***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndEvaluate(train_loader, val_loader, model, model_desc, batch_name, results_file, disease, lr,  dataformat, embedding, device, n_epoch, do, batch_size, hs, cv, use_decay):\n",
    "            \n",
    "    return_val = False\n",
    "\n",
    "    start_train = time.time()\n",
    "    model, loss_history = train_model(model, train_loader, n_epoch=n_epoch, lr = lr, device=device, model_name=model_desc, use_decay=use_decay)\n",
    "    end_train = time.time()\n",
    "\n",
    "    try:\n",
    "        #Evaluate model\n",
    "        start_eval = time.time()\n",
    "        pred, truth = eval_model(model, val_loader, device=device, model_name=model_desc)\n",
    "        end_eval = time.time()\n",
    "\n",
    "        auroc, f1, f1_macro, f1_micro = evaluate_predictions(truth, pred)\n",
    "        runtime = f\"Trn,Eval,Ttl={str(datetime.timedelta(seconds = (end_train-start_train)))},{str(datetime.timedelta(seconds = (end_eval-start_eval)))},{str(datetime.timedelta(seconds = (end_eval-start_train)))}\"\n",
    "        runtime_sec = end_eval-start_train\n",
    "\n",
    "        return_val = True\n",
    "\n",
    "    except:\n",
    "        auroc = -1\n",
    "        f1=-1\n",
    "        f1_macro = -1\n",
    "        f1_micro = -1\n",
    "        runtime_sec = end_train-start_train\n",
    "        runtime = 'Failure'\n",
    "        print(\"Failure!\")\n",
    "\n",
    "    results_file_metrics = f\"{results_file}.csv\"\n",
    "    results_file_loss = f\"{results_file}_loss.csv\"\n",
    "\n",
    "    #Append to results\n",
    "    if os.path.exists(results_file_metrics):\n",
    "        results = pd.read_csv(results_file_metrics)\n",
    "    else:\n",
    "        results = pd.DataFrame(columns=result_cols)\n",
    "\n",
    "    result = pd.DataFrame(columns=result_cols,data=[[batch_name, disease,embedding,auroc,f1,f1_macro,f1_micro,runtime,runtime_sec,n_epoch, do, batch_size, hs, lr,str(cv)]])\n",
    "    results = pd.concat([results,result])\n",
    "\n",
    "    #Save results - overwrite so we can see progress\n",
    "    results.to_csv(results_file_metrics, index=False)\n",
    "\n",
    "    #write loss_history (Batch,Epoch,Loss)\n",
    "    df_loss = pd.DataFrame(loss_history)\n",
    "    df_loss = df_loss.rename(columns={0:\"Loss\"})\n",
    "    df_loss['Epoch'] = df_loss.index + 1\n",
    "    df_loss['Batch'] = batch_name\n",
    "    df_loss['Disease'] = disease\n",
    "    df_loss['Embedding'] = embedding\n",
    "    df_loss['Epochs'] = n_epoch\n",
    "    df_loss['Dropout'] = do\n",
    "    df_loss['BatchSize'] = batch_size\n",
    "    df_loss['Hidden'] = hs\n",
    "    df_loss['LR'] = lr\n",
    "\n",
    "\n",
    "    #Append to results\n",
    "    if os.path.exists(results_file_loss):\n",
    "        results = pd.read_csv(results_file_loss)\n",
    "    else:\n",
    "        results = pd.DataFrame(columns=result_loss_cols)\n",
    "\n",
    "    results = pd.concat([results,df_loss])\n",
    "\n",
    "    #Save results - overwrite so we can see progress\n",
    "    results.to_csv(results_file_loss, index=False)\n",
    "\n",
    "    return return_val"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Bag of Word Models****\n",
    "\n",
    "Here we do a final tokenization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, entry in enumerate(all_df['word_tokenized']):\n",
    "    Final_words = []\n",
    "    #print(entry)\n",
    "    for word in entry:\n",
    "        #print(word)\n",
    "        Final_words.append(word)\n",
    "    all_df.loc[index, 'text_final'] = str(Final_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a dataset to help load the data and provide a collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDFClinicalNotesDataset(Dataset):\n",
    "    def __init__(self, X_array, y):\n",
    "        df = pd.DataFrame(index=y.index)\n",
    "        \n",
    "        df['tfidf_vector'] = [vector.tolist() for vector in X_array]\n",
    "        \n",
    "        self.tfidf_vector = df.tfidf_vector.tolist()\n",
    "        self.targets = y.tolist()\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.tfidf_vector[i], self.targets[i])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    tfidf = torch.tensor([item[0] for item in batch]).float()\n",
    "    target = torch.tensor([int(item[1]==True) for item in batch]).long()\n",
    "\n",
    "    return tfidf, target        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our PyTorch model.  Note we allow for the number of tokens, the final dropout, and the hidden size to be passed as parameters.  The model itself has 2 bidirectional LSTM layers, a dropout layer, and a fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClincalNoteTDFNet(nn.Module):\n",
    "    def __init__(self, tokens, dropout, hidden_size):\n",
    "        super(ClincalNoteTDFNet, self).__init__()\n",
    "        \n",
    "        self.tokens = tokens\n",
    "        self.dropout = dropout\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.hidden_dim1 = self.hidden_size\n",
    "        self.hidden_dim2 = int(self.hidden_size/2)\n",
    "        self.num_layers = 1\n",
    "\n",
    "        #Because it is bidirectional, the output from LTSM is coming in twice the size of the hidden states required.\n",
    "        #input is (batch, #of tokens)\n",
    "        self.bilstm1 = nn.LSTM(input_size = self.tokens, hidden_size = int(self.hidden_dim1/2), bidirectional = True,  \n",
    "                               batch_first = True, num_layers = self.num_layers) \n",
    "        \n",
    "        self.bilstm2 = nn.LSTM(input_size = self.hidden_dim1, hidden_size = int(self.hidden_dim2/2), bidirectional = True,  \n",
    "                               batch_first = True, num_layers=self.num_layers)\n",
    "\n",
    "        self.do = nn.Dropout(self.dropout)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(self.hidden_dim2, 2)\n",
    " \n",
    "    def forward(self, x):\n",
    "\n",
    "        x, states = self.bilstm1(x)\n",
    "        x, states = self.bilstm2(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.do(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a vocabulary that can be used within feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.feature_selection import SelectKBest, SelectFromModel\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
    "\n",
    "def getVocab(X_train, y_train, feature, max_tokens):\n",
    " \n",
    "    ## Step 1: Determine the Initial Vocabulary\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    vocab = list(tokenizer.word_index.keys())\n",
    "\n",
    "    ## Step 2: Create term  matrix\n",
    "    vectors = tokenizer.texts_to_matrix(X_train, mode='count')\n",
    "\n",
    "    ## Do feature selection on term matrix (column headers are words)\n",
    "    X = vectors\n",
    "    y = y_train\n",
    "\n",
    "    ##Choose algorithm\n",
    "    if feature == 'SelectKBest':\n",
    "        selector = SelectKBest(score_func=f_classif, k=max_tokens).fit(X,y)\n",
    "    else: \n",
    "        if feature == 'InfoGainAttributeVal':\n",
    "            #This should be similar to the InfoGain?\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=max_tokens).fit(X,y)\n",
    "        else:\n",
    "            #default to ExtraTreeClassifier\n",
    "            estimator = ExtraTreeClassifier(random_state = seed)\n",
    "            #selector = SelectFromModel(estimator, max_features = tokens,threshold=-np.inf)\n",
    "            selector = SelectFromModel(estimator, max_features = max_tokens)\n",
    "            selector = selector.fit(X, y)\n",
    "\n",
    "    support_idx = selector.get_support(True)\n",
    "    \n",
    "    #print(\"Vocab:\", [vocab[i-1].replace(\"'\",\"\") for i in support_idx])\n",
    "    tokenizer2 = Tokenizer()\n",
    "    tokenizer2.fit_on_texts([vocab[i-1].replace(\"'\",\"\") for i in support_idx])\n",
    "    new_vocab = list(tokenizer2.word_index.keys())\n",
    "\n",
    "    return new_vocab\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function to train and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterateTrainAndEvaluateTFIDF(df, k, disease_list, feature_list, lr_list, \n",
    "                            batch_name, results_file, batch_size, dataformat, device, tokens, epoch_list, do, hs, cv = False, use_decay=False):\n",
    "\n",
    "    for _,disease in enumerate(disease_list):\n",
    "        for features_idx,feature in enumerate(feature_list):\n",
    "            lr = lr_list[features_idx]\n",
    "            n_epoch = epoch_list[features_idx]\n",
    "\n",
    "            #Create a name for the model\n",
    "            model_name = f\"{disease}_{feature}_{batch_name}\"\n",
    "\n",
    "            disease_df = df[df['disease'] == disease].copy()\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(disease_df[dataformat], disease_df['judgment'], test_size=0.2, random_state=seed)\n",
    "\n",
    "            if feature != 'All':\n",
    "                vocab = getVocab(X_train,y_train, feature, tokens)\n",
    "                Tfidf_vect = TfidfVectorizer(max_features=tokens,vocabulary = vocab)\n",
    "            else:\n",
    "                Tfidf_vect = TfidfVectorizer(max_features=tokens)\n",
    "\n",
    "            X_train_values_list = Tfidf_vect.fit_transform(X_train).toarray()\n",
    "            X_training = pd.DataFrame(X_train_values_list, columns=Tfidf_vect.get_feature_names())\n",
    "            X_training = np.asarray(X_training, dtype=float)\n",
    "            X_training = torch.from_numpy(X_training).to(device)\n",
    "\n",
    "            X_test_values_list = Tfidf_vect.transform(X_test).toarray()\n",
    "            X_testing = pd.DataFrame(X_test_values_list, columns=Tfidf_vect.get_feature_names())\n",
    "            X_testing = np.asarray(X_testing, dtype=float)\n",
    "            X_testing = torch.from_numpy(X_testing).to(device)\n",
    "\n",
    "            tokens_to_use = X_training.shape[1]\n",
    "\n",
    "            #Create model\n",
    "            model = ClincalNoteTDFNet(tokens_to_use,do,hs)\n",
    "            model = model.to(device)\n",
    "\n",
    "            ds_train = TDFClinicalNotesDataset(X_training, y_train)\n",
    "            ds_test = TDFClinicalNotesDataset(X_testing, y_test)\n",
    "\n",
    "            #Load Data \n",
    "            train_loader = torch.utils.data.DataLoader(ds_train, batch_size = batch_size, collate_fn=collate_fn)\n",
    "            val_loader = torch.utils.data.DataLoader(ds_test, batch_size = batch_size,collate_fn=collate_fn)\n",
    "\n",
    "            model_desc = f\"{disease}_{feature}\"\n",
    "\n",
    "            trainAndEvaluate(train_loader, val_loader, model, model_desc, batch_name, results_file, disease, lr, dataformat, feature, device, n_epoch,  do, batch_size, hs, False, use_decay)\n",
    "\n",
    "            #Save model\n",
    "            torch.save(model.state_dict(), f'{MODELS_PATH}{model_name}.pkl')\n",
    "\n",
    "            #Delete model\n",
    "            del model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function to train and evaluate the model while varying feature selection, learning rate, number of epochs, batch size, and dropout to help tune the hyper parameters usesd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterateTrainAndEvaluateTFIDFHP(df, k, disease_list, feature_list, lr_list, \n",
    "                            batch_name, results_file, batch_size_list, dataformat, device, tokens, epoch_list, dropout_list, hs_list, cv = False, use_decay=False):\n",
    "\n",
    "    for _,disease in enumerate(disease_list):\n",
    "        for _,feature in enumerate(feature_list):\n",
    "            for _,lr in enumerate(lr_list):\n",
    "                for _, n_epoch in enumerate(epoch_list):\n",
    "                    for _,batch_size in enumerate(batch_size_list):\n",
    "                        for _, do in enumerate(dropout_list):\n",
    "                            for _, hs in enumerate(hs_list):\n",
    "                                #Create a name for the model\n",
    "                                model_name = f\"{disease}_{feature}_{batch_name}\"\n",
    "\n",
    "                                disease_df = df[df['disease'] == disease].copy()\n",
    "\n",
    "                                X_train, X_test, y_train, y_test = train_test_split(disease_df[dataformat], disease_df['judgment'], test_size=0.2, random_state=seed)\n",
    "\n",
    "                                if feature != 'All':\n",
    "                                    vocab = getVocab(X_train,y_train, feature, tokens)\n",
    "                                    Tfidf_vect = TfidfVectorizer(max_features=tokens,vocabulary = vocab)\n",
    "                                else:\n",
    "                                    Tfidf_vect = TfidfVectorizer(max_features=tokens)\n",
    "\n",
    "                                X_train_values_list = Tfidf_vect.fit_transform(X_train).toarray()\n",
    "                                X_training = pd.DataFrame(X_train_values_list, columns=Tfidf_vect.get_feature_names())\n",
    "                                X_training = np.asarray(X_training, dtype=float)\n",
    "                                X_training = torch.from_numpy(X_training).to(device)\n",
    "\n",
    "                                X_test_values_list = Tfidf_vect.transform(X_test).toarray()\n",
    "                                X_testing = pd.DataFrame(X_test_values_list, columns=Tfidf_vect.get_feature_names())\n",
    "                                X_testing = np.asarray(X_testing, dtype=float)\n",
    "                                X_testing = torch.from_numpy(X_testing).to(device)\n",
    "\n",
    "                                tokens_to_use = X_training.shape[1]\n",
    "\n",
    "                                #Create model\n",
    "                                model = ClincalNoteTDFNet(tokens_to_use,do,hs)\n",
    "                                model = model.to(device)\n",
    "\n",
    "                                ds_train = TDFClinicalNotesDataset(X_training, y_train)\n",
    "                                ds_test = TDFClinicalNotesDataset(X_testing, y_test)\n",
    "\n",
    "                                #Load Data \n",
    "                                train_loader = torch.utils.data.DataLoader(ds_train, batch_size = batch_size, collate_fn=collate_fn)\n",
    "                                val_loader = torch.utils.data.DataLoader(ds_test, batch_size = batch_size,collate_fn=collate_fn)\n",
    "\n",
    "                                model_desc = f\"{disease}_{feature}\"\n",
    "\n",
    "                                trainAndEvaluate(train_loader, val_loader, model, model_desc, batch_name, results_file, disease, lr, dataformat, feature, device, n_epoch,  do, batch_size, hs, False, use_decay)\n",
    "                                #Delete model\n",
    "                                del model\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where we run a parameter sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "#Override these if need be\n",
    "disease_list = ['Asthma', 'CAD', 'CHF', 'Depression', 'Diabetes', 'Gallstones', 'GERD', 'Gout', 'Hypercholesterolemia', 'Hypertension', 'Hypertriglyceridemia', 'OA', 'OSA', 'PVD', 'Venous Insufficiency', 'Obesity']\n",
    "feature_list = ['All','ExtraTreeClassifier','SelectKBest','InfoGainAttributeVal']\n",
    "\n",
    "\n",
    "#0.01 seems to be the most effective with no decay\n",
    "lr_list = [0.01,0.001]\n",
    "epoch_list = [20,40,60]\n",
    "dropout_list = [0, 0.1, 0.5]\n",
    "batch_size_list = [32,64]\n",
    "hs_list = [64,128]  #can't go above 128, 256 worked, but pushed memoery to limit\n",
    "\n",
    "#training parameters\n",
    "k = 2\n",
    "\n",
    "#These should not change\n",
    "dataformat = 'text_final'\n",
    "tokens = 600\n",
    "\n",
    "results_file = f'{RESULTS_PATH}DL_tfidf_results'\n",
    "result_time = datetime.datetime.now()\n",
    "result_name = result_time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "descriptor = 'HP'\n",
    "batch_name = f'DL_tfidf_{descriptor}_{result_name}'\n",
    "\n",
    "#commented out because working on Embeddings\n",
    "#iterateTrainAndEvaluateTFIDFHP(all_df, k, disease_list, feature_list, lr_list, batch_name, results_file, batch_size_list, dataformat, device, tokens, epoch_list, dropout_list, hs_list, False, False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where we train the final models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "#Override these if need be\n",
    "disease_list = ['Asthma', 'CAD', 'CHF', 'Depression', 'Diabetes', 'Gallstones', 'GERD', 'Gout', 'Hypercholesterolemia', 'Hypertension', 'Hypertriglyceridemia', 'OA', 'OSA', 'PVD', 'Venous Insufficiency', 'Obesity']\n",
    "#disease_list = ['Asthma']\n",
    "feature_list = ['All','InfoGainAttributeVal','ExtraTreeClassifier','SelectKBest']\n",
    "\n",
    "#0.01 seems to be the most effective with no decay\n",
    "lr_list = [0.001,0.01,0.01,0.01]\n",
    "epoch_list = [20,40,60,40]\n",
    "dropout_list = [0, 0.1, 0.5]\n",
    "\n",
    "\n",
    "#training parameters\n",
    "k = 2\n",
    "do = .1\n",
    "batch_size = 32\n",
    "hs = 64\n",
    "\n",
    "#These should not change\n",
    "dataformat = 'text_final'\n",
    "tokens = 600\n",
    "\n",
    "results_file = f'{RESULTS_PATH}DL_tfidf_results'\n",
    "result_time = datetime.datetime.now()\n",
    "result_name = result_time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "descriptor = 'All'\n",
    "batch_name = f'DL_tfidf_{descriptor}_{result_name}'\n",
    "\n",
    "#commented out because working on Embeddings\n",
    "#iterateTrainAndEvaluateTFIDF(all_df, k, disease_list, feature_list, lr_list, batch_name, results_file, batch_size, dataformat, device, tokens, epoch_list, do, hs, False, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****DL Model using word embeddings****\n",
    "\n",
    "First we start by creating a dataset.  Note this will have to take the disease as part of the init and filter just for those records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ClinicalNoteDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, disease, dataformat):\n",
    "        \"\"\"\n",
    "        TODO: init the Dataset instance.  datafomat is just the column to use from the dataframe 'vector_tokenized' , 'one_hot'\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        self.disease = disease\n",
    "        self.dataformat = dataformat\n",
    "\n",
    "        if(self.dataformat == oai_col):\n",
    "            #overriding the dataframe and merging in the annoutations\n",
    "            dataframe = pd.merge(allannot_df,alldocs_df_aoai, on='id')\n",
    "\n",
    "        self.df = dataframe[dataframe['disease'] == disease].copy()\n",
    "        self.df = self.df.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        TODO: Denotes the total number of samples\n",
    "        \"\"\"\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        TODO: Generates one sample of data\n",
    "            return X, y for the i-th data.\n",
    "        \"\"\"\n",
    "        #Cannot make tensors yet, will need to happen in collate\n",
    "        Y = self.df.iloc[i]['judgment']\n",
    "        X = self.df.iloc[i][self.dataformat]\n",
    "\n",
    "        return X,Y\n",
    "        \n",
    "def vectorize_batch_words(batch):\n",
    "    embedding_size_used = 300\n",
    " \n",
    "    Xi, Yi = batch[0]\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    X = torch.zeros(batch_size, len(Xi), dtype=torch.long)\n",
    "    Y = torch.zeros((batch_size), dtype=torch.long)\n",
    "    \n",
    "    for i in range(len(batch)):\n",
    "        x, y = batch[i]\n",
    "        vectors = voc.lookup_indices(x)\n",
    "\n",
    "        X[i] = torch.tensor(vectors).long()\n",
    "        Y[i] = torch.tensor(float(y == True))\n",
    "\n",
    "    return X,Y\n",
    "\n",
    "def vectorize_batch_USE(batch):\n",
    "    embedding_size_used = 512\n",
    "\n",
    "    Xi, Yi = batch[0]\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    X = torch.zeros(batch_size, len(Xi), embedding_size_used, dtype=torch.float)\n",
    "    Y = torch.zeros((batch_size), dtype=torch.long)\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        x, y = batch[i]\n",
    "        \n",
    "        tensor_flow_vectors = use_embeddings(x)\n",
    "        array_vectors = tensor_flow_vectors.numpy()\n",
    "\n",
    "        X[i] = torch.tensor(array_vectors).float()\n",
    "        Y[i] = torch.tensor(float(y == True))\n",
    "\n",
    "    return X,Y \n",
    "\n",
    "def vectorize_batch_AOAI(batch):\n",
    "    embedding_size_used = aoai_embedding_size\n",
    "\n",
    "    Xi, Yi = batch[0]\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    X = torch.zeros(batch_size, Xi.shape[0], embedding_size_used, dtype=torch.float)\n",
    "    Y = torch.zeros((batch_size), dtype=torch.long)\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        x, y = batch[i]\n",
    "\n",
    "        #all the work was done and stored in another notebook\n",
    "        \n",
    "        X[i] = torch.tensor(x).float()\n",
    "        Y[i] = torch.tensor(float(y == True))\n",
    "\n",
    "    return X,Y \n",
    "          \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code prepares the weights matrix for the nn.Embedding object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_len = len(voc)\n",
    "glove_weights_matrix = np.zeros((matrix_len, word_embedding_size))\n",
    "fasttext_weights_matrix = np.zeros((matrix_len, word_embedding_size))\n",
    "\n",
    "#GloVe\n",
    "for i in range(0,matrix_len-1):\n",
    "    word = voc.lookup_token(i)\n",
    "    try: \n",
    "        glove_weights_matrix[i] = glove_embeddings.get_vecs_by_tokens(word)\n",
    "    except KeyError:\n",
    "        glove_weights_matrix[i] = np.random.normal(scale=0.6, size=(word_embedding_size, ))\n",
    "#FastText\n",
    "for i in range(0,matrix_len-1):\n",
    "    word = voc.lookup_token(i)\n",
    "    try: \n",
    "        fasttext_weights_matrix[i] = fasttext_embeddings.get_vecs_by_tokens(word)\n",
    "    except KeyError:\n",
    "        fasttext_weights_matrix[i] = np.random.normal(scale=0.6, size=(word_embedding_size, ))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our PyTorch model.  Note we allow for the embedding type, number of tokens, the final dropout, and the hidden size to be passed as parameters.  The model itself has 2 bidirectional LSTM layers, a dropout layer, and a fully connected layer.  For the word embeddings we use the nn.Embeddings object which really increases performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClincalNoteEmbeddingNet(nn.Module):\n",
    "    def __init__(self, embedding_type, max_tokens, dropout, hidden_size):\n",
    "        super(ClincalNoteEmbeddingNet, self).__init__()\n",
    "        \n",
    "        self.max_tokens = max_tokens\n",
    "        self.dropout = dropout\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if(embedding_type == 'USE'):\n",
    "            self.embedding_dimension = sentence_embedding_size\n",
    "            self.em = None\n",
    "        else:\n",
    "            if embedding_type == 'AOAI':\n",
    "                self.embedding_dimension = aoai_embedding_size\n",
    "                self.em = None                                        \n",
    "            else:\n",
    "                self.embedding_dimension = word_embedding_size\n",
    "                if(embedding_type == 'GloVe'):\n",
    "                    self.em = nn.Embedding.from_pretrained(torch.tensor(glove_weights_matrix).float(), freeze=False)\n",
    "                else:\n",
    "                    self.em = nn.Embedding.from_pretrained(torch.tensor(fasttext_weights_matrix).float(), freeze=False)\n",
    "\n",
    "        self.hidden_dim1 = self.hidden_size\n",
    "        self.hidden_dim2 = int(self.hidden_size/2)\n",
    "        self.num_layers = 1\n",
    "\n",
    "        #Because it is bidirectional, the output from LTSM is coming in twice the size of the hidden states required.\n",
    "        #input is (batch, #of tokens * embedding_dimension)\n",
    "        self.bilstm1 = nn.LSTM(input_size = self.embedding_dimension, hidden_size = int(self.hidden_dim1/2), bidirectional = True,  \n",
    "                               batch_first = True, num_layers = self.num_layers) \n",
    "        \n",
    "        self.bilstm2 = nn.LSTM(input_size = self.hidden_dim1, hidden_size = int(self.hidden_dim2/2), bidirectional = True,  \n",
    "                               batch_first = True, num_layers=self.num_layers)\n",
    "\n",
    "        self.do = nn.Dropout(self.dropout)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(self.hidden_dim2 * self.max_tokens, 2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #using an embedding layer instead of just vectors\n",
    "        if self.em is not None:\n",
    "            x = self.em(x)  \n",
    "\n",
    "        x, states = self.bilstm1(x)\n",
    "        x, states = self.bilstm2(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.do(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a function do do a parameter sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterateTrainAndEvaluateHP(df, k, disease_list, embedding_list, lr_list, \n",
    "                            batch_name, results_file, device, epoch_list, dropout_list, batch_size_list, hs_list, use_decay = False):\n",
    "\n",
    "    for _,disease in enumerate(disease_list):\n",
    "        for _,embedding in enumerate(embedding_list):\n",
    "            for _,lr in enumerate(lr_list):\n",
    "                for _, n_epoch in enumerate(epoch_list):\n",
    "                    for _,batch_size in enumerate(batch_size_list):\n",
    "                        for _, do in enumerate(dropout_list):\n",
    "                            for _, hs in enumerate(hs_list):\n",
    "                                #Create a name for the model\n",
    "                                model_name = f\"{disease}_{embedding}_{batch_name}\"\n",
    "\n",
    "                                #Create model\n",
    "                                if embedding == 'USE':\n",
    "                                    model_tokens = max_sentences\n",
    "                                else:\n",
    "                                    if embedding == 'AOAI':\n",
    "                                        model_tokens = max_sentences_aoai\n",
    "                                    else:\n",
    "                                        model_tokens = max_tokens\n",
    "                                    \n",
    "                                model = ClincalNoteEmbeddingNet(embedding, max_tokens = model_tokens, dropout = do, hidden_size = hs)\n",
    "                                model = model.to(device)\n",
    "\n",
    "                                if embedding == 'GloVe':\n",
    "                                    custom_collate=vectorize_batch_words\n",
    "                                    dataformat = 'vector_tokenized'\n",
    "                                if embedding == 'FastText':\n",
    "                                    custom_collate=vectorize_batch_words\n",
    "                                    dataformat = 'vector_tokenized'\n",
    "                                if embedding == 'USE':\n",
    "                                    custom_collate=vectorize_batch_USE\n",
    "                                    dataformat = 'sentence_tokenized'\n",
    "                                if embedding == 'AOAI':\n",
    "                                    custom_collate=vectorize_batch_AOAI\n",
    "                                    dataformat = oai_col\n",
    "\n",
    "                                ds = ClinicalNoteDataset(df, disease, dataformat)\n",
    "                                ds_train, ds_test = train_test_split(ds, test_size=0.20, shuffle=True, random_state = seed)\n",
    "\n",
    "                                #Load Data \n",
    "                                train_loader = torch.utils.data.DataLoader(ds_train, batch_size = batch_size, collate_fn=custom_collate)\n",
    "                                val_loader = torch.utils.data.DataLoader(ds_test, batch_size = batch_size, collate_fn=custom_collate)\n",
    "                                \n",
    "                                model_desc = f\"{disease}_{embedding}\"\n",
    "\n",
    "                                trainAndEvaluate(train_loader, val_loader, model, model_desc, batch_name, results_file, disease, lr, dataformat, embedding, device, n_epoch,  do, batch_size, hs, False, use_decay)\n",
    "\n",
    "                                #Save model - don't need to save for hyper parameter tuning\n",
    "                                #torch.save(model.state_dict(), f'{MODELS_PATH}{model_name}.pkl')\n",
    "\n",
    "                                #Delete model\n",
    "                                del model\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a function to help with training and cross validation (if required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterateTrainAndEvaluate(df, k, disease_list, embedding_list, lr_list, \n",
    "                            batch_name, results_file, device, epoch_list, do, batch_size, hs, cv = False, use_decay = False):\n",
    "\n",
    "    for _,disease in enumerate(disease_list):\n",
    "        for embedding_idx,embedding in enumerate(embedding_list):\n",
    "            lr = lr_list[embedding_idx]\n",
    "            n_epoch = epoch_list[embedding_idx]\n",
    "            #Create a name for the model\n",
    "            model_name = f\"{disease}_{embedding}_{batch_name}\"\n",
    "\n",
    "            #Create model\n",
    "            if embedding == 'USE':\n",
    "                model_tokens = max_sentences\n",
    "            else:\n",
    "                if embedding == 'AOAI':\n",
    "                    model_tokens = max_sentences_aoai \n",
    "                else:\n",
    "                    model_tokens = max_tokens\n",
    "                \n",
    "            model = ClincalNoteEmbeddingNet(embedding, max_tokens = model_tokens, dropout = do, hidden_size=hs)\n",
    "            model = model.to(device)\n",
    "\n",
    "            if embedding == 'GloVe':\n",
    "                custom_collate=vectorize_batch_words\n",
    "                dataformat = 'vector_tokenized'\n",
    "            if embedding == 'FastText':\n",
    "                custom_collate=vectorize_batch_words\n",
    "                dataformat = 'vector_tokenized'\n",
    "            if embedding == 'USE':\n",
    "                custom_collate=vectorize_batch_USE\n",
    "                dataformat = 'sentence_tokenized'\n",
    "            if embedding == 'AOAI':\n",
    "                custom_collate=vectorize_batch_AOAI\n",
    "                dataformat = oai_col\n",
    "\n",
    "            ds = ClinicalNoteDataset(df, disease, dataformat)\n",
    "            ds_train, ds_test = train_test_split(ds, test_size=0.20, shuffle=True, random_state = seed)\n",
    "\n",
    "            #Load Data \n",
    "            train_loader = torch.utils.data.DataLoader(ds_train, batch_size = batch_size, collate_fn=custom_collate)\n",
    "            val_loader = torch.utils.data.DataLoader(ds_test, batch_size = batch_size, collate_fn=custom_collate)\n",
    "            \n",
    "            model_desc = f\"{disease}_{embedding}\"\n",
    "\n",
    "            trainAndEvaluate(train_loader, val_loader, model, model_desc, batch_name, results_file, disease, lr, dataformat, embedding, device, n_epoch, do, batch_size, hs, False, use_decay)\n",
    "\n",
    "            #Save model\n",
    "            torch.save(model.state_dict(), f'{MODELS_PATH}{model_name}.pkl')\n",
    "\n",
    "            #Delete model\n",
    "            del model\n",
    "\n",
    "            if cv:\n",
    "                #note, cross validation is only used to validate the model works consistently\n",
    "                splits=KFold(n_splits=k,shuffle=True,random_state=seed)\n",
    "\n",
    "                for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(ds)))):\n",
    "                    #for now, let's keep the results at the fold level\n",
    "                    model = ClincalNoteEmbeddingNet(embedding, max_tokens = max_tokens)\n",
    "                    model = model.to(device)\n",
    "                    \n",
    "                    train_sampler = SubsetRandomSampler(train_idx)\n",
    "                    val_sampler = SubsetRandomSampler(val_idx)\n",
    "                    #Load Data \n",
    "                    train_loader = torch.utils.data.DataLoader(ds, batch_size = batch_size, sampler=train_sampler, collate_fn=custom_collate)\n",
    "                    val_loader = torch.utils.data.DataLoader(ds, batch_size = batch_size, sampler=val_sampler, collate_fn=custom_collate)\n",
    "                    \n",
    "                    model_desc = f\"{disease}_{embedding}_Fold{fold+1}\"\n",
    "\n",
    "                    trainAndEvaluate(train_loader, val_loader, model, model_desc, batch_name, results_file, disease, lr, dataformat, embedding, device, n_epoch, do, batch_size, hs, cv, use_decay)\n",
    "\n",
    "                    del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where we execute our parameter sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "#Override these if need be\n",
    "#disease_list = ['Asthma', 'CAD', 'CHF', 'Depression', 'Diabetes', 'Gallstones', 'GERD', 'Gout', 'Hypercholesterolemia', 'Hypertension', 'Hypertriglyceridemia', 'OA', 'OSA', 'PVD', 'Venous Insufficiency', 'Obesity']\n",
    "disease_list = ['Asthma']\n",
    "#embedding_list = ['USE','GloVe','FastText']\n",
    "embedding_list = ['AOAI']\n",
    "#epoch_list = [15,25,35]\n",
    "epoch_list = [25,50, 75]\n",
    "#0.01 seems to be the most effective, although FastText prefers 0.001\n",
    "lr_list = [0.01,0.001]\n",
    "dropout_list = [0, 0.1]\n",
    "batch_size_list = [32,64] #can't go above 128, 256 worked, but pushed memoery to limit\n",
    "hs_list = [64,128]\n",
    "\n",
    "\n",
    "results_file = f'{RESULTS_PATH}DL_embedding_results'\n",
    "\n",
    "#training parameters\n",
    "k = 2\n",
    "\n",
    "#These should not change\n",
    "\n",
    "result_time = datetime.datetime.now()\n",
    "result_name = result_time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "descriptor = 'HP_AOAI'\n",
    "batch_name = f'DL_er_{descriptor}_{result_name}'\n",
    "\n",
    "#iterateTrainAndEvaluateHP(all_df_expanded, k, disease_list, embedding_list, lr_list, batch_name, results_file, device, epoch_list, dropout_list, batch_size_list, hs_list, False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where we train the models for each disease and embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "#Override these if need be\n",
    "disease_list = ['Asthma', 'CAD', 'CHF', 'Depression', 'Diabetes', 'Gallstones', 'GERD', 'Gout', 'Hypercholesterolemia', 'Hypertension', 'Hypertriglyceridemia', 'OA', 'OSA', 'PVD', 'Venous Insufficiency', 'Obesity']\n",
    "embedding_list = ['AOAI','GloVe','FastText','USE']\n",
    "epoch_list = [50,25,25,50]\n",
    "lr_list = [0.01,0.01,0.001,0.01]\n",
    "\n",
    "results_file = f'{RESULTS_PATH}DL_embedding_results'\n",
    "\n",
    "#training parameters\n",
    "batch_size = 32\n",
    "k = 2\n",
    "dropout = 0.1\n",
    "hs = 128\n",
    "\n",
    "#These should not change\n",
    "\n",
    "result_time = datetime.datetime.now()\n",
    "result_name = result_time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "descriptor = 'All_HSChange64'\n",
    "batch_name = f'DL_er_{descriptor}_{result_name}'\n",
    "#iterateTrainAndEvaluate(all_df_expanded, k, disease_list, embedding_list, lr_list, batch_name, results_file, device, epoch_list, dropout, batch_size, hs, False, False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "10pK5od01jfTHJyLN94dJxEube3sJszFm",
     "timestamp": 1678482671183
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
