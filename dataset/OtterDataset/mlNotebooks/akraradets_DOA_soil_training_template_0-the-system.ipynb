{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The system\n",
    "\n",
    "In any typical Machine/Deep Learning (ML/DL) work, the components are (1) Data, (2) Model.\n",
    "\n",
    "Now, take a look at the `Explorer` on the left tab of `vs code`.\n",
    "\n",
    "The directoies look like this\n",
    "\n",
    "```\n",
    "./\n",
    "  |- .image/\n",
    "  |- .venv/      <--- Python Libraries\n",
    "  |- dataset/    <--- Where we keep dataset\n",
    "  |- models/     <--- Where we store model\n",
    "  |- workshop/\n",
    "  |- .Dockerfile\n",
    "  |- docker-compose.yml\n",
    "  |- Pipfile     <--- Where the list of needed Python libraries is\n",
    "  |- Pipfile.lock  <- Version control of Python libraries\n",
    "  |- README.md\n",
    "```\n",
    "\n",
    "In this workshop, we will go through the concept of ML/DL.\n",
    "\n",
    "*Note: We assume that you use `GitHub Codespaces`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload some data\n",
    "\n",
    "Download this [workshop.zip](https://drive.google.com/file/d/10CZ6VRNnX006BxWRYlyNk8BRYmvWaXpK/view?usp=share_link) from `Google Drive`.\n",
    "Then, upload the zip file to `Codespace`.\n",
    "\n",
    "Extract the zip file into the `dataset` folder.\n",
    "\n",
    "This is what you should have in the `dataset` folder.\n",
    "\n",
    "```txt\n",
    "./dataset\n",
    "  |- workshop/\n",
    "    |- OM/\n",
    "        |- iphone 11/\n",
    "            |- Indoor/\n",
    "                |- 1/\n",
    "                |- 2/\n",
    "                ...\n",
    "            |- Outdoor/\n",
    "                |- 1/\n",
    "                |- 2/\n",
    "                ...\n",
    "        |- samsung S21/\n",
    "            |- Indoor/\n",
    "                |- 1/\n",
    "                |- 2/\n",
    "                ...\n",
    "            |- Outdoor/\n",
    "                |- 1/\n",
    "                |- 2/\n",
    "                ...\n",
    "    |- meta.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understand Data a bit\n",
    "\n",
    "The sample of image from `OM/iphone 11/Indoor/1/IMG_7455.JPG` folder.\n",
    "\n",
    "<img src=\"../.image/sample.JPG\" width=\"200\"/>\n",
    "\n",
    "And this is the first top 5 row of `meta.csv` files.\n",
    "\n",
    "```\n",
    "id,value\n",
    "1,1.96\n",
    "2,1.72\n",
    "3,2.78\n",
    "4,2.62\n",
    "5,2.49\n",
    "```\n",
    "\n",
    "The dataset is structure as `dataset/<datset_name>/<element>/<device>/<environment>/<id>/<image_name>`\n",
    "\n",
    "Thus, the level of `OM` of this image is `1.96`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Python Dataset Class\n",
    "\n",
    "The first piece of code we need to write is `Dataset` class.\n",
    "This will represent data in the world of `Python`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision import io\n",
    "from enum import Enum\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "class Devices(Enum):\n",
    "    iphone_11 = 'iphone 11'\n",
    "    samsung_s21 = 'samsung S21'\n",
    "    all = '*'\n",
    "\n",
    "class Environments(Enum):\n",
    "    indoor = 'Indoor'\n",
    "    outdoor = 'Outdoor'\n",
    "    all = '*'\n",
    "\n",
    "class Elements(Enum):\n",
    "    om = 'OM'\n",
    "    p = 'P'\n",
    "    k = 'K'\n",
    "\n",
    "class Preprocessing(Enum):\n",
    "    training = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(350),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "\n",
    "    inferencing  = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(350),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "\n",
    "\n",
    "class SoilDataset_bigset(Dataset):\n",
    "    def __init__(self, dataset_name:str, element:Elements, device:Devices, \n",
    "                 environment:Environments, \n",
    "                 preprocessing:Preprocessing, \n",
    "                 clip_target:bool=False, \n",
    "                 normalize_target:bool=False):\n",
    "        self.element:Elements = element\n",
    "        self.clip_target:bool = clip_target\n",
    "        self.normalize_target:bool = normalize_target\n",
    "        \n",
    "        # Set max value for clipping and normalizing\n",
    "        self.max_value:float = 0\n",
    "        if(self.element == Elements.om):\n",
    "            self.max_value = 8.0\n",
    "        elif(self.element == Elements.p):\n",
    "            self.max_value = 1000.0\n",
    "        elif(self.element == Elements.k):\n",
    "            self.max_value = 1500.0\n",
    "\n",
    "        # BasePath of the dataset\n",
    "        base_path:str = \"../dataset\"\n",
    "        dataset_path:str = os.path.join(base_path, dataset_name)\n",
    "        if(os.path.exists(dataset_path) == False):\n",
    "            raise ValueError(f\"Path={dataset_path} is not exist. Execution path={os.getcwd()}\")\n",
    "\n",
    "        # Inside this path there must be a list of folders arange by mobile phone. Use device enum.\n",
    "        # Inside those mobile phone are 2 folders indicate the environment the image was taken in. Use environment enum.\n",
    "        image_folder = os.path.join(dataset_path, element.value, device.value, environment.value)\n",
    "        self.imgs = glob(os.path.join(image_folder,'*/*'))\n",
    "        print(f\"Found {len(self.imgs)} images in {image_folder}.\")\n",
    "\n",
    "        # Load csv file for lookup the target value\n",
    "        target_path:str = os.path.join(dataset_path,element.value,'meta.csv')\n",
    "        self.target_df = pd.read_csv(target_path, index_col='id')\n",
    "\n",
    "        self.signature = os.path.join(element.value,device.value,environment.value)\n",
    "        self.preprocessing = preprocessing.value\n",
    "\n",
    "    def get_target(self, img_path:str) -> float:\n",
    "        assert len(img_path.split('/')) == 8, f\"Expect img_path to have 8 folders but got {img_path=}\"\n",
    "        target_id = int(img_path.split('/')[6])\n",
    "        target_value = float(self.target_df.loc[target_id].iloc[0]) # type:ignore\n",
    "        if(self.clip_target and (target_value > self.max_value)):\n",
    "            target_value = self.max_value\n",
    "        if(self.normalize_target):\n",
    "            target_value = target_value / (self.max_value)\n",
    "        return float(target_value)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.imgs[idx]\n",
    "        y = self.get_target(img_path=img_path)\n",
    "        y = torch.tensor(y)\n",
    "        X = io.read_image(img_path)\n",
    "        if self.preprocessing:\n",
    "            X = self.preprocessing(X)\n",
    "        return X.float(), y.float(), img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images in ../dataset/workshop/OM/*/Outdoor.\n"
     ]
    }
   ],
   "source": [
    "dataset = SoilDataset_bigset(dataset_name=\"workshop\", \n",
    "                             element=Elements.om, \n",
    "                             device=Devices.all, \n",
    "                             environment=Environments.outdoor, \n",
    "                             preprocessing=Preprocessing.training, \n",
    "                             clip_target=True, normalize_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images in ../dataset/workshop/OM/samsung S21/*.\n"
     ]
    }
   ],
   "source": [
    "# Try to load the dataset that only get `samsung S21`\n",
    "\n",
    "dataset = SoilDataset_bigset(dataset_name=\"workshop\", \n",
    "                             element=Elements.om, \n",
    "                             device=Devices.samsung_s21, \n",
    "                             environment=Environments.all, \n",
    "                             preprocessing=Preprocessing.training, \n",
    "                             clip_target=True, normalize_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images in ../dataset/workshop/OM/*/Outdoor.\n"
     ]
    }
   ],
   "source": [
    "# Try to load the dataset that only get `Outdoor` images from `all` devices\n",
    "dataset = SoilDataset_bigset(dataset_name=\"workshop\",\n",
    "                            element=Elements.om,\n",
    "                            device=Devices.all,\n",
    "                            environment=Environments.outdoor,\n",
    "                            preprocessing=Preprocessing.training,\n",
    "                            clip_target=True, normalize_target=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminalogy\n",
    "\n",
    "![Alt text](https://useruploads.socratic.org/vcKPankTBCVld2hWf2dw_7182277_orig.jpg)\n",
    "\n",
    "$ y = ax+b $ here $y$ is **dependent variable**, $x$ is **independent variable**, $a$ is **effect size**, and $b$ is **bias**.\n",
    "\n",
    "They also have different name based on researcher from each field.\n",
    "\n",
    "X:\n",
    "- Inputs\n",
    "- Features\n",
    "- Feature Vector\n",
    "- Independent/Explanatory/Exogenous variables\n",
    "- Predictor variables\n",
    "\n",
    "Y:\n",
    "- Outputs\n",
    "- Labels (known outcomes)\n",
    "- Dependent/Explained/Predicted variable\n",
    "- Outcome\n",
    "- Target\n",
    "\n",
    "A:\n",
    "- Slope\n",
    "- Effect\n",
    "\n",
    "B:\n",
    "- Bias\n",
    "- Intercept\n",
    "\n",
    "Together (a,b) is *weight*.\n",
    "\n",
    "In mathematic, we can also abstract the equation into a `function of` $y = f(x)$ or $f: X -> y$.\n",
    "\n",
    "In machine learning, we use the term `hypothesis` instead of function, so you may see $y = h(x)$ or $h: X -> y$\n",
    "\n",
    "Then, we want to differentiate the actual target with the predicted target so we use $\\hat{y}$ to indicate prediction.\n",
    "\n",
    "Finally, the aggrement on using $x$ for vector and $X$ for matrix. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model\n",
    "\n",
    "> A statistical model is a mathematical model that embodies a set of statistical assumptions concerning the generation of sample data (and similar data from a larger population). A statistical model represents, often in considerably idealized form, the data-generating process. When referring specifically to probabilities, the corresponding term is probabilistic model.\n",
    ">\n",
    "> [ref](https://en.wikipedia.org/wiki/Statistical_model)\n",
    "\n",
    "In short, a model is an equation. Modelling is an activity to find the model that get $\\hat{y}$ that very close to $y$.\n",
    "\n",
    "Here in Soil project, we want to read image and predict the level of something. Our images is $X$ and the level of something is $y$.\n",
    "\n",
    "The common family of model that we use for image sample is *Convolutional Neural Network (CNN)*.\n",
    "\n",
    "![Alt text](../.image/CNN.webp)\n",
    "\n",
    "For now, we see this model as $h()$ function that takes $X$ and give you $\\hat{y}$.\n",
    "\n",
    "Let's load the model name `AlexNet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from torchvision.models import AlexNet_Weights\n",
    "\n",
    "model = models.alexnet(weights=AlexNet_Weights.DEFAULT, progress=True)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems\n",
    "\n",
    "There are mainly two problems in ML/DL; (1) Regression problem and (2) classification problem.\n",
    "\n",
    "It is easy to identify. When $y$ is a continuous value, it is regression. When $y$ is a discrete value, it is classification.\n",
    "\n",
    "Our $y$ in Soil project is continuous, therefor, our problem is a regression problem.\n",
    "\n",
    "The answer from `h()` is in range ($-\\infty$, $\\infty$). \n",
    "\n",
    "We need one small modification to the `AlexNet` in order for it to answer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.eval of AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier[6] = torch.nn.Linear(in_features=4096, out_features=1, bias=True)\n",
    "model.eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using the model\n",
    "\n",
    "Let's just see the model in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y=tensor(0.3713), y_hat=tensor([[-0.0566]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x,y,image_path = dataset.__getitem__(99)\n",
    "\n",
    "# y_hat = h(x)\n",
    "y_hat = model(x.reshape((1,3,224,224)))\n",
    "\n",
    "print(f\"{y=}, {y_hat=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training\n",
    "\n",
    "You can see, using the model is pretty simple and a bit boring, to be frank.\n",
    "\n",
    "The prediction is also no where close to the answer.\n",
    "\n",
    "So, we now proceed to the training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset=dataset, lengths=[0.8,0.2], generator=torch.Generator().manual_seed(42))\n",
    "train_dataset.dataset.preprocessing = Preprocessing.training.value # type:ignore\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=4, shuffle=True,  num_workers=2)\n",
    "test_loader  = DataLoader(dataset=test_dataset,  batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def train_test(model:torch.nn.Module, train_loader:DataLoader, test_loader:DataLoader, epochs:int, lr:float, DEVICE:torch.device):\n",
    "    J_fn = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    import time\n",
    "    #for epochs\n",
    "    best_loss = math.inf\n",
    "\n",
    "    for e in range(epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        train_mse = 0\n",
    "        for b, (image, label, _) in enumerate(train_loader):\n",
    "            # print(f'start:{b}')\n",
    "            #image: (B, C, W, H)\n",
    "            #label: (B)\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            \n",
    "            yhat = model(image) #1. model\n",
    "            train_loss = J_fn(yhat.reshape(-1), label.reshape(-1)) #2. loss\n",
    "            #2.1 collect the loss and acc\n",
    "\n",
    "            optimizer.zero_grad() #3. zero_grad\n",
    "            train_loss.backward() #4. backward\n",
    "            optimizer.step() #5. step\n",
    "            \n",
    "            train_mse += train_loss.detach().cpu()\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"TRAIN|{e=} {total_time=} {train_mse}\")\n",
    "        \n",
    "        if( train_mse <= best_loss ):\n",
    "            print('save model!!')\n",
    "            torch.save(model.state_dict(), \"../models/alex.pth\")\n",
    "            best_loss = train_mse\n",
    "        # Testing\n",
    "        start_time = time.time()\n",
    "        model.eval()\n",
    "        test_mse = 0\n",
    "        for b, (image, label, _) in enumerate(test_loader):\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            yhat = model(image) #1. model\n",
    "            test_loss = J_fn(yhat.reshape(-1), label.reshape(-1)) #2. loss\n",
    "            test_mse += test_loss.detach().cpu()\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"TEST |{e=} {total_time=} {test_mse}\")\n",
    "        \n",
    "                \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN|e=0 total_time=38.60555672645569 142.1394500732422\n",
      "save model!!\n",
      "TEST |e=0 total_time=4.058891534805298 0.2912577986717224\n",
      "TRAIN|e=1 total_time=41.52930736541748 0.8152303099632263\n",
      "save model!!\n",
      "TEST |e=1 total_time=4.186108112335205 0.2021433711051941\n",
      "TRAIN|e=2 total_time=41.038854360580444 0.7103885412216187\n",
      "save model!!\n",
      "TEST |e=2 total_time=4.221719026565552 0.3206513226032257\n",
      "TRAIN|e=3 total_time=40.8500235080719 0.8834637403488159\n",
      "TEST |e=3 total_time=3.835456371307373 0.10868373513221741\n",
      "TRAIN|e=4 total_time=39.56671380996704 0.6896519660949707\n",
      "save model!!\n",
      "TEST |e=4 total_time=4.262863874435425 0.26721158623695374\n",
      "TRAIN|e=5 total_time=41.29210567474365 0.6864010691642761\n",
      "save model!!\n",
      "TEST |e=5 total_time=4.066243886947632 0.1045185849070549\n",
      "TRAIN|e=6 total_time=39.46724820137024 0.6413669586181641\n",
      "save model!!\n",
      "TEST |e=6 total_time=4.070074796676636 0.14533808827400208\n",
      "TRAIN|e=7 total_time=39.22891902923584 0.6574790477752686\n",
      "TEST |e=7 total_time=3.857133388519287 0.13205723464488983\n",
      "TRAIN|e=8 total_time=38.13504099845886 0.6854099631309509\n",
      "TEST |e=8 total_time=3.8698630332946777 0.10441426932811737\n",
      "TRAIN|e=9 total_time=36.57987642288208 0.6833851933479309\n",
      "TEST |e=9 total_time=3.672713041305542 0.10252056270837784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 10\n",
    "# learning_rate\n",
    "lr = 0.001\n",
    "DEVICE = 'cpu'\n",
    "train_test(model=model, train_loader=train_loader, test_loader=test_loader, epochs=epochs, lr=lr, DEVICE=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y=tensor(0.2450), y_hat=tensor([[0.2450]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x,y,image_path = dataset.__getitem__(0)\n",
    "\n",
    "y_hat = model(x.reshape((1,3,224,224)))\n",
    "\n",
    "print(f\"{y=}, {y_hat=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
