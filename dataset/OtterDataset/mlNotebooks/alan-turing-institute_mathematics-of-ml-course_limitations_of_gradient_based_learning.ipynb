{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "limitations_of_gradient_based_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X052qCZh00o"
      },
      "source": [
        "# Limitations of Gradient-Based Learning\n",
        "\n",
        "<font color='green'> In this practical session we explore some limitations of learning algorithms based on gradient updates. Our main objectives are the following:\n",
        "</font>\n",
        "- <font color='green'>introducing the problem of learning parity functions under a uniform distribution of covariates on the $d$-dimensional boolean hypercube;</font>\n",
        "- <font color='green'>understanding if the above problem is easy from the statistical perspective;</font>\n",
        "- <font color='green'>understanding if the above problem is easy from the computational perspective;</font>\n",
        "- <font color='green'>understanding intuitively why the above problem is not solvable in polynomial time for a large class of algorithms.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRPHmKQMiOwN"
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.experimental.numpy as tnp\n",
        "tnp.experimental_enable_numpy_behavior()\n",
        "plt.rcParams['figure.figsize'] = [9, 6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECKHl-WPsTlR"
      },
      "source": [
        "## The Problem of Learning Parity Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l34DTIMAlg5r"
      },
      "source": [
        "Learning parity functions is a classical example of a problem that is easy to state, yet hard to solve for a large class of machine learning algorithms (specifically, for *statistical query* algorithms; see the bibliographic remarks section). In this practical session, we explore why learning parity functions is difficult for parametric models trained by gradient descent.\n",
        "\n",
        "To introduce the problem, let $d$ denote the input dimension and let the input space be $\\mathcal{X} = \\{-1, +1\\}^{d}$ &mdash; the $d$-dimensional boolean hypercube. Given any subset $S \\subseteq \\{0, 1, \\dots, d - 1\\}$, the *parity function* $f_{S} : \\mathcal{X} \\to \\{-1, +1\\}$ is defined by\n",
        "$$\n",
        "  f_{S}(x) = \\prod_{i \\in S} x_{i}, \\quad \\text{where } x = (x_{0}\\, x_{1}\\, \\dots\\, x_{d-1})^{\\mathsf{T}} \\in \\mathcal{X}.\n",
        "$$\n",
        "Thus, $f_{S}(x)$ evaluates to $+1$ if the number of negative coordinates of $x$ with indexes in the support set $S$ is even; otherwise $f_{S}(x)$ evaluates to $-1$.\n",
        "\n",
        "In the following exercise, we are asked to implement functions for generating synthetic data for our experiments. Given natural numbers $n$, $d$ and a support set $S \\subset \\{0, \\dots, d-1\\}$, a sample dataset contains a matrix $X \\in \\{-1, +1\\}^{n \\times d}$ and a vector $y \\in \\{-1, +1\\}^{n}$. Letting $x_{i}$ denote the $i$-th row of the matrix $X$, the $i$-th data point is $(x_{i}, y_{i}) = (x_{i}, f_{S}(x_{i}))$. We will only consider covariate vectors $x_{i}$ whose elements are sampled independently and uniformly from the set $\\{-1, +1\\}$. To summarize, the data generating mechanism considered in this practical session can be described as follows:\n",
        "\\begin{align*}\n",
        "  X &\\sim \\text{Uniform}(\\mathcal{X}), \\\\\n",
        "  Y \\mid X = x &\\sim \\delta_{f_{S}(x)}\\text{ for some fixed parity function }f_{S}.\n",
        "\\end{align*}\n",
        "The notation $\\delta_{x}$ denotes a probability distribution that assigns all mass to the point $x$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfsLwa5glaaE"
      },
      "source": [
        "### Exercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOZ12WYViFEQ"
      },
      "source": [
        "Fill in the missing implementation details in the code cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JOOdRAbmAfI"
      },
      "source": [
        "class ParityFunction(object):\n",
        "\n",
        "  def __init__(self, S):\n",
        "    \"\"\" :S: A subset of {0,1,...,d-1} denoting the support of the parity\n",
        "          function represented by this object. \"\"\"\n",
        "    self.S = np.array(S)\n",
        "\n",
        "  def __call__(self, X):\n",
        "    \"\"\" :X: Either a d-dimensional vector or a matrix in {-1, +1}^{n \\times d},\n",
        "            whose each row contains an input x_{i} \\in {-1, +1}^{d}.\n",
        "        :returns: A vector in {-1, +1}^{n} whose i-th entry contains the output\n",
        "            of the parity function with support set self.S evaluated at x_{i}.\n",
        "    \"\"\"\n",
        "    ############################################################################\n",
        "    # Exercise 1.1. Fill in the missing code below.\n",
        "\n",
        "    ############################################################################\n",
        "\n",
        "def generate_uniform_covariates(n, d):\n",
        "    \"\"\" :n: The number of covariate vectors.\n",
        "        :d: The dimension of \n",
        "        :returns: A matrix in {-1, +1}^{n \\times d} with i.i.d. entries sampled\n",
        "            uniformly from {-1, +1}.\n",
        "    \"\"\"\n",
        "    ############################################################################\n",
        "    # Exercise 1.2. Fill in the missing code below.\n",
        "    \n",
        "    ############################################################################\n",
        "\n",
        "\n",
        "# We will now generate a small dataset with S={0,1}, d=4 and n=5.\n",
        "np.random.seed(0)\n",
        "X = generate_uniform_covariates(n=5, d=4)\n",
        "y = ParityFunction(S=[0,1])(X)\n",
        "print(\"X = \\n\", X)\n",
        "print(\"y = \\n\", y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4WVLOboo-Kr"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX7q94iRpA0O"
      },
      "source": [
        "Exercise 1.1.\n",
        "```\n",
        "  def __call__(self, X):\n",
        "    if len(X.shape) == 1:\n",
        "      # In case a vector was passed, convert it to a matrix.\n",
        "      X.reshape(1, -1)\n",
        "    y = np.prod(X[:, self.S], axis=1)\n",
        "    return y\n",
        "```\n",
        "\n",
        "Exercise 1.2.\n",
        "```\n",
        "return np.random.binomial(n=1, p=0.5, size=(n, d)) * 2.0 - 1.0\n",
        "````"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92ESYZOst_ci"
      },
      "source": [
        "## Training a Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZymgQXcqgCn"
      },
      "source": [
        "We will now attempt to learn a parity function by training a ReLU neural network with one hidden layer. First, let us generate training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwulokCorC-n"
      },
      "source": [
        "d = 50 # Dimension of the input space.\n",
        "n_train = d*10 # The number of data points for training.\n",
        "n_valid = 1000 # The number of data points for validating the learned function.\n",
        "\n",
        "# Let us start with a parity function supported on one variable only.\n",
        "# In Exercise 2 you will be asked to experiment with different supports.\n",
        "f_S = ParityFunction(S = [0])\n",
        "\n",
        "# Generate the covariates for training and validation.\n",
        "X_train = generate_uniform_covariates(n=n_train, d=d)\n",
        "X_valid = generate_uniform_covariates(n=n_valid, d=d)\n",
        "\n",
        "# Now generate the labels for training and validation.\n",
        "y_train = f_S(X_train)\n",
        "y_valid = f_S(X_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnTD38FIvoO8"
      },
      "source": [
        "In the following cell, we create a fully connected ReLU neural network with one hidden layer of hidden dimension ```d' = hidden_d```. Such a network can be expressed as a mapping\n",
        "$$\n",
        "  x \\mapsto W_2\\operatorname{ReLU}(W_1x + b_1) + b_2,\n",
        "$$\n",
        "where the first layer weights are $W_1 \\in \\mathbb{R}^{d' \\times d}, b_1 \\in \\mathbb{R}^{d'}$,\n",
        "and the second layer weights are $W_2 \\in \\mathbb{R}^{1 \\times d'}$, $b_{2} \\in \\mathbb{R}$. The function $\\operatorname{ReLU}$ applies a component-wise operation $x \\mapsto \\max(0, x)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FfdWlgEuBYY"
      },
      "source": [
        "hidden_d = 10*d # The hidden dimension denoted d' in the text cell above.\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(hidden_d, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation=None),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RYPA2O9xFuL"
      },
      "source": [
        "Notice that a neural network output is real-valued, while the parity function outputs take values in $\\{-1, +1\\}$. We can thus measure the accuracy of a trained ReLU network by computing the fraction of data points on which the sign of the ReLU network output agrees with the output of the true parity function that generated the data.\n",
        "\n",
        "At the same time, we need to specify a loss function and an optimization procedure for training the weights of our ReLU network. This is done in the following code cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1unVUUMyxF73"
      },
      "source": [
        "# Implement a function for tracking accuracy of the learned network.\n",
        "def accuracy(y_true, y_pred):\n",
        "  \"\"\" Given a real-valued prediction vector y_pred, calculate the proportion of\n",
        "  labels in y_pred whose signs agree with {-1, +1} valued y_true labels vector.\n",
        "  \"\"\"\n",
        "  y_pred_sign = tnp.sign(y_pred)\n",
        "  return tnp.average(y_true == y_pred_sign)\n",
        "  \n",
        "  #y_pred_sgn = tf.math.sign(y_pred)\n",
        "  #return tf.math.equal(y_true, y_pred_sgn)\n",
        "\n",
        "# Set the quadratic loss function and set the optimizer to gradient descent\n",
        "# with step size given by the learning_rate parameter.\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
        "              loss=tf.keras.losses.MeanSquaredError(),\n",
        "              metrics=[accuracy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf4umbMOnYuC"
      },
      "source": [
        "The below cell can be executed to print some summary statistics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF_U6Hnumz2l"
      },
      "source": [
        "# The parameters of the first layer are (W1, b1), hence the number of parameters\n",
        "# should be (d * d') + d'.\n",
        "# The parameters of the second layer are (W2, b2), hence the number of\n",
        "# parameters should be d' + 1.\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8pksiFWzwey"
      },
      "source": [
        "We are now ready to train our ReLU network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "legbxGCQvQ11"
      },
      "source": [
        "history = model.fit(\n",
        "    X_train, y_train, batch_size=n_train, \n",
        "    validation_data=(X_valid, y_valid), validation_batch_size=n_valid,\n",
        "    epochs=200,\n",
        "    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ0IfSGi00S5"
      },
      "source": [
        "# The below code visualizes the evolution of accuracy and loss curves throughout\n",
        "# the training process.\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.legend(['Training Accuracy', 'Validation Accuracy'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs Training Time')\n",
        "plt.show()\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['Training Loss', 'Validation Loss'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Loss vs Training Time')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21HEHf2xxyhF"
      },
      "source": [
        "### Exercise 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO-huEGt1g3j"
      },
      "source": [
        "In the above simulation we have observed that a two-layer ReLU network trained by gradient descent learns the correct parity function $f_{S}$ when the support set is $S=\\{0\\}$. Repeat the above experiment with parity functions supported on larger sets (e.g., $S=\\{0,1\\}; S=\\{0,1,\\dots,d-1\\}$; $S$ sampled uniformly at random). For what sizes of $S$ do you observe that the problem gets difficult? Explore the above experimental setup by:\n",
        "\n",
        "- modifying the neural network architecture (e.g., adding more layers, increasing/decreasing the hidden dimension, changing the activation function, etc);\n",
        "- modifying the optimization algorithm and its hyper-parameters;\n",
        "- modifying the dataset size and input dimension parameters.\n",
        "\n",
        "You may find it helpful to refer to Keras documentation available in the following link https://www.tensorflow.org/api_docs/python/tf/keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYxWXMd8pFOq"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGWQ9S1TQMqH"
      },
      "source": [
        "Exploring the above exercise you should have concluded that for large enough support sets $S$ (e.g., $|S|=20$) learning the true parity function becomes difficult regardless of the neural network architecture, choice of the optimizer, its hyper-parameters and other parameters of the problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4JnSQM7QEQD"
      },
      "source": [
        "## Is Learning Parity Functions Hard?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI27L7697IpS"
      },
      "source": [
        "In exercise 2, we have empirically concluded that learning parity functions\n",
        "is a difficult learning problem for neural networks trained by gradient descent.\n",
        "<font color='green'>**At this point, it is not clear if learning parities is possible for any algorithm, from either statistical or computational perspective.**</font> An open possibility remains that the problem of learning parities is difficult for any learning algorithm, and perhaps we have not observed anything special in the failure of neural networks trained by gradient descent. This section investigates whether learning parity functions is hard from a statistical perspective (Exercise 3) and whether it is hard from a computational perspective (Exercise 4)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzAtuIUV42z4"
      },
      "source": [
        "### Exercise 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnYcLep045YU"
      },
      "source": [
        "Argue why from an information-theoretic perspective, the problem of learning parity functions is not difficult. To do so, suggest a procedure that correctly learns the true parity function, disregarding any computational constraints (i.e., your algorithm is allowed to run in exponential time)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpBPz97Z6jpW"
      },
      "source": [
        "#### Hint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSAibJwp6qkL"
      },
      "source": [
        "There is a finite number of parity functions and the generated labels contain zero noise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0CCdbXP5Dib"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdXbZG3-5Ffz"
      },
      "source": [
        "Let $P$ denote the data generating distribution (i.e., the data points $(x_{i}, y_{i})$ are sampled i.i.d. from $P$). For a function $f : \\mathcal{X} \\to \\{-1, +1\\}$ denote its error as\n",
        "$$\n",
        "  \\operatorname{err}(f) = \\mathbb{P}_{(x,y) \\sim P}\\left( f(x) \\neq y\\right)\n",
        "$$\n",
        "\n",
        "\n",
        "For any function $f$, probability that it is consistent with the training data (i.e., correctly labels each of the data points) is at most\n",
        "$$\n",
        "  \\mathbb{P}\\left(f \\text{ is consistent with the training data }(X, y) \\right)\n",
        "  = (1 - \\operatorname{err}(f))^{n}\n",
        "  \\leq e^{-n \\operatorname{err}(f)},\n",
        "$$\n",
        "where $n$ is the number of data points.\n",
        "\n",
        "\n",
        "Let $\\mathcal{A} = \\{ f_{S} : S \\subseteq \\{0, \\dots, d-1\\}\\}$ denote the set of all parity functions. Fix any $\\varepsilon > 0$ and let $\\mathcal{A}_{\\mathrm{bad}} \\subseteq \\mathcal{A}$ denote the subset of parity functions whose error is at least $\\varepsilon$. It follows via the union bound that\n",
        "$$\n",
        "  \\mathbb{P}\\left( \\text{exists } f \\in \\mathcal{A}_{\\mathrm{bad}} \\text{ that is consistent with the training data (X, y) }\n",
        "    \\right)\n",
        "    \\leq |\\mathcal{A}_{\\mathrm{bad}}|e^{-n\\varepsilon}\n",
        "    \\leq 2^{d}e^{-n \\varepsilon}.\n",
        "$$\n",
        "For any $\\delta \\in (0,1)$, the above probability is at most $\\delta$ provided that $n \\geq \\frac{d\\log(2) + \\log(\\delta^{-1})}{\\varepsilon}$.\n",
        "\n",
        "<font color='green'>**In particular, the above argument establishes that for large enough sample sizes  N, it is enough to output any parity function that correctly labels the training data. Since there is a finite number of such functions, we can try all of them and return any parity function consistent with the training data. Such a parity function is guaranteed to exist since the training data is labeled by one such function.**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bX_i6vl-ydJ"
      },
      "source": [
        "### Exercise 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8NxU0EWBYKO"
      },
      "source": [
        "In Exercise 3, we have shown that the noise-free problem of learning parity functions is easy from an information-theoretic perspective; however, to do so we have provided an algorithm that requires exponential computational resources. This leaves an open possibility that no polynomial time algorithm can identify the correct parity function.\n",
        "\n",
        "In this exercise, you are asked to find a polynomial-time algorithm that learns the correct parity function, provided that the training data set is large enough (e.g., $n \\geq d$)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14kGigS_ByQj"
      },
      "source": [
        "def learn_the_correct_parity(X, y):\n",
        "  \"\"\" :X: A matrix in {-1, +1}^{n \\times d} of covariate vectors.\n",
        "      :y: A vector in {-1, +1}^{n} generated by some parity function f_{S}\n",
        "          applied to the rows of $X$.\n",
        "      :returns: A support set $S$ of the parity function f_{S} that generated\n",
        "          the labels.\n",
        "  \"\"\"\n",
        "  ##############################################################################\n",
        "  # Exercise 4. Implement a polynomial time algorithm that learns the true\n",
        "  # partity function.\n",
        "  \n",
        "  ##############################################################################\n",
        "\n",
        "\n",
        "# The below code tests our implementation.\n",
        "np.random.seed(0)\n",
        "d = 100\n",
        "S = np.random.choice(d, size = 10, replace=False)\n",
        "X = generate_uniform_covariates(n=d, d=d)\n",
        "print(X)\n",
        "print()\n",
        "y = ParityFunction(S=S)(X)\n",
        "print(\"True S =\\n\", np.sort(S))\n",
        "computed_S = learn_the_correct_parity(X, y)\n",
        "print(\"Computed S =\\n\", computed_S)\n",
        "print(\"Consistent output:\", np.allclose(y, ParityFunction(computed_S)(X)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOzCt1SfCV2s"
      },
      "source": [
        "#### Hint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl5FUhzwCj-T"
      },
      "source": [
        "Relabel $+1 \\mapsto 0$ and $-1 \\mapsto 1$ and think of parity functions as addition modulo 2 on $\\{0, 1\\}^{d}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jt1ycLE-z8f"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mkEXpCQUoy8"
      },
      "source": [
        "<font color='green'>**The key observation is that the problem we are trying to solve is just a linear system of equations over the finite field $\\text{GF}(2)$.**</font>\n",
        "Hence, we can recover the true parity function by simply solving the linear system (e.g., by performing Gaussian elimination). A sample implementation is provided below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au71IEiUEVCk"
      },
      "source": [
        "```\n",
        "  # First convert the data so that the elements of X,y are in {0,1} and we \n",
        "  # have y = Xw_{S} (where w_S represents the true parity function) and the\n",
        "  # addition is performed modulo 2.\n",
        "  X = (X - 1.0)/(-2.0) # -1 becomes 1, 1 becomes 0.\n",
        "  y = (y - 1.0)/(-2.0) # same as above.\n",
        "  X = X.astype(int)\n",
        "  y = y.astype(int)\n",
        "  # We will now implement Gaussian elimination to find the solution.\n",
        "  d = X.shape[1]\n",
        "  y = y.reshape(-1, 1)\n",
        "\n",
        "\n",
        "  def switch_rows(X, y, i, j):\n",
        "    \"\"\" Switches the i-th and j-th row in the matrix X and a vectory y (viewed\n",
        "    as an (n \\times 1) matrix. \"\"\"\n",
        "    X[[i,j],:] = X[[j,i],:]\n",
        "    y[[i,j],0] = y[[j,i],0]\n",
        "\n",
        "  for i in range(d):\n",
        "    # Switch some rows to make X[i,i]=1\n",
        "    if np.alltrue(X[i:, i] == 0):\n",
        "      # Skip this column, as all elements are equal to 0 below the i-th row.\n",
        "      continue\n",
        "    non_zero_idx = np.argmin(X[i:,i] == 0)+i\n",
        "    switch_rows(X, y, i, non_zero_idx)\n",
        "    # Right now we have X[i,i] = 1.0\n",
        "    # Perform all the row reduction operations for the i-th column, to make\n",
        "    # X[j,i] = 0 for all j not equal to i.\n",
        "    mask = X[:,i].copy().reshape(-1, 1)\n",
        "    mask[i,0] = 0 # Do not add the i-th row to itselt.\n",
        "    X += mask @ X[i,:].reshape(1, -1) # Perform the row operations on X\n",
        "    X %= 2\n",
        "    y += y[i,0] * mask # Perform the row operations on y\n",
        "    y %= 2\n",
        "  \n",
        "  S = []\n",
        "  for i in range(d):\n",
        "    if X[i,i] == 1 and y[i,0] == 1:\n",
        "      S.append(i)\n",
        "  return np.array(S)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oPnWb_tQcmT"
      },
      "source": [
        "## Understanding Why Neural Networks Fail"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMNcZhiXj-yq"
      },
      "source": [
        "One possible reason for failure to learn the correct parity function in our neural network experiments is that our chosen architecture was not expressive enough to contain the target parity functions. The below exercise asks us to rule out this possibility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqjBzsmbkTXO"
      },
      "source": [
        "### Exercise 5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyy8v8MUblRR"
      },
      "source": [
        "Complete the below code cell, which asks to find a configuration of ReLU network weights that realize *exactly* a given parity function $f_{S}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT9wt20Fksgi"
      },
      "source": [
        "# Fill in the missing code in the following function.\n",
        "def get_parity_relu_network(d, S):\n",
        "  \"\"\"\n",
        "  :d: The input dimension.\n",
        "  :S: An array specifying the indexes in {0, 1, ..., d-1} of the parity function\n",
        "      to be implemented by this function.\n",
        "  \n",
        "  :returns: A relu network with one hidden layer that implements the parity\n",
        "  function indexed by S.\n",
        "  \"\"\"\n",
        "\n",
        "  ##############################################################################\n",
        "  # Exercise 5.1. Set the hidden layer dimension required by your construction.\n",
        "\n",
        "  ##############################################################################\n",
        "\n",
        "  # Create a relu network with the specified dimensions.\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(d,)),\n",
        "    tf.keras.layers.Dense(hidden_d, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation=None),\n",
        "  ])\n",
        "\n",
        "  # Convert model parameters to numpy arrays.\n",
        "  W1, b1 = model.layers[0].get_weights()\n",
        "  W1 = np.array(W1).transpose()\n",
        "  b1 = np.array(b1)\n",
        "  W2, b2 = model.layers[1].get_weights()\n",
        "  W2 = np.array(W2).transpose()\n",
        "  b2 = np.array(b2)\n",
        "\n",
        "  # Recall that the above relu network implements the function\n",
        "  # x \\in \\R^{d} --> (W2) relu((W1) x + b1) + b2.\n",
        "  \n",
        "  ##############################################################################\n",
        "  # Exercise 5.2. Your code for setting W1, b1, W2, b2 goes below.\n",
        "  \n",
        "  ##############################################################################\n",
        "\n",
        "  # Set the model parameters with the above implemented construction.\n",
        "  model.layers[0].set_weights([W1.transpose(), b1])\n",
        "  model.layers[1].set_weights([W2.transpose(), b2])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "# The below code verifies if the implemented \"get_parity_relu_network\" function\n",
        "# agrees with the data generated via the ParitiesData class implemented earlier.\n",
        "d = 100\n",
        "# Sample a random support set S for the parity function f_S.\n",
        "coin_flips = np.random.binomial(n=1, p=0.5, size=d)\n",
        "S = np.arange(d)[coin_flips >= 1]\n",
        "f_S = ParityFunction(S=S)\n",
        "X = generate_uniform_covariates(n=2*d, d=d)\n",
        "y = f_S(X)\n",
        "\n",
        "parity_network = get_parity_relu_network(d, S)\n",
        "y_pred = np.array(parity_network(X)).flatten()\n",
        "\n",
        "# Check if the y_pred vector equals element-wise to the vector y.\n",
        "assert (y_pred == y).all()\n",
        "print(\"Assertion passed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKpn7FcNkZTZ"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxA1usBrYSt8"
      },
      "source": [
        "It is enough to set the hidden unit dimension to |S| + 1.\n",
        "```\n",
        "# Exercise 5.1.\n",
        "\n",
        "S = np.array(S)\n",
        "hidden_d = len(S)+1 # The hidden layer dimension of your construction. \n",
        "```\n",
        "Without loss of generality, assume that $S = \\{0, 1, \\dots, d-1\\}$, since otherwise the coordinates outside of the support set $S$ can be ignored. Suppose that an input vector $x \\in \\{-1, +1\\}^{d}$ contains $u$ entries equal to $+1$ and $v$ entries equal to $-1$. Hence, we have $u + v = d$.\n",
        "The output of the parity function $f_{S}$ is then equal to $+1$ if $v$ is even and $-1$ if $v$ is odd. Letting $\\mathbf{1} \\in \\mathbb{R}^{d}$ denote the vector of ones, we have $\\langle \\mathbf{1}, x \\rangle = u - v$. Therefore, we also have $\\langle \\mathbf{1}, x \\rangle = d - 2v$.\n",
        "\n",
        "**Setting the weights of the first layer.**\n",
        "\n",
        "Notice that if we set the first $d$ hidden units to $-\\frac{1}{2} \\mathbf{1}$, and if we set the first $d$ bias terms to $d_{i} = \\frac{d}{2} - i$ (with indexing starting from $0$), then the value computed by the $i$-th hidden unit is equal to $(W_{1}x)_{i} + b_{i} = v - i$. Finally, set the last, $d+1$-th hidden unit (i.e., the last row of the matrix $W_1$) to $-\\frac{1}{4}\\mathbf{1}$ and set the last bias term to $\\frac{d}{4}$. This results in the $d+1$-th activation equal to $\\frac{v}{2}$.\n",
        "\n",
        "**Setting the weights of the second layer.**\n",
        "The input received by the second layer is equal to\n",
        "$$\n",
        "  \\left(\\text{ReLU}(v), \\text{ReLu}(v-1), \\text{ReLu}(v-2), \\dots, \\text{ReLu}(v-d+1), \\frac{v}{2}\\right)^{\\mathsf{T}}.\n",
        "$$\n",
        "Setting the bias term $b_{2} = 1$ and the layer weights to the alternating pattern $-4, +4, -4, +4, \\dots$ realizes the desired parity function.\n",
        "\n",
        "\n",
        "Sample implementation of the above-described solution is provided below.\n",
        "```\n",
        "  # Exercise 5.2.\n",
        "\n",
        "  W1 *= 0\n",
        "  b1 *= 0\n",
        "  W2 *= 0\n",
        "  b2 *= 0\n",
        "  \n",
        "  # Set the weights of the first layer.\n",
        "  W1[:-1, S] = -1/2\n",
        "  b1[:-1] = len(S)/2 - np.arange(len(S))\n",
        "  W1[-1, S] = -1/4\n",
        "  b1[-1] = len(S)/4\n",
        "\n",
        "  # We now turn to the weights of the second layer.\n",
        "  W2[:] = 1\n",
        "  W2[0, ::2] = -1\n",
        "  W2[0, -1] = 1\n",
        "  W2 *= 4.\n",
        "  b2[:] = 1\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1qLY9HGHXqL"
      },
      "source": [
        "### Intuitive Reasons for Failure to Learn the True Parity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdRQ2zwvc98O"
      },
      "source": [
        "Let us summarize what we have discussed up to this point:\n",
        "- we found it challenging to learn the true parity function by training various architectures of neural networks;\n",
        "- we understood that the problem of learning parity functions is not hard from the statistical perspective;\n",
        "- we also understood that the problem is not difficult from a computational perspective;\n",
        "- we have also found that ReLU neural networks are expressive enough to realize parity functions with reasonably small weights.\n",
        "\n",
        "So why do the gradient methods fail to find the correct configuration of model weights? <font color='green'>**It turns out that this problem has nothing to do with neural networks; rather, it is known to be difficult (from a computational perspective) for a large class of algorithms known as statistical query algorithms.**</font> Such algorithms are, informally, procedures that do not inspect the individual data points but are instead based on some aggregate statistics of the observed data. Gradient descent can be shown to fall into this category (see the bibliographic remarks section)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDvoODkOe9GA"
      },
      "source": [
        "Let us now sketch the argument showing why gradient methods find it challenging to solve the parities problem. Let $\\mathcal{A} = \\{ a_{w} : \\{0,1\\}^{d} \\to \\mathbb{R} \\mid w \\in \\mathbb{R}^{m}\\}$ denote some class of parametric functions (such as neural networks considered above). Let $P_{S}$ denote the data generating distribution assuming that the true parity function is given by $f_{S}$ and as before, the covariates are sampled uniformly from the boolean hypercube. Let\n",
        "$$\n",
        "  r_{S}(a_w) = \\mathbf{E}_{(X,Y) \\sim P_{S}}[(a_w(X) - Y)^{2}]\n",
        "  = \\mathbf{E}_{(X,Y) \\sim P_{S}}[(a_w(X) - f_{S}(X))^{2}].\n",
        "$$\n",
        "Let $R_{S}$ denote the empirical risk function for a dataset of size $n$ sampled from the distribution $P_S$.\n",
        "When the dataset size $n$ is large enough, for any fixed point $w \\in \\mathbb{R}^{m}$ we have $\\nabla_{w} R_{S}(a_w) \\approx \\nabla_{w} r_{S}(a_w)$. <font color='green'>**We will now attempt to quantify the amount of information contained in the true gradient $\\nabla_{w} r_{S}(a_w)$.**</font>\n",
        "In order to do that, we will try to upper bound the variance of the gradient at an arbitrary point $w$, with the randomness coming from sampling $S$ uniformly over all possible subsets of $\\{0, 1, \\dots, d-1\\}$:\n",
        "$$\n",
        "  \\mathrm{Var}(\\nabla r, w)\n",
        "  = \\mathbf{E}_{S \\sim \\text{Uniform}} \\| \\nabla_{w} r_{S}(w) - \\mathbf{E}_{S' \\sim \\text{Uniform}}[\\nabla_{w} r_{S'}(w)]\\|_{2}^{2}.\n",
        "$$\n",
        "<font color='green'>**If the above quantity is small, it means that the gradient\n",
        "$\\nabla_{w} r_{S}(w)$ does not depend too much on $S$, and hence the gradient does not depend on the signal.**</font> We will now proceed to show that the below quantity is indeed exponentially small in the dimension $d$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXAS3WXgpOpj"
      },
      "source": [
        "First, notice that\n",
        "$$\n",
        "  \\nabla_{w}r_{S}(w) = \\mathbf{E}_{(X,Y)\\sim P_{S}}[\\nabla_{w} (a_w(X) - f_{S}(X))]\n",
        "  = \\mathbf{E}_{X}[2\\nabla_{w}a_{w}(X)(a_{w}(X) - f_{S}(X))].\n",
        "$$\n",
        "We hence have\n",
        "\\begin{align*}\n",
        "  \\frac{1}{4}\\mathrm{Var}(\\nabla r, w)\n",
        "  &= \\frac{1}{4}\\mathbf{E}_{S} \\| \\nabla_{w} r_{S}(w) - \\mathbf{E}_{S'}[\n",
        "    \\nabla_{w} r_{S'}(w)]\\|_{2}^{2}\n",
        "  \\\\\n",
        "  &\\leq \\frac{1}{4}\\mathbf{E}_{S} \\| \\nabla_{w} r_{S}(w) -\n",
        "  2\\mathbf{E}_{X}[\\nabla_{w}a_{w}(X)a_{w}(X)] \\|_{2}^{2}\n",
        "  \\\\\n",
        "  &= \\mathbf{E}_{S} \\|\\, \\mathbf{E}_{X}[\\nabla_{w}a_{w}(X)f_{S}(X)]\\, \\|_{2}^{2}.\n",
        "\\end{align*}\n",
        "To simplify the notation, let us now write $g(X) = \\nabla_{w}a_{w}(X)$.\n",
        "Further, let $\\langle f, g \\rangle_{P_{X}} = \\mathbf{E}_{X}[f(X)g(X)]$, where $X$ is distributed uniformly over the boolean hypercube. Using the fact that the $2^{d}$ parity functions form a basis over the Hilbert space $L_{2}(P_{X})$ of real-valued functions with the domain $\\{-1,1\\}^{d}$, we have\n",
        "\\begin{align*}\n",
        "  \\mathbf{E}_{S} \\sum_{i=1}^{m}\\left(\n",
        "    \\mathbf{E}_{X}[g_{i}(X)f_{S}(X)]\n",
        "    \\right)^{2} \n",
        "  &= \\mathbf{E}_{S} \\sum_{i=1}^{m}\\langle g_{i}, f_{S} \\rangle^{2}_{P_{X}} \\\\\n",
        "  &= \\frac{1}{2^{d}} \\sum_{S}\\sum_{i=1}^{m}\\langle g_{i}, f_{S} \\rangle^{2}_{P_{X}} \\\\\n",
        "  &= \\frac{1}{2^{d}} \\sum_{i=1}^{m} \\sum_{S} \\langle g_{i}, f_{S} \\rangle^{2}_{P_{X}} \\\\\n",
        "  &= \\frac{1}{2^{d}} \\sum_{i=1}^{m} \\|g_{i}\\|_{P_{X}}^{2} \\\\\n",
        "  &= \\frac{1}{2^{d}} \\mathbf{E}_{X}[\\|g(X)\\|_{2}^{2}].\n",
        "\\end{align*}\n",
        "To sum up the above derivations, we have shown that\n",
        "$$\n",
        "  \\mathrm{Var}(\\nabla r, w) \\leq \\frac{\\mathbf{E}_{X}[\\|g(X)\\|_{2}^{2}]}\n",
        "  {2^{d-2}}.\n",
        "$$\n",
        "<font color='green'>**In particular, the variance of the gradient with respect to uniform draw of a target parity function is exponentially small in the dimension $d$. This establishes that gradients for the parity problem are strongly concentrated in directions independent of the true signal.**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wdK4ezsQhKH"
      },
      "source": [
        "## Bibliographic Remarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHXqhJUSQpgX"
      },
      "source": [
        "The study of learning theory from the computational complexity theory point of view was pioneered by *Valiant [1984]*, where the so-called PAC learning framework was introduced. Various tweaks of the PAC learning models were subsequently studied. One such model motivated by the need to develop noise-tolerant learning algorithms, called the statistical\n",
        "query model, was introduced by *Kearns [1998]*, where it was also shown that the problem of learning parity functions cannot be solved using a polynomial number of queries; see also the textbook by *Kearns, Vazirani, and Vazirani [1994]*. Whether there exists an algorithm that can learn the underlying parity function in the presence of label noise is a long-standing\n",
        "open problem; see *Blum, Kalai, and Wasserman [2003]*. This practical session is based on Section 2 of *Shalev-Shwartz, Shamir, and Shammah [2017]*, where other limitations of gradient-based learning are also presented. Most of the learning algorithms used in practice can be implemented using statistical queries. For further background reading, see the discussions and references in the papers by *Feldman, Grigorescu, Reyzin, Vempala, and Xiao [2017]*, *Feldman [2017]*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6WgMbjIEiLD"
      },
      "source": [
        "**References**\n",
        "\n",
        "A. Blum, A. Kalai, and H. Wasserman. Noise-tolerant learning, the parity problem, and the statistical query model. Journal of the ACM (JACM), 50(4):506–519, 2003.\n",
        "\n",
        "V. Feldman. A general characterization of the statistical query complexity. In Conference on Learning Theory, pages 785–830. PMLR, 2017.\n",
        "\n",
        "V. Feldman, E. Grigorescu, L. Reyzin, S. S. Vempala, and Y. Xiao. Statistical algorithms and a lower bound for detecting planted cliques. Journal of the ACM (JACM), 64(2): 1–37, 2017.\n",
        "\n",
        "M. Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM (JACM), 45(6):983–1006, 1998.\n",
        "\n",
        "M. J. Kearns, U. V. Vazirani, and U. Vazirani. An introduction to computational learning theory. MIT press, 1994.\n",
        "\n",
        "S. Shalev-Shwartz, O. Shamir, and S. Shammah. Failures of gradient-based deep learning. In International Conference on Machine Learning, pages 3067–3075. PMLR, 2017.\n",
        "\n",
        "L. G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134–1142, 1984."
      ]
    }
  ]
}