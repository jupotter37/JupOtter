{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/HSE-LAMBDA/MLDM-2022/blob/main/09-neural-networks/PyTorch_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "EDtJOCB1sMsx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to PyTorch 🔥\n",
        "This tutorial is based on [official docs](https://pytorch.org/docs/stable/index.html) and [YSDA notebooks](https://github.com/yandexdataschool/Practical_DL), check them on your own as well. \n",
        "\n",
        "If you have existing code with data stored in NumPy\n",
        "ndarrays, you may wish to express that same data as PyTorch tensors,\n",
        "whether to take advantage of PyTorch’s GPU acceleration, or its\n",
        "efficient abstractions for building ML models. You may think of PyTorch as Numpy with autodiff. Check the repo: https://github.com/torch/torch7/wiki/Torch-for-Numpy-users"
      ],
      "metadata": {
        "id": "JeC-_qPtQLKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "numpy_array = np.ones((2, 3))\n",
        "print(numpy_array)\n",
        "\n",
        "pytorch_tensor = torch.from_numpy(numpy_array)\n",
        "print(pytorch_tensor)"
      ],
      "metadata": {
        "id": "N0wVCNuN9BzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch creates a tensor of the same shape and containing the same data\n",
        "as the NumPy array, going so far as to keep NumPy’s default 64-bit float\n",
        "data type.\n",
        "\n",
        "The conversion can just as easily go the other way:"
      ],
      "metadata": {
        "id": "74M_NQf09M84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pytorch_rand = torch.rand(2, 3)\n",
        "print(pytorch_rand)\n",
        "\n",
        "numpy_rand = pytorch_rand.numpy()\n",
        "print(numpy_rand)"
      ],
      "metadata": {
        "id": "Y8B4TKGZ9MR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to know that these converted objects are using the same underlying memory as their source objects, meaning that changes to one are reflected in the other:\n",
        "\n"
      ],
      "metadata": {
        "id": "VojHfAPn9W4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numpy_array[1, 1] = 23\n",
        "print(pytorch_tensor)\n",
        "\n",
        "pytorch_rand[1, 1] = 17\n",
        "print(numpy_rand)"
      ],
      "metadata": {
        "id": "OBv4aHf19YmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPVL2XUBsMO-"
      },
      "source": [
        "## PyTorch Tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLrU96cosMO_"
      },
      "source": [
        "Let’s see a few basic tensor manipulations. First, just a few of the\n",
        "ways to create tensors:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.empty(3, 4)\n",
        "print(type(x))\n",
        "print(x)"
      ],
      "metadata": {
        "id": "G419XQVrtKO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will probably see some random-looking values when printing your\n",
        "   tensor. The ``torch.empty()`` call allocates memory for the tensor,\n",
        "   but does not initialize it with any values - so what you’re seeing is\n",
        "   whatever was in memory at the time of allocation.\n",
        "\n",
        "More often than not, you’ll want to initialize your tensor with some\n",
        "value. Common cases are all zeros, all ones, or random values, and the\n",
        "``torch`` module provides factory methods for all of these:\n"
      ],
      "metadata": {
        "id": "Srxags4Wu5pc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEpt6mFbsMPA"
      },
      "outputs": [],
      "source": [
        "z = torch.zeros(5, 3)\n",
        "print(z)\n",
        "print(z.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zB9YrIrsMPA"
      },
      "outputs": [],
      "source": [
        "i = torch.ones((5, 3), dtype=torch.int16)\n",
        "print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FdfZwSJsMPB"
      },
      "source": [
        "It’s common to initialize learning weights randomly, often with a\n",
        "specific seed for the PRNG for reproducibility of results:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LStePn9EsMPB"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1729)\n",
        "r1 = torch.rand(2, 2)\n",
        "print('A random tensor:')\n",
        "print(r1)\n",
        "\n",
        "r2 = torch.rand(2, 2)\n",
        "print('\\nA different random tensor:')\n",
        "print(r2) # new values\n",
        "\n",
        "torch.manual_seed(1729)\n",
        "r3 = torch.rand(2, 2)\n",
        "print('\\nSame as r1:')\n",
        "print(r3) # repeats values of r1 because of re-seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSEAMtHisMPC"
      },
      "source": [
        "PyTorch tensors perform arithmetic operations intuitively. Tensors of\n",
        "similar shapes may be added, multiplied, etc. Operations with scalars\n",
        "are distributed over the tensor:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4efh2mBksMPC"
      },
      "outputs": [],
      "source": [
        "ones = torch.ones(2, 3)\n",
        "print(ones)\n",
        "\n",
        "twos = torch.ones(2, 3) * 2 # every element is multiplied by 2\n",
        "print(twos)\n",
        "\n",
        "threes = ones + twos       # addition allowed because shapes are similar\n",
        "print(threes)              # tensors are added element-wise\n",
        "print(threes.shape)        # this has the same dimensions as input tensors\n",
        "\n",
        "r1 = torch.rand(2, 3)\n",
        "r2 = torch.rand(3, 2)\n",
        "# uncomment this line to get a runtime error\n",
        "# r3 = r1 + r2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utuUatMWsMPC"
      },
      "source": [
        "Here’s a small sample of the mathematical operations available:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tW6oImXMsMPE"
      },
      "outputs": [],
      "source": [
        "r = (torch.rand(2, 2) - 0.5) * 2 # values between -1 and 1\n",
        "print('A random matrix, r:')\n",
        "print(r)\n",
        "\n",
        "# Common mathematical operations are supported:\n",
        "print('\\nAbsolute value of r:')\n",
        "print(torch.abs(r))\n",
        "\n",
        "# ...as are trigonometric functions:\n",
        "print('\\nInverse sine of r:')\n",
        "print(torch.asin(r))\n",
        "\n",
        "# ...and linear algebra operations like determinant and singular value decomposition\n",
        "print('\\nDeterminant of r:')\n",
        "print(torch.det(r))\n",
        "print('\\nSingular value decomposition of r:')\n",
        "print(torch.svd(r))\n",
        "\n",
        "# ...and statistical and aggregate operations:\n",
        "print('\\nAverage and standard deviation of r:')\n",
        "print(torch.std_mean(r))\n",
        "print('\\nMaximum value of r:')\n",
        "print(torch.max(r))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensor Data Types\n",
        "\n",
        "Setting the datatype of a tensor is possible a couple of ways:"
      ],
      "metadata": {
        "id": "uraV4r09vhrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.ones((2, 3), dtype=torch.int16)\n",
        "print(a)\n",
        "\n",
        "b = torch.rand((2, 3), dtype=torch.float64) * 20.\n",
        "print(b)\n",
        "\n",
        "c = b.to(torch.int32)\n",
        "print(c)"
      ],
      "metadata": {
        "id": "ZEKL2BsXs3pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may have also spotted that we went from specifying the tensor’s\n",
        "shape as a series of integer arguments, to grouping those arguments in a\n",
        "tuple. This is not strictly necessary - PyTorch will take a series of\n",
        "initial, unlabeled integer arguments as a tensor shape - but when adding\n",
        "the optional arguments, it can make your intent more readable.\n",
        "\n",
        "The other way to set the datatype is with the ``.to()`` method. In the\n",
        "cell above, we create a random floating point tensor ``b`` in the usual\n",
        "way. Following that, we create ``c`` by converting ``b`` to a 32-bit\n",
        "integer with the ``.to()`` method. Note that ``c`` contains all the same\n",
        "values as ``b``, but truncated to integers.\n",
        "\n",
        "Available data types include:\n",
        "\n",
        "-  ``torch.bool``\n",
        "-  ``torch.int8``\n",
        "-  ``torch.uint8``\n",
        "-  ``torch.int16``\n",
        "-  ``torch.int32``\n",
        "-  ``torch.int64``\n",
        "-  ``torch.half``\n",
        "-  ``torch.float``\n",
        "-  ``torch.double``\n",
        "-  ``torch.bfloat``\n",
        "\n",
        "## [Math & Logic](https://pytorch.org/docs/stable/torch.html#math-operations) with PyTorch Tensors\n",
        "\n",
        "Let’s look at basic arithmetic first, and how tensors interact with\n",
        "simple scalars:"
      ],
      "metadata": {
        "id": "d_gWFWV1v0Cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# common functions\n",
        "a = torch.rand(2, 4) * 2 - 1\n",
        "print('Common functions:')\n",
        "print(torch.abs(a))\n",
        "print(torch.ceil(a))\n",
        "print(torch.floor(a))\n",
        "print(torch.clamp(a, -0.5, 0.5))\n",
        "\n",
        "# introduce pi to PyTorch\n",
        "torch.pi = torch.acos(torch.zeros(1)).item() * 2\n",
        "# trigonometric functions and their inverses\n",
        "angles = torch.tensor([0, torch.pi / 4, torch.pi / 2, 3 * torch.pi / 4])\n",
        "sines = torch.sin(angles)\n",
        "inverses = torch.asin(sines)\n",
        "print('\\nSine and arcsine:')\n",
        "print(angles)\n",
        "print(sines)\n",
        "print(inverses)\n",
        "\n",
        "# bitwise operations\n",
        "print('\\nBitwise XOR:')\n",
        "b = torch.tensor([1, 5, 11])\n",
        "c = torch.tensor([2, 7, 10])\n",
        "print(torch.bitwise_xor(b, c))\n",
        "\n",
        "# comparisons:\n",
        "print('\\nBroadcasted, element-wise equality comparison:')\n",
        "d = torch.tensor([[1., 2.], [3., 4.]])\n",
        "e = torch.ones(1, 2)  # many comparison ops support broadcasting!\n",
        "print(torch.eq(d, e)) # returns a tensor of type bool\n",
        "\n",
        "# reductions:\n",
        "print('\\nReduction ops:')\n",
        "print(torch.max(d))        # returns a single-element tensor\n",
        "print(torch.max(d).item()) # extracts the value from the returned tensor\n",
        "print(torch.mean(d))       # average\n",
        "print(torch.std(d))        # standard deviation\n",
        "print(torch.prod(d))       # product of all numbers\n",
        "print(torch.unique(torch.tensor([1, 2, 1, 2, 1, 2]))) # filter unique elements\n",
        "\n",
        "# vector and linear algebra operations\n",
        "v1 = torch.tensor([1., 0., 0.])         # x unit vector\n",
        "v2 = torch.tensor([0., 1., 0.])         # y unit vector\n",
        "m1 = torch.rand(2, 2)                   # random matrix\n",
        "m2 = torch.tensor([[3., 0.], [0., 3.]]) # three times identity matrix\n",
        "\n",
        "print('\\nVectors & Matrices:')\n",
        "print(torch.cross(v2, v1)) # negative of z unit vector (v1 x v2 == -v2 x v1)\n",
        "print(m1)\n",
        "m3 = torch.matmul(m1, m2)\n",
        "print(m3)                  # 3 times m1\n",
        "print(torch.svd(m3))       # singular value decomposition"
      ],
      "metadata": {
        "id": "nJwbqaMRs3mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most binary operations on tensors will return a third, new tensor. When\n",
        "we say ``c = a * b`` (where ``a`` and ``b`` are tensors), the new tensor\n",
        "``c`` will occupy a region of memory distinct from the other tensors.\n",
        "\n",
        "There are times, though, that you may wish to alter a tensor in place -\n",
        "for example, if you’re doing an element-wise computation where you can\n",
        "discard intermediate values. For this, most of the math functions have a\n",
        "version with an appended underscore (``_``) that will alter a tensor in\n",
        "place.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ii_JIjC70bBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([0, torch.pi / 4, torch.pi / 2, 3 * torch.pi / 4])\n",
        "print('a:')\n",
        "print(a)\n",
        "print(torch.sin(a))   # this operation creates a new tensor in memory\n",
        "print(a)              # a has not changed\n",
        "\n",
        "b = torch.tensor([0, torch.pi / 4, torch.pi / 2, 3 * torch.pi / 4])\n",
        "print('\\nb:')\n",
        "print(b)\n",
        "print(torch.sin_(b))  # note the underscore\n",
        "print(b)              # b has changed\n",
        "\n",
        "a = torch.ones(2, 2)\n",
        "b = torch.rand(2, 2)\n",
        "\n",
        "print('\\nBefore:')\n",
        "print(a)\n",
        "print(b)\n",
        "print('\\nAfter adding:')\n",
        "print(a.add_(b))\n",
        "print(a)\n",
        "print(b)\n",
        "print('\\nAfter multiplying')\n",
        "print(b.mul_(b))\n",
        "print(b)"
      ],
      "metadata": {
        "id": "Fwq3UXefs3j8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Moving to GPU\n",
        "\n",
        "One of the major advantages of PyTorch is its robust acceleration on\n",
        "CUDA-compatible Nvidia GPUs. (“CUDA” stands for *Compute Unified Device\n",
        "Architecture*, which is Nvidia’s platform for parallel computing.) So\n",
        "far, everything we’ve done has been on CPU. \n",
        "\n",
        "First, we should check whether a GPU is available, with the\n",
        "``is_available()`` method.\n",
        "If you do not have a CUDA-compatible GPU and CUDA drivers installed, the executable cells in this section will not execute any GPU-related code."
      ],
      "metadata": {
        "id": "SvnZDOkL6OHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    print('We have a GPU!')\n",
        "else:\n",
        "    print('Sorry, CPU only.')"
      ],
      "metadata": {
        "id": "VM9DOSxns3hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we’ve determined that one or more GPUs is available, we need to put our data someplace where the GPU can see it. Your CPU does computation on data in your computer’s RAM. Your GPU has dedicated memory attached to it. Whenever you want to perform a computation on a device, you must move all the data needed for that computation to memory accessible by that device."
      ],
      "metadata": {
        "id": "YQcBIEgB6rwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, new tensors are created on the CPU, so we have to specify\n",
        "when we want to create our tensor on the GPU with the optional\n",
        "``device`` argument. You can see when we print the new tensor, PyTorch\n",
        "informs us which device it’s on (if it’s not on CPU).\n",
        "\n",
        "As a coding practice, specifying our devices everywhere with string\n",
        "constants is pretty fragile. In an ideal world, your code would perform\n",
        "robustly whether you’re on CPU or GPU hardware. You can do this by\n",
        "creating a device handle that can be passed to your tensors instead of a\n",
        "string:\n"
      ],
      "metadata": {
        "id": "GoK3PjkC7BNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    my_device = torch.device('cuda')\n",
        "else:\n",
        "    my_device = torch.device('cpu')\n",
        "print('Device: {}'.format(my_device))\n",
        "\n",
        "x = torch.rand(2, 2, device=my_device)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "hEUXDoBf6qia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to do computation involving two or\n",
        "more tensors, *all of the tensors must be on the same device*.\n",
        "\n",
        "If you have an existing tensor living on one device, you can move it to\n",
        "another with the ``to()`` method. The following line of code creates a\n",
        "tensor on CPU, and moves it to whichever device handle you acquired in\n",
        "the previous cell."
      ],
      "metadata": {
        "id": "MP3Gr4Rr8Tmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.rand(2, 2)\n",
        "y = y.to(my_device)"
      ],
      "metadata": {
        "id": "mW-LbnCVs3ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## The Fundamentals of Autograd\n",
        "\n",
        "PyTorch’s *Autograd* feature is part of what make PyTorch flexible and\n",
        "fast for building machine learning projects. It allows for the rapid and\n",
        "easy computation of multiple partial derivatives (also referred to as\n",
        "*gradients)* over a complex computation. This operation is central to\n",
        "backpropagation-based neural network learning.\n",
        "\n",
        "The power of autograd comes from the fact that it traces your\n",
        "computation dynamically *at runtime,* meaning that if your model has\n",
        "decision branches, or loops whose lengths are not known until runtime,\n",
        "the computation will still be traced correctly, and you’ll get correct\n",
        "gradients to drive learning. \n",
        "\n"
      ],
      "metadata": {
        "id": "4tGkiFCv-6-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In training a model, we want to minimize the loss. In the idealized case\n",
        "of a perfect model, that means adjusting its learning weights - that is,\n",
        "the adjustable parameters of the function - such that loss is zero for\n",
        "all inputs. In the real world, it means an iterative process of nudging\n",
        "the learning weights until we see that we get a tolerable loss for a\n",
        "wide variety of inputs.\n",
        "\n",
        "Let’s start with a straightforward example: we’ll create an input tensor full of evenly spaced values on the\n",
        "interval $[0, 2{\\pi}]$, and specify ``requires_grad=True``. (Like\n",
        "most functions that create tensors, ``torch.linspace()`` accepts an\n",
        "optional ``requires_grad`` option.) Setting this flag means that in\n",
        "every computation that follows, autograd will be accumulating the\n",
        "history of the computation in the output tensors of that computation.\n",
        "\n"
      ],
      "metadata": {
        "id": "GjqfhZ5N_6DX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker"
      ],
      "metadata": {
        "id": "QcbTM1L2AuTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.linspace(0., 2. * torch.pi, steps=25, requires_grad=True)\n",
        "print(a)"
      ],
      "metadata": {
        "id": "uxQJkiVIAl33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b = torch.sin(a)\n",
        "print(b)\n",
        "plt.plot(a.detach(), b.detach())"
      ],
      "metadata": {
        "id": "JjaWfzeAs3bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This ``grad_fn`` gives us a hint that when we execute the\n",
        "backpropagation step and compute gradients, we’ll need to compute the\n",
        "derivative of $sin(x)$ for all this tensor’s inputs.\n",
        "\n",
        "Let’s perform some more computations:"
      ],
      "metadata": {
        "id": "01sDbBNwA9uI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c = 2 * b\n",
        "print(c)\n",
        "\n",
        "d = c + 1\n",
        "print(d)\n",
        "\n",
        "out = d.sum()\n",
        "print(out)"
      ],
      "metadata": {
        "id": "GaOEMHyKs3Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('d:')\n",
        "print(d.grad_fn)\n",
        "print(d.grad_fn.next_functions)\n",
        "print(d.grad_fn.next_functions[0][0].next_functions)\n",
        "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
        "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)\n",
        "print('\\nc:')\n",
        "print(c.grad_fn)\n",
        "print('\\nb:')\n",
        "print(b.grad_fn)\n",
        "print('\\na:')\n",
        "print(a.grad_fn)"
      ],
      "metadata": {
        "id": "O2UsnPlDBalS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we call the `backward()` method on the output, and check the input’s grad property to inspect the gradients:"
      ],
      "metadata": {
        "id": "Dz8MiGAFBzxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "out.backward()\n",
        "print(a.grad)\n",
        "plt.plot(a.detach(), a.grad.detach())"
      ],
      "metadata": {
        "id": "kygd3nefBryT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding a constant, as we did to compute ``d``, does not change the\n",
        "derivative. That leaves $c = 2 * b = 2 * sin(a)$, the derivative\n",
        "of which should be $2 * cos(a)$. Looking at the graph above,\n",
        "that’s just what we see."
      ],
      "metadata": {
        "id": "uso0sp6NChk6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iXEKNXRsMPE"
      },
      "source": [
        "# PyTorch Models\n",
        "\n",
        "Let’s talk about how we can express models in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Sl_1C91sMPE"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn            # for torch.nn.Module, the parent object for PyTorch models\n",
        "import torch.nn.functional as F  # for the activation function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The most basic type of neural network layer is a linear or fully connected layer. This is a layer where every input influences every output of the layer to a degree specified by the layer’s weights. If a model has m inputs and n outputs, the weights will be an m x n matrix. For example:"
      ],
      "metadata": {
        "id": "toX5u-dUd_rK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lin = torch.nn.Linear(3, 2)\n",
        "x = torch.rand(5, 3)\n",
        "print('Input:')\n",
        "print(x)\n",
        "\n",
        "print('\\n\\nWeight and Bias parameters:')\n",
        "for param in lin.parameters():\n",
        "    print(param)\n",
        "\n",
        "y = lin(x)\n",
        "print('\\n\\nOutput:')\n",
        "print(y)"
      ],
      "metadata": {
        "id": "_UyYpJN1aVfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non-linear activations are what create the complex mappings between the model’s inputs and outputs. They are applied after linear transformations to introduce nonlinearity, helping neural networks learn a wide variety of phenomena.\n",
        "\n",
        "Here is an example of `nn.ReLU` , but there are other activations to introduce non-linearity in your model."
      ],
      "metadata": {
        "id": "NdaB2eTgazAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Before ReLU: {y}\\n\\n\")\n",
        "y = nn.ReLU()(y)\n",
        "print(f\"After ReLU: {y}\")"
      ],
      "metadata": {
        "id": "kEDj5HfmbAxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`nn.Sequential` is an ordered container of modules. The data is passed through all the modules in the same order as defined. You can use sequential containers to put together a quick network."
      ],
      "metadata": {
        "id": "5n3vOpKQbYmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_modules = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(in_features=28*28, out_features=20),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 10)\n",
        ")\n",
        "input = torch.rand(3,28,28)\n",
        "logits = seq_modules(input)"
      ],
      "metadata": {
        "id": "bQxi7DevbYFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last linear layer of `seq_modules` returns logits - raw values in [-$\\infty$, $\\infty$]. The logits can be scaled to values [0, 1] representing the model’s predicted probabilities for each class using `nn.Softmax`. "
      ],
      "metadata": {
        "id": "w3NI5KOUbwJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)\n",
        "logits, pred_probab"
      ],
      "metadata": {
        "id": "iV-lv_RpcxuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qd-sk1-sMPE"
      },
      "source": [
        "![LeNet](https://pytorch.org/tutorials/_images/mnist.png)\n",
        "\n",
        "Above is a diagram of LeNet-5, one of the earliest convolutional neural\n",
        "nets, and one of the drivers of the explosion in Deep Learning. It was\n",
        "built to read small images of handwritten numbers (the MNIST dataset),\n",
        "and correctly classify which digit was represented in the image.\n",
        "\n",
        "Here’s the abridged version of how it works:\n",
        "\n",
        "-  Layer C1 is a convolutional layer, meaning that it scans the input\n",
        "   image for features it learned during training. It outputs a map of\n",
        "   where it saw each of its learned features in the image. This\n",
        "   “activation map” is downsampled in layer S2.\n",
        "-  Layer C3 is another convolutional layer, this time scanning C1’s\n",
        "   activation map for *combinations* of features. It also puts out an\n",
        "   activation map describing the spatial locations of these feature\n",
        "   combinations, which is downsampled in layer S4.\n",
        "-  Finally, the fully-connected layers at the end, F5, F6, and OUTPUT,\n",
        "   are a *classifier* that takes the final activation map, and\n",
        "   classifies it into one of ten bins representing the 10 digits.\n",
        "\n",
        "How do we express this simple neural network in code?\n",
        "There is the model we’ll train below. It’s a variant of LeNet adapted for 3-color images.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtS-1QUysMPE"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvTZ45pNsMPF"
      },
      "source": [
        "Looking over this code, you should be able to spot some structural\n",
        "similarities with the diagram above.\n",
        "\n",
        "This demonstrates the structure of a typical PyTorch model: \n",
        "\n",
        "-  It inherits from ``torch.nn.Module`` - modules may be nested - in fact,\n",
        "   even the ``Conv2d`` and ``Linear`` layer classes inherit from\n",
        "   ``torch.nn.Module``.\n",
        "-  A model will have an ``__init__()`` function, where it instantiates\n",
        "   its layers, and loads any data artifacts it might\n",
        "   need (e.g., an NLP model might load a vocabulary).\n",
        "-  A model will have a ``forward()`` function. This is where the actual\n",
        "   computation happens: An input is passed through the network layers\n",
        "   and various functions to generate an output.\n",
        "-  Other than that, you can build out your model class like any other\n",
        "   Python class, adding whatever properties and methods you need to\n",
        "   support your model’s computation.\n",
        "\n",
        "Let’s instantiate this object and run a sample input through it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMeyNqNqsMPF"
      },
      "outputs": [],
      "source": [
        "net = Net()\n",
        "print(net)                         # what does the object tell us about itself?\n",
        "\n",
        "input = torch.rand(1, 3, 32, 32)   # stand-in for a 32x32 black & white image\n",
        "print('\\nImage batch shape:')\n",
        "print(input.shape)\n",
        "\n",
        "output = net(input)                # we don't call forward() directly\n",
        "print('\\nRaw output:')\n",
        "print(output)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A loss function takes the (output, target) pair of inputs, and computes a\n",
        "value that estimates how far away the output is from the target.\n",
        "\n",
        "There are several different\n",
        "[loss functions](https://pytorch.org/docs/nn.html#loss-functions) under the\n",
        "nn package .\n",
        "A simple loss is: ``nn.MSELoss`` which computes the mean-squared error\n",
        "between the output and the target.\n",
        "\n",
        "For example:"
      ],
      "metadata": {
        "id": "aJc3wZCNI69M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target = torch.randn(10)  # a dummy target, for example\n",
        "target = target.view(1, -1)  # make it the same shape as output\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "loss = criterion(output, target)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "PMxRYZY-G_1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To backpropagate the error all we have to do is to ``loss.backward()``.\n",
        "You need to clear the existing gradients though, else gradients will be\n",
        "accumulated to existing gradients.\n",
        "\n",
        "\n",
        "Now we shall call ``loss.backward()``, and have a look at conv1's bias\n",
        "gradients before and after the backward."
      ],
      "metadata": {
        "id": "BvllKco0JFen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
        "\n",
        "print('conv1.bias.grad before backward')\n",
        "print(net.conv1.bias.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('conv1.bias.grad after backward')\n",
        "print(net.conv1.bias.grad)"
      ],
      "metadata": {
        "id": "GFk-3n8XH8M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QByF9yzsMPF"
      },
      "source": [
        "Below, we’re going to use one of the ready-to-download,\n",
        "open-access datasets from TorchVision. The first thing we need to do is transform our incoming images into a\n",
        "PyTorch tensor.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYRDQ7FKsMPF"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_pMgIvksMPF"
      },
      "source": [
        "Here, we specify two transformations for our input:\n",
        "\n",
        "-  ``transforms.ToTensor()`` converts images loaded by Pillow into \n",
        "   PyTorch tensors.\n",
        "-  ``transforms.Normalize()`` adjusts the values of the tensor so\n",
        "   that their average is zero and their standard deviation is 0.5. Most\n",
        "   activation functions have their strongest gradients around x = 0, so\n",
        "   centering our data there can speed learning.\n",
        "\n",
        "There are many more transforms available, including cropping, centering,\n",
        "rotation, and reflection.\n",
        "\n",
        "Next, we’ll create an instance of the CIFAR10 dataset. This is a set of\n",
        "32x32 color image tiles representing 10 classes of objects: 6 of animals\n",
        "(bird, cat, deer, dog, frog, horse) and 4 of vehicles (airplane,\n",
        "automobile, ship, truck):\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HS-eICjsMPG"
      },
      "outputs": [],
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n14FdiqvsMPG"
      },
      "source": [
        " ``Dataset`` classes in PyTorch include the\n",
        "downloadable datasets in TorchVision, Torchtext, and TorchAudio, as well\n",
        "as utility dataset classes such as ``torchvision.datasets.ImageFolder``,\n",
        "which will read a folder of labeled images. You can also create your own\n",
        "subclasses of ``Dataset``.\n",
        "\n",
        "When we instantiate our dataset, we need to tell it a few things:\n",
        "\n",
        "-  The filesystem path to where we want the data to go. \n",
        "-  Whether or not we are using this set for training; most datasets\n",
        "   will be split into training and test subsets.\n",
        "-  Whether we would like to download the dataset if we haven’t already.\n",
        "-  The transformations we want to apply to the data.\n",
        "\n",
        "Once your dataset is ready, you can give it to the ``DataLoader``:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmTc-i3asMPG"
      },
      "outputs": [],
      "source": [
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0dbPa1VsMPG"
      },
      "source": [
        "A ``Dataset`` subclass wraps access to the data, and is specialized to\n",
        "the type of data it’s serving. The ``DataLoader`` knows *nothing* about\n",
        "the data, but organizes the input tensors served by the ``Dataset`` into\n",
        "batches with the parameters you specify.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pR4_ILwFsMPG"
      },
      "outputs": [],
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6UgQsbBsMPG"
      },
      "source": [
        "Let’s put all the pieces together, and train a model:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4rl2-s-sMPG"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VChQr_hBsMPH"
      },
      "source": [
        "The last ingredients we need are a loss function and an optimizer:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buzTjvvzsMPH"
      },
      "outputs": [],
      "source": [
        "net = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anT1VuJ_sMPH"
      },
      "source": [
        "Here we have created an\n",
        "optimizer that implements *stochastic gradient descent*. Besides parameters of the\n",
        "algorithm, like the learning rate (``lr``) and momentum, we also pass in\n",
        "``net.parameters()``, which is a collection of all the learning weights\n",
        "in the model - which is what the optimizer adjusts.\n",
        "\n",
        "Finally, all of this is assembled into the training loop:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-r2EbzdqsMPI"
      },
      "outputs": [],
      "source": [
        "net.to(my_device)\n",
        "\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs.to(my_device))\n",
        "        loss = criterion(outputs, labels.to(my_device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5fR-38JsMPI"
      },
      "source": [
        "As a final step, we should check that the model is actually doing\n",
        "*general* learning, and not simply “memorizing” the dataset. Download CIFAR10 `testset`, dedine the `testloader` and calculate the accuracy.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S007atqEsMPI"
      },
      "outputs": [],
      "source": [
        "testset = ...\n",
        "testloader = ...\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "      #  <YOUR CODE>\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYw2-dJIsMPI"
      },
      "source": [
        "You should see that the model is roughly 50%\n",
        "accurate at this point. That’s not exactly state-of-the-art, but it’s\n",
        "far better than the 10% accuracy we’d expect from a random output. This\n",
        "demonstrates that some general learning did happen in the model. We are going to improve the performance through next practical sessions.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}