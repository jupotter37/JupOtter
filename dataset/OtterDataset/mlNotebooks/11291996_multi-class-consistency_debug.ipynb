{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaewan/jaewan/anaconda3/envs/mcc/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange, repeat\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from diffusers import DDIMScheduler\n",
    "\n",
    "from diffuser_utils import MasaCtrlPipeline\n",
    "from mcc_utils import AttentionBase\n",
    "from mcc_utils import register_attention_editor_diffusers\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.io import read_image\n",
    "from pytorch_lightning import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'cross_attention_kwargs': {'scale': 0.5}} are not expected by MasaCtrlPipeline and will be ignored.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "/home/jaewan/jaewan/anaconda3/envs/mcc/lib/python3.11/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:115: FutureWarning: The configuration file of this scheduler: DDIMScheduler {\n",
      "  \"_class_name\": \"DDIMScheduler\",\n",
      "  \"_diffusers_version\": \"0.16.1\",\n",
      "  \"beta_end\": 0.012,\n",
      "  \"beta_schedule\": \"scaled_linear\",\n",
      "  \"beta_start\": 0.00085,\n",
      "  \"clip_sample\": false,\n",
      "  \"clip_sample_range\": 1.0,\n",
      "  \"dynamic_thresholding_ratio\": 0.995,\n",
      "  \"num_train_timesteps\": 1000,\n",
      "  \"prediction_type\": \"epsilon\",\n",
      "  \"sample_max_value\": 1.0,\n",
      "  \"set_alpha_to_one\": false,\n",
      "  \"steps_offset\": 0,\n",
      "  \"thresholding\": false,\n",
      "  \"trained_betas\": null\n",
      "}\n",
      " is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\n",
      "  deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    }
   ],
   "source": [
    "# Note that you may add your Hugging Face token to get access to the models\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = \"CompVis/stable-diffusion-v1-4\" #trained from \"laion-aesthetics v2 5+\" and 10% dropping of the text-conditioning\n",
    "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
    "model = MasaCtrlPipeline.from_pretrained(model_path, scheduler=scheduler, cross_attention_kwargs={\"scale\": 0.5}).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unet(net):\n",
    "    for name, net in net.named_children():\n",
    "        print(net)\n",
    "check_unet(model.unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "seed_everything(seed)\n",
    "\n",
    "prompts = [\n",
    "    \"1 boy\",  # source prompt\n",
    "]\n",
    "\n",
    "# initialize the noise map\n",
    "start_code = torch.randn([1, 4, 64, 64], device=device) #downsampling factor is 8, 512 -> 64\n",
    "start_code = start_code.expand(len(prompts), -1, -1, -1)\n",
    "\n",
    "# inference the synthesized image without MasaCtrl\n",
    "editor = AttentionBase()\n",
    "register_attention_editor_diffusers(model, editor)\n",
    "image_ori = model(prompts, latents=start_code, guidance_scale=7.5, device = device) #__call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "/data1/jaewan/developing/projects/machine learning/deep learning/diffusion/multiclass_consistency/mcc/diffuser_utils.py:87: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n",
      "  latents_shape = (batch_size, self.unet.in_channels, height//8, width//8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative text embeddings added : torch.Size([2, 77, 768])\n",
      "latents shape:  torch.Size([1, 4, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:07<00:00,  7.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  torch.Size([1, 3, 512, 512])\n",
      "MasaCtrl at denoising steps:  [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "MasaCtrl at U-Net layers:  [10, 11, 12, 13, 14, 15]\n",
      "negative text embeddings added : torch.Size([2, 77, 768])\n",
      "latents shape:  torch.Size([1, 4, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:07<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  torch.Size([1, 3, 512, 512])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">43</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">40 </span>save_image(out_image, os.path.join(out_dir, <span style=\"color: #808000; text-decoration-color: #808000\">f\"all_step{</span>STEP<span style=\"color: #808000; text-decoration-color: #808000\">}_layer{</span>LAYER<span style=\"color: #808000; text-decoration-color: #808000\">}.png\"</span>))            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">41 </span>save_image(out_image[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>], os.path.join(out_dir, <span style=\"color: #808000; text-decoration-color: #808000\">f\"source_step{</span>STEP<span style=\"color: #808000; text-decoration-color: #808000\">}_layer{</span>LAYER<span style=\"color: #808000; text-decoration-color: #808000\">}.png\"</span>))      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">42 </span>save_image(out_image[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>], os.path.join(out_dir, <span style=\"color: #808000; text-decoration-color: #808000\">f\"without_step{</span>STEP<span style=\"color: #808000; text-decoration-color: #808000\">}_layer{</span>LAYER<span style=\"color: #808000; text-decoration-color: #808000\">}.png\"</span>))     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>43 save_image(out_image[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>], os.path.join(out_dir, <span style=\"color: #808000; text-decoration-color: #808000\">f\"masactrl_step{</span>STEP<span style=\"color: #808000; text-decoration-color: #808000\">}_layer{</span>LAYER<span style=\"color: #808000; text-decoration-color: #808000\">}.png\"</span>))    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">44 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">45 </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Syntheiszed images are saved in\"</span>, out_dir)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">46 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">IndexError: </span>index <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> is out of bounds for dimension <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> with size <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m43\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m40 \u001b[0msave_image(out_image, os.path.join(out_dir, \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mall_step\u001b[0m\u001b[33m{\u001b[0mSTEP\u001b[33m}\u001b[0m\u001b[33m_layer\u001b[0m\u001b[33m{\u001b[0mLAYER\u001b[33m}\u001b[0m\u001b[33m.png\u001b[0m\u001b[33m\"\u001b[0m))            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m41 \u001b[0msave_image(out_image[\u001b[94m0\u001b[0m], os.path.join(out_dir, \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33msource_step\u001b[0m\u001b[33m{\u001b[0mSTEP\u001b[33m}\u001b[0m\u001b[33m_layer\u001b[0m\u001b[33m{\u001b[0mLAYER\u001b[33m}\u001b[0m\u001b[33m.png\u001b[0m\u001b[33m\"\u001b[0m))      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m42 \u001b[0msave_image(out_image[\u001b[94m1\u001b[0m], os.path.join(out_dir, \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mwithout_step\u001b[0m\u001b[33m{\u001b[0mSTEP\u001b[33m}\u001b[0m\u001b[33m_layer\u001b[0m\u001b[33m{\u001b[0mLAYER\u001b[33m}\u001b[0m\u001b[33m.png\u001b[0m\u001b[33m\"\u001b[0m))     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m43 save_image(out_image[\u001b[94m2\u001b[0m], os.path.join(out_dir, \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mmasactrl_step\u001b[0m\u001b[33m{\u001b[0mSTEP\u001b[33m}\u001b[0m\u001b[33m_layer\u001b[0m\u001b[33m{\u001b[0mLAYER\u001b[33m}\u001b[0m\u001b[33m.png\u001b[0m\u001b[33m\"\u001b[0m))    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m44 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m45 \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mSyntheiszed images are saved in\u001b[0m\u001b[33m\"\u001b[0m, out_dir)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m46 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mIndexError: \u001b[0mindex \u001b[1;36m2\u001b[0m is out of bounds for dimension \u001b[1;36m0\u001b[0m with size \u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#txt to img\n",
    "from mcc import MutualSelfAttentionControl\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)\n",
    "\n",
    "out_dir = \"./result\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "sample_count = len(os.listdir(out_dir))\n",
    "out_dir = os.path.join(out_dir, f\"sample_{sample_count}\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "prompts = [\n",
    "    #\"1 boy and 1 car\",  # source prompt\n",
    "    \"1 running boy and 1 running horse\"  # target prompt\n",
    "]\n",
    "\n",
    "# initialize the noise map\n",
    "start_code = torch.randn([1, 4, 64, 64], device=device) #downsampling factor is 8, 512 -> 64\n",
    "start_code = start_code.expand(len(prompts), -1, -1, -1)\n",
    "\n",
    "# inference the synthesized image without MasaCtrl\n",
    "editor = AttentionBase()\n",
    "register_attention_editor_diffusers(model, editor)\n",
    "image_ori = model(prompts, latents=start_code, guidance_scale=7.5, device = device) #__call__\n",
    "\n",
    "# inference the synthesized image with MasaCtrl\n",
    "STEP = 4\n",
    "LAYER = 10\n",
    "\n",
    "# hijack the attention module\n",
    "editor = MutualSelfAttentionControl(STEP, LAYER)\n",
    "register_attention_editor_diffusers(model, editor)\n",
    "\n",
    "# inference the synthesized image\n",
    "image_masactrl = model(prompts, latents=start_code, guidance_scale=7.5, device = device)[-1:]\n",
    "\n",
    "# save the synthesized image\n",
    "out_image = torch.cat([image_ori, image_masactrl], dim=0)\n",
    "save_image(out_image, os.path.join(out_dir, f\"all_step{STEP}_layer{LAYER}.png\"))\n",
    "save_image(out_image[0], os.path.join(out_dir, f\"source_step{STEP}_layer{LAYER}.png\"))\n",
    "save_image(out_image[1], os.path.join(out_dir, f\"without_step{STEP}_layer{LAYER}.png\"))\n",
    "save_image(out_image[2], os.path.join(out_dir, f\"masactrl_step{STEP}_layer{LAYER}.png\"))\n",
    "\n",
    "print(\"Syntheiszed images are saved in\", out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img to img\n",
    "\n",
    "from mcc import MutualSelfAttentionControl\n",
    "from torchvision.io import read_image\n",
    "\n",
    "def load_image(image_path, device):\n",
    "    image = read_image(image_path)\n",
    "    image = image[:3].unsqueeze_(0).float() / 127.5 - 1.  # [-1, 1]\n",
    "    image = F.interpolate(image, (512, 512))\n",
    "    image = image.to(device)\n",
    "    return image\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)\n",
    "\n",
    "out_dir = \"./result\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "sample_count = len(os.listdir(out_dir))\n",
    "out_dir = os.path.join(out_dir, f\"sample_{sample_count}\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# source image\n",
    "SOURCE_IMAGE_PATH = \"./img/corgi.jpg\"\n",
    "source_image = load_image(SOURCE_IMAGE_PATH, device)\n",
    "\n",
    "source_prompt = \"\"\n",
    "target_prompt = \"a photo of a running corgi\"\n",
    "prompts = [source_prompt, target_prompt]\n",
    "\n",
    "# invert the source image\n",
    "start_code, latents_list = model.invert(source_image,\n",
    "                                        source_prompt,\n",
    "                                        guidance_scale=7.5,\n",
    "                                        num_inference_steps=50,\n",
    "                                        return_intermediates=True, device = device)\n",
    "start_code = start_code.expand(len(prompts), -1, -1, -1)\n",
    "\n",
    "# results of direct synthesis\n",
    "editor = AttentionBase()\n",
    "register_attention_editor_diffusers(model, editor)\n",
    "image_fixed = model([target_prompt],\n",
    "                    latents=start_code[-1:],\n",
    "                    num_inference_steps=50,\n",
    "                    guidance_scale=7.5, device = device)\n",
    "\n",
    "# inference the synthesized image with MasaCtrl\n",
    "STEP = 4\n",
    "LAYER = 10\n",
    "\n",
    "# hijack the attention module\n",
    "editor = MutualSelfAttentionControl(STEP, LAYER)\n",
    "register_attention_editor_diffusers(model, editor)\n",
    "\n",
    "# inference the synthesized image\n",
    "image_masactrl = model(prompts,\n",
    "                       latents=start_code,\n",
    "                       guidance_scale=7.5, device = device)\n",
    "# Note: querying the inversion intermediate features latents_list\n",
    "# may obtain better reconstruction and editing results\n",
    "# image_masactrl = model(prompts,\n",
    "#                        latents=start_code,\n",
    "#                        guidance_scale=7.5,\n",
    "#                        ref_intermediate_latents=latents_list)\n",
    "\n",
    "# save the synthesized image\n",
    "out_image = torch.cat([source_image * 0.5 + 0.5,\n",
    "                       image_masactrl[0:1],\n",
    "                       image_fixed,\n",
    "                       image_masactrl[-1:]], dim=0)\n",
    "save_image(out_image, os.path.join(out_dir, f\"all_step{STEP}_layer{LAYER}.png\"))\n",
    "save_image(out_image[0], os.path.join(out_dir, f\"source_step{STEP}_layer{LAYER}.png\"))\n",
    "save_image(out_image[1], os.path.join(out_dir, f\"reconstructed_source_step{STEP}_layer{LAYER}.png\"))\n",
    "save_image(out_image[2], os.path.join(out_dir, f\"without_step{STEP}_layer{LAYER}.png\"))\n",
    "save_image(out_image[3], os.path.join(out_dir, f\"masactrl_step{STEP}_layer{LAYER}.png\"))\n",
    "\n",
    "print(\"Syntheiszed images are saved in\", out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
