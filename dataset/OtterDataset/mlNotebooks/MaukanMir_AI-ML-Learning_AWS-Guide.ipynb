{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AWS Step Functions is a serverless orchestration service that lets you coordinate multiple AWS services into serverless workflows. You can design and execute complex business logic as a series of steps, managing both sequential and parallel processes to automate tasks and workflows across different AWS applications and services. This service helps ensure that each step in your application executes in order, as intended, and handles error management and retry logic automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do you ensure the security and compliance of data when using AWS?\n",
    "\n",
    "- Data Encryption: Use AWS KMS to encrypt data at rest and in transit.\n",
    "- IAM Policies: Implement the principle of least privilege by using fine-grained IAM policies.\n",
    "- VPC Security: Utilize VPCs with security groups, NACLs, and private subnets to control network access.\n",
    "- Logging and Monitoring: Enable CloudTrail, AWS Config, and CloudWatch to track and audit activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would you set up an ETL pipeline using AWS Glue?\n",
    "\n",
    "#### Create a Crawler:\n",
    "- Define a data source (e.g., S3, RDS) and set up a Glue Crawler to automatically detect and catalog the schema of the source data.\n",
    "\n",
    "#### Set Up a Glue Job\n",
    "- Create a Glue job to transform the data. Use the Glue Studio visual interface or write custom PySpark scripts to handle the ETL logic.\n",
    "\n",
    "#### Define the Data Transformation:\n",
    "- Use the Glue ETL job to clean, enrich, or aggregate the data as needed. You can use built-in transformations or custom scripts.\n",
    "\n",
    "#### Destination Data\n",
    "- Specify the target data store (e.g., S3, Redshift) for the transformed data. The Glue job will write the processed data to this location.\n",
    "\n",
    "#### Schedule and Orchestrate:\n",
    "- Schedule the Glue job using triggers or AWS Step Functions to automate the ETL process. You can set the job to run on a schedule or in response to specific events.\n",
    "\n",
    "#### Monitor and Debug:\n",
    "- Use AWS CloudWatch Logs and Glue job metrics to monitor the ETL process and troubleshoot any issues that arise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "# Initialize Glue Context\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Step 1: Load data from S3\n",
    "datasource0 = glueContext.create_dynamic_frame.from_options(\n",
    "    connection_type=\"s3\",\n",
    "    connection_options={\"paths\": [\"s3://your-source-bucket/path/\"]},\n",
    "    format=\"json\"  # Example format; adjust according to your data format\n",
    ")\n",
    "\n",
    "# Step 2: Data transformation (e.g., filtering data)\n",
    "transformed_df = Filter.apply(frame=datasource0, f=lambda x: x[\"status\"] == \"active\")\n",
    "\n",
    "# Step 3: Write transformed data back to S3\n",
    "datasink = glueContext.write_dynamic_frame.from_options(\n",
    "    frame=transformed_df,\n",
    "    connection_type=\"s3\",\n",
    "    connection_options={\"path\": \"s3://your-target-bucket/path/\"},\n",
    "    format=\"parquet\"\n",
    ")\n",
    "\n",
    "# Commit the job\n",
    "job.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What strategies have you implemented to optimize cost while using AWS  services?\n",
    "\n",
    "- Utilizing Pipemode and streaming data from S3 instead of using an instance\n",
    "- Changing the dataformat to RecordIOprotobuff\n",
    "- Auto Scaling: Use Auto Scaling groups to automatically scale resources up or down based on demand.\n",
    "- Spot Instances: Utilize EC2 Spot Instances for non-critical, flexible workloads to benefit from lower pricing.\n",
    "- Elastic Inference: Use Elastic Inference to attach just the right amount of inference acceleration to your EC2 or SageMaker instances, reducing the need for more powerful and expensive GPUs.\n",
    "- Distributed Training: Optimize distributed training across multiple, lower-cost GPUs instead of using a single, high-cost GPU instance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Describe your experience with deploying models using AWS Lambda? \n",
    "\n",
    "- Model Preparation: I started by training a machine learning model on a separate environment. Once trained, I serialized the model into a format suitable for deployment, such as a .pkl or .h5 file.\n",
    "- Creating a Lambda Layer: I packaged all the necessary dependencies (like numpy, scikit-learn, or tensorflow) along with the serialized model into a ZIP file. This ZIP file was then uploaded to AWS Lambda as a Layer, which allows the code in the Lambda function to use these libraries and model files without bundling them directly with the function's deployment package.\n",
    "- Writing the Lambda Function: The Lambda function was written in Python. It included code to deserialize the model and use it to make predictions. The function handled JSON input, where data to be predicted was passed, and JSON output, which contained the prediction results.\n",
    "- API Gateway Integration: I used AWS API Gateway to create a RESTful endpoint that triggered the Lambda function. This setup allowed external applications to send data to the Lambda function via HTTP requests and receive predictions in response.\n",
    "- Deployment and Testing: After deploying the Lambda function and configuring the API Gateway, I tested the setup by sending HTTP requests with test data. I monitored the execution and performance via AWS CloudWatch to ensure that the function was running efficiently and within the resource limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Managment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can you discuss a scenario where you had to move data securely between on-premise and AWS cloud?\n",
    "- We used AWS Direct Connect to establish a dedicated network connection from our on-premise network to AWS. This bypassed the public internet, reducing exposure to security threats and improving transfer speeds.\n",
    "- AWS Database Migration Service (AWS DMS) is a managed migration and replication service that helps you move your databases and analytics workloads to AWS quickly and securely. \n",
    "\n",
    "## How do you handle large datasets in AWS S3?\n",
    "- Multipart Upload: Use multipart upload for large files to enhance upload performance and reliability.\n",
    "- Lifecycle Management: Implement S3 lifecycle policies to automatically transition data to more cost-effective storage tiers (like S3 Glacier) and manage data retention.\n",
    "- S3 Select: Use S3 Select to retrieve specific data from objects, reducing data transfer costs and improving efficiency.\n",
    "- Prefixes and Indexing: Organize files using logical prefixes and index them effectively to optimize data retrieval.\n",
    "- Data Transfer Tools: Utilize AWS DataSync or Transfer Acceleration for faster data transfer between on-premises systems and S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Profiency\n",
    "## What are your favorite Python libraries for data analysis and why?\n",
    "- Pandas\n",
    "- Seaborn\n",
    "- Matplotlib\n",
    "- Numpy\n",
    "- Scikit-Learn\n",
    "- Scipy\n",
    "\n",
    "## How do you handle memory management in Python for large datasets?\n",
    "- Use Efficient Data Structures: Opt for data structures that are memory efficient. For example, using Pandas’ categorical data type for categorical data can significantly reduce memory usage compared to object types.\n",
    "- Chunk Processing: Process large datasets in chunks rather than loading the entire dataset into memory at once. Libraries like Pandas support chunking directly via parameters like chunksize in read_csv().\n",
    "- Use Dask or Vaex: These libraries are designed for parallel computing and can handle large datasets that don’t fit into memory by breaking the data into chunks and processing these chunks in parallel.\n",
    "- Memory Profiling: Tools like memory_profiler can be used to monitor the memory usage of your Python script at runtime. Identifying memory hotspots can help you optimize them.\n",
    "- Garbage Collection: Force the garbage collector to release unreferenced memory more aggressively with gc.collect() especially after deleting large data structures.\n",
    "- Optimize Data Types: When using Pandas, optimize data types manually by converting columns to more appropriate types (like changing float64 to float32 or integers to uint8 if possible).\n",
    "\n",
    "## Describe a situation where you used Python to automate a repetitive task.\n",
    "- Created a chron job on a kubernetes pod that automtically updated workflows through a python script\n",
    "\n",
    "## How do you approach debugging and error handling in Python?\n",
    "- Try-Except Blocks: These are used to catch and handle exceptions that may cause the program to crash. I ensure to catch specific exceptions to avoid masking other bugs.\n",
    "- Custom Exceptions: For larger applications, I define custom exception classes to handle specific error conditions more gracefully.\n",
    "- Unit Testing: I write unit tests with frameworks like unittest or pytest to catch errors early and ensure each part of the program functions correctly as changes are made.\n",
    "- Print Statements: Simple yet effective for quick checks. I insert print statements to display variable values and flow of execution at different points in the code.\n",
    "- Logging: For more complex applications, I use Python’s logging module to log detailed debug information. This allows me to control the level of verbosity and is particularly useful for production environments where print statements are not suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience with LLM (Large Language Models) and NLP (Natural Language Processing)\n",
    "\n",
    "## Can you explain how LLMs differ from traditional machine learning models?\n",
    "- TM: Often designed for specific tasks, using algorithms like linear regression, decision trees, or SVMs. These models require feature engineering and explicit programming for specific outcomes.\n",
    "- LLMs: Based on neural networks, particularly transformers, which allow them to process and generate natural language. They learn directly from vast amounts of text data without needing explicit feature engineering.\n",
    "- TM: Typically handle structured data (e.g., tables). The quality and structure of the data are critical, and preprocessing steps are necessary to convert raw data into a suitable format.\n",
    "- LLMS Primarily handle unstructured data (e.g., text). They are adept at understanding and generating human language by training on diverse and large datasets of text.\n",
    "- TM: Often require less computational resources for training and can be trained on smaller datasets. They may need manual tuning of features and model parameters.\n",
    "- LLMS: Require substantial computational resources and large datasets for training. They automatically learn features from the data due to their deep learning nature.\n",
    "- TM: Typically excel at tasks they are specifically designed for but may struggle to generalize across unrelated tasks without retraining or significant adjustments.\n",
    "- LLMS: Designed to generalize across a wide variety of tasks without task-specific modifications. They can perform tasks like translation, summarization, question answering, and more, often with no or minimal task-specific training.\n",
    "- LLMS Extremely versatile and can handle multiple types of tasks with the same underlying model. This adaptability is a significant advancement over traditional models.\n",
    "\n",
    "## Discuss your experience with tokenization and how it affects model performance.\n",
    "#### Tokenization is a crucial preprocessing step in natural language processing (NLP) that involves breaking down text into smaller units, such as words, subwords, or characters. My experience with tokenization has shown its significant impact on model performance in various ways:\n",
    "- Reduced Vocabulary Size: Subword tokenization helps manage vocabulary size, reducing model complexity and improving memory efficiency.\n",
    "- Enhanced Context Understanding: Finer tokenization, like subwords, captures linguistic nuances better, aiding in more accurate text interpretation and generation.\n",
    "- Training Speed: Efficient tokenization streamlines training, speeds up model convergence, and reduces resource consumption.\n",
    "- Improved Generalization: Appropriate tokenization enhances a model's ability to perform well across different tasks and unseen data by capturing essential language units.\n",
    "\n",
    "## How do you approach sentiment analysis in text data?\n",
    "- How do you approach sentiment analysis in text data?\n",
    "- Feature Extraction: Use techniques like Bag of Words, TF-IDF, or Word Embeddings to transform text into numerical data.\n",
    "- Model Selection: Choose a model, ranging from simple (like Naive Bayes) to complex (like LSTM or BERT for deep insights).\n",
    "- Training: Train the model on labeled data to learn to predict sentiment.\n",
    "- Evaluation: Use metrics like accuracy and F1-score to assess performance.\n",
    "- Deployment: Implement the model in the target application for real-time analysis.\n",
    "\n",
    "## Describe a project where you implemented named entity recognition (NER).\n",
    "\n",
    "#### NER: Focuses on identifying and classifying key elements from the text into predefined categories such as person names, organizations, locations, etc. It's primarily used for extracting structured information from unstructured text.\n",
    "- Training: Although spaCy provides pre-trained models, I fine-tuned the model on a labeled subset of my dataset to improve its accuracy specifically for the types of articles I was analyzing.\n",
    "- Entity Extraction: I ran the trained NER model across the dataset, extracting entities and classifying them into predefined categories.\n",
    "- Post-processing: The extracted entities were used to tag the articles, and this metadata was then used to enhance search functionality within a content management system.\n",
    "- Evaluation: I evaluated the model’s performance using precision, recall, and F1-score, and made adjustments to the training data and model parameters based on the results.\n",
    "\n",
    "## How would you evaluate the performance of an NLP model?\n",
    "- BLEU (Bilingual Evaluation Understudy) score: Measures the quality of machine-generated text against reference translations, commonly used in machine translation.\n",
    "- ROUGE (Recall-Oriented Understudy for Gisting Evaluation): A set of metrics that evaluate the overlap between generated text and reference text, often used in summarization tasks. They help quantify the model's effectiveness and guide further improvements.\n",
    "- Perplexity is a metric used to evaluate how well a probabilistic model predicts a sample, such as a sequence of words.\n",
    "#### There are two main evaluation methods: intrinsic and extrinsic. Intrinsic evaluation focuses on the model's internal aspects, like accuracy and precision, while extrinsic evaluation focuses on external aspects, like user satisfaction. Intrinsic evaluation is useful for debugging and improving the model, but it doesn't indicate how well it performs in real-world settings.\n",
    "\n",
    "## Explain how you would handle bias in an NLP model trained on internal data.\n",
    "- Audit the Data: Start by thoroughly reviewing the data used for training the model. Look for imbalances or skewed representations of certain groups, themes, or sentiments.\n",
    "- Identify Potential Bias Sources: Determine potential sources of bias, such as historical biases in decision-making processes or subjective annotations in training data.\n",
    "- Rebalance the Dataset: Use techniques like oversampling underrepresented groups or undersampling overrepresented ones to create a more balanced training set.\n",
    "- Disaggregated Analysis: Evaluate model performance separately across different demographic groups or categories to check for disparities in error rates or effectiveness.\n",
    "- Continual Learning: Regularly update the model with new, more representative data as it becomes available. Continual learning approaches can help the model adapt to changes over time and mitigate emerging biases.\n",
    "- Feedback Loop: Implement a mechanism to collect feedback on model outputs from diverse user groups. Use this feedback to identify unrecognized biases and update the model accordingly.\n",
    "- Document Processes and Findings: Keep detailed documentation of all steps taken to identify and mitigate bias, including the methodologies used and the impacts observed. This documentation is crucial for transparency and for future audits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Development\n",
    "\n",
    "## What are the key considerations when designing a chatbot for internal use?\n",
    "- Centralize Data: Ensure the chatbot has access to centralized data sources like company databases or cloud storage to pull accurate information.\n",
    "- Structured Access: Structure data access patterns in the chatbot's logic to ensure it retrieves and manipulates data efficiently.\n",
    "- Document Retrieval: Enable the chatbot to search for and retrieve specific documents based on keywords or queries.\n",
    "- Keep Temperature 0\n",
    "\n",
    "## How do you ensure the chatbot remains up-to-date with internal data changes?\n",
    "- Scheduled Updates: Set up regular intervals for data synchronization to keep the chatbot's information current.\n",
    "- Real-Time Data Access: Use APIs or webhooks to integrate real-time data feeds from internal systems into the chatbot.\n",
    "- Monitoring and Alerts: Implement monitoring tools to detect data changes and alert the system, prompting updates.\n",
    "\n",
    "## How do you integrate NLP models into a chatbot system?\n",
    "#### The purpose of using an NLP model inside a chatbot system is to enable the chatbot to understand and interact with human language more effectively. Here are the key benefits:\n",
    "- Select an NLP model that suits your chatbot’s goals, such as language models for understanding context or intent recognition models for determining user needs.\n",
    "- Preprocess and normalize the input data (user queries) for the NLP model, which may include tokenization, removing stop words, and standardizing text.\n",
    "- Embed the NLP model directly into the chatbot’s architecture, or access it via APIs if the model runs on a separate server for scalability and maintenance.\n",
    "\n",
    "## Describe how you would handle misunderstandings or errors in chatbot responses.\n",
    "- Feedback Mechanism: Implement a simple way for users to report misunderstandings or incorrect responses, such as a thumbs up/thumbs down feature, or a \"Report a problem\" option.\n",
    "- If the chatbot detects uncertainty in its response or receives negative feedback, it should prompt the user for clarification or offer alternatives to refine the query.\n",
    "- For unresolved queries, have the chatbot escalate the issue to a human agent or provide contact details for further assistance.\n",
    "- Collect and analyze data on where and why misunderstandings occur. Use this data to continuously train and refine the chatbot's responses.\n",
    "- Design specific responses for when the chatbot fails to understand or when an error occurs, ensuring the user is not left without guidance.\n",
    "\n",
    "## How would you build a model to detect overpayments and underpayments in claims data?\n",
    "- Payment Ratios: Create features like the ratio of paid amount to billed amount (Paid/Billed Ratio), which helps in identifying unusual payments.\n",
    "- Service-Specific Trends: Develop features that reflect typical payment patterns for specific services or procedures, accounting for regional or provider variations.\n",
    "- Historical Payment Patterns: Use rolling averages, exponential smoothing, or time series analysis to capture historical payment trends for each claim type.\n",
    "- Isolation Forest: Detect claims that deviate significantly from the norm as potential overpayments or underpayments.\n",
    "- Threshold Tuning: Set and adjust thresholds for flagging claims based on business rules and risk tolerance (e.g., only flag claims where Paid/Billed Ratio is outside the 95th percentile).\n",
    "\n",
    "## Describe a project where you developed or improved a tool for auditing data.\n",
    "- Missing Values: Ensuring critical fields like patient ID, billing codes, and treatment dates were populated.\n",
    "- Data Type Validation: Ensuring fields had the correct data types (e.g., numeric values for billing amounts, dates in proper format).\n",
    "- Range Checks: Ensuring numerical fields, like patient age or billing amounts, were within logical and regulatory bounds.\n",
    "- Duplicate Detection: Identifying duplicate records based on patient ID and other key identifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario-Based Questions\n",
    "- User Feedback: Review user feedback or reports that flagged the incorrect advice to understand the context and frequency of the issue.\n",
    "- Data Freshness: Confirm whether the data being used by the chatbot is outdated. Check the timestamps on data imports or API calls to external data sources.\n",
    "- Manual Refresh: If the data is outdated, manually trigger a data refresh from the source systems to bring the chatbot's knowledge base up to date.\n",
    "- Data Pipeline Inspection: Inspect the data pipeline to ensure that data flows from the source to the chatbot without bottlenecks or errors. Look for broken API connections, ETL process failures, or misconfigured data jobs.\n",
    "- Simulate Scenarios: Test the chatbot by simulating the scenarios where it previously provided incorrect advice. Ensure the updated data is now being used and that the advice is correct.\n",
    "- Regression Testing: Perform regression testing to confirm that recent data updates have not introduced new issues or degraded the chatbot's overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "#### Word2vec is a technique in natural language processing (NLP) for obtaining vector representations of words. These vectors capture information about the meaning of the word based on the surrounding words. The word2vec algorithm estimates these representations by modeling text in a large corpus.\n",
    "\n",
    "## GLOVE\n",
    "### GloVe (Global Vectors for Word Representation) is an unsupervised learning algorithm used in natural language processing (NLP) that creates vector representations for words. The model is trained on a corpus of text by aggregating global word-word co-occurrence statistics. The goal of GloVe is to learn word vectors that capture the meaning of two words when they appear next to each other, and to do so in a way that their dot product equals the logarithm of their probability of co-occurrence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
