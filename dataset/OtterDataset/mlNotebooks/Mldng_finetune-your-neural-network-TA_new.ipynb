{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# KERAS\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset:\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the dataset:\n",
    "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the houses' prices in the training set\n",
    "sns.histplot(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values and types of each feature:\n",
    "pd.DataFrame(X_train).info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics about the numerical columns\n",
    "pd.DataFrame(X_train).describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0.3) Minimal Data Preprocessing\n",
    "\n",
    "üëâ Here, we don't have any duplicates or missing values. Let's do the bare minimum of data preprocessing, i.e. ***scaling**, and move on quickly to the modeling phase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0.4) Baseline Model\n",
    "\n",
    "üßëüèª‚Äçüè´ In a regression task, the baseline model **always predicts the average value of `y_train`**\n",
    "\n",
    "<details>\n",
    "    <summary>Really?</summary>\n",
    "    \n",
    "- üêí  Yes, in most cases!\n",
    "- ‚ùóÔ∏è  Be aware that this is not the only possible way of building a baseline model\n",
    "- üíπ  In Time Series, the baseline model predicts the **last seen value**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚ùìQuestion: what would be the performance of the baseline model here?\n",
    "\n",
    "Before running any Machine Learning algorithm or advanced Deep Learning Neural Networks, it would be great to establish a benchmark score that you are supposed to beat. Otherwise, what is the point of running a fancy algorithm if you cannot beat this benchmark score on the testing set (other than showing off)?\n",
    "\n",
    "Compute the Mean Absolute Error on the testing set using a \"dumb\" prediction of the mean value of `y_train`, computed on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) The Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùìInitializing a Neural Network with a Specific Architecture\n",
    "Write a function called `initialize_model` that generates a Neural Network with 3 layers:\n",
    "- Input layer: **10 neurons**, `relu` activation function, and the appropriate input dimension\n",
    "- Hidden layer: **7 neurons** and the `relu` activation function\n",
    "- Predictive layer: an appropriate layer corresponding to the problem we are trying to solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    pass  # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚ùìNumber of Parameters\n",
    "How many parameters do we have in this model? \n",
    "1. Compute this number yourself\n",
    "2. Double-check your answer with `model.summary()`\n",
    "\n",
    "We already covered the question about the number of parameters in a fully connected/dense network during **Deep Learning > 01. Fundamentals of Deep Learning** but it is always good to make sure you master the foundations of a new discipline üòâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üí°Answer\n",
    "\n",
    "<details>3\n",
    "    <summary>Click here</summary>\n",
    "\n",
    "- Each house has `X_train.shape[-1]` = 13 features\n",
    "- Remember that a neuron is a linear regression combined with an activation function, so we will have 13 weights and 1 bias\n",
    "1. First layer: **10 neurons** $\\times$ (13 weights + 1 bias ) = 140 params\n",
    "2. Second layer: **7 neurons** $\\times$ (10 weights + 1 bias ) = 77 params\n",
    "3. Third layer: **1 neuron** $\\times$ (7 weights + 1 bias) = 8 params\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) The Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùìCompiling Method\n",
    "Write a function that:\n",
    "1. takes _both_ a **model** and an **optimizer** as arguments\n",
    "2. **compiles** the model\n",
    "3. returns the compiled model\n",
    "\n",
    "Please select wisely:\n",
    "- the **Loss Function** to be optimized\n",
    "- the **metrics** on which the model should be evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, optimizer_name):\n",
    "    pass  # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùìEvaluating the Model\n",
    "\n",
    "- Initialize the model and compile it with the `adam` optimizer\n",
    "- Fit it on the training data\n",
    "- Evaluate your model on the testing data\n",
    "\n",
    "Don't forget to use an Early Stopping criterion to avoid overfitting!\n",
    "\n",
    "<details>\n",
    "    <summary>Notes</summary>\n",
    "\n",
    "As we saw in the **\"How to prevent overfitting\"** challenge,  you could also use L2 penalties and Dropout Layers to prevent overfitting but:\n",
    "- Early Stopping is the easiest and quickest code to implement, you just declare `es = EarlyStopping()` and call it back in the `.fit()` step\n",
    "\n",
    "- The main goal of this challenge is to understand the **impact of the optimizer**, so stay focused üòâ\n",
    "cscs\n",
    "cscs xxxxcxcx\n",
    "4fdgdsdcdcsddvd\n",
    "ngvgdvdvdxxssxssxcscscscscscscsxsx\n",
    "yjysdsrgrscscxsxxaxasxsxs\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
