{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNjHXjqCWleX2d6oOiLS5k0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/DTSC5502/blob/main/Module_12-Learning/Basic_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic PyTorch\n",
        "\n",
        "Your name somewhere"
      ],
      "metadata": {
        "id": "YFS9R9dTMTof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Runtime Processing\n",
        "\n",
        "Here's a breakdown of the differences between CPUs, T4 GPUs, and TPU v2-8s:\n",
        "\n",
        "**CPU (Central Processing Unit)**\n",
        "\n",
        "* **The Brain:** This is the general-purpose processor found in every computer. It handles all the basic operations of the system, from running your operating system and applications to executing calculations and managing data.\n",
        "* **Sequential Processing:** CPUs are designed to handle a wide variety of tasks, but they typically excel at sequential processing, executing instructions one after another.\n",
        "* **Limited Cores:** CPUs have a relatively small number of cores (processing units), typically ranging from 4 to 64 in consumer-grade processors.\n",
        "\n",
        "**T4 GPU (Graphics Processing Unit)**\n",
        "\n",
        "* **Parallel Powerhouse:**  Originally designed for graphics rendering, GPUs have evolved into powerful parallel processors. They excel at handling tasks that can be broken down into many smaller, simultaneous operations.\n",
        "* **Massive Cores:** GPUs contain thousands of cores, allowing them to perform massive parallel computations.\n",
        "* **Deep Learning Applications:** This parallel processing power makes GPUs well-suited for deep learning tasks like image recognition, natural language processing, and scientific simulations.\n",
        "* **NVIDIA Tesla T4:**  The T4 is a specific GPU model from NVIDIA's Tesla series, designed for high-performance computing and AI workloads. It offers a good balance of performance and power efficiency.\n",
        "\n",
        "**TPU v2-8 (Tensor Processing Unit)**\n",
        "\n",
        "* **Google's AI Specialist:** TPUs are custom-designed processors developed by Google specifically for machine learning and AI workloads.\n",
        "* **Matrix Multiplication Focus:** TPUs are optimized for matrix multiplication, a core operation in deep learning algorithms.\n",
        "* **High Throughput:** They offer very high throughput for matrix operations, leading to faster training and inference of deep learning models.\n",
        "* **TPU v2-8:** This refers to a specific generation and configuration of TPUs. It typically consists of multiple TPU chips interconnected to provide massive computational power.\n",
        "* **TensorFlow Integration:** TPUs are tightly integrated with Google's TensorFlow framework, providing optimized performance for TensorFlow models.\n",
        "\n",
        "**Key Differences**\n",
        "\n",
        "| Feature | CPU | T4 GPU | TPU v2-8 |\n",
        "|---|---|---|---|\n",
        "| **Primary Purpose** | General-purpose processing | Graphics and parallel processing | Machine learning and AI |\n",
        "| **Architecture** | Sequential processing, few cores | Parallel processing, many cores | Matrix multiplication focus, high throughput |\n",
        "| **Strengths** | Versatile, handles various tasks | High performance for parallel workloads | Extremely fast for deep learning |\n",
        "| **Weaknesses** | Limited parallel processing power | Can be power-hungry | Specialized for TensorFlow |\n",
        "\n",
        "**When to Use Each**\n",
        "\n",
        "* **CPU:** For general-purpose tasks, running applications, and tasks that don't require massive parallel computation.\n",
        "* **T4 GPU:** For deep learning, scientific simulations, graphics rendering, and other tasks that benefit from parallel processing.\n",
        "* **TPU v2-8:** For large-scale deep learning training and inference, especially with TensorFlow models.\n",
        "\n",
        "**In Summary**\n",
        "\n",
        "CPUs, T4 GPUs, and TPU v2-8s are all processors designed for different purposes and with varying strengths. CPUs are general-purpose workhorses, GPUs are parallel powerhouses, and TPUs are specialized for AI. Choosing the right processor depends on the specific workload and requirements."
      ],
      "metadata": {
        "id": "ZqOV_hqoL5AW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# the neural net\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.fc1 = nn.Linear(4, 5)\n",
        "    self.fc2 = nn.Linear(5, 4)\n",
        "    self.fc3 = nn.Linear(4, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.sigmoid(self.fc1(x))\n",
        "    x = torch.sigmoid(self.fc2(x))\n",
        "    x = torch.sigmoid(self.fc3(x))\n",
        "    return x\n",
        "\n",
        "X = torch.tensor([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\n",
        "y = torch.tensor([[0], [1]], dtype=torch.float32)\n",
        "\n",
        "net = Net()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
        "\n",
        "epochs = 10000\n",
        "for epoch in range(epochs):\n",
        "  outputs = net(X)\n",
        "  loss = criterion(outputs, y)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch % 1000 == 0:\n",
        "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
        "\n",
        "# X = torch.tensor([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\n",
        "predictions = net(X)\n",
        "# print(predictions)\n",
        "print(\"Final Predictions:\", [list(arr) for arr in predictions])\n"
      ],
      "metadata": {
        "id": "RhLGqAAMLRci",
        "outputId": "cf41e1aa-a8ca-4852-9476-cb295e876aa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.25482961535453796\n",
            "Epoch 1000, Loss: 0.24583175778388977\n",
            "Epoch 2000, Loss: 0.22925221920013428\n",
            "Epoch 3000, Loss: 0.09875045716762543\n",
            "Epoch 4000, Loss: 0.013522750698029995\n",
            "Epoch 5000, Loss: 0.005175087600946426\n",
            "Epoch 6000, Loss: 0.0029665709007531404\n",
            "Epoch 7000, Loss: 0.002019819337874651\n",
            "Epoch 8000, Loss: 0.0015089728403836489\n",
            "Epoch 9000, Loss: 0.001194087089970708\n",
            "Final Predictions: [[tensor(0.0316, grad_fn=<UnbindBackward0>)], [tensor(0.9689, grad_fn=<UnbindBackward0>)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor([[0.1, 0.2, 0.3, 0.4], [0.5, 0.6, 0.7, 0.8]])\n",
        "predictions = net(X)\n",
        "print(predictions)\n",
        "print(\"Final Predictions:\", [list(arr) for arr in predictions])"
      ],
      "metadata": {
        "id": "kxOczBlHywZ2",
        "outputId": "8ceecbff-711f-4f30-9149-089391a42926",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0316],\n",
            "        [0.9689]], grad_fn=<SigmoidBackward0>)\n",
            "Final Predictions: [[tensor(0.0316, grad_fn=<UnbindBackward0>)], [tensor(0.9689, grad_fn=<UnbindBackward0>)]]\n"
          ]
        }
      ]
    },
    {
      "source": [
        "nu_X = torch.tensor([0.3, 0.4, 0.5, 0.6], dtype=torch.float32)\n",
        "\n",
        "nu_predictions = net(nu_X)\n",
        "print(nu_predictions)\n",
        "# Instead of trying to iterate, directly convert the tensor to a list:\n",
        "print(\"Final Predictions:\", [nu_predictions.item()])"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "5NA3rQ9B1K19",
        "outputId": "c7d51701-8c86-45d0-fb84-f7867ca78869",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.6115], grad_fn=<SigmoidBackward0>)\n",
            "Final Predictions: [0.6114510297775269]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, a `torch.Tensor` is a multi-dimensional array that is the fundamental building block for all operations and models. You can think of it as the PyTorch equivalent of a NumPy array, but with some key advantages for deep learning.\n",
        "\n",
        "Here's a breakdown of what makes `torch.Tensor` special:\n",
        "\n",
        "**1. GPU Support**\n",
        "\n",
        "* **CUDA Integration:** One of the primary benefits of PyTorch tensors is their seamless integration with NVIDIA GPUs. This allows you to perform computations on the GPU, significantly accelerating training and inference of deep learning models.\n",
        "* **Automatic Transfers:** PyTorch can automatically transfer tensors between the CPU and GPU as needed, simplifying the process of utilizing GPU resources.\n",
        "\n",
        "**2. Automatic Differentiation**\n",
        "\n",
        "* **`autograd` Package:** PyTorch tensors are integrated with the `autograd` package, which enables automatic differentiation. This means PyTorch can automatically calculate gradients (derivatives) of operations performed on tensors, which is crucial for training neural networks using gradient-based optimization algorithms.\n",
        "\n",
        "**3. Optimized Operations**\n",
        "\n",
        "* **Efficient Computations:** PyTorch tensors are optimized for efficient numerical computation. They provide a wide range of built-in functions for tensor manipulation, linear algebra, and other mathematical operations commonly used in deep learning.\n",
        "\n",
        "**4. Neural Network Building Blocks**\n",
        "\n",
        "* **`torch.nn` Module:** PyTorch tensors are used extensively in the `torch.nn` module, which provides a collection of pre-built layers, activation functions, and other components for building neural networks.\n",
        "\n",
        "**5. Dynamic Computation Graph**\n",
        "\n",
        "* **Define-by-Run:** PyTorch uses a dynamic computation graph, which means the graph is constructed as you execute operations. This allows for flexibility in defining and modifying models during runtime, which is particularly useful for research and experimentation.\n",
        "\n",
        "**Creating Tensors**\n",
        "\n",
        "You can create PyTorch tensors in various ways:\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# From a list\n",
        "tensor_from_list = torch.tensor([1, 2, 3, 4])\n",
        "\n",
        "# From a NumPy array\n",
        "numpy_array = np.array([5, 6, 7, 8])\n",
        "tensor_from_numpy = torch.tensor(numpy_array)\n",
        "\n",
        "# With random values\n",
        "random_tensor = torch.randn(3, 4)  # Creates a 3x4 tensor with random values\n",
        "\n",
        "# With zeros\n",
        "zeros_tensor = torch.zeros(2, 5)  # Creates a 2x5 tensor filled with zeros\n",
        "```\n",
        "\n",
        "**Key Takeaways**\n",
        "\n",
        "* `torch.Tensor` is the fundamental data structure in PyTorch for numerical computation.\n",
        "* It offers GPU support, automatic differentiation, optimized operations, and integration with neural network modules.\n",
        "* PyTorch tensors are essential for building and training deep learning models efficiently."
      ],
      "metadata": {
        "id": "0QSe1E-QMpQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "criterion = nn.MSELoss()  # Mean Squared Error loss\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1)  # Stochastic Gradient Descent\n",
        "```\n",
        "\n",
        "These two lines of code are essential for training your neural network in PyTorch. They define the **loss function** and the **optimizer** that will be used to update the network's parameters during training.\n",
        "\n",
        "**`criterion = nn.MSELoss()`**\n",
        "\n",
        "* **Loss Function:** A loss function measures the difference between the network's predictions and the actual target values. It quantifies the error that the network is making.\n",
        "* **`nn.MSELoss()`:** This creates an instance of the `MSELoss` class from PyTorch's `nn` module. `MSELoss` stands for Mean Squared Error Loss, a common loss function for regression tasks. It calculates the average of the squared differences between the predicted and target values.\n",
        "* **Purpose:** The `criterion` (loss function) will be used during training to calculate the loss between the network's output and the true labels. This loss value guides the optimization process, indicating how well the network is performing and how the parameters should be adjusted to improve accuracy.\n",
        "\n",
        "**`optimizer = optim.SGD(net.parameters(), lr=0.1)`**\n",
        "\n",
        "* **Optimizer:** An optimizer is an algorithm that adjusts the network's parameters (weights and biases) to minimize the loss function.\n",
        "* **`optim.SGD(...)`:** This creates an instance of the `SGD` class from PyTorch's `optim` module. `SGD` stands for Stochastic Gradient Descent, a widely used optimization algorithm.\n",
        "* **`net.parameters()`:** This provides the optimizer with the parameters of your neural network (`net`) that need to be updated during training.\n",
        "* **`lr=0.1`:** This sets the learning rate for the optimizer. The learning rate controls the step size taken in the direction of the negative gradient during each iteration of the optimization process.\n",
        "\n",
        "**In Summary**\n",
        "\n",
        "* **`criterion`:** Defines the loss function (Mean Squared Error) to measure the network's prediction error.\n",
        "* **`optimizer`:** Defines the optimization algorithm (Stochastic Gradient Descent) to update the network's parameters and minimize the loss.\n",
        "\n",
        "These two components work together during the training loop:\n",
        "\n",
        "1. **Forward Pass:** The input data is passed through the network to generate predictions.\n",
        "2. **Loss Calculation:** The `criterion` (loss function) is used to calculate the error between the predictions and the true labels.\n",
        "3. **Backward Pass:** The `optimizer` calculates the gradients of the loss with respect to the parameters.\n",
        "4. **Parameter Update:** The `optimizer` updates the network's parameters based on the calculated gradients and the learning rate.\n",
        "\n",
        "This iterative process continues for a specified number of epochs, gradually improving the network's accuracy by minimizing the loss function."
      ],
      "metadata": {
        "id": "m4bR_tfmNBZ4"
      }
    }
  ]
}