{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v-bDj7E8fr65"
   },
   "source": [
    "# Computer, color my picture!\n",
    "A guide to re-color pictures based on natural language queries and Microsoft's Coco dataset  \n",
    "– Benjamin Beilharz –\n",
    "***\n",
    "<br> \n",
    "Autocolorizing images has been already achieved by interactive human input by giving signals in forms of clicks or strokes on an image while having selected specific colors.\n",
    "This implementation handles the input signals in form of natural language.  \n",
    "  \n",
    "As of today, merging multiple modalities in machine learning like language and images is still a complex tasks than people would think it is. Finding suitable ways to engineer proper feature combinations of above modalities, without depending on only one has been seen in a few publications in the field of multimodality. Either we depend on one feature only, or else we just are not able to find a suitable solution to train these, indeed difficult networks alltogether.\n",
    "  \n",
    "In this post I want to give you a quick overview about my experience diving into computer vision and conducting the attempt to reimplement the paper _Learning to Color from Language, Manjunatha et al. 2018_. The code of their implementation for Python2 is available on [Github](https://github.com/superhans/colorfromlanguage) and my code is mostly based on their implementation.  \n",
    "  \n",
    "**Disclaimer: I did not manage to get the network to learn properly.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zhBoCc77flLL"
   },
   "outputs": [],
   "source": [
    "# import modules commonly used for image processing and utilities provided by\n",
    "# the original authors\n",
    "\n",
    "import cv2\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import scipy\n",
    "import string\n",
    "import torch\n",
    "import torchtext\n",
    "import torchvision\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import scipy.ndimage.interpolation as sni\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from skimage import io, color\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from utils import decode_lookup, produce_minibatch_idxs, rgbim2lab,\\\n",
    "    enc_batch, prior_boosting, display, decode,init_modules, cvrgb2lab,\\\n",
    "    annealing, enc_batch_nnenc, LookupEncode, labim2rgb, error_metric, rmse_ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbrZvXzF85Du"
   },
   "source": [
    "## Data\n",
    "***\n",
    "In terms of data the previous authors went for the popular dataset from Microsoft, namely MS COCO (Common Objects in Context), a large dataset that is used for object detection and segmentation which has captions added to each image.  \n",
    "\n",
    "Feel free to read additional information and download it on the [offical page](http://cocodataset.org/#home).\n",
    "\n",
    "I used the data provided by the original authors of the paper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WdHsldufDH2A"
   },
   "source": [
    "### Images in a nutshell\n",
    "We probably all know what is known as an image, but in terms of physics and math, an image is commonly represented as either a greyscale image with one color channel (lightness), or as a colored one, displayed as three overlaying color channels (red, green, blue) whereas each pixel has an arbitrary lightness/color value between 0 (black) and 255 (white).\n",
    "\n",
    "So for an RGB image we have the following: $height\\times width \\times channels$, dealing with a tensor.  \n",
    "  \n",
    "<br>\n",
    "\n",
    "![Color channels](https://miro.medium.com/max/2146/1*icINeO4H7UKe3NlU1fXqlA.jpeg)\n",
    "<center>RGB color channels visualized</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "evoS2btuKTTG"
   },
   "source": [
    "## Feature extraction\n",
    "***\n",
    "\n",
    "For feature extraction the ResNet 101 architecture is used, which is a CNN consisting of 101 layers and available as a pretrained model in *torchvision*. It utilizes residual connections to learn into deeper layers by adding the input to the processed outputs after convolution layers.\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1200/1*6hF97Upuqg_LdsqWY6n_wg.png)\n",
    "<center>ResNet 101 architecture</center>  \n",
    "  \n",
    "<small>*Please look at the architecture output below to get a glimpse of the feature extraction layers used.*</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-luNYLZEO0nO",
    "outputId": "c28ee7f2-8c46-4fb4-b47b-32f814933367"
   },
   "outputs": [],
   "source": [
    "def resnet():\n",
    "    \"\"\"Loads and extract layers from pre-trained resnet101 model\n",
    "    \n",
    "    Returns:\n",
    "        pytorch model - torch.nn.Model\n",
    "    \"\"\"\n",
    "    cnn = torchvision.models.resnet101(pretrained=True)\n",
    "    layers = [cnn.conv1,\n",
    "              cnn.bn1,\n",
    "              cnn.relu,\n",
    "              cnn.maxpool\n",
    "             ]\n",
    "    for i in range(2):  # model_stage_2 -> 1200, 512, 28, 28\n",
    "        name = f'layer{i+1}'\n",
    "        layers.append(getattr(cnn, name))\n",
    "    \n",
    "    model = torch.nn.Sequential(*layers)\n",
    "    # model.cuda()\n",
    "    model.double()\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tMpvWPiMSTaK"
   },
   "outputs": [],
   "source": [
    "def process_batch(current, model):\n",
    "    \"\"\"Processes batch of images\n",
    "\n",
    "    Args:\n",
    "        current - np.array - batch of grayscale images BATCH x Channels x Height x Width\n",
    "        model - torch.nn.Module - pretrained resnet 101 model\n",
    "    \n",
    "    Returns:\n",
    "        features - np.array - Extracted features by resnet 101\n",
    "    \"\"\"\n",
    "    \n",
    "    batch = np.concatenate(current, 0).astype(np.float32)  # concat all matrices on 4th dim\n",
    "    # squeeze grayscale color into lightness channel according to lab color space\n",
    "    mean = np.array([0.485, 0.456, 0.406]).reshape(1, 3, 1, 1)\n",
    "    std = np.array([0.229, 0.224, 0.224]).reshape(1, 3, 1, 1)\n",
    "    batch = (batch/255. - mean) /std\n",
    "    batch = torch.tensor(batch, requires_grad=False).cuda()\n",
    "    features = model(batch)\n",
    "    return features.data.cpu().clone().numpy()\n",
    "\n",
    "def generate_vision_features(image: h5.File, model: nn.Module, BATCH_SIZE: int):\n",
    "    \"\"\"Extract vision features from coco dataset.\n",
    "    \n",
    "    Args:\n",
    "        image dataset - h5.File - coco dataset for feature extraction\n",
    "        model - nn.Module - resnet101 model\n",
    "        BATCH_SIZE - int - images per batch\n",
    "    \"\"\"\n",
    "    with h5.File('img_features.h5', 'w') as f:\n",
    "        # splitting dataset into image features for training and validation\n",
    "        for split in ['train', 'val']:\n",
    "            dataset = None\n",
    "            current_batch = []\n",
    "            lowerbound = 0\n",
    "            image_files = images[split+'_ims']\n",
    "            for i, img in tqdm(enumerate(image_files)):\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # convert color space to grayscale\n",
    "                img = np.stack((img, )*3, -1).transpose(2, 0, 1)[None]\n",
    "                current_batch.append(img)\n",
    "                if len(current_batch) == BATCH_SIZE:\n",
    "                    features = process_batch(current_batch, model)\n",
    "                    if dataset is None:\n",
    "                        N = len(image_files)\n",
    "                        _, C, H, W = features.shape\n",
    "                        dataset = f.create_dataset(split + '_features', (N, C, H, W),\n",
    "                                                   dtype=np.float32)\n",
    "                        upperbound = lowerbound + len(current_batch)\n",
    "                        dataset[lowerbound:upperbound] = features  # add image features to images\n",
    "                        lowerbound = upperbound\n",
    "                        current_batch.clear()\n",
    "                    if len(current_batch) > 0:  # if batch size results into remainders\n",
    "                        features = process_batch(current_batch, model)\n",
    "                        upperbound = lowerbound + len(current_batch)\n",
    "                        dataset[lowerbound:upperbound] = features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3_JkcdA_39XV"
   },
   "source": [
    "### Language encoding\n",
    "\n",
    "As we like to learn to colorize pictures based on captions, we also need to learn a contextualized representation of the with a standard bidirectional LSTM and use the last hidden state to condition the CNN throughout the convolution blocks.\n",
    "\n",
    "![LSTM EXPLAINED](https://www.mdpi.com/water/water-11-01387/article_deploy/html/images/water-11-01387-g004.png)\n",
    "<br>\n",
    "<center>LSTM Explained</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iIWja1Ch27H7"
   },
   "outputs": [],
   "source": [
    "class LanguageEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_dim: int,\n",
    "                 h_dim: int,\n",
    "                 vocab_size: int,\n",
    "                 pretrained_embeddings: bool):\n",
    "        super(LanguageEncoder, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        # warm start the model by adding pretrained weights\n",
    "        self.emb.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n",
    "        self.hidden_size = h_dim\n",
    "        self.rnn = nn.LSTM(emb_dim, h_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, query, lengths):\n",
    "        bsz, max_len = query.size()  # get dimensionalities by input \n",
    "        emb = self.dropout(self.emb(query))  # regulatization\n",
    "        lenghts, indices = torch.sort(lengths, dim=0, descending=True)\n",
    "        _, (h_t, _) = self.rnn(pack_padded_sequence(emb[indices], lengths.data.tolist(), batch_first=True, enforce_sorted=False))\n",
    "        h_t = torch.cat((h_t[0], h_t[1]), 1)  # concatenate both directional hidden state outputs\n",
    "        _, indices = torch.sort(indices, dim=0)\n",
    "        h_t = h_t[indices]\n",
    "        return h_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "25Xts6OnQq70"
   },
   "source": [
    "To combine the modalities a feature-wise affine transformation (FiLM: Visual Reasoning with a General Conditioning Layer, Perez et al. 2018) has been applied, which can be seen analog to the gating machinism we know from gated recurrent neural networks, like an input gate to determine which information should be used from the previous hidden state. \n",
    "\n",
    "\n",
    "![FILM Explained](./images/film.png)\n",
    "<center>FiLM Layer visualized from Perez et al. 2017</center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final FiLM architecture in between looks somewhat like this:  \n",
    "![FILM Arc](./images/filmed.png)\n",
    "<center>FiLM residual block from Perez et al. 2017</center>\n",
    "<br>\n",
    "\n",
    "We use a recurrent neural network to encode the textual information and add a linear transformation to add the extracted vector into the residual blocks with the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8LakjerC3SKF"
   },
   "outputs": [],
   "source": [
    "class FiLM(nn.Module):\n",
    "    \"\"\"Feature-wise affine transformation\n",
    "    Based on (Perez et al. 2018)\n",
    "    Implementation: https://github.com/ethanjperez/film/blob/master/vr/models/filmed_net.py\n",
    "    \n",
    "    Conditions the output of a convolutional block to the weights based on\n",
    "    language encoding.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(FiLM, self).__init__()\n",
    "    def forward(self, x, gammas, betas):\n",
    "        # add dimensions to gamma and beta and adjust dimensions to input\n",
    "        gammas = gammas.unsqueeze(2).unsqueeze(3).expand_as(x)  \n",
    "        betas = betas.unsqueeze(2).unsqueeze(3).expand_as(x)\n",
    "        return gammas * x + betas\n",
    "    \n",
    "class FiLMedResBlock(nn.Module):\n",
    "    \"\"\"Linear projection of gamma and beta by adding further parameters to learn\n",
    "    language conditioning.\n",
    "\n",
    "    Uses residual connections over convolution and FiLM layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, stride=1, padding=1, dilation=1):\n",
    "        super(FiLMedResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_dim, in_dim, kernel_size=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_dim, out_dim, kernel_size=3, stride=stride, padding=1, dilation=dilation)\n",
    "        self.bn2 = nn.BatchNorm2d(out_dim)\n",
    "        self.film = FiLM()\n",
    "        init_modules(self.modules())  # initialize weights\n",
    "        \n",
    "    def forward(self, x, gammas, betas):\n",
    "        # forward pass with residual connection\n",
    "        y = x\n",
    "        y = F.relu(self.conv1(y))\n",
    "        y = self.bn2(F.relu(self.conv2(y)))\n",
    "        y = F.relu(self.film(y, gammas, betas))\n",
    "        return y + x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ve0OXdMnUoze"
   },
   "source": [
    "## Autocolorize Architecture\n",
    "***\n",
    "The complete architecture for autocolorizing grayscale images based on captions has the following structure:   \n",
    "![colorresnet](./images/color.png)\n",
    "<center>ColorizeResnet</center>\n",
    "<br>\n",
    "\n",
    "1. Grayscale images (25, 25, width/height respectively)\n",
    "2. Extracting features with pre-trained resnet101\n",
    "3. LSTM Encoding of caption\n",
    "4. FiLM block to condition image features by element wise multiplication with language\n",
    "5. Linear layers inbetween FiLM blocks\n",
    "6. Classification with Cross Entropy Loss\n",
    "\n",
    "\n",
    "### Training objective\n",
    "\n",
    "In this task we use the **CIELAB** color space as our ground truth. The **LAB** color space consists of a lightness channel (squeezed into a range of 0 to 100, as mentioned above with two additional channels where:\n",
    "\n",
    "* L, a lightness channel, while 0 being black and 100 being white\n",
    "* A, as the green-red component, with negative values being green hues and positive being reds\n",
    "* B, as the blue-yellow channel, with negative values being blue hues and positive being yellows\n",
    "  \n",
    "![LAB Color space](https://upload.wikimedia.org/wikipedia/commons/7/7d/CIELAB_color_space_front_view.png)\n",
    "<center>A front view of the LAB color space</center>  \n",
    "<br>\n",
    "\n",
    "The training objective is defined as given an input lightness channel $X \\in \\mathbb{R}^{H\\times W\\times 1}$ we predict the two other color channels for **LAB**, $Y \\in \\mathbb{R}^{H\\times W\\times 2}$.\n",
    "\n",
    "\n",
    "Below the full model and training procedure as well as evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8CzM0IRgO-y3"
   },
   "outputs": [],
   "source": [
    "class AutocolorizeResnet(nn.Module):\n",
    "    def __init__(self, vocab_size, feature_dim=(512, 28, 28), h_dim=256, emb_dim=300, num_modules=4, num_classes=625, train_vocab_embeddings=None):\n",
    "        super(AutocolorizeResnet, self).__init__()\n",
    "        self.num_modules = num_modules\n",
    "        self.n_lstm_hidden = h_dim\n",
    "        self.block = FiLMedResBlock\n",
    "        self.in_dim = feature_dim[0]\n",
    "        self.num_classes = num_classes\n",
    "        dilations = [1, 1, 1, 1]\n",
    "\n",
    "        # standard bidirectional LSTM encoder\n",
    "        self.language_encoder = LanguageEncoder(emb_dim, h_dim, vocab_size, train_vocab_embeddings)\n",
    "\n",
    "        # 512x512\n",
    "        self.mod1 = self.block(self.in_dim, self.in_dim, dilations[0])\n",
    "        self.mod2 = self.block(self.in_dim, self.in_dim, dilations[1])\n",
    "        self.mod3 = self.block(self.in_dim, self.in_dim, dilations[2])\n",
    "        self.mod4 = self.block(self.in_dim, self.in_dim, dilations[3])\n",
    "\n",
    "        # language representations are projected into input dimensionality of CNN\n",
    "        # mutliplying hidden dimensions by 2 because of bidirectional encoding\n",
    "        # 512 x 1024\n",
    "        self.dense_film_1 = nn.Linear(self.n_lstm_hidden*2, self.in_dim*2) \n",
    "        self.dense_film_2 = nn.Linear(self.n_lstm_hidden*2, self.in_dim*2)\n",
    "        self.dense_film_3 = nn.Linear(self.n_lstm_hidden*2, self.in_dim*2)\n",
    "        self.dense_film_4 = nn.Linear(self.n_lstm_hidden*2, self.in_dim*2)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.classifier = nn.Conv2d(512, self.num_classes, kernel_size=1, stride=1, dilation=1)\n",
    "\n",
    "\n",
    "    def forward(self, x, query, query_lens):\n",
    "        # encode caption/query\n",
    "        query_features = self.language_encoder(query, query_lens)\n",
    "\n",
    "        # linear projections for each of convolution block\n",
    "        dense_film_1 = self.dense_film_1(query_features)\n",
    "        dense_film_2 = self.dense_film_2(query_features)\n",
    "        dense_film_3 = self.dense_film_3(query_features)\n",
    "        dense_film_4 = self.dense_film_4(query_features) # bsz * 128\n",
    "\n",
    "        # prepare gammas and betas for language weighting on convolutional layers\n",
    "        gammas1, betas1 = torch.split(dense_film_1, self.in_dim, dim=-1)\n",
    "        gammas2, betas2 = torch.split(dense_film_2, self.in_dim, dim=-1)\n",
    "        gammas3, betas3 = torch.split(dense_film_3, self.in_dim, dim=-1)\n",
    "        gammas4, betas4 = torch.split(dense_film_4, self.in_dim, dim=-1)\n",
    "        \n",
    "        # out is 2x512x28x28\n",
    "        out = self.mod1(x, gammas1, betas1)\n",
    "        out = self.mod2(out, gammas2, betas2)\n",
    "        out = self.mod3(out, gammas3, betas3)\n",
    "        out_last = self.mod4(out, gammas4, betas4) \n",
    "        \n",
    "        out = self.upsample(out_last)  # 4x1024x56x56\n",
    "        out = self.classifier(out) # 4 x nclasses x 56 x 56\n",
    "        out = out.permute(0, 2, 3, 1).contiguous() # 4 x 56 x 56 x nclasses\n",
    "        out = out.view(-1, self.num_classes)  # * x nclasses\n",
    "        return out, out_last\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OTpkZ6uQPE4W"
   },
   "outputs": [],
   "source": [
    "def train(minibatches, net, optimizer, epoch, prior_probs, img_save_folder, writer):\n",
    "    stime = time.time()\n",
    "    c = Counter()\n",
    "    for i, (batch_start, batch_end) in enumerate(minibatches):\n",
    "        img_rgbs = train_origs[batch_start:batch_end]\n",
    "        # convert all original rbg images to lab color space\n",
    "        img_labs = np.array([cvrgb2lab(img_rgb) for img_rgb in img_rgbs])\n",
    "\n",
    "        input_ = torch.from_numpy(train_ims[batch_start:batch_end])\n",
    "        \n",
    "        # lookup LAB values to create ground truth\n",
    "        target = torch.from_numpy(lookup_enc.encode_points(img_labs[:, ::4, ::4, 1:]))\n",
    "\n",
    "        input_query_ = train_words[batch_start:batch_end]\n",
    "        input_lengths_ = train_lengths[batch_start:batch_end]\n",
    "\n",
    "        # choose a caption and encode it as long dtype to be processed by embedding layer\n",
    "        input_query = torch.from_numpy(input_query_.astype('int32')).long().cuda()\n",
    "        input_query_lens = torch.from_numpy(input_lengths_.astype('int32')).long().cuda()\n",
    "\n",
    "        # define input images\n",
    "        input_ims = torch.tensor(input_.float()).cuda()\n",
    "        target = torch.tensor(target.float()).cuda()\n",
    "\n",
    "        # reset gradient\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = net(input_ims, input_query, input_query_lens)\n",
    "\n",
    "        # calculate cross_entropy loss\n",
    "        loss = loss_function(output, target.view(-1)) \n",
    "\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # write loss to Tensorboard\n",
    "        writer.add_scalar('train/loss', loss.item(), i)\n",
    "        \n",
    "        # print at every\n",
    "        if i % 50 == 0:\n",
    "            print('loss at epoch %d, batch %d / %d = %f, time: %f s' % \\\n",
    "                (epoch, i, len(minibatches), loss.item(), time.time()-stime))\n",
    "            stime = time.time()\n",
    "\n",
    "            if True:\n",
    "                # applying softmax and transform with a/b color values\n",
    "                dec_inp = F.softmax(output, dim=1)\n",
    "                AB_vals = torch.matmul(dec_inp, cuda_cc) # 12544x2\n",
    "                \n",
    "                # reshape and select last image of batch\n",
    "                AB_vals = AB_vals.view(len(img_labs), 56, 56, 2)[-1].data.cpu().numpy()[None,:,:,:]\n",
    "                # resize image to previous width/height\n",
    "                AB_vals = cv2.resize(AB_vals[0], (224, 224),\n",
    "                     interpolation=cv2.INTER_CUBIC)\n",
    "                # convert image back to rgb color space\n",
    "                img_dec = labim2rgb(np.dstack((np.expand_dims(img_labs[-1, :, :, 0], axis=2), AB_vals)))\n",
    "\n",
    "                # save last sample with caption\n",
    "                img_labs_tosave = labim2rgb(img_labs[-1])\n",
    "                word_list = input_query_[-1, :input_lengths_[-1]]\n",
    "                words = '_'.join(vrev.get(w, 'unk') for w in word_list) \n",
    "                \n",
    "                # save the grayscale, colored (ground truth) and recolored image\n",
    "                cv2.imwrite(f'{img_save_folder}/{epoch}_{i}_bw.jpg',\n",
    "                    cv2.cvtColor(img_rgbs[-1].astype('uint8'), \n",
    "                    cv2.COLOR_RGB2GRAY))\n",
    "                cv2.imwrite(f'{img_save_folder}/{epoch}_{i}_color.jpg',\n",
    "                    img_rgbs[-1].astype('uint8'))\n",
    "                cv2.imwrite(f'{img_save_folder}/{epoch}_{i}_rec_{words}.jpg',\n",
    "                    img_dec.astype('uint8'))\n",
    "           \n",
    "                # save model at every epoch\n",
    "                if i == 0:                 \n",
    "                    torch.save({\n",
    "                        'epoch': epoch + 1,\n",
    "                        'state_dict': net.state_dict(),\n",
    "                        'optimizer' : optimizer.state_dict(),\n",
    "                        'loss': loss.item(),\n",
    "                    }, 'model_' + str(epoch)+'_'+str(i)+'.pth.tar')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_attention_map(x):\n",
    "    \"\"\"Scale visual attention map to image size\n",
    "    \n",
    "    Args:\n",
    "        x - np.array - \n",
    "    \"\"\"\n",
    "    x = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "    y = x * 255.\n",
    "    y = cv2.cvtColor(y.astype('uint8'), cv2.COLOR_GRAY2RGB).astype('uint8')\n",
    "    y = cv2.applyColorMap(y, cv2.COLORMAP_JET)\n",
    "    return cv2.resize(y, (224, 224), interpolation=cv2.INTER_LANCZOS4)\n",
    "\n",
    "def evaluate(minibatches, net, epoch, img_save_folder, save_every=20):\n",
    "    stime = time.time()\n",
    "    c = Counter()\n",
    "    val_full_loss = 0.\n",
    "    val_masked_loss = 0.\n",
    "    val_loss = 0.\n",
    "    n_val_ims = 0\n",
    "\n",
    "    for i, (batch_start, batch_end) in enumerate(val_minibatches):\n",
    "        img_rgbs = val_origs[batch_start:batch_end]\n",
    "        img_labs = np.array([cvrgb2lab(img_rgb) for img_rgb in img_rgbs])\n",
    "\n",
    "        input_ = torch.from_numpy(val_ims[batch_start:batch_end])\n",
    "        gt_abs = img_labs[:, ::4, ::4, 1:]\n",
    "        target = torch.from_numpy(lookup_enc.encode_points(gt_abs))\n",
    "\n",
    "        input_query_ = val_words[batch_start:batch_end]\n",
    "        input_lengths_ = val_lengths[batch_start:batch_end]\n",
    "\n",
    "        input_query = torch.from_numpy(input_query_.astype('int32')).long().cuda()\n",
    "        input_query_lens = torch.from_numpy(input_lengths_.astype('int32')).long().cuda()\n",
    "\n",
    "        input_ims = torch.tensor(input_.float().cuda())\n",
    "        target = torch.tensor(target.long().cuda())\n",
    "    \n",
    "        output, output_maps = net(input_ims, input_query, input_query_lens)\n",
    "\n",
    "        dec_inp = F.softmax(output, dim=1)\n",
    "        AB_vals = torch.matmul(dec_inp, cuda_cc)\n",
    "        AB_vals = AB_vals.view(len(img_labs), 56, 56, 2).data.cpu().numpy()\n",
    "\n",
    "        n_val_ims += len(AB_vals)\n",
    "        \n",
    "        for k, (img_rgb, AB_val) in enumerate(zip(img_rgbs, AB_vals)):\n",
    "            AB_val = cv2.resize(AB_val, (224, 224),\n",
    "            interpolation=cv2.INTER_CUBIC)\n",
    "            img_dec = labim2rgb(np.dstack((np.expand_dims(img_labs[k, :, :, 0], axis=2), AB_val)))\n",
    "            val_loss += error_metric(img_dec, img_rgb)\n",
    "\n",
    "            if k == 0 and i%save_every == 0:\n",
    "                output_maps = torch.mean(output_maps, dim=1).data.numpy().cpu()\n",
    "                output_maps = scale_attention_map(output_maps[k])\n",
    "\n",
    "                word_list = input_query_[k, :input_lengths_[k]]\n",
    "                words = '_'.join(vrev.get(w, 'unk') for w in word_list)\n",
    "\n",
    "                img_labs_tosave = labim2rgb(img_labs[k])\n",
    "                cv2.imwrite(f'{img_save_folder}/{epoch}_{i}_bw.jpg',\n",
    "                            cv2.cvtColor(img_rgbs[k].astype('uint8'), \n",
    "                            cv2.COLOR_RGB2GRAY))\n",
    "                cv2.imwrite(f'{img_save_folder}/{epoch}_{i}_color.jpg',\n",
    "                            img_rgbs[k].astype('uint8'))\n",
    "                cv2.imwrite(f'{img_save_folder}/{epoch}_{i}_rec_{words}.jpg',\n",
    "                            img_dec.astype('uint8'))\n",
    "                cv2.imwrite(f'{img_save_folder}/{epoch}_{i}_att.jpg', output_maps)\n",
    "\n",
    "    return val_loss / len(val_minibatches) \n",
    "\n",
    "\n",
    "def minibatch_idx(n, b):\n",
    "    return [(i*b, (i+1)*b) for i in range(n//b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "LR = 0.001\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 24\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 150\n",
    "\n",
    "# init tensorboard\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# load vocab and pretrained w2v embeddings on captions\n",
    "train_vocab = pickle.load(open('priors/coco_colors_vocab.p', 'rb'))\n",
    "train_vocab_embeddings = pickle.load(open('priors/w2v_embeddings_colors.p', 'rb'), \n",
    "                                     encoding='latin1')\n",
    "\n",
    "# setting seeds to ensure reproducability\n",
    "torch.manual_seed(1337)\n",
    "random.seed(1337)\n",
    "np.random.seed(1337)\n",
    "\n",
    "\n",
    "# initialize LAB color space encoder\n",
    "lookup_enc = LookupEncode('priors/full_lab_grid_10.npy')\n",
    "num_classes = lookup_enc.cc.shape[0]\n",
    "\n",
    "# add the LAB color lookup\n",
    "cuda_cc = torch.from_numpy(lookup_enc.cc).cuda()\n",
    "\n",
    "# load images and previously extracted features\n",
    "hfile = 'coco_colors.h5'\n",
    "hf = h5.File(hfile, 'r')\n",
    "features_file = 'img_features.h5'\n",
    "ff = h5.File(features_file, 'r')\n",
    "\n",
    "# color rebalancing\n",
    "alpha = 1.\n",
    "gamma = 0.5\n",
    "gradient_prior_factor = torch.from_numpy(prior_boosting('./priors/coco_priors_onehot_625.npy', \n",
    "                                                        alpha, gamma)).float().cuda()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(weight=gradient_prior_factor)\n",
    "\n",
    "# preparing vocabulary for embedding layer - vocab 2 idx\n",
    "vrev = {v: k for k, v in train_vocab.items()}          \n",
    "n_vocab = len(train_vocab)\n",
    "\n",
    "# initialize network\n",
    "net = AutocolorizeResnet(n_vocab, train_vocab_embeddings=train_vocab_embeddings) \n",
    "net.float()  # change all parameters in network to float\n",
    "net.cuda()  # activate cuda\n",
    "\n",
    "# init optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "\n",
    "# loading images from dataset\n",
    "\n",
    "# training images\n",
    "train_origs = hf['train_ims']\n",
    "train_ims = ff['train_features']\n",
    "train_words = hf['train_words']                                        \n",
    "train_lengths = hf['train_length']\n",
    "\n",
    "# validation images\n",
    "val_origs = hf['val_ims']                            \n",
    "val_ims = ff['val_features']\n",
    "val_words = hf['val_words']                                            \n",
    "val_lengths = hf['val_length']\n",
    "\n",
    "# create minibatches\n",
    "n_train_ims = len(train_ims)\n",
    "minibatches = minibatch_idx(n_train_ims, BATCH_SIZE)[:-1]\n",
    "n_val_ims = len(val_ims)\n",
    "val_minibatches = minibatch_idx(n_val_ims, 4)[:-1]\n",
    "\n",
    "# define image folders\n",
    "val_img_save_folder = 'image_val'\n",
    "if not os.path.exists(val_img_save_folder): \n",
    "    os.makedirs(val_img_save_folder) \n",
    "\n",
    "img_save_folder = 'image_train'\n",
    "if not os.path.exists(img_save_folder):\n",
    "    os.makedirs(img_save_folder)                                    \n",
    "\n",
    "print('Training')\n",
    "print('='*50)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    random.shuffle(minibatches)\n",
    "    random.shuffle(val_minibatches)\n",
    "\n",
    "    net = train(minibatches, net, optimizer, epoch, gradient_prior_factor,\n",
    "                img_save_folder, writer)\n",
    "    t = time.time()\n",
    "    val_full_loss = evaluate(val_minibatches, net, epoch, val_img_save_folder)\n",
    "    writer.add_scalar('val/loss', val_full_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "***\n",
    "\n",
    "The authors have showed impressive results in this overview:  \n",
    "<br>\n",
    "![eval_overview](https://raw.githubusercontent.com/superhans/colorfromlanguage/master/images/Activations4.png)\n",
    "<br>\n",
    "The model was able to detect react on the exchange of color words and could change the actual color of the object that was encoded in the caption. Also the attention maps in the last *FiLM Residual Blocks* showed that the model actually is able to detect the objects successfully at the images.  \n",
    "  \n",
    "In my reimplementation, I was eager to dive into computer vision with the mere foundamentals of knowing how convolutions work. Unfortunately, I was not able to get actual results, but a pattern that something might be off in my image preprocessing, because my model mostly proposes square regions (with a bias to the color in the caption), such that the attention maps look the same througout the epochs, which is also to be seen in the recolored training images where a slight color hue is present, but always in form of a square.\n",
    "\n",
    "![comparison](./images/comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "***\n",
    "#### Computer Vision is...\n",
    "* not as simple as just throwing convolutions around and hope things turn out fine.\n",
    "* not just deep learning, but demands knowledge about image processing.\n",
    "* actually intruiging, because you can get really creative.\n",
    "* a field which is democratizing its findings and pretrained models, which is **AWESOME**\n",
    "\n",
    "#### Personal:\n",
    "* I want to further investigate my problems in this implementation and get the model to work properly to learn more.\n",
    "* For me as a novice, it looks that the regions have not been correctly proposed, because parts of the images have been colorized correctly. Looking at the attention maps also supports this thought."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project was thrilling and working in computer vision is actually super responsive, because of the ways we gain the feedback by visualizing the attention maps. In further projects I want to continue learning more about how images are processed and how neural networks and non-neural methods are applied to work with images and videos.\n",
    "\n",
    "Thanks for reading!\n",
    "– Ben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### Literature Notes\n",
    "\n",
    "* _Learning to Color from Language, Manjunatha and Iyyer et al. 2018_: https://arxiv.org/pdf/1804.06026.pdf\n",
    "* _Colorful Image Colorization, Zhang et al. 2016_: https://arxiv.org/pdf/1603.08511.pdf\n",
    "* _FiLM: Visual Reasoning with a General Conditioning Layer, Perez et al. 2017_: https://arxiv.org/pdf/1709.07871.pdf\n",
    "* _CIE 1931 Color Space, Wikipedia_: https://en.wikipedia.org/wiki/CIE_1931_color_space\n",
    "\n",
    "##### Code & Data Sources\n",
    "\n",
    "* _FiLM_: https://github.com/ethanjperez/film/blob/master/vr/models/filmed_net.py\n",
    "* _Color from Language_: https://github.com/superhans/colorfromlanguage\n",
    "\n",
    "##### Image Sources\n",
    "\n",
    "* RGB Channels: https://miro.medium.com/max/2146/1*icINeO4H7UKe3NlU1fXqlA.jpeg\n",
    "* RESNET 101: https://miro.medium.com/max/1200/1*6hF97Upuqg_LdsqWY6n_wg.png\n",
    "* CIELAB Color space: https://upload.wikimedia.org/wikipedia/commons/7/7d/CIELAB_color_space_front_view.png\n",
    "* LSTM Legend: https://www.mdpi.com/water/water-11-01387/article_deploy/html/images/water-11-01387-g004.png\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Computer, color my picture.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
