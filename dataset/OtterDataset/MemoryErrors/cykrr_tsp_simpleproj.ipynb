{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31962b4c-ce20-43e5-b4af-412b20b6a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8af147a-a740-4eb4-8053-0ba0630e1f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5000, 50, 2]), torch.Size([5000, 51]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.nn.functional import one_hot\n",
    "from eda.TSP import TSP_Instance, TSP_Environment, TSP_State\n",
    "from eda.solveTSP_v2 import solve\n",
    "\n",
    "def generate_data(n_cities=50, nb_sample=512):\n",
    "    X = []\n",
    "    Y = []\n",
    "    while len(X) < nb_sample:\n",
    "        city_points = np.random.rand(n_cities, 2)\n",
    "        inst_info = TSP_Instance(city_points)\n",
    "        solution = solve(city_points)\n",
    "\n",
    "        X.append(torch.from_numpy(city_points))\n",
    "        Y.append(torch.tensor(solution.visited))\n",
    "    return torch.stack(X).float(), torch.stack(Y).float()\n",
    "\n",
    "X, Y = generate_data(50, 5000)\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2aa5a6a-59b2-4449-b490-2504454f8e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "def myMHA(Q, K, V, nb_heads, mask=None, clip_value=None):\n",
    "    \"\"\"\n",
    "    Compute multi-head attention (MHA) given a query Q, key K, value V and attention mask :\n",
    "      h = Concat_{k=1}^nb_heads softmax(Q_k^T.K_k).V_k \n",
    "    Note : We did not use nn.MultiheadAttention to avoid re-computing all linear transformations at each call.\n",
    "    Inputs : Q of size (bsz, dim_emb, 1)                batch of queries\n",
    "             K of size (bsz, dim_emb, nb_nodes+1)       batch of keys\n",
    "             V of size (bsz, dim_emb, nb_nodes+1)       batch of values\n",
    "             mask of size (bsz, nb_nodes+1)             batch of masks of visited cities\n",
    "             clip_value is a scalar \n",
    "    Outputs : attn_output of size (bsz, 1, dim_emb)     batch of attention vectors\n",
    "              attn_weights of size (bsz, 1, nb_nodes+1) batch of attention weights\n",
    "    \"\"\"\n",
    "    bsz, nb_nodes, emd_dim = K.size() #  dim_emb must be divisable by nb_heads\n",
    "    if nb_heads>1:\n",
    "        # PyTorch view requires contiguous dimensions for correct reshaping\n",
    "        Q = Q.transpose(1,2).contiguous() # size(Q)=(bsz, dim_emb, 1)\n",
    "        Q = Q.view(bsz*nb_heads, emd_dim//nb_heads, 1) # size(Q)=(bsz*nb_heads, dim_emb//nb_heads, 1)\n",
    "        Q = Q.transpose(1,2).contiguous() # size(Q)=(bsz*nb_heads, 1, dim_emb//nb_heads)\n",
    "        K = K.transpose(1,2).contiguous() # size(K)=(bsz, dim_emb, nb_nodes+1)\n",
    "        K = K.view(bsz*nb_heads, emd_dim//nb_heads, nb_nodes) # size(K)=(bsz*nb_heads, dim_emb//nb_heads, nb_nodes+1)\n",
    "        K = K.transpose(1,2).contiguous() # size(K)=(bsz*nb_heads, nb_nodes+1, dim_emb//nb_heads)\n",
    "        V = V.transpose(1,2).contiguous() # size(V)=(bsz, dim_emb, nb_nodes+1)\n",
    "        V = V.view(bsz*nb_heads, emd_dim//nb_heads, nb_nodes) # size(V)=(bsz*nb_heads, dim_emb//nb_heads, nb_nodes+1)\n",
    "        V = V.transpose(1,2).contiguous() # size(V)=(bsz*nb_heads, nb_nodes+1, dim_emb//nb_heads)\n",
    "    attn_weights = torch.bmm(Q, K.transpose(1,2))/ Q.size(-1)**0.5 # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\n",
    "    if clip_value is not None:\n",
    "        attn_weights = clip_value * torch.tanh(attn_weights)\n",
    "    if mask is not None:\n",
    "        if nb_heads>1:\n",
    "            mask = torch.repeat_interleave(mask, repeats=nb_heads, dim=0) # size(mask)=(bsz*nb_heads, nb_nodes+1)\n",
    "        #attn_weights = attn_weights.masked_fill(mask.unsqueeze(1), float('-inf')) # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\n",
    "        attn_weights = attn_weights.masked_fill(mask.unsqueeze(1), float('-1e9')) # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\n",
    "    attn_weights = torch.softmax(attn_weights, dim=-1) # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\n",
    "    attn_output = torch.bmm(attn_weights, V) # size(attn_output)=(bsz*nb_heads, 1, dim_emb//nb_heads)\n",
    "    if nb_heads>1:\n",
    "        attn_output = attn_output.transpose(1,2).contiguous() # size(attn_output)=(bsz*nb_heads, dim_emb//nb_heads, 1)\n",
    "        attn_output = attn_output.view(bsz, emd_dim, 1) # size(attn_output)=(bsz, dim_emb, 1)\n",
    "        attn_output = attn_output.transpose(1,2).contiguous() # size(attn_output)=(bsz, 1, dim_emb)\n",
    "        attn_weights = attn_weights.view(bsz, nb_heads, 1, nb_nodes) # size(attn_weights)=(bsz, nb_heads, 1, nb_nodes+1)\n",
    "        attn_weights = attn_weights.mean(dim=1) # mean over the heads, size(attn_weights)=(bsz, 1, nb_nodes+1)\n",
    "    return attn_output, attn_weights\n",
    "    \n",
    "    \n",
    "class AutoRegressiveDecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single decoder layer based on self-attention and query-attention\n",
    "    Inputs :  \n",
    "      h_t of size      (bsz, 1, dim_emb)          batch of input queries\n",
    "      K_att of size    (bsz, nb_nodes+1, dim_emb) batch of query-attention keys\n",
    "      V_att of size    (bsz, nb_nodes+1, dim_emb) batch of query-attention values\n",
    "      mask of size     (bsz, nb_nodes+1)          batch of masks of visited cities\n",
    "    Output :  \n",
    "      h_t of size (bsz, nb_nodes+1)               batch of transformed queries\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_emb, nb_heads):\n",
    "        super(AutoRegressiveDecoderLayer, self).__init__()\n",
    "        self.dim_emb = dim_emb\n",
    "        self.nb_heads = nb_heads\n",
    "        self.Wq_selfatt = nn.Linear(dim_emb, dim_emb)\n",
    "        self.Wk_selfatt = nn.Linear(dim_emb, dim_emb)\n",
    "        self.Wv_selfatt = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W0_selfatt = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W0_att = nn.Linear(dim_emb, dim_emb)\n",
    "        self.Wq_att = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W1_MLP = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W2_MLP = nn.Linear(dim_emb, dim_emb)\n",
    "        self.BN_selfatt = nn.LayerNorm(dim_emb)\n",
    "        self.BN_att = nn.LayerNorm(dim_emb)\n",
    "        self.BN_MLP = nn.LayerNorm(dim_emb)\n",
    "        self.K_sa = None\n",
    "        self.V_sa = None\n",
    "\n",
    "    def reset_selfatt_keys_values(self):\n",
    "        self.K_sa = None\n",
    "        self.V_sa = None\n",
    "        \n",
    "    def forward(self, h_t, K_att, V_att, mask):\n",
    "        bsz = h_t.size(0)\n",
    "        h_t = h_t.view(bsz,1,self.dim_emb) # size(h_t)=(bsz, 1, dim_emb)\n",
    "        # embed the query for self-attention\n",
    "        q_sa = self.Wq_selfatt(h_t) # size(q_sa)=(bsz, 1, dim_emb)\n",
    "        k_sa = self.Wk_selfatt(h_t) # size(k_sa)=(bsz, 1, dim_emb)\n",
    "        v_sa = self.Wv_selfatt(h_t) # size(v_sa)=(bsz, 1, dim_emb)\n",
    "        # concatenate the new self-attention key and value to the previous keys and values\n",
    "        if self.K_sa is None:\n",
    "            self.K_sa = k_sa # size(self.K_sa)=(bsz, 1, dim_emb)\n",
    "            self.V_sa = v_sa # size(self.V_sa)=(bsz, 1, dim_emb)\n",
    "        else:\n",
    "            self.K_sa = torch.cat([self.K_sa, k_sa], dim=1)\n",
    "            self.V_sa = torch.cat([self.V_sa, v_sa], dim=1)\n",
    "        # compute self-attention between nodes in the partial tour\n",
    "        h_t = h_t + self.W0_selfatt( myMHA(q_sa, self.K_sa, self.V_sa, self.nb_heads)[0] ) # size(h_t)=(bsz, 1, dim_emb)\n",
    "        h_t = self.BN_selfatt(h_t.squeeze()) # size(h_t)=(bsz, dim_emb)\n",
    "        h_t = h_t.view(bsz, 1, self.dim_emb) # size(h_t)=(bsz, 1, dim_emb)\n",
    "        # compute attention between self-attention nodes and encoding nodes in the partial tour (translation process)\n",
    "        q_a = self.Wq_att(h_t) # size(q_a)=(bsz, 1, dim_emb)\n",
    "        h_t = h_t + self.W0_att( myMHA(q_a, K_att, V_att, self.nb_heads, mask)[0] ) # size(h_t)=(bsz, 1, dim_emb)\n",
    "        h_t = self.BN_att(h_t.squeeze()) # size(h_t)=(bsz, dim_emb)\n",
    "        h_t = h_t.view(bsz, 1, self.dim_emb) # size(h_t)=(bsz, 1, dim_emb)\n",
    "        # MLP\n",
    "        h_t = h_t + self.W2_MLP(torch.relu(self.W1_MLP(h_t)))\n",
    "        h_t = self.BN_MLP(h_t.squeeze(1)) # size(h_t)=(bsz, dim_emb)\n",
    "        return h_t\n",
    "        \n",
    "        \n",
    "class Transformer_decoder_net(nn.Module): \n",
    "    \"\"\"\n",
    "    Decoder network based on self-attention and query-attention transformers\n",
    "    Inputs :  \n",
    "      h_t of size      (bsz, 1, dim_emb)                            batch of input queries\n",
    "      K_att of size    (bsz, nb_nodes+1, dim_emb*nb_layers_decoder) batch of query-attention keys for all decoding layers\n",
    "      V_att of size    (bsz, nb_nodes+1, dim_emb*nb_layers_decoder) batch of query-attention values for all decoding layers\n",
    "      mask of size     (bsz, nb_nodes+1)                            batch of masks of visited cities\n",
    "    Output :  \n",
    "      prob_next_node of size (bsz, nb_nodes+1)                      batch of probabilities of next node\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_emb, nb_heads, nb_layers_decoder):\n",
    "        super(Transformer_decoder_net, self).__init__()\n",
    "        self.dim_emb = dim_emb\n",
    "        self.nb_heads = nb_heads\n",
    "        self.nb_layers_decoder = nb_layers_decoder\n",
    "        self.decoder_layers = nn.ModuleList( [AutoRegressiveDecoderLayer(dim_emb, nb_heads) for _ in range(nb_layers_decoder-1)] )\n",
    "        self.Wq_final = nn.Linear(dim_emb, dim_emb)\n",
    "        \n",
    "    # Reset to None self-attention keys and values when decoding starts \n",
    "    def reset_selfatt_keys_values(self): \n",
    "        for l in range(self.nb_layers_decoder-1):\n",
    "            self.decoder_layers[l].reset_selfatt_keys_values()\n",
    "            \n",
    "    def forward(self, h_t, K_att, V_att, mask):\n",
    "        for l in range(self.nb_layers_decoder):\n",
    "            K_att_l = K_att[:,:,l*self.dim_emb:(l+1)*self.dim_emb].contiguous()  # size(K_att_l)=(bsz, nb_nodes+1, dim_emb)\n",
    "            V_att_l = V_att[:,:,l*self.dim_emb:(l+1)*self.dim_emb].contiguous()  # size(V_att_l)=(bsz, nb_nodes+1, dim_emb)\n",
    "            if l<self.nb_layers_decoder-1: # decoder layers with multiple heads (intermediate layers)\n",
    "                h_t = self.decoder_layers[l](h_t, K_att_l, V_att_l, mask)\n",
    "            else: # decoder layers with single head (final layer)\n",
    "                q_final = self.Wq_final(h_t)\n",
    "                bsz = h_t.size(0)\n",
    "                q_final = q_final.view(bsz, 1, self.dim_emb)\n",
    "                attn_weights = myMHA(q_final, K_att_l, V_att_l, 1, mask, 10)[1] \n",
    "        prob_next_node = attn_weights.squeeze(1) \n",
    "        return prob_next_node\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_dim = 6  # Dimensión de la entrada\n",
    "        self.num_heads = 16  # Número de cabezas en la atención multi-cabeza\n",
    "        self.head_dim = 8  # Dimensión de cada cabeza\n",
    "        self.node_dim = 2\n",
    "        self.embd_dim = self.num_heads * self.head_dim\n",
    "        self.ff_dim=128\n",
    "        self.nb_layers=6\n",
    "        self.batchnorm=True\n",
    "        \n",
    "        self.MHA_layers = nn.ModuleList( [nn.MultiheadAttention(self.embd_dim, self.num_heads) for _ in range(self.nb_layers)] )\n",
    "        self.linear1_layers = nn.ModuleList( [nn.Linear(self.embd_dim, self.ff_dim) for _ in range(self.nb_layers)] )\n",
    "        self.linear2_layers = nn.ModuleList( [nn.Linear(self.ff_dim, self.embd_dim) for _ in range(self.nb_layers)] )   \n",
    "        if self.batchnorm:\n",
    "            self.norm1_layers = nn.ModuleList( [nn.BatchNorm1d(self.embd_dim) for _ in range(self.nb_layers)] )\n",
    "            self.norm2_layers = nn.ModuleList( [nn.BatchNorm1d(self.embd_dim) for _ in range(self.nb_layers)] )\n",
    "        else:\n",
    "            self.norm1_layers = nn.ModuleList( [nn.LayerNorm(self.embd_dim) for _ in range(self.nb_layers)] )\n",
    "            self.norm2_layers = nn.ModuleList( [nn.LayerNorm(self.embd_dim) for _ in range(self.nb_layers)] )\n",
    "\n",
    "        self.norm = nn.BatchNorm1d(self.embd_dim)\n",
    "    def forward(self, h):      \n",
    "        # PyTorch nn.MultiheadAttention requires input size (seq_len, bsz, dim_emb) \n",
    "        h = h.transpose(0,1) # size(h)=(nb_nodes, bsz, dim_emb)  \n",
    "        # L layers\n",
    "        for i in range(self.nb_layers):\n",
    "            h_rc = h # residual connection, size(h_rc)=(nb_nodes, bsz, dim_emb)\n",
    "            h, score = self.MHA_layers[i](h, h, h) # size(h)=(nb_nodes, bsz, dim_emb), size(score)=(bsz, nb_nodes, nb_nodes)\n",
    "            # add residual connection\n",
    "            h = h_rc + h # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            if self.batchnorm:\n",
    "                # Pytorch nn.BatchNorm1d requires input size (bsz, dim, seq_len)\n",
    "                h = h.permute(1,2,0).contiguous() # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = self.norm1_layers[i](h)       # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = h.permute(2,0,1).contiguous() # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            else:\n",
    "                h = self.norm1_layers[i](h)       # size(h)=(nb_nodes, bsz, dim_emb) \n",
    "            # feedforward\n",
    "            h_rc = h # residual connection\n",
    "            h = self.linear2_layers[i](torch.relu(self.linear1_layers[i](h)))\n",
    "            h = h_rc + h # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            if self.batchnorm:\n",
    "                h = h.permute(1,2,0).contiguous() # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = self.norm2_layers[i](h)       # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = h.permute(2,0,1).contiguous() # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            else:\n",
    "                h = self.norm2_layers[i](h) # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "        # Transpose h\n",
    "        h = h.transpose(0,1) # size(h)=(bsz, nb_nodes, dim_emb)\n",
    "        return h, score\n",
    "\n",
    "def generate_positional_encoding(d_model, max_len):\n",
    "    \"\"\"\n",
    "    Create standard transformer PEs.\n",
    "    Inputs :  \n",
    "      d_model is a scalar correspoding to the hidden dimension\n",
    "      max_len is the maximum length of the sequence\n",
    "    Output :  \n",
    "      pe of size (max_len, d_model), where d_model=dim_emb, max_len=1000\n",
    "    \"\"\"\n",
    "    pe = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "    pe[:,0::2] = torch.sin(position * div_term)\n",
    "    pe[:,1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Parámetros del modelo\n",
    "        self.input_dim = 6  # Dimensión de la entrada\n",
    "        self.num_heads = 16  # Número de cabezas en la atención multi-cabeza\n",
    "        self.head_dim = 8  # Dimensión de cada cabeza\n",
    "        self.node_dim = 2\n",
    "        self.embd_dim = self.num_heads * self.head_dim\n",
    "        self.ff_dim=256\n",
    "        self.nb_dec_layers = 2\n",
    "        self.nb_nodes=50\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Parámetros del modelo\n",
    "        self.input_dim = 6  # Dimensión de la entrada\n",
    "        self.num_heads = 16  # Número de cabezas en la atención multi-cabeza\n",
    "        self.head_dim = 8  # Dimensión de cada cabeza\n",
    "        self.node_dim = 2\n",
    "        self.embd_dim = self.num_heads * self.head_dim\n",
    "        self.ff_dim=256\n",
    "        self.nb_dec_layers = 2\n",
    "        self.nb_nodes=50\n",
    "        max_len_PE = 10000\n",
    "\n",
    "        self.input_emb = nn.Linear(self.node_dim, self.embd_dim)\n",
    "        self.ff = nn.Linear(self.embd_dim, 1)\n",
    "        self.start_placeholder = nn.Parameter(torch.randn(self.embd_dim))\n",
    "\n",
    "        self.enc = Encoder()\n",
    "        \n",
    "        self.WKatt_dec = nn.Linear(self.embd_dim, self.nb_dec_layers * self.embd_dim)\n",
    "        self.WVatt_dec = nn.Linear(self.embd_dim, self.nb_dec_layers * self.embd_dim)\n",
    "        self.PE = generate_positional_encoding(self.embd_dim, max_len_PE)     \n",
    "\n",
    "        self.dec = Transformer_decoder_net(self.embd_dim, self.num_heads, self.nb_dec_layers)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "            \n",
    "    def forward(self, x, deterministic=False):\n",
    "        # x: (bsz, nb_nodes, dim)\n",
    "        zero_to_bsz = torch.arange(x.shape[0])\n",
    "        bsz = x.shape[0]\n",
    "\n",
    "        attn_mask = None\n",
    "        h = self.input_emb(x)\n",
    "        \n",
    "        repeated_placeholder = self.start_placeholder.repeat(bsz, 1, 1)\n",
    "        h = torch.cat([h, repeated_placeholder ], dim=1)\n",
    "\n",
    "        h_enc, _ = self.enc(h)\n",
    "        tours = []\n",
    "        sumLog = []\n",
    "        Katt_dec = self.WKatt_dec(h_enc)\n",
    "        Vatt_dec = self.WVatt_dec(h_enc)\n",
    "        self.PE = self.PE.to(x.device)\n",
    "        idx_start_placeholder = torch.tensor([self.nb_nodes]).long().repeat(bsz).to(x.device)\n",
    "        h_start = h_enc[zero_to_bsz, idx_start_placeholder, :] + self.PE[0].repeat(bsz, 1) \n",
    "\n",
    "        mask_visited_nodes = torch.zeros(bsz, self.nb_nodes +1, device = x.device)\n",
    "        mask_visited_nodes[zero_to_bsz, idx_start_placeholder] = True\n",
    "        mask_visited_nodes = mask_visited_nodes.bool()\n",
    "\n",
    "        \n",
    "\n",
    "        h_t = h_start\n",
    "        for t in range(self.nb_nodes):\n",
    "            prob_next_node = self.dec(h_t, Katt_dec, Vatt_dec, mask_visited_nodes)\n",
    "            if deterministic:\n",
    "                idx = torch.argmax(prob_next_node, dim=1)\n",
    "            else:\n",
    "                idx = Categorical(prob_next_node).sample()\n",
    "            ProbOfChoices = prob_next_node[zero_to_bsz, idx]\n",
    "            sumLog.append(ProbOfChoices)\n",
    "            h_t = h_enc[zero_to_bsz, idx, :]\n",
    "            h_t = h_t + self.PE[t+1].expand(bsz, self.embd_dim)\n",
    "\n",
    "            tours.append(idx)\n",
    "\n",
    "            mask_visited_nodes = mask_visited_nodes.clone()\n",
    "            mask_visited_nodes[zero_to_bsz, idx] = True\n",
    "\n",
    "        sumLog = torch.stack(sumLog, dim=1).sum(dim=1)\n",
    "        tours = torch.stack(tours, dim=1)\n",
    "            \n",
    "        return tours, sumLog\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6132a083-03dc-4811-8ed9-94357f390010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 11.99 GiB of which 0 bytes is free. Of the allocated memory 10.53 GiB is allocated by PyTorch, and 690.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     39\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Limpia los gradientes\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(X_batch)  \u001b[38;5;66;03m# Obtenemos logits\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_function(outputs[\u001b[38;5;241m0\u001b[39m], y_batch)  \u001b[38;5;66;03m# Calcular la pérdida\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[1;32m~\\scoop\\apps\\anaconda3\\current\\App\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\scoop\\apps\\anaconda3\\current\\App\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[14], line 271\u001b[0m, in \u001b[0;36mCustomModel.forward\u001b[1;34m(self, x, deterministic)\u001b[0m\n\u001b[0;32m    268\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_emb(x)\n\u001b[0;32m    270\u001b[0m repeated_placeholder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_placeholder\u001b[38;5;241m.\u001b[39mrepeat(bsz, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 271\u001b[0m h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([h, repeated_placeholder ], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    273\u001b[0m h_enc, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc(h)\n\u001b[0;32m    274\u001b[0m tours \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 11.99 GiB of which 0 bytes is free. Of the allocated memory 10.53 GiB is allocated by PyTorch, and 690.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "model = CustomModel()\n",
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)\n",
    "X = X.to(device)\n",
    "Y= Y.to(device)\n",
    "import pandas as pd\n",
    "batch_size=512\n",
    "# Asumiendo que X_padded y Y_stacked ya están definidos y son tensores de PyTorch\n",
    "dataset = TensorDataset(X, Y)\n",
    "\n",
    "# Dividir el dataset en entrenamiento y prueba}\n",
    "train_size = int(0.5 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Definir el modelo, la función de pérdida y el optimizador\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Initialize the DataFrame to store training results\n",
    "df = pd.DataFrame(columns=[\"Model Name\", \"cities\", \"iter\", \"Epoch\",\n",
    "                         \"Training Loss\", \"Training Accuracy\",\n",
    "                         \"Validation Loss\", \"Validation Accuracy\"])\n",
    "\n",
    "\n",
    "print(\"Entrenando modelo...\")\n",
    "epochs = 10\n",
    "city_count = 50\n",
    "num_iter=1\n",
    "for epoch in range(epochs):\n",
    "  model.train()\n",
    "  train_loss = 0\n",
    "  correct = 0; total = 0\n",
    "  for X_batch, y_batch in train_loader:\n",
    "      optimizer.zero_grad()  # Limpia los gradientes\n",
    "      outputs = model(X_batch)  # Obtenemos logits\n",
    "      loss = loss_function(outputs[0], y_batch)  # Calcular la pérdida\n",
    "      loss.backward()  # Backward pass\n",
    "      optimizer.step()  # Actualizar parámetros\n",
    "      train_loss += loss.item() * X_batch.size(0)\n",
    "      _, predicted = torch.max(outputs.data, 1)\n",
    "      total += y_batch.size(0)\n",
    "      correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "  train_loss /= len(train_loader.dataset)\n",
    "  train_accuracy = 100 * correct / total\n",
    "  \n",
    "  # Validación\n",
    "  model.eval()\n",
    "  validation_loss = 0\n",
    "  correct = 0; total = 0\n",
    "  with torch.no_grad():\n",
    "      for X_batch, y_batch in test_loader:\n",
    "          outputs = model(X_batch)\n",
    "          loss = loss_function(outputs[0], y_batch)\n",
    "          validation_loss += loss.item() * X_batch.size(0)\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total += y_batch.size(0)\n",
    "          correct += (predicted == y_batch).sum().item()\n",
    "  validation_loss /= len(test_loader.dataset)\n",
    "  validation_accuracy = 100 * correct / total\n",
    "\n",
    "\n",
    "  # Log results to DataFrame\n",
    "  df = pd.concat([df, pd.DataFrame([{\n",
    "        \"Model Name\": 'CustomModel',\n",
    "        \"cities\": city_count,\n",
    "        \"iter\": num_iter,\n",
    "        \"Epoch\": epoch +1,\n",
    "        \"Training Loss\": train_loss,\n",
    "        \"Training Accuracy\": train_accuracy,\n",
    "        \"Validation Loss\": validation_loss,\n",
    "        \"Validation Accuracy\": validation_accuracy\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "  print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "  print(f'Epoch {epoch+1}, Val Loss: {validation_loss:.4f}, Val Accuracy: {validation_accuracy:.2f}%')\n",
    "\n",
    "return df;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f7aa97-9051-4444-9cc9-5b612b163505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b59a5ef-bf7c-4ff7-a683-a9a403162f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67b3b5c-1a7b-4bb2-89b4-25c9b5f6bbcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a851ef44-1121-4237-b648-5ec5745e094b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c004f-240f-49e8-b464-7503bfe28ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9319e-b069-4e9f-b8f6-70fb6162f3fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f7cca1-e9de-454a-a7b2-f4b5d618e190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7752bc-c25f-493d-a002-46305e03f9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b497735-1d02-4392-b371-2f09bcfc1ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01229299-e2d8-4ed9-8e54-6c58ec0c1821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f2ca3a-2934-479f-86d9-2e78b69fdf59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f7ff8-c88c-4bd2-b455-bdea5309f14a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218adfac-d7ea-45b2-b3bf-146a29878342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36733bcc-74a7-42b2-8ad0-bc7b0a57a6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
