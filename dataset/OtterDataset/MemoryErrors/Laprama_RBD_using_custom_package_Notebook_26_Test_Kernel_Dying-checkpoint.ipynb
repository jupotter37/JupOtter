{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14e0e248-2af5-435d-9467-11cd73dbb793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/home/ko20929/miniconda3/envs/sktime_latest/lib/python3.10/site-packages/outdated/utils.py:14: OutdatedPackageWarning: The package yasa is out of date. Your version is 0.6.3, the latest is 0.6.5.\n",
      "Set the environment variable OUTDATED_IGNORE=1 to disable these warnings.\n",
      "  return warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mne as mne\n",
    "import os \n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import constants\n",
    "from IPython.utils import io\n",
    "import time\n",
    "import sys\n",
    "import yasa\n",
    "from scipy.signal import welch\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from scipy.signal import ShortTimeFFT\n",
    "from scipy.signal.windows import gaussian\n",
    "\n",
    "#Import my modules\n",
    "import format_eeg_data\n",
    "import constants\n",
    "import eeg_stat_ts\n",
    "\n",
    "#Pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "#Import Braindecode Model EEG Conformer\n",
    "from braindecode.models import EEGConformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b88cbbf-754e-450f-9352-3fad0f9c2964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11520, 30720, 61440]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 45 seconds , 2 minutes , 4 minutes?\n",
    "[45*256 , 2*60*256 , 4*60*256 ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "131dd0cf-92c2-445e-8ceb-1efffc7300a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = ['Wake', 'N1', 'N2', 'N3', 'REM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52525549-857d-459a-8554-6ffa9db7de9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wake\n",
      "N1\n",
      "N2\n",
      "N3\n",
      "REM\n"
     ]
    }
   ],
   "source": [
    "for data_type in data_types:\n",
    "    print(data_type)\n",
    "    df_list = joblib.load(data_type + '_normalised_dataframes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0d1ee9c-8605-414d-b9fd-33f621df4623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fp1</th>\n",
       "      <th>Fpz</th>\n",
       "      <th>Fp2</th>\n",
       "      <th>F1</th>\n",
       "      <th>Fz</th>\n",
       "      <th>F2</th>\n",
       "      <th>AF7</th>\n",
       "      <th>F7</th>\n",
       "      <th>F5</th>\n",
       "      <th>F3</th>\n",
       "      <th>...</th>\n",
       "      <th>P7</th>\n",
       "      <th>PO7</th>\n",
       "      <th>P4</th>\n",
       "      <th>P6</th>\n",
       "      <th>P8</th>\n",
       "      <th>PO8</th>\n",
       "      <th>POz</th>\n",
       "      <th>O1</th>\n",
       "      <th>O2</th>\n",
       "      <th>Oz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.202487</td>\n",
       "      <td>-0.462038</td>\n",
       "      <td>-0.733921</td>\n",
       "      <td>-1.152298</td>\n",
       "      <td>-1.005832</td>\n",
       "      <td>-1.362765</td>\n",
       "      <td>-0.255209</td>\n",
       "      <td>0.138546</td>\n",
       "      <td>-0.706350</td>\n",
       "      <td>-1.375309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595723</td>\n",
       "      <td>1.449010</td>\n",
       "      <td>2.613345</td>\n",
       "      <td>1.996243</td>\n",
       "      <td>1.202629</td>\n",
       "      <td>1.079892</td>\n",
       "      <td>1.944060</td>\n",
       "      <td>1.505696</td>\n",
       "      <td>0.985024</td>\n",
       "      <td>1.602628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.321214</td>\n",
       "      <td>-0.851278</td>\n",
       "      <td>-1.197451</td>\n",
       "      <td>-1.083627</td>\n",
       "      <td>-1.224791</td>\n",
       "      <td>-1.658344</td>\n",
       "      <td>-0.410637</td>\n",
       "      <td>0.385693</td>\n",
       "      <td>-0.548234</td>\n",
       "      <td>-1.055233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.467483</td>\n",
       "      <td>1.316949</td>\n",
       "      <td>2.298550</td>\n",
       "      <td>1.658619</td>\n",
       "      <td>0.879438</td>\n",
       "      <td>0.475790</td>\n",
       "      <td>1.749582</td>\n",
       "      <td>1.161533</td>\n",
       "      <td>0.140525</td>\n",
       "      <td>1.036844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.154216</td>\n",
       "      <td>-1.533291</td>\n",
       "      <td>-1.690322</td>\n",
       "      <td>-1.367817</td>\n",
       "      <td>-1.571395</td>\n",
       "      <td>-1.993919</td>\n",
       "      <td>-0.700250</td>\n",
       "      <td>-0.033538</td>\n",
       "      <td>-1.221464</td>\n",
       "      <td>-1.544072</td>\n",
       "      <td>...</td>\n",
       "      <td>1.236026</td>\n",
       "      <td>1.628591</td>\n",
       "      <td>2.704376</td>\n",
       "      <td>2.254461</td>\n",
       "      <td>1.464690</td>\n",
       "      <td>1.415053</td>\n",
       "      <td>2.581833</td>\n",
       "      <td>1.605601</td>\n",
       "      <td>1.430553</td>\n",
       "      <td>1.931817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.025685</td>\n",
       "      <td>-0.669911</td>\n",
       "      <td>-0.701558</td>\n",
       "      <td>-1.157080</td>\n",
       "      <td>-1.361118</td>\n",
       "      <td>-1.400138</td>\n",
       "      <td>-1.279812</td>\n",
       "      <td>-0.105268</td>\n",
       "      <td>-1.233136</td>\n",
       "      <td>-1.418139</td>\n",
       "      <td>...</td>\n",
       "      <td>1.010382</td>\n",
       "      <td>1.621931</td>\n",
       "      <td>2.289233</td>\n",
       "      <td>1.877775</td>\n",
       "      <td>0.998412</td>\n",
       "      <td>1.094637</td>\n",
       "      <td>2.391346</td>\n",
       "      <td>1.234920</td>\n",
       "      <td>0.908907</td>\n",
       "      <td>1.818207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.358881</td>\n",
       "      <td>-0.509137</td>\n",
       "      <td>-1.112945</td>\n",
       "      <td>-1.066729</td>\n",
       "      <td>-1.348051</td>\n",
       "      <td>-1.290335</td>\n",
       "      <td>-0.524343</td>\n",
       "      <td>0.443623</td>\n",
       "      <td>-0.737285</td>\n",
       "      <td>-1.151816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.772697</td>\n",
       "      <td>1.171882</td>\n",
       "      <td>2.351440</td>\n",
       "      <td>2.154283</td>\n",
       "      <td>1.702516</td>\n",
       "      <td>1.761661</td>\n",
       "      <td>2.030685</td>\n",
       "      <td>0.667831</td>\n",
       "      <td>1.572043</td>\n",
       "      <td>1.590685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46075</th>\n",
       "      <td>-0.710324</td>\n",
       "      <td>-0.873905</td>\n",
       "      <td>-1.011747</td>\n",
       "      <td>-0.638937</td>\n",
       "      <td>-0.605864</td>\n",
       "      <td>-0.556492</td>\n",
       "      <td>-1.659820</td>\n",
       "      <td>-1.016613</td>\n",
       "      <td>-1.702074</td>\n",
       "      <td>-1.231307</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.335602</td>\n",
       "      <td>0.456602</td>\n",
       "      <td>1.480808</td>\n",
       "      <td>1.199734</td>\n",
       "      <td>0.399813</td>\n",
       "      <td>0.448685</td>\n",
       "      <td>1.744321</td>\n",
       "      <td>0.392405</td>\n",
       "      <td>0.292240</td>\n",
       "      <td>0.640623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46076</th>\n",
       "      <td>0.604815</td>\n",
       "      <td>-0.750069</td>\n",
       "      <td>-1.707842</td>\n",
       "      <td>-0.382144</td>\n",
       "      <td>-0.403619</td>\n",
       "      <td>-0.498851</td>\n",
       "      <td>-0.670497</td>\n",
       "      <td>-0.947983</td>\n",
       "      <td>-1.216907</td>\n",
       "      <td>-0.702741</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106406</td>\n",
       "      <td>0.765074</td>\n",
       "      <td>1.921499</td>\n",
       "      <td>1.807712</td>\n",
       "      <td>1.186509</td>\n",
       "      <td>1.229868</td>\n",
       "      <td>1.644475</td>\n",
       "      <td>0.627281</td>\n",
       "      <td>1.711914</td>\n",
       "      <td>1.043183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46077</th>\n",
       "      <td>-1.981735</td>\n",
       "      <td>-1.086110</td>\n",
       "      <td>-1.018644</td>\n",
       "      <td>-0.592439</td>\n",
       "      <td>-0.539438</td>\n",
       "      <td>-0.594167</td>\n",
       "      <td>-1.599412</td>\n",
       "      <td>-0.752566</td>\n",
       "      <td>-1.329349</td>\n",
       "      <td>-1.134326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035948</td>\n",
       "      <td>1.151379</td>\n",
       "      <td>1.998146</td>\n",
       "      <td>1.869892</td>\n",
       "      <td>1.535260</td>\n",
       "      <td>1.549119</td>\n",
       "      <td>1.650688</td>\n",
       "      <td>1.397964</td>\n",
       "      <td>1.897276</td>\n",
       "      <td>1.638019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46078</th>\n",
       "      <td>-1.276338</td>\n",
       "      <td>-1.177982</td>\n",
       "      <td>-1.006440</td>\n",
       "      <td>-0.379631</td>\n",
       "      <td>-0.461284</td>\n",
       "      <td>-0.733904</td>\n",
       "      <td>-0.668060</td>\n",
       "      <td>0.463042</td>\n",
       "      <td>0.374981</td>\n",
       "      <td>-0.045925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319112</td>\n",
       "      <td>1.078198</td>\n",
       "      <td>1.783206</td>\n",
       "      <td>1.841537</td>\n",
       "      <td>1.910974</td>\n",
       "      <td>1.693603</td>\n",
       "      <td>1.112630</td>\n",
       "      <td>1.130598</td>\n",
       "      <td>1.167284</td>\n",
       "      <td>0.964059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46079</th>\n",
       "      <td>-1.017056</td>\n",
       "      <td>-1.137782</td>\n",
       "      <td>-1.325412</td>\n",
       "      <td>-0.343579</td>\n",
       "      <td>-0.597615</td>\n",
       "      <td>-0.823426</td>\n",
       "      <td>0.082226</td>\n",
       "      <td>0.980890</td>\n",
       "      <td>0.967650</td>\n",
       "      <td>0.443138</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.122277</td>\n",
       "      <td>0.432864</td>\n",
       "      <td>1.021755</td>\n",
       "      <td>1.189405</td>\n",
       "      <td>1.412709</td>\n",
       "      <td>0.851499</td>\n",
       "      <td>0.683677</td>\n",
       "      <td>0.717950</td>\n",
       "      <td>0.880481</td>\n",
       "      <td>0.808123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46080 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Fp1       Fpz       Fp2        F1        Fz        F2       AF7  \\\n",
       "0     -0.202487 -0.462038 -0.733921 -1.152298 -1.005832 -1.362765 -0.255209   \n",
       "1     -0.321214 -0.851278 -1.197451 -1.083627 -1.224791 -1.658344 -0.410637   \n",
       "2     -1.154216 -1.533291 -1.690322 -1.367817 -1.571395 -1.993919 -0.700250   \n",
       "3     -1.025685 -0.669911 -0.701558 -1.157080 -1.361118 -1.400138 -1.279812   \n",
       "4     -0.358881 -0.509137 -1.112945 -1.066729 -1.348051 -1.290335 -0.524343   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "46075 -0.710324 -0.873905 -1.011747 -0.638937 -0.605864 -0.556492 -1.659820   \n",
       "46076  0.604815 -0.750069 -1.707842 -0.382144 -0.403619 -0.498851 -0.670497   \n",
       "46077 -1.981735 -1.086110 -1.018644 -0.592439 -0.539438 -0.594167 -1.599412   \n",
       "46078 -1.276338 -1.177982 -1.006440 -0.379631 -0.461284 -0.733904 -0.668060   \n",
       "46079 -1.017056 -1.137782 -1.325412 -0.343579 -0.597615 -0.823426  0.082226   \n",
       "\n",
       "             F7        F5        F3  ...        P7       PO7        P4  \\\n",
       "0      0.138546 -0.706350 -1.375309  ...  0.595723  1.449010  2.613345   \n",
       "1      0.385693 -0.548234 -1.055233  ...  0.467483  1.316949  2.298550   \n",
       "2     -0.033538 -1.221464 -1.544072  ...  1.236026  1.628591  2.704376   \n",
       "3     -0.105268 -1.233136 -1.418139  ...  1.010382  1.621931  2.289233   \n",
       "4      0.443623 -0.737285 -1.151816  ...  0.772697  1.171882  2.351440   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "46075 -1.016613 -1.702074 -1.231307  ... -0.335602  0.456602  1.480808   \n",
       "46076 -0.947983 -1.216907 -0.702741  ... -0.106406  0.765074  1.921499   \n",
       "46077 -0.752566 -1.329349 -1.134326  ...  0.035948  1.151379  1.998146   \n",
       "46078  0.463042  0.374981 -0.045925  ...  0.319112  1.078198  1.783206   \n",
       "46079  0.980890  0.967650  0.443138  ... -0.122277  0.432864  1.021755   \n",
       "\n",
       "             P6        P8       PO8       POz        O1        O2        Oz  \n",
       "0      1.996243  1.202629  1.079892  1.944060  1.505696  0.985024  1.602628  \n",
       "1      1.658619  0.879438  0.475790  1.749582  1.161533  0.140525  1.036844  \n",
       "2      2.254461  1.464690  1.415053  2.581833  1.605601  1.430553  1.931817  \n",
       "3      1.877775  0.998412  1.094637  2.391346  1.234920  0.908907  1.818207  \n",
       "4      2.154283  1.702516  1.761661  2.030685  0.667831  1.572043  1.590685  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "46075  1.199734  0.399813  0.448685  1.744321  0.392405  0.292240  0.640623  \n",
       "46076  1.807712  1.186509  1.229868  1.644475  0.627281  1.711914  1.043183  \n",
       "46077  1.869892  1.535260  1.549119  1.650688  1.397964  1.897276  1.638019  \n",
       "46078  1.841537  1.910974  1.693603  1.112630  1.130598  1.167284  0.964059  \n",
       "46079  1.189405  1.412709  0.851499  0.683677  0.717950  0.880481  0.808123  \n",
       "\n",
       "[46080 rows x 57 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab0cfcfc-25f0-44e6-857e-0bfa074c7ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain_regions_chans = ['Left Frontal', 'Left Frontal', 'Left Frontal', 'Left Central', 'Left Temporal', 'Left Parietal', 'Occipital', 'Occipital', 'Right Parietal', \n",
    " 'Right Temporal', 'Right Central', 'Right Frontal', 'Right Frontal', 'Right Frontal']\n",
    "\n",
    "len(brain_regions_chans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20c540cd-19a8-4582-8339-605b23137e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Left Central', 'Left Frontal', 'Left Parietal', 'Left Temporal',\n",
       "       'Occipital', 'Right Central', 'Right Frontal', 'Right Parietal',\n",
       "       'Right Temporal'], dtype='<U14')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(brain_regions_chans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76634fb8-ec4b-456b-9d0c-6d508c970024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Channels from Prefrontal, Frontal, Central or Parietal - I could add channels from these regions 2 per region = 20 channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df628d31-5f00-475b-bdf5-4c9606abf70c",
   "metadata": {},
   "source": [
    "#### Old code from notebook 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee3b6380-55b1-408d-839c-675b2a508193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N1\n",
      "Remember to change n_splits back to 4\n",
      "Commencing Training ...\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/home/ko20929/.conda/envs/sktime_latest/lib/python3.11/site-packages/braindecode/models/base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.09 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 225\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 225\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    229\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(trn_loss)\n",
      "File \u001b[0;32m~/.conda/envs/sktime_latest/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/sktime_latest/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/sktime_latest/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.09 GiB. GPU "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "#1.Load the data ________________________________________________________________________________________\n",
    "\n",
    "\n",
    "for data_type in [ 'N1']:\n",
    "    for segment_length in [30720, 61440]:\n",
    "    # for segment_length in [15360]:\n",
    "    \n",
    "        df_list = joblib.load(data_type + '_normalised_dataframes.pkl')\n",
    "        \n",
    "        print(data_type)    \n",
    "        \n",
    "        # 1. generate all path names and class list(s) etc. \n",
    "        folder = '/user/home/ko20929/work/RBD_using_custom_package/Blue_pebble/'\n",
    "        paths = joblib.load(folder + data_type + '_paths.pkl') # keys : ['selected_paths', 's_class_list', 's_night_list', 's_sleep_type', 's_p_id']\n",
    "        \n",
    "        class_label_dict = {'HC': 0 , 'PD' : 1 , 'PD+RBD' : 2 , 'RBD' : 3} #Dictionary used to label the classes for reference\n",
    "        y = np.array([class_label_dict[c_name] for c_name in paths['s_class_list'] ] )\n",
    "        groups = paths['s_p_id']\n",
    "        \n",
    "        wake_dfs_binary = []\n",
    "        y_binary = []\n",
    "        groups_binary = []\n",
    "        \n",
    "        for df , class_label , group in zip(df_list, y, groups):\n",
    "            if class_label in [0,1]:\n",
    "                wake_dfs_binary.append(df)\n",
    "                y_binary.append(class_label)\n",
    "                groups_binary.append(group)\n",
    "        \n",
    "        y_binary = np.array(y_binary)\n",
    "    \n",
    "    #2. Generate 2 second segments of the data______________________________________________________________________ \n",
    "    \n",
    "        #segment_length is user input and overlap is user input at the start of the script\n",
    "        overlap = 0.5\n",
    "        \n",
    "        signal_slices = []\n",
    "        y_slice_labels = []\n",
    "        y_slice_groups = []\n",
    "        \n",
    "        for df, label, group in zip(wake_dfs_binary[:5], y_binary[:5], groups_binary[:5]):\n",
    "            \n",
    "            num_segments = int( np.floor(len(df)/segment_length) )\n",
    "            new_specs = []\n",
    "            \n",
    "            for i in np.arange(0,num_segments, 1 - overlap):\n",
    "                if i > num_segments - 1 :\n",
    "                    # I don't want it to try to take an incomplete slice\n",
    "                    # will be an incomplete slice causing errors downstream, needs to stop \n",
    "                    \n",
    "                    break\n",
    "                  \n",
    "                start_index = int( np.floor(i*segment_length) )\n",
    "                end_index = start_index + segment_length\n",
    "                \n",
    "                slice_df = df.iloc[start_index : end_index, :].copy()        \n",
    "                signal_slices.append(slice_df)\n",
    "                \n",
    "                y_slice_labels.append(label)\n",
    "                y_slice_groups.append(group)\n",
    "                \n",
    "        plt.hist(y_slice_labels)\n",
    "    \n",
    "    #3. Select the correct channels ______________________________________________________________________________\n",
    "        \n",
    "        channels_selected = eeg_14_channels # will later move this to the top of the notebook\n",
    "        \n",
    "        # signal_slices, y_slice_labels, y_slice_groups\n",
    "        signal_slices_14_channels = [ df.loc[:, channels_selected].copy() for df in signal_slices ]\n",
    "        \n",
    "        # Make the slices an np array of the correct dimensions\n",
    "        signal_slices_14_channels_np = [df.T.values for df in signal_slices_14_channels]\n",
    "    \n",
    "    #4. Create the network from the paper (REMOVED as I'm using EEG Conformer) \n",
    "    #The model is defined inside the training loop\n",
    "    \n",
    "    \n",
    "    #5. Do Train Validation Splits\n",
    "        # Train and Validation splits only ----> NO TEST\n",
    "        # spectrogram_slices, y_slice_labels and y_slice_groups to work with\n",
    "        train_val_dict = {}\n",
    "        \n",
    "        for value in ['train' , 'val']:\n",
    "            train_val_dict[value] = {}\n",
    "        \n",
    "        X = np.stack(signal_slices_14_channels_np)\n",
    "        y = np.array(y_slice_labels)\n",
    "        groups = np.array( [int(group) for group in y_slice_groups] )\n",
    "        \n",
    "        gkf = GroupKFold(n_splits = 2)\n",
    "        \n",
    "        print('Remember to change n_splits back to 4')\n",
    "        \n",
    "        fold = 0\n",
    "        \n",
    "        for train_index, val_index   in gkf.split(X, y, groups*1):\n",
    "            fold += 1\n",
    "            \n",
    "            X_train, y_train, groups_train  = X[train_index], y[train_index] , groups[train_index]\n",
    "            X_val, y_val, groups_val =  X[val_index], y[val_index] , groups[val_index]   \n",
    "            \n",
    "            train_val_dict['train'][fold] = X_train, y_train, groups_train\n",
    "            train_val_dict['val'][fold]   = X_val, y_val, groups_val\n",
    "            \n",
    "            total_len = len(X) \n",
    "            val_percent = 100*(len(X_val) / total_len)\n",
    "            train_percent = 100*(len(X_train) / total_len)\n",
    "            \n",
    "            # Commented out the printing here\n",
    "            # print('fold ' + str(fold) ) \n",
    "            # print( str(train_percent)[:3] + ' | '  + str(val_percent)[:3] + ' |' )\n",
    "        \n",
    "            # # testing that the splits are as expected\n",
    "            # print( np.unique(groups_train) )\n",
    "            # print( np.unique(groups_val) )\n",
    "            \n",
    "            # print('__________________________________________________________________________')\n",
    "        \n",
    "        #Output from this section of code is X_train, y_train, groups_train AND X_test, y_test, groups_test \n",
    "        import time\n",
    "    \n",
    "        start_time = time.time()\n",
    "        \n",
    "        seeds = [2]\n",
    "        rows = len(seeds) # Make the figure the right size \n",
    "        \n",
    "        fig = plt.figure()\n",
    "        fig = plt.figure(figsize=(24,4*rows),dpi=100)\n",
    "        \n",
    "        # k is for subplots within the overall figure \n",
    "        \n",
    "        k = 1\n",
    "        print('Commencing Training ...')\n",
    "        # Test out for 1 fold to start with\n",
    "        \n",
    "        for fold in [1,2,3,4]:\n",
    "            print(fold)\n",
    "            X_train, y_train, groups_train = train_val_dict['train'][fold]\n",
    "            X_val, y_val, groups_val = train_val_dict['val'][fold]  \n",
    "            \n",
    "            # Creating train and test data loaders\n",
    "            train_data = [ (torch.from_numpy(input_slice).float().view(num_channels,segment_length), val) for input_slice, val in zip(X_train, y_train) ] \n",
    "            train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "            \n",
    "            val_data = [ (torch.from_numpy(input_slice).float().view(num_channels,segment_length), val) for input_slice, val in zip(X_val, y_val) ] \n",
    "            val_loader = DataLoader(val_data , batch_size=16, shuffle=False)\n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "            \n",
    "            # for seed in [2,5,15,50]:\n",
    "            for seed in seeds:\n",
    "                # set all seeds \n",
    "                random.seed(seed)\n",
    "                np.random.seed(seed)\n",
    "                torch.manual_seed(seed)\n",
    "                torch.cuda.manual_seed_all(seed) \n",
    "                \n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                # print(device) - to check that device is actually cuda\n",
    "                \n",
    "                model = EEGConformer(\n",
    "                    n_chans = num_channels,\n",
    "                    n_outputs = 2 ,\n",
    "                    n_times=segment_length,\n",
    "                    att_depth=3,\n",
    "                    att_heads=5,\n",
    "                    final_fc_length = 'auto' \n",
    "                    )\n",
    "                \n",
    "                model.to(device)\n",
    "                \n",
    "                criterion = torch.nn.NLLLoss()\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr = 0.001 )\n",
    "    \n",
    "                \n",
    "                epochs = 35\n",
    "                train_losses = []\n",
    "                val_losses = []\n",
    "                test_losses = []\n",
    "                \n",
    "                train_correct = []\n",
    "                val_correct = []\n",
    "                test_correct = []\n",
    "                \n",
    "                for i in range(epochs):\n",
    "                    \n",
    "                    trn_corr = 0\n",
    "                    val_corr = 0\n",
    "                    tst_corr = 0\n",
    "                     \n",
    "                    \n",
    "                    trn_loss = 0\n",
    "                    val_loss = 0\n",
    "                    tst_loss = 0\n",
    "                    \n",
    "                    model.train()\n",
    "                    # Run the training batches\n",
    "                    for b, (X_train_batch, y_train_batch) in enumerate(train_loader):\n",
    "                        # b+=1\n",
    "                \n",
    "                        #Move train data to the GPU\n",
    "                        X_train_batch = X_train_batch.to(device)\n",
    "                        y_train_batch = y_train_batch.to(device)\n",
    "                        \n",
    "                        # Apply the model\n",
    "                        print(b)\n",
    "                        y_pred = model(X_train_batch)  # we don't flatten X-train here\n",
    "                        loss = criterion(y_pred, y_train_batch)\n",
    "                 \n",
    "                        # Tally the number of correct predictions\n",
    "                        predicted = torch.argmax(torch.exp( y_pred.detach() ) ,  dim = 1 ) \n",
    "        \n",
    "                        predicted = predicted.reshape(y_train_batch.shape)\n",
    "                        \n",
    "                        batch_corr = (predicted == y_train_batch).sum()\n",
    "                        trn_corr += batch_corr\n",
    "                        trn_loss += loss.item()\n",
    "                        \n",
    "                        # Update parameters\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        \n",
    "                    train_losses.append(trn_loss)\n",
    "                    train_correct.append(trn_corr)\n",
    "                \n",
    "                    # Run the validation batches\n",
    "                    # Some of the variables in this loop have the same name as the variables in the above loop... be aware of that plz!\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        for b, (X_val_batch, y_val_batch) in enumerate(val_loader):\n",
    "                            # b+=1\n",
    "                            \n",
    "                            #Move train data to the GPU\n",
    "                            X_val_batch = X_val_batch.to(device)\n",
    "                            y_val_batch = y_val_batch.to(device)\n",
    "                \n",
    "                            # Apply the model\n",
    "                            y_val = model(X_val_batch)\n",
    "                \n",
    "                            # Tally the number of correct predictions\n",
    "                            predicted = torch.argmax(y_val.detach(),  dim = 1 ) \n",
    "                            predicted = predicted.reshape(y_val_batch.shape)\n",
    "                            \n",
    "                            batch_corr = (predicted == y_val_batch).sum()\n",
    "                            val_corr += batch_corr\n",
    "                \n",
    "                            loss = criterion(y_val, y_val_batch)\n",
    "                            val_loss += loss.item()\n",
    "                           \n",
    "                    val_losses.append(val_loss)\n",
    "                    val_correct.append(val_corr)\n",
    "                \n",
    "                    \n",
    "                   \n",
    "                \n",
    "                # Plot the outcome from the loop\n",
    "                \n",
    "                ax = fig.add_subplot(rows,4,k)\n",
    "                k+=1\n",
    "                plt.title('fold ' + str(fold), fontsize = 10)\n",
    "                plt.plot([(val.cpu() / len(X_train) ) for val in train_correct], label='training set accuracy')\n",
    "                plt.plot([(val.cpu()/len(X_val) ) for val in val_correct], label='validation set accuracy')\n",
    "                plt.ylabel('accuracy')\n",
    "                plt.xlabel('epochs') \n",
    "                plt.grid()\n",
    "            \n",
    "            \n",
    "            plt.tight_layout()\n",
    "        \n",
    "        \n",
    "        plt.legend()   \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        #Add text at the bottom of the figure\n",
    "        # fig.text(0.5, 0, 'This is a caption at the bottom of the figure | Model : ' + str(model) , va='bottom')\n",
    "        fig.text(0.5, 0, f'\\nDuration: {time.time() - start_time:.0f} seconds' , ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout(pad = 2.0)\n",
    "    \n",
    "        time_segment = str(segment_length/256)\n",
    "        channels_num = str(num_channels)\n",
    "        \n",
    "        save_name = 'Results/Folder_3/' + data_type + '_' + 'Conformer_window_' + time_segment + '_secs_' + channels_num + '_channels' \n",
    "        plt.savefig(save_name +'.png')\n",
    "            \n",
    "        \n",
    "        print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed  \n",
    "        print('Results Saved, on to next data type ...')\n",
    "        \n",
    "t2 = time.time()\n",
    "\n",
    "t2 - t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a99f8bd-c838-4f4e-b688-385ebff1a5c6",
   "metadata": {},
   "source": [
    "Wake 45 seconds took 260 seconds to run <br> \n",
    "Then it got an out of memory error --> OutOfMemoryError: CUDA out of memory. Tried to allocate 1.55 GiB. GPU \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
