{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc95442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo -H pip install -q transformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7c4a3ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autoawq==0.1.7+cu118\n",
      "\u001b[?25l  Downloading https://github.com/casper-hansen/AutoAWQ/releases/download/v0.1.7/autoawq-0.1.7+cu118-cp38-cp38-linux_x86_64.whl (20.0MB)\n",
      "\u001b[K     |████████████████████████████████| 20.0MB 6.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: accelerate in /opt/conda/lib/python3.8/site-packages (from autoawq==0.1.7+cu118) (0.21.0)\n",
      "Collecting toml\n",
      "  Downloading https://files.pythonhosted.org/packages/44/6f/7120676b6d73228c96e17f1f794d8ab046fc910d781c8d151120c3f1569e/toml-0.10.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.8/site-packages (from autoawq==0.1.7+cu118) (0.15.2)\n",
      "Requirement already satisfied: tokenizers>=0.12.1 in /opt/conda/lib/python3.8/site-packages (from autoawq==0.1.7+cu118) (0.15.0)\n",
      "Requirement already satisfied: torch>=2.0.1 in /opt/conda/lib/python3.8/site-packages (from autoawq==0.1.7+cu118) (2.0.1)\n",
      "Collecting tabulate\n",
      "  Downloading https://files.pythonhosted.org/packages/40/44/4a5f08c96eb108af5cb50b41f76142f0afa346dfa99d5296fe7202a11854/tabulate-0.9.0-py3-none-any.whl\n",
      "Collecting texttable\n",
      "  Downloading https://files.pythonhosted.org/packages/24/99/4772b8e00a136f3e01236de33b0efda31ee7077203ba5967fcc76da94d65/texttable-1.7.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.8/site-packages (from autoawq==0.1.7+cu118) (0.1.99)\n",
      "Collecting lm-eval\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/c5/bff92e6b61fc2b0c1b7ac769633731910152e5176a404912ce7c07329ba0/lm_eval-0.3.0-py3-none-any.whl (178kB)\n",
      "\u001b[K     |████████████████████████████████| 184kB 18.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting attributedict\n",
      "  Downloading https://files.pythonhosted.org/packages/de/db/337b36e8d85a07293f5a792644fa972b53a52b13587f484c627009206697/attributedict-0.3.0-py3-none-any.whl\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.8/site-packages (from autoawq==0.1.7+cu118) (4.24.0)\n",
      "Requirement already satisfied: transformers>=4.35.0 in /opt/conda/lib/python3.8/site-packages (from autoawq==0.1.7+cu118) (4.36.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from accelerate->autoawq==0.1.7+cu118) (5.9.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from accelerate->autoawq==0.1.7+cu118) (1.21.1)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from accelerate->autoawq==0.1.7+cu118) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from accelerate->autoawq==0.1.7+cu118) (23.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from torchvision->autoawq==0.1.7+cu118) (2.29.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.8/site-packages (from torchvision->autoawq==0.1.7+cu118) (10.0.0)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /opt/conda/lib/python3.8/site-packages (from tokenizers>=0.12.1->autoawq==0.1.7+cu118) (0.19.4)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (11.7.99)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (3.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (11.7.101)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (4.1.1)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (2.0.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (11.10.3.66)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (1.12)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from torch>=2.0.1->autoawq==0.1.7+cu118) (3.1.2)\n",
      "Collecting sqlitedict\n",
      "  Downloading https://files.pythonhosted.org/packages/12/9a/7620d1e9dcb02839ed6d4b14064e609cdd7a8ae1e47289aa0456796dd9ca/sqlitedict-2.1.0.tar.gz\n",
      "Requirement already satisfied: scikit-learn>=0.24.1 in /opt/conda/lib/python3.8/site-packages (from lm-eval->autoawq==0.1.7+cu118) (1.3.0)\n",
      "Collecting numexpr\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/47/a2ede0e136a8ddc288b447c260aa035f3e75251f808aa61f6454b16dfd04/numexpr-2.8.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (384kB)\n",
      "\u001b[K     |████████████████████████████████| 389kB 108.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacrebleu==1.5.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/7f/4fd83db8570288c3899d8e57666c2841403c15659f3d792a3cb8dc1c6689/sacrebleu-1.5.0-py3-none-any.whl (65kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 41.1MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm-multiprocess\n",
      "  Downloading https://files.pythonhosted.org/packages/25/7e/0d889fc6c84e3df6b69aaafe893fc77f69b3d968ac9ce574d1c62c688050/tqdm_multiprocess-0.0.11-py3-none-any.whl\n",
      "Collecting pytablewriter\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/74/b39b823ee7dba155b117634e62733a0dfdfe5aa100a553b435062cee2062/pytablewriter-1.2.0-py3-none-any.whl (111kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 119.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zstandard in /opt/conda/lib/python3.8/site-packages (from lm-eval->autoawq==0.1.7+cu118) (0.19.0)\n",
      "Collecting jsonlines\n",
      "  Downloading https://files.pythonhosted.org/packages/f8/62/d9ba6323b9202dd2fe166beab8a86d29465c41a0288cbe229fac60c1ab8d/jsonlines-4.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from lm-eval->autoawq==0.1.7+cu118) (2.14.4)\n",
      "Collecting pycountry\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/12/fdbcd29b5a243af2f1c1a83636a21e3837aeaa070c9212ebe657e39ce563/pycountry-23.12.11-py3-none-any.whl (6.2MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |█████████████████▊              | 3.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 3.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 3.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 3.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 3.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 3.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 3.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▏             | 3.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▏             | 3.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 3.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 3.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▍             | 3.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▍             | 3.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▌             | 3.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▌             | 3.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 3.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 3.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▊             | 3.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▊             | 3.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▊             | 3.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▉             | 3.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▉             | 3.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 3.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 3.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 3.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 3.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▏            | 3.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▏            | 3.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 3.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 3.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▍            | 3.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▍            | 3.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 3.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 3.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 3.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 3.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▊            | 3.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▊            | 3.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▊            | 3.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 3.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 3.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 3.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 3.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 3.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 3.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 3.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 3.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 3.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 3.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 4.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 4.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▌           | 4.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▌           | 4.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 4.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 4.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 4.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 4.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 4.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 4.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 4.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 4.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 4.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 4.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 4.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 4.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 4.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 4.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 4.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▍          | 4.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▍          | 4.2MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▌          | 4.2MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▌          | 4.2MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 4.2MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 4.2MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 4.2MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 4.2MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 4.2MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 4.2MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 4.2MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 4.3MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 4.3MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 4.3MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 4.3MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 4.3MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 4.3MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 4.3MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 4.3MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▍         | 4.3MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▍         | 4.4MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▌         | 4.4MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▌         | 4.4MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 4.4MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 4.4MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 4.4MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 4.4MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▉         | 4.4MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▉         | 4.4MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▉         | 4.4MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 4.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 4.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 4.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 4.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 4.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 4.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▎        | 4.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▎        | 4.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 4.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 4.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 4.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 4.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 4.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 4.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▊        | 4.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▊        | 4.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 4.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 4.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 4.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 4.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 4.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 4.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 4.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 4.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 4.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 4.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 4.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 4.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 4.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▌       | 4.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▌       | 4.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▋       | 4.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▋       | 4.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 4.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 4.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 4.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 4.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 4.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 4.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 4.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 4.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 4.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 4.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 4.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▎      | 4.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▎      | 4.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 4.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 4.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 4.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 5.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 5.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 5.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▊      | 5.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▊      | 5.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 5.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 5.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 5.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 5.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 5.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 5.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 5.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 5.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 5.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▎     | 5.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▎     | 5.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▍     | 5.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▍     | 5.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 5.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 5.2MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▋     | 5.2MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▋     | 5.2MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▊     | 5.2MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▊     | 5.2MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 5.2MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 5.2MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 5.2MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 5.2MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 5.2MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 5.3MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 5.3MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▏    | 5.3MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▏    | 5.3MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 5.3MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 5.3MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 5.3MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 5.3MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 5.3MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 5.3MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▋    | 5.4MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▋    | 5.4MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 5.4MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 5.4MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 5.4MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 5.4MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 5.4MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 5.4MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 5.4MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 5.4MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 5.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 5.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 5.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▎   | 5.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▎   | 5.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 5.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 5.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 5.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 5.5MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 5.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 5.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▊   | 5.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▊   | 5.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 5.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 5.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 5.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 5.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 5.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 5.6MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 5.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 5.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 5.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 5.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 5.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▍  | 5.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▍  | 5.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 5.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 5.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 5.7MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 5.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 5.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 5.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▉  | 5.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▉  | 5.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 5.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 5.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 5.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 5.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 5.8MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▏ | 5.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▏ | 5.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 5.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 5.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▍ | 5.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▍ | 5.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 5.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 5.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 5.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 5.9MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 6.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 6.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▉ | 6.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▉ | 6.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 6.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 6.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 6.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 6.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 6.0MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 6.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 6.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▎| 6.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▎| 6.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▍| 6.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▍| 6.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 6.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 6.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 6.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 6.1MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▊| 6.2MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▊| 6.2MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 6.2MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 6.2MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 6.2MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 6.2MB 99.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 6.2MB 99.9MB/s \r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25hCollecting rouge-score>=0.0.4\n",
      "  Downloading https://files.pythonhosted.org/packages/e2/c5/9136736c37022a6ad27fea38f3111eb8f02fe75d067f9a985cc358653102/rouge_score-0.1.2.tar.gz\n",
      "Collecting openai>=0.6.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/db/057823642344c26a8cbc54dabe2e9f3a1047a3fb0df75c8e3a80e35191b7/openai-1.3.9-py3-none-any.whl (221kB)\n",
      "\u001b[K     |████████████████████████████████| 225kB 105.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pybind11>=2.6.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/55/9f73c32dda93fa4f539fafa268f9504e83c489f460c380371d94296126cd/pybind11-2.11.1-py3-none-any.whl (227kB)\n",
      "\u001b[K     |████████████████████████████████| 235kB 114.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tox>=3.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/88/de28a027acdb3a1b070a8acdff62bd23fdc23ce32acc7ac4b92b088979a4/tox-4.11.4-py3-none-any.whl (153kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 114.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting colour-runner>=0.0.5\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/82/ce3250026add1910739dcabc796571ad1d182cb47332716c8bb96ee5d624/colour_runner-0.1.1-py2.py3-none-any.whl\n",
      "Collecting deepdiff>=3.3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/8f/a9d39ec15f40e8169cb134317824ee4618b864b2e4b91a9b310d3ef94729/deepdiff-6.7.1-py3-none-any.whl (76kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 44.4MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting rootpath>=0.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/f9/959835686c78b7a95d8d806a97fa0be020c2deccb96de2b60659744319b9/rootpath-0.1.1-py3-none-any.whl\n",
      "Collecting codecov>=2.0.15\n",
      "  Downloading https://files.pythonhosted.org/packages/af/02/18785edcdf6266cdd6c6dc7635f1cbeefd9a5b4c3bb8aff8bd681e9dd095/codecov-2.1.13-py2.py3-none-any.whl\n",
      "Collecting coverage>=4.5.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/19/432d0e438546b17fe20bed3a26f447ba7bfb24fc3414b61b6f545994a9dc/coverage-7.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (230kB)\n",
      "\u001b[K     |████████████████████████████████| 235kB 112.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting inspecta>=0.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/03/aa/5ad8e223fa564d474b465771710b8b7b23896b59651cf115f510bcfda3ee/inspecta-0.1.3-py3-none-any.whl\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.8/site-packages (from transformers>=4.35.0->autoawq==0.1.7+cu118) (0.3.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers>=4.35.0->autoawq==0.1.7+cu118) (2023.8.8)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers>=4.35.0->autoawq==0.1.7+cu118) (4.65.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->autoawq==0.1.7+cu118) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->autoawq==0.1.7+cu118) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->autoawq==0.1.7+cu118) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->autoawq==0.1.7+cu118) (1.26.15)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.8/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.12.1->autoawq==0.1.7+cu118) (2023.6.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.8/site-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=2.0.1->autoawq==0.1.7+cu118) (0.38.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from nvidia-cuda-runtime-cu11==11.7.99; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=2.0.1->autoawq==0.1.7+cu118) (67.8.0)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.8/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=2.0.1->autoawq==0.1.7+cu118) (16.0.6)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.8/site-packages (from triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\"->torch>=2.0.1->autoawq==0.1.7+cu118) (3.27.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.8/site-packages (from sympy->torch>=2.0.1->autoawq==0.1.7+cu118) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->torch>=2.0.1->autoawq==0.1.7+cu118) (2.1.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.24.1->lm-eval->autoawq==0.1.7+cu118) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.24.1->lm-eval->autoawq==0.1.7+cu118) (1.3.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.24.1->lm-eval->autoawq==0.1.7+cu118) (1.10.1)\n",
      "Collecting portalocker\n",
      "  Downloading https://files.pythonhosted.org/packages/17/9e/87671efcca80ba6203811540ed1f9c0462c1609d2281d7b7f53cef05da3d/portalocker-2.8.2-py3-none-any.whl\n",
      "Collecting colorama\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/d6/3965ed04c63042e047cb6a3e6ed1a63a35087b6a609aa3a15ed8ac56c221/colorama-0.4.6-py2.py3-none-any.whl\n",
      "Collecting mbstrdecoder<2,>=1.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/c2/0f/726229136022b154895138bb10ba35e8435c4143f614cb5ad4d4e3fc21ec/mbstrdecoder-1.1.3-py3-none-any.whl\n",
      "Collecting pathvalidate<4,>=2.3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/0c/ab/673cce13ab635fd755d206b18c0a371ef6e28ddbe25fadba9ae6c59f22a5/pathvalidate-3.2.0-py3-none-any.whl\n",
      "Collecting tcolorpy<1,>=0.0.5\n",
      "  Downloading https://files.pythonhosted.org/packages/34/d0/8a701df46bf546fd155da02934b0bb2ac6ad4186e29f4a3297e744ab259d/tcolorpy-0.1.4-py3-none-any.whl\n",
      "Collecting typepy[datetime]<2,>=1.3.2\n",
      "  Downloading https://files.pythonhosted.org/packages/f1/10/0d6dc654bb4e0eca017bbaf43a315b464c888576a68a2883cd4a74bd1b6b/typepy-1.3.2-py3-none-any.whl\n",
      "Collecting DataProperty<2,>=1.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/b1/3b/90ebd66ad57c588d6087e86e327436343e9cc60776a9445b79c6e80a022d/DataProperty-1.0.1-py3-none-any.whl\n",
      "Collecting tabledata<2,>=1.3.1\n",
      "  Downloading https://files.pythonhosted.org/packages/06/e2/96b10ebc00d20b55967200e3d95c2137d91f58af1af672627683431c9d5c/tabledata-1.3.3-py3-none-any.whl\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.8/site-packages (from jsonlines->lm-eval->autoawq==0.1.7+cu118) (23.1.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (0.70.15)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (0.3.7)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (3.8.5)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (3.3.0)\n",
      "Requirement already satisfied: importlib-resources<6.0.0,>=5.12.0; python_version < \"3.9\" in /opt/conda/lib/python3.8/site-packages (from pycountry->lm-eval->autoawq==0.1.7+cu118) (5.12.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.8/site-packages (from rouge-score>=0.0.4->lm-eval->autoawq==0.1.7+cu118) (1.4.0)\n",
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/0a/0d20d2c0f16be91b9fa32a77b76c60f9baf6eba419e5ef5deca17af9c582/nltk-3.8.1-py3-none-any.whl (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 103.3MB/s eta 0:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.8/site-packages (from rouge-score>=0.0.4->lm-eval->autoawq==0.1.7+cu118) (1.16.0)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.8/site-packages (from openai>=0.6.4->lm-eval->autoawq==0.1.7+cu118) (1.3.0)\n",
      "Collecting pydantic<3,>=1.9.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/2b/64066de1c4cf3d4ed623beeb3bbf3f8d0cc26661f1e7d180ec5eb66b75a5/pydantic-2.5.2-py3-none-any.whl (381kB)\n",
      "\u001b[K     |████████████████████████████████| 389kB 108.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting distro<2,>=1.7.0\n",
      "  Downloading https://files.pythonhosted.org/packages/f4/2c/c90a3adaf0ddb70afe193f5ebfb539612af57cffe677c3126be533df3098/distro-1.8.0-py3-none-any.whl\n",
      "Collecting httpx<1,>=0.23.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/65/6940eeb21dcb2953778a6895281c179efd9100463ff08cb6232bb6480da7/httpx-0.25.2-py3-none-any.whl (74kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 43.3MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.8/site-packages (from openai>=0.6.4->lm-eval->autoawq==0.1.7+cu118) (3.7.0)\n",
      "Collecting platformdirs>=3.10\n",
      "  Downloading https://files.pythonhosted.org/packages/be/53/42fe5eab4a09d251a76d0043e018172db324a23fcdac70f77a551c11f618/platformdirs-4.1.0-py3-none-any.whl\n",
      "Collecting pyproject-api>=1.6.1\n",
      "  Downloading https://files.pythonhosted.org/packages/cf/b4/39eea50542e50e93876ebc09c4349a9c9eee9f6b9c9d30f88c7dc5433db8/pyproject_api-1.6.1-py3-none-any.whl\n",
      "Collecting tomli>=2.0.1; python_version < \"3.11\"\n",
      "  Downloading https://files.pythonhosted.org/packages/97/75/10a9ebee3fd790d20926a90a2547f0bf78f371b2f13aa822c759680ca7b9/tomli-2.0.1-py3-none-any.whl\n",
      "Requirement already satisfied: cachetools>=5.3.1 in /opt/conda/lib/python3.8/site-packages (from tox>=3.0.0->attributedict->autoawq==0.1.7+cu118) (5.3.1)\n",
      "Collecting pluggy>=1.3\n",
      "  Downloading https://files.pythonhosted.org/packages/05/b8/42ed91898d4784546c5f06c60506400548db3f7a4b3fb441cba4e5c17952/pluggy-1.3.0-py3-none-any.whl\n",
      "Collecting virtualenv>=20.24.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/22/54b1180756d2d6194bcafb7425d437c3034c4bff92129c3e1e633079e2c4/virtualenv-20.25.0-py3-none-any.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 101.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting chardet>=5.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/6f/f5fbc992a329ee4e0f288c1fe0e2ad9485ed064cac731ed2fe47dcc38cbf/chardet-5.2.0-py3-none-any.whl (199kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 113.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from colour-runner>=0.0.5->attributedict->autoawq==0.1.7+cu118) (2.15.1)\n",
      "Collecting blessings\n",
      "  Downloading https://files.pythonhosted.org/packages/03/74/489f85a78247609c6b4f13733cbf3ba0d864b11aa565617b645d6fdf2a4a/blessings-1.7-py3-none-any.whl\n",
      "Collecting ordered-set<4.2.0,>=4.0.2\n",
      "  Downloading https://files.pythonhosted.org/packages/33/55/af02708f230eb77084a299d7b08175cff006dea4f2721074b92cdb0296c0/ordered_set-4.1.0-py3-none-any.whl\n",
      "Collecting coloredlogs>=10.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/06/3d6badcf13db419e25b07041d9c7b4a2c331d3f4e7134445ec5df57714cd/coloredlogs-15.0.1-py2.py3-none-any.whl (46kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 29.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/5f/8c716e47b3a50cbd7c146f45881e11d9414def768b7cd9c5e6650ec2a80a/termcolor-2.4.0-py3-none-any.whl\n",
      "Requirement already satisfied: pytz>=2018.9; extra == \"datetime\" in /opt/conda/lib/python3.8/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval->autoawq==0.1.7+cu118) (2023.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0; extra == \"datetime\" in /opt/conda/lib/python3.8/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval->autoawq==0.1.7+cu118) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (2023.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (1.9.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets>=2.0.0->lm-eval->autoawq==0.1.7+cu118) (1.3.1)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /opt/conda/lib/python3.8/site-packages (from importlib-resources<6.0.0,>=5.12.0; python_version < \"3.9\"->pycountry->lm-eval->autoawq==0.1.7+cu118) (3.15.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk->rouge-score>=0.0.4->lm-eval->autoawq==0.1.7+cu118) (8.1.7)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading https://files.pythonhosted.org/packages/28/78/d31230046e58c207284c6b2c4e8d96e6d3cb4e52354721b944d3e1ee4aa5/annotated_types-0.6.0-py3-none-any.whl\n",
      "Collecting pydantic-core==2.14.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/95/d1104b88d5e3ad42db30935a4c258da2385139dd216ec8dfbc347a32dbff/pydantic_core-2.14.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1MB 92.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting httpcore==1.*\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/ba/78b0a99c4da0ff8b0f59defa2f13ca4668189b134bd9840b6202a93d9a0f/httpcore-1.0.2-py3-none-any.whl (76kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 35.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: exceptiongroup; python_version < \"3.11\" in /opt/conda/lib/python3.8/site-packages (from anyio<5,>=3.5.0->openai>=0.6.4->lm-eval->autoawq==0.1.7+cu118) (1.1.1)\n",
      "Collecting distlib<1,>=0.3.7\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/41/9307e4f5f9976bc8b7fea0b66367734e8faf3ec84bc0d412d8cfabbb66cd/distlib-0.3.8-py2.py3-none-any.whl (468kB)\n",
      "\u001b[K     |████████████████████████████████| 471kB 101.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting humanfriendly>=9.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/0f/310fb31e39e2d734ccaa2c0fb981ee41f7bd5056ce9bc29b2248bd569169/humanfriendly-10.0-py2.py3-none-any.whl (86kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 41.3MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting h11<0.15,>=0.13\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 36.7MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: sqlitedict, rouge-score\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-cp38-none-any.whl size=16865 sha256=81ca72d189c6320264a96f9ceb96c6ad8f95ab52c2a693ffa86a00ddfaa35fa5\n",
      "  Stored in directory: /root/.cache/pip/wheels/2e/10/02/9725042cf8a987b45e38bcfb106f4b0cdcc9d8968e248b93e1\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-cp38-none-any.whl size=24937 sha256=089c1bc9f5103d2560bcf42ae1a8c07345af85d04a4b60378824553c311ab885\n",
      "  Stored in directory: /root/.cache/pip/wheels/df/aa/59/74f33db3bbedf322bcaadca4a43750ea5eb523bbe742d78b25\n",
      "Successfully built sqlitedict rouge-score\n",
      "\u001b[31mERROR: pydantic-core 2.14.5 has requirement typing-extensions!=4.7.0,>=4.6.0, but you'll have typing-extensions 4.1.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: pydantic 2.5.2 has requirement typing-extensions>=4.6.1, but you'll have typing-extensions 4.1.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: openai 1.3.9 has requirement typing-extensions<5,>=4.5, but you'll have typing-extensions 4.1.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: pyproject-api 1.6.1 has requirement packaging>=23.1, but you'll have packaging 23.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tox 4.11.4 has requirement filelock>=3.12.3, but you'll have filelock 3.12.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tox 4.11.4 has requirement packaging>=23.1, but you'll have packaging 23.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: toml, tabulate, texttable, sqlitedict, numexpr, portalocker, sacrebleu, colorama, tqdm-multiprocess, chardet, mbstrdecoder, pathvalidate, tcolorpy, typepy, DataProperty, tabledata, pytablewriter, jsonlines, pycountry, nltk, rouge-score, annotated-types, pydantic-core, pydantic, distro, h11, httpcore, httpx, openai, pybind11, lm-eval, platformdirs, tomli, pyproject-api, pluggy, distlib, virtualenv, tox, blessings, colour-runner, ordered-set, deepdiff, humanfriendly, coloredlogs, coverage, codecov, termcolor, rootpath, inspecta, attributedict, autoawq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found existing installation: platformdirs 3.6.0\n",
      "    Uninstalling platformdirs-3.6.0:\n",
      "      Successfully uninstalled platformdirs-3.6.0\n",
      "  Found existing installation: pluggy 1.0.0\n",
      "    Uninstalling pluggy-1.0.0:\n",
      "      Successfully uninstalled pluggy-1.0.0\n",
      "Successfully installed DataProperty-1.0.1 annotated-types-0.6.0 attributedict-0.3.0 autoawq-0.1.7+cu118 blessings-1.7 chardet-5.2.0 codecov-2.1.13 colorama-0.4.6 coloredlogs-15.0.1 colour-runner-0.1.1 coverage-7.3.3 deepdiff-6.7.1 distlib-0.3.8 distro-1.8.0 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 humanfriendly-10.0 inspecta-0.1.3 jsonlines-4.0.0 lm-eval-0.3.0 mbstrdecoder-1.1.3 nltk-3.8.1 numexpr-2.8.6 openai-1.3.9 ordered-set-4.1.0 pathvalidate-3.2.0 platformdirs-4.1.0 pluggy-1.3.0 portalocker-2.8.2 pybind11-2.11.1 pycountry-23.12.11 pydantic-2.5.2 pydantic-core-2.14.5 pyproject-api-1.6.1 pytablewriter-1.2.0 rootpath-0.1.1 rouge-score-0.1.2 sacrebleu-1.5.0 sqlitedict-2.1.0 tabledata-1.3.3 tabulate-0.9.0 tcolorpy-0.1.4 termcolor-2.4.0 texttable-1.7.0 toml-0.10.2 tomli-2.0.1 tox-4.11.4 tqdm-multiprocess-0.0.11 typepy-1.3.2 virtualenv-20.25.0\n"
     ]
    }
   ],
   "source": [
    "!sudo -H pip install https://github.com/casper-hansen/AutoAWQ/releases/download/v0.1.7/autoawq-0.1.7+cu118-cp38-cp38-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f29fd336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bac0ff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "603676d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "684c1b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvidia-cuda-cupti-cu11   11.7.101   \r\n",
      "nvidia-cuda-nvrtc-cu11   11.7.99    \r\n",
      "nvidia-cuda-runtime-cu11 11.7.99    \r\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb53d3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffccfab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1+cu117'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aee117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/data/mistral/query-to-mql/exp-10/nov-20/merged-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08c42249",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_path = '/data/mistral/query-to-mql/exp-10/nov-20/quant-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f89d01c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = {\"zero_point\":True,\n",
    "               \"q_group_size\": 1024,\n",
    "               \"w_bit\": 8,\n",
    "               \"version\": \"GEMM\"\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f853010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3a9fdb9c504bddb931da9e2d96cd96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load model\n",
    "# model = AutoAWQForCausalLM.from_pretrained(model_path, **{\"low_cpu_mem_usage\":True})\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_path, safetensors=True, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e50621ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48e2dc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 15.60 GiB total capacity; 14.75 GiB already allocated; 120.94 MiB free; 15.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Quantize\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# dont run it again\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/awq/models/base.py:49\u001b[0m, in \u001b[0;36mBaseAWQForCausalLM.quantize\u001b[0;34m(self, tokenizer, quant_config, calib_data, split, text_column)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquantize\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokenizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, quant_config\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m     45\u001b[0m                    calib_data: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]]\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpileval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     46\u001b[0m                    split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, text_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_config: AwqConfig \u001b[38;5;241m=\u001b[39m AwqConfig\u001b[38;5;241m.\u001b[39mfrom_dict(quant_config)\n\u001b[0;32m---> 49\u001b[0m     quantizer \u001b[38;5;241m=\u001b[39m \u001b[43mAwqQuantizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw_bit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_group_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalib_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_column\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     quantizer\u001b[38;5;241m.\u001b[39mquantize()\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_quantized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/awq/quantize/quantizer.py:27\u001b[0m, in \u001b[0;36mAwqQuantizer.__init__\u001b[0;34m(self, awq_model, model, tokenizer, w_bit, group_size, version, calib_data, split, text_column)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;241m=\u001b[39m split\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_column \u001b[38;5;241m=\u001b[39m text_column\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/awq/quantize/quantizer.py:330\u001b[0m, in \u001b[0;36mAwqQuantizer.init_quant\u001b[0;34m(self, n_samples, seqlen)\u001b[0m\n\u001b[1;32m    328\u001b[0m modules[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m Catcher(modules[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:  \u001b[38;5;66;03m# work with early exit\u001b[39;00m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py:1009\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1006\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1009\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1022\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py:837\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    834\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, seq_length)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 837\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    840\u001b[0m     attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    841\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_flash_attn_2_enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_flash_attn_2_enabled\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    844\u001b[0m ):\n\u001b[1;32m    845\u001b[0m     is_padding_right \u001b[38;5;241m=\u001b[39m attention_mask[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m!=\u001b[39m batch_size\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 260.00 MiB (GPU 0; 15.60 GiB total capacity; 14.75 GiB already allocated; 120.94 MiB free; 15.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#Quantize\n",
    "# dont run it again\n",
    "#model.quantize(tokenizer, quant_config=quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb126257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:`quant_config.json` is being deprecated in the future in favor of quantization_config in config.json.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/data/quantization-trials/quant-model/tokenizer_config.json',\n",
       " '/data/quantization-trials/quant-model/special_tokens_map.json',\n",
       " '/data/quantization-trials/quant-model/vocab.json',\n",
       " '/data/quantization-trials/quant-model/merges.txt',\n",
       " '/data/quantization-trials/quant-model/added_tokens.json',\n",
       " '/data/quantization-trials/quant-model/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.save_quantized(quant_path)\n",
    "#tokenizer.save_pretrained(quant_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac71427",
   "metadata": {},
   "source": [
    "### Inference on quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2fc875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "quant_path = '/data/quantization-trials/quant-model'\n",
    "model_path = '/data/quantization-trials/merged-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ed7b2db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_id = quant_path\n",
    "quant_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "def predict_from_quant(user_query):\n",
    "    _inputs = tokenizer.encode(user_query, return_tensors=\"pt\").to('cuda')\n",
    "    outputs = quant_model.generate(input_ids=_inputs, max_length= 1000, pad_token_id=tokenizer.eos_token_id)\n",
    "    output = tokenizer.decode(outputs[0])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "574a42ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ace8938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken is : 21.147445678710938\n"
     ]
    }
   ],
   "source": [
    "# Using quant model\n",
    "start = time.time()\n",
    "output1 = predict_from_quant(\"what is art\")\n",
    "print(\"time taken is :\", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "484f1d27",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>what is art?\n",
      "\n",
      "Art is a way of expressing one's self. It is a way of expressing one's thoughts, feelings, and ideas. It is a way of expressing one's self through the use of various media.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to describe a wide variety of art forms.\n",
      "\n",
      "The term art is used to describe a wide variety of art forms. The term art is used to\n"
     ]
    }
   ],
   "source": [
    "print(output1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3fee82",
   "metadata": {},
   "source": [
    "### Inference on un-quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdf676e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba90fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/data/quantization-trials/merged-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4a6c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = model_path\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "def predict_from_normal(user_query):\n",
    "    _inputs = tokenizer.encode(user_query, return_tensors=\"pt\").to('cuda')\n",
    "    outputs = model.generate(input_ids=_inputs, max_length= 1000, pad_token_id=tokenizer.eos_token_id)\n",
    "    output = tokenizer.decode(outputs[0])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "010b00db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken is : 23.784337282180786\n"
     ]
    }
   ],
   "source": [
    "# Using unquant model\n",
    "import time\n",
    "start = time.time()\n",
    "output2 = predict_from_normal(\"what is art\")\n",
    "print(\"time taken is :\", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b5b6f9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>what is art?\n",
      "\n",
      "Art is a creative expression of the human mind. It is a way of expressing the human mind through the use of various media.\n",
      "\n",
      "The word art is derived from the Latin word artus, which means \"to make\". The word art is used in many different contexts, including:\n",
      "\n",
      "- to describe the beauty of a work of art\n",
      "\n",
      "- to describe the meaning of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of art\n",
      "\n",
      "- to describe the structure of a work of\n"
     ]
    }
   ],
   "source": [
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0949f05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 2048, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
       "      (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f139bb23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 2048, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
       "      (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): WQLinear_GEMM(in_features=2048, out_features=2048, bias=True, w_bit=4, group_size=512)\n",
       "            (v_proj): WQLinear_GEMM(in_features=2048, out_features=2048, bias=True, w_bit=4, group_size=512)\n",
       "            (q_proj): WQLinear_GEMM(in_features=2048, out_features=2048, bias=True, w_bit=4, group_size=512)\n",
       "            (out_proj): WQLinear_GEMM(in_features=2048, out_features=2048, bias=True, w_bit=4, group_size=512)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): WQLinear_GEMM(in_features=2048, out_features=8192, bias=True, w_bit=4, group_size=512)\n",
       "          (fc2): WQLinear_GEMM(in_features=8192, out_features=2048, bias=True, w_bit=4, group_size=512)\n",
       "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
