{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.signal import convolve2d\n",
    "from tqdm import tqdm, notebook\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096*2 # Batch size 16384\n",
    "image_shape = (1,28,28)\n",
    "image_1d_shape = np.prod(image_shape)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(\"./data/\",download=True, train=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(\"./data/\",download=True, train=False, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True )\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=True)\n",
    "\n",
    "def show_image(x, name=None):\n",
    "    x = x.squeeze()\n",
    "    if name:\n",
    "        plt.title(name)\n",
    "    plt.imshow(x, cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "# The method for generating masks for negative data mentioned by Geoffrey Hinton in the article\n",
    "def mask_gen():\n",
    "    random_iter = np.random.randint(5,10)\n",
    "    random_image = np.random.randint(2, size=image_shape).squeeze().astype(np.float32)\n",
    "    blur_filter = np.array([[1, 2, 1], [2, 4, 2], [1, 2, 1]]) / 16\n",
    "    for i in range(random_iter):\n",
    "        random_image = convolve2d(random_image, blur_filter, mode='same', boundary='symm')\n",
    "    mask = (random_image > 0.50).astype(np.float32)\n",
    "    return mask\n",
    "\n",
    "# The method for creating masks for negative data that I tried for testing purposes.\n",
    "def mask_gen1():\n",
    "    n = image_1d_shape\n",
    "    arr1 = np.random.normal(loc=0, scale=0.01, size=int(5*n/8))\n",
    "    arr1 = arr1+ abs(0-arr1.min())\n",
    "    arr2 = np.random.normal(loc=1, scale=0.01, size=int(3*n/8))\n",
    "    arr2 = arr2 + abs(1-arr2.max())\n",
    "    arr = np.concatenate([arr1,arr2])\n",
    "    np.random.shuffle(arr)\n",
    "    mask = arr.reshape(image_shape).astype(np.float32)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_data_gen(batch, mask_num=1):\n",
    "    indexes = torch.randperm(batch.shape[0])\n",
    "    x1 = batch\n",
    "    x2 = batch[indexes]\n",
    "    if mask_num == 1:\n",
    "        mask = mask_gen1()\n",
    "    else:\n",
    "        mask = mask_gen()\n",
    "    merged_x1 = x1*mask\n",
    "    merged_x2 = x2*(1-mask)\n",
    "    hybrid_image = merged_x1+merged_x2\n",
    "    return hybrid_image\n",
    "\n",
    "# take the data and create x_pos and x_neg in cuda\n",
    "def prepocess_data(x, device, training=True):\n",
    "    x_pos = x\n",
    "    x_pos = x_pos.view(x_pos.shape[0], -1).to(device)\n",
    "    if training:\n",
    "        x_neg = negative_data_gen(x,0)\n",
    "        x_neg = x_neg.view(x_neg.shape[0], -1).to(device)\n",
    "        return x_pos, x_neg\n",
    "    else:\n",
    "        return x_pos\n",
    "    \n",
    "    # function who onehot encode the labels in dim (batch, labels)\n",
    "def onehot(labels, num_classes=10):\n",
    "    return torch.zeros(labels.shape[0], num_classes).scatter_(1, labels.unsqueeze(1), 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Multilayer_Layer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_a_size, hidden_b_size, learning_rate=3e-4, threshold=2):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_a_size = hidden_a_size\n",
    "        self.hidden_b_size = hidden_b_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.Wx = nn.Parameter(torch.randn(output_size, self.input_size))\n",
    "\n",
    "        self.Wh_above = nn.Parameter(torch.randn(output_size, self.hidden_a_size))\n",
    "        self.Wh_below = nn.Parameter(torch.randn(output_size, self.hidden_b_size))\n",
    "        self.Bh = nn.Parameter(torch.zeros(self.output_size))\n",
    "        \n",
    "        self.Wy = nn.Parameter(torch.randn(self.output_size, self.output_size))\n",
    "        self.By = nn.Parameter(torch.zeros(self.output_size))\n",
    "\n",
    "        self.loss = []\n",
    "        self.update_rate = 0.7\n",
    "\n",
    "        self.threshold = threshold\n",
    "        self.opt = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def normalize(self,x):\n",
    "        return x / (x.norm(2, 1, keepdim=True) + 1e-4)\n",
    "        \n",
    "    def forward(self,x, hidden_state, hidden_below_state, hidden_above_state):\n",
    "        norm_hidden_above_state = self.normalize(hidden_above_state)\n",
    "        norm_hidden_below_state = self.normalize(hidden_below_state)\n",
    "        prev_hidden_state = norm_hidden_above_state  @ self.Wh_above.T + norm_hidden_below_state @ self.Wh_below.T\n",
    "        computed_new_state = torch.tanh((x @ self.Wx.T) + (prev_hidden_state) + self.Bh)\n",
    "        new_hidden_state = self.update_rate * computed_new_state +  (1 - self.update_rate) * hidden_state\n",
    "        output = torch.tanh(new_hidden_state @ self.Wy.T + self.By)\n",
    "        return output, new_hidden_state\n",
    "    \n",
    "    def train(self, x_pos, x_neg, pos_hidden_state, neg_hidden_state, num_epochs=10):\n",
    "        pos_hidden_state, pos_hidden_below_state, pos_hidden_above_state = pos_hidden_state\n",
    "        neg_hidden_state, neg_hidden_below_state, neg_hidden_above_state = neg_hidden_state\n",
    "        for epoch in range(num_epochs):\n",
    "            output_pos, _ = self.forward(x_pos, pos_hidden_state, pos_hidden_below_state, pos_hidden_above_state)\n",
    "            output_neg, _ = self.forward(x_neg, neg_hidden_state, neg_hidden_below_state, neg_hidden_above_state)\n",
    "            loss = torch.log(1 + torch.exp(torch.cat([\n",
    "                    -output_pos + self.threshold,\n",
    "                    output_neg - self.threshold]))).mean()\n",
    "            self.opt.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            self.loss.append(loss.item())\n",
    "            self.opt.step()\n",
    "\n",
    "        return (self.forward(x_pos, pos_hidden_state, pos_hidden_below_state, pos_hidden_above_state), self.forward(x_neg, neg_hidden_state, neg_hidden_below_state, neg_hidden_above_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Cell(nn.Module):\n",
    "    def __init__(self, layers, learning_rate, threshold):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten(start_dim=2, end_dim=3)\n",
    "        self.layers = layers\n",
    "        self.rnn_layer1 = RNN_Multilayer_Layer(input_size= layers[0], output_size= layers[1], hidden_a_size= layers[2], hidden_b_size= layers[0], learning_rate=learning_rate, threshold=threshold)\n",
    "        self.rnn_layer2 = RNN_Multilayer_Layer(input_size= layers[1], output_size= layers[2], hidden_a_size= layers[3], hidden_b_size= layers[1], learning_rate=learning_rate, threshold=threshold)\n",
    "        self.rnn_layer3 = RNN_Multilayer_Layer(input_size= layers[2], output_size= layers[3], hidden_a_size= layers[3], hidden_b_size= layers[2], learning_rate=learning_rate, threshold=threshold)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.losses = [[],[],[]]\n",
    "    \n",
    "    def forward(self,x,y, hidden_layers):\n",
    "        hidden_1_prev, hidden_2_prev, hidden_3_prev = hidden_layers\n",
    "        flattened_x = self.flatten(x)\n",
    "        x, y = flattened_x, y\n",
    "        print(\"\\t Layer1\")\n",
    "        out, hidden_1_new = self.rnn_layer1(x, hidden_1_prev, x, hidden_2_prev)\n",
    "        print(\"\\t Layer2\")\n",
    "        out, hidden_2_new = self.rnn_layer2(out, hidden_2_prev, hidden_1_prev, hidden_3_prev)\n",
    "        print(\"\\t Layer3\")\n",
    "        out, hidden_3_new = self.rnn_layer3(out, hidden_3_prev, hidden_2_prev, y)\n",
    "        return out , [hidden_1_new, hidden_2_new, hidden_3_new]\n",
    "    \n",
    "    def train(self, x_pos, x_neg, y, pos_hidden_layers, neg_hidden_layers):\n",
    "        pos_hidden_1_prev, pos_hidden_2_prev, pos_hidden_3_prev = pos_hidden_layers\n",
    "        neg_hidden_1_prev, neg_hidden_2_prev, neg_hidden_3_prev = neg_hidden_layers\n",
    "        flattened_x_pos, flattened_x_neg = self.flatten(x_pos),self.flatten(x_neg)\n",
    "        x_pos, x_neg, y = flattened_x_pos, flattened_x_neg, y\n",
    "        self.rnn_layer1.zero_grad()\n",
    "        print(\"Layer 1\")\n",
    "        (out_pos, hidden_new_1_pos), (out_neg, hidden_new_1_neg) = self.rnn_layer1.train(x_pos, x_neg,\n",
    "                                                                                         pos_hidden_state=(pos_hidden_1_prev, x_pos, pos_hidden_2_prev),\n",
    "                                                                                          neg_hidden_state=(neg_hidden_1_prev, x_neg, neg_hidden_2_prev))\n",
    "        self.rnn_layer2.zero_grad()\n",
    "        print(\"Layer 2\")\n",
    "        (out_pos, hidden_new_2_pos), (out_neg, hidden_new_2_neg) = self.rnn_layer2.train(out_pos, out_neg,\n",
    "                                                                                         pos_hidden_state=(pos_hidden_2_prev, pos_hidden_1_prev, pos_hidden_3_prev),\n",
    "                                                                                          neg_hidden_state=(neg_hidden_2_prev, neg_hidden_1_prev, neg_hidden_3_prev))\n",
    "        self.rnn_layer3.zero_grad()\n",
    "        print(\"Layer 3\")\n",
    "        (out_pos, hidden_new_3_pos), (out_neg, hidden_new_3_neg) = self.rnn_layer3.train(out_pos, out_neg,\n",
    "                                                                                            pos_hidden_state=(pos_hidden_3_prev,pos_hidden_2_prev, y),\n",
    "                                                                                            neg_hidden_state=(neg_hidden_3_prev,neg_hidden_2_prev, y))\n",
    "        pos_hidden_layers = [hidden_new_1_pos, hidden_new_2_pos, hidden_new_3_pos]\n",
    "        neg_hidden_layers = [hidden_new_1_neg, hidden_new_2_neg, hidden_new_3_neg]\n",
    "        self.losses[0].append(self.rnn_layer1.loss)\n",
    "        self.losses[1].append(self.rnn_layer2.loss)\n",
    "        self.losses[2].append(self.rnn_layer3.loss)\n",
    "        return out_pos, out_neg, pos_hidden_layers, neg_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randn(16,1, 28, 28)\n",
    "# y = torch.randn(16,10)\n",
    "# layers = [784,2000,2000,10]\n",
    "# hidden_states = [torch.zeros(1, layers[1]), torch.zeros(1, layers[2]),torch.zeros(1, layers[3])]\n",
    "# out, hidden_states = RNN_Cell(layers, 1,1).train(x ,x, y, hidden_states, hidden_states)\n",
    "# # out, hidden_states = RNN_Cell(layers, 1,1).train(x, x, y, hidden_states, hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_FF_Net(nn.Module):\n",
    "    def __init__(self, layers, learning_rate, threshold, time_steps):\n",
    "        super().__init__()\n",
    "        self.rnn_cell = RNN_Cell(layers, learning_rate, threshold)\n",
    "        self.num_time_steps = time_steps\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.hidden_state = [[[torch.zeros(1, layers[1]).to(device), torch.zeros(1, layers[2]).to(device),torch.zeros(1, layers[3]).to(device)],[torch.zeros(1, layers[1]).to(device), torch.zeros(1, layers[2]).to(device),torch.zeros(1, layers[3]).to(device)]]]\n",
    "    def forward(self, x, y):\n",
    "        out = 0\n",
    "        for i in range(self.num_time_steps):\n",
    "            print(i)\n",
    "            out, hiddent_state_new = self.rnn_cell(x, y, self.hidden_state[i])\n",
    "            self.hidden_state.append(hiddent_state_new)\n",
    "        return out\n",
    "    def train(self, x_pos, x_neg, y):\n",
    "        out = 0\n",
    "        for i in range(self.num_time_steps):\n",
    "            print(i)\n",
    "            _, _,pos_hidden_states, neg_hidden_states = self.rnn_cell.train(x_pos, x_neg, y, self.hidden_state[i][0],self.hidden_state[i][1])\n",
    "            hiddent_state_new = [pos_hidden_states, neg_hidden_states]\n",
    "            self.hidden_state.append(hiddent_state_new)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Layer 1\n",
      "Layer 2\n",
      "Layer 3\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.50 GiB (GPU 0; 3.82 GiB total capacity; 2.65 GiB already allocated; 306.00 MiB free; 2.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m x_neg \u001b[39m=\u001b[39m negative_data_gen(x, \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m y \u001b[39m=\u001b[39m onehot(y,\u001b[39m10\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 12\u001b[0m model\u001b[39m.\u001b[39;49mtrain(x_pos, x_neg, y)\n",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m, in \u001b[0;36mRNN_FF_Net.train\u001b[0;34m(self, x_pos, x_neg, y)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_time_steps):\n\u001b[1;32m     23\u001b[0m     \u001b[39mprint\u001b[39m(i)\n\u001b[0;32m---> 24\u001b[0m     _, _,pos_hidden_states, neg_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn_cell\u001b[39m.\u001b[39;49mtrain(x_pos, x_neg, y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_state[i][\u001b[39m0\u001b[39;49m],\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_state[i][\u001b[39m1\u001b[39;49m])\n\u001b[1;32m     25\u001b[0m     hiddent_state_new \u001b[39m=\u001b[39m [pos_hidden_states, neg_hidden_states]\n\u001b[1;32m     26\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_state\u001b[39m.\u001b[39mappend(hiddent_state_new)\n",
      "Cell \u001b[0;32mIn[6], line 41\u001b[0m, in \u001b[0;36mRNN_Cell.train\u001b[0;34m(self, x_pos, x_neg, y, pos_hidden_layers, neg_hidden_layers)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn_layer3\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     40\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLayer 3\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m (out_pos, hidden_new_3_pos), (out_neg, hidden_new_3_neg) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn_layer3\u001b[39m.\u001b[39;49mtrain(out_pos, out_neg,\n\u001b[1;32m     42\u001b[0m                                                                                     pos_hidden_state\u001b[39m=\u001b[39;49m(pos_hidden_3_prev,pos_hidden_2_prev, y),\n\u001b[1;32m     43\u001b[0m                                                                                     neg_hidden_state\u001b[39m=\u001b[39;49m(neg_hidden_3_prev,neg_hidden_2_prev, y))\n\u001b[1;32m     44\u001b[0m pos_hidden_layers \u001b[39m=\u001b[39m [hidden_new_1_pos, hidden_new_2_pos, hidden_new_3_pos]\n\u001b[1;32m     45\u001b[0m neg_hidden_layers \u001b[39m=\u001b[39m [hidden_new_1_neg, hidden_new_2_neg, hidden_new_3_neg]\n",
      "Cell \u001b[0;32mIn[5], line 40\u001b[0m, in \u001b[0;36mRNN_Multilayer_Layer.train\u001b[0;34m(self, x_pos, x_neg, pos_hidden_state, neg_hidden_state, num_epochs)\u001b[0m\n\u001b[1;32m     38\u001b[0m neg_hidden_state, neg_hidden_below_state, neg_hidden_above_state \u001b[39m=\u001b[39m neg_hidden_state\n\u001b[1;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 40\u001b[0m     output_pos, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(x_pos, pos_hidden_state, pos_hidden_below_state, pos_hidden_above_state)\n\u001b[1;32m     41\u001b[0m     output_neg, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(x_neg, neg_hidden_state, neg_hidden_below_state, neg_hidden_above_state)\n\u001b[1;32m     42\u001b[0m     loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlog(\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mexp(torch\u001b[39m.\u001b[39mcat([\n\u001b[1;32m     43\u001b[0m             \u001b[39m-\u001b[39moutput_pos \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthreshold,\n\u001b[1;32m     44\u001b[0m             output_neg \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthreshold])))\u001b[39m.\u001b[39mmean()\n",
      "Cell \u001b[0;32mIn[5], line 31\u001b[0m, in \u001b[0;36mRNN_Multilayer_Layer.forward\u001b[0;34m(self, x, hidden_state, hidden_below_state, hidden_above_state)\u001b[0m\n\u001b[1;32m     29\u001b[0m norm_hidden_below_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize(hidden_below_state)\n\u001b[1;32m     30\u001b[0m prev_hidden_state \u001b[39m=\u001b[39m norm_hidden_above_state  \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mWh_above\u001b[39m.\u001b[39mT \u001b[39m+\u001b[39m norm_hidden_below_state \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mWh_below\u001b[39m.\u001b[39mT\n\u001b[0;32m---> 31\u001b[0m computed_new_state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtanh((x \u001b[39m@\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mWx\u001b[39m.\u001b[39;49mT) \u001b[39m+\u001b[39;49m (prev_hidden_state) \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mBh)\n\u001b[1;32m     32\u001b[0m new_hidden_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_rate \u001b[39m*\u001b[39m computed_new_state \u001b[39m+\u001b[39m  (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_rate) \u001b[39m*\u001b[39m hidden_state\n\u001b[1;32m     33\u001b[0m output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtanh(new_hidden_state \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mWy\u001b[39m.\u001b[39mT \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mBy)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.50 GiB (GPU 0; 3.82 GiB total capacity; 2.65 GiB already allocated; 306.00 MiB free; 2.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "layers = [28*28, 100, 100, 10]\n",
    "model = RNN_FF_Net(layers, 3e-4, 2, 8).to(device)\n",
    "\n",
    "\n",
    "train_iterator = iter(train_loader)\n",
    "for x,y in train_iterator:\n",
    "    x_pos = x.to(device)\n",
    "    x_neg = negative_data_gen(x, 0).to(device)\n",
    "    y = onehot(y,10).to(device)\n",
    "    model.train(x_pos, x_neg, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(train_iterator)[0]\n",
    "print(x.shape)\n",
    "print(negative_data_gen(x, 0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
