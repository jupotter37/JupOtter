{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1488335-0f39-4bf8-af50-f40bc209d1eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthew.schmitz/Matthew/utils/miniforge3/envs/antipode/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "Number of GPUs: 1\n",
      "GPU Name: Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "# various import statements\n",
    "import os\n",
    "import inspect\n",
    "import seaborn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import scanpy as sc\n",
    "import pyro\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"GPU is not available\")\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/matthew.schmitz/Matthew/code/scANTIPODE/')\n",
    "import antipode\n",
    "from antipode.antipode_model import *\n",
    "import antipode.model_functions\n",
    "from antipode.model_functions import *\n",
    "import antipode.model_distributions\n",
    "from antipode.model_distributions import *\n",
    "import antipode.model_modules\n",
    "from antipode.model_modules import *\n",
    "import antipode.train_utils\n",
    "from antipode.train_utils import *\n",
    "import antipode.plotting\n",
    "from antipode.plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "035d8b81-ae82-4402-9515-913e44693588",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata=sc.read_h5ad(os.path.expanduser('/allen/programs/celltypes/workgroups/rnaseqanalysis/EvoGen/Team/Matthew/data/taxtest/HvQvM/HvQvMall_cere_clean_nodoublets.h5ad'),backed='r')\n",
    "#adata=sc.read_h5ad(os.path.expanduser('/allen/programs/celltypes/workgroups/rnaseqanalysis/EvoGen/Team/Matthew/data/taxtest/HvQvM/HvQvMall_part0.h5ad'),backed='r')\n",
    "\n",
    "batch_key='batch_name'\n",
    "discov_key='species'\n",
    "layer_key='spliced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed81a094-1df5-479d-a548-4da8408300e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Derived from testing version PBS1.9.1.8.1\n",
    "import os\n",
    "import sys\n",
    "import sklearn\n",
    "from sklearn import cluster\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import anndata\n",
    "import scvi\n",
    "import inspect\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import scipy\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import softplus, softmax\n",
    "from torch.distributions import constraints\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "import pyro.optim\n",
    "import re\n",
    "import inspect\n",
    "from anndata import AnnData\n",
    "from mudata import MuData\n",
    "from typing import Literal, Optional\n",
    "from scvi.module.base import PyroBaseModuleClass\n",
    "\n",
    "\n",
    "class AntipodeTrainingMixin:\n",
    "    '''\n",
    "    Mixin class providing functions to actually run ANTIPODE.\n",
    "    The naive model trains in 3 phases, first a nonhierarchical block phase to estimate cell type manifolds, then phase 2 learns parameters for a fixed discrete clustering on a fixed latent space for initialization (or supervised). Phase 3 makes all parameters learnable.\n",
    "    You can also use supervised taxonomy by providing a clustering as a discrete obsm matrix and training only phase2 with freeze_encoder=False.\n",
    "    '''\n",
    "    \n",
    "    def save_params_to_uns(self,prefix=''):\n",
    "        pstore=param_store_to_numpy()\n",
    "        pstore={n:pstore[n] for n in pstore.keys() if not re.search('encoder|classifier|be_nn|\\$\\$\\$',n)}\n",
    "        pstore={n:pstore[n] for n in pstore.keys() if not np.isnan(pstore[n]).any()}\n",
    "        self.adata_manager.adata.uns[prefix+'param_store']=pstore\n",
    "\n",
    "    def get_antipode_outputs(self,batch_size=2048,device='cuda'):\n",
    "        if 'discov_onehot' not in self.adata_manager.adata.obsm.keys():\n",
    "            self.adata_manager.adata.obs[self.discov_key]=self.adata_manager.adata.obs[self.discov_key].astype('category')\n",
    "            self.adata_manager.adata.obsm['discov_onehot']=numpy_onehot(self.adata_manager.adata.obs[self.discov_key].cat.codes)\n",
    "        self.adata_manager.register_new_fields([scvi.data.fields.ObsmField('discov_onehot','discov_onehot')])\n",
    "    \n",
    "        field_types={\"s\":np.float32,\"discov_onehot\":np.float32}\n",
    "        dataloader=scvi.dataloaders.AnnDataLoader(self.adata_manager,batch_size=32,drop_last=False,shuffle=False,data_and_attributes=field_types)#supervised_field_types for supervised step \n",
    "        encoder_outs=batch_output_from_dataloader(dataloader,self.zl_encoder,batch_size=batch_size,device=device)\n",
    "        encoder_outs[0]=self.z_transform(encoder_outs[0])\n",
    "        encoder_out=[x.detach().cpu().numpy() for x in encoder_outs]\n",
    "        classifier_outs=batch_torch_outputs([(encoder_outs[0])],self.classifier,batch_size=batch_size,device='cuda')\n",
    "        classifier_out=[x.detach().cpu().numpy() for x in classifier_outs]\n",
    "        return encoder_out,classifier_out\n",
    "\n",
    "    def store_outputs(self,device='cuda',prefix=''):\n",
    "        self.save_params_to_uns(prefix='')\n",
    "        self.to('cpu')\n",
    "        self.eval()\n",
    "        antipode_outs=self.get_antipode_outputs(batch_size=2048,device=device)\n",
    "        self.allDone()\n",
    "        taxon=antipode_outs[1][0]\n",
    "        self.adata_manager.adata.obsm[prefix+'X_antipode']=antipode_outs[0][0]\n",
    "        for i in range(antipode_outs[1][1].shape[1]):\n",
    "            self.adata_manager.adata.obs[prefix+'psi_'+str(i)]=numpy_centered_sigmoid(antipode_outs[1][1][...,i])\n",
    "        self.adata_manager.adata.obs[prefix+'q_score']=scipy.special.expit(antipode_outs[0][2])\n",
    "        level_edges=[numpy_hardmax(self.adata_manager.adata.uns[prefix+'param_store']['edges_'+str(i)],axis=-1) for i in range(len(self.level_sizes)-1)]\n",
    "        levels=self.tree_convergence_bottom_up.just_propagate(scipy.special.softmax(taxon[...,-self.level_sizes[-1]:],axis=-1),level_edges,s=torch.ones(1))\n",
    "        prop_taxon=np.concatenate(levels,axis=-1)\n",
    "        self.adata_manager.adata.obsm[prefix+'taxon_probs']=prop_taxon\n",
    "        levels=self.tree_convergence_bottom_up.just_propagate(numpy_hardmax(levels[-1],axis=-1),level_edges,s=torch.ones(1))\n",
    "        for i in range(len(levels)):\n",
    "            cur_clust=prefix+'level_'+str(i)\n",
    "            self.adata_manager.adata.obs[cur_clust]=levels[i].argmax(1)\n",
    "            self.adata_manager.adata.obs[cur_clust]=self.adata_manager.adata.obs[cur_clust].astype(str)\n",
    "        self.adata_manager.adata.obs[prefix+'antipode_cluster'] = self.adata_manager.adata.obs.apply(lambda x: '_'.join([x[prefix+'level_'+str(i)] for i in range(len(levels))]), axis=1)\n",
    "        self.adata_manager.adata.obs[prefix+'antipode_cluster'] = self.adata_manager.adata.obs[prefix+'antipode_cluster'].astype(str)    \n",
    "    \n",
    "    def pretrain_classifier(self,epochs = 5,learning_rate = 0.001,batch_size = 64,prefix='',cluster='kmeans',device='cuda'):\n",
    "        '''basic pytorch training of feed forward classifier to ease step 2'''        \n",
    "        self.train()\n",
    "        \n",
    "        model = self.classifier.to(device)\n",
    "        input_tensor =  torch.tensor(self.adata_manager.adata.obsm[self.dimension_reduction])  # Your input features tensor, shape [n_samples, n_features]\n",
    "        target_tensor = torch.tensor(self.adata_manager.adata.obsm[cluster+'_onehot'])  # Your target labels tensor, shape [n_samples]    \n",
    "        \n",
    "        # Step 1: Prepare to train\n",
    "        dataset = torch.utils.data.TensorDataset(input_tensor, target_tensor)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        #Training loop\n",
    "        for epoch in range(epochs):\n",
    "            for inputs, targets in dataloader:\n",
    "                # Forward pass\n",
    "                outputs = model(inputs.to(device))\n",
    "                loss = criterion(softmax(outputs[0],-1)[:,-targets.shape[-1]:], targets.to(device))\n",
    "        \n",
    "                # Backward pass and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')     \n",
    "\n",
    "    def fix_scale_factor(self,svi,x,ideal_val=0.1):\n",
    "        o1=svi.evaluate_loss(*x)\n",
    "        s1=self.scale_factor\n",
    "        s2=ideal_val*s1/o1\n",
    "        self.scale_factor=s2\n",
    "\n",
    "    def prepare_phase_2(self,cluster='kmeans',prefix='',epochs = 5,device=None,dimension_reduction='X_antipode'):\n",
    "        '''Run this if not running in supervised only mode (JUST phase2 with provided obsm clustering), \n",
    "        runs kmeans if cluster=kmeans, else uses the obs column provided by cluster. epochs=None skips pretraing of classifier\n",
    "        To learn a latent space from scratch set dimension_reduction to None and use freeze_encoder=False'''\n",
    "        if cluster=='kmeans':\n",
    "            kmeans = sklearn.cluster.MiniBatchKMeans(n_clusters=self.level_sizes[-1],init='k-means++',max_iter=1000,reassignment_ratio=0.001,n_init=100,random_state=0).fit(self.adata_manager.adata.obsm[dimension_reduction])\n",
    "            self.adata_manager.adata.obs['kmeans']=kmeans.labels_\n",
    "            self.adata_manager.adata.obs['kmeans']=self.adata_manager.adata.obs['kmeans'].astype(int).astype('category')\n",
    "            self.adata_manager.adata.obsm['kmeans_onehot']=numpy_onehot(self.adata_manager.adata.obs['kmeans'].cat.codes,num_classes=self.level_sizes[-1])\n",
    "        else:\n",
    "            self.adata_manager.adata.obs[cluster]=self.adata_manager.adata.obs[cluster].astype('category')\n",
    "            self.adata_manager.adata.obsm[cluster+'_onehot']=numpy_onehot(self.adata_manager.adata.obs[cluster].cat.codes,num_classes=self.level_sizes[-1])\n",
    "        device=pyro.param('locs').device if device is None else device\n",
    "        self.adata_manager.register_new_fields([make_field('taxon',('obsm',cluster+'_onehot'))])\n",
    "        if dimension_reduction is not None:#For supervised Z register dr\n",
    "            self.dimension_reduction=dimension_reduction\n",
    "            self.adata_manager.register_new_fields([make_field('Z_obs',('obsm',dimension_reduction))])\n",
    "        if (epochs is not None) and (dimension_reduction is not None):\n",
    "            self.pretrain_classifier(cluster=cluster,prefix=prefix,epochs=epochs,device=device)\n",
    "        kmeans_means=group_aggr_anndata(self.adata_manager.adata,[cluster], agg_func=np.mean,layer=dimension_reduction,obsm=True)[0]\n",
    "        if 'locs' not in [x for x in pyro.get_param_store()]:\n",
    "            print('quick init')\n",
    "            self.train_phase(phase=1,max_steps=1,print_every=10000,num_particles=1,device=device, max_learning_rate=1e-10, one_cycle_lr=True, steps=0, batch_size=4)\n",
    "            self.cpu()\n",
    "\n",
    "        hierarchy=scipy.cluster.hierarchy.ward(kmeans_means)\n",
    "        level_assignments=[scipy.cluster.hierarchy.cut_tree(hierarchy,n_clusters=x) for x in self.level_sizes]\n",
    "        adj_means_dict=calculate_layered_tree_means(kmeans_means, level_assignments)\n",
    "        new_clusts=[adj_means_dict[k][j] for k in adj_means_dict.keys() for j in adj_means_dict[k].keys()]\n",
    "        new_locs=torch.tensor(new_clusts,device=device).float()\n",
    "        \n",
    "        edge_matrices=create_edge_matrices(level_assignments)\n",
    "        edge_matrices=[torch.tensor(x,device=device) for x in edge_matrices]\n",
    "        for i in range(len(self.level_sizes)-1):\n",
    "            #pyro.get_param_store().__setitem__('edges_'+str(i), pyro.param('edges_'+str(i)).detach()+edge_matrices[i].T)\n",
    "            pyro.get_param_store().__setitem__('edges_'+str(i), 1e-4*torch.randn(edge_matrices[i].T.shape,device=device).float()+edge_matrices[i].T.float())\n",
    "        \n",
    "        self.adata_manager.adata.obs[cluster].astype(int)\n",
    "        new_scales=group_aggr_anndata(self.adata_manager.adata,[cluster], agg_func=np.std,layer=dimension_reduction,obsm=True)[0]\n",
    "        new_scales=torch.concatenate(\n",
    "            [1e-5 * self.scale_init_val * new_locs.new_ones(sum(self.level_sizes[:-1]), pyro.param('locs').shape[1],requires_grad=True),\n",
    "             torch.tensor(new_scales+1e-10,device=device,requires_grad=True)],axis=0).float()\n",
    "        self.adata_manager.adata.obs[cluster].astype(str)\n",
    "        pyro.get_param_store().__setitem__('locs',new_locs)\n",
    "        pyro.get_param_store().__setitem__('locs_dynam',new_locs.new_zeros(new_locs.shape))\n",
    "        pyro.get_param_store().__setitem__('scales',new_scales)\n",
    "        self.adata_manager.adata.obs[cluster]=self.adata_manager.adata.obs[cluster].astype(str)\n",
    "        pyro.get_param_store().__setitem__('discov_dm',new_locs.new_zeros(pyro.param('discov_dm').shape))\n",
    "        pyro.get_param_store().__setitem__('batch_dm',new_locs.new_zeros(pyro.param('batch_dm').shape))\n",
    "        pyro.get_param_store().__setitem__('discov_di',new_locs.new_zeros(pyro.param('discov_di').shape))\n",
    "        pyro.get_param_store().__setitem__('batch_di',new_locs.new_zeros(pyro.param('batch_di').shape))\n",
    "        pyro.get_param_store().__setitem__('cluster_intercept',new_locs.new_zeros(pyro.param('cluster_intercept').shape))\n",
    "        pyro.get_param_store().__setitem__('discov_dc',new_locs.new_zeros(pyro.param('discov_dc').shape))\n",
    "\n",
    "    \n",
    "    def common_training_loop(self, dataloader, max_steps, scheduler, svi, print_every, device, steps=0):\n",
    "        self.losses = []\n",
    "        pbar = tqdm.tqdm(total=max_steps, position=0)\n",
    "        while steps < max_steps:\n",
    "            for x in dataloader:\n",
    "                x['step'] = torch.ones(1).to(device) * steps\n",
    "                x = [x[k].squeeze(0).to(device) if k in x.keys() else torch.zeros(1) for k in self.args]\n",
    "                if self.scale_factor == 1.:\n",
    "                    self.fix_scale_factor(svi, x)\n",
    "                pbar.update(1)\n",
    "                loss = svi.step(*x)\n",
    "                steps += 1\n",
    "                if hasattr(scheduler, 'step'):\n",
    "                    scheduler.step()\n",
    "                if steps >= max_steps - 1 :\n",
    "                    break\n",
    "                \n",
    "                self.losses.append(loss)\n",
    "                if steps % print_every == 0:\n",
    "                    pbar.write(f\"[Step {steps:02d}]  Loss: {np.mean(self.losses[-print_every:]):.5f}\")\n",
    "        pbar.close()\n",
    "        try:\n",
    "            self.allDone()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def setup_scheduler(self, max_learning_rate, max_steps, one_cycle_lr):\n",
    "        if one_cycle_lr:\n",
    "            return pyro.optim.OneCycleLR({\n",
    "                'max_lr': max_learning_rate,\n",
    "                'total_steps': max_steps,\n",
    "                'div_factor': 100,\n",
    "                'optim_args': {},\n",
    "                'optimizer': torch.optim.Adam\n",
    "            })\n",
    "        else:\n",
    "            return pyro.optim.ClippedAdam({\n",
    "                'lr': max_learning_rate,\n",
    "                'lrd': (1 - (5e-6))\n",
    "            })\n",
    "\n",
    "    def train_phase(self, phase, max_steps, print_every=10000, device='cuda', max_learning_rate=0.001, num_particles=1, one_cycle_lr=True, steps=0, batch_size=32,freeze_encoder=None):\n",
    "        self.scale_factor=1.\n",
    "        freeze_encoder = True if freeze_encoder is None and phase == 2 else freeze_encoder\n",
    "        freeze_encoder = False if freeze_encoder is None else  freeze_encoder\n",
    "        self.set_freeze_encoder(freeze_encoder) \n",
    "        supervised_field_types=self.field_types.copy()\n",
    "        supervised_fields=self.fields.copy()\n",
    "        supervised_field_types[\"taxon\"]=np.float32\n",
    "        if not freeze_encoder and (\"Z_obs\" in [x.registry_key for x in  self.adata_manager.fields]) and phase == 2: #Running supervised D.R. (can't freeze encoder and run d.r.)\n",
    "            supervised_field_types[\"Z_obs\"]=np.float32\n",
    "        field_types=self.field_types if phase != 2 else supervised_field_types\n",
    "        sampler=create_weighted_random_sampler(self.adata_manager.adata.obs[self.sampler_category]) if self.sampler_category is not None else create_weighted_random_sampler(pd.Series([\"same_category\"] * self.adata_manager.adata.shape[0]))\n",
    "        sampler= torch.utils.data.BatchSampler(sampler=sampler,batch_size=batch_size,drop_last=True)\n",
    "        dataloader = scvi.dataloaders.AnnDataLoader(self.adata_manager, batch_size=batch_size, drop_last=True, sampler=sampler, data_and_attributes=field_types)\n",
    "        scheduler = self.setup_scheduler(max_learning_rate, max_steps, one_cycle_lr)\n",
    "        elbo_class = pyro.infer.JitTrace_ELBO\n",
    "        elbo = elbo_class(num_particles=num_particles, strict_enumeration_warning=False)\n",
    "        hide_params=[name for name in pyro.get_param_store() if re.search('encoder',name)]\n",
    "        guide=self.guide if not self.freeze_encoder else poutine.block(self.guide,hide=hide_params)\n",
    "        svi = SafeSVI(self.model, guide, scheduler, elbo,clip_std_multiplier=6.0)  \n",
    "        self.train()\n",
    "        self.zl_encoder.eval() if self.freeze_encoder else self.zl_encoder.train()\n",
    "        self = self.to(device)\n",
    "        self.set_approx(phase == 1)\n",
    "        return self.common_training_loop(dataloader, max_steps, scheduler, svi, print_every, device, steps)\n",
    "        \n",
    "    def allDone(self):\n",
    "        print(\"Finished training!\")\n",
    "        self.to('cpu')\n",
    "        try:\n",
    "            import IPython\n",
    "            from IPython.display import Audio, display\n",
    "            IPython.display.clear_output()#Make compatible with jupyter nbconvert\n",
    "            display(Audio(url='https://notification-sounds.com/soundsfiles/Meditation-bell-sound.mp3', autoplay=True))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def clear_cuda(self):\n",
    "        '''Throw the kitchen sink at clearing the cuda cache for jupyter notebooks. \n",
    "        Might want to wrap in tryexcept'''\n",
    "        import traceback\n",
    "        self.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        try:\n",
    "            a = 1/0 \n",
    "        except Exception as e:  \n",
    "            exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "            traceback.clear_frames(exc_traceback)\n",
    "\n",
    "class AntipodeSaveLoadMixin:\n",
    "    '''Directly taken and modified from scvi-tools base_model and auxiliary functions'''\n",
    "    def _get_user_attributes(self):\n",
    "        \"\"\"Returns all the self attributes defined in a model class, e.g., `self.is_trained_`.\"\"\"\n",
    "        attributes = inspect.getmembers(self, lambda a: not (inspect.isroutine(a)))\n",
    "        attributes = [a for a in attributes if not (a[0].startswith(\"__\") and a[0].endswith(\"__\"))]\n",
    "        attributes = [a for a in attributes if not a[0].startswith(\"_abc_\")]\n",
    "        return attributes\n",
    "\n",
    "    @classmethod\n",
    "    def _initialize_model(cls, adata, attr_dict,param_store_path,device):\n",
    "        \"\"\"Helper to initialize a model.\"\"\"\n",
    "        try:\n",
    "            attr_dict.pop('__class__')\n",
    "        except:\n",
    "            pass\n",
    "        model = cls(adata, **attr_dict)\n",
    "        \n",
    "        pyro.get_param_store().load(param_store_path,map_location=device)\n",
    "        for k in list(pyro.get_param_store()):\n",
    "            if '$$$' in k:\n",
    "                pyro.get_param_store().__delitem__(k)\n",
    "        return model\n",
    "\n",
    "    def save(\n",
    "        self,\n",
    "        dir_path: str,\n",
    "        prefix: str | None = None,\n",
    "        overwrite: bool = False,\n",
    "        save_anndata: bool = False,\n",
    "        save_kwargs: dict | None = None,\n",
    "        **anndata_write_kwargs,\n",
    "    ):\n",
    "        \"\"\"Save the state of the model.\n",
    "\n",
    "        Neither the trainer optimizer state nor the trainer history are saved.\n",
    "        Model files are not expected to be reproducibly saved and loaded across versions\n",
    "        until we reach version 1.0.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dir_path\n",
    "            Path to a directory.\n",
    "        prefix\n",
    "            Prefix to prepend to saved file names.\n",
    "        overwrite\n",
    "            Overwrite existing data or not. If `False` and directory\n",
    "            already exists at `dir_path`, error will be raised.\n",
    "        save_anndata\n",
    "            If True, also saves the anndata\n",
    "        save_kwargs\n",
    "            Keyword arguments passed into :func:`~torch.save`.\n",
    "        anndata_write_kwargs\n",
    "            Kwargs for :meth:`~anndata.AnnData.write`\n",
    "        \"\"\"\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path, exist_ok=overwrite)\n",
    "\n",
    "        file_name_prefix = prefix or \"\"\n",
    "        save_kwargs = save_kwargs or {}\n",
    "\n",
    "        model_save_path = os.path.join(dir_path, f\"{file_name_prefix}model.pt\")\n",
    "\n",
    "        # save the model state dict and the trainer state dict only\n",
    "        model_state_dict = self.state_dict()\n",
    "        \n",
    "        var_names = self.adata_manager.adata.var_names.astype(str)\n",
    "        var_names = var_names.to_numpy()\n",
    "\n",
    "        user_attributes = self.init_args\n",
    "        try:\n",
    "            user_attributes.pop('adata')\n",
    "            user_attributes.pop('self')\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        pyro.get_param_store().save(os.path.join(dir_path,prefix+'antipode.paramstore'))\n",
    "\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": model_state_dict,\n",
    "                \"var_names\": var_names,\n",
    "                \"attr_dict\": user_attributes,\n",
    "            },\n",
    "            model_save_path,\n",
    "            **save_kwargs,\n",
    "        )\n",
    "        \n",
    "        if save_anndata:\n",
    "            file_suffix = \"\"\n",
    "            if isinstance(self.adata_manager.adata, AnnData):\n",
    "                file_suffix = \"adata.h5ad\"\n",
    "            elif isinstance(self.adata_manager.adata, MuData):\n",
    "                file_suffix = \"mdata.h5mu\"\n",
    "            self.adata_manager.adata.write_h5ad(\n",
    "                os.path.join(dir_path, f\"{file_name_prefix}{file_suffix}\"),\n",
    "                **anndata_write_kwargs,\n",
    "            )\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def _validate_var_names(cls,adata, source_var_names):\n",
    "        user_var_names = adata.var_names.astype(str)\n",
    "        if not np.array_equal(source_var_names, user_var_names):\n",
    "            warnings.warn(\n",
    "                \"var_names for adata passed in does not match var_names of adata used to \"\n",
    "                \"train the model. For valid results, the vars need to be the same and in \"\n",
    "                \"the same order as the adata used to train the model.\",\n",
    "                UserWarning,\n",
    "                stacklevel=settings.warnings_stacklevel,\n",
    "            )    \n",
    "    \n",
    "    @classmethod\n",
    "    def _load_saved_files(\n",
    "        cls,\n",
    "        dir_path: str,\n",
    "        load_adata: bool,\n",
    "        prefix: Optional[str] = None,\n",
    "        is_mudata = False,\n",
    "        load_kw_args = {'backed':'r'}\n",
    "    ) -> tuple[dict, np.ndarray, dict, AnnData]:\n",
    "        \"\"\"Helper to load saved files.\"\"\"\n",
    "        file_name_prefix = prefix or \"\"\n",
    "    \n",
    "        model_file_name = f\"{file_name_prefix}model.pt\"\n",
    "        model_path = os.path.join(dir_path, model_file_name)\n",
    "        try:\n",
    "            model = torch.load(model_path)\n",
    "        except FileNotFoundError as exc:\n",
    "            raise ValueError(\n",
    "                f\"Failed to load model file at {model_path}. \"\n",
    "                \"If attempting to load a saved model from <v0.15.0, please use the util function \"\n",
    "                \"`convert_legacy_save` to convert to an updated format.\"\n",
    "            ) from exc\n",
    "    \n",
    "        model_state_dict = model[\"model_state_dict\"]\n",
    "        var_names = model[\"var_names\"]\n",
    "        attr_dict = model[\"attr_dict\"]\n",
    "    \n",
    "        if load_adata:\n",
    "            file_suffix = \"adata.h5ad\"\n",
    "            adata_path = os.path.join(dir_path, f\"{file_name_prefix}{file_suffix}\")\n",
    "            if os.path.exists(adata_path):\n",
    "                if is_mudata:\n",
    "                    adata = mudata.read(adata_path,**load_kw_args)\n",
    "                else:\n",
    "                    adata = anndata.read_h5ad(adata_path,**load_kw_args)\n",
    "            else:\n",
    "                raise ValueError(\"Save path contains no saved anndata and no adata was passed.\")\n",
    "        else:\n",
    "            adata = None\n",
    "\n",
    "        return attr_dict, var_names, model_state_dict, adata\n",
    "        \n",
    "    @classmethod\n",
    "    def load(\n",
    "        cls,\n",
    "        dir_path: str,\n",
    "        adata = None,\n",
    "        accelerator: str = \"auto\",\n",
    "        device: int | str = \"auto\",\n",
    "        prefix: str | None = None,\n",
    "        is_mudata: bool = False, \n",
    "        load_kw_args = {'backed':'r'}\n",
    "    ):\n",
    "        \"\"\"Instantiate a model from the saved output.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dir_path\n",
    "            Path to saved outputs.\n",
    "        adata\n",
    "            AnnData organized in the same way as data used to train model.\n",
    "            It is not necessary to run setup_anndata,\n",
    "            as AnnData is validated against the saved `scvi` setup dictionary.\n",
    "            If None, will check for and load anndata saved with the model.\n",
    "        %(param_accelerator)s\n",
    "        %(param_device)s\n",
    "        prefix\n",
    "            Prefix of saved file names.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Model with loaded state dictionaries.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> model = ModelClass.load(save_path, adata)\n",
    "        >>> model.get_....\n",
    "        \"\"\"\n",
    "        load_adata = adata is None\n",
    "\n",
    "        (\n",
    "            attr_dict,\n",
    "            var_names,\n",
    "            model_state_dict,\n",
    "            new_adata,\n",
    "        ) = cls._load_saved_files(\n",
    "            dir_path,\n",
    "            load_adata,\n",
    "            prefix=prefix,\n",
    "            is_mudata=is_mudata,\n",
    "            load_kw_args=load_kw_args,\n",
    "        )\n",
    "        \n",
    "        adata = new_adata if new_adata is not None else adata\n",
    "\n",
    "        cls._validate_var_names(adata, var_names)\n",
    "        \n",
    "        \n",
    "        model = cls._initialize_model(adata, attr_dict,os.path.join(dir_path,prefix+'antipode.paramstore'),device=device)\n",
    "        model.load_state_dict(model_state_dict)\n",
    "        model.eval()\n",
    "        #,os.path.join(dir_path,'antipode.paramstore')\n",
    "        #model._validate_anndata(adata)\n",
    "        return model\n",
    "\n",
    "\n",
    "class ANTIPODE(PyroBaseModuleClass,AntipodeTrainingMixin, AntipodeSaveLoadMixin):\n",
    "    \"\"\"\n",
    "    ANTIPODE (Single Cell Ancestral Node Taxonomy Inference by Parcellation of Differential Expression) \n",
    "    leverages variational inference for analyzing and categorizing cell types by accounting for biological and batch covariates and discrete and continuous latent variables. This model works by simultaneously integrating evolution-inspired differential expression parcellation, taxonomy generation (clustering) and batch correction.\n",
    "\n",
    "    Parameters:\n",
    "    adata (AnnData): The single-cell dataset encapsulated in an AnnData object.\n",
    "    discov_pair (tuple): Specifies the discovery covariate's key and its location ('obs' or 'obsm') in the AnnData object.\n",
    "    batch_pair (tuple): Specifies the batch covariate's key and its location ('obs' or 'obsm') in the AnnData object.\n",
    "    layer (str): The specific layer of the AnnData object to be analyzed.\n",
    "    level_sizes (list of int): Defines the hierarchical model structure (corresponding to a layered tree) by specifying the size of each level. Make sure each layer gets progressively larger and ideally start with a single root. Defaults to [1, 10, 100].\n",
    "    bi_depth (int): Tree depth (from root) for batch identity effect correction. Defaults to 2. Should be less than length of level_sizes\n",
    "    psi_levels (list of bool): Whether or not to allow a psi at each level of the layered tree. Should be length 1 (all levels) or len(level_sizes)\n",
    "    \n",
    "    num_latent (int): The number of latent dimensions to model. Defaults to 50.\n",
    "    num_batch_embed (int): Number of embedding dimensions for batch effects. Defaults to 10. \n",
    "    scale_factor (float, optional): Factor for scaling the data normalization. Inferred from data if None. [DANGER]\n",
    "    prior_scale (float): Scale for the Laplace prior distributions. Defaults to 100. [DANGER]\n",
    "    dcd_prior (float, optional): Scale for discov_constitutive_de. Use this for missing genes (set to large negative value and rest 0. Zeros if None.\n",
    "    use_psi (bool): Whether to utilize psi continuous variation parameter. Defaults to True.\n",
    "    use_q_score (bool): Whether to use q continuous \"quality\" scores. Defaults to True.\n",
    "    dist_normalize (bool): EXPERIMENTAL. Whether to apply distance normalization. Defaults to False.\n",
    "    z_transform (pytorch function): Function to be applied to latent space (Z) e.g. centered_sigmoid, sigmoid. This will mess up DE Parameter scaling.\n",
    "    loc_as_param, zdw_as_param, intercept_as_param (bool): Flags for using location, Z decoder weight, and intercept as parameters instead (maximum likelihood inference instead of Laplace MAP), respectively. All default to False.\n",
    "    theta_prior (float): Initial value for the inverse dispersion of the negative binomial. Defaults to 50. [DANGER]\n",
    "    scale_init_val (float): Initial value for scaling parameters. Defaults to 0.01. [DANGER]\n",
    "    classifier_hidden, encoder_hidden, batch_embedder_hidden (list of int): Sizes of hidden layers for the classifier, encoder and batch embedding networks, respectively.\n",
    "    sampler_category (string): Obs categorical column which will be used with the dataloader to sample each category with equal probability. (suggested use is the discov category)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, adata, discov_pair, batch_pair, layer, level_sizes=[1,10,100],\n",
    "                 num_latent=50,scale_factor=None, prior_scale=100,dcd_prior=None,\n",
    "                 use_psi=True,loc_as_param=False,zdw_as_param=False,intercept_as_param=False,use_q_score=True,psi_levels=[True],\n",
    "                 num_batch_embed=10,theta_prior=50.,scale_init_val=0.01,bi_depth=2,dist_normalize=False,z_transform=None,\n",
    "                 classifier_hidden=[3000,3000,3000],encoder_hidden=[6000,5000,3000,1000],batch_embedder_hidden=[1000,500,500],\n",
    "                 sampler_category=None):\n",
    "\n",
    "        pyro.clear_param_store()\n",
    "        self.init_args = dict(locals())\n",
    "        # Determine num_discov and num_batch from the AnnData object\n",
    "        self.discov_loc, self.discov_key = discov_pair\n",
    "        self.batch_loc, self.batch_key = batch_pair\n",
    "        self.num_discov = adata.obsm[self.discov_key].shape[-1] if self.discov_loc == 'obsm' else len(adata.obs[self.discov_key].unique())\n",
    "        self.num_batch = adata.obsm[self.batch_key].shape[-1] if self.batch_loc == 'obsm' else len(adata.obs[self.batch_key].unique())\n",
    "        self.design_matrix = (self.discov_loc == 'obsm')\n",
    "        self.layer = layer\n",
    "\n",
    "        self._setup_adata_manager_store: dict[str, type[scvi.data.AnnDataManager]] = {}\n",
    "        self.num_var = adata.layers[layer].shape[-1]\n",
    "        self.num_latent = num_latent\n",
    "        self.scale_factor = 1.0#scale_factor if scale_factor is not None else 2e2 / (self.num_var * num_particles * num_latent)\n",
    "        self.num_batch_embed = num_batch_embed\n",
    "        self.temperature = 0.1\n",
    "        self.epsilon = 1e-5\n",
    "        self.approx = False\n",
    "        self.prior_scale = prior_scale\n",
    "        self.use_psi = use_psi\n",
    "        self.use_q_score = use_q_score\n",
    "        self.loc_as_param = loc_as_param\n",
    "        self.zdw_as_param = zdw_as_param\n",
    "        self.intercept_as_param = intercept_as_param\n",
    "        self.theta_prior = theta_prior\n",
    "        self.scale_init_val = scale_init_val\n",
    "        self.level_sizes = level_sizes\n",
    "        self.num_labels = sum(level_sizes)\n",
    "        self.bi_depth = bi_depth\n",
    "        self.bi_depth = sum(self.level_sizes[:self.bi_depth])\n",
    "        self.dist_normalize = dist_normalize\n",
    "        self.sampler_category = sampler_category\n",
    "        self.psi_levels = [float(x) for x in psi_levels]\n",
    "\n",
    "        self.dcd_prior = torch.zeros((self.num_discov,self.num_var)) if dcd_prior is None else dcd_prior#Use this for \n",
    "                \n",
    "        # Initialize plates to be used during sampling\n",
    "        self.var_plate = pyro.plate('var_plate',self.num_var,dim=-1)\n",
    "        self.discov_plate = pyro.plate('discov_plate',self.num_discov,dim=-3)\n",
    "        self.batch_plate = pyro.plate('batch_plate',self.num_batch,dim=-3)\n",
    "        self.latent_plate = pyro.plate('latent_plate',self.num_latent,dim=-1)\n",
    "        self.latent_plate2 = pyro.plate('latent_plate2',self.num_latent,dim=-2)\n",
    "        self.label_plate = pyro.plate('label_plate',self.num_labels,dim=-2)\n",
    "        self.batch_embed_plate = pyro.plate('batch_embed_plate',self.num_batch_embed,dim=-3)\n",
    "        self.bi_depth_plate = pyro.plate('bi_depth_plate',self.bi_depth,dim=-2)\n",
    "\n",
    "        #Initialize MAP inference modules\n",
    "        self.dm=MAPLaplaceModule(self,'discov_dm',[self.num_discov,self.num_labels,self.num_latent],[self.discov_plate,self.label_plate,self.latent_plate])\n",
    "        self.bm=MAPLaplaceModule(self,'batch_dm',[self.num_batch,self.num_labels,self.num_latent],[self.batch_plate,self.label_plate,self.latent_plate])\n",
    "        self.di=MAPLaplaceModule(self,'discov_di',[self.num_discov,self.num_labels,self.num_var],[self.discov_plate,self.label_plate,self.var_plate])\n",
    "        self.bei=MAPLaplaceModule(self,'batch_di',[self.num_batch_embed,self.bi_depth,self.num_var],[self.batch_embed_plate,self.bi_depth_plate,self.var_plate])\n",
    "        self.ci=MAPLaplaceModule(self,'cluster_intercept',[self.num_labels, self.num_var],[self.label_plate,self.var_plate],param_only=self.intercept_as_param)\n",
    "        self.dc=MAPLaplaceModule(self,'discov_dc',[self.num_discov,self.num_latent,self.num_var],[self.discov_plate,self.latent_plate2,self.var_plate])\n",
    "        self.zdw=MAPLaplaceModule(self,'z_decoder_weight',[self.num_latent,self.num_var],[self.latent_plate2,self.var_plate],init_val=((2/self.num_latent)*(torch.rand(self.num_latent,self.num_var)-0.5)),param_only=self.zdw_as_param)\n",
    "        self.zl=MAPLaplaceModule(self,'locs',[self.num_labels,self.num_latent],[self.label_plate,self.latent_plate],param_only=self.loc_as_param)\n",
    "        self.zs=MAPHalfCauchyModule(self,'scales',[self.num_labels,self.num_latent],[self.label_plate,self.latent_plate],init_val=self.scale_init_val*torch.ones(self.num_labels,self.num_latent),constraint=constraints.positive,param_only=False)\n",
    "        self.zld=MAPLaplaceModule(self,'locs_dynam',[self.num_labels,self.num_latent],[self.label_plate,self.latent_plate],param_only=False)\n",
    "        self.qg=MAPLaplaceModule(self,'quality_genes',[1,self.num_var],[self.var_plate],param_only=False)\n",
    "        \n",
    "        self.tree_edges = TreeEdges(self,straight_through=False)\n",
    "        self.tree_convergence_bottom_up = TreeConvergenceBottomUp(self)        \n",
    "        self.z_transform = null_function if z_transform is None else z_transform#centered_sigmoid#torch.special.expit\n",
    "\n",
    "        if self.design_matrix:\n",
    "            fields={'s':('layers',self.layer),\n",
    "            'discov_ind':('obsm',self.discov_key),\n",
    "            'batch_ind':('obsm',self.batch_key)}\n",
    "            field_types={\"s\":np.float32,\"batch_ind\":np.float32,\"discov_ind\":np.float32}\n",
    "        else:\n",
    "            fields={'s':('layers',self.layer),\n",
    "            'discov_ind':('obs',self.discov_key),\n",
    "            'batch_ind':('obs',self.batch_key)}\n",
    "            field_types={\"s\":np.float32,\"batch_ind\":np.int64,\"discov_ind\":np.int64}\n",
    "\n",
    "        self.fields=fields\n",
    "        self.field_types=field_types\n",
    "        self.setup_anndata(adata, {'discov_ind': discov_pair, 'batch_ind': batch_pair}, self.field_types)\n",
    "        \n",
    "        super().__init__()\n",
    "        # Setup the various neural networks used in the model and guide\n",
    "        self.z_decoder=ZDecoder(num_latent=self.num_latent, num_var=self.num_var)        \n",
    "        self.zl_encoder=ZLEncoder(num_var=self.num_var,hidden_dims=encoder_hidden,num_cat_input=self.num_discov,\n",
    "                    outputs=[(self.num_latent,None),(self.num_latent,softplus),(1,None),(1,softplus)])\n",
    "        \n",
    "        self.classifier=Classifier(num_latent=self.num_latent,hidden_dims=classifier_hidden,\n",
    "                    outputs=[(self.num_labels,None),(len(self.level_sizes),None),(len(self.level_sizes),softplus)])\n",
    "\n",
    "        #Too large to exactly model gene-level batch effects for all cluster x batch\n",
    "        self.be_nn=SimpleFFNN(in_dim=self.num_batch,hidden_dims=batch_embedder_hidden,\n",
    "                    out_dim=self.num_batch_embed)\n",
    "        \n",
    "        self.epsilon = 0.006\n",
    "        #Initialize model not in fuzzy mode\n",
    "        self.approx=False\n",
    "        self.prior_scale=prior_scale\n",
    "        self.args=inspect.getfullargspec(self.model).args[1:]#skip self\n",
    "\n",
    "    def setup_anndata(self,adata: anndata.AnnData,fields,field_types,**kwargs,):\n",
    "        \n",
    "        anndata_fields=[make_field(x,self.fields[x]) for x in self.fields.keys()]\n",
    "            \n",
    "        adata_manager = scvi.data.AnnDataManager(\n",
    "            fields=anndata_fields\n",
    "        )\n",
    "        adata_manager.register_fields(adata, **kwargs)\n",
    "        self.register_manager(adata_manager)\n",
    "        if fields['discov_ind'][0]=='obsm':\n",
    "            self.design_matrix=True\n",
    "            if fields['batch_ind'][0]!='obsm':\n",
    "                raise Exception(\"If discov is design matrix, batch must be as well!\")\n",
    "\n",
    "    def register_manager(self, adata_manager: scvi.data.AnnDataManager):\n",
    "        adata_id = adata_manager.adata_uuid\n",
    "        self._setup_adata_manager_store[adata_id] = adata_manager\n",
    "        self.adata_manager=adata_manager\n",
    "    \n",
    "    def set_approx(self,b: bool):\n",
    "        self.approx=b\n",
    "\n",
    "    def set_freeze_encoder(self,b: bool):\n",
    "        self.freeze_encoder=b\n",
    "        \n",
    "    def model(self, s,discov_ind=torch.zeros(1),batch_ind=torch.zeros(1),step=torch.ones(1),taxon=torch.zeros(1),Z_obs=torch.zeros(1)):\n",
    "        # Register various nn.Modules (i.e. the decoder/encoder networks) with Pyro\n",
    "        pyro.module(\"antipode\", self)\n",
    "\n",
    "        if not self.design_matrix:\n",
    "            batch=index_to_onehot(batch_ind,[s.shape[0],self.num_batch]).to(s.device)\n",
    "            discov=index_to_onehot(discov_ind,[s.shape[0],self.num_discov]).to(s.device)\n",
    "            batch_ind=batch_ind.squeeze()\n",
    "            discov_ind=discov_ind.squeeze()\n",
    "        else:\n",
    "            batch=batch_ind\n",
    "            discov=discov_ind\n",
    "        \n",
    "        minibatch_plate=pyro.plate(\"minibatch_plate\", s.shape[0],dim=-1)\n",
    "        minibatch_plate2=pyro.plate(\"minibatch_plate2\", s.shape[0],dim=-2)\n",
    "        l = s.sum(1).unsqueeze(-1)\n",
    "        \n",
    "        # Scale all sample statements for numerical stability\n",
    "        with poutine.scale(scale=self.scale_factor):\n",
    "            # Counts parameter of NB (variance of the observation distribution)\n",
    "            s_theta = pyro.param(\"s_inverse_dispersion\", self.theta_prior * s.new_ones(self.num_var),\n",
    "                               constraint=constraints.positive)\n",
    "            #Weak overall histogram normalization\n",
    "            discov_mul = pyro.param(\"discov_mul\", s.new_ones(self.num_discov,1),constraint=constraints.positive) if self.dist_normalize else s.new_ones(self.num_discov,1)\n",
    "\n",
    "            dcd=pyro.param(\"discov_constitutive_de\", self.dcd_prior.to(s.device))\n",
    "            level_edges=self.tree_edges.model_sample(s,approx=self.approx)\n",
    "            quality_genes=self.qg.model_sample(s) if self.use_q_score else 0.\n",
    "            \n",
    "            with minibatch_plate:\n",
    "                batch_embed=centered_sigmoid(pyro.sample('batch_embed', dist.Laplace(s.new_zeros(self.num_batch_embed),\n",
    "                                self.prior_scale*s.new_ones(self.num_batch_embed),validate_args=True).to_event(1)))\n",
    "                beta_prior_a=1.*s.new_ones(self.num_labels)\n",
    "                beta_prior_a[0]=10. #0 block is consititutive\n",
    "                if self.approx:#Bernoulli blocks approx?\n",
    "                    taxon_probs = pyro.sample(\"taxon_probs\", dist.Beta(beta_prior_a,s.new_ones(self.num_labels),validate_args=True).to_event(1))\n",
    "                    taxon = pyro.sample('taxon',dist.RelaxedBernoulli(temperature=0.1*s.new_ones(1),probs=taxon_probs).to_event(1))\n",
    "                else:\n",
    "                    taxon_probs=pyro.sample('taxon_probs',dist.Dirichlet(s.new_ones(s.shape[0],self.level_sizes[-1]),validate_args=True))\n",
    "                    if sum(taxon.shape) > 1:#Supervised?\n",
    "                        if taxon.shape[-1]==self.num_labels:#Totally supervised?\n",
    "                            pass\n",
    "                        else:#Only bottom layer is supervised?\n",
    "                            taxon = taxon_probs = pyro.sample(\"taxon\", dist.OneHotCategorical(probs=taxon_probs,validate_args=True),obs=taxon)\n",
    "                            taxon = self.tree_convergence_bottom_up.just_propagate(taxon,level_edges,s) if self.freeze_encoder else self.tree_convergence_bottom_up.just_propagate(taxon,level_edges,s)\n",
    "                    else:#Unsupervised\n",
    "                        taxon = pyro.sample(\"taxon\", \n",
    "                                         model_distributions.SafeAndRelaxedOneHotCategorical(temperature=self.temperature*s.new_ones(1),probs=taxon_probs,validate_args=True))                    \n",
    "                        taxon = self.tree_convergence_bottom_up.just_propagate(taxon,level_edges,s) if self.freeze_encoder else self.tree_convergence_bottom_up.just_propagate(taxon,level_edges,s)\n",
    "                    taxon = torch.concat(taxon,-1)\n",
    "                    taxon_probs=self.tree_convergence_bottom_up.just_propagate(taxon_probs[...,-self.level_sizes[-1]:],level_edges,s) if self.freeze_encoder else self.tree_convergence_bottom_up.just_propagate(taxon_probs[...,-self.level_sizes[-1]:],level_edges,s)\n",
    "                    taxon_probs=torch.cat(taxon_probs,-1)\n",
    "                   \n",
    "            locs=self.zl.model_sample(s,scale=fest([taxon_probs],-1))\n",
    "            scales=self.zs.model_sample(s,scale=fest([taxon_probs],-1))\n",
    "            locs_dynam=self.zld.model_sample(s,scale=fest([taxon_probs],-1))\n",
    "            discov_dm=self.dm.model_sample(s,scale=fest([discov,taxon_probs],-1))\n",
    "            discov_di=self.di.model_sample(s,scale=fest([discov,taxon_probs],-1))\n",
    "            batch_dm=self.bm.model_sample(s,scale=fest([batch,taxon_probs],-1))\n",
    "            \n",
    "            bei=self.bei.model_sample(s,scale=fest([batch_embed.abs(),taxon_probs[...,:self.bi_depth]],-1))\n",
    "            cluster_intercept=self.ci.model_sample(s,scale=fest([taxon_probs],-1))\n",
    "            \n",
    "            with minibatch_plate:\n",
    "                bi=torch.einsum('...bi,...ijk->...bjk',batch_embed,bei)\n",
    "                bi=torch.einsum('...bj,...bjk->...bk',taxon[...,:self.bi_depth],bi)\n",
    "                psi = centered_sigmoid(pyro.sample('psi',dist.Laplace(s.new_zeros(s.shape[0],len(self.level_sizes)),self.prior_scale*s.new_ones(s.shape[0],len(self.level_sizes))).to_event(1)))\n",
    "                #psi = centered_sigmoid(pyro.sample('psi',dist.Logistic(s.new_zeros(s.shape[0],len(self.level_sizes)),s.new_ones(s.shape[0],len(self.level_sizes))).to_event(1)))\n",
    "                psi=psi*torch.tensor(self.psi_levels).to(s.device).unsqueeze(0)\n",
    "                psi = 0 if not self.use_psi or self.approx else torch.repeat_interleave(psi, torch.tensor(self.level_sizes).to(s.device), dim=1)\n",
    "                q = torch.sigmoid(pyro.sample('q',dist.Logistic(s.new_zeros(s.shape[0],1),s.new_ones(s.shape[0],1)).to_event(1))) if self.use_q_score else 1.0\n",
    "                this_locs=oh_index(locs,taxon)\n",
    "                this_scales=oh_index(scales,taxon)\n",
    "                z=pyro.sample('z_loc',dist.Laplace(this_locs,0.5*self.prior_scale*s.new_ones(s.shape[0],self.num_latent),validate_args=True).to_event(1))\n",
    "                z_dist=dist.Normal(this_locs,this_scales+self.epsilon,validate_args=True).to_event(1)\n",
    "                if sum(Z_obs.shape) <=1: \n",
    "                     z=pyro.sample('z', z_dist) \n",
    "                else: #Supervised latent space\n",
    "                    z=pyro.sample('z', z_dist)\n",
    "                    z=pyro.sample('z_obs', dist.Normal(z,this_scales+self.epsilon,validate_args=True).to_event(1),obs=Z_obs)\n",
    "\n",
    "            cur_discov_dm = oh_index1(discov_dm, discov_ind) if self.design_matrix else discov_dm[discov_ind]\n",
    "            cur_batch_dm = oh_index1(batch_dm, batch_ind) if self.design_matrix else batch_dm[batch_ind]\n",
    "            cur_dcd = oh_index(dcd, discov) if self.design_matrix else  dcd[discov_ind]\n",
    "            \n",
    "            z=z+oh_index2(cur_discov_dm,taxon) + oh_index2(cur_batch_dm,taxon)+(oh_index(locs_dynam,taxon*psi))\n",
    "            z=self.z_transform(z)                \n",
    "            pseudo_z=oh_index(locs,taxon_probs)+oh_index2(discov_dm[discov_ind],taxon_probs) + oh_index2(batch_dm[batch_ind],taxon_probs)+(oh_index(locs_dynam,taxon_probs*psi))\n",
    "            pseudo_z=self.z_transform(pseudo_z)\n",
    "            z_decoder_weight=self.zdw.model_sample(s,scale=fest([pseudo_z.abs()],-1))\n",
    "            discov_dc=self.dc.model_sample(s,scale=fest([discov,pseudo_z.abs()],-1))\n",
    "            cur_discov_di = oh_index1(discov_di, discov_ind) if self.design_matrix else discov_di[discov_ind]\n",
    "            cur_discov_dc = oh_index1(discov_dc, discov_ind) if self.design_matrix else discov_dc[discov_ind]\n",
    "            cur_discov_di=oh_index2(cur_discov_di,taxon)\n",
    "            cur_cluster_intercept=oh_index(cluster_intercept,taxon) if not self.approx else 0.\n",
    "            \n",
    "            mu=torch.einsum('...bi,...bij->...bj',z,z_decoder_weight+cur_discov_dc)#+bc\n",
    "            spliced_mu=mu+cur_dcd+cur_discov_di+cur_cluster_intercept+bi+((1-q)*quality_genes)\n",
    "            norm_spliced_mu=spliced_mu*discov_mul[discov_ind]\n",
    "            spliced_out=softmax(norm_spliced_mu,dim=-1)\n",
    "            log_mu = (l * spliced_out + 1e-6).log()\n",
    "            \n",
    "            with self.var_plate,minibatch_plate2:\n",
    "                s_dist = dist.NegativeBinomial(total_count=(s_theta*q)+0.1,logits=log_mu-s_theta.log(),validate_args=True)\n",
    "                s_out=pyro.sample(\"s\", s_dist, obs=s.int())\n",
    "\n",
    "    \n",
    "    # The guide specifies the variational distribution\n",
    "    def guide(self, s,discov_ind=torch.zeros(1),batch_ind=torch.zeros(1),step=torch.ones(1),taxon=torch.zeros(1),Z_obs=torch.zeros(1)):\n",
    "        pyro.module(\"antipode\", self)\n",
    "        \n",
    "        if not self.design_matrix:\n",
    "            batch=index_to_onehot(batch_ind,[s.shape[0],self.num_batch]).to(s.device)\n",
    "            discov=index_to_onehot(discov_ind,[s.shape[0],self.num_discov]).to(s.device)\n",
    "            batch_ind=batch_ind.squeeze()\n",
    "            discov_ind=discov_ind.squeeze()\n",
    "        else:\n",
    "            batch=batch_ind\n",
    "            discov=discov_ind\n",
    "        \n",
    "        minibatch_plate=pyro.plate(\"minibatch_plate\", s.shape[0])\n",
    "        \n",
    "        with poutine.scale(scale=self.scale_factor):\n",
    "            level_edges=self.tree_edges.guide_sample(s,approx=self.approx) \n",
    "            with minibatch_plate:\n",
    "                batch_embed=self.be_nn(batch)\n",
    "                batch_embed=centered_sigmoid(pyro.sample('batch_embed', dist.Delta(batch_embed,validate_args=True).to_event(1)))\n",
    "                if self.freeze_encoder:\n",
    "                    with torch.no_grad():\n",
    "                        z_loc, z_scale , q_loc,q_scale= self.zl_encoder(s,discov)\n",
    "                        z_loc=z_loc.detach()\n",
    "                        z_scale=z_scale.detach()\n",
    "                        q_loc=q_loc.detach()\n",
    "                        q_scale=q_scale.detach()\n",
    "                else:\n",
    "                    z_loc, z_scale, q_loc,q_scale= self.zl_encoder(s,discov)\n",
    "                z=pyro.sample('z',dist.Normal(z_loc,z_scale+self.epsilon).to_event(1))\n",
    "                q=pyro.sample('q',dist.Normal(q_loc,q_scale+self.epsilon).to_event(1))\n",
    "                pyro.sample('z_loc',dist.Delta(z_loc).to_event(1))\n",
    "                z=self.z_transform(z)\n",
    "                taxon_logits,psi_loc,psi_scale=self.classifier(z)\n",
    "                #psi=centered_sigmoid(pyro.sample('psi_loc',dist.Delta(psi_loc).to_event(1)))\n",
    "                psi=centered_sigmoid(pyro.sample('psi',dist.Normal(psi_loc,psi_scale+self.epsilon).to_event(1)))\n",
    "                psi=psi*torch.tensor(self.psi_levels).to(s.device).unsqueeze(0)\n",
    "                psi = 0 if not self.use_psi or self.approx else torch.repeat_interleave(psi, torch.tensor(self.level_sizes).to(s.device), dim=1)\n",
    "                if self.approx:\n",
    "                    taxon_dist = dist.Delta(safe_sigmoid(taxon_logits),validate_args=True).to_event(1)\n",
    "                    taxon_probs = pyro.sample(\"taxon_probs\", taxon_dist)\n",
    "                    taxon = pyro.sample('taxon',dist.RelaxedBernoulli(temperature=self.temperature*s.new_ones(1),probs=taxon_probs).to_event(1))\n",
    "                else:\n",
    "                    taxon_probs=pyro.sample('taxon_probs',dist.Delta(safe_softmax(taxon_logits[...,-self.level_sizes[-1]:],eps=1e-5)).to_event(1))\n",
    "                    if sum(taxon.shape) > 1:\n",
    "                        pass\n",
    "                    else:\n",
    "                        taxon = pyro.sample(\"taxon\", \n",
    "                                         model_distributions.SafeAndRelaxedOneHotCategorical(temperature=self.temperature*s.new_ones(1),probs=taxon_probs,validate_args=True))                    \n",
    "                    if taxon.shape[-1]<self.num_labels:\n",
    "                        taxon = self.tree_convergence_bottom_up.just_propagate(taxon,level_edges,s) if self.freeze_encoder else self.tree_convergence_bottom_up.just_propagate(taxon,level_edges,s)\n",
    "                        taxon = torch.concat(taxon,-1)\n",
    "                    taxon_probs=self.tree_convergence_bottom_up.just_propagate(taxon_probs[...,-self.level_sizes[-1]:],level_edges,s) if self.freeze_encoder else self.tree_convergence_bottom_up.just_propagate(taxon_probs[...,-self.level_sizes[-1]:],level_edges,s)\n",
    "                    taxon_probs=torch.cat(taxon_probs,-1)\n",
    "\n",
    "            quality_genes=self.qg.guide_sample(s) if self.use_q_score else 0.\n",
    "            locs=self.zl.guide_sample(s,scale=fest([taxon_probs],-1))\n",
    "            scales=self.zs.guide_sample(s,scale=fest([taxon_probs],-1))\n",
    "            locs_dynam=self.zld.guide_sample(s,scale=fest([taxon_probs],-1))\n",
    "            discov_dm=self.dm.guide_sample(s,scale=fest([discov,taxon_probs],-1))\n",
    "            batch_dm=self.bm.guide_sample(s,scale=fest([batch,taxon_probs],-1))\n",
    "            discov_di=self.di.guide_sample(s,scale=fest([discov,taxon_probs],-1))\n",
    "            cluster_intercept=self.ci.guide_sample(s,scale=fest([taxon_probs],-1))\n",
    "            bei=self.bei.guide_sample(s,scale=fest([batch_embed.abs(),taxon_probs[...,:self.bi_depth]],-1))#maybe should be abs sum bei\n",
    "            #For scaling\n",
    "            if self.design_matrix:\n",
    "                z=z+oh_index2(oh_index1(discov_dm,discov_ind),taxon) + oh_index2(oh_index1(batch_dm,batch_ind),taxon)+(oh_index(locs_dynam,taxon*psi))\n",
    "            else:\n",
    "                z=z+oh_index2(discov_dm[discov_ind],taxon) + oh_index2(batch_dm[batch_ind],taxon)+(oh_index(locs_dynam,taxon*psi))\n",
    "            z=self.z_transform(z)\n",
    "            pseudo_z=oh_index(locs,taxon_probs)+oh_index2(discov_dm[discov_ind],taxon_probs) + oh_index2(batch_dm[batch_ind],taxon_probs)+(oh_index(locs_dynam,taxon_probs*psi))\n",
    "            pseudo_z=self.z_transform(pseudo_z)\n",
    "            z_decoder_weight=self.zdw.guide_sample(s,scale=fest([pseudo_z.abs()],-1))\n",
    "            discov_dc=self.dc.guide_sample(s,scale=fest([discov,pseudo_z.abs()],-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a543175c-60c7-4900-9104-60957c1a52aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "num_var=adata.shape[1]\n",
    "batch_size=32\n",
    "level_sizes=[1,50,300]\n",
    "num_latent=200\n",
    "steps=0\n",
    "max_steps=500000\n",
    "print_every=5000\n",
    "\n",
    "# Clear Pyro param store so we don't conflict with previous run\n",
    "try:\n",
    "    pyro.clear_param_store()\n",
    "    del antipode_model\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n",
    "# Fix random number seed to a lucky number\n",
    "pyro.util.set_rng_seed(13)\n",
    "# Enable optional validation warnings\n",
    "pyro.enable_validation(False)\n",
    "\n",
    "model_tag='1.9.1.8.3_Relu-NoInterceptApprox-Reset-Tighter'\n",
    "# Instantiate instance of model/guide and various neural networks\n",
    "antipode_model = ANTIPODE(num_latent=num_latent,level_sizes=level_sizes,bi_depth=2,num_batch_embed=10,psi_levels=[0.,1.,1.],\n",
    "                adata=adata,discov_pair=('obs',discov_key),batch_pair=('obs',batch_key),layer=layer_key,\n",
    "                use_psi=True,use_q_score=True,prior_scale=10.,sampler_category='species',dist_normalize=False,\n",
    "                scale_init_val=0.01,loc_as_param=False,zdw_as_param=False,intercept_as_param=False,z_transform=torch.nn.functional.relu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dacb93-bcd6-4d8d-8d39-a730066e2f9a",
   "metadata": {},
   "source": [
    "# Training Phase 1: Block Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29ed1e88-db62-475a-b6fb-0f7c7a0d9927",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/100000 [00:02<75:20:54,  2.71s/it]/allen/programs/celltypes/workgroups/rnaseqanalysis/EvoGen/Team/Matthew/code/scANTIPODE/antipode/train_utils.py:282: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sum(index.shape) == 1:\n",
      "/allen/programs/celltypes/workgroups/rnaseqanalysis/EvoGen/Team/Matthew/code/scANTIPODE/antipode/train_utils.py:285: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  index=torch.nn.functional.one_hot(index.squeeze(),num_classes=out_shape[1]).float() if index.shape[-1]==1 else index\n",
      "/scratch/fast/15941535/ipykernel_164410/15659687.py:781: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  psi=psi*torch.tensor(self.psi_levels).to(s.device).unsqueeze(0)\n",
      "/allen/programs/celltypes/workgroups/rnaseqanalysis/EvoGen/Team/Matthew/code/scANTIPODE/antipode/train_utils.py:282: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sum(index.shape) == 1:\n",
      "/allen/programs/celltypes/workgroups/rnaseqanalysis/EvoGen/Team/Matthew/code/scANTIPODE/antipode/train_utils.py:285: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  index=torch.nn.functional.one_hot(index.squeeze(),num_classes=out_shape[1]).float() if index.shape[-1]==1 else index\n",
      "/scratch/fast/15941535/ipykernel_164410/15659687.py:706: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  psi=psi*torch.tensor(self.psi_levels).to(s.device).unsqueeze(0)\n",
      "/scratch/fast/15941535/ipykernel_164410/15659687.py:713: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sum(Z_obs.shape) <=1:\n",
      "/home/matthew.schmitz/Matthew/utils/miniforge3/envs/antipode/lib/python3.11/site-packages/pyro/distributions/torch_patch.py:74: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  value = torch.as_tensor(\n",
      "  0%|          | 3/100000 [00:10<87:42:32,  3.16s/it] "
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 718.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 679.56 MiB is free. Process 73828 has 23.61 GiB memory in use. Including non-PyTorch memory, this process has 7.46 GiB memory in use. Of the allocated memory 5.75 GiB is allocated by PyTorch, and 1.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mantipode_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_phase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnum_particles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_learning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_cycle_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 235\u001b[0m, in \u001b[0;36mAntipodeTrainingMixin.train_phase\u001b[0;34m(self, phase, max_steps, print_every, device, max_learning_rate, num_particles, one_cycle_lr, steps, batch_size, freeze_encoder)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_approx(phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommon_training_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msvi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 180\u001b[0m, in \u001b[0;36mAntipodeTrainingMixin.common_training_loop\u001b[0;34m(self, dataloader, max_steps, scheduler, svi, print_every, device, steps)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfix_scale_factor(svi, x)\n\u001b[1;32m    179\u001b[0m pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 180\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43msvi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(scheduler, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m/allen/programs/celltypes/workgroups/rnaseqanalysis/EvoGen/Team/Matthew/code/scANTIPODE/antipode/model_modules.py:274\u001b[0m, in \u001b[0;36mSafeSVI.step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# Compute loss and gradients\u001b[39;00m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m poutine\u001b[38;5;241m.\u001b[39mtrace(param_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m param_capture:\n\u001b[0;32m--> 274\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_and_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     loss_val \u001b[38;5;241m=\u001b[39m torch_item(loss)\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mappend(loss_val)\n",
      "File \u001b[0;32m~/Matthew/utils/miniforge3/envs/antipode/lib/python3.11/site-packages/pyro/infer/trace_elbo.py:253\u001b[0m, in \u001b[0;36mJitTrace_ELBO.loss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_and_grads\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, guide, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    250\u001b[0m     loss, surrogate_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_and_surrogate_loss(\n\u001b[1;32m    251\u001b[0m         model, guide, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    252\u001b[0m     )\n\u001b[0;32m--> 253\u001b[0m     \u001b[43msurrogate_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    256\u001b[0m     warn_if_nan(loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Matthew/utils/miniforge3/envs/antipode/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Matthew/utils/miniforge3/envs/antipode/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 718.00 MiB. GPU 0 has a total capacity of 31.74 GiB of which 679.56 MiB is free. Process 73828 has 23.61 GiB memory in use. Including non-PyTorch memory, this process has 7.46 GiB memory in use. Of the allocated memory 5.75 GiB is allocated by PyTorch, and 1.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "antipode_model.train_phase(phase=1,max_steps=max_steps,print_every=10000,num_particles=3,device=device, max_learning_rate=0.001, one_cycle_lr=True, steps=0, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeabc5e-ee9c-46e8-add2-3a6259838ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "antipode_model.clear_cuda()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe74601-c2b9-47ee-879a-2e5ffd2e5d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "antipode_model.store_outputs(device=device,prefix='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80e0bd3-6ba5-4ff9-bd01-a4258ed1f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "antipode_model.clear_cuda()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb92d789-ff19-495c-a566-918542917a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(antipode_model.losses)\n",
    "plot_gmm_heatmaps(antipode_model)\n",
    "plot_d_hists(antipode_model)\n",
    "plot_batch_embedding_pca(antipode_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d098c45d-1bea-4685-af62-1feeed0908cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MDE_KEY = \"X_antipode_MDE\"\n",
    "adata.obsm[MDE_KEY] = clip_latent_dimensions(scvi.model.utils.mde(adata.obsm['X_antipode'],init='random'),0.1)\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=[\"antipode_cluster\"],legend_fontsize=6,legend_fontweight='normal',\n",
    "    legend_loc='on data',palette=sc.pl.palettes.godsnot_102\n",
    ")\n",
    "\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=['q_score',discov_key,batch_key],palette=sc.pl.palettes.godsnot_102,cmap='coolwarm'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a844b6f-957f-40b0-9d86-f2de4ef2e222",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "antipode_model.clear_cuda()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9b130a-32dd-4ec4-b74a-df65a7661a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "antipode_model.save(os.path.join('/home/matthew.schmitz/Matthew/models/',model_tag),save_anndata=False,prefix='p1_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462106b5-d11a-4b7b-8d2e-cfdafeedf19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del adata\n",
    "# antipode_model=ANTIPODE.load(os.path.join('/home/matthew.schmitz/Matthew/models/',model_tag),prefix='p1_')\n",
    "# antipode_model.train()\n",
    "# adata=antipode_model.adata_manager.adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d062fd82-868e-4208-b59e-fedb37395849",
   "metadata": {},
   "source": [
    "# Training Phase 2: Intializing layered tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7df1f1-f822-4abc-9d1a-acc932414d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "antipode_model.prepare_phase_2(epochs=2,device=device,dimension_reduction='X_antipode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8422b8be-a515-448b-a4af-2a0744e67817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "antipode_model.train_phase(phase=2,max_steps=int(max_steps/2),print_every=10000,num_particles=1,device=device, max_learning_rate=1e-3, one_cycle_lr=True, batch_size=32,freeze_encoder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6848529b-8e11-40d2-b8d4-897a63e9ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "antipode_model.clear_cuda()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4223f5-6a22-4cd6-978b-a9220351d4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "antipode_model.store_outputs(device=device,prefix='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ddd247-d4db-487e-9856-4a4405b6ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "antipode_model.clear_cuda()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b55c653-d710-42e0-bb3d-a490b05e585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(antipode_model.losses)\n",
    "plot_gmm_heatmaps(antipode_model)\n",
    "plot_d_hists(antipode_model)\n",
    "plot_tree_edge_weights(antipode_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d514f1-ce61-45a9-911b-d14ccbece61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MDE_KEY = \"X_antipode_MDE\"\n",
    "#adata.obsm[MDE_KEY] = clip_latent_dimensions(scvi.model.utils.mde(adata.obsm['X_antipode'],init='random'),0.1)\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=[\"antipode_cluster\",\"kmeans\"],legend_fontsize=6,legend_fontweight='normal',\n",
    "    legend_loc='on data',palette=sc.pl.palettes.godsnot_102\n",
    ")\n",
    "\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=['q_score',discov_key,batch_key],palette=sc.pl.palettes.godsnot_102,cmap='coolwarm'\n",
    ")\n",
    "\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=[x for x in adata.obs.columns if 'psi' in x],\n",
    "    cmap='coolwarm',\n",
    "    legend_loc='on data'\n",
    ")\n",
    "\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=[x for x in adata.obs.columns if 'level' in x],\n",
    "    palette=sc.pl.palettes.godsnot_102,\n",
    "    legend_loc='on data'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9324b9-eb7b-4842-adb6-41f050056111",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "antipode_model.clear_cuda()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158d8291-aa7b-45fa-acee-068f7d6b4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "antipode_model.save(os.path.join('/home/matthew.schmitz/Matthew/models/',model_tag),save_anndata=False,prefix='p2_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b898a7-ce64-4ac2-9768-d52623f29b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del adata\n",
    "# antipode_model=ANTIPODE.load(os.path.join('/home/matthew.schmitz/Matthew/models/',model_tag),prefix='p2_')\n",
    "# antipode_model.train()\n",
    "# adata=antipode_model.adata_manager.adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616370fc-b669-407f-a92c-950120e03be9",
   "metadata": {},
   "source": [
    "# Training Phase 3: Refining the final tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63280fdb-ea9a-4be2-a6a7-19ef4c1f2684",
   "metadata": {},
   "outputs": [],
   "source": [
    "antipode_model.train_phase(phase=3,max_steps=max_steps,print_every=10000,num_particles=3,device=device, max_learning_rate=5e-4, one_cycle_lr=True, steps=0, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58032afe-2cd2-43da-884c-7caadcf60fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "antipode_model.clear_cuda()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71220a1-d2d7-4d2f-9801-831804ce0fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(antipode_model.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d33a003-2eca-4573-8cc6-915cb433cb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "antipode_model.store_outputs(device=device,prefix='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacee074-7c5d-4ded-a44e-eae7954c01fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "antipode_model.clear_cuda()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9f41a1-2e15-496d-a769-86afbbae1f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gmm_heatmaps(antipode_model)\n",
    "plot_d_hists(antipode_model)\n",
    "plot_tree_edge_weights(antipode_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486c1c84-0dba-4fd5-88dd-2c0cb83f407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MDE_KEY = \"X_antipode_MDE\"\n",
    "adata.obsm[MDE_KEY] = clip_latent_dimensions(scvi.model.utils.mde(adata.obsm['X_antipode'],init='random'),0.1)\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=[\"antipode_cluster\",\"kmeans\"],legend_fontsize=6,legend_fontweight='normal',\n",
    "    legend_loc='on data',palette=sc.pl.palettes.godsnot_102\n",
    ")\n",
    "\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=[x for x in adata.obs.columns if 'level' in x],\n",
    "    palette=sc.pl.palettes.godsnot_102,\n",
    "    legend_loc='on data'\n",
    ")\n",
    "\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=['q_score',discov_key,batch_key],palette=sc.pl.palettes.godsnot_102,cmap='coolwarm'\n",
    ")\n",
    "\n",
    "sc.pl.embedding(\n",
    "    adata,\n",
    "    basis=MDE_KEY,\n",
    "    color=[x for x in adata.obs.columns if 'psi' in x],\n",
    "    cmap='coolwarm',\n",
    "    legend_loc='on data'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab131c3-37fe-4297-9996-0eddeed3cf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "antipode_model.save(os.path.join('/home/matthew.schmitz/Matthew/models/',model_tag),save_anndata=True,prefix='p3_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a493cd7-eeff-4826-b946-97970d4b7f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del adata\n",
    "# antipode_model=ANTIPODE.load(os.path.join('/home/matthew.schmitz/Matthew/models/',model_tag),prefix='p3_')\n",
    "# antipode_model.train()\n",
    "# adata=antipode_model.adata_manager.adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12acc36-4c13-4366-bef4-11e1b0317854",
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.histplot(adata.obs['q_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7081ead-6a38-4cc2-bddf-a4ad560b77ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_choice=np.random.choice(adata.obs.index,size=100000,replace=False)\n",
    "random_choice=np.where(adata.obs.index.isin(random_choice))[0]\n",
    "xdata=adata[random_choice,:]\n",
    "xdata=xdata.to_memory().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a903ab-ef45-453a-84b8-312f89b65957",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata.X=xdata.layers[layer_key]\n",
    "sc.pp.normalize_per_cell(xdata)\n",
    "sc.pp.log1p(xdata)\n",
    "#sc.pp.scale(xdata,max_value=10)\n",
    "\n",
    "gene_list=['RBFOX3','PDGFRA','AQP4','FOXJ1','AIF1','MOG','COL1A2','CD34','COL4A1','FOXG1','SATB2','RORB','SLC17A7','TLE4','FEZF2',\n",
    "           'DLX2','PROX1','SCGN','NKX2-1','LHX6','SST','PVALB','CRABP1','MEIS2','TSHZ1','NPY','FOXP1','FOXP2','PDYN','PENK','ISL1',\n",
    "           'MKI67','RPL7','RPS17','RPL13A','MEF2C',\n",
    "           'HMX3','TH','LMX1A','TFAP2A','TFAP2B','RSPO1','NKX3-1','IGF1','ITPR2','OTX2','HOXB3','PAX1','PAX2','PAX3','PAX5','PAX6','PAX7','PAX8']\n",
    "gene_list=[x for x in gene_list if x in xdata.var.index]\n",
    "sc.pl.embedding(\n",
    "    xdata,\n",
    "    basis=MDE_KEY,\n",
    "    color=gene_list,cmap='Purples',\n",
    "    palette=sc.pl.palettes.godsnot_102,legend_fontsize=6,\n",
    "    legend_loc='on data',use_raw=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4eb6a3b-f398-48b2-93d3-ebe32b2b457f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "antipode",
   "language": "python",
   "name": "antipode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
