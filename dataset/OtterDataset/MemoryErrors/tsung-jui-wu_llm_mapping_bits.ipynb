{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e708546b-2687-4272-9176-cbf80ace4adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f76703c7-c82e-4d7b-886b-f67f0f6fa4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3481088-0407-4249-829e-c4bdb0f2b25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2cb763c-984e-4d4a-aff2-6c9c30da2907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc85c2d2670>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 854\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d26f84-1c64-4307-a196-383363001517",
   "metadata": {},
   "source": [
    "## Bits Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b6f58b-d338-4a5c-a7af-d37d4e84f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(binary_input):\n",
    "    return int(binary_input, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db626d04-fe90-4ad7-a150-7d6d8450cdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bits vocab len: 256\n"
     ]
    }
   ],
   "source": [
    "bits = 8\n",
    "bits_vocab_len = 2**bits\n",
    "\n",
    "print(f\"bits vocab len: {bits_vocab_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d89968d-38a5-4e8f-b03a-b4b5dad3f9f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Open the file in binary read modef\n",
    "with open('../data/midi/MMD_MIDI/0/0/0/00000ec8a66b6bd2ef809b0443eeae41.mid', 'rb') as file:\n",
    "    file_bytes = file.read()\n",
    "\n",
    "# If you want to see the byte representation in a more readable format,\n",
    "# you can iterate over the bytes object and print each byte.\n",
    "byte_stream = \"\"\n",
    "for byte in file_bytes:\n",
    "    byte_stream += f'{byte:08b}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75d370bc-1934-450f-8fa8-431705f8b107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "870"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(byte_stream)//bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69cf22e8-3240-4a81-8fd6-d2004024a8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "\n",
    "num_parts = len(byte_stream)//bits\n",
    "for i in range(num_parts):\n",
    "    a.append(encoder(byte_stream[i*bits:(i+1)*bits]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6791998a-7076-4728-b5fc-34815cef131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_binary(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        file_bytes = file.read()\n",
    "\n",
    "    byte_stream = \"\"\n",
    "    for byte in file_bytes:\n",
    "        byte_stream += f'{byte:08b}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214bd214-bbac-4179-ba7a-465caf0a7c04",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa2246-be2d-4e1f-b23a-93cd247a4318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "llm = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(llm)\n",
    "llm_tokenizer = GPT2Tokenizer.from_pretrained(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74df56fc-34cf-4a9f-9adb-f46265012a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.lm_head.weight\n",
    "# embedding_matrix = model.transformer.wte.weight\n",
    "llm_feature_dim = model.config.hidden_size\n",
    "llm_vocab_len = model.config.vocab_size\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12edf371-1493-4de2-a0dd-bf90c42a5dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2ea0b9-7e20-4af0-8b10-1402a7d5cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"gpt2 feature dim length:\", llm_feature_dim)\n",
    "print(\"gpt2 vocabulary length:\", llm_vocab_len)\n",
    "print(\"gpt2 embedding shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d12aa-adee-44bc-8e85-8543abbbe1a2",
   "metadata": {},
   "source": [
    "## Google Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14af8113-e10f-4ae2-9027-2d10125de710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b816da678e44abb4058f07cdc2d0ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "llm = \"google/gemma-2b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm)\n",
    "model = AutoModelForCausalLM.from_pretrained(llm, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ce47949-2283-46bd-8d31-d387c687a2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaConfig {\n",
       "  \"_name_or_path\": \"google/gemma-2b\",\n",
       "  \"architectures\": [\n",
       "    \"GemmaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 2,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"head_dim\": 256,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_activation\": null,\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 16384,\n",
       "  \"max_position_embeddings\": 8192,\n",
       "  \"model_type\": \"gemma\",\n",
       "  \"num_attention_heads\": 8,\n",
       "  \"num_hidden_layers\": 18,\n",
       "  \"num_key_value_heads\": 1,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.39.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 256000\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fee2e1ea-a400-4428-b941-b6ebd8aeaf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256000, 2048])\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.lm_head.weight\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d7891e9-c6b5-4f5f-8206-7a29a51b94c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_feature_dim = model.config.hidden_size\n",
    "llm_vocab_len = model.config.vocab_size\n",
    "# model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52347b89-bd86-4fe5-872b-feeadcb6fcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2cd2245-3913-477e-b0ee-675713b02252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm feature dim length: 2048\n",
      "llm vocabulary length: 256000\n",
      "llm embedding shape: torch.Size([256000, 2048])\n"
     ]
    }
   ],
   "source": [
    "print(\"llm feature dim length:\", llm_feature_dim)\n",
    "print(\"llm vocabulary length:\", llm_vocab_len)\n",
    "print(\"llm embedding shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac29d84-7551-4cf5-911b-09d24d705cc7",
   "metadata": {},
   "source": [
    "## Mapper Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa0557f2-67c5-40d2-bfcc-3799d25f2962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e29ded59-355f-4a38-b2a0-8f5969880672",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenMapper(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.mapper = nn.Linear(input_dim, output_dim)\n",
    "        self.mapper.to(device)\n",
    "\n",
    "    def forward(self, one_hot_token):\n",
    "        return self.mapper(one_hot_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8025a92a-ef5d-43de-9da5-b8b3db5e58bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mapper\n",
    "# mapper maps vocabulary_size of target modality to feature_dimension size of llm\n",
    "# mapper = TokenMapper(midi_vocab_len, llm_feature_dim, device=device)\n",
    "mapper = TokenMapper(bits_vocab_len, llm_feature_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4059303-7ca1-4403-bd8c-480eafac7601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenMapper(\n",
       "  (mapper): Linear(in_features=256, out_features=2048, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d113d5e0-720d-40e3-9043-2cdfdf81e0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverseMapper = TokenMapper(bits_vocab_len, llm_feature_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6194e164-afc1-47f0-bb2e-8b78e61a4699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenMapper(\n",
       "  (mapper): Linear(in_features=256, out_features=2048, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverseMapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6146d5-6b67-4d4e-a2d1-537b245b3a82",
   "metadata": {},
   "source": [
    "## Prompt Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8efd0dea-a6db-41b5-9202-b081138ec8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_len = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c53bd5ad-76f1-4d60-b818-bba5b9f5fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prompt(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.model = nn.Linear(input_dim, output_dim, bias=False)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def forward(self, one_hot_token):\n",
    "        return self.model(one_hot_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abf04ddf-8b76-413e-ab0f-f224c1f6c3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if prompt_len!=0:\n",
    "    prompt = Prompt(prompt_len, llm_feature_dim, device=device)\n",
    "    prompt_inputs = F.one_hot(torch.arange(prompt_len), num_classes=prompt_len).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22c79a3-e452-4372-937b-1b6fb3eee2f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d0217db-f3a4-4523-908c-1efdf20f8ec0",
   "metadata": {},
   "source": [
    "## Generate Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ccf30cc1-4ecb-4555-9433-ba439645512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_token_predictions(token_sequences):\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=token_sequences, output_hidden_states=True)\n",
    "    \n",
    "    return outputs.hidden_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42db0c2e-75e5-4d58-bd9e-5f940ab54d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_token_predictions_withfv(token_fv):\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs_embeds=token_fv, output_hidden_states=True)\n",
    "    \n",
    "    return outputs.hidden_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94aeae5f-5f43-4896-a45f-2c46e22e8146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(batch_feature_vectors, embeddings, temperature=1.0):\n",
    "    batch_size, seq_len, embedding_dim = batch_feature_vectors.shape\n",
    "\n",
    "    # Normalize the embedding matrix\n",
    "    embedding_matrix_norm = F.normalize(embeddings, dim=1)\n",
    "\n",
    "    batch_feature_vector_norm = F.normalize(batch_feature_vectors, dim=2)\n",
    "    cosine_similarities = torch.matmul(batch_feature_vector_norm, embedding_matrix_norm.T)\n",
    "    # cosine_similarities = torch.matmul(batch_feature_vectors, embeddings.T)\n",
    "    sfmx = torch.softmax(cosine_similarities/temperature, dim=2)\n",
    "    closest_tokens = torch.argmax(sfmx, dim=2)\n",
    "    \n",
    "    mm = torch.matmul(sfmx, embeddings)\n",
    "\n",
    "    return mm, cosine_similarities, closest_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070d0801-d421-43be-b284-691505453268",
   "metadata": {},
   "source": [
    "## Reinforce Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eaf1422a-e497-4d89-ba5f-3733ffe138c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reinforce_Loss(logits, translated, loss, gamma=0.9, alpha=1, temperature=1):\n",
    "    \"\"\"\n",
    "    Calculate the REINFORCE loss for sequence prediction.\n",
    "\n",
    "    :param logits: Logits from the model, shape [batch_size, seq_len, vocab_size].\n",
    "    :param targets: Ground truth sequence, shape [batch_size, seq_len].\n",
    "    :param rewards: Reward for each step in the sequence, shape [batch_size, seq_len].\n",
    "    :param gamma: Discount factor for future rewards.\n",
    "    :return: The REINFORCE loss (to be maximized).\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, _ = logits.shape\n",
    "    translated = translated.to(torch.int64)\n",
    "    # shape = [batch_size, seq_len, llm_vocab_len]\n",
    "    log_probs = F.log_softmax(logits/temperature, dim=-1)\n",
    "    log_probs_targets = log_probs.gather(2, translated.unsqueeze(2)).squeeze(2)\n",
    "    \n",
    "    # Create a discount matrix\n",
    "    discount_matrix = torch.zeros((seq_len, seq_len)).to(device)\n",
    "\n",
    "    # Fill the matrix according to the given pattern\n",
    "    for i in range(seq_len):\n",
    "        for j in range(i, seq_len):\n",
    "            discount_matrix[i, j] = gamma ** (j - i)\n",
    "\n",
    "    normalize_factor = discount_matrix.sum(dim=1)\n",
    "    \n",
    "    # Calculate discounted rewards\n",
    "    discounted_loss = loss.unsqueeze(1) * discount_matrix\n",
    "    cumulative_loss = discounted_loss.sum(dim=-1) / normalize_factor / alpha\n",
    "    \n",
    "    # Calculate loss\n",
    "    total_loss = torch.sum(log_probs_targets * cumulative_loss) / batch_size / seq_len\n",
    "\n",
    "    del discount_matrix\n",
    "    del cumulative_loss\n",
    "    del log_probs_targets\n",
    "    del normalize_factor\n",
    "    del log_probs\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e3811a-ca11-4e25-a36d-4eaf2646f1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f119795-050b-4958-96a3-9e23946524a2",
   "metadata": {},
   "source": [
    "## Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37545a1a-1735-4251-a492-94cea3436c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import BatchSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "992e29d1-be34-49df-95d6-f02f5152b891",
   "metadata": {},
   "outputs": [],
   "source": [
    "bits = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "357b431c-1671-4e18-a133-ed477ed84c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "image_size = 128\n",
    "seq_len = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11eae52-3585-4c63-af30-d3fa4a76cfd0",
   "metadata": {},
   "source": [
    "### Midi Bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6b6ca52-5fc5-4f60-b516-16cfe389a8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single file\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        file_bytes = file.read()\n",
    "    \n",
    "    byte_stream = \"\"\n",
    "    for byte in file_bytes:\n",
    "        byte_stream += f'{byte:08b}'\n",
    "    \n",
    "    a = []\n",
    "    num_parts = len(byte_stream) // bits\n",
    "    for i in range(num_parts):\n",
    "        a.append(encoder(byte_stream[i*bits:(i+1)*bits]))\n",
    "    \n",
    "    return a\n",
    "\n",
    "# Main function to walk through the directory and process each file\n",
    "def process_directory(file_path):\n",
    "    supported_extensions = ('.mid', '.midi')\n",
    "    all_integers = []\n",
    "\n",
    "    for root, dirs, files in os.walk(file_path):\n",
    "        for file in files:\n",
    "            if file.endswith(supported_extensions):\n",
    "                full_path = os.path.join(root, file)\n",
    "                file_integers = process_file(full_path)\n",
    "                all_integers.extend(file_integers)\n",
    "    \n",
    "    return all_integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4f2801c-212b-463a-a9c1-43b2161f454d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "file_path = '../data/midi/Maestro'\n",
    "# Call the function with your specific file path\n",
    "resulting_integers = process_directory(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ada1e5b1-dc37-4362-9031-19a8c61c6cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83869091"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(resulting_integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdd1e65a-032b-47c4-b58e-d8bf14fe96af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegerDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping integers.\"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc3a1748-d1c9-453c-88b8-8c8efb5fde28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedBatchSampler(BatchSampler):\n",
    "    \"\"\"BatchSampler that groups multiple batches together.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, batch_size=128, group_size=seq_len, shuffle=False):\n",
    "        # If shuffle is True, use a RandomSampler instead of SequentialSampler\n",
    "        sampler = SequentialSampler(dataset)\n",
    "        super().__init__(sampler, batch_size=batch_size * group_size, drop_last=False)\n",
    "        self.group_size = group_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch = []\n",
    "        for idx in self.sampler:\n",
    "            batch.append(idx)\n",
    "            # When we've collected enough indices for a group of batches, yield them\n",
    "            if len(batch) == self.batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        if batch and not self.drop_last:\n",
    "            yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1f3b820-bf27-4eef-af69-f1dca054b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup your dataset\n",
    "dataset = IntegerDataset(resulting_integers)\n",
    "\n",
    "# Create the GroupedBatchSampler\n",
    "grouped_batch_sampler = GroupedBatchSampler(dataset, batch_size=batch_size, group_size=seq_len, shuffle=True)\n",
    "\n",
    "# Create the DataLoader using the custom sampler\n",
    "dataloader = DataLoader(dataset, batch_sampler=grouped_batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d322138c-2bbb-4656-aff9-d3dd482b21d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for grouped_batch in dataloader:\n",
    "    # `grouped_batch` is now a tensor with the shape [10, 128]\n",
    "    # Perform your training operations here\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "198fce39-c5e8-4f7f-af40-165cae1cc428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2560])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122cfdc6-a880-46fc-9f4b-a069edb23dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1191d5f6-34df-4578-be11-d03b5cc0707d",
   "metadata": {},
   "source": [
    "### Image Bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a626fea4-f96a-486b-b7f1-b7faa4f923fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from dall_e import map_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d729dc2b-a18e-4d0d-893c-99a0d62754af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToBinaryString:\n",
    "    \"\"\"Converts a PIL image or a tensor to a binary string.\"\"\"\n",
    "    def __call__(self, pic):\n",
    "        # Convert PIL Image to Tensor if it's not already a tensor\n",
    "        if not isinstance(pic, torch.Tensor):\n",
    "            transform_to_tensor = T.ToTensor()\n",
    "            pic = transform_to_tensor(pic)\n",
    "        \n",
    "        # Ensure the tensor is in CPU and in uint8 format\n",
    "        pic = (pic * 255).byte()\n",
    "        \n",
    "        # Convert the image tensor to a numpy array and then to bytes\n",
    "        image_bytes = pic.numpy().tobytes()\n",
    "        \n",
    "        # Convert bytes to binary string\n",
    "        binary_string = ''.join(f'{byte:08b}' for byte in image_bytes)\n",
    "        integers = [int(binary_string[i:i+bits], 2) for i in range(0, len(binary_string), bits)]\n",
    "        \n",
    "        return integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89eb5d9-f841-4615-96ff-81ef015b8fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_crop(img):\n",
    "    # Resize while maintaining aspect ratio and center crop\n",
    "    s = min(img.size)\n",
    "    r = image_size / s\n",
    "    s = (round(r * img.size[1]), round(r * img.size[0]))\n",
    "    img = TF.resize(img, s, interpolation=Image.LANCZOS)\n",
    "    img = TF.center_crop(img, output_size=2 * [image_size])\n",
    "    return img\n",
    "\n",
    "def modified_map_pixels(img):\n",
    "    # Add a batch dimension, apply map_pixels, and then remove the batch dimension\n",
    "    img = img.unsqueeze(0)\n",
    "    img = map_pixels(img)\n",
    "    return img.squeeze(0)\n",
    "\n",
    "# Now, include your custom transform in the Compose\n",
    "transform = T.Compose([\n",
    "    T.Lambda(resize_and_crop),\n",
    "    T.ToTensor()\n",
    "    # ToBinaryString(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd1b408-8698-4030-b316-050a26cbf6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsun_path = '../data/lsun'\n",
    "\n",
    "dataset = datasets.LSUN(root=lsun_path, classes=['classroom_train'], transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ada58d-d069-4792-8e30-97681f4cda7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dataset size:\", len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c20e16-e72f-4d08-bf3e-625aefc332a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where you want to save the temporary .jpg files\n",
    "temp_dir = 'temp_images'\n",
    "os.makedirs(temp_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120f9624-6b2e-4c0b-846d-d06d6732d0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_binary(file_path):\n",
    "    \"\"\"\n",
    "    Convert an image file to its binary representation.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        binary_data = file.read()\n",
    "    \n",
    "    binary_string = ''.join(format(byte, '08b') for byte in binary_data)\n",
    "    integers = [int(binary_string[i:i+bits], 2) for i in range(0, len(binary_string), bits)]\n",
    "        \n",
    "    return integers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945cc5c2-4fee-45f7-87aa-b05a09ffc6db",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40c1b57c-b523-4922-a98c-86f0a5670e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "learning_rate = 1e-5\n",
    "epochs = 1\n",
    "gamma = 0.1\n",
    "temperature = 0.001\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "46da34a3-eec2-44cd-af37-5d8eb4a04cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'midi/base/base_single/8bits/google/gemma-2b/lr=1e-05,gamma=0.1,temp=0.001,promptlen=0'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment = \"base_single\"\n",
    "algo = \"base\"\n",
    "exp_type = \"midi\"\n",
    "name = f\"{bits}bits\"\n",
    "experiment_name = f\"{exp_type}/{algo}/{experiment}/{name}/{llm}/lr={learning_rate},gamma={gamma},temp={temperature},promptlen={prompt_len}\"\n",
    "\n",
    "experiment_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4f211e-daf4-4435-98da-c2cebd7cb3e1",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb58004a-e239-4571-a881-511b76a055fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapper.load_state_dict(torch.load(f\"../models/{experiment_name}/mapper.pt\"))\n",
    "# reverseMapper.load_state_dict(torch.load(f\"../models/{experiment_name}/reversemapper.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73a69ad-84c3-4ed6-a3da-e2f264281a83",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8391026b-3485-4641-88d2-0def399638d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Create a SummaryWriter instance (logs will be saved in 'runs' folder)\n",
    "writer = SummaryWriter(log_dir = f'../runs/{experiment_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "35f9039c-0ac8-4caa-ae07-a831dabe01c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "if prompt_len==0 and \"dual\" in experiment_name:\n",
    "    optimizer = optim.Adam(list(mapper.parameters()) + list(reverseMapper.parameters()), lr=learning_rate)\n",
    "elif \"dual\" in experiment_name:\n",
    "    optimizer = optim.Adam(list(mapper.parameters()) + list(prompt.parameters()), lr=learning_rate)\n",
    "elif \"single\" in experiment_name:\n",
    "    optimizer = optim.Adam(mapper.parameters(), lr=learning_rate)\n",
    "rl_criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b1515d5-3e4e-4020-b08f-5027a6b0e9a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.44 GiB. GPU 0 has a total capacty of 10.91 GiB of which 254.12 MiB is free. Including non-PyTorch memory, this process has 10.66 GiB memory in use. Of the allocated memory 10.09 GiB is allocated by PyTorch, and 6.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 43\u001b[0m\n\u001b[1;32m     35\u001b[0m inputs_feature_vector \u001b[38;5;241m=\u001b[39m mapper(one_hot_tokens)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Add prompt to input\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# prompt_feature_vector = prompt(prompt_inputs)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# prompt_feature_vector = prompt_feature_vector.unsqueeze(0).repeat(batch_len, 1, 1)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# inputs_feature_vector = torch.cat((prompt_feature_vector, mapped_feature_vector), dim=1)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Map tokens and get ground truth from LLM\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m translated_feature_vector, translated_logits, translated_text_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_feature_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# translated_feature_vector, translated_logits, translated_text_tokens = translate(inputs_feature_vector, embeddings.detach(), temperature=temperature)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Calculate Representation of Last Layer in LLM\u001b[39;00m\n\u001b[1;32m     47\u001b[0m final_layer_fv \u001b[38;5;241m=\u001b[39m generate_next_token_predictions_withfv(translated_feature_vector)\n",
      "Cell \u001b[0;32mIn[28], line 10\u001b[0m, in \u001b[0;36mtranslate\u001b[0;34m(batch_feature_vectors, embeddings, temperature)\u001b[0m\n\u001b[1;32m      8\u001b[0m cosine_similarities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(batch_feature_vector_norm, embedding_matrix_norm\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# cosine_similarities = torch.matmul(batch_feature_vectors, embeddings.T)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m sfmx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(\u001b[43mcosine_similarities\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mtemperature\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     11\u001b[0m closest_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(sfmx, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     13\u001b[0m mm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(sfmx, embeddings)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.44 GiB. GPU 0 has a total capacty of 10.91 GiB of which 254.12 MiB is free. Including non-PyTorch memory, this process has 10.66 GiB memory in use. Of the allocated memory 10.09 GiB is allocated by PyTorch, and 6.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(epochs):\n",
    "    mapper.train()\n",
    "    # mapper.eval()\n",
    "    for i, dd in enumerate(dataloader):\n",
    "        \n",
    "        # optimizer.zero_grad()\n",
    "\n",
    "        # images = dd[0]\n",
    "\n",
    "        # tokens = []\n",
    "        # for image_idx, image in enumerate(images):\n",
    "        #     # Construct a unique filename for each image\n",
    "        #     filename = os.path.join(temp_dir, f'batch_{i}_image_{image_idx}.jpg')\n",
    "        #     # Save the image as a .jpg file\n",
    "        #     save_image(image, filename)\n",
    "        #     # Convert the saved image file to binary representation\n",
    "        #     binary_representation = image_to_binary(filename)\n",
    "        #     tokens.append(binary_representation)\n",
    "        #     # Optionally, delete the file if it's no longer needed\n",
    "        #     os.remove(filename)\n",
    "\n",
    "        # tensor_list = [torch.tensor(sublist) for sublist in tokens]\n",
    "        # # Pad the sequence of tensors, padding zeros behind each sequence\n",
    "        # data = pad_sequence(tensor_list, batch_first=True, padding_value=0)        \n",
    "        # num_chunks = data.shape[1] // seq_len\n",
    "        \n",
    "        # data = data[:,:num_chunks*seq_len]\n",
    "        data = dd\n",
    "        \n",
    "        ground_truth_tokens = data.reshape(-1, seq_len).to(device)\n",
    "        one_hot_tokens = F.one_hot(ground_truth_tokens, num_classes=bits_vocab_len).float()\n",
    "\n",
    "        # Logits are to be compared with the next ground truth tokens\n",
    "        ground_truth_tokens = ground_truth_tokens[:,1:]\n",
    "        inputs_feature_vector = mapper(one_hot_tokens)\n",
    "        \n",
    "        # Add prompt to input\n",
    "        # prompt_feature_vector = prompt(prompt_inputs)\n",
    "        # prompt_feature_vector = prompt_feature_vector.unsqueeze(0).repeat(batch_len, 1, 1)\n",
    "        # inputs_feature_vector = torch.cat((prompt_feature_vector, mapped_feature_vector), dim=1)\n",
    "\n",
    "        # Map tokens and get ground truth from LLM\n",
    "        translated_feature_vector, translated_logits, translated_text_tokens = translate(inputs_feature_vector, embeddings.detach(), temperature=temperature)\n",
    "        # translated_feature_vector, translated_logits, translated_text_tokens = translate(inputs_feature_vector, embeddings.detach(), temperature=temperature)\n",
    "\n",
    "        # Calculate Representation of Last Layer in LLM\n",
    "        final_layer_fv = generate_next_token_predictions_withfv(translated_feature_vector)\n",
    "        # final_layer_fv = generate_next_token_predictions(translated_text_tokens.long()).to(device)\n",
    "\n",
    "        # Calculate Logits with mapper function\n",
    "        # final_layer_fv = F.normalize(final_layer_fv, dim=-1)\n",
    "        # mapper_embeds = F.normalize(mapper.mapper.weight, dim=0)\n",
    "        # logits = torch.matmul(final_layer_fv, reverseMapper.mapper.weight)\n",
    "        logits = torch.matmul(final_layer_fv, mapper.mapper.weight)\n",
    "        # logits = logits[:,prompt_len:-1]\n",
    "        logits = logits[:,:-1]\n",
    "        logits_ = logits.reshape(-1, bits_vocab_len)\n",
    "        ground_truth_tokens = ground_truth_tokens.reshape(-1)        \n",
    "        ce_loss = criterion(logits_, ground_truth_tokens)\n",
    "        ce_loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        if 'base' in algo: \n",
    "            ce_loss.backward()\n",
    "            optimizer.step()\n",
    "            writer.add_scalar(\"training/cross_entropy_base\", ce_loss.item(), epoch*len(dataloader)+i)\n",
    "            if i%50==0:\n",
    "                print(f\"Epoch {epoch+1}, Batch {i}, CE Loss: {ce_loss.mean().item()}\")\n",
    "        # RL Loss\n",
    "        if 'rl' in algo:\n",
    "            optimizer.zero_grad()\n",
    "            # action_logits = torch.matmul(mapped_feature_vector, embeddings.T.detach())\n",
    "            translated_feature_vector, translate_logits, translated_text_tokens = translate(inputs_feature_vector, embeddings.detach(), temperature=temperature)\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                final_layer_fv = generate_next_token_predictions_withfv(translated_feature_vector)\n",
    "                logits = torch.matmul(final_layer_fv, reverseMapper.mapper.weight)\n",
    "                logits = logits[:,prompt_len:-1]  \n",
    "\n",
    "                \n",
    "                logits_ = logits.reshape(-1, bits_vocab_len)\n",
    "                ce_loss = rl_criterion(logits_, ground_truth_tokens)\n",
    "                ce_loss = ce_loss.reshape(-1, logits.size(1))\n",
    "                \n",
    "            rl_loss = Reinforce_Loss(translate_logits[:,1:], translated_text_tokens[:,1:].detach(), ce_loss, alpha=alpha, gamma=gamma, temperature=temperature)\n",
    "            \n",
    "            rl_loss.backward()\n",
    "            optimizer.step()\n",
    "            # writer.add_scalar(\"training_rl\", rl_loss.item(), epoch*len(midi_loader)+i)\n",
    "            # Log the losses\n",
    "            writer.add_scalars(\n",
    "                \"training\",\n",
    "                {\n",
    "                    \"rl_loss\": rl_loss.item(),\n",
    "                    \"cross_entropy_rl\": ce_loss.mean().item(),\n",
    "                },\n",
    "                epoch * len(dataloader) + i + 1000 + 1000\n",
    "            )\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Batch {i}, CE Loss: {ce_loss.mean().item()}, RL Loss: {rl_loss.item()}\")\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} completed.\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71d6870-9fde-4ab2-9f09-e623e297c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(f\"../models/{experiment_name}\").mkdir(parents=True, exist_ok=True)\n",
    "torch.save(mapper.state_dict(), f\"../models/{experiment_name}/mapper.pt\")\n",
    "torch.save(reverseMapper.state_dict(), f\"../models/{experiment_name}/reversemapper.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9af34f1-11d6-4b09-8703-5c6bba7b0deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e21a5f-bed4-48d1-b00c-6266cae3463f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqvae",
   "language": "python",
   "name": "vqvae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
