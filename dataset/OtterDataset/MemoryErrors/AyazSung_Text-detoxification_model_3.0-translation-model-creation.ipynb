{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-05T17:50:15.258378700Z",
     "start_time": "2023-11-05T17:50:15.082526800Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "original_data = pd.read_csv('../data/raw_filtered.tsv', sep='\\t')\n",
    "\n",
    "# remove rows with empty reference or translation\n",
    "original_data = original_data[original_data['reference'].notna()]\n",
    "original_data = original_data[original_data['translation'].notna()]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T17:50:19.142816400Z",
     "start_time": "2023-11-05T17:50:15.119530100Z"
    }
   },
   "id": "810dc88ee4650925"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0                                          reference  \\\n0           0  If Alkar is flooding her with psychic waste, t...   \n1           1                          Now you're getting nasty.   \n2           2           Well, we could spare your life, for one.   \n3           3          Ah! Monkey, you've got to snap out of it.   \n4           4                   I've got orders to put her down.   \n\n                                         translation  similarity  lenght_diff  \\\n0  if Alkar floods her with her mental waste, it ...    0.785171     0.010309   \n1                        you're becoming disgusting.    0.749687     0.071429   \n2                      well, we can spare your life.    0.919051     0.268293   \n3                       monkey, you have to wake up.    0.664333     0.309524   \n4                         I have orders to kill her.    0.726639     0.181818   \n\n    ref_tox   trn_tox  \n0  0.014195  0.981983  \n1  0.065473  0.999039  \n2  0.213313  0.985068  \n3  0.053362  0.994215  \n4  0.009402  0.999348  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>reference</th>\n      <th>translation</th>\n      <th>similarity</th>\n      <th>lenght_diff</th>\n      <th>ref_tox</th>\n      <th>trn_tox</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>If Alkar is flooding her with psychic waste, t...</td>\n      <td>if Alkar floods her with her mental waste, it ...</td>\n      <td>0.785171</td>\n      <td>0.010309</td>\n      <td>0.014195</td>\n      <td>0.981983</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Now you're getting nasty.</td>\n      <td>you're becoming disgusting.</td>\n      <td>0.749687</td>\n      <td>0.071429</td>\n      <td>0.065473</td>\n      <td>0.999039</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Well, we could spare your life, for one.</td>\n      <td>well, we can spare your life.</td>\n      <td>0.919051</td>\n      <td>0.268293</td>\n      <td>0.213313</td>\n      <td>0.985068</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Ah! Monkey, you've got to snap out of it.</td>\n      <td>monkey, you have to wake up.</td>\n      <td>0.664333</td>\n      <td>0.309524</td>\n      <td>0.053362</td>\n      <td>0.994215</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>I've got orders to put her down.</td>\n      <td>I have orders to kill her.</td>\n      <td>0.726639</td>\n      <td>0.181818</td>\n      <td>0.009402</td>\n      <td>0.999348</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T17:50:19.165808Z",
     "start_time": "2023-11-05T17:50:19.143814300Z"
    }
   },
   "id": "81892eef07d7dd26"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# read tsv file cleaned_new\n",
    "df = pd.read_csv('../data/final_cleaned.tsv', sep='\\t')\n",
    "\n",
    "# remove rows with empty reference or translation\n",
    "df = df[df['reference'].notna()]\n",
    "df = df[df['translation'].notna()]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T17:50:21.869787600Z",
     "start_time": "2023-11-05T17:50:19.158808300Z"
    }
   },
   "id": "ff12f372b1856c2c"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T17:50:21.886323400Z",
     "start_time": "2023-11-05T17:50:21.873357Z"
    }
   },
   "id": "e4ba924a2f0de72f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GENERATIVE MODEL\n",
    "\n",
    "##### Will take original toxic sentence as input and generate a non-toxic sentence"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b7fd2c5930c3b14"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "X = df['reference']\n",
    "y = df['translation']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T17:50:22.493442700Z",
     "start_time": "2023-11-05T17:50:21.886323400Z"
    }
   },
   "id": "781e3cf8915d8ab4"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SunagatullinAyaz\\.conda\\envs\\pythonProject2\\lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.6.0) was trained with spaCy v3.6.0 and may not be 100% compatible with the current version (3.7.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# prepare pandas dataframes of sentences for training\n",
    "df = pd.DataFrame({'original': X, 'translation': y})\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return [token.text.lower() for token in nlp.tokenizer(text) if not token.is_space]\n",
    "\n",
    "\n",
    "df['original'] = df['original'].apply(tokenize)\n",
    "df['translation'] = df['translation'].apply(tokenize)\n",
    "# take only 100 rows\n",
    "original_data = original_data[:100000]\n",
    "original_data['reference'] = original_data['reference'].apply(tokenize)\n",
    "original_data['translation'] = original_data['translation'].apply(tokenize)\n",
    "\n",
    "\n",
    "source_vocab = set(token for tokens in original_data['reference'] for token in tokens).union(\n",
    "    set(token for tokens in df['original'] for token in tokens))\n",
    "target_vocab = set(token for tokens in original_data['translation'] for token in tokens).union(\n",
    "    set(token for tokens in df['translation'] for token in tokens))\n",
    "\n",
    "source_vocab.add(\"<PAD>\")\n",
    "target_vocab.add(\"<PAD>\")\n",
    "target_vocab.add(\"<SOS>\")\n",
    "target_vocab.add(\"<EOS>\")\n",
    "\n",
    "# add <UNK> token to vocabularies\n",
    "source_vocab.add(\"<UNK>\")\n",
    "target_vocab.add(\"<UNK>\")\n",
    "\n",
    "\n",
    "# Create vocabularies with integer mappings\n",
    "source_vocab_to_int = {token: i for i, token in enumerate(source_vocab)}\n",
    "target_vocab_to_int = {token: i for i, token in enumerate(target_vocab)}\n",
    "# Reverse mappings\n",
    "int_to_source_vocab = {i: token for token, i in source_vocab_to_int.items()}\n",
    "int_to_target_vocab = {i: token for token, i in target_vocab_to_int.items()}\n",
    "\n",
    "# add <UNK> token to vocabularies\n",
    "source_vocab_to_int['<UNK>'] = len(source_vocab_to_int)\n",
    "target_vocab_to_int['<UNK>'] = len(target_vocab_to_int)\n",
    "\n",
    "# add <UNK> token to vocabularies\n",
    "int_to_source_vocab[len(source_vocab_to_int)] = '<UNK>'\n",
    "int_to_target_vocab[len(target_vocab_to_int)] = '<UNK>'\n",
    "\n",
    "\n",
    "# Step 4: Sequence Padding\n",
    "max_source_length = max(len(tokens) for tokens in df['original'])\n",
    "max_target_length = max(len(tokens) for tokens in df['translation'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T17:51:02.444267700Z",
     "start_time": "2023-11-05T17:50:22.502550500Z"
    }
   },
   "id": "29db65fd6b85b31d"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# save all 4 vocabularies\n",
    "import pickle\n",
    "\n",
    "with open('../models/source_vocab_to_int.pickle', 'wb') as handle:\n",
    "    pickle.dump(source_vocab_to_int, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('../models/target_vocab_to_int.pickle', 'wb') as handle:\n",
    "    pickle.dump(target_vocab_to_int, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('../models/int_to_source_vocab.pickle', 'wb') as handle:\n",
    "    pickle.dump(int_to_source_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('../models/int_to_target_vocab.pickle', 'wb') as handle:\n",
    "    pickle.dump(int_to_target_vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T17:51:02.555000400Z",
     "start_time": "2023-11-05T17:51:02.449266300Z"
    }
   },
   "id": "11f0cbcc0cc21de6"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# Define a custom dataset class for your data\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source_data, target_data, source_vocab_to_int, target_vocab_to_int):\n",
    "        self.source_data = source_data\n",
    "        self.target_data = target_data\n",
    "        self.source_vocab_to_int = source_vocab_to_int\n",
    "        self.target_vocab_to_int = target_vocab_to_int\n",
    "        self.pad_int = self.target_vocab_to_int['<PAD>']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source_sequence = self.source_data[idx]\n",
    "        target_sequence = self.target_data[idx]\n",
    "\n",
    "        source_sequence_int = [self.source_vocab_to_int[word] for word in source_sequence]\n",
    "        target_sequence_int = [self.target_vocab_to_int[word] for word in target_sequence]\n",
    "\n",
    "        # Pad sequences to the maximum length\n",
    "        source_sequence_int = source_sequence_int + [self.pad_int] * (max_source_length - len(source_sequence_int))\n",
    "        target_sequence_int = target_sequence_int + [self.pad_int] * (max_target_length - len(target_sequence_int))\n",
    "\n",
    "        # Create a mask to ignore padding during training\n",
    "        source_mask = [1 if token != self.pad_int else 0 for token in source_sequence_int]\n",
    "\n",
    "        return {\n",
    "            'source_sequence': torch.tensor(source_sequence_int),\n",
    "            'target_sequence': torch.tensor(target_sequence_int),\n",
    "            'source_mask': torch.tensor(source_mask)\n",
    "        }\n",
    "\n",
    "\n",
    "# Create datasets and data loaders\n",
    "translation_dataset = TranslationDataset(df['original'], df['translation'], source_vocab_to_int, target_vocab_to_int)\n",
    "batch_size = 32  # Adjust as needed"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T17:51:02.593800600Z",
     "start_time": "2023-11-05T17:51:02.572994700Z"
    }
   },
   "id": "e3d68cf9cdffd5f2"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def custom_Collate(batch):\n",
    "    batch = [item for item in batch if item['source_sequence'].shape[0] > 0 and item['target_sequence'].shape[0] > 0]\n",
    "    return {\n",
    "        'source_sequence': pad_sequence([item['source_sequence'] for item in batch], batch_first=False),\n",
    "        'target_sequence': pad_sequence([item['target_sequence'] for item in batch], batch_first=False),\n",
    "        'source_mask': pad_sequence([item['source_mask'] for item in batch], batch_first=False)\n",
    "    }\n",
    "\n",
    "data_loader = DataLoader(translation_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_Collate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T17:51:02.807099900Z",
     "start_time": "2023-11-05T17:51:02.797538100Z"
    }
   },
   "id": "6427a01b40de160f"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "import random\n",
    "# create seq2seq model with attention, encoder and decoder\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embedding_size, num_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, source_sequence, source_mask):\n",
    "        # source_sequence shape: (source_sequence_length, batch_size)\n",
    "        # source_mask shape: (source_sequence_length, batch_size)\n",
    "\n",
    "        embedded = self.dropout(self.embedding(source_sequence))\n",
    "        # embedded shape: (source_sequence_length, batch_size, embedding_size)\n",
    "\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, source_mask.cpu().sum(0).long(), enforce_sorted=False)\n",
    "        outputs, (hidden, cell) = self.lstm(packed)\n",
    "        # outputs shape: (source_sequence_length, batch_size, hidden_size)\n",
    "        # hidden shape: (num_layers, batch_size, hidden_size)\n",
    "        # cell shape: (num_layers, batch_size, hidden_size)\n",
    "\n",
    "        return outputs, hidden, cell\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embedding_size, num_layers, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, target_sequence, hidden, cell, encoder_outputs):\n",
    "        # target_sequence shape: (batch_size)\n",
    "        # hidden shape: (num_layers, batch_size, hidden_size)\n",
    "        # cell shape: (num_layers, batch_size, hidden_size)\n",
    "        # encoder_outputs shape: (source_sequence_length, batch_size, hidden_size)\n",
    "\n",
    "        target_sequence = target_sequence.unsqueeze(0)\n",
    "        # target_sequence shape: (1, batch_size)\n",
    "\n",
    "        hidden = hidden.view(self.num_layers, target_sequence.size(1), self.hidden_size)\n",
    "        cell = cell.view(self.num_layers, target_sequence.size(1), self.hidden_size)\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(target_sequence))\n",
    "        # embedded shape: (target_sequence_length, batch_size, embedding_size)\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        # output shape: (1, batch_size, hidden_size)\n",
    "        # hidden shape: (num_layers, batch_size, hidden_size)\n",
    "        # cell shape: (num_layers, batch_size, hidden_size)\n",
    "\n",
    "        prediction = self.fc(outputs.squeeze(0))\n",
    "        # prediction shape: (batch_size, input_size)\n",
    "\n",
    "        return prediction, hidden, cell\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, source_sequence, source_mask, target_sequence):\n",
    "        # source_sequence shape: (source_sequence_length, batch_size)\n",
    "        # source_mask shape: (source_sequence_length, batch_size)\n",
    "        # target_sequence shape: (target_sequence_length, batch_size)\n",
    "\n",
    "        batch_size = source_sequence.shape[1]\n",
    "        target_sequence_length = target_sequence.shape[0]\n",
    "        target_vocab_size = self.decoder.input_size\n",
    "\n",
    "        outputs = torch.zeros(target_sequence_length, batch_size, target_vocab_size).to(self.device)\n",
    "        # outputs shape: (target_sequence_length, batch_size, target_vocab_size)\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(source_sequence, source_mask)\n",
    "        # encoder_outputs shape: (source_sequence_length, batch_size, hidden_size)\n",
    "        # hidden shape: (num_layers, batch_size, hidden_size)\n",
    "        # cell shape: (num_layers, batch_size, hidden_size)\n",
    "\n",
    "        # First input to the decoder is the <SOS> tokens\n",
    "        input = target_sequence[0, :]\n",
    "\n",
    "        for t in range(1, target_sequence_length):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            # output shape: (batch_size, target_vocab_size)\n",
    "            outputs[t] = output\n",
    "            # outputs shape: (target_sequence_length, batch_size, target_vocab_size)\n",
    "\n",
    "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            # With probability 0.5 we take the actual next word in the sequence\n",
    "            # otherwise we take the word that the Decoder predicted it to be.\n",
    "            input = target_sequence[t] if random.random() < 0.5 else best_guess\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def predict(self, source_sequence, source_mask, target_vocab_to_int, int_to_target_vocab):\n",
    "        # source_sequence shape: (source_sequence_length, batch_size)\n",
    "        # source_mask shape: (source_sequence_length, batch_size)\n",
    "\n",
    "        batch_size = source_sequence.shape[1]\n",
    "        target_vocab_size = self.decoder.input_size\n",
    "        max_target_length = 100\n",
    "\n",
    "        outputs = torch.zeros(max_target_length, batch_size, target_vocab_size).to(self.device)\n",
    "        # outputs shape: (target_sequence_length, batch_size, target_vocab_size)\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(source_sequence, source_mask)\n",
    "        # encoder_outputs shape: (source_sequence_length, batch_size, hidden_size)\n",
    "        # hidden shape: (num_layers, batch_size, hidden_size)\n",
    "        # cell shape: (num_layers, batch_size, hidden_size)\n",
    "\n",
    "        # First input to the decoder is the <SOS> tokens\n",
    "        input = torch.tensor([target_vocab_to_int['<SOS>']] * batch_size).to(self.device)\n",
    "\n",
    "        for t in range(1, max_target_length):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "            # output shape: (batch_size, target_vocab_size)\n",
    "            outputs[t] = output\n",
    "            # outputs shape: (target_sequence_length, batch_size, target_vocab_size)\n",
    "\n",
    "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            # With probability 0.5 we take the actual next word in the sequence\n",
    "            # otherwise we take the word that the Decoder predicted it to be.\n",
    "            input = best_guess\n",
    "\n",
    "        # Remove <SOS> token\n",
    "        outputs = outputs[1:]\n",
    "        # outputs shape: (target_sequence_length, batch_size, target_vocab_size)\n",
    "\n",
    "        # Get the best word indices (indexes in the vocabulary) per time step\n",
    "        best_guess = outputs.argmax(2)\n",
    "        # best_guess shape: (target_sequence_length, batch_size)\n",
    "\n",
    "        # Convert the indices into actual words\n",
    "        decoded_words = []\n",
    "        for i in range(best_guess.shape[1]):\n",
    "            \n",
    "            predicted_sentence = [int_to_target_vocab[int_.item()] for int_ in best_guess[:, i]]\n",
    "            decoded_words.append(' '.join(predicted_sentence))\n",
    "\n",
    "        return decoded_words\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T17:51:02.868733800Z",
     "start_time": "2023-11-05T17:51:02.824107800Z"
    }
   },
   "id": "8704b63a69f0dc1e"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 1\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "\n",
    "# Model hyperparameters\n",
    "input_size_encoder = len(source_vocab_to_int)\n",
    "input_size_decoder = len(target_vocab_to_int)\n",
    "output_size = len(target_vocab_to_int)\n",
    "encoder_embedding_size = 256\n",
    "decoder_embedding_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T17:51:29.029371200Z",
     "start_time": "2023-11-05T17:51:28.983375300Z"
    }
   },
   "id": "e6b0f01797569fac"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84317\n",
      "60639\n"
     ]
    }
   ],
   "source": [
    "print(input_size_encoder)\n",
    "print(input_size_decoder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T17:51:29.452393100Z",
     "start_time": "2023-11-05T17:51:29.415390600Z"
    }
   },
   "id": "20e511e660ec7919"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# Initialize network\n",
    "encoder_net = Encoder(input_size_encoder, hidden_size, encoder_embedding_size, num_layers, encoder_dropout)\n",
    "decoder_net = Decoder(input_size_decoder, hidden_size, decoder_embedding_size, num_layers, decoder_dropout)\n",
    "model = Seq2Seq(encoder_net, decoder_net, device).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T17:51:30.691078300Z",
     "start_time": "2023-11-05T17:51:29.430395Z"
    }
   },
   "id": "b8b7007ec37ca235"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "pad_int = target_vocab_to_int['<PAD>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_int)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T17:51:30.724336Z",
     "start_time": "2023-11-05T17:51:30.698201900Z"
    }
   },
   "id": "8df9c31a0b461908"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 794.00 MiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of the allocated memory 2.86 GiB is allocated by PyTorch, and 617.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[40], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m source_mask \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msource_mask\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# Forward\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource_sequence\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msource_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_sequence\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# outputs shape: (target_sequence_length, batch_size, target_vocab_size)\u001B[39;00m\n\u001B[0;32m     12\u001B[0m outputs \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m1\u001B[39m:]\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, outputs\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m])\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[28], line 90\u001B[0m, in \u001B[0;36mSeq2Seq.forward\u001B[1;34m(self, source_sequence, source_mask, target_sequence)\u001B[0m\n\u001B[0;32m     87\u001B[0m target_sequence_length \u001B[38;5;241m=\u001B[39m target_sequence\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     88\u001B[0m target_vocab_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder\u001B[38;5;241m.\u001B[39minput_size\n\u001B[1;32m---> 90\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtarget_sequence_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_vocab_size\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     91\u001B[0m \u001B[38;5;66;03m# outputs shape: (target_sequence_length, batch_size, target_vocab_size)\u001B[39;00m\n\u001B[0;32m     93\u001B[0m encoder_outputs, hidden, cell \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(source_sequence, source_mask)\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 794.00 MiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of the allocated memory 2.86 GiB is allocated by PyTorch, and 617.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Train Network\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, batch in enumerate(data_loader):\n",
    "        \n",
    "        source_sequence = batch['source_sequence'].to(device)\n",
    "        target_sequence = batch['target_sequence'].to(device)\n",
    "        source_mask = batch['source_mask'].to(device)\n",
    "\n",
    "        # Forward\n",
    "        outputs = model(source_sequence, source_mask, target_sequence)\n",
    "        # outputs shape: (target_sequence_length, batch_size, target_vocab_size)\n",
    "        outputs = outputs[1:].reshape(-1, outputs.shape[2])\n",
    "        target_sequence = target_sequence[1:].reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, target_sequence)\n",
    "\n",
    "        # Backward\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip to avoid exploding gradient issues\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss every 100 batches\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T17:51:49.309334400Z",
     "start_time": "2023-11-05T17:51:48.859856100Z"
    }
   },
   "id": "dcec37aa1771f2e7"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "1350"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T17:51:45.582062500Z",
     "start_time": "2023-11-05T17:51:43.898032900Z"
    }
   },
   "id": "92132a8176514747"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_sentence = \"Now you are getting very nasty\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T17:51:05.762176500Z",
     "start_time": "2023-11-05T17:51:05.760176400Z"
    }
   },
   "id": "f61c09c4790fc1f0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prepare_sent(sentence: str):\n",
    "    print(type(sentence))\n",
    "    sentence = tokenize(sentence)\n",
    "    sentence = [source_vocab_to_int.get(word, source_vocab_to_int['<UNK>']) for word in sentence]\n",
    "    sentence = torch.tensor(sentence).unsqueeze(1).to(device)\n",
    "    return sentence"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T17:51:05.765178900Z",
     "start_time": "2023-11-05T17:51:05.764178400Z"
    }
   },
   "id": "13d2a00810ff302e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sent = prepare_sent(test_sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-05T17:51:05.769178200Z"
    }
   },
   "id": "c74c48f42974826d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# predict\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    translation = model.predict(sent,\n",
    "                                torch.ones(sent.shape).to(device),\n",
    "                                target_vocab_to_int,\n",
    "                                int_to_target_vocab)\n",
    "    \n",
    "print(translation)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-05T17:51:05.771179300Z"
    }
   },
   "id": "f5f0be0a405a1e54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), '../models/seq2seq_NEW_model.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-05T17:51:05.774179800Z"
    }
   },
   "id": "849dcc3f70e01d75"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
