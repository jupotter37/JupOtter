{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "170eb7f3-b83b-4958-93c9-77e0469e4cc4",
   "metadata": {},
   "source": [
    "# 3 Introduction\n",
    "* How to set up the dependencies for this book\n",
    "* Model Integrations\n",
    "* Building an application for customer service\n",
    "\n",
    "### Setting up the dependencies for this learning\n",
    "- Python version 3.10 or 3.11\n",
    "\n",
    "Suggest to use conda and pip for local environment, and if only needed we will use Docker.\n",
    "\n",
    "A set of instructions and the corresponding configuration files in the book's repository at https://github.com/benman1/generative_ai_with_langchain includes these files:\n",
    "- requirement.txt for pip\n",
    "- pyproject.toml for Poetry\n",
    "- langchain_ai.yaml for Conda\n",
    "- Dockerfile for Docker.\n",
    "\n",
    "For all instructions, please make sure to have book's repository download (using the GitHub user interface) or cloned on your computer, and you have changed into the project's root directory.\n",
    "\n",
    "This sets up a reproducible environment to run all examples in the book.\n",
    "\n",
    "#### pip\n",
    "\n",
    "1. If not already included in your Python distribution, install pip following the instructions here: https://pip.pypa.io/.\n",
    "2. Use a virtual environment for isolation (for example, venv).\n",
    "3. Install the dependencies from requirements.txt:\n",
    "\n",
    "   ``` pip install -r requirements.txt```\n",
    "\n",
    "#### Conda\n",
    "Conda manages Python environments and dependencies. To use Conda:\n",
    "1. Install Miniconda or Anaconda following the instructions from this link: https://docs.continuum.io/anaconda/install/.\n",
    "2. Create the environment from langchain_ai.yml\n",
    "   \n",
    "   ```conda env create --langchain_ai.yml```\n",
    "4. Activate the environment:\n",
    "   \n",
    "   ``` conda activate langchain_ai```\n",
    "\n",
    "#### Docker\n",
    "Docker provides isolated, reproducible environ,ents using containers. To use Docker:\n",
    "1. Install Docker Engine; follow the installa instructions here: _https://docs.docker.com/get-docker/._\n",
    "2. Build the Docker image from the Dockerfile in this repository\n",
    "\n",
    "   ```docker build -t langchain_ai```\n",
    "4. Run the Docker container interactively\n",
    "\n",
    "   ```docker run -it langchain-ai```\n",
    "\n",
    "### Exploring API model integrations\n",
    "Full list of supported integrations for LLMs at https://integrations.langchain.com/llms. (Oct 2023 screenshot)\n",
    "\n",
    "![](../fig/f3-1.png)\n",
    "\n",
    "LangChain implements three interfaces - we can use chat models, LLMs, and embedding models. Chat models and LLMs are similar in that they both process text input and produce text output. However, their differences are in the types of input and output they handle. Chat models are specifically designed to handle a list of chat messages as input and generate a chat message as output. They are commonly used in chatbot applications where conversations are exchanged. E.g. https://python.langchain.com/docs/integrations/chat.\n",
    "\n",
    "Finally, text embedding models are used to convert text inputs into numerical representations called embeddings. Focus in this chapter is on text generation, while embeddings, vector databases, and neural search in _Chapter 5_. Embeddings are a way to capture and extract information from the input text, used in NLP tasks such as sentiment analysis, text classification, and information retrieval. E.g. https://python.langchain.com/docs/integrations/text_embedding.\n",
    "\n",
    "For image models we can refer to OpenAI (DALL-E), Midjourney, Inc. (Midjourney) and Stability AI (Stable Diffusion). LangChain currently doesn not have out-of-the-box handling of models that are not for text; however, its its documentation describe how to work with Replicate, which also provides an interface to Stable Diffusion models.\n",
    "\n",
    "---\n",
    "For each of these providers, to make calls against their API, an account has to be created and obtain an API key. To set an API key in an environment, in Python, we can execute the following lines:\n",
    "```python\n",
    "import os \n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<your token>\"\n",
    "```\n",
    "\n",
    "Here, `OPENAI_API_KEY` is the environment key that is appropriate for OpenAI. Setting the keys in your environment has the advantage of not needing to include them as parameters in your code every time you use a model or service integration. \n",
    "\n",
    "---\n",
    "--_Settings in Linux and macOS_--\n",
    "\n",
    "Alternatively, these variables can be exposed in the system environment from the terminal. In Linux and macOS the system environment variable from the terminal using the *export* command:\n",
    "```\n",
    "    export OPENAI_API_KEY=<your token>\n",
    "```\n",
    "\n",
    "To permanently set the environment variable in Linux or macOS, you would need to add the preceding line to the `~/.bashrc` or `~/.bash_profile` file, respectively, and then reload the shell using the command `source ~/.bashrc` or `source ~/.bash_profile`.\n",
    "\n",
    "--_Settings in Windows_--\n",
    "\n",
    "In Windows, you can set a system environment variable from the command prompt using the set command:\n",
    "```\n",
    "set OPENAI_API_KEY=<your token>\n",
    "```\n",
    "To permanently set the environment variable in Windows, the preceding line can be added to the batch script. For instance, create the config.py file to store the keys, then import a function from this module that will load all these keys into the environment. The `config.py` can be described as follows:\n",
    "\n",
    "```python\n",
    "import os \n",
    "OPENAI_API_KEY = \"...\"\n",
    "# Other keys can put here\n",
    "def set_environment():\n",
    "    variable_dict = globals().items()\n",
    "    for key, value in variable_dict:\n",
    "        if \"API\" in key or \"ID\" in key:\n",
    "            os.environ[key] = value\n",
    "```\n",
    "This function loads the keys into the environment as it is mentioned.\n",
    "```python\n",
    "from config import set_environment\n",
    "set_environment()\n",
    "```\n",
    "\n",
    "### Fake LLM\n",
    "The fake LLM allows you to simulate LLM responses during testing without needing actual API calls. This is useful for rapid prototyping and unit testing agents. Using the FakeLLM avoid hitting rate limits during testings. The fake LLM is only for esting purposes. The LangChain documentation has an example of tool use with LLMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ecee0ab-ded9-4cb1-b6c5-fce23ca15f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FakeListLLM(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, responses=['Hello'], sleep=None, i=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms.fake import FakeListLLM\n",
    "fake_llm =  FakeListLLM(responses=['Hello'])\n",
    "fake_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa909732-1475-4209-9610-eb76f121e33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mAction: Python_REPL\n",
      "Action Input: print(2 + 2)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m4\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: 4\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms.fake import FakeListLLM\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "tools = load_tools([\"python_repl\"])\n",
    "responses = [\"Action: Python_REPL\\nAction Input: print(2 + 2)\", \"Final Answer: 4\"]\n",
    "llm = FakeListLLM(responses=responses)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "agent.run(\"whats 2+2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee2d05d-4f20-4f8a-86c1-2e09e9b45849",
   "metadata": {},
   "source": [
    "The above code setup an agent that makes decisions based on React strategy that we explained in _Chapter 2, LangChain for LM Apps_ `(ZERO_sHOT_REACT_DESCRIPTION)`. We run the agent with the text: the qeustion what's 2 + 2.\n",
    "\n",
    "By connecting the tool, a Python Read-Eval-Print-Loop (REPL), that will be called depending on the output of the LLM. FakeListLLM will give two responses `(\"Action: Python_REPL\\nAction Input: print(2 + 2)\" and \" Final Answer: 4\") that won't changed based on the input.\n",
    "\n",
    "We can also observe how the fake LLM output leads to a call to the Python interpreter, which returns 4. Please note that the action must match the `name` attribute of the tool, `PythonREPL Tool`, which starts like this:\n",
    "\n",
    "```python\n",
    "class PythonREPLTool(BaseTool):\n",
    "    \"\"\"A tool for running python code in a REPL.\"\"\"\n",
    "    name = \"Python_REPL\"\n",
    "    description = ( \"A Python shell. Use this to execute python commands.\"\n",
    "                \"Input should be a valid python command.\"\n",
    "                \"If you want to see the output of a value, you should print it out \"\n",
    "                \"with `print(...)`.\"\n",
    "                  )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404da165-2378-4c96-95ca-5899b335fb86",
   "metadata": {},
   "source": [
    "# 3.1 OpenAI\n",
    "In this chapter we will learn how to interact with OpenAI models with the LangChain and the OpenAI python client libraries. OpenAI also offers an __Embedding__ class for text embedding models.\n",
    "\n",
    "When a prompt is send to an LLM API, it processes the prompt word by word, breaking down (tokenizing) the text into individual tokens. The number of tokens directly correlates with the amount of text. When uing commercial LLMs like GPT-3 and GPT-4 via APIs, each token has an associated cost based on factors like the LLM model and API pricing tiers. Token usage refers to how many tokens from the model's quota have been consumed to generate a response. Strategies like using smaller models, summarizing outputs, and preprocessing inputs help reduce the tokens required to get useful results. Being aware of token usage is key for optimizing productivity within budge constraints when leveraging commercial LLMs.\n",
    "\n",
    "To obatin an OpenAI API key first, create the API key with the following steps:\n",
    "1. Create a login at _https://platform.openai.com/._\n",
    "2. Setup your billing information.\n",
    "3. You can see the API keys under __Personal | View API Keys__.\n",
    "4. Click on __Create new secret key__ and give a name.\n",
    "\n",
    "![](../fig/f3-2.png)\n",
    "\n",
    "Copy the key generated to set the key as environment variable (`OPEN_API_KEY`) or pass it as a parameter every time you construct a class for OpenAI calls.\n",
    "\n",
    "Using the OpenAI language model class to set up an LLM to interact with and create an agent that calculates using this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22652418-9cb4-48ad-be59-31813ba8d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activating the OpenAI\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import set_environment\n",
    "set_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "675acc08-8e6c-4136-9c36-29fc2455bf65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"When you can't fall asleep because reality is finally better than your dreams.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "llm = OpenAI(temperature=0., model=\"gpt-3.5-turbo-instruct\")\n",
    "#llm = OpenAI(temperature=0., model=\"text-davinci-003\")\n",
    "tools = load_tools([\"python_repl\"])\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbse=True)\n",
    "agent.run(\"How to know you are in love?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a34fc85a-e7d5-49cb-9bbf-ec74593c7152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agent stopped due to iteration limit or time limit.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"What is the current Singapore time?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d271c3c6-33fe-4799-ae62-6291f4dee014",
   "metadata": {},
   "source": [
    "For the above process I have been charged $5USD for my OpenAI platform billing. Next up HuggingFace provider."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae7a7a7-a14f-4e53-8acb-3959d58fac6a",
   "metadata": {},
   "source": [
    "# 3.2 Hugging Face\n",
    "A prominent player in the NLP space and has considerable traction in open-source and hosting solutions. It develops tools for building machine learning applications, e.g., the Transformers Python library, which is used for NLP tasks, includes implementations of state-of-the-art and popular models like Mistral 7B, BERT, and GPT-2, and is compatible with PyTorch, TensorFlow, and JAX.\n",
    "\n",
    "Hugging Face also provides the Hugging Face Hub, a platform for hosting Git-based code repositories, machine learning models, datasets, and web applications, which provides over 120k models, 20k datasets, and 50k demo apps (spaces) for machine learning. It is an online patform where people can collaborate and facilitate machine learning development.\n",
    "\n",
    "These tools allow users to load and use models, embeddings, and datasets from Hugging Face. The `HuggingFaceHub` integration, e.g., provides access to different models for tasks like text generation and text classification. The `HuggingfaceEmbeddings` integration allows users to work with sentence-transformer models.\n",
    "\n",
    "Hugging Face also offers various other libraries within their ecosystem, including `Datasets` for dataset processing, `Evaluate` for model evaluation, `Simulate` for simulation, and `Gradio` for machine learning demos.\n",
    "\n",
    "In addition, Hugging Face has been involved in initiatives such as the BigScience Research Workshop, where they releaed an open LLM called BLOOM with 176 billion parameters. They have receiveed \\$40 million Series B round and a Series C funding round led by Coatue and Sequoia at a \\\\$2 billion valuation.\n",
    "\n",
    "To use Hugging Face as a provider for your models, you can create an account and API keys at _https://huggingface.co/settings/profile_. Additionally, you can make the token available in your environment as `HUGGINGFACEHUB_API_TOKEN`.\n",
    "\n",
    "In the following, is an example as a open-source model developed by Google, the Flan-T5-XXL model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4813a197-f906-4ceb-9d2a-5a0cbb26cedf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error raised by inference API: Cannot override task for LLM models",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m llm \u001b[38;5;241m=\u001b[39m HuggingFaceHub(model_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m64\u001b[39m}, repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-t5-xxl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn which country is Tokyo?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(completion)\n",
      "File \u001b[0;32m~/miniconda3/envs/aigc/lib/python3.11/site-packages/langchain/llms/base.py:825\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    823\u001b[0m     )\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    835\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/aigc/lib/python3.11/site-packages/langchain/llms/base.py:621\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    614\u001b[0m         )\n\u001b[1;32m    615\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    616\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    617\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m), [prompt], invocation_params\u001b[38;5;241m=\u001b[39mparams, options\u001b[38;5;241m=\u001b[39moptions\n\u001b[1;32m    618\u001b[0m         )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    619\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m callback_manager, prompt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(callback_managers, prompts)\n\u001b[1;32m    620\u001b[0m     ]\n\u001b[0;32m--> 621\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/aigc/lib/python3.11/site-packages/langchain/llms/base.py:523\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    522\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 523\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    524\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/miniconda3/envs/aigc/lib/python3.11/site-packages/langchain/llms/base.py:510\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    502\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    507\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    509\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 510\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    514\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    518\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    519\u001b[0m         )\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    521\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/miniconda3/envs/aigc/lib/python3.11/site-packages/langchain/llms/base.py:1000\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    997\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m    999\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1000\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1003\u001b[0m     )\n\u001b[1;32m   1004\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/miniconda3/envs/aigc/lib/python3.11/site-packages/langchain/llms/huggingface_hub.py:112\u001b[0m, in \u001b[0;36mHuggingFaceHub._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient(inputs\u001b[38;5;241m=\u001b[39mprompt, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by inference API: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# Text generation return includes the starter text.\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     text \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;28mlen\u001b[39m(prompt) :]\n",
      "\u001b[0;31mValueError\u001b[0m: Error raised by inference API: Cannot override task for LLM models"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "llm = HuggingFaceHub(model_kwargs={\"temperature\":0.5, \"max_length\": 64}, repo_id=\"google/flan-t5-xxl\")\n",
    "prompt = \"In which country is Tokyo?\"\n",
    "completion = llm(prompt)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b103848-5409-420e-aaf9-19721ce4d0a3",
   "metadata": {},
   "source": [
    "Above error is due to non-subscription of their inference model. Later, we will see a local machine running of hugging face models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2526d6ca-c13f-428e-b350-ca93e70374e6",
   "metadata": {},
   "source": [
    "# 3.3 Google  CLoud Platform\n",
    "There are many models and function available through Google Cloud Platform (GCP) and Vertex AI, GCP's machine learning platform. GCP access to LLMs like LaMDA, T5, and PaLM. Google has also updated the Google Cloud Natural Language (NL) API with a new LLM-based model for Content Classification. The updated version offers an expansive pre-trained classification taxonomy to help with ad targeting and content-based filtering. The NL API's improved v2 classification model is enhanced with over 1,000 labels and supports 11 languages with improved accuracy.\n",
    "\n",
    "For models with GCP, you need to have the gcloud command-line interface (CLI) installed. The instructions are here: _https://cloud.google.com/sdk/docs/install_. \n",
    "\n",
    "Authentication and print a key token with this command from the terminal:\n",
    "\n",
    "```\n",
    "gcloud auth application-default login\n",
    "```\n",
    "\n",
    "You also need to enable Vertex AI for your project. To enable Vertex AI, install the Google Vertex AI SDK with the :\n",
    "```\n",
    "pip install google-cloud-aiplatform\n",
    "```\n",
    "This should have been installed based on previous package installation.\n",
    "\n",
    "To set up the Google Cloud project ID, there are different options for this:\n",
    "* Using `gcloud config set project my-project`\n",
    "* Passing a constructor argument when initializing the LLM\n",
    "* Using aiplatform.init()\n",
    "* Setting a GCOP environment variable\n",
    "\n",
    "You can find more details about these options in the Vertex documentation. The GCP environment variable works well with the config.py file created earlier (utils).\n",
    "\n",
    "```python\n",
    "from langchain.llms import VertexAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "template = \"\"\"Question: {question} Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm = VertexAI()\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "llm_chain.run(question)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b044a442-49ab-4983-917a-485d83d13b65",
   "metadata": {},
   "source": [
    "Vertex AI offers a range of models tailored for tasks following instructions, conversation, and code generation/assistance:\n",
    "* __text-bison__ is fine-tuned to follow natural language instructions, with a max input of 8,192 tokens and an output of 1,024\n",
    "* __chat-bison__ is optimized for multi-turn conversation with a max input of 4096 tokens, an output of 1024 tokens and up to 2,500 turns.\n",
    "* __code-bison__ generates code from natural language descriptions, with a max input of 4096 tokens and an output of 2048 tokens\n",
    "* __codechat-bison__ is a chatbot that is fine-tuned to help with code-related questions. It has an input limit of 4096 tokens and an output limit of 2048 tokens.\n",
    "* __code-gecko__ suggests code completions. It has a max input length of 2048 tokens and an output of 64 tokens.\n",
    "\n",
    "These models also have different input/outputs limits and training data and are often updated. More detailed and up-to-date information about models including when models have been updated can be checked at _https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fbceff-77d5-4dd5-9256-1b624a19d3b6",
   "metadata": {},
   "source": [
    "Skipping following API providers:\n",
    "- Jina\n",
    "- Azure\n",
    "- Anthropic\n",
    "In the following section, will be exploring the usage of local models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac4bc5f-99f2-42aa-8bd1-8a07807e4904",
   "metadata": {},
   "source": [
    "# 3.5 Exploring Local Models\n",
    "The advantages of running models locally are complete control over the model and not sharing any data over the internet.\n",
    "* Do not need an API token for local models\n",
    "\n",
    "In this section, we will focus on Hugging Face's transformers, llama.cpp and GPT4All (also Mistral which has been installed previously). These tools provide huge power and are full of great functionality too broad to cover in this chapter. Thus, this chapter will focus on how we can run a model with the transformers library by Hugging Face.\n",
    "\n",
    "### Hugging Face Transformers\n",
    "A general recipe for setting up and running a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1def036f-5349-419c-afc0-7f2c307455a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the following if you don't have all libraries installed\n",
    "#!pip install transformers accelerate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5033c785-825b-42dd-bd9a-86ebc5733c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In this chapter, we'll discuss first steps with generative AI in Python. First, we'll discuss the best practice in the relevant language. Next, we'll briefly discuss the specifics of how generative AI works, focusing on areas like facial recognition, language modeling, and speech synthesis. Finally, we'll provide concrete examples to illustrate the work in practice.\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "generate_text = pipeline(\n",
    "        model=\"aisquared/dlite-v1-355m\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\", \n",
    "        framework=\"pt\"\n",
    "    )\n",
    "generate_text(\"In this chapter, we'll discuss first steps with generative AI in Python.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "370b8f5c-c2cd-4433-8acc-155986e294a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Weiyun wants a boyfriend because she is curious and wants to explore life on her own terms. She also values relationships and feels that having someone to share her life with is important for her success.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(\"Why does Wei Yun wants a boyfriend? Weiyun is a girl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949fac05-e8e4-4e8d-b0b7-50d0a89c983d",
   "metadata": {},
   "source": [
    "Running the preceding code will download everything that's needed for the model such as the tokenizer and model weights from Hugging Face. This model is quite small (355 million parameters) but relatively performant and instruction-tuned for conversations. To plug this pipeline into LangChain agent or chain, we can use it the same way that we've seen in the other examples in this chapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c5ca71b-47c4-46c3-8fc0-753d6028adcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for LLMChain\nllm\n  value is not a valid dict (type=type_error.dict)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{question}\u001b[39;00m\u001b[38;5;124m, Answer: Let\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms think step by step.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      3\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate(template\u001b[38;5;241m=\u001b[39mtemplate, input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 4\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m \u001b[43mLLMChain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerate_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is electroencephalography?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(llm_chain\u001b[38;5;241m.\u001b[39mrun(question))\n",
      "File \u001b[0;32m~/miniconda3/envs/aigc/lib/python3.11/site-packages/langchain/load/serializable.py:75\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/aigc/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for LLMChain\nllm\n  value is not a valid dict (type=type_error.dict)"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "template = \"\"\"Question: {question}, Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=generate_text)\n",
    "question = \"What is electroencephalography?\"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce86c84-1757-4ccf-b82c-14649b0d5290",
   "metadata": {},
   "source": [
    "Adhered to the template code given by the text, but it seems that it is not working. WIll skip this for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c96de72-c026-4b2f-801a-9b2821d5ba91",
   "metadata": {},
   "source": [
    "### llama.cpp\n",
    "Written and maintained by Georgi Gerganov llama.cpp is a C++ toolkit that executes models based on architectures based on or like LLaMA, one of the first large open-source models, which was released by Meta, and which spawned the development of many other models in turn. One of the main use cases of llama.cpp is to run models efficiently on the CPU; however, there are also some options for GPU.\n",
    "\n",
    "Please note that you need to have an `md5 checksum` tool installed. This is included by default in seeral Linux distributions such as Ubuntu. For macOS you can install it with brew like: `brew install md5sha1sum`.\n",
    "\n",
    "Firstly, to download the llama.cpp repository from GitHub or you can use a git command from the terminal like ths: `git clone https://github.com/ggerganov/llama.cpp.git`\n",
    "\n",
    "For Python requirements, we can perform with the pip package installer.\n",
    "```bash\n",
    "cd lama.cpp\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "If there is an error message at the end that a few libraries were missing, you might need to execute the following:\n",
    "```bash\n",
    "pip install 'blosc2==2.0.0' cython FuzzyTM\n",
    "```\n",
    "After that we will need to compile llama.cpp. This can be performed by parallelizing the build with 4 processes:\n",
    "```bash\n",
    "make -C . -j4 # runs make in subdir with 4 processes\n",
    "```\n",
    "\n",
    "To get the Llama model weights, you need to sign up with the T&Cs and wait for a registration email from Meta. There are tools such as the `llama` model downloader in the pyllama project, but they might not conform to the license stipulations by Meta.\n",
    "\n",
    "Alternative models with more permissive licensing such as Falcon or Mistral, Vicuna, OpeLLaMA, or Alpaca. Assuming you downloaded the model weights and tokenizer model for OpenLLaMA 3b/ LLaM-2_7B, the model file should be about 6.8 GB and the tokenizer is much smaller. Then move the two files into models/3B or models/7B directory.\n",
    "\n",
    "Then we have to convert the model to llama.cpp format, which is called ggml, using the convert script\n",
    "```bash\n",
    "python3 convert.py models/3B/ --ctx 2048\n",
    "```\n",
    "\n",
    "Optionally the models can be quantized to save memory when doing inference. Quantization refers to reducing the number of bits that are used to store weight:\n",
    "```bash\n",
    "./quantize ./models/3B/ggml-model-f16.gguf ./models/3B/ggml-model-q4_0.bin q4_0\n",
    "```\n",
    "\n",
    "This last file is much smaller and will take up much less space in memory as well. With the chosen model, we can integrate it into an agent or a chain, e.g., :\n",
    "```python\n",
    "llm = LlamaCpp(model_path=\"./ggml-model-q4_0.bin\", verbose=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edbd0c57-585d-4c34-8481-57982272cb50",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 196.00 MiB. GPU 0 has a total capacty of 7.79 GiB of which 10.56 MiB is free. Including non-PyTorch memory, this process has 7.76 GiB memory in use. Of the allocated memory 7.55 GiB is allocated by PyTorch, and 115.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# model_path = 'openlm-research/open_llama_7b_v2'\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m## v1 models\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# model_path = 'openlm-research/open_llama_3b'\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# model_path = 'openlm-research/open_llama_7b'\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# model_path = 'openlm-research/open_llama_13b'\u001b[39;00m\n\u001b[1;32m     13\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m LlamaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ: What is the largest animal?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mA:\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     19\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\n",
      "File \u001b[0;32m~/miniconda3/envs/aigc/lib/python3.11/site-packages/transformers/modeling_utils.py:3754\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3744\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3745\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3747\u001b[0m     (\n\u001b[1;32m   3748\u001b[0m         model,\n\u001b[1;32m   3749\u001b[0m         missing_keys,\n\u001b[1;32m   3750\u001b[0m         unexpected_keys,\n\u001b[1;32m   3751\u001b[0m         mismatched_keys,\n\u001b[1;32m   3752\u001b[0m         offload_index,\n\u001b[1;32m   3753\u001b[0m         error_msgs,\n\u001b[0;32m-> 3754\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3755\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3758\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3761\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3764\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3765\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3766\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3773\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3774\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/miniconda3/envs/aigc/lib/python3.11/site-packages/transformers/modeling_utils.py:4214\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[1;32m   4210\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   4211\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4212\u001b[0m                 )\n\u001b[1;32m   4213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4214\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4215\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4216\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4217\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4218\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4219\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4220\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4221\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4222\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4223\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4224\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4225\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4226\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4227\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4228\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4229\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4230\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4231\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/aigc/lib/python3.11/site-packages/transformers/modeling_utils.py:887\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    876\u001b[0m     state_dict_index \u001b[38;5;241m=\u001b[39m offload_weight(param, param_name, state_dict_folder, state_dict_index)\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m is_quantized\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m hf_quantizer\u001b[38;5;241m.\u001b[39mrequires_parameters_quantization)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m ):\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 887\u001b[0m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    889\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m~/miniconda3/envs/aigc/lib/python3.11/site-packages/accelerate/utils/modeling.py:400\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    398\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 400\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 196.00 MiB. GPU 0 has a total capacty of 7.79 GiB of which 10.56 MiB is free. Including non-PyTorch memory, this process has 7.76 GiB memory in use. Of the allocated memory 7.55 GiB is allocated by PyTorch, and 115.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "## v2 models\n",
    "model_path = 'openlm-research/open_llama_3b_v2'\n",
    "# model_path = 'openlm-research/open_llama_7b_v2'\n",
    "\n",
    "## v1 models\n",
    "# model_path = 'openlm-research/open_llama_3b'\n",
    "# model_path = 'openlm-research/open_llama_7b'\n",
    "# model_path = 'openlm-research/open_llama_13b'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path, torch_dtype=torch.float16, device_map='cuda:0',\n",
    ")\n",
    "\n",
    "prompt = 'Q: What is the largest animal?\\nA:'\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids, max_new_tokens=32\n",
    ")\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a3a6a5-84a8-465d-b127-236ce57f8303",
   "metadata": {},
   "source": [
    "# GPT4ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d1f47cb-2b4c-4bec-91b2-bde9948e529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.llms import GPT4All\n",
    "from gpt4all import GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2de8457e-f909-45fe-943a-dd3678d822cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT4All(\"/media/WachnResearch/Projects/llm/langchain/models/mistral7b_gpt4all/mistral-7b-openorca.gguf2.04.0.gguf\",device='gpu')\n",
    "response = model.generate(\"We can run large language models locally for all kinds of applications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0df8db7-6918-4b98-8e60-f5e89fb448c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' with the birth of this little monk in 1958 in Hong Kong. His parents named him after their favorite kung fu movie character at that time, which was Monkey.\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(\"Ok, I will tell you a story of Monkey Bro aka Pang Chun Ho. It all begins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43b0e633-fe43-4e1f-8b67-a444d296e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"gpt2\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 10},\n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230430e8-da99-47f4-b1d9-9c38ac7a081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative :\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)s\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10)\n",
    "hf = HuggingFacePipeline(pipeline=pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd1207a6-cff3-41a2-9f5b-1e40ea23d586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Electricity is electrical energy, something that\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | hf\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e514bb3-1139-4dd6-b7a0-50d5cb7380e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wachn/miniconda3/envs/aigc/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'InferenceApi' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "summarizer = HuggingFaceHub(repo_id=\"facebook/bart-large-cnn\", model_kwargs={\"temperature\":0, \"max_length\":180})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f0ab0a-5eef-4a0e-a1e1-37f2f80e65b5",
   "metadata": {},
   "source": [
    "I am writing to pour my heart out about the recent unfortunate experience I had with one of your coffee machines that arrived broken. I anxiously unwrapped the box containing my highly anticipated coffee machine. However, what I discovered within broke not only my spirit but also any semblance of confidence I had placed in your brand. Its once elegant exterior was marred by the scars of travel, resembling a war-torn soldier who had fought valiangtly on the fields of some espresso battlefield. This heartbreaking display of negligence shattered my dreams of indulging in daily coffee perfection, leaving me emotionally distraught an inconsolable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5668e6e-d1db-4f2f-b0ed-09b004fa8647",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_email = \"I am writing to pour my heart out about the recent unfortunate experience I had with one of your coffee machines that arrived broken. I anxiously unwrapped the box containing my highly anticipated coffee machine. However, what I discovered within broke not only my spirit but also any semblance of confidence I had placed in your brand. Its once elegant exterior was marred by the scars of travel, resembling a war-torn soldier who had fought valiangtly on the fields of some espresso battlefield. This heartbreaking display of negligence shattered my dreams of indulging in daily coffee perfection, leaving me emotionally distraught an inconsolable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6a76984-e63b-4d4f-aa76-b3a8419e53ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"I am writing to pour my heart out about the recent unfortunate experience I had with one of your coffee machines that arrived broken\" \"Its once elegant exterior was marred by the scars of travel, resembling a war-torn soldier who had fought valiangtly on the fields of some espresso battlefield. This heartbreaking display of negligence shattered my dreams of indulging in daily coffee perfection\"'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def summarize(llm, text)-> str: \n",
    "    return llm(f\"Summarize this: {text}!\")\n",
    "summarize(summarizer, customer_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91081a8d-81e4-4d28-bdbf-af5eab5f2b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Wearing a c up bra is like wearing a c-cup bra. It's not a bra, it's a bra with a hole in the front. It looks like a bra that's supposed to be on the back of a woman's body. The bra is supposed to cover the top of the body.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(summarizer, \"Wearing a c up bra 我天天想着猴哥，但是我心里有好多顾虑，真不知所措\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9bd130-5f61-41d2-b6d1-1a8116ffbf8f",
   "metadata": {},
   "source": [
    "This summary is just passable, but not very convincing. There is still a lot of rambling in the summary. Alternative models can be used or even asking LLM with a prompt to summarize the content. Which will be investigating into Chapter 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c24b8f6-e5c0-42d3-90f1-be1fcf74e041",
   "metadata": {},
   "source": [
    "### VertexAI\n",
    "- before executing the following code, ensure authentication has been done with GCP and set GCP project in previous section on Vertex AI.\n",
    "\n",
    "```python\n",
    "from langchain.llms import VertexAI\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "template = \"\"\"Given this text, decide what is the issue the customer is concerned about. Valid categories are these:\n",
    "* product issues\n",
    "* delivery problems\n",
    "* missing or late orders\n",
    "* wrong product\n",
    "* cancellation request\n",
    "* refund or exchange\n",
    "* bad support experience\n",
    "* no clear reason to be upset\n",
    "Text: {email}\n",
    "Category:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables = [\"email\"])\n",
    "llm = VertexAI()\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
    "print(llm_chain.run(customer_email))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215f1d4c-d832-424c-936b-05e0bf8212b2",
   "metadata": {},
   "source": [
    "# Summary\n",
    "4 distinct ways of installing LangChain and other libraries need in this book as an environment. Then introduced several providers of models for text and images. \n",
    "- Developed LLM app for text categorization (intent classification) and sentiment analysis in a use case for customer service\n",
    "- Q: How do you generate images with LangChain: E.g. stable diffusion with text-2-image replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b4674c-cdc5-4528-b13e-b61f34ddc311",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
