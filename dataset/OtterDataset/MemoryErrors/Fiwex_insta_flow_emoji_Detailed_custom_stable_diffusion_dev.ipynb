{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# InstaFlow Emoji"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ce4ca3c76e83d8b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. Introduction\n",
    "## 0.1. Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cc1105e1de7977c"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:01.473108462Z",
     "start_time": "2024-02-02T04:01:57.473839002Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2938/1432609872.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import CLIPTokenizer\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0.2. Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4e489a47ace86ce"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                             text  \\\nimage_id                                            \n0                               —É—Ö–º—ã–ª—è—é—â–µ–µ—Å—è –ª–∏—Ü–æ   \n1            —É—Ö–º—ã–ª—è—é—â–µ–µ—Å—è –ª–∏—Ü–æ —Å –±–æ–ª—å—à–∏–º–∏ –≥–ª–∞–∑–∞–º–∏   \n2         —É–ª—ã–±–∞—é—â–µ–µ—Å—è –ª–∏—Ü–æ —Å —É–ª—ã–±–∞—é—â–∏–º–∏—Å—è –≥–ª–∞–∑–∞–º–∏   \n3             —Å–∏—è—é—â–µ–µ –ª–∏—Ü–æ —Å —É–ª—ã–±–∞—é—â–∏–º–∏—Å—è –≥–ª–∞–∑–∞–º–∏   \n4                     —É—Ö–º—ã–ª—è—é—â–µ–µ—Å—è —â—É—Ä—è—â–µ–µ—Å—è –ª–∏—Ü–æ   \n\n                                                       name  \\\nimage_id                                                      \n0                                             grinning face   \n1                              smiling face with open mouth   \n2             smiling face with open mouth and smiling eyes   \n3                           grinning face with smiling eyes   \n4         smiling face with open mouth and tightly close...   \n\n                      group     sub_group emoji  train  text_length  \\\nimage_id                                                              \n0         Smileys & Emotion  face-smiling     üòÄ      1           17   \n1         Smileys & Emotion  face-smiling     üòÉ      1           36   \n2         Smileys & Emotion  face-smiling     üòÑ      1           39   \n3         Smileys & Emotion  face-smiling     üòÅ      1           35   \n4         Smileys & Emotion  face-smiling     üòÜ      1           27   \n\n          word_count  \nimage_id              \n0                  2  \n1                  5  \n2                  5  \n3                  5  \n4                  3  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>name</th>\n      <th>group</th>\n      <th>sub_group</th>\n      <th>emoji</th>\n      <th>train</th>\n      <th>text_length</th>\n      <th>word_count</th>\n    </tr>\n    <tr>\n      <th>image_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>—É—Ö–º—ã–ª—è—é—â–µ–µ—Å—è –ª–∏—Ü–æ</td>\n      <td>grinning face</td>\n      <td>Smileys &amp; Emotion</td>\n      <td>face-smiling</td>\n      <td>üòÄ</td>\n      <td>1</td>\n      <td>17</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>—É—Ö–º—ã–ª—è—é—â–µ–µ—Å—è –ª–∏—Ü–æ —Å –±–æ–ª—å—à–∏–º–∏ –≥–ª–∞–∑–∞–º–∏</td>\n      <td>smiling face with open mouth</td>\n      <td>Smileys &amp; Emotion</td>\n      <td>face-smiling</td>\n      <td>üòÉ</td>\n      <td>1</td>\n      <td>36</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>—É–ª—ã–±–∞—é—â–µ–µ—Å—è –ª–∏—Ü–æ —Å —É–ª—ã–±–∞—é—â–∏–º–∏—Å—è –≥–ª–∞–∑–∞–º–∏</td>\n      <td>smiling face with open mouth and smiling eyes</td>\n      <td>Smileys &amp; Emotion</td>\n      <td>face-smiling</td>\n      <td>üòÑ</td>\n      <td>1</td>\n      <td>39</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>—Å–∏—è—é—â–µ–µ –ª–∏—Ü–æ —Å —É–ª—ã–±–∞—é—â–∏–º–∏—Å—è –≥–ª–∞–∑–∞–º–∏</td>\n      <td>grinning face with smiling eyes</td>\n      <td>Smileys &amp; Emotion</td>\n      <td>face-smiling</td>\n      <td>üòÅ</td>\n      <td>1</td>\n      <td>35</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>—É—Ö–º—ã–ª—è—é—â–µ–µ—Å—è —â—É—Ä—è—â–µ–µ—Å—è –ª–∏—Ü–æ</td>\n      <td>smiling face with open mouth and tightly close...</td>\n      <td>Smileys &amp; Emotion</td>\n      <td>face-smiling</td>\n      <td>üòÜ</td>\n      <td>1</td>\n      <td>27</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/marking.csv', index_col='image_id')\n",
    "df['text'] = df['text'].str.lower()\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:01.541900934Z",
     "start_time": "2024-02-02T04:02:01.413190721Z"
    }
   },
   "id": "5513d4efd58bc472",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:01.591186318Z",
     "start_time": "2024-02-02T04:02:01.427105059Z"
    }
   },
   "id": "4ad7eb31dbc1f9d5",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Unet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2511570ef819c64b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(n_embd, 4 * n_embd)\n",
    "        self.linear_2 = nn.Linear(4 * n_embd, 4 * n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (1, 320)\n",
    "\n",
    "        # (1, 320) -> (1, 1280)\n",
    "        x = self.linear_1(x)\n",
    "\n",
    "        # (1, 1280) -> (1, 1280)\n",
    "        x = F.silu(x)\n",
    "\n",
    "        # (1, 1280) -> (1, 1280)\n",
    "        x = self.linear_2(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:01.737583975Z",
     "start_time": "2024-02-02T04:02:01.433357390Z"
    }
   },
   "id": "b8548e532aa63cdb",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class UNET_ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_time=1280):\n",
    "        super().__init__()\n",
    "        self.groupnorm_feature = nn.GroupNorm(32, in_channels)\n",
    "        self.conv_feature = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.linear_time = nn.Linear(n_time, out_channels)\n",
    "\n",
    "        self.groupnorm_merged = nn.GroupNorm(32, out_channels)\n",
    "        self.conv_merged = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        if in_channels == out_channels:\n",
    "            self.residual_layer = nn.Identity()\n",
    "        else:\n",
    "            self.residual_layer = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, feature, time):\n",
    "        # feature: (Batch_Size, In_Channels, Height, Width)\n",
    "        # time: (1, 1280)\n",
    "\n",
    "        residue = feature\n",
    "\n",
    "        # (Batch_Size, In_Channels, Height, Width) -> (Batch_Size, In_Channels, Height, Width)\n",
    "        feature = self.groupnorm_feature(feature)\n",
    "\n",
    "        # (Batch_Size, In_Channels, Height, Width) -> (Batch_Size, In_Channels, Height, Width)\n",
    "        feature = F.silu(feature)\n",
    "\n",
    "        # (Batch_Size, In_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n",
    "        feature = self.conv_feature(feature)\n",
    "\n",
    "        # (1, 1280) -> (1, 1280)\n",
    "        time = F.silu(time)\n",
    "\n",
    "        # (1, 1280) -> (1, Out_Channels)\n",
    "        time = self.linear_time(time)\n",
    "\n",
    "        # Add width and height dimension to time. \n",
    "        # (Batch_Size, Out_Channels, Height, Width) + (1, Out_Channels, 1, 1) -> (Batch_Size, Out_Channels, Height, Width)\n",
    "        merged = feature + time.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        # (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n",
    "        merged = self.groupnorm_merged(merged)\n",
    "\n",
    "        # (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n",
    "        merged = F.silu(merged)\n",
    "\n",
    "        # (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n",
    "        merged = self.conv_merged(merged)\n",
    "\n",
    "        # (Batch_Size, Out_Channels, Height, Width) + (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n",
    "        return merged + self.residual_layer(residue)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:01.816256962Z",
     "start_time": "2024-02-02T04:02:01.446799112Z"
    }
   },
   "id": "4c6d27a958eab59e",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_embed, d_cross, in_proj_bias=True, out_proj_bias=True):\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(d_embed, d_embed, bias=in_proj_bias)\n",
    "        self.k_proj = nn.Linear(d_cross, d_embed, bias=in_proj_bias)\n",
    "        self.v_proj = nn.Linear(d_cross, d_embed, bias=in_proj_bias)\n",
    "        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_embed // n_heads\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # x (latent): # (Batch_Size, Seq_Len_Q, Dim_Q)\n",
    "        # y (context): # (Batch_Size, Seq_Len_KV, Dim_KV) = (Batch_Size, 77, 768)\n",
    "\n",
    "        input_shape = x.shape\n",
    "        batch_size, sequence_length, d_embed = input_shape\n",
    "        # Divide each embedding of Q into multiple heads such that d_heads * n_heads = Dim_Q\n",
    "        interim_shape = (batch_size, -1, self.n_heads, self.d_head)\n",
    "\n",
    "        # (Batch_Size, Seq_Len_Q, Dim_Q) -> (Batch_Size, Seq_Len_Q, Dim_Q)\n",
    "        q = self.q_proj(x)\n",
    "        # (Batch_Size, Seq_Len_KV, Dim_KV) -> (Batch_Size, Seq_Len_KV, Dim_Q)\n",
    "        k = self.k_proj(y)\n",
    "        # (Batch_Size, Seq_Len_KV, Dim_KV) -> (Batch_Size, Seq_Len_KV, Dim_Q)\n",
    "        v = self.v_proj(y)\n",
    "\n",
    "        # (Batch_Size, Seq_Len_Q, Dim_Q) -> (Batch_Size, Seq_Len_Q, H, Dim_Q / H) -> (Batch_Size, H, Seq_Len_Q, Dim_Q / H)\n",
    "        q = q.view(interim_shape).transpose(1, 2)\n",
    "        # (Batch_Size, Seq_Len_KV, Dim_Q) -> (Batch_Size, Seq_Len_KV, H, Dim_Q / H) -> (Batch_Size, H, Seq_Len_KV, Dim_Q / H)\n",
    "        k = k.view(interim_shape).transpose(1, 2)\n",
    "        # (Batch_Size, Seq_Len_KV, Dim_Q) -> (Batch_Size, Seq_Len_KV, H, Dim_Q / H) -> (Batch_Size, H, Seq_Len_KV, Dim_Q / H)\n",
    "        v = v.view(interim_shape).transpose(1, 2)\n",
    "\n",
    "        # (Batch_Size, H, Seq_Len_Q, Dim_Q / H) @ (Batch_Size, H, Dim_Q / H, Seq_Len_KV) -> (Batch_Size, H, Seq_Len_Q, Seq_Len_KV)\n",
    "        weight = q @ k.transpose(-1, -2)\n",
    "\n",
    "        # (Batch_Size, H, Seq_Len_Q, Seq_Len_KV)\n",
    "        weight /= np.sqrt(self.d_head)\n",
    "\n",
    "        # (Batch_Size, H, Seq_Len_Q, Seq_Len_KV)\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "\n",
    "        # (Batch_Size, H, Seq_Len_Q, Seq_Len_KV) @ (Batch_Size, H, Seq_Len_KV, Dim_Q / H) -> (Batch_Size, H, Seq_Len_Q, Dim_Q / H)\n",
    "        output = weight @ v\n",
    "\n",
    "        # (Batch_Size, H, Seq_Len_Q, Dim_Q / H) -> (Batch_Size, Seq_Len_Q, H, Dim_Q / H)\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "\n",
    "        # (Batch_Size, Seq_Len_Q, H, Dim_Q / H) -> (Batch_Size, Seq_Len_Q, Dim_Q)\n",
    "        output = output.view(input_shape)\n",
    "\n",
    "        # (Batch_Size, Seq_Len_Q, Dim_Q) -> (Batch_Size, Seq_Len_Q, Dim_Q)\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        # (Batch_Size, Seq_Len_Q, Dim_Q)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:01.840138070Z",
     "start_time": "2024-02-02T04:02:01.493274432Z"
    }
   },
   "id": "2b58a181bbdacf03",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class UNET_AttentionBlock(nn.Module):\n",
    "    def __init__(self, n_head: int, n_embd: int, d_context=768):\n",
    "        super().__init__()\n",
    "        channels = n_head * n_embd\n",
    "\n",
    "        self.groupnorm = nn.GroupNorm(32, channels, eps=1e-6)\n",
    "        self.conv_input = nn.Conv2d(channels, channels, kernel_size=1, padding=0)\n",
    "\n",
    "        self.layernorm_1 = nn.LayerNorm(channels)\n",
    "        self.attention_1 = SelfAttention(n_head, channels, in_proj_bias=False)\n",
    "        self.layernorm_2 = nn.LayerNorm(channels)\n",
    "        self.attention_2 = CrossAttention(n_head, channels, d_context, in_proj_bias=False)\n",
    "        self.layernorm_3 = nn.LayerNorm(channels)\n",
    "        self.linear_geglu_1 = nn.Linear(channels, 4 * channels * 2)\n",
    "        self.linear_geglu_2 = nn.Linear(4 * channels, channels)\n",
    "\n",
    "        self.conv_output = nn.Conv2d(channels, channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        # x: (Batch_Size, Features, Height, Width)\n",
    "        # context: (Batch_Size, Seq_Len, Dim)\n",
    "\n",
    "        residue_long = x\n",
    "\n",
    "        # (Batch_Size, Features, Height, Width) -> (Batch_Size, Features, Height, Width)\n",
    "        x = self.groupnorm(x)\n",
    "\n",
    "        # (Batch_Size, Features, Height, Width) -> (Batch_Size, Features, Height, Width)\n",
    "        x = self.conv_input(x)\n",
    "\n",
    "        n, c, h, w = x.shape\n",
    "\n",
    "        # (Batch_Size, Features, Height, Width) -> (Batch_Size, Features, Height * Width)\n",
    "        x = x.view((n, c, h * w))\n",
    "\n",
    "        # (Batch_Size, Features, Height * Width) -> (Batch_Size, Height * Width, Features)\n",
    "        x = x.transpose(-1, -2)\n",
    "\n",
    "        # Normalization + Self-Attention with skip connection\n",
    "\n",
    "        # (Batch_Size, Height * Width, Features)\n",
    "        residue_short = x\n",
    "\n",
    "        # (Batch_Size, Height * Width, Features) -> (Batch_Size, Height * Width, Features)\n",
    "        x = self.layernorm_1(x)\n",
    "\n",
    "        # (Batch_Size, Height * Width, Features) -> (Batch_Size, Height * Width, Features)\n",
    "        x = self.attention_1(x)\n",
    "\n",
    "        # (Batch_Size, Height * Width, Features) + (Batch_Size, Height * Width, Features) -> (Batch_Size, Height * Width, Features)\n",
    "        x += residue_short\n",
    "\n",
    "        # (Batch_Size, Height * Width, Features)\n",
    "        residue_short = x\n",
    "\n",
    "        # Normalization + Cross-Attention with skip connection\n",
    "\n",
    "        # (Batch_Size, Height * Width, Features) -> (Batch_Size, Height * Width, Features)\n",
    "        x = self.layernorm_2(x)\n",
    "\n",
    "        # (Batch_Size, Height * Width, Features) -> (Batch_Size, Height * Width, Features)\n",
    "        x = self.attention_2(x, context)\n",
    "\n",
    "        # (Batch_Size, Height * Width, Features) + (Batch_Size, Height * Width, Features) -> (Batch_Size, Height * Width, Features)\n",
    "        x += residue_short\n",
    "\n",
    "        # (Batch_Size, Height * Width, Features)\n",
    "        residue_short = x\n",
    "\n",
    "        # Normalization + FFN with GeGLU and skip connection\n",
    "\n",
    "        # (Batch_Size, Height * Width, Features) -> (Batch_Size, Height * Width, Features)\n",
    "        x = self.layernorm_3(x)\n",
    "\n",
    "        # GeGLU as implemented in the original code: https://github.com/CompVis/stable-diffusion/blob/21f890f9da3cfbeaba8e2ac3c425ee9e998d5229/ldm/modules/attention.py#L37C10-L37C10\n",
    "        # (Batch_Size, Height * Width, Features) -> two tensors of shape (Batch_Size, Height * Width, Features * 4)\n",
    "        x, gate = self.linear_geglu_1(x).chunk(2, dim=-1)\n",
    "\n",
    "        # Element-wise product: (Batch_Size, Height * Width, Features * 4) * (Batch_Size, Height * Width, Features * 4) -> (Batch_Size, Height * Width, Features * 4)\n",
    "        x = x * F.gelu(gate)\n",
    "\n",
    "        # (Batch_Size, Height * Width, Features * 4) -> (Batch_Size, Height * Width, Features)\n",
    "        x = self.linear_geglu_2(x)\n",
    "\n",
    "        # (Batch_Size, Height * Width, Features) + (Batch_Size, Height * Width, Features) -> (Batch_Size, Height * Width, Features)\n",
    "        x += residue_short\n",
    "\n",
    "        # (Batch_Size, Height * Width, Features) -> (Batch_Size, Features, Height * Width)\n",
    "        x = x.transpose(-1, -2)\n",
    "\n",
    "        # (Batch_Size, Features, Height * Width) -> (Batch_Size, Features, Height, Width)\n",
    "        x = x.view((n, c, h, w))\n",
    "\n",
    "        # Final skip connection between initial input and output of the block\n",
    "        # (Batch_Size, Features, Height, Width) + (Batch_Size, Features, Height, Width) -> (Batch_Size, Features, Height, Width)\n",
    "        return self.conv_output(x) + residue_long"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:01.848188395Z",
     "start_time": "2024-02-02T04:02:01.493903702Z"
    }
   },
   "id": "b6f6578b42ae2fa4",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Upsample(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (Batch_Size, Features, Height, Width) -> (Batch_Size, Features, Height * 2, Width * 2)\n",
    "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        return self.conv(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:01.871272963Z",
     "start_time": "2024-02-02T04:02:01.494424160Z"
    }
   },
   "id": "50dc498427cd5fc2",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class SwitchSequential(nn.Sequential):\n",
    "    def forward(self, x, context, time):\n",
    "        for layer in self:\n",
    "            if isinstance(layer, UNET_AttentionBlock):\n",
    "                x = layer(x, context)\n",
    "            elif isinstance(layer, UNET_ResidualBlock):\n",
    "                x = layer(x, time)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:01.885964984Z",
     "start_time": "2024-02-02T04:02:01.536871105Z"
    }
   },
   "id": "4f23bd2776a2ee77",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoders = nn.ModuleList([\n",
    "            # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)\n",
    "            SwitchSequential(nn.Conv2d(4, 320, kernel_size=3, padding=1)),\n",
    "\n",
    "            # (Batch_Size, 320, Height / 8, Width / 8) -> # (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)\n",
    "            SwitchSequential(UNET_ResidualBlock(320, 320), UNET_AttentionBlock(8, 40)),\n",
    "\n",
    "            # (Batch_Size, 320, Height / 8, Width / 8) -> # (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)\n",
    "            SwitchSequential(UNET_ResidualBlock(320, 320), UNET_AttentionBlock(8, 40)),\n",
    "\n",
    "            # (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 16, Width / 16)\n",
    "            SwitchSequential(nn.Conv2d(320, 320, kernel_size=3, stride=2, padding=1)),\n",
    "\n",
    "            # (Batch_Size, 320, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16)\n",
    "            SwitchSequential(UNET_ResidualBlock(320, 640), UNET_AttentionBlock(8, 80)),\n",
    "\n",
    "            # (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16)\n",
    "            SwitchSequential(UNET_ResidualBlock(640, 640), UNET_AttentionBlock(8, 80)),\n",
    "\n",
    "            # (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 32, Width / 32)\n",
    "            SwitchSequential(nn.Conv2d(640, 640, kernel_size=3, stride=2, padding=1)),\n",
    "\n",
    "            # (Batch_Size, 640, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32)\n",
    "            SwitchSequential(UNET_ResidualBlock(640, 1280), UNET_AttentionBlock(8, 160)),\n",
    "\n",
    "            # (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32)\n",
    "            SwitchSequential(UNET_ResidualBlock(1280, 1280), UNET_AttentionBlock(8, 160)),\n",
    "\n",
    "            # (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 64, Width / 64)\n",
    "            SwitchSequential(nn.Conv2d(1280, 1280, kernel_size=3, stride=2, padding=1)),\n",
    "\n",
    "            # (Batch_Size, 1280, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64)\n",
    "            SwitchSequential(UNET_ResidualBlock(1280, 1280)),\n",
    "\n",
    "            # (Batch_Size, 1280, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64)\n",
    "            SwitchSequential(UNET_ResidualBlock(1280, 1280)),\n",
    "        ])\n",
    "\n",
    "        self.bottleneck = SwitchSequential(\n",
    "            # (Batch_Size, 1280, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64)\n",
    "            UNET_ResidualBlock(1280, 1280),\n",
    "\n",
    "            # (Batch_Size, 1280, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64)\n",
    "            UNET_AttentionBlock(8, 160),\n",
    "\n",
    "            # (Batch_Size, 1280, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64)\n",
    "            UNET_ResidualBlock(1280, 1280),\n",
    "        )\n",
    "\n",
    "        self.decoders = nn.ModuleList([\n",
    "            # (Batch_Size, 2560, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64)\n",
    "            SwitchSequential(UNET_ResidualBlock(2560, 1280)),\n",
    "\n",
    "            # (Batch_Size, 2560, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64)\n",
    "            SwitchSequential(UNET_ResidualBlock(2560, 1280)),\n",
    "\n",
    "            # (Batch_Size, 2560, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 32, Width / 32) \n",
    "            SwitchSequential(UNET_ResidualBlock(2560, 1280), Upsample(1280)),\n",
    "\n",
    "            # (Batch_Size, 2560, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32)\n",
    "            SwitchSequential(UNET_ResidualBlock(2560, 1280), UNET_AttentionBlock(8, 160)),\n",
    "\n",
    "            # (Batch_Size, 2560, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32)\n",
    "            SwitchSequential(UNET_ResidualBlock(2560, 1280), UNET_AttentionBlock(8, 160)),\n",
    "\n",
    "            # (Batch_Size, 1920, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 16, Width / 16)\n",
    "            SwitchSequential(UNET_ResidualBlock(1920, 1280), UNET_AttentionBlock(8, 160), Upsample(1280)),\n",
    "\n",
    "            # (Batch_Size, 1920, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16)\n",
    "            SwitchSequential(UNET_ResidualBlock(1920, 640), UNET_AttentionBlock(8, 80)),\n",
    "\n",
    "            # (Batch_Size, 1280, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16)\n",
    "            SwitchSequential(UNET_ResidualBlock(1280, 640), UNET_AttentionBlock(8, 80)),\n",
    "\n",
    "            # (Batch_Size, 960, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 8, Width / 8)\n",
    "            SwitchSequential(UNET_ResidualBlock(960, 640), UNET_AttentionBlock(8, 80), Upsample(640)),\n",
    "\n",
    "            # (Batch_Size, 960, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)\n",
    "            SwitchSequential(UNET_ResidualBlock(960, 320), UNET_AttentionBlock(8, 40)),\n",
    "\n",
    "            # (Batch_Size, 640, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)\n",
    "            SwitchSequential(UNET_ResidualBlock(640, 320), UNET_AttentionBlock(8, 40)),\n",
    "\n",
    "            # (Batch_Size, 640, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)\n",
    "            SwitchSequential(UNET_ResidualBlock(640, 320), UNET_AttentionBlock(8, 40)),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, context, time):\n",
    "        # x: (Batch_Size, 4, Height / 8, Width / 8)\n",
    "        # context: (Batch_Size, Seq_Len, Dim) \n",
    "        # time: (1, 1280)\n",
    "\n",
    "        skip_connections = []\n",
    "        for layers in self.encoders:\n",
    "            x = layers(x, context, time)\n",
    "            skip_connections.append(x)\n",
    "\n",
    "        x = self.bottleneck(x, context, time)\n",
    "\n",
    "        for layers in self.decoders:\n",
    "            # Since we always concat with the skip connection of the encoder, the number of features increases before being sent to the decoder's layer\n",
    "            x = torch.cat((x, skip_connections.pop()), dim=1)\n",
    "            x = layers(x, context, time)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:01.892091950Z",
     "start_time": "2024-02-02T04:02:01.537522584Z"
    }
   },
   "id": "10167399254d1658",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class UNET_OutputLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.groupnorm = nn.GroupNorm(32, in_channels)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (Batch_Size, 320, Height / 8, Width / 8)\n",
    "\n",
    "        # (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)\n",
    "        x = self.groupnorm(x)\n",
    "\n",
    "        # (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 8, Width / 8)\n",
    "        x = F.silu(x)\n",
    "\n",
    "        # (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 4, Height / 8, Width / 8)\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # (Batch_Size, 4, Height / 8, Width / 8) \n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:01.932032850Z",
     "start_time": "2024-02-02T04:02:01.538173365Z"
    }
   },
   "id": "8ea21d8a41029e10",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Diffusion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.time_embedding = TimeEmbedding(320)\n",
    "        self.unet = UNET()\n",
    "        self.final = UNET_OutputLayer(320, 4)\n",
    "\n",
    "    def forward(self, latent, context, time):\n",
    "        # latent: (Batch_Size, 4, Height / 8, Width / 8)\n",
    "        # context: (Batch_Size, Seq_Len, Dim)\n",
    "        # time: (1, 320)\n",
    "\n",
    "        # (1, 320) -> (1, 1280)\n",
    "        time = self.time_embedding(time)\n",
    "\n",
    "        # (Batch, 4, Height / 8, Width / 8) -> (Batch, 320, Height / 8, Width / 8)\n",
    "        output = self.unet(latent, context, time)\n",
    "\n",
    "        # (Batch, 320, Height / 8, Width / 8) -> (Batch, 4, Height / 8, Width / 8)\n",
    "        output = self.final(output)\n",
    "\n",
    "        # (Batch, 4, Height / 8, Width / 8)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:01.961645832Z",
     "start_time": "2024-02-02T04:02:01.539108749Z"
    }
   },
   "id": "b8802a1fb7026ff5",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. VAE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "243ef28175ce04cf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Attention"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91f9ca5bb395c29b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Self Attention"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fb9e0c79038aac2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_embed, in_proj_bias=True, out_proj_bias=True):\n",
    "        super().__init__()\n",
    "        # This combines the Wq, Wk and Wv matrices into one matrix\n",
    "        self.in_proj = nn.Linear(d_embed, 3 * d_embed, bias=in_proj_bias)\n",
    "        # This one represents the Wo matrix\n",
    "        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_embed // n_heads\n",
    "\n",
    "    def forward(self, x, causal_mask=False):\n",
    "        # x: # (Batch_Size, Seq_Len, Dim)\n",
    "\n",
    "        # (Batch_Size, Seq_Len, Dim)\n",
    "        input_shape = x.shape\n",
    "\n",
    "        # (Batch_Size, Seq_Len, Dim)\n",
    "        batch_size, sequence_length, d_embed = input_shape\n",
    "\n",
    "        # (Batch_Size, Seq_Len, H, Dim / H)\n",
    "        interim_shape = (batch_size, sequence_length, self.n_heads, self.d_head)\n",
    "\n",
    "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim * 3) -> 3 tensor of shape (Batch_Size, Seq_Len, Dim)\n",
    "        q, k, v = self.in_proj(x).chunk(3, dim=-1)\n",
    "\n",
    "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, H, Dim / H) -> (Batch_Size, H, Seq_Len, Dim / H)\n",
    "        q = q.view(interim_shape).transpose(1, 2)\n",
    "        k = k.view(interim_shape).transpose(1, 2)\n",
    "        v = v.view(interim_shape).transpose(1, 2)\n",
    "\n",
    "        # (Batch_Size, H, Seq_Len, Dim) @ (Batch_Size, H, Dim, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n",
    "        weight = q @ k.transpose(-1, -2)\n",
    "\n",
    "        if causal_mask:\n",
    "            # Mask where the upper triangle (above the principal diagonal) is 1\n",
    "            mask = torch.ones_like(weight, dtype=torch.bool).triu(1)\n",
    "            # Fill the upper triangle with -inf\n",
    "            weight.masked_fill_(mask, -torch.inf)\n",
    "\n",
    "            # Divide by d_k (Dim / H). \n",
    "        # (Batch_Size, H, Seq_Len, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n",
    "        weight /= np.sqrt(self.d_head)\n",
    "\n",
    "        # (Batch_Size, H, Seq_Len, Seq_Len) -> (Batch_Size, H, Seq_Len, Seq_Len)\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "\n",
    "        # (Batch_Size, H, Seq_Len, Seq_Len) @ (Batch_Size, H, Seq_Len, Dim / H) -> (Batch_Size, H, Seq_Len, Dim / H)\n",
    "        output = weight @ v\n",
    "\n",
    "        # (Batch_Size, H, Seq_Len, Dim / H) -> (Batch_Size, Seq_Len, H, Dim / H)\n",
    "        output = output.transpose(1, 2)\n",
    "\n",
    "        # (Batch_Size, Seq_Len, H, Dim / H) -> (Batch_Size, Seq_Len, Dim)\n",
    "        output = output.reshape(input_shape)\n",
    "\n",
    "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
    "        output = self.out_proj(output)\n",
    "\n",
    "        # (Batch_Size, Seq_Len, Dim)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:01.978653370Z",
     "start_time": "2024-02-02T04:02:01.585238704Z"
    }
   },
   "id": "f0c26e8f6b43d9de",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86eafa88d4c43ecd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class VAE_AttentionBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.groupnorm = nn.GroupNorm(32, channels)\n",
    "        self.attention = SelfAttention(1, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (Batch_Size, Features, Height, Width)\n",
    "\n",
    "        residue = x\n",
    "\n",
    "        # (Batch_Size, Features, Height, Width) -> (Batch_Size, Features, Height, Width)\n",
    "        x = self.groupnorm(x)\n",
    "\n",
    "        n, c, h, w = x.shape\n",
    "\n",
    "        # (Batch_Size, Features, Height, Width) -> (Batch_Size, Features, Height * Width)\n",
    "        x = x.view((n, c, h * w))\n",
    "\n",
    "        # (Batch_Size, Features, Height * Width) -> (Batch_Size, Height * Width, Features). Each pixel becomes a feature of size \"Features\", the sequence length is \"Height * Width\".\n",
    "        x = x.transpose(-1, -2)\n",
    "\n",
    "        # Perform self-attention WITHOUT mask\n",
    "        # (Batch_Size, Height * Width, Features) -> (Batch_Size, Height * Width, Features)\n",
    "        x = self.attention(x)\n",
    "\n",
    "        # (Batch_Size, Height * Width, Features) -> (Batch_Size, Features, Height * Width)\n",
    "        x = x.transpose(-1, -2)\n",
    "\n",
    "        # (Batch_Size, Features, Height * Width) -> (Batch_Size, Features, Height, Width)\n",
    "        x = x.view((n, c, h, w))\n",
    "\n",
    "        # (Batch_Size, Features, Height, Width) + (Batch_Size, Features, Height, Width) -> (Batch_Size, Features, Height, Width) \n",
    "        x += residue\n",
    "\n",
    "        # (Batch_Size, Features, Height, Width)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:01.986545993Z",
     "start_time": "2024-02-02T04:02:01.585912393Z"
    }
   },
   "id": "a965805c85244290",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class VAE_ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.groupnorm_1 = nn.GroupNorm(32, in_channels)\n",
    "        self.conv_1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        self.groupnorm_2 = nn.GroupNorm(32, out_channels)\n",
    "        self.conv_2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        if in_channels == out_channels:\n",
    "            self.residual_layer = nn.Identity()\n",
    "        else:\n",
    "            self.residual_layer = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (Batch_Size, In_Channels, Height, Width)\n",
    "\n",
    "        residue = x\n",
    "\n",
    "        # (Batch_Size, In_Channels, Height, Width) -> (Batch_Size, In_Channels, Height, Width)\n",
    "        x = self.groupnorm_1(x)\n",
    "\n",
    "        # (Batch_Size, In_Channels, Height, Width) -> (Batch_Size, In_Channels, Height, Width)\n",
    "        x = F.silu(x)\n",
    "\n",
    "        # (Batch_Size, In_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n",
    "        x = self.conv_1(x)\n",
    "\n",
    "        # (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n",
    "        x = self.groupnorm_2(x)\n",
    "\n",
    "        # (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n",
    "        x = F.silu(x)\n",
    "\n",
    "        # (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n",
    "        x = self.conv_2(x)\n",
    "\n",
    "        # (Batch_Size, Out_Channels, Height, Width) -> (Batch_Size, Out_Channels, Height, Width)\n",
    "        return x + self.residual_layer(residue)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:01.992733931Z",
     "start_time": "2024-02-02T04:02:01.586493403Z"
    }
   },
   "id": "363ea8539b9f2109",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class VAE_Decoder(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 4, Height / 8, Width / 8)\n",
    "            nn.Conv2d(4, 4, kernel_size=1, padding=0),\n",
    "\n",
    "            # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
    "            nn.Conv2d(4, 512, kernel_size=3, padding=1),\n",
    "\n",
    "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
    "            VAE_AttentionBlock(512),\n",
    "\n",
    "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            # Repeats the rows and columns of the data by scale_factor (like when you resize an image by doubling its size).\n",
    "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 4, Width / 4)\n",
    "            nn.Upsample(scale_factor=2),\n",
    "\n",
    "            # (Batch_Size, 512, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 4, Width / 4)\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "\n",
    "            # (Batch_Size, 512, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 4, Width / 4)\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            # (Batch_Size, 512, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 4, Width / 4)\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            # (Batch_Size, 512, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 4, Width / 4)\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            # (Batch_Size, 512, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 2, Width / 2)\n",
    "            nn.Upsample(scale_factor=2),\n",
    "\n",
    "            # (Batch_Size, 512, Height / 2, Width / 2) -> (Batch_Size, 512, Height / 2, Width / 2)\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "\n",
    "            # (Batch_Size, 512, Height / 2, Width / 2) -> (Batch_Size, 256, Height / 2, Width / 2)\n",
    "            VAE_ResidualBlock(512, 256),\n",
    "\n",
    "            # (Batch_Size, 256, Height / 2, Width / 2) -> (Batch_Size, 256, Height / 2, Width / 2)\n",
    "            VAE_ResidualBlock(256, 256),\n",
    "\n",
    "            # (Batch_Size, 256, Height / 2, Width / 2) -> (Batch_Size, 256, Height / 2, Width / 2)\n",
    "            VAE_ResidualBlock(256, 256),\n",
    "\n",
    "            # (Batch_Size, 256, Height / 2, Width / 2) -> (Batch_Size, 256, Height, Width)\n",
    "            nn.Upsample(scale_factor=2),\n",
    "\n",
    "            # (Batch_Size, 256, Height, Width) -> (Batch_Size, 256, Height, Width)\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "\n",
    "            # (Batch_Size, 256, Height, Width) -> (Batch_Size, 128, Height, Width)\n",
    "            VAE_ResidualBlock(256, 128),\n",
    "\n",
    "            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width)\n",
    "            VAE_ResidualBlock(128, 128),\n",
    "\n",
    "            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width)\n",
    "            VAE_ResidualBlock(128, 128),\n",
    "\n",
    "            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width)\n",
    "            nn.GroupNorm(32, 128),\n",
    "\n",
    "            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width)\n",
    "            nn.SiLU(),\n",
    "\n",
    "            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 3, Height, Width)\n",
    "            nn.Conv2d(128, 3, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (Batch_Size, 4, Height / 8, Width / 8)\n",
    "\n",
    "        # Remove the scaling added by the Encoder.\n",
    "        x /= 0.18215\n",
    "\n",
    "        for module in self:\n",
    "            x = module(x)\n",
    "\n",
    "        # (Batch_Size, 3, Height, Width)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:02.009960631Z",
     "start_time": "2024-02-02T04:02:01.587245733Z"
    }
   },
   "id": "1d316c198cebd622",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Encoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "103486a8a0acb754"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class VAE_Encoder(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            # (Batch_Size, Channel, Height, Width) -> (Batch_Size, 128, Height, Width)\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
    "\n",
    "            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width)\n",
    "            VAE_ResidualBlock(128, 128),\n",
    "\n",
    "            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height, Width)\n",
    "            VAE_ResidualBlock(128, 128),\n",
    "\n",
    "            # (Batch_Size, 128, Height, Width) -> (Batch_Size, 128, Height / 2, Width / 2)\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=0),\n",
    "\n",
    "            # (Batch_Size, 128, Height / 2, Width / 2) -> (Batch_Size, 256, Height / 2, Width / 2)\n",
    "            VAE_ResidualBlock(128, 256),\n",
    "\n",
    "            # (Batch_Size, 256, Height / 2, Width / 2) -> (Batch_Size, 256, Height / 2, Width / 2)\n",
    "            VAE_ResidualBlock(256, 256),\n",
    "\n",
    "            # (Batch_Size, 256, Height / 2, Width / 2) -> (Batch_Size, 256, Height / 4, Width / 4)\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=0),\n",
    "\n",
    "            # (Batch_Size, 256, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 4, Width / 4)\n",
    "            VAE_ResidualBlock(256, 512),\n",
    "\n",
    "            # (Batch_Size, 512, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 4, Width / 4)\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            # (Batch_Size, 512, Height / 4, Width / 4) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=0),\n",
    "\n",
    "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
    "            VAE_AttentionBlock(512),\n",
    "\n",
    "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
    "            VAE_ResidualBlock(512, 512),\n",
    "\n",
    "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
    "            nn.GroupNorm(32, 512),\n",
    "\n",
    "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 512, Height / 8, Width / 8)\n",
    "            nn.SiLU(),\n",
    "\n",
    "            # Because the padding=1, it means the width and height will increase by 2\n",
    "            # Out_Height = In_Height + Padding_Top + Padding_Bottom\n",
    "            # Out_Width = In_Width + Padding_Left + Padding_Right\n",
    "            # Since padding = 1 means Padding_Top = Padding_Bottom = Padding_Left = Padding_Right = 1,\n",
    "            # Since the Out_Width = In_Width + 2 (same for Out_Height), it will compensate for the Kernel size of 3\n",
    "            # (Batch_Size, 512, Height / 8, Width / 8) -> (Batch_Size, 8, Height / 8, Width / 8). \n",
    "            nn.Conv2d(512, 8, kernel_size=3, padding=1),\n",
    "\n",
    "            # (Batch_Size, 8, Height / 8, Width / 8) -> (Batch_Size, 8, Height / 8, Width / 8)\n",
    "            nn.Conv2d(8, 8, kernel_size=1, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, noise):\n",
    "        # x: (Batch_Size, Channel, Height, Width)\n",
    "        # noise: (Batch_Size, 4, Height / 8, Width / 8)\n",
    "\n",
    "        for module in self:\n",
    "\n",
    "            if getattr(module, 'stride', None) == (2, 2):  # Padding at downsampling should be asymmetric (see #8)\n",
    "                # Pad: (Padding_Left, Padding_Right, Padding_Top, Padding_Bottom).\n",
    "                # Pad with zeros on the right and bottom.\n",
    "                # (Batch_Size, Channel, Height, Width) -> (Batch_Size, Channel, Height + Padding_Top + Padding_Bottom, Width + Padding_Left + Padding_Right) = (Batch_Size, Channel, Height + 1, Width + 1)\n",
    "                x = F.pad(x, (0, 1, 0, 1))\n",
    "\n",
    "            x = module(x)\n",
    "        # (Batch_Size, 8, Height / 8, Width / 8) -> two tensors of shape (Batch_Size, 4, Height / 8, Width / 8)\n",
    "        mean, log_variance = torch.chunk(x, 2, dim=1)\n",
    "        # Clamp the log variance between -30 and 20, so that the variance is between (circa) 1e-14 and 1e8. \n",
    "        # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 4, Height / 8, Width / 8)\n",
    "        log_variance = torch.clamp(log_variance, -30, 20)\n",
    "        # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 4, Height / 8, Width / 8)\n",
    "        variance = log_variance.exp()\n",
    "        # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 4, Height / 8, Width / 8)\n",
    "        stdev = variance.sqrt()\n",
    "\n",
    "        # Transform N(0, 1) -> N(mean, stdev) \n",
    "        # (Batch_Size, 4, Height / 8, Width / 8) -> (Batch_Size, 4, Height / 8, Width / 8)\n",
    "        x = mean + stdev * noise\n",
    "\n",
    "        # Scale by a constant\n",
    "        # Constant taken from: https://github.com/CompVis/stable-diffusion/blob/21f890f9da3cfbeaba8e2ac3c425ee9e998d5229/configs/stable-diffusion/v1-inference.yaml#L17C1-L17C1\n",
    "        x *= 0.18215\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:02.205301939Z",
     "start_time": "2024-02-02T04:02:01.604534104Z"
    }
   },
   "id": "eeca9ca1e88d4a3f",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Pre-trained CLIP encoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de466466f4d882ab"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CLIPEmbedding(nn.Module):\n",
    "    def __init__(self, n_vocab: int, n_embd: int, n_token: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(n_vocab, n_embd)\n",
    "        # A learnable weight matrix encodes the position information for each token\n",
    "        self.position_embedding = nn.Parameter(torch.zeros((n_token, n_embd)))\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # (Batch_Size, Seq_Len) -> (Batch_Size, Seq_Len, Dim) \n",
    "        x = self.token_embedding(tokens)\n",
    "        # (Batch_Size, Seq_Len) -> (Batch_Size, Seq_Len, Dim)\n",
    "        x += self.position_embedding\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:02.210322060Z",
     "start_time": "2024-02-02T04:02:01.649189430Z"
    }
   },
   "id": "c545b000b532ae53",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CLIPLayer(nn.Module):\n",
    "    def __init__(self, n_head: int, n_embd: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Pre-attention norm\n",
    "        self.layernorm_1 = nn.LayerNorm(n_embd)\n",
    "        # Self attention\n",
    "        self.attention = SelfAttention(n_head, n_embd)\n",
    "        # Pre-FNN norm\n",
    "        self.layernorm_2 = nn.LayerNorm(n_embd)\n",
    "        # Feedforward layer\n",
    "        self.linear_1 = nn.Linear(n_embd, 4 * n_embd)\n",
    "        self.linear_2 = nn.Linear(4 * n_embd, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (Batch_Size, Seq_Len, Dim)\n",
    "        residue = x\n",
    "\n",
    "        ### SELF ATTENTION ###\n",
    "\n",
    "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
    "        x = self.layernorm_1(x)\n",
    "\n",
    "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
    "        x = self.attention(x, causal_mask=True)\n",
    "\n",
    "        # (Batch_Size, Seq_Len, Dim) + (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
    "        x += residue\n",
    "\n",
    "        ### FEEDFORWARD LAYER ###\n",
    "        # Apply a feedforward layer where the hidden dimension is 4 times the embedding dimension. \n",
    "\n",
    "        residue = x\n",
    "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
    "        x = self.layernorm_2(x)\n",
    "\n",
    "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, 4 * Dim)\n",
    "        x = self.linear_1(x)\n",
    "\n",
    "        # (Batch_Size, Seq_Len, 4 * Dim) -> (Batch_Size, Seq_Len, 4 * Dim)\n",
    "        x = x * torch.sigmoid(1.702 * x)  # QuickGELU activation function\n",
    "\n",
    "        # (Batch_Size, Seq_Len, 4 * Dim) -> (Batch_Size, Seq_Len, Dim)\n",
    "        x = self.linear_2(x)\n",
    "\n",
    "        # (Batch_Size, Seq_Len, Dim) + (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
    "        x += residue\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:02.272328823Z",
     "start_time": "2024-02-02T04:02:01.649730909Z"
    }
   },
   "id": "dd45ffea44480152",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = CLIPEmbedding(49408, 768, 77)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            CLIPLayer(12, 768) for i in range(12)\n",
    "        ])\n",
    "\n",
    "        self.layernorm = nn.LayerNorm(768)\n",
    "\n",
    "    def forward(self, tokens: torch.LongTensor) -> torch.FloatTensor:\n",
    "        tokens = tokens.type(torch.long)\n",
    "\n",
    "        # (Batch_Size, Seq_Len) -> (Batch_Size, Seq_Len, Dim)\n",
    "        state = self.embedding(tokens)\n",
    "\n",
    "        # Apply encoder layers similar to the Transformer's encoder.\n",
    "        for layer in self.layers:\n",
    "            # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
    "            state = layer(state)\n",
    "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, Seq_Len, Dim)\n",
    "        output = self.layernorm(state)\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:02.298767519Z",
     "start_time": "2024-02-02T04:02:01.650208763Z"
    }
   },
   "id": "1e88c7ac8a6c6214",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Noise Scheduler"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9fde47fd0e56e76"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DDPMSampler:\n",
    "\n",
    "    def __init__(self, generator: torch.Generator, num_training_steps=1000, beta_start: float = 0.00085,\n",
    "                 beta_end: float = 0.0120):\n",
    "        # Params \"beta_start\" and \"beta_end\" taken from: https://github.com/CompVis/stable-diffusion/blob/21f890f9da3cfbeaba8e2ac3c425ee9e998d5229/configs/stable-diffusion/v1-inference.yaml#L5C8-L5C8\n",
    "        # For the naming conventions, refer to the DDPM paper (https://arxiv.org/pdf/2006.11239.pdf)\n",
    "        self.betas = torch.linspace(beta_start ** 0.5, beta_end ** 0.5, num_training_steps, dtype=torch.float32) ** 2\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.one = torch.tensor(1.0)\n",
    "\n",
    "        self.generator = generator\n",
    "\n",
    "        self.num_train_timesteps = num_training_steps\n",
    "        self.timesteps = torch.from_numpy(np.arange(0, num_training_steps)[::-1].copy())\n",
    "\n",
    "    def set_inference_timesteps(self, num_inference_steps=50):\n",
    "        self.num_inference_steps = num_inference_steps\n",
    "        step_ratio = self.num_train_timesteps // self.num_inference_steps\n",
    "        timesteps = (np.arange(0, num_inference_steps) * step_ratio).round()[::-1].copy().astype(np.int64)\n",
    "        self.timesteps = torch.from_numpy(timesteps)\n",
    "\n",
    "    def _get_previous_timestep(self, timestep: int) -> int:\n",
    "        prev_t = timestep - self.num_train_timesteps // self.num_inference_steps\n",
    "        return prev_t\n",
    "\n",
    "    def _get_variance(self, timestep: int) -> torch.Tensor:\n",
    "        prev_t = self._get_previous_timestep(timestep)\n",
    "\n",
    "        alpha_prod_t = self.alphas_cumprod[timestep]\n",
    "        alpha_prod_t_prev = self.alphas_cumprod[prev_t] if prev_t >= 0 else self.one\n",
    "        current_beta_t = 1 - alpha_prod_t / alpha_prod_t_prev\n",
    "\n",
    "        # For t > 0, compute predicted variance Œ≤t (see formula (6) and (7) from https://arxiv.org/pdf/2006.11239.pdf)\n",
    "        # and sample from it to get previous sample\n",
    "        # x_{t-1} ~ N(pred_prev_sample, variance) == add variance to pred_sample\n",
    "        variance = (1 - alpha_prod_t_prev) / (1 - alpha_prod_t) * current_beta_t\n",
    "\n",
    "        # we always take the log of variance, so clamp it to ensure it's not 0\n",
    "        variance = torch.clamp(variance, min=1e-20)\n",
    "\n",
    "        return variance\n",
    "\n",
    "    def set_strength(self, strength=1):\n",
    "        \"\"\"\n",
    "            Set how much noise to add to the input image. \n",
    "            More noise (strength ~ 1) means that the output will be further from the input image.\n",
    "            Less noise (strength ~ 0) means that the output will be closer to the input image.\n",
    "        \"\"\"\n",
    "        # start_step is the number of noise levels to skip\n",
    "        start_step = self.num_inference_steps - int(self.num_inference_steps * strength)\n",
    "        self.timesteps = self.timesteps[start_step:]\n",
    "        self.start_step = start_step\n",
    "\n",
    "    def step(self, timestep: int, latents: torch.Tensor, model_output: torch.Tensor):\n",
    "        t = timestep\n",
    "        prev_t = self._get_previous_timestep(t)\n",
    "\n",
    "        # 1. compute alphas, betas\n",
    "        alpha_prod_t = self.alphas_cumprod[t]\n",
    "        alpha_prod_t_prev = self.alphas_cumprod[prev_t] if prev_t >= 0 else self.one\n",
    "        beta_prod_t = 1 - alpha_prod_t\n",
    "        beta_prod_t_prev = 1 - alpha_prod_t_prev\n",
    "        current_alpha_t = alpha_prod_t / alpha_prod_t_prev\n",
    "        current_beta_t = 1 - current_alpha_t\n",
    "\n",
    "        # 2. compute predicted original sample from predicted noise also called\n",
    "        # \"predicted x_0\" of formula (15) from https://arxiv.org/pdf/2006.11239.pdf\n",
    "        pred_original_sample = (latents - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)\n",
    "\n",
    "        # 4. Compute coefficients for pred_original_sample x_0 and current sample x_t\n",
    "        # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n",
    "        pred_original_sample_coeff = (alpha_prod_t_prev ** (0.5) * current_beta_t) / beta_prod_t\n",
    "        current_sample_coeff = current_alpha_t ** (0.5) * beta_prod_t_prev / beta_prod_t\n",
    "\n",
    "        # 5. Compute predicted previous sample ¬µ_t\n",
    "        # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n",
    "        pred_prev_sample = pred_original_sample_coeff * pred_original_sample + current_sample_coeff * latents\n",
    "\n",
    "        # 6. Add noise\n",
    "        variance = 0\n",
    "        if t > 0:\n",
    "            device = model_output.device\n",
    "            noise = torch.randn(model_output.shape, generator=self.generator, device=device, dtype=model_output.dtype)\n",
    "            # Compute the variance as per formula (7) from https://arxiv.org/pdf/2006.11239.pdf\n",
    "            variance = (self._get_variance(t) ** 0.5) * noise\n",
    "\n",
    "        # sample from N(mu, sigma) = X can be obtained by X = mu + sigma * N(0, 1)\n",
    "        # the variable \"variance\" is already multiplied by the noise N(0, 1)\n",
    "        pred_prev_sample = pred_prev_sample + variance\n",
    "\n",
    "        return pred_prev_sample\n",
    "\n",
    "    def add_noise(\n",
    "            self,\n",
    "            original_samples: torch.FloatTensor,\n",
    "            timesteps: torch.IntTensor,\n",
    "    ) -> torch.FloatTensor:\n",
    "        alphas_cumprod = self.alphas_cumprod.to(device=original_samples.device, dtype=original_samples.dtype)\n",
    "        timesteps = timesteps.to(original_samples.device)\n",
    "\n",
    "        sqrt_alpha_prod = alphas_cumprod[timesteps] ** 0.5\n",
    "        sqrt_alpha_prod = sqrt_alpha_prod.flatten()\n",
    "        while len(sqrt_alpha_prod.shape) < len(original_samples.shape):\n",
    "            sqrt_alpha_prod = sqrt_alpha_prod.unsqueeze(-1)\n",
    "\n",
    "        sqrt_one_minus_alpha_prod = (1 - alphas_cumprod[timesteps]) ** 0.5\n",
    "        sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.flatten()\n",
    "        while len(sqrt_one_minus_alpha_prod.shape) < len(original_samples.shape):\n",
    "            sqrt_one_minus_alpha_prod = sqrt_one_minus_alpha_prod.unsqueeze(-1)\n",
    "\n",
    "        # Sample from q(x_t | x_0) as in equation (4) of https://arxiv.org/pdf/2006.11239.pdf\n",
    "        # Because N(mu, sigma) = X can be obtained by X = mu + sigma * N(0, 1)\n",
    "        # here mu = sqrt_alpha_prod * original_samples and sigma = sqrt_one_minus_alpha_prod\n",
    "        noise = torch.randn(original_samples.shape, generator=self.generator, device=original_samples.device,\n",
    "                            dtype=original_samples.dtype)\n",
    "        noisy_samples = sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise\n",
    "        return noisy_samples"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:02.317767350Z",
     "start_time": "2024-02-02T04:02:01.651156929Z"
    }
   },
   "id": "3645552458973889",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Pipeline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1511aa2ff2130e6f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "WIDTH = 512\n",
    "HEIGHT = 512\n",
    "LATENTS_WIDTH = WIDTH // 8\n",
    "LATENTS_HEIGHT = HEIGHT // 8"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:02.323663141Z",
     "start_time": "2024-02-02T04:02:01.692990667Z"
    }
   },
   "id": "7ee3f89b099eeecc",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def rescale(x, old_range, new_range, clamp=False):\n",
    "    old_min, old_max = old_range\n",
    "    new_min, new_max = new_range\n",
    "    x -= old_min\n",
    "    x *= (new_max - new_min) / (old_max - old_min)\n",
    "    x += new_min\n",
    "    if clamp:\n",
    "        x = x.clamp(new_min, new_max)\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:02.328317643Z",
     "start_time": "2024-02-02T04:02:01.693443448Z"
    }
   },
   "id": "e16456017224a283",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_time_embedding(timestep):\n",
    "    # Shape: (160,)\n",
    "    freqs = torch.pow(10000, -torch.arange(start=0, end=160, dtype=torch.float32) / 160)\n",
    "    # Shape: (1, 160)\n",
    "    x = torch.tensor([timestep], dtype=torch.float32)[:, None] * freqs[None]\n",
    "    # Shape: (1, 160 * 2)\n",
    "    return torch.cat([torch.cos(x), torch.sin(x)], dim=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:02.343557359Z",
     "start_time": "2024-02-02T04:02:01.693862496Z"
    }
   },
   "id": "77f043a3cd6eb6d2",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate(\n",
    "        prompt,\n",
    "        uncond_prompt=None,\n",
    "        input_image=None,\n",
    "        strength=0.8,\n",
    "        do_cfg=True,\n",
    "        cfg_scale=7.5,\n",
    "        sampler_name=\"ddpm\",\n",
    "        n_inference_steps=50,\n",
    "        models={},\n",
    "        seed=None,\n",
    "        device=None,\n",
    "        idle_device=None,\n",
    "        tokenizer=None,\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        if not 0 < strength <= 1:\n",
    "            raise ValueError(\"strength must be between 0 and 1\")\n",
    "\n",
    "        if idle_device:\n",
    "            to_idle = lambda x: x.to(idle_device)\n",
    "        else:\n",
    "            to_idle = lambda x: x\n",
    "\n",
    "        # Initialize random number generator according to the seed specified\n",
    "        generator = torch.Generator(device=device)\n",
    "        if seed is None:\n",
    "            generator.seed()\n",
    "        else:\n",
    "            generator.manual_seed(seed)\n",
    "\n",
    "        clip = models[\"clip\"]\n",
    "        clip.to(device)\n",
    "\n",
    "        if do_cfg:\n",
    "            # Convert into a list of length Seq_Len=77\n",
    "            cond_tokens = tokenizer.batch_encode_plus(\n",
    "                [prompt], padding=\"max_length\", max_length=77\n",
    "            ).input_ids\n",
    "            # (Batch_Size, Seq_Len)\n",
    "            cond_tokens = torch.tensor(cond_tokens, dtype=torch.long, device=device)\n",
    "            # (Batch_Size, Seq_Len) -> (Batch_Size, Seq_Len, Dim)\n",
    "            cond_context = clip(cond_tokens)\n",
    "            # Convert into a list of length Seq_Len=77\n",
    "            uncond_tokens = tokenizer.batch_encode_plus(\n",
    "                [uncond_prompt], padding=\"max_length\", max_length=77\n",
    "            ).input_ids\n",
    "            # (Batch_Size, Seq_Len)\n",
    "            uncond_tokens = torch.tensor(uncond_tokens, dtype=torch.long, device=device)\n",
    "            # (Batch_Size, Seq_Len) -> (Batch_Size, Seq_Len, Dim)\n",
    "            uncond_context = clip(uncond_tokens)\n",
    "            # (Batch_Size, Seq_Len, Dim) + (Batch_Size, Seq_Len, Dim) -> (2 * Batch_Size, Seq_Len, Dim)\n",
    "            context = torch.cat([cond_context, uncond_context])\n",
    "        else:\n",
    "            # Convert into a list of length Seq_Len=77\n",
    "            tokens = tokenizer.batch_encode_plus(\n",
    "                [prompt], padding=\"max_length\", max_length=77\n",
    "            ).input_ids\n",
    "            # (Batch_Size, Seq_Len)\n",
    "            tokens = torch.tensor(tokens, dtype=torch.long, device=device)\n",
    "            # (Batch_Size, Seq_Len) -> (Batch_Size, Seq_Len, Dim)\n",
    "            context = clip(tokens)\n",
    "        to_idle(clip)\n",
    "\n",
    "        if sampler_name == \"ddpm\":\n",
    "            sampler = DDPMSampler(generator)\n",
    "            sampler.set_inference_timesteps(n_inference_steps)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown sampler value %s. \")\n",
    "\n",
    "        latents_shape = (1, 4, LATENTS_HEIGHT, LATENTS_WIDTH)\n",
    "\n",
    "        if input_image:\n",
    "            encoder = models[\"encoder\"]\n",
    "            encoder.to(device)\n",
    "\n",
    "            input_image_tensor = input_image.resize((WIDTH, HEIGHT))\n",
    "            # (Height, Width, Channel)\n",
    "            input_image_tensor = np.array(input_image_tensor)\n",
    "            # (Height, Width, Channel) -> (Height, Width, Channel)\n",
    "            input_image_tensor = torch.tensor(input_image_tensor, dtype=torch.float32)\n",
    "            # (Height, Width, Channel) -> (Height, Width, Channel)\n",
    "            input_image_tensor = rescale(input_image_tensor, (0, 255), (-1, 1))\n",
    "            # (Height, Width, Channel) -> (Batch_Size, Height, Width, Channel)\n",
    "            input_image_tensor = input_image_tensor.unsqueeze(0)\n",
    "            # (Batch_Size, Height, Width, Channel) -> (Batch_Size, Channel, Height, Width)\n",
    "            input_image_tensor = input_image_tensor.permute(0, 3, 1, 2)\n",
    "\n",
    "            # (Batch_Size, 4, Latents_Height, Latents_Width)\n",
    "            encoder_noise = torch.randn(latents_shape, generator=generator, device=device)\n",
    "            # (Batch_Size, 4, Latents_Height, Latents_Width)\n",
    "            latents = encoder(input_image_tensor, encoder_noise)\n",
    "\n",
    "            # Add noise to the latents (the encoded input image)\n",
    "            # (Batch_Size, 4, Latents_Height, Latents_Width)\n",
    "            sampler.set_strength(strength=strength)\n",
    "            latents = sampler.add_noise(latents, sampler.timesteps[0])\n",
    "\n",
    "            to_idle(encoder)\n",
    "        else:\n",
    "            # (Batch_Size, 4, Latents_Height, Latents_Width)\n",
    "            latents = torch.randn(latents_shape, generator=generator, device=device)\n",
    "\n",
    "        diffusion = models[\"diffusion\"]\n",
    "        diffusion.to(device)\n",
    "\n",
    "        timesteps = tqdm(sampler.timesteps)\n",
    "        for i, timestep in enumerate(timesteps):\n",
    "            # (1, 320)\n",
    "            time_embedding = get_time_embedding(timestep).to(device)\n",
    "\n",
    "            # (Batch_Size, 4, Latents_Height, Latents_Width)\n",
    "            model_input = latents\n",
    "\n",
    "            if do_cfg:\n",
    "                # (Batch_Size, 4, Latents_Height, Latents_Width) -> (2 * Batch_Size, 4, Latents_Height, Latents_Width)\n",
    "                model_input = model_input.repeat(2, 1, 1, 1)\n",
    "\n",
    "            # model_output is the predicted noise\n",
    "            # (Batch_Size, 4, Latents_Height, Latents_Width) -> (Batch_Size, 4, Latents_Height, Latents_Width)\n",
    "            model_output = diffusion(model_input, context, time_embedding)\n",
    "\n",
    "            if do_cfg:\n",
    "                output_cond, output_uncond = model_output.chunk(2)\n",
    "                model_output = cfg_scale * (output_cond - output_uncond) + output_uncond\n",
    "\n",
    "            # (Batch_Size, 4, Latents_Height, Latents_Width) -> (Batch_Size, 4, Latents_Height, Latents_Width)\n",
    "            latents = sampler.step(timestep, latents, model_output)\n",
    "\n",
    "        to_idle(diffusion)\n",
    "\n",
    "        decoder = models[\"decoder\"]\n",
    "        decoder.to(device)\n",
    "        # (Batch_Size, 4, Latents_Height, Latents_Width) -> (Batch_Size, 3, Height, Width)\n",
    "        images = decoder(latents)\n",
    "        to_idle(decoder)\n",
    "\n",
    "        images = rescale(images, (-1, 1), (0, 255), clamp=True)\n",
    "        # (Batch_Size, Channel, Height, Width) -> (Batch_Size, Height, Width, Channel)\n",
    "        images = images.permute(0, 2, 3, 1)\n",
    "        images = images.to(\"cpu\", torch.uint8).numpy()\n",
    "        return images[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:02.349634808Z",
     "start_time": "2024-02-02T04:02:01.695964020Z"
    }
   },
   "id": "71265b3d674970bd",
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Run the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a17474fe0822a8b2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from model_lib.model_loader import preload_models_from_standard_weights\n",
    "from model_lib.pipeline import generate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:02.368336626Z",
     "start_time": "2024-02-02T04:02:01.701527437Z"
    }
   },
   "id": "69a8bee25e565bfa",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cpu\"\n",
    "ALLOW_CUDA = True\n",
    "ALLOW_MPS = False\n",
    "\n",
    "if torch.cuda.is_available() and ALLOW_CUDA:\n",
    "    DEVICE = \"cuda\"\n",
    "elif (torch.backends.mps.is_built() or torch.backends.mps.is_available()) and ALLOW_MPS:\n",
    "    DEVICE = \"mps\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:03.085599644Z",
     "start_time": "2024-02-02T04:02:01.713908899Z"
    }
   },
   "id": "86f60d37ecd0a69b",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:03.228760996Z",
     "start_time": "2024-02-02T04:02:03.003572738Z"
    }
   },
   "id": "4ea0d8de7f5d782c",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 146.00 MiB (GPU 0; 3.81 GiB total capacity; 3.53 GiB already allocated; 108.38 MiB free; 3.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m CLIPTokenizer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstable_diffusion_files/tokenizer_vocab.json\u001B[39m\u001B[38;5;124m\"\u001B[39m, merges_file\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstable_diffusion_files/tokenizer_merges.txt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      2\u001B[0m model_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstable_diffusion_files/v1-5-pruned-emaonly.ckpt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 3\u001B[0m models \u001B[38;5;241m=\u001B[39m \u001B[43mpreload_models_from_standard_weights\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/SCIA_2/insta_flow_emoji/model_lib/model_loader.py:10\u001B[0m, in \u001B[0;36mpreload_models_from_standard_weights\u001B[0;34m(ckpt_path, device)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpreload_models_from_standard_weights\u001B[39m(ckpt_path, device):\n\u001B[0;32m---> 10\u001B[0m     state_dict \u001B[38;5;241m=\u001B[39m \u001B[43mload_from_standard_weights\u001B[49m\u001B[43m(\u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m     encoder \u001B[38;5;241m=\u001B[39m VAE_Encoder()\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     13\u001B[0m     encoder\u001B[38;5;241m.\u001B[39mload_state_dict(state_dict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoder\u001B[39m\u001B[38;5;124m'\u001B[39m], strict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/Documents/SCIA_2/insta_flow_emoji/model_lib/model_converter.py:6\u001B[0m, in \u001B[0;36mload_from_standard_weights\u001B[0;34m(input_file, device)\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_from_standard_weights\u001B[39m(input_file: \u001B[38;5;28mstr\u001B[39m, device: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;66;03m# Taken from: https://github.com/kjsman/stable-diffusion-pytorch/issues/7#issuecomment-1426839447\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m     original_model \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstate_dict\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m      8\u001B[0m     converted \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m      9\u001B[0m     converted[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdiffusion\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[0;32m~/.pyenv/versions/InstaFlow/lib/python3.11/site-packages/torch/serialization.py:809\u001B[0m, in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001B[0m\n\u001B[1;32m    807\u001B[0m             \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    808\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m pickle\u001B[38;5;241m.\u001B[39mUnpicklingError(UNSAFE_MESSAGE \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(e)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 809\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mopened_zipfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpickle_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpickle_load_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    810\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m weights_only:\n\u001B[1;32m    811\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/.pyenv/versions/InstaFlow/lib/python3.11/site-packages/torch/serialization.py:1172\u001B[0m, in \u001B[0;36m_load\u001B[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001B[0m\n\u001B[1;32m   1170\u001B[0m unpickler \u001B[38;5;241m=\u001B[39m UnpicklerWrapper(data_file, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpickle_load_args)\n\u001B[1;32m   1171\u001B[0m unpickler\u001B[38;5;241m.\u001B[39mpersistent_load \u001B[38;5;241m=\u001B[39m persistent_load\n\u001B[0;32m-> 1172\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43munpickler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1174\u001B[0m torch\u001B[38;5;241m.\u001B[39m_utils\u001B[38;5;241m.\u001B[39m_validate_loaded_sparse_tensors()\n\u001B[1;32m   1176\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/.pyenv/versions/InstaFlow/lib/python3.11/site-packages/torch/serialization.py:1142\u001B[0m, in \u001B[0;36m_load.<locals>.persistent_load\u001B[0;34m(saved_id)\u001B[0m\n\u001B[1;32m   1140\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1141\u001B[0m     nbytes \u001B[38;5;241m=\u001B[39m numel \u001B[38;5;241m*\u001B[39m torch\u001B[38;5;241m.\u001B[39m_utils\u001B[38;5;241m.\u001B[39m_element_size(dtype)\n\u001B[0;32m-> 1142\u001B[0m     typed_storage \u001B[38;5;241m=\u001B[39m \u001B[43mload_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_maybe_decode_ascii\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1144\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m typed_storage\n",
      "File \u001B[0;32m~/.pyenv/versions/InstaFlow/lib/python3.11/site-packages/torch/serialization.py:1116\u001B[0m, in \u001B[0;36m_load.<locals>.load_tensor\u001B[0;34m(dtype, numel, key, location)\u001B[0m\n\u001B[1;32m   1112\u001B[0m storage \u001B[38;5;241m=\u001B[39m zip_file\u001B[38;5;241m.\u001B[39mget_storage_from_record(name, numel, torch\u001B[38;5;241m.\u001B[39mUntypedStorage)\u001B[38;5;241m.\u001B[39m_typed_storage()\u001B[38;5;241m.\u001B[39m_untyped_storage\n\u001B[1;32m   1113\u001B[0m \u001B[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001B[39;00m\n\u001B[1;32m   1114\u001B[0m \u001B[38;5;66;03m# stop wrapping with TypedStorage\u001B[39;00m\n\u001B[1;32m   1115\u001B[0m typed_storage \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstorage\u001B[38;5;241m.\u001B[39mTypedStorage(\n\u001B[0;32m-> 1116\u001B[0m     wrap_storage\u001B[38;5;241m=\u001B[39m\u001B[43mrestore_location\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m   1117\u001B[0m     dtype\u001B[38;5;241m=\u001B[39mdtype,\n\u001B[1;32m   1118\u001B[0m     _internal\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m   1120\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m typed_storage\u001B[38;5;241m.\u001B[39m_data_ptr() \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1121\u001B[0m     loaded_storages[key] \u001B[38;5;241m=\u001B[39m typed_storage\n",
      "File \u001B[0;32m~/.pyenv/versions/InstaFlow/lib/python3.11/site-packages/torch/serialization.py:1083\u001B[0m, in \u001B[0;36m_get_restore_location.<locals>.restore_location\u001B[0;34m(storage, location)\u001B[0m\n\u001B[1;32m   1082\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrestore_location\u001B[39m(storage, location):\n\u001B[0;32m-> 1083\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdefault_restore_location\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/InstaFlow/lib/python3.11/site-packages/torch/serialization.py:217\u001B[0m, in \u001B[0;36mdefault_restore_location\u001B[0;34m(storage, location)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_restore_location\u001B[39m(storage, location):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m _, _, fn \u001B[38;5;129;01min\u001B[39;00m _package_registry:\n\u001B[0;32m--> 217\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    219\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/.pyenv/versions/InstaFlow/lib/python3.11/site-packages/torch/serialization.py:187\u001B[0m, in \u001B[0;36m_cuda_deserialize\u001B[0;34m(obj, location)\u001B[0m\n\u001B[1;32m    185\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mUntypedStorage(obj\u001B[38;5;241m.\u001B[39mnbytes(), device\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(location))\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 187\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/InstaFlow/lib/python3.11/site-packages/torch/_utils.py:81\u001B[0m, in \u001B[0;36m_cuda\u001B[0;34m(self, device, non_blocking, **kwargs)\u001B[0m\n\u001B[1;32m     79\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m new_type(indices, values, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msize())\n\u001B[1;32m     80\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 81\u001B[0m     untyped_storage \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mUntypedStorage\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     82\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     83\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     84\u001B[0m     untyped_storage\u001B[38;5;241m.\u001B[39mcopy_(\u001B[38;5;28mself\u001B[39m, non_blocking)\n\u001B[1;32m     85\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m untyped_storage\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 146.00 MiB (GPU 0; 3.81 GiB total capacity; 3.53 GiB already allocated; 108.38 MiB free; 3.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "tokenizer = CLIPTokenizer(\"stable_diffusion_files/tokenizer_vocab.json\", merges_file=\"stable_diffusion_files/tokenizer_merges.txt\")\n",
    "model_file = \"stable_diffusion_files/v1-5-pruned-emaonly.ckpt\"\n",
    "models = preload_models_from_standard_weights(model_file, DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:11.032058900Z",
     "start_time": "2024-02-02T04:02:03.088696686Z"
    }
   },
   "id": "e9e529292a69378",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## TEXT TO IMAGE\n",
    "\n",
    "# prompt = \"A dog with sunglasses, wearing comfy hat, looking at camera, highly detailed, ultra sharp, cinematic, 100mm lens, 8k resolution.\"\n",
    "prompt = \"A cat stretching on the floor, highly detailed, ultra sharp, cinematic, 100mm lens, 8k resolution.\"\n",
    "uncond_prompt = \"\"  # Also known as negative prompt\n",
    "do_cfg = True\n",
    "cfg_scale = 8  # min: 1, max: 14"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-02T04:02:11.076678467Z"
    }
   },
   "id": "2e4a86cf7bf10360"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## IMAGE TO IMAGE\n",
    "\n",
    "input_image = None\n",
    "# Comment to disable image to image\n",
    "image_path = \"../images/dog.jpg\"\n",
    "# input_image = Image.open(image_path)\n",
    "# Higher values means more noise will be added to the input image, so the result will further from the input image.\n",
    "# Lower values means less noise is added to the input image, so output will be closer to the input image.\n",
    "strength = 0.9"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-02T04:02:11.077039477Z"
    }
   },
   "id": "b9d9ff6beaac72fa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## SAMPLER\n",
    "\n",
    "sampler = \"ddpm\"\n",
    "num_inference_steps = 50\n",
    "seed = 42"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T04:02:11.107156293Z",
     "start_time": "2024-02-02T04:02:11.077183909Z"
    }
   },
   "id": "520d3bf5bbf5ce08"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "output_image = generate(\n",
    "    prompt=prompt,\n",
    "    uncond_prompt=uncond_prompt,\n",
    "    input_image=input_image,\n",
    "    strength=strength,\n",
    "    do_cfg=do_cfg,\n",
    "    cfg_scale=cfg_scale,\n",
    "    sampler_name=sampler,\n",
    "    n_inference_steps=num_inference_steps,\n",
    "    seed=seed,\n",
    "    models=models,\n",
    "    device=DEVICE,\n",
    "    idle_device=\"cpu\",\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-02T04:02:11.077312626Z"
    }
   },
   "id": "58975be4456d0f61",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Combine the input image and the output image into a single image.\n",
    "Image.fromarray(output_image)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-02T04:02:11.077438690Z"
    }
   },
   "id": "da0892d3aff082ef"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
