{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# first import all of the packages required in this entire project:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from glob import glob\n",
    "import copy\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim import lr_scheduler\n",
    "import cv2\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from torchvision.transforms import functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-03T17:24:28.141093Z",
     "end_time": "2023-06-03T17:24:30.875221Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-06-03T17:24:30.875221Z",
     "end_time": "2023-06-03T17:24:30.922286Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# all model configs go here so that they can be changed when we want to:\n",
    "class model_config:\n",
    "    seed = 42\n",
    "    encoder_name = \"timm-resnest200e\"  #\"efficientnet-b4\" # \"tu-efficientnetv2_m\" # from https://smp.readthedocs.io/en/latest/encoders_timm.html\n",
    "    train_batch_size = 4\n",
    "    valid_batch_size = 4\n",
    "    infer_batch_size = 2\n",
    "    epochs = 5\n",
    "    learning_rate = 0.001\n",
    "    scheduler = \"CosineAnnealingLR\"\n",
    "    T_max = int(30000 / train_batch_size * epochs)  # for cosineannealingLR, explore different values\n",
    "    weight_decay = 1e-6  # explore different weight decay (Adam optimizer)\n",
    "    n_accumulate = 1\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    iters_to_accumulate = max(1, 32 // train_batch_size)  # for scaling accumulated gradients\n",
    "    eta_min = 1e-5\n",
    "    model_save_directory = os.path.join(os.getcwd(), \"model\",\n",
    "                                        str(encoder_name))  #assuming os.getcwd is the current training script directory\n",
    "\n",
    "\n",
    "# sets the seed of the entire notebook so results are the same every time we run for reproducibility. no randomness, everything is controlled.\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)  #numpy specific random\n",
    "    random.seed(seed)  # python specific random (also for albumentation augmentations)\n",
    "    torch.manual_seed(seed)  # torch specific random\n",
    "    torch.cuda.manual_seed(seed)  # cuda specific random\n",
    "    # when running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  # when deterministic = true, benchmark = False, otherwise might not be deterministic\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # set a fixed value for the hash seed, for hases like dictionary\n",
    "\n",
    "\n",
    "set_seed(model_config.seed)\n",
    "val_transform = A.Compose([\n",
    "    # validate at 1024 x 1024, you want to use val dataset to real world application, but maybe resize to 384 if performance is bad.\n",
    "    #A.Resize(384),\n",
    "    ToTensorV2(),\n",
    "    #A.Normalize(mean=(0.8989, 0.9101, 0.9236), std=(0.0377, 0.0389, 0.0389)) #calculated above mean & std\n",
    "])\n",
    "mean = torch.tensor([0.8989, 0.9106, 0.9245])\n",
    "std = torch.tensor([0.0393, 0.0389, 0.0409])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# define the normalization function\n",
    "def normalize(image):\n",
    "    image = image.float() / 255.0  # convert image to float and scale to [0, 1]\n",
    "    image = (image - mean[:, None, None]) / std[:, None,\n",
    "                                            None]  # normalize each channel, where mean std is torch.Size[3,1,1] and image size is [3,512,512]\n",
    "    return image\n",
    "\n",
    "# build test dataset\n",
    "class TestDataSet(Dataset):\n",
    "    # initialize df, label, imagepath and transforms\n",
    "    def __init__(self, df, label=True, transforms=None):\n",
    "        self.df = df\n",
    "        self.label = label\n",
    "        self.imagepaths = df[\"image_path\"].tolist()\n",
    "        self.maskpaths = df[\"mask_path\"].tolist()\n",
    "        self.transforms = transforms\n",
    "\n",
    "    # define length, which is simply length of all imagepaths\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    # define main function to read image and label, apply transform function and return the transformed images.\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.imagepaths[idx]\n",
    "        image = cv2.imread(image_path, cv2.COLOR_BGR2RGB)\n",
    "        # image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = np.array(image)\n",
    "        if self.label:\n",
    "            mask_path = self.maskpaths[idx]\n",
    "            # mask = Image.open(mask_path).convert(\"L\")\n",
    "            mask = cv2.imread(mask_path, 0)\n",
    "            mask = np.array(mask)\n",
    "            mask_12ch = np.zeros((1024, 1024, 12), dtype=np.float32)\n",
    "            for class_idx in range(1, 13):\n",
    "                class_pixels = (np.array(mask) == class_idx)\n",
    "                mask_12ch[:, :, class_idx - 1] = class_pixels.astype(np.float32)\n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(image=image, mask=mask_12ch)\n",
    "            image = transformed['image']\n",
    "            image = normalize(image)\n",
    "            mask = transformed['mask']\n",
    "            mask = torch.permute(mask, (2, 0, 1))  # 512 x 512 x 12 -> 12 x 512 x 512 so it becomes N x C x H x W\n",
    "        image = image.float()\n",
    "        image = F.normalize(image, mean=(0.8989, 0.9101, 0.9236), std=(0.0377, 0.0389, 0.0389))\n",
    "        return image, mask, mask_path  # return tensors of image arrays, image should be 1024 x 1024 x 3, mask 1024 x 1024, image_path just a list of image paths"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-03T17:24:30.922286Z",
     "end_time": "2023-06-03T17:24:30.937669Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "      Unnamed: 0                  id     wsi_name   \n0              0   44930_16605xy0001  OTS_14684_3  \\\n1              1   44930_17629xy0002  OTS_14684_3   \n2              2   44930_18653xy0003  OTS_14684_3   \n3              3   44930_19677xy0004  OTS_14684_3   \n4              4   44930_20701xy0005  OTS_14684_3   \n...          ...                 ...          ...   \n7195        7195  142210_89309xy7197  OTS_14684_3   \n7196        7196  142210_90333xy7198  OTS_14684_3   \n7197        7197  142210_91357xy7199  OTS_14684_3   \n7198        7198  142210_92381xy7200  OTS_14684_3   \n7199        7199   98178_73949xy3957  OTS_14684_3   \n\n                                             image_path   \n0     \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...  \\\n1     \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...   \n2     \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...   \n3     \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...   \n4     \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...   \n...                                                 ...   \n7195  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...   \n7196  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...   \n7197  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...   \n7198  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...   \n7199  \\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...   \n\n                                              mask_path   \n0     \\\\shelter\\Kyu\\unstain2mask\\masks\\OTS_14684_3\\4...  \\\n1     \\\\shelter\\Kyu\\unstain2mask\\masks\\OTS_14684_3\\4...   \n2     \\\\shelter\\Kyu\\unstain2mask\\masks\\OTS_14684_3\\4...   \n3     \\\\shelter\\Kyu\\unstain2mask\\masks\\OTS_14684_3\\4...   \n4     \\\\shelter\\Kyu\\unstain2mask\\masks\\OTS_14684_3\\4...   \n...                                                 ...   \n7195  \\\\shelter\\Kyu\\unstain2mask\\masks\\OTS_14684_3\\1...   \n7196  \\\\shelter\\Kyu\\unstain2mask\\masks\\OTS_14684_3\\1...   \n7197  \\\\shelter\\Kyu\\unstain2mask\\masks\\OTS_14684_3\\1...   \n7198  \\\\shelter\\Kyu\\unstain2mask\\masks\\OTS_14684_3\\1...   \n7199  \\\\shelter\\Kyu\\unstain2mask\\masks\\OTS_14684_3\\9...   \n\n                                            composition   \n0                 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]  \\\n1                 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]   \n2                 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]   \n3                 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]   \n4                 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]   \n...                                                 ...   \n7195              [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]   \n7196              [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]   \n7197              [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]   \n7198              [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]   \n7199  [0.    0.    0.    0.    0.    0.    0.    0. ...   \n\n               composition_freq  \n0     [0 0 0 0 0 0 0 0 0 0 0 1]  \n1     [0 0 0 0 0 0 0 0 0 0 0 1]  \n2     [0 0 0 0 0 0 0 0 0 0 0 1]  \n3     [0 0 0 0 0 0 0 0 0 0 0 1]  \n4     [0 0 0 0 0 0 0 0 0 0 0 1]  \n...                         ...  \n7195  [0 0 0 0 0 0 0 0 0 0 0 1]  \n7196  [0 0 0 0 0 0 0 0 0 0 0 1]  \n7197  [0 0 0 0 0 0 0 0 0 0 0 1]  \n7198  [0 0 0 0 0 0 0 0 0 0 0 1]  \n7199  [0 0 0 0 0 0 0 0 0 1 0 1]  \n\n[7200 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>id</th>\n      <th>wsi_name</th>\n      <th>image_path</th>\n      <th>mask_path</th>\n      <th>composition</th>\n      <th>composition_freq</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>44930_16605xy0001</td>\n      <td>OTS_14684_3</td>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>\\\\shelter\\Kyu\\unstain2mask\\masks\\OTS_14684_3\\4...</td>\n      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]</td>\n      <td>[0 0 0 0 0 0 0 0 0 0 0 1]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>44930_17629xy0002</td>\n      <td>OTS_14684_3</td>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>\\\\shelter\\Kyu\\unstain2mask\\masks\\OTS_14684_3\\4...</td>\n      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]</td>\n      <td>[0 0 0 0 0 0 0 0 0 0 0 1]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>44930_18653xy0003</td>\n      <td>OTS_14684_3</td>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>\\\\shelter\\Kyu\\unstain2mask\\masks\\OTS_14684_3\\4...</td>\n      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]</td>\n      <td>[0 0 0 0 0 0 0 0 0 0 0 1]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>44930_19677xy0004</td>\n      <td>OTS_14684_3</td>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>\\\\shelter\\Kyu\\unstain2mask\\masks\\OTS_14684_3\\4...</td>\n      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]</td>\n      <td>[0 0 0 0 0 0 0 0 0 0 0 1]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>44930_20701xy0005</td>\n      <td>OTS_14684_3</td>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>\\\\shelter\\Kyu\\unstain2mask\\masks\\OTS_14684_3\\4...</td>\n      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]</td>\n      <td>[0 0 0 0 0 0 0 0 0 0 0 1]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7195</th>\n      <td>7195</td>\n      <td>142210_89309xy7197</td>\n      <td>OTS_14684_3</td>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>\\\\shelter\\Kyu\\unstain2mask\\masks\\OTS_14684_3\\1...</td>\n      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]</td>\n      <td>[0 0 0 0 0 0 0 0 0 0 0 1]</td>\n    </tr>\n    <tr>\n      <th>7196</th>\n      <td>7196</td>\n      <td>142210_90333xy7198</td>\n      <td>OTS_14684_3</td>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>\\\\shelter\\Kyu\\unstain2mask\\masks\\OTS_14684_3\\1...</td>\n      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]</td>\n      <td>[0 0 0 0 0 0 0 0 0 0 0 1]</td>\n    </tr>\n    <tr>\n      <th>7197</th>\n      <td>7197</td>\n      <td>142210_91357xy7199</td>\n      <td>OTS_14684_3</td>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>\\\\shelter\\Kyu\\unstain2mask\\masks\\OTS_14684_3\\1...</td>\n      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]</td>\n      <td>[0 0 0 0 0 0 0 0 0 0 0 1]</td>\n    </tr>\n    <tr>\n      <th>7198</th>\n      <td>7198</td>\n      <td>142210_92381xy7200</td>\n      <td>OTS_14684_3</td>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>\\\\shelter\\Kyu\\unstain2mask\\masks\\OTS_14684_3\\1...</td>\n      <td>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]</td>\n      <td>[0 0 0 0 0 0 0 0 0 0 0 1]</td>\n    </tr>\n    <tr>\n      <th>7199</th>\n      <td>7199</td>\n      <td>98178_73949xy3957</td>\n      <td>OTS_14684_3</td>\n      <td>\\\\shelter\\Kyu\\unstain2stain\\tiles\\registered_t...</td>\n      <td>\\\\shelter\\Kyu\\unstain2mask\\masks\\OTS_14684_3\\9...</td>\n      <td>[0.    0.    0.    0.    0.    0.    0.    0. ...</td>\n      <td>[0 0 0 0 0 0 0 0 0 1 0 1]</td>\n    </tr>\n  </tbody>\n</table>\n<p>7200 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load test_df in from directory (includes images to run inference and ground truth mask path):\n",
    "test_df_src = r\"\\\\shelter\\Kyu\\unstain2mask\\main\\test_df.xlsx\"\n",
    "test_df = pd.read_excel(test_df_src)\n",
    "test_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-03T17:24:30.937669Z",
     "end_time": "2023-06-03T17:24:31.578261Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images have a tensor size of torch.Size([2, 3, 1024, 1024]), Masks have a tensor size of torch.Size([2, 12, 1024, 1024]), and Mask Paths have a length of 2\n"
     ]
    }
   ],
   "source": [
    "# define dataloading function:\n",
    "test_dataset = TestDataSet(df=test_df, transforms=val_transform)\n",
    "test_dataloader = DataLoader(dataset=test_dataset,\n",
    "                             batch_size=model_config.infer_batch_size,\n",
    "                             num_workers=0, pin_memory=True, shuffle=True)\n",
    "\n",
    "image, masks, mask_path = next(iter(test_dataloader))\n",
    "print(\"Images have a tensor size of {}, Masks have a tensor size of {}, and Mask Paths have a length of {}\".\n",
    "      format(image.size(), masks.size(), len(mask_path)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-03T17:24:31.578261Z",
     "end_time": "2023-06-03T17:24:32.906448Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = smp.UnetPlusPlus(encoder_name=model_config.encoder_name, encoder_weights=\"imagenet\", activation=None,\n",
    "                             in_channels=3, classes=12, decoder_use_batchnorm=True)\n",
    "    model.to(model_config.device)  # model to gpu\n",
    "    return model\n",
    "\n",
    "\n",
    "def dice_coef(y_true, y_pred, thr=0.5, dim=(2, 3), epsilon=0.001):\n",
    "    \"\"\"\n",
    "    Function to calculate dice coef, both y_true and y_pred needs to be of tensor size N x 12 x 1024 x 1024 (batch size x num_classes x image height x image width).\n",
    "    \"\"\"\n",
    "    y_true = y_true.to(torch.float32)\n",
    "    y_pred = (y_pred > thr).to(torch.float32)  # binary tensor\n",
    "    intersection = (y_true * y_pred).sum(dim=dim)  # calculate overlapping pixels b/w pred and true for height and width\n",
    "    den = y_true.sum(dim=dim) + y_pred.sum(dim=dim)  # denominator, A+B along height and width\n",
    "    dice = ((2 * intersection + epsilon) / (den + epsilon)).mean(\n",
    "        dim=(1, 0))  # avg over batch & channel to return scalar\n",
    "    return dice\n",
    "\n",
    "\n",
    "### Run inference, predicted mask tensor should be saved as the original mask array (take argmax along axis = 0). Dice score should be calculated b/w  return list of arrays size of batch size.\n",
    "@torch.no_grad()\n",
    "def infer(model_paths, test_loader, thr=0.5):\n",
    "    model = build_model()  #initialize the model outside the loop\n",
    "    pred_labels = []\n",
    "    mask_names = []\n",
    "    pred_labels_raw = []  # 12-channel one, returned so that dice score can be calculated after inference\n",
    "    # dice_scores = []\n",
    "    for idx, (image, masks, mask_path) in enumerate(tqdm(test_loader, total=len(test_loader), desc='Inference')):\n",
    "        y_pred_list = []\n",
    "        image = image.to(model_config.device, dtype=torch.float)\n",
    "        masks = masks.to(model_config.device, dtype=torch.float)\n",
    "        mask_names.append(mask_path)\n",
    "        # y_pred = torch.zeros((model_config.infer_batch_size,12,1024,1024), device = model_config.device)# empty N x 12 x 1024 x 1024 tensor reinitialized for every batch\n",
    "        # for path in model_paths:  # load five best models from each fold\n",
    "        model.load_state_dict(torch.load(model_paths))\n",
    "        model.eval()  # change model to eval stage\n",
    "        output = model(\n",
    "            image)  # output of size N x 12 x 1024 x 1024, each channel in dim=1 with 12 classes has logits of label for that specific pixel\n",
    "        output = nn.Sigmoid()(\n",
    "            output)  #make the output to 0~1 probabilities, model last layer doesn't have sigmoid in it, so this becomes probabilities\n",
    "        y_pred_list.append(output)  #ensemble, append the y_preds as a list\n",
    "        y_pred = torch.stack(y_pred_list, dim=0)  # stack the list along a fifth dimension\n",
    "        y_pred = torch.mean(y_pred,\n",
    "                            dim=0)  #ensemble, average the probabilities along the 5-th dimension to make it 4d again\n",
    "        # calculate dice score before making 12-channel to 1-channel\n",
    "        # dice_score = dice_coef(masks,y_pred) # dice score should be scalar, size BS x 1\n",
    "        pred_labels_raw.append(y_pred)  # save the 12-channel pred labels\n",
    "        y_pred = torch.argmax(y_pred,\n",
    "                              dim=1)  # argmax along channel axis, returns the highest probable channel for that pixel\n",
    "        y_pred = y_pred\n",
    "        pred_labels.append(y_pred)  # list of arrays\n",
    "        # dice_scores.append(dice_score) # list of dice scores\n",
    "    return pred_labels_raw, pred_labels, mask_names  #,dice_scores"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-03T17:28:13.968847Z",
     "end_time": "2023-06-03T17:28:14.000185Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model list are C:\\Users\\labadmin\\PycharmProjects\\wsi_analysis\\kevin\\unstain2mask\\model\\timm-resnest200e\\best_epoch-00.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   4%|▍         | 152/3600 [01:58<44:57,  1.28it/s]  \n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.13 GiB (GPU 0; 23.99 GiB total capacity; 20.40 GiB already allocated; 0 bytes free; 22.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m model_paths \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mUsers\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mlabadmin\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mPycharmProjects\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mwsi_analysis\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mkevin\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124munstain2mask\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mtimm-resnest200e\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mbest_epoch-00.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;66;03m#glob(f'{saved_model_path}/best_epoch*.pt') # try one model, keep getting cuda out of memory\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel list are \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(model_paths))\n\u001B[1;32m----> 6\u001B[0m pred_labels_raw, pred_labels, mask_names \u001B[38;5;241m=\u001B[39m \u001B[43minfer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_paths\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\wsi_analysis\\venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "Cell \u001B[1;32mIn[14], line 38\u001B[0m, in \u001B[0;36minfer\u001B[1;34m(model_paths, test_loader, thr)\u001B[0m\n\u001B[0;32m     36\u001B[0m model\u001B[38;5;241m.\u001B[39mload_state_dict(torch\u001B[38;5;241m.\u001B[39mload(model_paths))\n\u001B[0;32m     37\u001B[0m model\u001B[38;5;241m.\u001B[39meval()  \u001B[38;5;66;03m# change model to eval stage\u001B[39;00m\n\u001B[1;32m---> 38\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     39\u001B[0m \u001B[43m    \u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# output of size N x 12 x 1024 x 1024, each channel in dim=1 with 12 classes has logits of label for that specific pixel\u001B[39;00m\n\u001B[0;32m     40\u001B[0m output \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mSigmoid()(\n\u001B[0;32m     41\u001B[0m     output)  \u001B[38;5;66;03m#make the output to 0~1 probabilities, model last layer doesn't have sigmoid in it, so this becomes probabilities\u001B[39;00m\n\u001B[0;32m     42\u001B[0m y_pred_list\u001B[38;5;241m.\u001B[39mappend(output)  \u001B[38;5;66;03m#ensemble, append the y_preds as a list\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\wsi_analysis\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\PycharmProjects\\wsi_analysis\\venv\\lib\\site-packages\\segmentation_models_pytorch\\base\\model.py:30\u001B[0m, in \u001B[0;36mSegmentationModel.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_input_shape(x)\n\u001B[0;32m     29\u001B[0m features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(x)\n\u001B[1;32m---> 30\u001B[0m decoder_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m masks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msegmentation_head(decoder_output)\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclassification_head \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\wsi_analysis\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\PycharmProjects\\wsi_analysis\\venv\\lib\\site-packages\\segmentation_models_pytorch\\decoders\\unetplusplus\\decoder.py:135\u001B[0m, in \u001B[0;36mUnetPlusPlusDecoder.forward\u001B[1;34m(self, *features)\u001B[0m\n\u001B[0;32m    133\u001B[0m             cat_features \u001B[38;5;241m=\u001B[39m [dense_x[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00midx\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdense_l_i\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(depth_idx \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, dense_l_i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)]\n\u001B[0;32m    134\u001B[0m             cat_features \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(cat_features \u001B[38;5;241m+\u001B[39m [features[dense_l_i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m]], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m--> 135\u001B[0m             dense_x[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdepth_idx\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdense_l_i\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mblocks\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mx_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mdepth_idx\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mdense_l_i\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    136\u001B[0m \u001B[43m                \u001B[49m\u001B[43mdense_x\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mx_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mdepth_idx\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mdense_l_i\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcat_features\u001B[49m\n\u001B[0;32m    137\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    138\u001B[0m dense_x[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;241m0\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdepth\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocks[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;241m0\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdepth\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m](dense_x[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;241m0\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdepth\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m dense_x[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;241m0\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdepth\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\PycharmProjects\\wsi_analysis\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\PycharmProjects\\wsi_analysis\\venv\\lib\\site-packages\\segmentation_models_pytorch\\decoders\\unetplusplus\\decoder.py:38\u001B[0m, in \u001B[0;36mDecoderBlock.forward\u001B[1;34m(self, x, skip)\u001B[0m\n\u001B[0;32m     36\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39minterpolate(x, scale_factor\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnearest\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m skip \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 38\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskip\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     39\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention1(x)\n\u001B[0;32m     40\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv1(x)\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 1.13 GiB (GPU 0; 23.99 GiB total capacity; 20.40 GiB already allocated; 0 bytes free; 22.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Evaluate dice score save the array of arrays as an image at an directory :\n",
    "import glob\n",
    "# saved_model_path = model_config.model_save_directory\n",
    "model_paths = r\"C:\\Users\\labadmin\\PycharmProjects\\wsi_analysis\\kevin\\unstain2mask\\model\\timm-resnest200e\\best_epoch-00.pt\" #glob(f'{saved_model_path}/best_epoch*.pt') # try one model, keep getting cuda out of memory\n",
    "print(\"Model list are {}\".format(model_paths))\n",
    "pred_labels_raw, pred_labels, mask_names = infer(model_paths, test_dataloader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def convert_mask_to_binary_channel(mask_path):\n",
    "    mask = cv2.imread(mask_path, 0)\n",
    "    mask = np.array(mask)\n",
    "    mask_12ch = np.zeros((1024, 1024, 12), dtype=np.float32)\n",
    "    for class_idx in range(1, 13):\n",
    "        class_pixels = (np.array(mask) == class_idx)\n",
    "        mask_12ch[:, :, class_idx - 1] = class_pixels.astype(np.float32)\n",
    "    mask_12ch = np.transpose(mask_12ch, (2, 0, 1))\n",
    "    mask_12ch = np.expand_dims(mask_12ch, axis=0)\n",
    "    return mask_12ch  # needs to be 1 x 12 x 1024 x 1024 (1 x num_classes x H x W)\n",
    "\n",
    "def save_inference_results(pred_labels_raw, pred_labels, mask_names, save_dst_pth):\n",
    "    results_df = pd.DataFrame(columns=[\"dice_score\", \"inference_mask_save_path\", \"true_mask_path\"])\n",
    "    pred_labels_raw_numpy = [x.cpu().detach().numpy() for x in pred_labels_raw]\n",
    "    pred_labels_numpy = [x.cpu().detach().numpy() for x in pred_labels]  # list of pred_labels\n",
    "    mask_names_numpy = [x for x in mask_names]\n",
    "    dice_scores = []\n",
    "    true_mask_names = []\n",
    "    inference_save_paths = []\n",
    "    for num_batch_idx in range(int(test_df.shape[\n",
    "                                       0] / model_config.infer_batch_size)):  # length of test_df must be divisible by infer_batch_size\n",
    "        batches_pred_labels_raw = pred_labels_raw_numpy[num_batch_idx]\n",
    "        batches_pred_labels = pred_labels_numpy[num_batch_idx]\n",
    "        batches_mask_names = mask_names_numpy[num_batch_idx]\n",
    "        for batch_idx in range(model_config.infer_batch_size):\n",
    "            each_pred_labels_raw = batches_pred_labels_raw[batch_idx]  # 12 x 1024 x 1024\n",
    "            each_pred_labels_raw = np.expand_dims(each_pred_labels_raw, axis=0)  # 1 x 12 x 1024 x 1024 for dice score\n",
    "            each_pred_label = batches_pred_labels[batch_idx]\n",
    "            each_pred_label = each_pred_label.astype('uint8')\n",
    "            each_mask_name = batches_mask_names[batch_idx]\n",
    "            true_mask_names.append(each_mask_name)\n",
    "            each_mask_ext = os.path.basename(each_mask_name)  #get only name to save!\n",
    "            y_true = convert_mask_to_binary_channel(each_mask_name)\n",
    "            y_true = torch.from_numpy(y_true)\n",
    "            each_pred_labels_raw = torch.from_numpy(each_pred_labels_raw)\n",
    "            dice_score = dice_coef(y_true, each_pred_labels_raw)\n",
    "            dice_score = dice_score.cpu().detach().numpy()\n",
    "            dice_scores.append(dice_score)\n",
    "            img_save_pth = os.path.join(save_dst_pth, each_mask_ext)\n",
    "            inference_save_paths.append(img_save_pth)\n",
    "            Image.fromarray(each_pred_label).save(img_save_pth)\n",
    "    results_df[\"dice_score\"] = dice_scores\n",
    "    results_df[\"inference_mask_save_path\"] = inference_save_paths\n",
    "    results_df[\"true_mask_path\"] = true_mask_names\n",
    "    return results_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_df = save_inference_results(pred_labels_raw, pred_labels, mask_names,\n",
    "                                    r\"\\\\shelter\\Kyu\\unstain2mask\\main\\inference\")\n",
    "results_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_src = r\"\\\\shelter\\Kyu\\unstain2mask\\main\\inference\\results_df.xlsx\"\n",
    "results_df.to_excel(save_src)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
