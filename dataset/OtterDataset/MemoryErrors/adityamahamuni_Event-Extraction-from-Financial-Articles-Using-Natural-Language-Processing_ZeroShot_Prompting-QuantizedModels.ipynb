{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_event_information(model, tokenizer, prompt, max_input_length=500, max_new_tokens=50):\n",
    "    # Tokenize the input prompt, truncating to the maximum input length\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\",\n",
    "                       truncation=True, max_length=max_input_length)\n",
    "\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'], max_new_tokens=max_new_tokens)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_response(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    outputs = model.generate(**inputs, max_length=1024)\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir='/dcs/large/u5579267/.huggingface'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlaMa 3 8B Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 12:29:13.707441: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-17 12:29:19.387029: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /local/java/cuda-11.6.0/lib64/:/local/java/cudnn-linux-x86_64-8.5.0.96_cuda11-archive/lib/:/local/java/cuda-12.2/lib64/:/local/java/cudnn-linux-x86_64-8.9.4.25_cuda12-archive/lib/:/local/java/cuda-12.2/extras/CUPTI/lib64/\n",
      "2024-07-17 12:29:19.388218: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /local/java/cuda-11.6.0/lib64/:/local/java/cudnn-linux-x86_64-8.5.0.96_cuda11-archive/lib/:/local/java/cuda-12.2/lib64/:/local/java/cudnn-linux-x86_64-8.9.4.25_cuda12-archive/lib/:/local/java/cuda-12.2/extras/CUPTI/lib64/\n",
      "2024-07-17 12:29:19.388228: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "def extract_company_name(sentence):\n",
    "    \"\"\"Extracts the company name from the sentence based on common patterns.\"\"\"\n",
    "    # Regular expression to find company names\n",
    "    pattern = r\"\\b[A-Z][a-zA-Z]*\\s(?:Inc|Corp|Ltd|LLC|Group|Company|PLC|Corporation|Incorporated|N\\.A\\.)\\b\"\n",
    "    match = re.search(pattern, sentence)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_events_zero_shot(model, tokenizer, sentence, event_types):\n",
    "    \"\"\"Performs zero-shot event extraction and company name extraction.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the Hugging Face model (e.g., \"google/flan-t5-xl\").\n",
    "        sentence (str): The sentence to extract events from.\n",
    "        event_types (list): List of possible event types.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted event type and company name.\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    model.to(device)\n",
    "\n",
    "    extractor = pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer, device=0)\n",
    "    candidate_labels = event_types + [\"Company\"]\n",
    "\n",
    "    result = extractor(sentence, candidate_labels)\n",
    "\n",
    "    # Find the event type with the highest score\n",
    "    event_type = max(result[\"labels\"], key=lambda label: result[\"scores\"][result[\"labels\"].index(label)])\n",
    "    if event_type not in event_types:\n",
    "        event_type = \"Other/None\"  # If no valid event type found\n",
    "\n",
    "    company_name = extract_company_name(sentence)\n",
    "    if not company_name:\n",
    "        company_name = \"Unknown\"  # \"Unknown\" if no company name is found\n",
    "\n",
    "    return {\"event_type\": event_type, \"company\": company_name}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"CBTX Inc. Declares Quarterly Dividend and Suspends Repurchase Program. HOUSTON, March 18, 2020 ( ) CBTX, Inc., the bank holding company for CommunityBank of Texas N.A., today announced that its Board of Directors declared a quarterly cash dividend in the amount of $0.10 per share of common stock. The dividend will be payable on April 15, 2020 to shareholders of record as of the close of business on April 1, 2020. In addition, CBTX, Inc. today announced that it has temporarily suspended its share repurchase program in light of the challenges presented by the COVID-19 pandemic and surrounding events. CBTX, Inc. believes that it remains strong and well-capitalized, and the Company may reinstate the share repurchase program in the future. The Company repurchased 240,445 shares of common stock during the first quarter of 2020 for an aggregate purchase price of approximately $5.4 million under its repurchase program.\"\n",
    "event_types = [\n",
    "    \"Acquisition (A)\", \"Clinical Trial (CT)\", \"Regular Dividend (RD)\",\n",
    "    \"Dividend Cut (DC)\", \"Dividend Increase (DI)\", \"Guidance Increase (GI)\",\n",
    "    \"New Contract (NC)\", \"Reverse Stock Split (RSS)\", \"Special Dividend (SD)\",\n",
    "    \"Stock Repurchase (SR)\", \"Stock Split (SS)\", \"Other/None (O)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93095572542a4ffe9111b7e743fe15e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB (GPU 0; 10.75 GiB total capacity; 10.46 GiB already allocated; 18.50 MiB free; 10.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m llama_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n\u001b[1;32m      6\u001b[0m llama_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n\u001b[0;32m----> 8\u001b[0m extracted_info \u001b[38;5;241m=\u001b[39m \u001b[43mextract_events_zero_shot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllama_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllama_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_types\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(extracted_info, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m, in \u001b[0;36mextract_events_zero_shot\u001b[0;34m(model, tokenizer, sentence, event_types)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"Performs zero-shot event extraction and company name extraction.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    dict: A dictionary containing the extracted event type and company name.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m---> 13\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m extractor \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzero-shot-classification\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     16\u001b[0m candidate_labels \u001b[38;5;241m=\u001b[39m event_types \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompany\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/modeling_utils.py:2796\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2792\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2793\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2794\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2795\u001b[0m         )\n\u001b[0;32m-> 2796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local/java/python-ml/22-12-21-python3.9/lib64/python3.9/site-packages/torch/nn/modules/module.py:989\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 989\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/local/java/python-ml/22-12-21-python3.9/lib64/python3.9/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/local/java/python-ml/22-12-21-python3.9/lib64/python3.9/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 641 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/local/java/python-ml/22-12-21-python3.9/lib64/python3.9/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/local/java/python-ml/22-12-21-python3.9/lib64/python3.9/site-packages/torch/nn/modules/module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 664\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/local/java/python-ml/22-12-21-python3.9/lib64/python3.9/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 987\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB (GPU 0; 10.75 GiB total capacity; 10.46 GiB already allocated; 18.50 MiB free; 10.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "\n",
    "extracted_info = extract_events_zero_shot(llama_model, llama_tokenizer, sentence, event_types)\n",
    "print(json.dumps(extracted_info, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': ['JTI report warns of a \\'Gathering Storm\\' in the black market English English Intelligence shows criminals are ready for post-Covid boom GENEVA , Sept . 11 , 2020 / / JTI ( Japan Tobacco International ) has published a report , independently verified by Intrinsic Insight Ltd. , entitled \\'The Gathering Storm\\' , on how the illegal tobacco trade are operating during the Covid-19 global pandemic and preparing to reap the rewards in the economic aftermath that will follow . Law enforcement agencies around the world have welcomed the report , which is based on 63 field studies , conducted across 50 countries including Russia , Canada , Malaysia , and the Philippines where tobacco smugglers currently have a strong presence . JTI intelligence found that the global public health crisis and financial downturn has created the conditions for a \\'perfect storm\\' where organized criminal groups will further exploit public demand for cheap goods , and capitalize on dwindling buying power in the impending global recession , particularly in countries with high tax regimes . The report has provided JTI with a global picture of four emerging trends , consistent with Euromonitor and Europol intelligence: 1 . Evidence shows that criminal groups are biding their time in readiness for an anticipated boom in illegal tobacco sales; 2 . After initial disruption to the illegal supply chain in Western European markets , organizedcriminals quickly exploited the inconsistent approach to travel and lockdownrulesand found alternative routes from production to distribution , leading to significant seizures of illegal factories or their components in countries such as the Czech Republic , Greece , Ireland , Belgium , and Spain; 3 . Changed law enforcement priorities and border restrictions have been mixed in limiting supply and the availability of illegal tobacco: whilst governments and authorities in Far East Asia were quicker to impose restrictions , those in the West failed to act with such precision; 4 . Technology has been increasingly deployed throughout the pandemic to enable sales of illegal tobacco to continue where strict lockdowns were put in place by governments throughout Eastern Europe , the Middle East , Africa and Asia Pacific , where WhatsApp and Facebook have provided quick and easy methods of communication between the consumer and criminals . Furthermore , the International Chamber of Commerce predicts that global counterfeit trade will reach $4 trillion by 2022 , primarily fueled by e-commerce[1] . According to the World Bank , the global trade in illegal tobacco is already worth an estimated $40-50 billion each year to the criminal groups who produce , manufacture , smuggle , distribute and sell tobacco products on which there is no tax duty paid . The loss of revenue to law-abiding retailers is also significantly felt , as is the impact on consumers who are lured into buying sub-standard products . \"To some consumers illegal tobacco is a victimless crime , which is why we need to inform them not only of the hidden dangers they are consuming , but the wider social consequences of buying from criminal groups'], 'events': ['O']}\n",
      "\n",
      "{'sentence': ['Windtree Therapeutics Announces Reverse Stock Split WARRINGTON , Pa. , April 28 , 2020 / / Windtree Therapeutics , Inc . ( OTCQB: WINT ) , a biotechnology and medical device company focused on developing drug product candidates and medical device technologies to address acute cardiovascular and pulmonary diseases , today announced a 1-for-3 reverse stock split of its issued and outstanding common stock . The Company\\'s common stock will begin trading on a split-adjusted basis at the opening of the OTCQB Market on Wednesday , April 29 , 2020 under the symbol \"WINTD\" . After 20 trading days , the symbol will revert to WINT . Effective with the reverse split , a new CUSIP number of 97382D 303 has been assigned to the Company\\'s common stock . The number of shares of common stock authorized under the Company\\'s Amended and Restated Certificate of Incorporation is unchanged at 120 million shares . The Company is pursuing a strategy intended to result in the listing of its common stock on The Nasdaq Capital Market ( \"Nasdaq\" ) . The Company implemented the reverse stock split to increase the bid price of its common stock on the OTCQB market and potentially assure compliance with Nasdaq\\'s initial listing requirement for minimum bid price . Before any listing of the common stock on Nasdaq could occur , Nasdaq will need to approve the Company\\'s application for listing . There can be no assurance that the Company will satisfy other applicable requirements for listing its common stock on Nasdaq or that the Company\\'s application to up-list its common stock will be approved . On June 20 , 2019 , a majority of the Company\\'s stockholders approved the reverse stock split by a written consent . The written consent granted the board of directors the authority to implement and determine the exact split ratio , which was set by a committee of the board of directors at 1 for 3 on April 8 , 2020 . An amendment to the Company\\'s Amended and Restated Certificate of Incorporation to implement the reverse split stock was filed on April 28 , 2020 . No fractional shares will be issued as a result of the reverse stock split . Stockholders who would otherwise be entitled to a fractional share will receive a prorated cash payment , without interest , based on the closing sales price on the OTCQB Market of the Company\\'s common stock on the business day immediately preceding the effective date of the reverse stock split . The Company\\'s transfer agent , Continental Stock Transfer & Trust Company , which is also acting as the exchange agent for the reverse split , will send instructions to stockholders of record who hold stock certificates regarding the exchange of their old certificates for new certificates , should they wish to do so . Stockholders who hold their shares in brokerage accounts or \"street name\" are not required to take any action to effect the exchange of their shares . About Windtree Therapeutics Windtree Therapeutics , Inc . is a clinical-stage'], 'events': ['I-RSS']}\n",
      "\n",
      "{'sentence': ['TOP Ships Announces Reverse Stock Split ATHENS , Greece , Aug . 07 , 2020 ( ) TOP Ships Inc . ( NASDAQ: TOPS ) ( the Company ) , announced today that it has determined to effect a 1-for-25 reverse stock split of the Company s issued common shares . The Company s shareholders approved the reverse stock split and granted the Board the authority to determine the exact split ratio and when to proceed with the reverse stock split at the Company s Annual Meeting of Shareholders held on May 29 , 2020 . The reverse stock split will take effect , and the Company s common stock will begin trading on a split-adjusted basis on the NASDAQ Capital Market , as of the opening of trading on Monday , August 10 , 2020 under the existing ticker symbol TOPS . The new CUSIP number for the Company s common stock will be Y8897Y 180 . When the reverse stock split becomes effective , every 25 shares of the Company s issued and outstanding common stock will be automatically combined into one issued and outstanding share of common stock without any change in the par value per share or the total number of authorized shares . This will reduce the number of outstanding shares of the Company s common stock from approximately 995.8 million shares to approximately 39.8 million shares . No fractional shares will be issued in connection with the reverse split of the issued and outstanding common stock . Shareholders shall be paid cash-in-lieu of a fractional shares that occur as a result of the reverse stock split . Shareholders will receive instructions from the Company s exchange agent , American Stock Transfer & Trust Company , LLC , as to how to exchange existing share certificates for new certificates representing the post-reverse split shares . Additional information about the reverse stock split can be found in the Company s proxy statement furnished to the Securities and Exchange Commission on April 10 , 2020 , a copy of which is available at www.sec.gov . About TOP Ships Inc . TOPS is an international owner and operator of modern , fuel efficient ECO tanker vessels currently focusing on the transportation of crude oil and petroleum products . Cautionary Note Regarding Forward-Looking Statements Matters discussed in this press release may constitute forward-looking statements within the meaning of the U.S . federal securities laws . The Private Securities Litigation Reform Act of 1995 provides safe harbor protections for forward-looking statements in order to encourage companies to provide prospective information about their business . Forward-looking statements include statements concerning plans , objectives , goals , strategies , future events or performance , and underlying assumptions and other statements , which are other than statements of historical facts . The Company desires to take advantage of the safe harbor provisions of the Private Securities Litigation Reform Act of 1995 and is including this cautionary statement in connection with this safe harbor legislation . The words believe , anticipate , intends , estimate ,'], 'events': ['I-RSS']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(os.getcwd(), \"EDT_dataset\", \"Event_detection\", \"train.json\")\n",
    "\n",
    "# Load the JSON object from the file\n",
    "with open(data_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print the first few entries\n",
    "for i in range(3):\n",
    "    print(data[-i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(model_name, sentence):\n",
    "        event_types = [\n",
    "            \"Acquisition (A)\", \"Clinical Trial (CT)\", \"Regular Dividend (RD)\",\n",
    "            \"Dividend Cut (DC)\", \"Dividend Increase (DI)\", \"Guidance Increase (GI)\",\n",
    "            \"New Contract (NC)\", \"Reverse Stock Split (RSS)\", \"Special Dividend (SD)\",\n",
    "            \"Stock Repurchase (SR)\", \"Stock Split (SS)\", \"Other/None (O)\"\n",
    "        ]\n",
    "\n",
    "        extracted_events = extract_events_zero_shot(model_name, sentence, event_types)\n",
    "\n",
    "        return extracted_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716403be54e94e3682f2f46619d2a5fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google/flan-t5-xl and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da3e2cc3ef840b5b9cbfcf8a96e2505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google/flan-t5-xl and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m      6\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m     extracted_events \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m\"\u001b[39m: sentence, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextracted_events\u001b[39m\u001b[38;5;124m\"\u001b[39m: extracted_events, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual_events\u001b[39m\u001b[38;5;124m\"\u001b[39m: item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevents\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m, in \u001b[0;36mprocess_sentence\u001b[0;34m(model_name, sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_sentence\u001b[39m(model_name, sentence):\n\u001b[1;32m      2\u001b[0m         event_types \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAcquisition (A)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClinical Trial (CT)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRegular Dividend (RD)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDividend Cut (DC)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDividend Increase (DI)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGuidance Increase (GI)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew Contract (NC)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReverse Stock Split (RSS)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecial Dividend (SD)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStock Repurchase (SR)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStock Split (SS)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOther/None (O)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m         ]\n\u001b[0;32m----> 9\u001b[0m         extracted_events \u001b[38;5;241m=\u001b[39m \u001b[43mextract_events_zero_shot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_types\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m extracted_events\n",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m, in \u001b[0;36mextract_events_zero_shot\u001b[0;34m(model_name, sentence, event_types)\u001b[0m\n\u001b[1;32m     13\u001b[0m extractor \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzero-shot-classification\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel_name)\n\u001b[1;32m     14\u001b[0m candidate_labels \u001b[38;5;241m=\u001b[39m event_types \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompany\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 16\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mextractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Find the event type with the highest score\u001b[39;00m\n\u001b[1;32m     19\u001b[0m event_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m], key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m label: result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m][result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mindex(label)])\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/pipelines/zero_shot_classification.py:206\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline.__call__\u001b[0;34m(self, sequences, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to understand extra arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/pipelines/base.py:1246\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[0;32m-> 1246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/pipelines/pt_utils.py:269\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 269\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/pipelines/base.py:1161\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1159\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1160\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1161\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1162\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/pipelines/zero_shot_classification.py:229\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline._forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    228\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidate_label\u001b[39m\u001b[38;5;124m\"\u001b[39m: candidate_label,\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m\"\u001b[39m: sequence,\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_last\u001b[39m\u001b[38;5;124m\"\u001b[39m: inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_last\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moutputs,\n\u001b[1;32m    236\u001b[0m }\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_outputs\n",
      "File \u001b[0;32m/local/java/python-ml/22-12-21-python3.9/lib64/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:2051\u001b[0m, in \u001b[0;36mT5ForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2044\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2045\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf no `decoder_input_ids` or `decoder_inputs_embeds` are \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2046\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassed, `input_ids` cannot be `None`. Please pass either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2047\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`input_ids` or `decoder_input_ids` or `decoder_inputs_embeds`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2048\u001b[0m         )\n\u001b[1;32m   2049\u001b[0m     decoder_input_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shift_right(input_ids)\n\u001b[0;32m-> 2051\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2060\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2062\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2063\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2064\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2066\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2067\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   2069\u001b[0m eos_mask \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39meq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39meos_token_id)\u001b[38;5;241m.\u001b[39mto(sequence_output\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/local/java/python-ml/22-12-21-python3.9/lib64/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:1514\u001b[0m, in \u001b[0;36mT5Model.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1511\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1514\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1515\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1517\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m/local/java/python-ml/22-12-21-python3.9/lib64/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:1106\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1091\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1092\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1093\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1103\u001b[0m         output_attentions,\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1106\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m/local/java/python-ml/22-12-21-python3.9/lib64/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:716\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    714\u001b[0m     query_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 716\u001b[0m cross_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    727\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    729\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n",
      "File \u001b[0;32m/local/java/python-ml/22-12-21-python3.9/lib64/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:627\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    616\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    625\u001b[0m ):\n\u001b[1;32m    626\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 627\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEncDecAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    639\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/local/java/python-ml/22-12-21-python3.9/lib64/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:512\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# get query states\u001b[39;00m\n\u001b[0;32m--> 512\u001b[0m query_states \u001b[38;5;241m=\u001b[39m shape(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m# get key/value states\u001b[39;00m\n\u001b[1;32m    515\u001b[0m key_states \u001b[38;5;241m=\u001b[39m project(\n\u001b[1;32m    516\u001b[0m     hidden_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk, key_value_states, past_key_value[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m )\n",
      "File \u001b[0;32m/local/java/python-ml/22-12-21-python3.9/lib64/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/local/java/python-ml/22-12-21-python3.9/lib64/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_copy = list(reversed(data))\n",
    "data_copy = data_copy[:10]\n",
    "results = []\n",
    "\n",
    "for item in data:\n",
    "    sentence = item[\"sentence\"][0]\n",
    "    extracted_events = process_sentence(model_name, sentence)\n",
    "    results.append({\"sentence\": sentence, \"extracted_events\": extracted_events, \"actual_events\": item[\"events\"]})\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class EventType(Enum):\n",
    "    A = \"Acquisition (A)\"\n",
    "    CT = \"Clinical Trial (CT)\"\n",
    "    RD = \"Regular Dividend (RD)\"\n",
    "    DC = \"Dividend Cut (DC)\"\n",
    "    DI = \"Dividend Increase (DI)\"\n",
    "    GI = \"Guidance Increase (GI)\"\n",
    "    NC = \"New Contract (NC)\"\n",
    "    RSS = \"Reverse Stock Split (RSS)\"\n",
    "    SD = \"Special Dividend (SD)\"\n",
    "    SR = \"Stock Repurchase (SR)\"\n",
    "    SS = \"Stock Split (SS)\"\n",
    "    O = \"Other/None (O)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_events(results):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for result in results:\n",
    "        actual_events = result[\"actual_events\"]\n",
    "        extracted_event = result[\"extracted_events\"]\n",
    "\n",
    "        if not actual_events:\n",
    "            actual_events = [EventType.O.value]\n",
    "\n",
    "        for actual, extract in zip(actual_events, extracted_event.values()):\n",
    "            print(f\"Event type: {actual}\")\n",
    "            actual_event_enum = next(\n",
    "                (e for e in EventType if e.value == actual), EventType.O)\n",
    "            y_true.append(actual_event_enum.value)\n",
    "            print(f\"y true : {y_true}\")\n",
    "\n",
    "            extracted_event_enum = next(\n",
    "                (e for e in EventType if e.value == extracted_event[\"event_type\"]), EventType.O)\n",
    "\n",
    "            y_pred.append(extracted_event_enum.value)\n",
    "            print(f\"y pred : {y_pred}\")\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Calculate Exact Match (EM)\n",
    "    exact_matches = sum(1 for yt, yp in zip(y_true, y_pred) if yt == yp)\n",
    "    em_score = exact_matches / len(y_true)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='weighted')\n",
    "\n",
    "    print(f\"Evaluation metrics - Exact Match (EM): {em_score}, F1 Score: {f1}\")\n",
    "    return {\"exact_match\": em_score, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_events(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot Learning\n",
    "### 1. Schema Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google/flan-t5-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized EventExtraction model\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "if \"flan-t5\" in model_name:\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=True)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "elif \"bart\" in model_name:\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "else:\n",
    "    raise ValueError(f\"Model {model_name} not supported\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "zero_shot_classifier = pipeline(\"zero-shot-classification\", model=model_name, device=device)\n",
    "\n",
    "print(\"Initialized EventExtraction model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class EventType(Enum):\n",
    "    A = \"Acquisition (A)\"\n",
    "    CT = \"Clinical Trial (CT)\"\n",
    "    RD = \"Regular Dividend (RD)\"\n",
    "    DC = \"Dividend Cut (DC)\"\n",
    "    DI = \"Dividend Increase (DI)\"\n",
    "    GI = \"Guidance Increase (GI)\"\n",
    "    NC = \"New Contract (NC)\"\n",
    "    RSS = \"Reverse Stock Split (RSS)\"\n",
    "    SD = \"Special Dividend (SD)\"\n",
    "    SR = \"Stock Repurchase (SR)\"\n",
    "    SS = \"Stock Split (SS)\"\n",
    "    O = \"Other/None (O)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema_prompt(sentence):\n",
    "    return f\"\"\"\n",
    "        Extract event information from the following sentence and return events in JSON format as this: {{\"event_type\": event_type, \"company\": company_name}}.\n",
    "\n",
    "        Event_type:\n",
    "        - Acquisition (A)\n",
    "        - Clinical Trial (CT)\n",
    "        - Regular Dividend (RD)\n",
    "        - Dividend Cut (DC)\n",
    "        - Dividend Increase (DI)\n",
    "        - Guidance Increase (GI)\n",
    "        - New Contract (NC)\n",
    "        - Reverse Stock Split (RSS)\n",
    "        - Special Dividend (SD)\n",
    "        - Stock Repurchase (SR)\n",
    "        - Stock Split (SS)\n",
    "        - Other/None (O)\n",
    "\n",
    "        Argument type:\n",
    "        - company\n",
    "\n",
    "        Sentence: \"{sentence}\"\n",
    "\n",
    "        Output:\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(sentence):\n",
    "        print(f\"Processing sentence: {sentence}\")\n",
    "        prompt = get_schema_prompt(sentence)\n",
    "\n",
    "        # Use the zero-shot-classification pipeline\n",
    "        response = get_model_response(prompt)\n",
    "        \n",
    "        print(response)\n",
    "        \n",
    "#         # Extract and format the response\n",
    "#         try:\n",
    "#             extracted_events = json.loads(response)\n",
    "#         except json.JSONDecodeError:\n",
    "#             logging.error(f\"Invalid JSON response: {response}\")\n",
    "#             extracted_events = {\"event_type\": \"Other/None\", \"company\": \"Unknown\"}\n",
    "\n",
    "#         logging.info(f\"Extracted events: {extracted_events}\")\n",
    "#         return extracted_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence: CBTX Inc. Declares Quarterly Dividend and Suspends Repurchase Program. HOUSTON, March 18, 2020 ( ) CBTX, Inc., the bank holding company for CommunityBank of Texas N.A., today announced that its Board of Directors declared a quarterly cash dividend in the amount of $0.10 per share of common stock. The dividend will be payable on April 15, 2020 to shareholders of record as of the close of business on April 1, 2020. In addition, CBTX, Inc. today announced that it has temporarily suspended its share repurchase program in light of the challenges presented by the COVID-19 pandemic and surrounding events. CBTX, Inc. believes that it remains strong and well-capitalized, and the Company may reinstate the share repurchase program in the future. The Company repurchased 240,445 shares of common stock during the first quarter of 2020 for an aggregate purchase price of approximately $5.4 million under its repurchase program.\n",
      "Model response: \"event_type\": event_type, \"company\": \"CommunityBank of Texas N.A.\".\n",
      "\"event_type\": event_type, \"company\": \"CommunityBank of Texas N.A.\".\n",
      "Extracted Events : None\n"
     ]
    }
   ],
   "source": [
    "sentence = \"CBTX Inc. Declares Quarterly Dividend and Suspends Repurchase Program. HOUSTON, March 18, 2020 ( ) CBTX, Inc., the bank holding company for CommunityBank of Texas N.A., today announced that its Board of Directors declared a quarterly cash dividend in the amount of $0.10 per share of common stock. The dividend will be payable on April 15, 2020 to shareholders of record as of the close of business on April 1, 2020. In addition, CBTX, Inc. today announced that it has temporarily suspended its share repurchase program in light of the challenges presented by the COVID-19 pandemic and surrounding events. CBTX, Inc. believes that it remains strong and well-capitalized, and the Company may reinstate the share repurchase program in the future. The Company repurchased 240,445 shares of common stock during the first quarter of 2020 for an aggregate purchase price of approximately $5.4 million under its repurchase program.\"\n",
    "\n",
    "extracted_events = process_sentence(sentence)\n",
    "print(f\"Extracted Events : {extracted_events}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_few_shot_examples():\n",
    "        \"\"\"Returns a few-shot learning prompt with examples.\"\"\"\n",
    "        examples = \"\"\"\n",
    "        Example 1:\n",
    "        Sentence: \"ABC Corp. Acquires XYZ Ltd.\"\n",
    "        Output: {\"event_type\": \"Acquisition (A)\", \"company\": \"ABC Corp.\"}\n",
    "\n",
    "        Example 2:\n",
    "        Sentence: \"DEF Inc. Declares Regular Dividend of $0.50 per share.\"\n",
    "        Output: {\"event_type\": \"Regular Dividend (RD)\", \"company\": \"DEF Inc.\"}\n",
    "\n",
    "        Example 3:\n",
    "        Sentence: \"GHI Ltd. Cuts Dividend to $0.20 per share.\"\n",
    "        Output: {\"event_type\": \"Dividend Cut (DC)\", \"company\": \"GHI Ltd.\"}\n",
    "        \"\"\"\n",
    "        return examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Code Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "Event = {\n",
    "    \"event_type\": str #options: [Acquisition (A), Clinical Trial (CT), Regular Dividend (RD), Dividend Cut (DC), Dividend Increase (DI), Guidance Increase (GI), New Contract (NC), Reverse Stock Split (RSS), Special Dividend (SD), Stock Repurchase (SR), Stock Split (SS), Other/None (O)]\n",
    "    \"company\": str\n",
    "}\n",
    "\n",
    "events: List[Event] = extract events in the sentence: \"CBTX Inc. Declares Quarterly Dividend and Suspends Repurchase Program. HOUSTON, March 18, 2020 ( ) CBTX, Inc., the bank holding company for CommunityBank of Texas N.A., today announced that its Board of Directors declared a quarterly cash dividend in the amount of $0.10 per share of common stock. The dividend will be payable on April 15, 2020 to shareholders of record as of the close of business on April 1, 2020. In addition, CBTX, Inc. today announced that it has temporarily suspended its share repurchase program in light of the challenges presented by the COVID-19 pandemic and surrounding events. CBTX, Inc. believes that it remains strong and well-capitalized, and the Company may reinstate the share repurchase program in the future. The Company repurchased 240,445 shares of common stock during the first quarter of 2020 for an aggregate purchase price of approximately $5.4 million under its repurchase program.\"\n",
    "print(json.dumps(events))\n",
    "\"\"\"\n",
    "\n",
    "codePrompt_response = get_model_response(flan_model_test, flan_tokenizer_test, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "# Task: Extract Events from Text\n",
    "# Input:\n",
    "text = \"CBTX Inc. Declares Quarterly Dividend and Suspends Repurchase Program. HOUSTON, March 18, 2020 ( ) CBTX, Inc., the bank holding company for CommunityBank of Texas N.A., today announced that its Board of Directors declared a quarterly cash dividend in the amount of $0.10 per share of common stock. The dividend will be payable on April 15, 2020 to shareholders of record as of the close of business on April 1, 2020. In addition, CBTX, Inc. today announced that it has temporarily suspended its share repurchase program in light of the challenges presented by the COVID-19 pandemic and surrounding events. CBTX, Inc. believes that it remains strong and well-capitalized, and the Company may reinstate the share repurchase program in the future. The Company repurchased 240,445 shares of common stock during the first quarter of 2020 for an aggregate purchase price of approximately $5.4 million under its repurchase program.\"\n",
    "\n",
    "Instructions:\n",
    "1. dataclass `Event` to represents extracted events:\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class Event:\n",
    "    event_type: str  # Choose from: [A, CT, RD, DC, DI, GI, NC, RSS, SD, SR, SS, O]\n",
    "    company: str\n",
    "\n",
    "2. Function `extract_events` takes the input text and returns a list of Event objects:\n",
    "\n",
    "def extract_events(text: str) -> List[Event]:\n",
    "    # - Identify financial/corporate events in the text.\n",
    "    # - Classify events based on the provided types.\n",
    "    # - Extract the primary company associated with each event.\n",
    "    return events\n",
    "\n",
    "3. Extract events from the provided text and print the result as JSON:\n",
    "\n",
    "events = extract_events(text)\n",
    "import json\n",
    "print(json.dumps(events))\n",
    "\"\"\"\n",
    "\n",
    "codePrompt_response = get_model_response(flan_model_test, flan_tokenizer_test, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(codePrompt_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Explanation Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_prompt=\"\"\"\n",
    "Task: Event Extraction\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. Identify any financial or corporate events within the sentence.\n",
    "2. Classify each identified event using the following types:\n",
    "   - Acquisition (A): A company purchases another company or a significant portion of it.\n",
    "   - Clinical Trial (CT): A company conducts a research study to test new medical treatments or drugs.\n",
    "   - Regular Dividend (RD): A company distributes a portion of its earnings to shareholders regularly.\n",
    "   - Dividend Cut (DC): A company reduces the amount of dividend it pays out to shareholders.\n",
    "   - Dividend Increase (DI): A company increases the amount of dividend it pays out to shareholders.\n",
    "   - Guidance Increase (GI): A company raises its future earnings or revenue forecast.\n",
    "   - New Contract (NC): A company secures a new agreement for providing goods or services.\n",
    "   - Reverse Stock Split (RSS): A company reduces the number of its outstanding shares to increase the share price.\n",
    "   - Special Dividend (SD): A company makes a one-time distribution of additional earnings to shareholders.\n",
    "   - Stock Repurchase (SR): A company buys back its own shares from the marketplace.\n",
    "   - Stock Split (SS): A company increases the number of its outstanding shares by dividing its current shares.\n",
    "   - Other/None (O): Events that do not fit into any of the specified categories.\n",
    "3. Extract the primary corporate entity (company name) directly associated with each event. Use contextual clues and focus on entities performing financial actions. Prioritise the company name. A company associated is the primary corporate entity directly involved in the financial or corporate event being reported.\n",
    "\n",
    "4. Output a JSON array of dictionaries, each containing:\n",
    "    - \"event_type\": [Use the exact event classification code from the provided list, e.g., \"RD\" for Regular Dividend]\n",
    "    - \"company\": [Identify the primary company performing the financial action, often the publicly traded parent company]\n",
    "\n",
    "5. Ensure the output is valid JSON. Double-check for proper formatting, brackets, commas, and quotation marks.\n",
    "\n",
    "## Important: The final output should be a JSON array, not an explanation of the instructions.\n",
    "\n",
    "\n",
    "Example 1:\n",
    "Input: \"Company A announces a regular dividend and a new stock repurchase program.\"\n",
    "Output: [{\"event_type\": \"RD\", \"company\": \"Company A\"}, {\"event_type\": \"SR\", \"company\": \"Company A\"}]\n",
    "\n",
    "Example 2:\n",
    "Input: \"MegaCorp (parent company of Subsidiary B) declares a quarterly cash dividend payable to shareholders of record.\"\n",
    "Output: [{\"event_type\": \"RD\", \"company\": \"MegaCorp\"}]\n",
    "\n",
    "Example 3:\n",
    "Input: \"BigPharma Inc., the parent company of BioTech Labs, announced a successful Phase III clinical trial.\"\n",
    "Output: [{\"event_type\": \"CT\", \"company\": \"BigPharma Inc.\"}]\n",
    "\n",
    "Extract the event from the following sentence:\n",
    "Input: \"CBTX Inc. Declares Quarterly Dividend and Suspends Repurchase Program. HOUSTON, March 18, 2020 ( ) CBTX, Inc., the bank holding company for CommunityBank of Texas N.A., today announced that its Board of Directors declared a quarterly cash dividend in the amount of $0.10 per share of common stock. The dividend will be payable on April 15, 2020 to shareholders of record as of the close of business on April 1, 2020. In addition, CBTX, Inc. today announced that it has temporarily suspended its share repurchase program in light of the challenges presented by the COVID-19 pandemic and surrounding events. CBTX, Inc. believes that it remains strong and well-capitalized, and the Company may reinstate the share repurchase program in the future. The Company repurchased 240,445 shares of common stock during the first quarter of 2020 for an aggregate purchase price of approximately $5.4 million under its repurchase program.\"\n",
    "\"\"\"\n",
    "\n",
    "explanation_response = get_model_response(flan_model_test, flan_tokenizer_test, explanation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(explanation_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Pipeline Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_prompt=\"\"\"\n",
    "\n",
    "Stage 1: \n",
    "Extract financial or corporate events in the sentence, as well as the primary corporate entity (company) involved in the event. Return the output in JSON format as this: [{\"event_type\": event type, \"company\": \"company name\"}].\n",
    "Event type options: Acquisition (A), Clinical Trial (CT), Regular Dividend (RD), Dividend Cut (DC), Dividend Increase (DI), Guidance Increase (GI), New Contract (NC), Reverse Stock Split (RSS), Special Dividend (SD), Stock Repurchase (SR), Stock Split (SS), Other/None (O).\n",
    "\n",
    "Instructions:\n",
    "1. Clearly identify the main events described in the sentence.\n",
    "2. Classify each event using the provided list of event types.\n",
    "3. Extract the company name directly associated with each event, prioritizing the main corporate entity mentioned in the sentence.\n",
    "4. Ensure the output is a valid JSON array with square brackets.\n",
    "\n",
    "Sentence: \"CBTX Inc. Declares Quarterly Dividend and Suspends Repurchase Program. HOUSTON, March 18, 2020 ( ) CBTX, Inc., the bank holding company for CommunityBank of Texas N.A., today announced that its Board of Directors declared a quarterly cash dividend in the amount of $0.10 per share of common stock. The dividend will be payable on April 15, 2020 to shareholders of record as of the close of business on April 1, 2020. In addition, CBTX, Inc. today announced that it has temporarily suspended its share repurchase program in light of the challenges presented by the COVID-19 pandemic and surrounding events. CBTX, Inc. believes that it remains strong and well-capitalized, and the Company may reinstate the share repurchase program in the future. The Company repurchased 240,445 shares of common stock during the first quarter of 2020 for an aggregate purchase price of approximately $5.4 million under its repurchase program.\"\n",
    "\n",
    "Output (Stage 1): \n",
    "\n",
    "Stage 2: Answer the question related to the given sentence and given event information.\n",
    "\n",
    "Instructions:\n",
    "1. Analyze the extracted events from Stage 1.\n",
    "2. Iterate over each event, and for each:\n",
    "    - Determine the corresponding question from the list below based on the event type.\n",
    "    - Find the answer to the question within the provided sentence.\n",
    "    - Extract the exact span of text that answers the question.\n",
    "    - If no answer is found, return \"N/A\".\n",
    "3. Output the answers as a JSON array, in the format: `[{\"event_type\": event type, \"question\": question, \"answer\": answer}]`\n",
    "\n",
    "Questions for each event type:\n",
    "Acquisition (A): What company was acquired?\n",
    "Clinical Trial (CT): What was the result of the clinical trial?\n",
    "Regular Dividend (RD): What is the amount of the regular dividend?\n",
    "Dividend Cut (DC): By how much was the dividend cut?\n",
    "Dividend Increase (DI): By how much was the dividend increased?\n",
    "Guidance Increase (GI): What is the new guidance value?\n",
    "New Contract (NC): What is the nature of the new contract?\n",
    "Reverse Stock Split (RSS): What is the ratio of the reverse stock split?\n",
    "Special Dividend (SD): What is the amount of the special dividend?\n",
    "Stock Repurchase (SR): How many shares were repurchased?\n",
    "Stock Split (SS): What is the ratio of the stock split?\n",
    "\n",
    "Sentence: \"CBTX Inc. Declares Quarterly Dividend and Suspends Repurchase Program. HOUSTON, March 18, 2020 ( ) CBTX, Inc., the bank holding company for CommunityBank of Texas N.A., today announced that its Board of Directors declared a quarterly cash dividend in the amount of $0.10 per share of common stock. The dividend will be payable on April 15, 2020 to shareholders of record as of the close of business on April 1, 2020. In addition, CBTX, Inc. today announced that it has temporarily suspended its share repurchase program in light of the challenges presented by the COVID-19 pandemic and surrounding events. CBTX, Inc. believes that it remains strong and well-capitalized, and the Company may reinstate the share repurchase program in the future. The Company repurchased 240,445 shares of common stock during the first quarter of 2020 for an aggregate purchase price of approximately $5.4 million under its repurchase program.\"\n",
    "Complete Stage 1 and then Stage 2 based on Stage 1's output.\n",
    "\"\"\"\n",
    "\n",
    "pipeline_response = get_model_response(flan_model_test, flan_tokenizer_test, pipeline_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipeline_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6876ab4585d448bbfed26cc98d5d2c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /dcs/pg23/u5579267/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Set your Hugging Face token\n",
    "token = os.environ['HUGGINGFACE_TOKEN']\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059527a12da241bea1dd257081b3adf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b84cd0ceb2417ebb5ecc5b4b3c5198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb87d75a08a40529bdb3f6f1e65a473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9df7d9e13946bb8a66e65f9b249e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8316e41b1297420cad22795171ff93e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6cb2b357e8d4580810c880fb3f4c62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e733ea78c3cf4d9d8459da69b1ff4566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the model and tokenizer\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extract event information from the following sentences and return just the most matching event type. Event Types: Acquisition (A), Clinical Trial (CT), Regular Dividend (RD), Dividend Cut (DC), Dividend Increase (DI), Guidance Increase (GI), New Contract (NC), Reverse Stock Split (RSS), Special Dividend (SD), Stock Repurchase (SR), Stock Split (SS). If there is no event, use \"O\".\n",
      "\n",
      "Sentence: \"JTI report warns of a 'Gathering Storm' in the black market English English Intelligence shows criminals are ready for post-Covid boom GENEVA, Sept. 11, 2020 / / JTI ( Japan Tobacco International ) has published a report, independently verified by Intrinsic Insight Ltd., entitled 'The Gathering Storm', on how the illegal tobacco trade are operating during the Covid-19 global pandemic and preparing to reap the rewards in the economic aftermath that will follow. Law enforcement agencies around the world have welcomed the report, which is based on 63 field studies, conducted across 50 countries including Russia, Canada, Malaysia, and the Philippines where tobacco smugglers currently have a strong presence. JTI intelligence found that the global public health crisis and financial downturn has created the conditions for a 'perfect storm' where organized criminal groups will further exploit public demand for cheap goods, and capitalize on dwindling buying power in the impending global recession, particularly in countries with high tax regimes. The report has provided JTI with a global picture of four emerging trends, consistent with Euromonitor and Europol intelligence: 1. Evidence shows that criminal groups are biding their time in readiness for an anticipated boom in illegal tobacco sales; 2. After initial disruption to the illegal supply chain in Western European markets, organizedcriminals quickly exploited the inconsistent approach to travel and lockdownrulesand found alternative routes from production to distribution, leading to significant seizures of illegal factories or their components in countries such as the Czech Republic, Greece, Ireland, Belgium, and Spain; 3. Changed law enforcement priorities and border restrictions have been mixed in limiting supply and the availability of illegal tobacco: whilst governments and authorities in Far East Asia were quicker to impose restrictions, those in the West failed to act with such precision; 4. Technology has been increasingly deployed throughout the pandemic to enable sales of illegal tobacco to continue where strict lockdowns were put in place by governments throughout Eastern Europe, the Middle East, Africa and Asia Pacific, where WhatsApp and Facebook have provided quick and easy methods of communication between the consumer and criminals. Furthermore, the International Chamber of Commerce predicts that global counterfeit trade will reach $4 trillion by 2022, primarily fueled by e-commerce[1]. According to the World Bank, the global trade in illegal tobacco is already worth an estimated $40-50 billion each year to the criminal groups who produce, manufacture, smuggle, distribute and sell tobacco products on which there is no tax duty paid. The loss of revenue to law-abiding retailers is also significantly felt, as is the impact on consumers who are lured into buying sub-standard products. \"To some consumers illegal tobacco is a victimless crime, which is why we need to inform them not only of the hidden dangers they are consuming, but the wider social consequences of buying from criminal groups\"\n",
      "\n",
      "Sentence: \"JTI report warns of a 'Gathering Storm' in the black market English Intelligence shows criminals are ready for post-Covid boom GENEVA, Sept. 11, 2020 / / JTI ( Japan Tobacco International ) has published a report, independently verified by Intrinsic Insight Ltd., entitled 'The Gathering Storm', on how the illegal tobacco trade are operating during the Covid-19 global pandemic and preparing to reap the rewards in the economic aftermath that will follow. Law enforcement agencies around the world have welcomed the report, which is based on 63 field studies, conducted across 50 countries including Russia, Canada, Malaysia, and the Philippines where tobacco smugglers currently have a strong presence. JTI intelligence found that the global public health crisis and financial downturn has created the conditions for a 'perfect storm' where organized criminal groups will further exploit public demand for cheap goods, and capitalize on dwindling buying power in the impending global recession, particularly in countries with high tax regimes. The report has provided JTI with a global picture of four emerging trends, consistent with Euromonitor and Europol intelligence: 1. Evidence shows that criminal groups are biding their time in readiness for an anticipated boom in illegal tobacco sales; 2. After initial disruption to the illegal supply chain in Western European markets, organizedcriminals quickly exploited the inconsistent approach to travel and lockdownrulesand found alternative routes from production to distribution, leading to significant seizures of illegal factories or their components in countries such as the Czech Republic, Greece, Ireland, Belgium, and Spain; 3. Changed law enforcement priorities and border restrictions have been mixed in limiting supply and the availability of illegal tobacco: whilst governments and authorities in Far East Asia were quicker to impose restrictions, those in\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Extract event information from the following sentences and return just the most matching event type. Event Types: Acquisition (A), Clinical Trial (CT), Regular Dividend (RD), Dividend Cut (DC), Dividend Increase (DI), Guidance Increase (GI), New Contract (NC), Reverse Stock Split (RSS), Special Dividend (SD), Stock Repurchase (SR), Stock Split (SS). If there is no event, use \"O\".\n",
    "\n",
    "Sentence: \"JTI report warns of a \\'Gathering Storm\\' in the black market English English Intelligence shows criminals are ready for post-Covid boom GENEVA , Sept . 11 , 2020 / / JTI ( Japan Tobacco International ) has published a report , independently verified by Intrinsic Insight Ltd. , entitled \\'The Gathering Storm\\' , on how the illegal tobacco trade are operating during the Covid-19 global pandemic and preparing to reap the rewards in the economic aftermath that will follow . Law enforcement agencies around the world have welcomed the report , which is based on 63 field studies , conducted across 50 countries including Russia , Canada , Malaysia , and the Philippines where tobacco smugglers currently have a strong presence . JTI intelligence found that the global public health crisis and financial downturn has created the conditions for a \\'perfect storm\\' where organized criminal groups will further exploit public demand for cheap goods , and capitalize on dwindling buying power in the impending global recession , particularly in countries with high tax regimes . The report has provided JTI with a global picture of four emerging trends , consistent with Euromonitor and Europol intelligence: 1 . Evidence shows that criminal groups are biding their time in readiness for an anticipated boom in illegal tobacco sales; 2 . After initial disruption to the illegal supply chain in Western European markets , organizedcriminals quickly exploited the inconsistent approach to travel and lockdownrulesand found alternative routes from production to distribution , leading to significant seizures of illegal factories or their components in countries such as the Czech Republic , Greece , Ireland , Belgium , and Spain; 3 . Changed law enforcement priorities and border restrictions have been mixed in limiting supply and the availability of illegal tobacco: whilst governments and authorities in Far East Asia were quicker to impose restrictions , those in the West failed to act with such precision; 4 . Technology has been increasingly deployed throughout the pandemic to enable sales of illegal tobacco to continue where strict lockdowns were put in place by governments throughout Eastern Europe , the Middle East , Africa and Asia Pacific , where WhatsApp and Facebook have provided quick and easy methods of communication between the consumer and criminals . Furthermore , the International Chamber of Commerce predicts that global counterfeit trade will reach $4 trillion by 2022 , primarily fueled by e-commerce[1] . According to the World Bank , the global trade in illegal tobacco is already worth an estimated $40-50 billion each year to the criminal groups who produce , manufacture , smuggle , distribute and sell tobacco products on which there is no tax duty paid . The loss of revenue to law-abiding retailers is also significantly felt , as is the impact on consumers who are lured into buying sub-standard products . \"To some consumers illegal tobacco is a victimless crime , which is why we need to inform them not only of the hidden dangers they are consuming , but the wider social consequences of buying from criminal groups\"\n",
    "\"\"\"\n",
    "\n",
    "response = get_model_response(gpt2_model, gpt2_tokenizer, prompt)\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"openlm-research/open_llama_3b_v2\")\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\"openlm-research/open_llama_3b_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"\n",
    "Task: Event Extraction\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. Identify any financial or corporate events within the sentence.\n",
    "2. Classify each identified event using the following types:\n",
    "   - Acquisition (A)\n",
    "   - Clinical Trial (CT)\n",
    "   - Regular Dividend (RD)\n",
    "   - Dividend Cut (DC)\n",
    "   - Dividend Increase (DI)\n",
    "   - Guidance Increase (GI)\n",
    "   - New Contract (NC)\n",
    "   - Reverse Stock Split (RSS)\n",
    "   - Special Dividend (SD)\n",
    "   - Stock Repurchase (SR)\n",
    "   - Stock Split (SS)\n",
    "   - Other/None (O)\n",
    "3. Extract the primary corporate entity (company name) directly associated with each event. Use contextual clues and focus on entities performing financial actions. Prioritise the company name.\n",
    "\n",
    "4. Output a JSON array of dictionaries, each containing:\n",
    "    - \"event_type\": [Use the exact event classification code from the provided list, e.g., \"RD\" for Regular Dividend]\n",
    "    - \"company\": [Identify the primary company performing the financial action, often the publicly traded parent company]\n",
    "\n",
    "5. Ensure the output is valid JSON. Double-check for proper formatting, brackets, commas, and quotation marks.\n",
    "\n",
    "Example 1:\n",
    "Input: \"Company A announces a regular dividend and a new stock repurchase program.\"\n",
    "Output: [{\"event_type\": \"RD\", \"company\": \"Company A\"}, {\"event_type\": \"SR\", \"company\": \"Company A\"}]\n",
    "\n",
    "Example 2:\n",
    "Input: \"MegaCorp (parent company of Subsidiary B) declares a quarterly cash dividend payable to shareholders of record.\"\n",
    "Output: [{\"event_type\": \"RD\", \"company\": \"MegaCorp\"}]\n",
    "\n",
    "Example 3:\n",
    "Input: \"BigPharma Inc., the parent company of BioTech Labs, announced a successful Phase III clinical trial.\"\n",
    "Output: [{\"event_type\": \"CT\", \"company\": \"BigPharma Inc.\"}]\n",
    "\n",
    "Extract the event from the following sentence:\n",
    "Input: \"CBTX Inc. Declares Quarterly Dividend and Suspends Repurchase Program. HOUSTON, March 18, 2020 ( ) CBTX, Inc., the bank holding company for CommunityBank of Texas N.A., today announced that its Board of Directors declared a quarterly cash dividend in the amount of $0.10 per share of common stock. The dividend will be payable on April 15, 2020 to shareholders of record as of the close of business on April 1, 2020. In addition, CBTX, Inc. today announced that it has temporarily suspended its share repurchase program in light of the challenges presented by the COVID-19 pandemic and surrounding events. CBTX, Inc. believes that it remains strong and well-capitalized, and the Company may reinstate the share repurchase program in the future. The Company repurchased 240,445 shares of common stock during the first quarter of 2020 for an aggregate purchase price of approximately $5.4 million under its repurchase program.\"\n",
    "\"\"\"\n",
    "\n",
    "response = get_model_response(llama_model, llama_tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FinGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "config = PeftConfig.from_pretrained(\"FinGPT/fingpt-forecaster_dow30_llama2-7b_lora\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = PeftModel.from_pretrained(base_model, \"FinGPT/fingpt-forecaster_dow30_llama2-7b_lora\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
