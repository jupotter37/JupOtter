{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: Get ITI working on 1) BERT models and 2) LLaMa Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (0.1.99)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "git-lfs is already the newest version (2.9.2-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n",
      "\u001b[33mWARNING: Skipping transformerlens as it is not installed.\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Updated git hooks.\n",
      "Git LFS initialized.\n",
      "fatal: destination path 'llama-7b-hf' already exists and is not an empty directory.\n",
      "fatal: destination path 'llama-13b-hf' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# Very fast installation of LLaMA weights via decapoda-research/llama-7b-hf\n",
    "!pip install sentencepiece\n",
    "!apt install git-lfs\n",
    "!pip uninstall transformerlens\n",
    "!git lfs install\n",
    "!git clone https://huggingface.co/decapoda-research/llama-7b-hf\n",
    "!git clone https://huggingface.co/decapoda-research/llama-13b-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from jaxtyping import Float, Int\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, LlamaForCausalLM\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML, display\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, AutoTokenizer\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "from huggingface_hub import snapshot_download\n",
    "import sentencepiece\n",
    "\n",
    "# Install a modified transformerlens\n",
    "import os\n",
    "import sys\n",
    "tl_path = f\"{os.getcwd()}/TransformerLens\"\n",
    "sys.path.insert(0, tl_path)\n",
    "from transformer_lens.loading_from_pretrained import get_pretrained_model_config\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "from transformer_lens import utils, HookedTransformer, HookedTransformerConfig, ActivationCache\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "def imshow(tensor, renderer=None, **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, **kwargs):\n",
    "    px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ec020c6bf84fac942f3ca1059a2503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LLaMA tokenizer not loaded. Please load manually.\n",
      "Loaded pretrained model llama-7b-hf into HookedTransformer\n",
      "Changing model dtype to torch.float16\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-31): 32 x TransformerBlock(\n",
       "      (ln1): RMSNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): RMSNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "        (hook_rot_k): HookPoint()\n",
       "        (hook_rot_q): HookPoint()\n",
       "      )\n",
       "      (mlp): GatedMLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): RMSNorm(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up model\n",
    "model = LlamaForCausalLM.from_pretrained('llama-7b-hf')\n",
    "tokenizer = LlamaTokenizer.from_pretrained('llama-7b-hf')\n",
    "model = HookedTransformer.from_pretrained(\"llama-7b-hf\", hf_model=model, device='cpu', fold_ln=False, center_writing_weights=False, center_unembed=False)\n",
    "model.to(torch.float16)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3891c2f3e0542158dafc653dac8c62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 42 files:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/root/.cache/huggingface/hub/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### IDK IF ANY OF THESE WORK, SAVING HERE\n",
    "\n",
    "# # from huggingface\n",
    "\n",
    "# from huggingface_hub import snapshot_download\n",
    "# from accelerate import load_checkpoint_and_dispatch, init_empty_weights\n",
    "\n",
    "# checkpoint_location = snapshot_download(\"decapoda-research/llama-7b-hf\")\n",
    "# checkpoint_location\n",
    "\n",
    "# # from huggingface method 2\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    "\n",
    "# # from huggingface method 3\n",
    "\n",
    "# with init_empty_weights():\n",
    "#     model = LlamaForCausalLM.from_pretrained(checkpoint_location)\n",
    "# model = load_checkpoint_and_dispatch(\n",
    "#     model,\n",
    "#     checkpoint_location,\n",
    "#     device_map=\"auto\",\n",
    "#     no_split_module_classes=[\"LlamaDecoderLayer\"],\n",
    "# )\n",
    "\n",
    "# tok = LlamaTokenizer.from_pretrained(checkpoint_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LLaMA tokenizer not loaded. Please load manually.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 22.19 GiB total capacity; 21.39 GiB already allocated; 122.50 MiB free; 21.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m HookedTransformer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mllama-7b\u001b[39;49m\u001b[39m\"\u001b[39;49m, hf_model\u001b[39m=\u001b[39;49mmodel, device\u001b[39m=\u001b[39;49mdevice, fold_ln\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, center_writing_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, center_unembed\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformer_lens/HookedTransformer.py:758\u001b[0m, in \u001b[0;36mHookedTransformer.from_pretrained\u001b[0;34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, move_state_dict_to_device, **model_kwargs)\u001b[0m\n\u001b[1;32m    753\u001b[0m state_dict \u001b[39m=\u001b[39m loading\u001b[39m.\u001b[39mget_pretrained_state_dict(\n\u001b[1;32m    754\u001b[0m     official_model_name, cfg, hf_model\n\u001b[1;32m    755\u001b[0m )\n\u001b[1;32m    757\u001b[0m \u001b[39m# Create the HookedTransformer object\u001b[39;00m\n\u001b[0;32m--> 758\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(cfg, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m    760\u001b[0m model\u001b[39m.\u001b[39mload_and_process_state_dict(\n\u001b[1;32m    761\u001b[0m     state_dict,\n\u001b[1;32m    762\u001b[0m     fold_ln\u001b[39m=\u001b[39mfold_ln,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    766\u001b[0m     move_state_dict_to_device\u001b[39m=\u001b[39mmove_state_dict_to_device,\n\u001b[1;32m    767\u001b[0m )\n\u001b[1;32m    769\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoaded pretrained model \u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m into HookedTransformer\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformer_lens/HookedTransformer.py:161\u001b[0m, in \u001b[0;36mHookedTransformer.__init__\u001b[0;34m(self, cfg, tokenizer, move_to_device)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_weights()\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m move_to_device:\n\u001b[1;32m    158\u001b[0m     \u001b[39m# We load the devices in a pipeline manner - the first device gets the embed and pos_embed layers and the first n_layers // n_devices blocks,\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[39m# the second gets the next n_layers // n_devices blocks ... the last gets the last n_layers // n_devices blocks, the final\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[39m# normalization layer (if it exists) and the unembed layer\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m     HookedTransformer\u001b[39m.\u001b[39;49mmove_model_modules_to_device(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    163\u001b[0m \u001b[39m# Helper variable to store a small (10K-20K) dataset of training data. Empty by default, can be loaded with load_sample_training_dataset\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformer_lens/HookedTransformer.py:679\u001b[0m, in \u001b[0;36mHookedTransformer.move_model_modules_to_device\u001b[0;34m(cls, model)\u001b[0m\n\u001b[1;32m    675\u001b[0m model\u001b[39m.\u001b[39munembed\u001b[39m.\u001b[39mto(\n\u001b[1;32m    676\u001b[0m     devices\u001b[39m.\u001b[39mget_device_for_block_index(model\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mn_layers \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, model\u001b[39m.\u001b[39mcfg)\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m \u001b[39mfor\u001b[39;00m i, block \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(model\u001b[39m.\u001b[39mblocks):\n\u001b[0;32m--> 679\u001b[0m     block\u001b[39m.\u001b[39;49mto(devices\u001b[39m.\u001b[39;49mget_device_for_block_index(i, model\u001b[39m.\u001b[39;49mcfg))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1170\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1167\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1168\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1170\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:822\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    821\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 822\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    824\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    825\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    826\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    827\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    832\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    833\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:845\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    842\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    843\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 845\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    846\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    847\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1168\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1166\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1167\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1168\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 172.00 MiB (GPU 0; 22.19 GiB total capacity; 21.39 GiB already allocated; 122.50 MiB free; 21.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    ")\n",
    "\n",
    "model.tokenizer = tokenizer\n",
    "model.tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "\n",
    "model.generate(\"The capital of Germany is\", max_new_tokens=20, temperature=0)\n",
    "\n",
    "outputs, cache = model.run_with_cache(\"The capital of Australia is Canberra, the country’s largest inland city. It is located in the Australian Capital Territory (ACT), which is a federal territory. The city is located on the banks of the Molonglo River, 280 km south-west of Sydney.\\nThe city was founded in 1913 as a compromise between the six Australian colonies. The city was built to be the capital of the country, and it was named after the Aboriginal word for “meeting place”.\\nThe city is the seat of the Australian Parliament, and the Parliament House is the most important building in the city. The Parliament House is a modern building, which was built in the 1980s. It is a circular building, which is surrounded by a lake.\\nThe city is also the seat of the Australian Government. The city is the home of the High Court of Australia, the Supreme Court of the Australian Capital Territory, and the Australian Def\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einsum\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Goal: define a function to take in hyperparameters Alpha and K, model probe values, and model activations to calculate the new activations\n",
    "# Need to calculate alpha * sigma * theta\n",
    "# theta is the truthful direction given by probe parameter\n",
    "# sigma: standard deviation of head activation along truthful_dir (theta, either mass mean shift or probe weight direction)\n",
    "# adapted from get_interventions_dict in Kenneth Li github\n",
    "\n",
    "def get_act_std(head_activation, truthful_dir): # calculates standard deviations for one head\n",
    "    \"\"\"\n",
    "    head_activation: (batch, d_model,)\n",
    "    # truthful_dir: (d_model, )\n",
    "    \"\"\"\n",
    "    truthful_dir /= torch.norm(truthful_dir, dim=-1, keepdim=True)\n",
    "    proj_act = einops.einsum(head_activation, truthful_dir , \"b d_m, d_m -> b d_m\")\n",
    "    return torch.std(proj_act, dim=0) # (d_m)\n",
    "\n",
    "# truthful direction is difference in mean \n",
    "# returns (*, d_model)\n",
    "def get_mass_mean_dir(all_activations, truth_indices): # \n",
    "    \"\"\"\n",
    "    all_activations: (batch, *, d_model)\n",
    "    truth_indices: (batch, )\n",
    "    \"\"\"\n",
    "    # print(f\"shape of activations is {all_activations.shape}\")\n",
    "    # print(f\"shape of truth_indices is {truth_indices.shape}\")\n",
    "    true_mass_mean = torch.mean(all_activations[truth_indices == 1], dim=0) #(*, d_model)\n",
    "    false_mass_mean = torch.mean(all_activations[truth_indices == 0], dim=0)\n",
    "    # (* d_model)\n",
    "\n",
    "    return (true_mass_mean - false_mass_mean) / (true_mass_mean - false_mass_mean).norm()\n",
    "\n",
    "# truthful direction is probe weight\n",
    "# def get_probe_dirs(probe_list):\n",
    "#     # probe is a list (n_heads len) of LogisticRegression objects\n",
    "#     coefs = []\n",
    "#     for probe in probe_list:\n",
    "#         coefs.append(probe.coef_)\n",
    "        \n",
    "#     return torch.tensor(coefs, dtype=torch.float32, device=device)\n",
    "\n",
    "def get_probe_dir(probe):\n",
    "    probe_weights = torch.tensor(probe.coef_, dtype=torch.float32, device=device).squeeze()\n",
    "    return probe_weights / probe_weights.norm(dim=-1)\n",
    "\n",
    "\n",
    "# calculate the ITI addition (sigma * theta) for one head\n",
    "# uses either MMD or probe\n",
    "def calc_truth_proj(activation, use_MMD=False, use_probe=False, truth_indices=None, probe=None):\n",
    "    '''\n",
    "    activation is (batch, d_m)\n",
    "    '''\n",
    "    if use_MMD: # use mass mean direction -- average difference between true and false classified prompts (only one head)\n",
    "        assert truth_indices is not None\n",
    "        truthful_dir = get_mass_mean_dir(activation, truth_indices)\n",
    "    else: # probe -- just the coefficients of the probe\n",
    "        assert use_probe\n",
    "        assert probe is not None\n",
    "        truthful_dir = get_probe_dir(probe)\n",
    "\n",
    "    # print(f\"Old truthful dir direc is {truthful_dir.shape}\")\n",
    "    truthful_dir /= truthful_dir.norm(dim=-1)\n",
    "    # print(f\"New truthful dir direc is {truthful_dir.shape}\")\n",
    "    act_std = get_act_std(activation, truthful_dir)\n",
    "    \n",
    "    return einops.einsum(act_std, truthful_dir, \"d_m, d_m -> d_m\")\n",
    "\n",
    "def patch_activation_hook_fn(activations, hook: HookPoint, head, old_activations, use_MMD=True, use_probe=False, truth_indices=None, probe=None):\n",
    "    \"\"\"\n",
    "    activations: (batch, n_heads, d_model)\n",
    "    hook: HookPoint\n",
    "    term_to_add: (*, d_model)\n",
    "\n",
    "    A hook that is meant to act on the \"z\" (output) of a given head, and add the \"term_to_add\" on top of it. Only meant to work a certain head. Will broadcast.\n",
    "    \"\"\"\n",
    "    # print(f\"in hook fn, old act shape is {old_activations.shape}\")\n",
    "    term_to_add = calc_truth_proj(old_activations[:,head], use_MMD, use_probe, truth_indices, probe)\n",
    "    # print(f\"v shape is {term_to_add.shape}\")\n",
    "    # print(f\"activations shape is {activations.shape}\")\n",
    "    activations[:,-1,head] += term_to_add\n",
    "\n",
    "# Calculates new_activations for topk and adds temporary hooks\n",
    "def patch_top_activations(model, probe_accuracies, old_activations, topk=20, alpha=20, use_MMD=False, use_probe=False, truth_indices=None, probes=None):\n",
    "    '''\n",
    "    probe_accuracies: (n_layers, n_heads)\n",
    "    old_activations: (batch, n_layers, n_heads, d_model)\n",
    "\n",
    "    if use_probe is True, probes should be list in shape (n_layers, n_heads) filled with probes\n",
    "\n",
    "    Goes into every single activation, and then tells it to add the ITI\n",
    "    '''\n",
    "\n",
    "    # print(f\"old activations shape is {old_activations.shape}\")\n",
    "\n",
    "    top_head_indices = torch.topk(einops.rearrange(probe_accuracies, \"n_l n_h -> (n_l n_h)\"), k=topk).indices # take top k indices\n",
    "    top_head_bools = torch.zeros(size=(probe_accuracies.shape[0] * probe_accuracies.shape[1],)) # set all the ones that aren't top to 0\n",
    "\n",
    "    top_head_bools[top_head_indices] = torch.ones_like(top_head_bools[top_head_indices]) # set all the ones that are top to 1\n",
    "    top_head_bools = einops.rearrange(top_head_bools, \"(n_l n_h) -> n_l n_h\", n_l=model.cfg.n_layers) # rearrange back\n",
    "    \n",
    "    for layer in range(probe_accuracies.shape[0]):\n",
    "        for head in range(probe_accuracies.shape[1]):\n",
    "            if top_head_bools[layer, head] == 1:\n",
    "\n",
    "                if use_probe:\n",
    "                    patch_activation_with_head = partial(patch_activation_hook_fn, head = head, old_activations = old_activations[:, layer], use_MMD=False, use_probe=use_probe, truth_indices=None, probe=probes[layer][head])\n",
    "                else:\n",
    "                    patch_activation_with_head = partial(patch_activation_hook_fn, head = head, old_activations = old_activations[:, layer], use_MMD=use_MMD, use_probe=False, truth_indices=truth_indices, probe=None)\n",
    "                model.add_hook(utils.get_act_name(\"result\", layer), patch_activation_with_head)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
