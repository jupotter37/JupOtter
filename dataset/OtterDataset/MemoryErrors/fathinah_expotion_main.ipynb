{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/fathinah.izzati/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Embeddings shape: torch.Size([1, 2048, 50, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Choose the `slow_r50` model \n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)\n",
    "import json\n",
    "import urllib\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample\n",
    ")\n",
    "# Set to GPU or CPU\n",
    "device = \"cuda\"\n",
    "model = model.eval()\n",
    "model = model.to(device)\n",
    "# json_url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
    "json_filename = \"kinetics_classnames.json\"\n",
    "# try: urllib.URLopener().retrieve(json_url, json_filename)\n",
    "# except: urllib.request.urlretrieve(json_url, json_filename)\n",
    "with open(json_filename, \"r\") as f:\n",
    "    kinetics_classnames = json.load(f)\n",
    "# Create an id to label name mapping\n",
    "kinetics_id_to_classname = {}\n",
    "for k, v in kinetics_classnames.items():\n",
    "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")\n",
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 50\n",
    "\n",
    "\n",
    "# Note that this transform is specific to the slow_R50 model.\n",
    "transform =  ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(num_frames),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(\n",
    "                size=side_size\n",
    "            ),\n",
    "            CenterCropVideo(crop_size=(crop_size, crop_size))\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# The duration of the input clip is also specific to the model.\n",
    "clip_duration = 10\n",
    "print(clip_duration)\n",
    "video_path = '/l/users/fathinah.izzati/Synchformer/output/tnj_test.mp4/segment_001.mp4'\n",
    "# Select the duration of the clip to load by specifying the start and end duration\n",
    "# The start_sec should correspond to where the action occurs in the video\n",
    "start_sec = 0\n",
    "end_sec = start_sec + clip_duration\n",
    "\n",
    "# Initialize an EncodedVideo helper class and load the video\n",
    "video = EncodedVideo.from_path(video_path)\n",
    "\n",
    "# Load the desired clip\n",
    "video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "\n",
    "# Apply a transform to normalize the video input\n",
    "video_data = transform(video_data)\n",
    "\n",
    "# # Move the inputs to the desired device\n",
    "inputs = video_data[\"video\"]\n",
    "inputs = inputs.to(device)\n",
    "# Dictionary to store the embeddings\n",
    "embeddings = {}\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    embeddings['feature'] = output.detach()\n",
    "\n",
    "# Register the hook to the 'blocks[4]' layer\n",
    "target_layer = model.blocks[4]\n",
    "hook = target_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "# Forward pass to trigger the hook\n",
    "with torch.no_grad():\n",
    "    _ = model(inputs[None, ...])\n",
    "\n",
    "hook.remove()\n",
    "\n",
    "# Access the embeddings\n",
    "feature_embeddings = embeddings['feature']\n",
    "print(\"Embeddings shape:\", feature_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/fathinah.izzati/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_014.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_014/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_013.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_013/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_026.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_026/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_021.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_021/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_028.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_028/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_002.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_002/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_005.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_005/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_029.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_029/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_020.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_020/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_027.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_027/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_012.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_012/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_015.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_015/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_004.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_004/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_003.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_003/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_009.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_009/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_007.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_007/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_024.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_024/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_023.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_023/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_016.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_016/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_011.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_011/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_018.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_018/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_006.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_006/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_001.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_001/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_008.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_008/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_019.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_019/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_010.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_010/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_017.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_017/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_022.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_022/embeddings2.npy\n",
      "combined_embedding torch.Size([50, 2048, 8, 8, 8])\n",
      "Saved embeddings for video segment_025.mp4 to /l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_025/embeddings2.npy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample\n",
    ")\n",
    "# ---------------------------- Setup and Configuration ----------------------------\n",
    "\n",
    "# Choose the `slow_r50` model from PyTorch Hub\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=True)\n",
    "\n",
    "# Set device to GPU if available, else CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.eval().to(device)\n",
    "\n",
    "\n",
    "# Define transformation parameters\n",
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 8#20        # Window size: current frame + next 7 frames\n",
    "sampling_rate = 1      # Since we're sampling at 5 fps, a sampling rate of 1 maintains the rate\n",
    "frames_per_second = 5  # Desired frame rate\n",
    "\n",
    "\n",
    "# Define the transformation pipeline specific to the slow_r50 model\n",
    "transform = ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose([\n",
    "        UniformTemporalSubsample(num_frames),  # Ensure the clip has exactly `num_frames`\n",
    "        Lambda(lambda x: x / 255.0),           # Normalize pixel values to [0, 1]\n",
    "        NormalizeVideo(mean, std),             # Normalize using mean and std\n",
    "        ShortSideScale(size=side_size),        # Scale the shorter side to `side_size`\n",
    "        CenterCropVideo(crop_size=(crop_size, crop_size))  # Center crop to `crop_size`\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Path to your input directory containing videos\n",
    "input_directory = '/l/users/fathinah.izzati/Synchformer/output/tnj_test.mp4/'\n",
    "output_directory = '/l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Path to your input video\n",
    "# video_path = '/l/users/fathinah.izzati/datasets/data-videomme-videochunked01/0ag_Qi5OEd0.mp4'\n",
    "\n",
    "\n",
    "# Dictionary to store embeddings from the hook\n",
    "embeddings_dict = {}\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    \"\"\"\n",
    "    Hook function to capture the output of the target layer.\n",
    "    \"\"\"\n",
    "    embeddings_dict['feature'] = output.detach()\n",
    "\n",
    "# Register the hook to the 5th block (index 4) of the model\n",
    "target_layer = model.blocks[4]\n",
    "hook = target_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "# ---------------------------- Embedding Extraction Loop ----------------------------\n",
    "# Iterate over each video file in the input directory\n",
    "for video_file in os.listdir(input_directory):\n",
    "    if video_file.endswith('.mp4'):\n",
    "        video_path = os.path.join(input_directory, video_file)\n",
    "        \n",
    "        # Initialize the EncodedVideo helper class and load the video\n",
    "        video = EncodedVideo.from_path(video_path)\n",
    "        \n",
    "        # Total duration and frame count\n",
    "        total_seconds = 10\n",
    "        total_frames = frames_per_second * total_seconds  # 50 frames for 10 seconds at 5 fps\n",
    "\n",
    "        # List to store all embeddings\n",
    "        embeddings = []\n",
    "\n",
    "        # Iterate over each frame to extract embeddings\n",
    "        for i in range(total_frames):\n",
    "            # Calculate the start and end times for the current window\n",
    "            start_sec = i / frames_per_second\n",
    "            clip_duration = (num_frames * sampling_rate) / frames_per_second  # 20 frames / 5 fps = 4 seconds\n",
    "            end_sec = start_sec + clip_duration\n",
    "\n",
    "            # Ensure end_sec does not exceed the video's total duration\n",
    "            end_sec = min(end_sec, total_seconds)\n",
    "\n",
    "            # Extract the clip from the video\n",
    "            video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "\n",
    "            # Apply the transformation pipeline\n",
    "            video_data = transform(video_data)\n",
    "\n",
    "            # Move the video tensor to the desired device\n",
    "            inputs = video_data[\"video\"].to(device)\n",
    "\n",
    "            # Forward pass through the model to trigger the hook\n",
    "            with torch.no_grad():\n",
    "                _ = model(inputs[None, ...])  # Add batch dimension\n",
    "\n",
    "            # Retrieve the embedding from the hook\n",
    "            embedding = embeddings_dict.get('feature')\n",
    "\n",
    "            if embedding is not None:\n",
    "                # Move embedding to CPU and detach from the computation graph\n",
    "                embeddings.append(embedding.cpu())\n",
    "            else:\n",
    "                print(f\"Warning: No embedding extracted for frame {i} in video {video_file}.\")\n",
    "\n",
    "        # Concatenate all frame embeddings along the temporal dimension\n",
    "        if len(embeddings) > 0:\n",
    "            combined_embeddings = torch.cat(embeddings, dim=0)\n",
    "            print('combined_embedding', combined_embeddings.shape)\n",
    "            \n",
    "            # Save the combined embeddings to a .npy file\n",
    "            output_path = os.path.join(output_directory, f\"{video_file.split('.')[0]}/embeddings2.npy\")\n",
    "            np.save(output_path, combined_embeddings.numpy())\n",
    "            print(f\"Saved embeddings for video {video_file} to {output_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: No embeddings were collected for video {video_file}.\")\n",
    "\n",
    "# Remove the hook after extraction\n",
    "hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the combined 3D embeddings tensor: torch.Size([50, 2048, 512])\n"
     ]
    }
   ],
   "source": [
    "loaded_embeddings = np.load(output_path)\n",
    "combined_embeddings = torch.from_numpy(loaded_embeddings)\n",
    "# Reshape the tensor to have 3 dimensions\n",
    "# The new shape will be [50, 2048, 20 * 8 * 8], resulting in [50, 2048, 1280]\n",
    "combined_embeddings_3d = combined_embeddings.view(50, 2048, -1)\n",
    "\n",
    "# Print the shape of the new 3D tensor\n",
    "print(f\"Shape of the combined 3D embeddings tensor: {combined_embeddings_3d.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining audio+video from inference result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_001.mp4.\n",
      "MoviePy - Writing audio in combined_segment_001TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_001.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_001.mp4\n",
      "Combined video and audio for segment 001 saved to /l/users/fathinah.izzati/combined_output/combined_segment_001.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_002.mp4.\n",
      "MoviePy - Writing audio in combined_segment_002TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_002.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_002.mp4\n",
      "Combined video and audio for segment 002 saved to /l/users/fathinah.izzati/combined_output/combined_segment_002.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_003.mp4.\n",
      "MoviePy - Writing audio in combined_segment_003TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_003.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_003.mp4\n",
      "Combined video and audio for segment 003 saved to /l/users/fathinah.izzati/combined_output/combined_segment_003.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_004.mp4.\n",
      "MoviePy - Writing audio in combined_segment_004TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_004.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_004.mp4\n",
      "Combined video and audio for segment 004 saved to /l/users/fathinah.izzati/combined_output/combined_segment_004.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_005.mp4.\n",
      "MoviePy - Writing audio in combined_segment_005TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_005.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_005.mp4\n",
      "Combined video and audio for segment 005 saved to /l/users/fathinah.izzati/combined_output/combined_segment_005.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_006.mp4.\n",
      "MoviePy - Writing audio in combined_segment_006TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_006.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_006.mp4\n",
      "Combined video and audio for segment 006 saved to /l/users/fathinah.izzati/combined_output/combined_segment_006.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_007.mp4.\n",
      "MoviePy - Writing audio in combined_segment_007TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_007.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_007.mp4\n",
      "Combined video and audio for segment 007 saved to /l/users/fathinah.izzati/combined_output/combined_segment_007.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_008.mp4.\n",
      "MoviePy - Writing audio in combined_segment_008TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_008.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_008.mp4\n",
      "Combined video and audio for segment 008 saved to /l/users/fathinah.izzati/combined_output/combined_segment_008.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_009.mp4.\n",
      "MoviePy - Writing audio in combined_segment_009TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_009.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_009.mp4\n",
      "Combined video and audio for segment 009 saved to /l/users/fathinah.izzati/combined_output/combined_segment_009.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_010.mp4.\n",
      "MoviePy - Writing audio in combined_segment_010TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_010.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_010.mp4\n",
      "Combined video and audio for segment 010 saved to /l/users/fathinah.izzati/combined_output/combined_segment_010.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_011.mp4.\n",
      "MoviePy - Writing audio in combined_segment_011TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_011.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_011.mp4\n",
      "Combined video and audio for segment 011 saved to /l/users/fathinah.izzati/combined_output/combined_segment_011.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_012.mp4.\n",
      "MoviePy - Writing audio in combined_segment_012TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_012.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_012.mp4\n",
      "Combined video and audio for segment 012 saved to /l/users/fathinah.izzati/combined_output/combined_segment_012.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_013.mp4.\n",
      "MoviePy - Writing audio in combined_segment_013TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_013.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_013.mp4\n",
      "Combined video and audio for segment 013 saved to /l/users/fathinah.izzati/combined_output/combined_segment_013.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_014.mp4.\n",
      "MoviePy - Writing audio in combined_segment_014TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_014.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_014.mp4\n",
      "Combined video and audio for segment 014 saved to /l/users/fathinah.izzati/combined_output/combined_segment_014.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_015.mp4.\n",
      "MoviePy - Writing audio in combined_segment_015TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_015.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_015.mp4\n",
      "Combined video and audio for segment 015 saved to /l/users/fathinah.izzati/combined_output/combined_segment_015.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_016.mp4.\n",
      "MoviePy - Writing audio in combined_segment_016TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_016.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_016.mp4\n",
      "Combined video and audio for segment 016 saved to /l/users/fathinah.izzati/combined_output/combined_segment_016.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_017.mp4.\n",
      "MoviePy - Writing audio in combined_segment_017TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_017.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_017.mp4\n",
      "Combined video and audio for segment 017 saved to /l/users/fathinah.izzati/combined_output/combined_segment_017.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_018.mp4.\n",
      "MoviePy - Writing audio in combined_segment_018TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_018.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_018.mp4\n",
      "Combined video and audio for segment 018 saved to /l/users/fathinah.izzati/combined_output/combined_segment_018.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_019.mp4.\n",
      "MoviePy - Writing audio in combined_segment_019TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_019.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_019.mp4\n",
      "Combined video and audio for segment 019 saved to /l/users/fathinah.izzati/combined_output/combined_segment_019.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_020.mp4.\n",
      "MoviePy - Writing audio in combined_segment_020TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_020.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_020.mp4\n",
      "Combined video and audio for segment 020 saved to /l/users/fathinah.izzati/combined_output/combined_segment_020.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_021.mp4.\n",
      "MoviePy - Writing audio in combined_segment_021TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_021.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_021.mp4\n",
      "Combined video and audio for segment 021 saved to /l/users/fathinah.izzati/combined_output/combined_segment_021.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_022.mp4.\n",
      "MoviePy - Writing audio in combined_segment_022TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_022.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_022.mp4\n",
      "Combined video and audio for segment 022 saved to /l/users/fathinah.izzati/combined_output/combined_segment_022.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_023.mp4.\n",
      "MoviePy - Writing audio in combined_segment_023TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_023.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_023.mp4\n",
      "Combined video and audio for segment 023 saved to /l/users/fathinah.izzati/combined_output/combined_segment_023.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_024.mp4.\n",
      "MoviePy - Writing audio in combined_segment_024TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_024.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_024.mp4\n",
      "Combined video and audio for segment 024 saved to /l/users/fathinah.izzati/combined_output/combined_segment_024.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_025.mp4.\n",
      "MoviePy - Writing audio in combined_segment_025TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_025.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_025.mp4\n",
      "Combined video and audio for segment 025 saved to /l/users/fathinah.izzati/combined_output/combined_segment_025.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_026.mp4.\n",
      "MoviePy - Writing audio in combined_segment_026TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_026.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_026.mp4\n",
      "Combined video and audio for segment 026 saved to /l/users/fathinah.izzati/combined_output/combined_segment_026.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_027.mp4.\n",
      "MoviePy - Writing audio in combined_segment_027TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_027.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_027.mp4\n",
      "Combined video and audio for segment 027 saved to /l/users/fathinah.izzati/combined_output/combined_segment_027.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_028.mp4.\n",
      "MoviePy - Writing audio in combined_segment_028TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_028.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_028.mp4\n",
      "Combined video and audio for segment 028 saved to /l/users/fathinah.izzati/combined_output/combined_segment_028.mp4\n",
      "Moviepy - Building video /l/users/fathinah.izzati/combined_output/combined_segment_029.mp4.\n",
      "MoviePy - Writing audio in combined_segment_029TEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /l/users/fathinah.izzati/combined_output/combined_segment_029.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /l/users/fathinah.izzati/combined_output/combined_segment_029.mp4\n",
      "Combined video and audio for segment 029 saved to /l/users/fathinah.izzati/combined_output/combined_segment_029.mp4\n",
      "Audio or video file for segment 030 not found.\n"
     ]
    }
   ],
   "source": [
    "[from moviepy.editor import VideoFileClip, AudioFileClip\n",
    "import os\n",
    "\n",
    "# Define the paths for audio and video directories\n",
    "audio_dir = \"/l/users/fathinah.izzati/coco-mulla-repo/demo/output/expe_tnj_2\"\n",
    "video_dir = \"/l/users/fathinah.izzati/Synchformer/vis\"\n",
    "output_dir = \"/l/users/fathinah.izzati/combined_output\"  # Directory to save combined videos\n",
    "\n",
    "# Create output directory if it does not exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Loop through each segment and combine the audio and video\n",
    "for i in range(1, 31):\n",
    "    segment_num = f\"{i:03d}\"  # Format the segment number to 3 digits\n",
    "    audio_path = os.path.join(audio_dir, f\"segment_{segment_num}\", \"video-only.wav\")\n",
    "    video_path = os.path.join(video_dir, f\"segment_{segment_num}_25fps_256side_16000hz.mp4\")\n",
    "    \n",
    "    # Check if both audio and video files exist\n",
    "    if os.path.exists(audio_path) and os.path.exists(video_path):\n",
    "        # Load video and audio clips\n",
    "        video_clip = VideoFileClip(video_path)\n",
    "        audio_clip = AudioFileClip(audio_path)\n",
    "        \n",
    "        # Set audio of video to the audio clip\n",
    "        video_with_audio = video_clip.set_audio(audio_clip)\n",
    "        \n",
    "        # Define output path\n",
    "        output_path = os.path.join(output_dir, f\"combined_segment_{segment_num}.mp4\")\n",
    "        \n",
    "        # Write the result to a file\n",
    "        video_with_audio.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\", fps=25)\n",
    "        \n",
    "        print(f\"Combined video and audio for segment {segment_num} saved to {output_path}\")\n",
    "    else:\n",
    "        print(f\"Audio or video file for segment {segment_num} not found.\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fathinah.izzati/miniconda3/envs/cocomulla/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/fathinah.izzati/miniconda3/envs/cocomulla/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/l/users/fathinah.izzati/Synchformer/vis/segment_251_25fps_256side_16000hz.wav\n",
      "segment_251_25fps_256side_16000hz\n",
      "torch.Size([4, 252])\n"
     ]
    }
   ],
   "source": [
    "# 4. create the npy of music and put it into training_input. \n",
    "\n",
    "import os\n",
    "\n",
    "path = '/l/users/fathinah.izzati/Synchformer/vis'\n",
    "filenames = [path+'/'+i for i in os.listdir(path) if i.endswith('.wav') ]\n",
    "from coco_mulla.utilities.encodec_utils import extract_rvq\n",
    "import librosa\n",
    "import torch\n",
    "import numpy as np\n",
    "device='cuda'\n",
    "\n",
    "mix_output_path = '/l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_train'\n",
    "sr =16_000\n",
    "for audio_path in filenames[]:\n",
    "    print(audio_path)\n",
    "    name = audio_path.split('/')[-1].split('.')[0]\n",
    "    print(name)\n",
    "    wav, _ = librosa.load(audio_path, sr=sr, mono=True)\n",
    "    wav = torch.from_numpy(wav).to(device)[None, None, ...]\n",
    "    mix_rvq = extract_rvq(wav, sr=sr)\n",
    "    print(mix_rvq.shape)\n",
    "    break\n",
    "    # np.save(mix_output_path+f'/{name}/music.npy', mix_rvq.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/l/users/fathinah.izzati/Synchformer/vis/segment_089_25fps_256side_16000hz.wav\n",
      "segment_089_25fps_256side_16000hz\n",
      "torch.Size([4, 252])\n"
     ]
    }
   ],
   "source": [
    "for audio_path in filenames[3:]:\n",
    "    print(audio_path)\n",
    "    name = audio_path.split('/')[-1].split('.')[0]\n",
    "    print(name)\n",
    "    wav, _ = librosa.load(audio_path, sr=sr, mono=True)\n",
    "    wav = torch.from_numpy(wav).to(device)[None, None, ...]\n",
    "    mix_rvq = extract_rvq(wav, sr=sr)\n",
    "    print(mix_rvq.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 205])\n"
     ]
    }
   ],
   "source": [
    "audio_path = '/l/users/fathinah.izzati/datasets/swimming_125.wav'\n",
    "wav, _ = librosa.load(audio_path, sr=sr, mono=True)\n",
    "wav = torch.from_numpy(wav).to(device)[None, None, ...]\n",
    "mix_rvq = extract_rvq(wav, sr=sr)\n",
    "print(mix_rvq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.625"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "205/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "90/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "252/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base path and file to write\n",
    "base_path = \"/l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/segment_\"\n",
    "output_file = \"test.lst\"\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(output_file, \"w\") as f:\n",
    "    # Loop to generate 500 lines with the required format\n",
    "    for i in range(1, 30):\n",
    "        segment_path = f\"{base_path}{i:03d}\"\n",
    "        line = f\"{segment_path} 0 10\\n\"\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fathinah.izzati/miniconda3/envs/cocomulla/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## 6. fix train.py \n",
    "\n",
    "import argparse\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch.distributed as dist\n",
    "from torch.multiprocessing import spawn\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from coco_mulla.utilities.trainer_utils import Trainer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from config import TrainCfg\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from coco_mulla.data_loader.dataset_sampler import Dataset, collate_fn\n",
    "from coco_mulla.models import CoCoMulla\n",
    "\n",
    "device = \"cuda\"\n",
    "N_GPUS = 1\n",
    "\n",
    "\n",
    "def _get_free_port():\n",
    "    import socketserver\n",
    "    with socketserver.TCPServer(('localhost', 0), None) as s:\n",
    "        return s.server_address[1]\n",
    "\n",
    "\n",
    "\n",
    "def get_dataset(dataset_split, sampling_strategy, sampling_prob):\n",
    "\n",
    "    file_lst = [\"data/text/musdb18_full.lst\",\n",
    "                \"data/text/closed_dataset_fm_full.lst\"]\n",
    "    splits = [\n",
    "        [1],\n",
    "        [0],\n",
    "        [0, 1],\n",
    "    ]\n",
    "    dataset = Dataset(\n",
    "        rid=0, # No distributed rank needed\n",
    "        path_lst=[dataset_split],\n",
    "        sampling_prob=sampling_prob,\n",
    "        sampling_strategy=sampling_strategy,\n",
    "        cfg=TrainCfg)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=TrainCfg.batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        # sampler=DistributedSampler(dataset),\n",
    "        pin_memory=True,\n",
    "        drop_last=True)\n",
    "\n",
    "    return dataset, dataloader\n",
    "\n",
    "\n",
    "def train_dist(replica_id, replica_count, port, model_dir, args):\n",
    "    print('masuk sini')\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = str(port)\n",
    "    torch.distributed.init_process_group('nccl', rank=replica_id, world_size=replica_count)\n",
    "    device = torch.device('cuda', replica_id)\n",
    "    print(device)\n",
    "    torch.cuda.set_device(device)\n",
    "    model = CoCoMulla(TrainCfg.sample_sec, num_layers=args.num_layers, latent_dim=args.latent_dim).to(device)\n",
    "    model.set_training()\n",
    "    model = DDP(model, [replica_id])\n",
    "    dataset, dataloader = get_dataset(rid=replica_id, dataset_split=args.dataset,\n",
    "                                      sampling_strategy=args.sampling_strategy,\n",
    "                                      sampling_prob=[args.sampling_prob_a, args.sampling_prob_b])\n",
    "\n",
    "    # train(replica_id, model, dataset, dataloader, device, model_dir,\n",
    "    #       args.learning_rate)\n",
    "\n",
    "\n",
    "def loss_fn(outputs, y):\n",
    "    prob = outputs.logits\n",
    "    mask = outputs.mask\n",
    "    prob = prob[mask]\n",
    "    y = y[mask]\n",
    "    prob = prob.view(-1, 2048)\n",
    "    return nn.CrossEntropyLoss()(prob, y)\n",
    "\n",
    "\n",
    "def train(model, dataset, dataloader, device, model_dir, learning_rate):\n",
    "    # optimizer and lr scheduler\n",
    "    num_steps = len(dataloader)\n",
    "    epochs = TrainCfg.epoch\n",
    "    rng = np.random.RandomState(569)\n",
    "    writer = SummaryWriter(model_dir, flush_secs=20)\n",
    "\n",
    "    trainer = Trainer(params=model.parameters(), lr=learning_rate, num_epochs=epochs, num_steps=num_steps)\n",
    "\n",
    "    model = model.to(device)\n",
    "    step = 0\n",
    "    for e in range(0, epochs):\n",
    "        mean_loss = 0\n",
    "        n_element = 0\n",
    "        model.train()\n",
    "\n",
    "        dl = tqdm(dataloader, desc=f\"Epoch {e}\")\n",
    "        r = rng.randint(0, 233333)\n",
    "        dataset.reset_random_seed(r, e)\n",
    "        for i, batch in enumerate(dl):\n",
    "            desc = batch[\"desc\"]\n",
    "            music = batch[\"music\"].to(device).long()\n",
    "            video = batch[\"video\"].to(device).long()\n",
    "            cond_mask = batch[\"cond_mask\"].to(device).long()\n",
    "\n",
    "            batch_1 = {\n",
    "                \"music\": music,\n",
    "                \"video\": video,\n",
    "                \"cond_mask\": cond_mask,\n",
    "                \"desc\": desc,\n",
    "\n",
    "            }\n",
    "            # with autocast:\n",
    "            outputs = model(**batch_1)\n",
    "            print(\"==========================================\")\n",
    "            print(outputs)\n",
    "            \n",
    "            r_loss = loss_fn(outputs, music)\n",
    "\n",
    "            grad_1, lr_1 = trainer.step(r_loss, model.parameters())\n",
    "\n",
    "            step += 1\n",
    "            n_element += 1\n",
    "            writer.add_scalar(\"r_loss\", r_loss.item(), step)\n",
    "            writer.add_scalar(\"grad_1\", grad_1, step)\n",
    "            writer.add_scalar(\"lr_1\", lr_1, step)\n",
    "\n",
    "            mean_loss += r_loss.item()\n",
    "\n",
    "        mean_loss = mean_loss / n_element\n",
    "        with torch.no_grad():\n",
    "            writer.add_scalar('train/mean_loss', mean_loss, step)\n",
    "            model.save_weights(os.path.join(model_dir, f\"diff_{e}_end.pth\"))\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    experiment_folder = args.experiment_folder\n",
    "    experiment_name = args.experiment_name\n",
    "\n",
    "    if not os.path.exists(experiment_folder):\n",
    "        os.mkdir(experiment_folder)\n",
    "    model_dir = os.path.join(experiment_folder, experiment_name)\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.mkdir(model_dir)\n",
    "    world_size = N_GPUS\n",
    "    port = _get_free_port()\n",
    "    spawn(train_dist, args=(world_size, port, model_dir, args), nprocs=world_size, join=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 /l/users/fathinah.izzati/coco-mulla-repo/test.lst\n",
      "num of files 29\n",
      "samling strategy prob-based [0.0, 0.8]\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "args = {\n",
    "    \"num_layers\": 48,\n",
    "    \"latent_dim\": 14,\n",
    "    \"experiment_folder\": \"/l/users/fathinah.izzati/coco-mulla-repo/expe\",\n",
    "    \"experiment_name\": \"experiment_tnj_3\",\n",
    "    \"prompt_path\": \"/l/users/fathinah.izzati/coco-mulla-repo/demo/input/tnj/tnj.prompt.txt\",\n",
    "    'sampling_strategy':'prob-based',\n",
    "    \"dataset\": '/l/users/fathinah.izzati/coco-mulla-repo/test.lst',\n",
    "    'learning_rate':0.05\n",
    "\n",
    "}\n",
    "args = SimpleNamespace(**args)\n",
    "\n",
    "experiment_folder = args.experiment_folder\n",
    "experiment_name = args.experiment_name\n",
    "if not os.path.exists(experiment_folder):\n",
    "    os.mkdir(experiment_folder)\n",
    "model_dir = os.path.join(experiment_folder, experiment_name)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "    \n",
    "dataset, dataloader = get_dataset(\n",
    "        dataset_split=args.dataset,\n",
    "        sampling_strategy=args.sampling_strategy,\n",
    "        sampling_prob=None\n",
    "    )\n",
    "# model = CoCoMulla(TrainCfg.sample_sec, num_layers=args.num_layers, latent_dim=args.latent_dim).to(device)\n",
    "# model.set_training()\n",
    "# train(model, dataset, dataloader, device, model_dir, args.learning_rate)\n",
    "\n",
    "\n",
    "### original\n",
    "### music shape\n",
    "### drums before torch.Size([1, 4, 1001])\n",
    "### drums after self.encodec_emb torch.Size([1, 1001, 12])\n",
    "###  chords shape torch.Size([1, 1001, 37])\n",
    "### cond concat on the three torch.Size([1, 1001, 61])\n",
    "### mask embed per layer torch.Size([1, 1001, 61])\n",
    "# ccond_mask torch.Size([1, 1001, 61])\n",
    "# Inside CPTransfoermer forward\n",
    "# torch.Size([1, 4, 1000])\n",
    "\n",
    "\n",
    "## adapter\n",
    "## video video.shape b4 torch.Size([1, 14, 251])\n",
    "## video after self.encodec_emb torch.Size([1, 251, 14])\n",
    "# cond_mask.shape torch.Size([1, 251, 14])\n",
    "# mask embedding per layer torch.Size([1, 251, 14])\n",
    "## music torch.Size([1, 4, 251])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mdataloader\u001b[49m):\n\u001b[1;32m      2\u001b[0m         \u001b[38;5;28mprint\u001b[39m(batch\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      3\u001b[0m         desc \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdesc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dataloader):\n",
    "        print(batch.keys())\n",
    "        desc = batch[\"desc\"]\n",
    "        music = batch[\"music\"]\n",
    "        print('music', music.shape)\n",
    "        video = batch[\"video\"]\n",
    "        print('video', video.shape)\n",
    "        video = batch[\"videmb\"]\n",
    "        print('videmb', video.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fathinah.izzati/miniconda3/envs/cocomulla/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacty of 23.62 GiB of which 8.94 MiB is free. Process 3896909 has 456.00 MiB memory in use. Process 3959383 has 3.96 GiB memory in use. Including non-PyTorch memory, this process has 19.03 GiB memory in use. Of the allocated memory 18.55 GiB is allocated by PyTorch, and 288.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \n\u001b[1;32m     16\u001b[0m device \u001b[38;5;241m=\u001b[39m get_device()\n\u001b[0;32m---> 18\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCoCoMulla\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrainCfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_sec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m## Change this one!!\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m48\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/l/users/fathinah.izzati/coco-mulla-repo/expe/experiment_tnj_2/diff_4_end.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m,)\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/l/users/fathinah.izzati/coco-mulla-repo/coco_mulla/models/model.py:331\u001b[0m, in \u001b[0;36mCoCoMulla.__init__\u001b[0;34m(self, sec, num_layers, latent_dim)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, sec, num_layers, latent_dim):\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 331\u001b[0m     lm \u001b[38;5;241m=\u001b[39m \u001b[43mCondMusicgen\u001b[49m\u001b[43m(\u001b[49m\u001b[43msec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_model \u001b[38;5;241m=\u001b[39m lm\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmusicgen \u001b[38;5;241m=\u001b[39m lm\u001b[38;5;241m.\u001b[39mmusicgen\n",
      "File \u001b[0;32m/l/users/fathinah.izzati/coco-mulla-repo/coco_mulla/models/model.py:18\u001b[0m, in \u001b[0;36mCondMusicgen.__init__\u001b[0;34m(self, sec, device)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, sec, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 18\u001b[0m     mg \u001b[38;5;241m=\u001b[39m \u001b[43mget_musicgen\u001b[49m\u001b[43m(\u001b[49m\u001b[43msec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmusicgen \u001b[38;5;241m=\u001b[39m mg\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm \u001b[38;5;241m=\u001b[39m mg\u001b[38;5;241m.\u001b[39mlm\n",
      "File \u001b[0;32m/l/users/fathinah.izzati/coco-mulla-repo/coco_mulla/models/model.py:8\u001b[0m, in \u001b[0;36mget_musicgen\u001b[0;34m(sec, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_musicgen\u001b[39m(sec, device):\n\u001b[0;32m----> 8\u001b[0m     mg \u001b[38;5;241m=\u001b[39m \u001b[43mMusicGen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlarge\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     mg\u001b[38;5;241m.\u001b[39mset_generation_params(duration\u001b[38;5;241m=\u001b[39msec, extend_stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m250\u001b[39m)\n\u001b[1;32m     10\u001b[0m     mg\u001b[38;5;241m.\u001b[39mlm\u001b[38;5;241m.\u001b[39mhere()\n",
      "File \u001b[0;32m/l/users/fathinah.izzati/coco-mulla-repo/coco_mulla/models/musicgen_cc.py:101\u001b[0m, in \u001b[0;36mMusicGen.get_pretrained\u001b[0;34m(name, device)\u001b[0m\n\u001b[1;32m     99\u001b[0m cache_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMUSICGEN_ROOT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    100\u001b[0m compression_model \u001b[38;5;241m=\u001b[39m load_compression_model(name, device\u001b[38;5;241m=\u001b[39mdevice, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n\u001b[0;32m--> 101\u001b[0m lm \u001b[38;5;241m=\u001b[39m \u001b[43mload_lm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmelody\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    103\u001b[0m     lm\u001b[38;5;241m.\u001b[39mcondition_provider\u001b[38;5;241m.\u001b[39mconditioners[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mself_wav\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmatch_len_on_eval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/l/users/fathinah.izzati/coco-mulla-repo/coco_mulla/models/loaders_cc.py:90\u001b[0m, in \u001b[0;36mload_lm_model\u001b[0;34m(file_or_url_or_id, device, cache_dir)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     cfg\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat16\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 90\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuilders\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_lm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(pkg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_state\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     92\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/l/users/fathinah.izzati/coco-mulla-repo/coco_mulla/models/builders_cc.py:109\u001b[0m, in \u001b[0;36mget_lm_model\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    105\u001b[0m         codebooks_pattern_cfg \u001b[38;5;241m=\u001b[39m omegaconf\u001b[38;5;241m.\u001b[39mOmegaConf\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    106\u001b[0m             {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodeling\u001b[39m\u001b[38;5;124m'\u001b[39m: q_modeling, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelay\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelays\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(n_q))}}\n\u001b[1;32m    107\u001b[0m         )\n\u001b[1;32m    108\u001b[0m     pattern_provider \u001b[38;5;241m=\u001b[39m get_codebooks_pattern_provider(n_q, codebooks_pattern_cfg)\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLMModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpattern_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpattern_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcondition_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcondition_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfuser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcfg_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcfg_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_coef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattribute_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattribute_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(cfg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnexpected LM model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mlm_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/l/users/fathinah.izzati/coco-mulla-repo/coco_mulla/models/lm_cc.py:168\u001b[0m, in \u001b[0;36mLMModel.__init__\u001b[0;34m(self, pattern_provider, condition_provider, fuser, n_q, card, dim, num_heads, hidden_scale, norm, norm_first, emb_lr, bias_proj, weight_init, depthwise_init, zero_bias_init, cfg_dropout, cfg_coef, attribute_dropout, two_step_cfg, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    167\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m get_activation_fn(kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;241m=\u001b[39m \u001b[43mStreamingTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_feedforward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhidden_scale\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_norm: tp\u001b[38;5;241m.\u001b[39mOptional[nn\u001b[38;5;241m.\u001b[39mModule] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m norm_first:\n",
      "File \u001b[0;32m/l/users/fathinah.izzati/coco-mulla-repo/coco_mulla/models/transformer_cc.py:699\u001b[0m, in \u001b[0;36mStreamingTransformer.__init__\u001b[0;34m(self, d_model, num_heads, num_layers, dim_feedforward, dropout, bias_ff, bias_attn, causal, past_context, custom, memory_efficient, attention_as_float32, cross_attention, layer_scale, positional_embedding, max_period, positional_scale, xpos, lr, weight_decay, layer_class, checkpointing, device, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList()\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers):\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 699\u001b[0m         \u001b[43mlayer_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m            \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_feedforward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_feedforward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias_ff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias_ff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcausal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmemory_efficient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_efficient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_as_float32\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_as_float32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointing \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;66;03m# see audiocraft/optim/fsdp.py, magic signal to indicate this requires fixing the\u001b[39;00m\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;66;03m# backward hook inside of FSDP...\u001b[39;00m\n",
      "File \u001b[0;32m/l/users/fathinah.izzati/coco-mulla-repo/coco_mulla/models/transformer_cc.py:527\u001b[0m, in \u001b[0;36mStreamingTransformerLayer.__init__\u001b[0;34m(self, d_model, num_heads, dim_feedforward, dropout, bias_ff, bias_attn, causal, past_context, custom, memory_efficient, attention_as_float32, qk_layer_norm, qk_layer_norm_cross, cross_attention, layer_scale, rope, attention_dropout, kv_repeat, norm, device, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, d_model: \u001b[38;5;28mint\u001b[39m, num_heads: \u001b[38;5;28mint\u001b[39m, dim_feedforward: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m, dropout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m    520\u001b[0m              bias_ff: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, bias_attn: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    521\u001b[0m              past_context: tp\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, custom: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    525\u001b[0m              rope: tp\u001b[38;5;241m.\u001b[39mOptional[RotaryEmbedding] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, attention_dropout: tp\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    526\u001b[0m              kv_repeat: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, norm: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer_norm\u001b[39m\u001b[38;5;124m'\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 527\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_feedforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m     factory_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: device, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m: dtype}\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;66;03m# Redefine self_attn to our streaming multi-head attention\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cocomulla/lib/python3.11/site-packages/torch/nn/modules/transformer.py:553\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.__init__\u001b[0;34m(self, d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, bias, device, dtype)\u001b[0m\n\u001b[1;32m    551\u001b[0m factory_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: device, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m: dtype}\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 553\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m \u001b[43mMultiheadAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# Implementation of Feedforward model\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1 \u001b[38;5;241m=\u001b[39m Linear(d_model, dim_feedforward, bias\u001b[38;5;241m=\u001b[39mbias, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/cocomulla/lib/python3.11/site-packages/torch/nn/modules/activation.py:999\u001b[0m, in \u001b[0;36mMultiheadAttention.__init__\u001b[0;34m(self, embed_dim, num_heads, dropout, bias, add_bias_kv, add_zero_attn, kdim, vdim, batch_first, device, dtype)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124min_proj_weight\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 999\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq_proj_weight\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1001\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk_proj_weight\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacty of 23.62 GiB of which 8.94 MiB is free. Process 3896909 has 456.00 MiB memory in use. Process 3959383 has 3.96 GiB memory in use. Including non-PyTorch memory, this process has 19.03 GiB memory in use. Of the allocated memory 18.55 GiB is allocated by PyTorch, and 288.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# 7. fix inference.py\n",
    "\n",
    "import argparse\n",
    "import librosa\n",
    "\n",
    "from coco_mulla.models import CoCoMulla ## Change this one!!\n",
    "from coco_mulla.utilities import *\n",
    "from coco_mulla.utilities.encodec_utils import save_rvq\n",
    "\n",
    "from coco_mulla.utilities.sep_utils import separate\n",
    "from config import TrainCfg  ##change this one!!\n",
    "import torch.nn.functional as F\n",
    "import numpy as np \n",
    "\n",
    "device = get_device()\n",
    "\n",
    "model = CoCoMulla(TrainCfg.sample_sec,           ## Change this one!!\n",
    "                      num_layers=48,\n",
    "                      latent_dim=14).to(device)\n",
    "model.load_weights(\"/l/users/fathinah.izzati/coco-mulla-repo/expe/experiment_tnj_2/diff_4_end.pth\",)\n",
    "model.eval()\n",
    "def generate(batch):\n",
    "    print(batch)\n",
    "    with torch.no_grad():\n",
    "        gen_tokens = model(**batch)\n",
    "\n",
    "    return gen_tokens\n",
    "\n",
    "\n",
    "def generate_mask(xlen):\n",
    "    names = [\"video-only\"]\n",
    "    mask = torch.ones([1, 1, xlen]).to(device)\n",
    "    # mask[1, 1] = 1\n",
    "    # mask[2, 0] = 1\n",
    "    # mask[3] += 1\n",
    "    return mask, names\n",
    "\n",
    "\n",
    "def load_data(video_path, offset):\n",
    "    sr = TrainCfg.sample_rate\n",
    "    res = TrainCfg.frame_res\n",
    "    sample_sec = TrainCfg.sample_sec\n",
    "    video_rvq = np.load(video_path)\n",
    "    print('video', video_rvq.shape)\n",
    "    drums_rvq = crop(video_rvq[None, ...], \"video_rvq\", sample_sec, res, offset=offset)\n",
    "    video = torch.from_numpy(drums_rvq).to(device).long()\n",
    "    print('video', video.shape)\n",
    "    return video\n",
    "\n",
    "\n",
    "def crop(x, mode, sample_sec, res, offset=0):\n",
    "    xlen = x.shape[-1]\n",
    "    st = offset * res\n",
    "    ed = int((offset + sample_sec) * res) + 1\n",
    "    return x[:, :, st: ed]\n",
    "\n",
    "\n",
    "def save_pred(output_folder, tags, pred):\n",
    "    mkdir(output_folder)\n",
    "    output_list = [os.path.join(output_folder, tag) for tag in tags]\n",
    "    save_rvq(output_list=output_list, tokens=pred)\n",
    "\n",
    "\n",
    "def wrap_batch(video, cond_mask, prompt):\n",
    "    num_samples = len(cond_mask)\n",
    "    video = video.repeat(num_samples, 1, 1)\n",
    "    prompt = [prompt] * num_samples\n",
    "    batch = {\n",
    "        \"music\": None,\n",
    "        \"desc\": prompt,\n",
    "        \"video\": video,\n",
    "        \"num_samples\": num_samples,\n",
    "        \"cond_mask\": cond_mask,\n",
    "        \"mode\": \"inference\",\n",
    "    }\n",
    "    return batch\n",
    "\n",
    "\n",
    "def inference(args):\n",
    "    video = load_data(video_path=args.video_path,\n",
    "                                       offset=args.offset)\n",
    "    cond_mask, names = generate_mask(video.shape[-1])\n",
    "    batch = wrap_batch(video, cond_mask, read_lst(args.prompt_path)[0])\n",
    "    print(batch)\n",
    "    pred = generate(\n",
    "                    batch=batch)\n",
    "    save_pred(output_folder=args.output_folder,\n",
    "              tags=names,\n",
    "              pred=pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment_024\n",
      "video (14, 768)\n",
      "video torch.Size([1, 14, 251])\n",
      "{'music': None, 'desc': ['A realistic and high quality soundtrack and sound effect for the video'], 'video': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0'), 'num_samples': 1, 'cond_mask': tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
      "       device='cuda:0'), 'mode': 'inference'}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "generate() got an unexpected keyword argument 'model_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m      3\u001b[0m args \u001b[39m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmodel_path\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39m/l/users/fathinah.izzati/coco-mulla-repo/expe/experiment_tnj_2/diff_4_end.pth\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnum_layers\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m48\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39moffset\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0\u001b[39m\n\u001b[1;32m     11\u001b[0m     }\n\u001b[1;32m     12\u001b[0m args \u001b[39m=\u001b[39m SimpleNamespace(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39margs)\n\u001b[0;32m---> 13\u001b[0m inference(args)\n",
      "Cell \u001b[0;32mIn[2], line 86\u001b[0m, in \u001b[0;36minference\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     84\u001b[0m batch \u001b[39m=\u001b[39m wrap_batch(video, cond_mask, read_lst(args\u001b[39m.\u001b[39mprompt_path)[\u001b[39m0\u001b[39m])\n\u001b[1;32m     85\u001b[0m \u001b[39mprint\u001b[39m(batch)\n\u001b[0;32m---> 86\u001b[0m pred \u001b[39m=\u001b[39m generate(model_path\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mmodel_path,\n\u001b[1;32m     87\u001b[0m                 batch\u001b[39m=\u001b[39;49mbatch)\n\u001b[1;32m     88\u001b[0m save_pred(output_folder\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39moutput_folder,\n\u001b[1;32m     89\u001b[0m           tags\u001b[39m=\u001b[39mnames,\n\u001b[1;32m     90\u001b[0m           pred\u001b[39m=\u001b[39mpred)\n",
      "\u001b[0;31mTypeError\u001b[0m: generate() got an unexpected keyword argument 'model_path'"
     ]
    }
   ],
   "source": [
    "for i in os.listdir('/l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test'):\n",
    "    print(i)\n",
    "    args = {\n",
    "            # \"model_path\":\"/l/users/fathinah.izzati/coco-mulla-repo/expe/experiment_tnj_2/diff_4_end.pth\",\n",
    "            \"num_layers\": 48,\n",
    "            \"latent_dim\": 14,\n",
    "            \"output_folder\": f\"/l/users/fathinah.izzati/coco-mulla-repo/demo/output/expe_tnj_2/{i}\",\n",
    "            \"prompt_path\": \"/l/users/fathinah.izzati/coco-mulla-repo/demo/input/tnj/tnj.prompt.txt\",\n",
    "            \"video_path\": f\"/l/users/fathinah.izzati/coco-mulla-repo/demo/training_input/tom_and_jerry_test/{i}/video.npy\",\n",
    "            \"offset\": 0\n",
    "        }\n",
    "    args = SimpleNamespace(**args)\n",
    "    inference(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cocomulla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
