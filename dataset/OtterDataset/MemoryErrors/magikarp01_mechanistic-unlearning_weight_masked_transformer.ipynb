{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from transformer_lens import HookedTransformer, ActivationCache\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets\n",
    "import transformers\n",
    "import pickle\n",
    "\n",
    "from tasks import PileTask, OWTTask, InductionTask, GreaterThanTask\n",
    "from tasks.ioi.IOITask import IOITask, IOITask_NPO, IOITask_Uniform\n",
    "from tasks.induction.InductionTask import InductionTask, InductionTask_NPO, InductionTask_Uniform\n",
    "from tasks.facts.SportsTask import SportsTask, SportsTask_NPO, SportsTask_Uniform\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPTNeoXTokenizerFast, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from datasets import load_dataset\n",
    "train_dataset = load_dataset('monology/pile-uncopyrighted', split='train', streaming=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "os.environ['HF_TOKEN'] = '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'\n",
    "model_name = 'google/gemma-2b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    device='cuda',\n",
    "    default_padding_side=\"right\",\n",
    "    fold_ln=False,\n",
    "    fold_value_biases=False,\n",
    "    center_writing_weights=False,\n",
    "    dtype=torch.bfloat16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key not found, will not be able to run evaluations on Sports Trivia Task\n"
     ]
    }
   ],
   "source": [
    "from tasks import PileTask, OWTTask, InductionTask, GreaterThanTask\n",
    "from tasks.ioi.IOITask import IOITask, IOITask_NPO, IOITask_Uniform\n",
    "from tasks.induction.InductionTask import InductionTask, InductionTask_NPO, InductionTask_Uniform\n",
    "from tasks.facts.SportsTask import SportsTask, SportsTask_NPO, SportsTask_Uniform\n",
    "from tasks.facts.SportsTaskAdversarial import adversarial_sports_eval\n",
    "from tasks.facts.SportsTaskSideEffects import run_side_effects_evals\n",
    "\n",
    "\n",
    "train_batch_size = 10\n",
    "eval_batch_size = 50\n",
    "\n",
    "device = \"cuda\"\n",
    "train_loss_type = \"sports\"\n",
    "forget_sport = \"basketball\"\n",
    "maintain_sport = None\n",
    "# val_sport = \"baseball\"\n",
    "\n",
    "\n",
    "sports_1mp = SportsTask(batch_size=train_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"log_1_minus_p\", forget_sport_subset={forget_sport}, is_forget_dataset=True)\n",
    "\n",
    "if maintain_sport is None:\n",
    "    maintain_sports = SportsTask(batch_size=train_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"cross_entropy\", forget_sport_subset={forget_sport}, is_forget_dataset=False)\n",
    "else:\n",
    "    maintain_sports = SportsTask(batch_size=train_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"cross_entropy\", forget_sport_subset={maintain_sport}, is_forget_dataset=True)\n",
    "\n",
    "train_pile = PileTask(batch_size=train_batch_size, tokenizer=tokenizer, device=device, ctx_length=100, shuffle=True, buffer_size=50000)\n",
    "train_tasks = {\"sports_1mp\": (sports_1mp, .2), \"maintain_sports\": (maintain_sports, 1), \"pile\": (train_pile, 1)}\n",
    "\n",
    "# want to eval on other sports\n",
    "forget_sport_eval = SportsTask(batch_size=eval_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"cross_entropy\", forget_sport_subset={forget_sport}, is_forget_dataset=True)\n",
    "test_pile = PileTask(batch_size=eval_batch_size, tokenizer=tokenizer, device=device, ctx_length=100, shuffle=True, buffer_size=50000)\n",
    "\n",
    "induction_eval = InductionTask(batch_size=eval_batch_size, tokenizer=tokenizer, prep_acdcpp=False, seq_len=15, device=device)\n",
    "if maintain_sport is None:\n",
    "    maintain_sports_eval = SportsTask(batch_size=eval_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"cross_entropy\", forget_sport_subset={forget_sport}, is_forget_dataset=False)\n",
    "    eval_tasks = {\"induction\": induction_eval, \"pile\": test_pile, \"forget_sport\": forget_sport_eval, \"maintain_sport\": maintain_sports_eval}\n",
    "else:\n",
    "    maintain_sport_eval = SportsTask(batch_size=eval_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"cross_entropy\", forget_sport_subset={maintain_sport}, is_forget_dataset=True)\n",
    "    val_sport_eval = SportsTask(batch_size=eval_batch_size, tokenizer=tokenizer, device=device, prep_acdcpp=False, criterion=\"cross_entropy\", forget_sport_subset={val_sport}, is_forget_dataset=True)\n",
    "    eval_tasks = {\"induction\": induction_eval, \"pile\": test_pile, \"forget_sport\": forget_sport_eval, \"maintain_sport\": maintain_sport_eval, \"val_sport\": val_sport_eval}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import random\n",
    "\n",
    "def create_random_weight_mask_dicts(model):\n",
    "    # Creates random weight masks for testing\n",
    "    weight_mask_attn_dict = {}\n",
    "    weight_mask_mlp_dict = {}\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        weight_mask_attn_dict[layer] = {}\n",
    "        # Want bool of length n_head, randomly set to True\n",
    "        weight_mask_attn_dict[layer]['W_Q'] = torch.rand(model.cfg.n_heads) < 0.8\n",
    "        weight_mask_attn_dict[layer]['W_K'] = torch.rand(model.cfg.n_heads) < 0.8\n",
    "        weight_mask_attn_dict[layer]['W_V'] = torch.rand(model.cfg.n_heads) < 0.8\n",
    "        weight_mask_attn_dict[layer]['W_O'] = torch.rand(model.cfg.n_heads) < 0.8\n",
    "\n",
    "        # Randomly set to true or false\n",
    "        weight_mask_mlp_dict[layer] = random.randint(0, 1) == 1\n",
    "\n",
    "    return weight_mask_attn_dict, weight_mask_mlp_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Masking Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "def make_partly_differentiable_mask(W, frozen_heads, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    W is Parameter of shape (n_heads, ...). \n",
    "    Returns baseline and frozen (both only 1d arrays of (n_heads,)), \n",
    "    and forward pass should be W_frozen.float() + W_baseline.float() * W \n",
    "    \"\"\"\n",
    "    W_frozen = torch.nn.Parameter(torch.zeros(W.shape[0], dtype=torch.bool), requires_grad=False).to(device)\n",
    "\n",
    "    # unsqueeze to broadcast efficiently, until W_baseline has same shape as W\n",
    "    while len(W_frozen.shape) < len(W.shape):\n",
    "        W_frozen = W_frozen.unsqueeze(-1)\n",
    "    \n",
    "    W_frozen[frozen_heads] = True\n",
    "\n",
    "    W_baseline = (~W_frozen).float()\n",
    "    W_baseline = torch.nn.Parameter(W_baseline, requires_grad=True)\n",
    "    # convert into float\n",
    "    return W_frozen.float(), W_baseline.float()\n",
    "\n",
    "class WeightMaskedTransformer(nn.Module):\n",
    "    def __init__(self, tl_transformer, weight_mask_attn_dict=None, weight_mask_mlp_dict=None, torch_dtype=torch.bfloat16):\n",
    "        \"\"\"\n",
    "        weight_mask_attn_dict: {layer: {\"W_Q\": unfrozen_heads, \"W_K\": unfrozen_heads, \"W_V\": unfrozen_heads, \"W_O\": unfrozen_heads}} (frozen_heads is shape (n_heads,) of bools). If none, train mask over all heads\n",
    "        weight_mask_mlp_dict: {layer: bool}. If none, train mask over all mlps\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.torch_dtype = torch_dtype\n",
    "        # tl_transformer should be a HookedTransformer\n",
    "        self.tl_transformer = tl_transformer\n",
    "        # turn off gradients for tl_transformer\n",
    "        # for param in self.tl_transformer.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        self.weight_mask_attn_dict = weight_mask_attn_dict\n",
    "        self.weight_mask_mlp_dict = weight_mask_mlp_dict\n",
    "        # store weight masks for every component that is unfrozen\n",
    "        \n",
    "        # need to store reference weights so that you can reset W_Q, etc after a forward pass\n",
    "        self.reference_attn_weights = {}\n",
    "        self.reference_mlp_weights = {}\n",
    "\n",
    "        self.attention_masks = {}\n",
    "        self.mlp_masks = {}\n",
    "        for layer in range(tl_transformer.cfg.n_layers):\n",
    "            self.attention_masks[layer] = {}\n",
    "            self.reference_attn_weights[layer] = {}\n",
    "            self.mlp_masks[layer] = {}\n",
    "            self.reference_mlp_weights[layer] = {}\n",
    "            # Attention heads\n",
    "            for component, parameter in [(\"W_Q\", tl_transformer.blocks[layer].attn.W_Q), (\"W_K\", tl_transformer.blocks[layer].attn.W_K), (\"W_V\", tl_transformer.blocks[layer].attn.W_V), (\"W_O\", tl_transformer.blocks[layer].attn.W_O)]:\n",
    "                if self.weight_mask_attn_dict is None:\n",
    "                    unfrozen_heads = list(range(tl_transformer.cfg.n_heads)) # all heads are unfrozen\n",
    "                else:\n",
    "                    unfrozen_heads = self.weight_mask_attn_dict[layer][component]\n",
    "                # make frozen and baseline masks, and also a copy of the original weights\n",
    "\n",
    "                if unfrozen_heads is not None and len(unfrozen_heads) > 0:\n",
    "                    W_frozen, W_baseline = make_partly_differentiable_mask(parameter, unfrozen_heads)\n",
    "                    weight_mask = nn.Parameter(torch.ones_like(parameter).type(torch_dtype), requires_grad=True)\n",
    "                    \n",
    "                    self.attention_masks[layer][component] = (W_frozen, W_baseline, weight_mask)\n",
    "                    self.reference_attn_weights[layer][component] = parameter.clone()\n",
    "\n",
    "            # MLPs\n",
    "\n",
    "            for component, parameter in [(\"W_in\", tl_transformer.blocks[layer].mlp.W_in), (\"W_out\", tl_transformer.blocks[layer].mlp.W_out)]:\n",
    "                if self.weight_mask_mlp_dict is None or self.weight_mask_mlp_dict[layer][component]:\n",
    "                    weight_mask = nn.Parameter(torch.ones_like(parameter).type(torch_dtype), requires_grad=True)\n",
    "\n",
    "                    self.mlp_masks[layer][component] = weight_mask\n",
    "                    self.reference_mlp_weights[layer][component] = parameter.clone()\n",
    "\n",
    "                \n",
    "    def forward(self, *args, **kwargs):\n",
    "        for layer in range(self.tl_transformer.cfg.n_layers):\n",
    "            for component, parameter in [(\"W_Q\", self.tl_transformer.blocks[layer].attn.W_Q), (\"W_K\", self.tl_transformer.blocks[layer].attn.W_K), (\"W_V\", self.tl_transformer.blocks[layer].attn.W_V), (\"W_O\", self.tl_transformer.blocks[layer].attn.W_O)]:\n",
    "                if self.weight_mask_attn_dict is None or component in self.attention_masks[layer]:\n",
    "                    W_frozen, W_baseline, weight_mask = self.attention_masks[layer][component]\n",
    "                    reference_data = self.reference_attn_weights[layer][component]\n",
    "                    mask = W_frozen + W_baseline * weight_mask\n",
    "                    self.tl_transformer.blocks[layer].attn.__dict__['_parameters'][component] = reference_data * mask\n",
    "\n",
    "            for component, parameter in [(\"W_in\", self.tl_transformer.blocks[layer].mlp.W_in), (\"W_out\", self.tl_transformer.blocks[layer].mlp.W_out)]:\n",
    "                if self.weight_mask_mlp_dict is None or self.weight_mask_mlp_dict[layer][component]:\n",
    "                    weight_mask = self.mlp_masks[layer][component]\n",
    "                    reference_data = self.reference_mlp_weights[layer][component]\n",
    "                    self.tl_transformer.blocks[layer].mlp.__dict__['_parameters'][component] = reference_data * weight_mask\n",
    "\n",
    "        return self.tl_transformer(*args, **kwargs)\n",
    "\n",
    "    def generate(self, *args, **kwargs):\n",
    "        return self.tl_transformer.generate(*args, **kwargs)\n",
    "\n",
    "    def regularization_loss(self):\n",
    "        # Compute the L1 sparsity penalty using the masks\n",
    "        loss = 0\n",
    "        for layer in range(self.tl_transformer.cfg.n_layers):\n",
    "            num_comps = 0\n",
    "            comp_loss = 0\n",
    "            for component, parameter in [(\"W_Q\", self.tl_transformer.blocks[layer].attn.W_Q), (\"W_K\", self.tl_transformer.blocks[layer].attn.W_K), (\"W_V\", self.tl_transformer.blocks[layer].attn.W_V), (\"W_O\", self.tl_transformer.blocks[layer].attn.W_O)]:\n",
    "                num_comps += 1\n",
    "                if self.weight_mask_attn_dict is None or component in self.attention_masks[layer]:\n",
    "                    W_frozen, W_baseline, weight_mask = self.attention_masks[layer][component]\n",
    "                    mask = W_frozen + (W_baseline * weight_mask) # 1s for frozen, heads\n",
    "                    # Add (weights away from 1) / (total weights * percent_masks_active)\n",
    "                    comp_loss += torch.sum(torch.abs(mask - 1)) / (mask.numel() * (W_baseline.sum() / W_baseline.numel()) + 1e-5)\n",
    "                    \n",
    "            loss += comp_loss / (num_comps + 1e-5)\n",
    "\n",
    "            for component, parameter in [(\"W_in\", self.tl_transformer.blocks[layer].mlp.W_in), (\"W_out\", self.tl_transformer.blocks[layer].mlp.W_out)]:\n",
    "                num_comps = 0\n",
    "                comp_loss = 0\n",
    "                if self.weight_mask_mlp_dict is None or self.weight_mask_mlp_dict[layer][component]:\n",
    "                    weight_mask = self.mlp_masks[layer][component]\n",
    "                    comp_loss += torch.sum(torch.abs(weight_mask - 1)) / weight_mask.numel()\n",
    "\n",
    "            loss += comp_loss / (num_comps + 1e-5)\n",
    "        loss /= self.tl_transformer.cfg.n_layers\n",
    "        return loss\n",
    "    \n",
    "    def on_step_end(self):\n",
    "        # Clip all the masks\n",
    "\n",
    "        for layer in range(self.tl_transformer.cfg.n_layers):\n",
    "            for component, parameter in [(\"W_Q\", self.tl_transformer.blocks[layer].attn.W_Q), (\"W_K\", self.tl_transformer.blocks[layer].attn.W_K), (\"W_V\", self.tl_transformer.blocks[layer].attn.W_V), (\"W_O\", self.tl_transformer.blocks[layer].attn.W_O)]:\n",
    "                if self.weight_mask_attn_dict is None or component in self.attention_masks[layer]:\n",
    "                    W_frozen, W_baseline, weight_mask = self.attention_masks[layer][component]\n",
    "                    weight_mask.data = torch.clamp(weight_mask.data, 0, 1)\n",
    "\n",
    "            for component, parameter in [(\"W_in\", self.tl_transformer.blocks[layer].mlp.W_in), (\"W_out\", self.tl_transformer.blocks[layer].mlp.W_out)]:\n",
    "                if self.weight_mask_mlp_dict is None or self.weight_mask_mlp_dict[layer][component]:\n",
    "                    weight_mask = self.mlp_masks[layer][component]\n",
    "                    weight_mask.data = torch.clamp(weight_mask.data, 0, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Weight Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_mask_from_ap_graph(model, ap_graph, threshold):\n",
    "    # Attention masks are of form:\n",
    "    # {layer: {\"W_Q\": frozen_heads, \"W_K\": frozen_heads, \"W_V\": frozen_heads, \"W_O\": frozen_heads}}\n",
    "    # TRUE for the heads we want to FREEZE, FALSE for heads we want to MASK over\n",
    "    # MLP masks are of form:\n",
    "    # {layer: bool}\n",
    "\n",
    "    # Localizations are of form:\n",
    "    # {alayer.head_{q,k,v,result}:int, mlayer_{in,out}: int}\n",
    "\n",
    "    weight_mask_attn_dict = {}\n",
    "    weight_mask_mlp_dict = {}\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        weight_mask_attn_dict[layer] = {}\n",
    "        weight_mask_mlp_dict[layer] = {}\n",
    "\n",
    "        if 'a0.0_q' in ap_graph:\n",
    "            weight_mask_attn_dict[layer]['W_Q'] = torch.tensor(\n",
    "                [\n",
    "                    abs(ap_graph[f\"a{layer}.{head}_q\"]) < threshold \n",
    "                    for head in range(model.cfg.n_heads)\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            weight_mask_attn_dict[layer]['W_Q'] = None\n",
    "\n",
    "        if 'a0.0_k' in ap_graph:\n",
    "            weight_mask_attn_dict[layer]['W_K'] = torch.tensor(\n",
    "                [\n",
    "                    abs(ap_graph[f\"a{layer}.{head}_k\"]) < threshold \n",
    "                    for head in range(model.cfg.n_heads)\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            weight_mask_attn_dict[layer]['W_K'] = None\n",
    "        \n",
    "        if 'a0.0_v' in ap_graph:\n",
    "            weight_mask_attn_dict[layer]['W_V'] = torch.tensor(\n",
    "                [\n",
    "                    abs(ap_graph[f\"a{layer}.{head}_v\"]) < threshold \n",
    "                    for head in range(model.cfg.n_heads)\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            weight_mask_attn_dict[layer]['W_V'] = None\n",
    "        \n",
    "        if 'a0.0_result' in ap_graph:\n",
    "            weight_mask_attn_dict[layer]['W_O'] = torch.tensor(\n",
    "                [\n",
    "                    abs(ap_graph[f\"a{layer}.{head}_result\"]) < threshold \n",
    "                    for head in range(model.cfg.n_heads)\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            weight_mask_attn_dict[layer]['W_O'] = None\n",
    "            \n",
    "        if 'm0_in' in ap_graph:\n",
    "            weight_mask_mlp_dict[layer]['W_in'] = abs(ap_graph[f\"m{layer}_in\"]) < threshold\n",
    "        else:\n",
    "            weight_mask_mlp_dict[layer]['W_in'] = None\n",
    "        \n",
    "        if 'm0_out' in ap_graph:\n",
    "            weight_mask_mlp_dict[layer]['W_out'] = abs(ap_graph[f\"m{layer}_out\"]) < threshold\n",
    "        else:\n",
    "            weight_mask_mlp_dict[layer]['W_out'] = None\n",
    "\n",
    "    return weight_mask_attn_dict, weight_mask_mlp_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_from_ct_graph(model, ct_graph, threshold):\n",
    "    # Attention masks are of form:\n",
    "    # {layer: {\"W_Q\": frozen_heads, \"W_K\": frozen_heads, \"W_V\": frozen_heads, \"W_O\": frozen_heads}}\n",
    "    # TRUE for the heads we want to FREEZE, FALSE for heads we want to MASK over\n",
    "    # MLP masks are of form:\n",
    "    # {layer: bool}\n",
    "\n",
    "    # Localizations are of form:\n",
    "    # {alayer.head:int, mlayer: int}\n",
    "\n",
    "    weight_mask_attn_dict = {}\n",
    "    weight_mask_mlp_dict = {}\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        weight_mask_attn_dict[layer] = {}\n",
    "        weight_mask_mlp_dict[layer] = {}\n",
    "\n",
    "        weight_mask_attn_dict[layer]['W_O'] = torch.tensor(\n",
    "            [\n",
    "                abs(ct_graph[f\"a{layer}.{head}\"]) < threshold \n",
    "                for head in range(model.cfg.n_heads)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        weight_mask_mlp_dict[layer]['W_out'] = torch.tensor(\n",
    "            [\n",
    "                abs(ct_graph[f\"m{layer}\"]) < threshold\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    return weight_mask_attn_dict, weight_mask_mlp_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "localization_type = \"ap\"\n",
    "with open(f\"models/{model_name.replace('/', '_')}_sports_{forget_sport}_{localization_type}_graph.pkl\", \"rb\") as f:\n",
    "    ap_graph = pickle.load(f)\n",
    "\n",
    "weight_mask_attn_dict, weight_mask_mlp_dict = get_mask_from_ap_graph(model, ap_graph, 0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mask = WeightMaskedTransformer(\n",
    "    model, \n",
    "    weight_mask_attn_dict=weight_mask_attn_dict, \n",
    "    weight_mask_mlp_dict=weight_mask_mlp_dict\n",
    ")\n",
    "# for n, param in mask.tl_transformer.named_parameters():\n",
    "#     param.requires_grad = False\n",
    "sports_train = SportsTask(batch_size=8, tokenizer=tokenizer)\n",
    "with torch.autocast(device_type=\"cuda\"):\n",
    "    loss = sports_train.get_train_loss(mask, 1)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "\n",
    "print(mask.attention_masks[3]['W_Q'][-1].grad[-2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weight_mask_attn_dict[3]['W_Q']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(mask.attention_masks[3]['W_Q'][-1][-2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maaquib111\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/mechanistic-unlearning/wandb/run-20240509_002033-s75ghnt4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aaquib111/mech-unlearning/runs/s75ghnt4' target=\"_blank\">gemma-2b-basketball</a></strong> to <a href='https://wandb.ai/aaquib111/mech-unlearning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aaquib111/mech-unlearning' target=\"_blank\">https://wandb.ai/aaquib111/mech-unlearning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aaquib111/mech-unlearning/runs/s75ghnt4' target=\"_blank\">https://wandb.ai/aaquib111/mech-unlearning/runs/s75ghnt4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/root/venv/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:39: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  grads = [p.grad for p in parameters if p.grad is not None]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running adversarial evals\n",
      "Running side effects evals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:26<00:00,  7.59it/s]\n",
      " 50%|█████     | 1/2 [01:44<01:44, 104.56s/it]/root/venv/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:39: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  grads = [p.grad for p in parameters if p.grad is not None]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running adversarial evals\n",
      "Running side effects evals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:26<00:00,  7.42it/s]\n",
      "100%|██████████| 2/2 [03:18<00:00, 99.28s/it] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_loss_forget_sport</td><td>▁█</td></tr><tr><td>test_loss_induction</td><td>█▁</td></tr><tr><td>test_loss_maintain_sport</td><td>▁█</td></tr><tr><td>test_loss_pile</td><td>█▁</td></tr><tr><td>train_loss_maintain_sports</td><td>█▁</td></tr><tr><td>train_loss_pile</td><td>█▁</td></tr><tr><td>train_loss_reg</td><td>█▁</td></tr><tr><td>train_loss_sports_1mp</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_loss_forget_sport</td><td>9.16551</td></tr><tr><td>test_loss_induction</td><td>0.37922</td></tr><tr><td>test_loss_maintain_sport</td><td>0.09509</td></tr><tr><td>test_loss_pile</td><td>2.74861</td></tr><tr><td>train_loss_maintain_sports</td><td>0.04276</td></tr><tr><td>train_loss_pile</td><td>2.6149</td></tr><tr><td>train_loss_reg</td><td>-18.48189</td></tr><tr><td>train_loss_sports_1mp</td><td>0.06194</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gemma-2b-basketball</strong> at: <a href='https://wandb.ai/aaquib111/mech-unlearning/runs/s75ghnt4' target=\"_blank\">https://wandb.ai/aaquib111/mech-unlearning/runs/s75ghnt4</a><br/> View project at: <a href='https://wandb.ai/aaquib111/mech-unlearning' target=\"_blank\">https://wandb.ai/aaquib111/mech-unlearning</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240509_002033-s75ghnt4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "import wandb\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "mask = WeightMaskedTransformer(\n",
    "    model, \n",
    "    weight_mask_attn_dict=weight_mask_attn_dict, \n",
    "    weight_mask_mlp_dict=weight_mask_mlp_dict\n",
    ")\n",
    "for param in mask.tl_transformer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_type = 'gemma'\n",
    "learning_rate = 1e-2\n",
    "n_epochs = 2\n",
    "grad_accum_steps = 5\n",
    "# max_gpu_batch_size=8\n",
    "alpha = 0.2\n",
    "beta = 1/100\n",
    "clip_grad = 1\n",
    "\n",
    "evaluate_every = 5\n",
    "n_eval_iters = 5\n",
    "do_adversarial_evals = True\n",
    "do_side_effects_evals = True \n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"mech-unlearning\",\n",
    "    name=f\"{model_name.split('/')[-1]}-{forget_sport}\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"model_type\": model_type,\n",
    "        \"model_name\": model_name,\n",
    "        \"forget_sport\": forget_sport,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"grad_accum_steps\": grad_accum_steps,\n",
    "        \"alpha\": alpha,\n",
    "        \"beta\": beta,\n",
    "        \"clip_grad\": clip_grad,\n",
    "        \"evaluate_every\": evaluate_every,\n",
    "        \"n_eval_iters\": n_eval_iters,\n",
    "        \"do_adversarial_evals\": do_adversarial_evals,\n",
    "        \"do_side_effects_evals\": do_side_effects_evals,\n",
    "        \"train_task_weights\": {k:v[1] for k, v in train_tasks.items()}\n",
    "    }\n",
    ")\n",
    "\n",
    "from collections import defaultdict\n",
    "all_train_losses = defaultdict(list)\n",
    "all_test_losses = defaultdict(list)\n",
    "adversarial_evals = []\n",
    "side_effect_evals = []\n",
    "\n",
    "# Initialize optimizer\n",
    "mask = mask.cuda()\n",
    "mask_params = [\n",
    "    v[-1]\n",
    "    for layer, layer_mask_weights in mask.attention_masks.items()\n",
    "    for k, v in layer_mask_weights.items()\n",
    "] + \\\n",
    "[\n",
    "    v\n",
    "    for layer, layer_mask_weights in mask.mlp_masks.items()\n",
    "    for k, v in layer_mask_weights.items()\n",
    "]\n",
    "optimizer = torch.optim.AdamW(mask_params, lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=n_epochs)\n",
    "# Cycle dataloaders\n",
    "# Train a sparse mask\n",
    "pbar = tqdm(range(n_epochs))\n",
    "for epoch in pbar:\n",
    "    # Sample batches\n",
    "    # Reset grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    with torch.autocast(device_type=\"cuda\"):\n",
    "        # Compute normal loss over retain\n",
    "        for task_name, (task, task_weight) in train_tasks.items():\n",
    "            task_loss = 0\n",
    "            for i in range(grad_accum_steps):\n",
    "                loss = task.get_train_loss(mask) / grad_accum_steps\n",
    "                task_loss += loss.item()\n",
    "                loss *= task_weight\n",
    "                # print(task_name, i, loss)\n",
    "                loss.backward()\n",
    "\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "            all_train_losses[task_name].append(task_loss)\n",
    "\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        # Add sparsity loss and backprop\n",
    "        # Linearly increase from negative to positive, with 0 at 10\n",
    "        loss = min(beta * (epoch-10), beta) * mask.regularization_loss()\n",
    "        loss.backward()\n",
    "        # print(f\"reg loss, {loss.item()}\")\n",
    "        all_train_losses[\"reg\"].append(loss.item())\n",
    "        # Step and log\n",
    "        if clip_grad is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(mask.parameters(), clip_grad)\n",
    "        # zero_nan_grads(mask)\n",
    "        optimizer.step()\n",
    "        mask.on_step_end()\n",
    "        scheduler.step()\n",
    "\n",
    "        # print(mask.attention_masks[3]['W_Q'][-1].grad[4])\n",
    "        # print(mask.attention_masks[3]['W_Q'][-1])\n",
    "        # print((mask.attention_masks[3]['W_Q'][-1] - 1).sum())\n",
    "\n",
    "        if epoch % evaluate_every == 0 or epoch == n_epochs - 1:\n",
    "            for task_name, task in eval_tasks.items():\n",
    "                task_loss = 0\n",
    "                for i in range(n_eval_iters):\n",
    "                    task_loss += task.get_test_loss(mask).item()\n",
    "                all_test_losses[task_name].append(task_loss / n_eval_iters)\n",
    "            if do_adversarial_evals:\n",
    "                print(\"Running adversarial evals\")\n",
    "                adversarial_evals.append(adversarial_sports_eval(mask, model_type=model_type, batch_size=eval_batch_size, use_system_prompt=True))\n",
    "            if do_side_effects_evals:\n",
    "                print(\"Running side effects evals\")\n",
    "                side_effect_evals.append(run_side_effects_evals(mask, model_type=model_type, batch_size=eval_batch_size, evals_to_run=[\"Sports Answers\"]))\n",
    "        \n",
    "        log_dict = {\n",
    "        }\n",
    "        for k, v in all_train_losses.items():\n",
    "            log_dict[f\"train_loss_{k}\"] = v[-1]\n",
    "        for k, v in all_test_losses.items():\n",
    "            log_dict[f\"test_loss_{k}\"] = v[-1]\n",
    "        for k, v in adversarial_evals[-1].items():\n",
    "            log_dict[f\"adversarial_{k}\"] = v\n",
    "        for k, v in side_effect_evals[-1].items():\n",
    "            log_dict[f\"side_effects_{k}\"] = v\n",
    "        wandb.log(log_dict)\n",
    "    \n",
    "wandb.finish()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt: adversarial evals are {'Normal': {'football': 0.9972548604011535, 'baseball': 0.989544904232025, 'basketball': 4.2576355241408236e-08}, 'MC': {'football': 0.28016403317451477, 'baseball': 0.21671688854694368, 'basketball': 0.5323783993721007}, 'Capitalized': {'football': 0.9958770394325257, 'baseball': 0.9876289129257202, 'basketball': 4.089648939498147e-07}, 'Dashed': {'football': 0.9511160492897034, 'baseball': 0.13082387149333952, 'basketball': 0.08908162564039232}}\n",
      "No System Prompt: adversarial evals are {'Normal': {'football': 0.9761888384819031, 'baseball': 0.9950854659080505, 'basketball': 4.590647023405836e-07}, 'MC': {'football': 0.21360788941383363, 'baseball': 0.22751041352748874, 'basketball': 0.5542259573936463}, 'Capitalized': {'football': 0.9895583033561707, 'baseball': 0.9975607156753541, 'basketball': 2.857764548025443e-06}, 'Dashed': {'football': 0.9419174432754518, 'baseball': 0.25516764521598817, 'basketball': 0.2764859080314636}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:27<00:00,  7.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sports Answers:\n",
      "football: 50/50\n",
      "baseball: 3/50\n",
      "Processing questions 0 to 50 of 200\n",
      "Falling back to custom generation due to exception: HookedTransformer.generate() got an unexpected keyword argument 'input_ids'\n",
      "Running model as a model inference function instead of a huggingface model.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacity of 44.48 GiB of which 1.93 GiB is free. Process 2958008 has 42.54 GiB memory in use. Of the allocated memory 36.45 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/mechanistic-unlearning/tasks/facts/SportsTaskSideEffects.py:105\u001b[0m, in \u001b[0;36mSportsFamiliarity.generate_sentence\u001b[0;34m(self, model, tokenizer, strs, with_logprobs, max_new_tokens, top_tokens, show_token_strs, do_sample, temperature, include_input, device)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    106\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtokenized_inputs,\n\u001b[1;32m    107\u001b[0m         max_length\u001b[39m=\u001b[39;49mtokenized_inputs[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m] \u001b[39m+\u001b[39;49m max_new_tokens,\n\u001b[1;32m    108\u001b[0m         do_sample\u001b[39m=\u001b[39;49mdo_sample,\n\u001b[1;32m    109\u001b[0m         temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m    110\u001b[0m         top_k\u001b[39m=\u001b[39;49mtop_tokens,\n\u001b[1;32m    111\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    112\u001b[0m         output_scores\u001b[39m=\u001b[39;49mwith_logprobs,\n\u001b[1;32m    113\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m    114\u001b[0m     )\n\u001b[1;32m    115\u001b[0m     sequences \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39msequences\n",
      "\u001b[1;32m/root/mechanistic-unlearning/weight_masked_transformer.ipynb Cell 19\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/mechanistic-unlearning/weight_masked_transformer.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/mechanistic-unlearning/weight_masked_transformer.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtl_transformer\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: HookedTransformer.generate() got an unexpected keyword argument 'input_ids'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/root/mechanistic-unlearning/weight_masked_transformer.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/mechanistic-unlearning/weight_masked_transformer.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m final_adversarial_eval \u001b[39m=\u001b[39m adversarial_sports_eval(m, model_type\u001b[39m=\u001b[39mmodel_type, batch_size\u001b[39m=\u001b[39meval_batch_size, use_system_prompt\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/mechanistic-unlearning/weight_masked_transformer.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo System Prompt: adversarial evals are \u001b[39m\u001b[39m{\u001b[39;00mfinal_adversarial_eval\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/mechanistic-unlearning/weight_masked_transformer.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m final_side_effects \u001b[39m=\u001b[39m run_side_effects_evals(m, model_type\u001b[39m=\u001b[39;49mmodel_type, batch_size\u001b[39m=\u001b[39;49meval_batch_size, evals_to_run\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mSports Answers\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mSports Familiarity\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mCross Entropy\u001b[39;49m\u001b[39m\"\u001b[39;49m], verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/mechanistic-unlearning/weight_masked_transformer.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(final_side_effects)\n",
      "File \u001b[0;32m~/mechanistic-unlearning/tasks/facts/SportsTaskSideEffects.py:488\u001b[0m, in \u001b[0;36mrun_side_effects_evals\u001b[0;34m(model, evals_to_run, model_type, use_short, eval_model, batch_size, verbose)\u001b[0m\n\u001b[1;32m    486\u001b[0m     dataset_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtasks/facts/data/side_effect_sport_trivia.json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m sports_trivia_familiarity \u001b[39m=\u001b[39m SportsFamiliarity(dataset_path\u001b[39m=\u001b[39mdataset_path)\n\u001b[0;32m--> 488\u001b[0m sports_trivia_familiarity\u001b[39m.\u001b[39;49mgenerate_responses(model, left_tokenizer, save_path\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, eval_onthe_fly\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, do_sample\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, batch_size\u001b[39m=\u001b[39;49mbatch_size, prompt_format\u001b[39m=\u001b[39;49mFORMAT_GENERATION_INPUT)\n\u001b[1;32m    489\u001b[0m sports_trivia_familiarity\u001b[39m.\u001b[39mrun_model_evals(eval_model\u001b[39m=\u001b[39meval_model, max_eval_tokens\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, save_path\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, batch_size\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[1;32m    490\u001b[0m familiarity, responses \u001b[39m=\u001b[39m sports_trivia_familiarity\u001b[39m.\u001b[39mget_accuracies(split_by_sport\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/mechanistic-unlearning/tasks/facts/SportsTaskSideEffects.py:238\u001b[0m, in \u001b[0;36mSportsFamiliarity.generate_responses\u001b[0;34m(self, model, tokenizer, save_path, eval_onthe_fly, eval_model, batch_size, n_questions, verbose, max_new_tokens, max_eval_tokens, prompt_format, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     strs \u001b[39m=\u001b[39m prompts_batch\n\u001b[0;32m--> 238\u001b[0m responses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_sentence(\n\u001b[1;32m    239\u001b[0m     strs\u001b[39m=\u001b[39;49mstrs,\n\u001b[1;32m    240\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    241\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m    242\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49mmax_new_tokens,\n\u001b[1;32m    243\u001b[0m     include_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    244\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    245\u001b[0m )\n\u001b[1;32m    247\u001b[0m \u001b[39m# Assuming generate_sentence returns a list of responses corresponding to prompts_batch\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[39mfor\u001b[39;00m j, response \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(responses):\n",
      "File \u001b[0;32m~/mechanistic-unlearning/tasks/facts/SportsTaskSideEffects.py:122\u001b[0m, in \u001b[0;36mSportsFamiliarity.generate_sentence\u001b[0;34m(self, model, tokenizer, strs, with_logprobs, max_new_tokens, top_tokens, show_token_strs, do_sample, temperature, include_input, device)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    120\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFalling back to custom generation due to exception: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mRunning model as a model inference function instead of a huggingface model.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 122\u001b[0m     custom_output \u001b[39m=\u001b[39m custom_generate(\n\u001b[1;32m    123\u001b[0m         model_inference_fn\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    124\u001b[0m         \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mtokenized_inputs[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    125\u001b[0m         num_new_tokens\u001b[39m=\u001b[39;49mmax_new_tokens,\n\u001b[1;32m    126\u001b[0m         do_sample\u001b[39m=\u001b[39;49mdo_sample,\n\u001b[1;32m    127\u001b[0m         temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m    128\u001b[0m         stop_tokens\u001b[39m=\u001b[39;49m[tokenizer\u001b[39m.\u001b[39;49meos_token_id],\n\u001b[1;32m    129\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m  \u001b[39m# Set to True for progress bar\u001b[39;49;00m\n\u001b[1;32m    130\u001b[0m     )\n\u001b[1;32m    131\u001b[0m     sequences \u001b[39m=\u001b[39m custom_output[\u001b[39m\"\u001b[39m\u001b[39msequences\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    132\u001b[0m     scores \u001b[39m=\u001b[39m custom_output[\u001b[39m\"\u001b[39m\u001b[39mscores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/mechanistic-unlearning/tasks/inference_utils.py:188\u001b[0m, in \u001b[0;36mcustom_generate\u001b[0;34m(model_inference_fn, input, num_new_tokens, do_sample, temperature, stop_tokens, verbose)\u001b[0m\n\u001b[1;32m    185\u001b[0m token_iter \u001b[39m=\u001b[39m tqdm(\u001b[39mrange\u001b[39m(num_new_tokens)) \u001b[39mif\u001b[39;00m verbose \u001b[39melse\u001b[39;00m \u001b[39mrange\u001b[39m(num_new_tokens)\n\u001b[1;32m    186\u001b[0m \u001b[39mfor\u001b[39;00m token_num \u001b[39min\u001b[39;00m token_iter:\n\u001b[1;32m    187\u001b[0m     \u001b[39m# Get logits using model's inference function\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     logits \u001b[39m=\u001b[39m model_inference_fn(sequences)\n\u001b[1;32m    189\u001b[0m     logits \u001b[39m=\u001b[39m process_model_output(logits)\n\u001b[1;32m    191\u001b[0m     \u001b[39m# Sample a new token for each sequence in the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/root/mechanistic-unlearning/weight_masked_transformer.ipynb Cell 19\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/mechanistic-unlearning/weight_masked_transformer.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=88'>89</a>\u001b[0m             reference_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreference_mlp_weights[layer][component]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/mechanistic-unlearning/weight_masked_transformer.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=89'>90</a>\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtl_transformer\u001b[39m.\u001b[39mblocks[layer]\u001b[39m.\u001b[39mmlp\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m][component] \u001b[39m=\u001b[39m reference_data \u001b[39m*\u001b[39m weight_mask\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/mechanistic-unlearning/weight_masked_transformer.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtl_transformer(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:568\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 568\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munembed(residual)  \u001b[39m# [batch, pos, d_vocab]\u001b[39;00m\n\u001b[1;32m    569\u001b[0m     \u001b[39mif\u001b[39;00m return_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    570\u001b[0m         \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.10/site-packages/transformer_lens/components.py:81\u001b[0m, in \u001b[0;36mUnembed.forward\u001b[0;34m(self, residual)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     78\u001b[0m     \u001b[39mself\u001b[39m, residual: Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     79\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_vocab_out\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m---> 81\u001b[0m         einsum(\n\u001b[1;32m     82\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mbatch pos d_model, d_model vocab -> batch pos vocab\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     83\u001b[0m             residual,\n\u001b[1;32m     84\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW_U,\n\u001b[1;32m     85\u001b[0m         )\n\u001b[1;32m     86\u001b[0m         \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mb_U\n\u001b[1;32m     87\u001b[0m     )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.53 GiB. GPU 0 has a total capacity of 44.48 GiB of which 1.93 GiB is free. Process 2958008 has 42.54 GiB memory in use. Of the allocated memory 36.45 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Final evals\n",
    "\n",
    "with torch.autocast(device_type=\"cuda\"):\n",
    "    with torch.set_grad_enabled(False):\n",
    "        final_adversarial_eval = adversarial_sports_eval(m, model_type=model_type, batch_size=eval_batch_size, use_system_prompt=True)\n",
    "        print(f\"System Prompt: adversarial evals are {final_adversarial_eval}\")\n",
    "        final_adversarial_eval = adversarial_sports_eval(m, model_type=model_type, batch_size=eval_batch_size, use_system_prompt=False)\n",
    "        print(f\"No System Prompt: adversarial evals are {final_adversarial_eval}\")\n",
    "\n",
    "        final_side_effects = run_side_effects_evals(m, model_type=model_type, batch_size=eval_batch_size, evals_to_run=[\"Sports Answers\", \"Sports Familiarity\", \"Cross Entropy\"], verbose=True)\n",
    "        print(final_side_effects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/mechanistic-unlearning/weight_masked_transformer.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bvast/root/mechanistic-unlearning/weight_masked_transformer.ipynb#X53sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m1\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m0\u001b[39;49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "1/0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = WeightMaskedTransformer(model)\n",
    "loaded_state_dict = torch.load(f\"models/masks/{model_name.replace('/', '_')}-{forget_sport}.pt\")\n",
    "m.load_state_dict(\n",
    "    loaded_state_dict\n",
    ")\n",
    "m.attention_masks = loaded_state_dict['attention_masks']\n",
    "m.mlp_masks = loaded_state_dict['mlp_masks']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save masks state dict to neuron_cb\n",
    "state_dict = mask.state_dict()\n",
    "state_dict['attention_masks'] = mask.attention_masks\n",
    "state_dict['mlp_masks'] = mask.mlp_masks\n",
    "torch.save(mask.mlp_masks, f\"models/masks/{model_name.replace('/', '_')}-{forget_sport}-{localization_type}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'W_out': Parameter containing:\n",
       "  tensor([[0.9844, 1.0000, 0.9883,  ..., 1.0000, 0.9844, 1.0000],\n",
       "          [0.9961, 0.9844, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.9844, 1.0000, 0.9844,  ..., 0.9844, 0.9844, 0.9844],\n",
       "          ...,\n",
       "          [0.9844, 1.0000, 0.9883,  ..., 0.9844, 1.0000, 0.9844],\n",
       "          [0.9844, 0.9844, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.9844, 0.9844, 0.9844,  ..., 0.9844, 0.9844, 1.0000]],\n",
       "         device='cuda:0', dtype=torch.bfloat16, requires_grad=True)},\n",
       " 1: {'W_out': Parameter containing:\n",
       "  tensor([[0.9844, 1.0000, 0.9961,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 0.9844, 0.9844,  ..., 0.9844, 0.9844, 0.9844],\n",
       "          [1.0000, 0.9844, 1.0000,  ..., 1.0000, 0.9883, 1.0000],\n",
       "          ...,\n",
       "          [0.9844, 1.0000, 0.9883,  ..., 0.9844, 1.0000, 0.9961],\n",
       "          [0.9844, 0.9844, 1.0000,  ..., 0.9844, 0.9844, 0.9844],\n",
       "          [1.0000, 0.9883, 1.0000,  ..., 0.9844, 0.9883, 0.9844]],\n",
       "         device='cuda:0', dtype=torch.bfloat16, requires_grad=True)},\n",
       " 2: {},\n",
       " 3: {},\n",
       " 4: {'W_out': Parameter containing:\n",
       "  tensor([[1.0000, 0.9844, 1.0000,  ..., 0.9844, 0.9883, 1.0000],\n",
       "          [0.9844, 0.9961, 0.9844,  ..., 0.9844, 1.0000, 0.9961],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 0.9844, 0.9844],\n",
       "          ...,\n",
       "          [0.9883, 1.0000, 0.9883,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.9883, 1.0000, 0.9883,  ..., 1.0000, 1.0000, 0.9844],\n",
       "          [1.0000, 0.9844, 0.9844,  ..., 0.9844, 1.0000, 0.9844]],\n",
       "         device='cuda:0', dtype=torch.bfloat16, requires_grad=True)},\n",
       " 5: {},\n",
       " 6: {'W_out': Parameter containing:\n",
       "  tensor([[0.9883, 0.9844, 0.9844,  ..., 0.9844, 1.0000, 0.9961],\n",
       "          [1.0000, 0.9883, 0.9844,  ..., 0.9844, 1.0000, 0.9844],\n",
       "          [0.9844, 1.0000, 0.9844,  ..., 1.0000, 0.9844, 0.9961],\n",
       "          ...,\n",
       "          [1.0000, 0.9844, 1.0000,  ..., 1.0000, 0.9844, 1.0000],\n",
       "          [1.0000, 0.9844, 1.0000,  ..., 0.9883, 1.0000, 0.9844],\n",
       "          [1.0000, 1.0000, 0.9883,  ..., 0.9883, 0.9922, 0.9844]],\n",
       "         device='cuda:0', dtype=torch.bfloat16, requires_grad=True)},\n",
       " 7: {'W_out': Parameter containing:\n",
       "  tensor([[1.0000, 0.9883, 0.9844,  ..., 0.9844, 1.0000, 1.0000],\n",
       "          [0.9844, 1.0000, 1.0000,  ..., 1.0000, 0.9883, 1.0000],\n",
       "          [0.9844, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 0.9844],\n",
       "          ...,\n",
       "          [1.0000, 0.9844, 1.0000,  ..., 0.9844, 0.9844, 1.0000],\n",
       "          [0.9883, 1.0000, 0.9844,  ..., 1.0000, 0.9844, 0.9961],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.9844, 0.9883, 0.9883]],\n",
       "         device='cuda:0', dtype=torch.bfloat16, requires_grad=True)},\n",
       " 8: {'W_out': Parameter containing:\n",
       "  tensor([[0.9883, 0.9844, 0.9844,  ..., 0.9844, 0.9883, 1.0000],\n",
       "          [0.9883, 0.9844, 0.9961,  ..., 1.0000, 0.9844, 0.9961],\n",
       "          [1.0000, 0.9844, 1.0000,  ..., 1.0000, 0.9883, 0.9883],\n",
       "          ...,\n",
       "          [0.9883, 0.9844, 0.9844,  ..., 0.9883, 1.0000, 1.0000],\n",
       "          [0.9844, 0.9844, 1.0000,  ..., 1.0000, 0.9844, 0.9844],\n",
       "          [1.0000, 0.9961, 0.9844,  ..., 0.9844, 1.0000, 0.9844]],\n",
       "         device='cuda:0', dtype=torch.bfloat16, requires_grad=True)},\n",
       " 9: {},\n",
       " 10: {},\n",
       " 11: {},\n",
       " 12: {},\n",
       " 13: {},\n",
       " 14: {},\n",
       " 15: {},\n",
       " 16: {'W_out': Parameter containing:\n",
       "  tensor([[0.9883, 0.9844, 0.9844,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 0.9844, 0.9844,  ..., 0.9844, 1.0000, 0.9883],\n",
       "          [0.9844, 1.0000, 0.9844,  ..., 1.0000, 0.9844, 0.9844],\n",
       "          ...,\n",
       "          [0.9883, 1.0000, 1.0000,  ..., 0.9844, 0.9844, 1.0000],\n",
       "          [0.9844, 0.9883, 0.9883,  ..., 0.9883, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 0.9844, 1.0000]],\n",
       "         device='cuda:0', dtype=torch.bfloat16, requires_grad=True)},\n",
       " 17: {}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(f\"models/masks/{model_name.replace('/', '_')}-{forget_sport}-{localization_type}.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
