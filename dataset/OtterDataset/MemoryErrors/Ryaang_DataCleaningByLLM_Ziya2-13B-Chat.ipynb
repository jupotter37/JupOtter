{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é…ç½®ç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
      "Fetching 1 files:   0%|                                   | 0/1 [00:00<?, ?it/s]downloading https://hf-mirror.com/IDEA-CCNL/Ziya2-13B-Chat/resolve/e5d697b2daf4ec1f5f12036bef2597245d826910/pytorch_model-00001-of-00003.bin to /root/.cache/huggingface/hub/models--IDEA-CCNL--Ziya2-13B-Chat/blobs/f712a4eb70a485dab8dbaf82a74271e0ded2bb14976078f55d7bba988e349875.incomplete\n",
      "\n",
      "pytorch_model-00001-of-00003.bin:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 8.76G/9.90G [00:00<?, ?B/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  89%|â–ˆâ–ˆâ–ˆâ–Œ| 8.77G/9.90G [00:01<03:29, 5.41MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  89%|â–ˆâ–ˆâ–ˆâ–Œ| 8.78G/9.90G [00:02<02:05, 8.96MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  89%|â–ˆâ–ˆâ–ˆâ–Œ| 8.79G/9.90G [00:03<01:32, 12.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  89%|â–ˆâ–ˆâ–ˆâ–Œ| 8.80G/9.90G [00:03<01:19, 13.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  89%|â–ˆâ–ˆâ–ˆâ–Œ| 8.81G/9.90G [00:04<01:14, 14.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  89%|â–ˆâ–ˆâ–ˆâ–Œ| 8.82G/9.90G [00:04<01:10, 15.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  89%|â–ˆâ–ˆâ–ˆâ–Œ| 8.83G/9.90G [00:05<01:02, 17.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  89%|â–ˆâ–ˆâ–ˆâ–Œ| 8.84G/9.90G [00:06<01:02, 17.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  89%|â–ˆâ–ˆâ–ˆâ–Œ| 8.85G/9.90G [00:06<00:59, 17.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  90%|â–ˆâ–ˆâ–ˆâ–Œ| 8.86G/9.90G [00:07<00:57, 18.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  90%|â–ˆâ–ˆâ–ˆâ–Œ| 8.87G/9.90G [00:07<00:58, 17.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  90%|â–ˆâ–ˆâ–ˆâ–Œ| 8.88G/9.90G [00:08<00:55, 18.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  90%|â–ˆâ–ˆâ–ˆâ–Œ| 8.89G/9.90G [00:08<00:55, 18.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  90%|â–ˆâ–ˆâ–ˆâ–Œ| 8.90G/9.90G [00:09<00:56, 17.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  90%|â–ˆâ–ˆâ–ˆâ–Œ| 8.91G/9.90G [00:10<00:56, 17.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  90%|â–ˆâ–ˆâ–ˆâ–Œ| 8.92G/9.90G [00:10<00:51, 18.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  90%|â–ˆâ–ˆâ–ˆâ–Œ| 8.93G/9.90G [00:11<00:53, 18.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  90%|â–ˆâ–ˆâ–ˆâ–Œ| 8.94G/9.90G [00:11<00:51, 18.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  90%|â–ˆâ–ˆâ–ˆâ–Œ| 8.95G/9.90G [00:12<00:50, 18.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  91%|â–ˆâ–ˆâ–ˆâ–Œ| 8.97G/9.90G [00:12<00:52, 17.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  91%|â–ˆâ–ˆâ–ˆâ–‹| 8.98G/9.90G [00:13<00:49, 18.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  91%|â–ˆâ–ˆâ–ˆâ–‹| 8.99G/9.90G [00:14<00:50, 18.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  91%|â–ˆâ–ˆâ–ˆâ–‹| 9.00G/9.90G [00:14<00:50, 17.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  91%|â–ˆâ–ˆâ–ˆâ–‹| 9.01G/9.90G [00:15<00:53, 16.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  91%|â–ˆâ–ˆâ–ˆâ–‹| 9.02G/9.90G [00:15<00:46, 19.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  91%|â–ˆâ–ˆâ–ˆâ–‹| 9.03G/9.90G [00:16<00:48, 17.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  91%|â–ˆâ–ˆâ–ˆâ–‹| 9.04G/9.90G [00:16<00:45, 19.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  91%|â–ˆâ–ˆâ–ˆâ–‹| 9.05G/9.90G [00:17<00:45, 18.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  92%|â–ˆâ–ˆâ–ˆâ–‹| 9.06G/9.90G [00:18<00:47, 17.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  92%|â–ˆâ–ˆâ–ˆâ–‹| 9.07G/9.90G [00:18<00:44, 18.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  92%|â–ˆâ–ˆâ–ˆâ–‹| 9.08G/9.90G [00:19<00:45, 18.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  92%|â–ˆâ–ˆâ–ˆâ–‹| 9.09G/9.90G [00:19<00:45, 17.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  92%|â–ˆâ–ˆâ–ˆâ–‹| 9.10G/9.90G [00:20<00:43, 18.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  92%|â–ˆâ–ˆâ–ˆâ–‹| 9.11G/9.90G [00:20<00:43, 18.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  92%|â–ˆâ–ˆâ–ˆâ–‹| 9.12G/9.90G [00:21<00:44, 17.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  92%|â–ˆâ–ˆâ–ˆâ–‹| 9.13G/9.90G [00:22<00:40, 19.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  92%|â–ˆâ–ˆâ–ˆâ–‹| 9.14G/9.90G [00:22<00:41, 18.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  92%|â–ˆâ–ˆâ–ˆâ–‹| 9.15G/9.90G [00:23<00:39, 19.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  93%|â–ˆâ–ˆâ–ˆâ–‹| 9.16G/9.90G [00:23<00:39, 18.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  93%|â–ˆâ–ˆâ–ˆâ–‹| 9.18G/9.90G [00:24<00:40, 18.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  93%|â–ˆâ–ˆâ–ˆâ–‹| 9.19G/9.90G [00:24<00:37, 19.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  93%|â–ˆâ–ˆâ–ˆâ–‹| 9.20G/9.90G [00:25<00:39, 17.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  93%|â–ˆâ–ˆâ–ˆâ–‹| 9.21G/9.90G [00:26<00:37, 18.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  93%|â–ˆâ–ˆâ–ˆâ–‹| 9.22G/9.90G [00:26<00:37, 18.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  93%|â–ˆâ–ˆâ–ˆâ–‹| 9.23G/9.90G [00:27<00:37, 17.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  93%|â–ˆâ–ˆâ–ˆâ–‹| 9.24G/9.90G [00:27<00:35, 18.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  93%|â–ˆâ–ˆâ–ˆâ–‹| 9.25G/9.90G [00:28<00:35, 18.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  94%|â–ˆâ–ˆâ–ˆâ–‹| 9.26G/9.90G [00:29<00:36, 17.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  94%|â–ˆâ–ˆâ–ˆâ–‹| 9.27G/9.90G [00:29<00:33, 18.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  94%|â–ˆâ–ˆâ–ˆâ–‹| 9.28G/9.90G [00:30<00:33, 18.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  94%|â–ˆâ–ˆâ–ˆâ–Š| 9.29G/9.90G [00:30<00:34, 17.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  94%|â–ˆâ–ˆâ–ˆâ–Š| 9.30G/9.90G [00:31<00:32, 18.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  94%|â–ˆâ–ˆâ–ˆâ–Š| 9.31G/9.90G [00:31<00:31, 18.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  94%|â–ˆâ–ˆâ–ˆâ–Š| 9.32G/9.90G [00:32<00:30, 19.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  94%|â–ˆâ–ˆâ–ˆâ–Š| 9.33G/9.90G [00:32<00:30, 18.7MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  94%|â–ˆâ–ˆâ–ˆâ–Š| 9.34G/9.90G [00:33<00:30, 18.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  94%|â–ˆâ–ˆâ–ˆâ–Š| 9.35G/9.90G [00:34<00:28, 19.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  95%|â–ˆâ–ˆâ–ˆâ–Š| 9.36G/9.90G [00:34<00:29, 18.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  95%|â–ˆâ–ˆâ–ˆâ–Š| 9.37G/9.90G [00:35<00:27, 19.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  95%|â–ˆâ–ˆâ–ˆâ–Š| 9.38G/9.90G [00:35<00:27, 18.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  95%|â–ˆâ–ˆâ–ˆâ–Š| 9.40G/9.90G [00:36<00:27, 18.0MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  95%|â–ˆâ–ˆâ–ˆâ–Š| 9.41G/9.90G [00:36<00:26, 18.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  95%|â–ˆâ–ˆâ–ˆâ–Š| 9.42G/9.90G [00:37<00:26, 18.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  95%|â–ˆâ–ˆâ–ˆâ–Š| 9.43G/9.90G [00:38<00:26, 17.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  95%|â–ˆâ–ˆâ–ˆâ–Š| 9.44G/9.90G [00:38<00:24, 18.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  95%|â–ˆâ–ˆâ–ˆâ–Š| 9.45G/9.90G [00:39<00:24, 18.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  96%|â–ˆâ–ˆâ–ˆâ–Š| 9.46G/9.90G [00:39<00:24, 17.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  96%|â–ˆâ–ˆâ–ˆâ–Š| 9.47G/9.90G [00:40<00:22, 19.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  96%|â–ˆâ–ˆâ–ˆâ–Š| 9.48G/9.90G [00:40<00:22, 18.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  96%|â–ˆâ–ˆâ–ˆâ–Š| 9.49G/9.90G [00:41<00:22, 18.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  96%|â–ˆâ–ˆâ–ˆâ–Š| 9.50G/9.90G [00:42<00:23, 17.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  96%|â–ˆâ–ˆâ–ˆâ–Š| 9.51G/9.90G [00:42<00:20, 18.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  96%|â–ˆâ–ˆâ–ˆâ–Š| 9.52G/9.90G [00:43<00:19, 19.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  96%|â–ˆâ–ˆâ–ˆâ–Š| 9.53G/9.90G [00:43<00:19, 18.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  96%|â–ˆâ–ˆâ–ˆâ–Š| 9.54G/9.90G [00:44<00:19, 18.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  97%|â–ˆâ–ˆâ–ˆâ–Š| 9.55G/9.90G [00:44<00:18, 19.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  97%|â–ˆâ–ˆâ–ˆâ–Š| 9.56G/9.90G [00:45<00:18, 18.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  97%|â–ˆâ–ˆâ–ˆâ–Š| 9.57G/9.90G [00:46<00:18, 17.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  97%|â–ˆâ–ˆâ–ˆâ–Š| 9.58G/9.90G [00:46<00:16, 18.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  97%|â–ˆâ–ˆâ–ˆâ–‰| 9.59G/9.90G [00:47<00:16, 18.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  97%|â–ˆâ–ˆâ–ˆâ–‰| 9.60G/9.90G [00:47<00:15, 19.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  97%|â–ˆâ–ˆâ–ˆâ–‰| 9.62G/9.90G [00:48<00:15, 18.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  97%|â–ˆâ–ˆâ–ˆâ–‰| 9.63G/9.90G [00:48<00:15, 17.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  97%|â–ˆâ–ˆâ–ˆâ–‰| 9.64G/9.90G [00:49<00:13, 19.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  97%|â–ˆâ–ˆâ–ˆâ–‰| 9.65G/9.90G [00:49<00:13, 18.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  98%|â–ˆâ–ˆâ–ˆâ–‰| 9.66G/9.90G [00:50<00:12, 19.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  98%|â–ˆâ–ˆâ–ˆâ–‰| 9.67G/9.90G [00:51<00:12, 18.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  98%|â–ˆâ–ˆâ–ˆâ–‰| 9.68G/9.90G [00:51<00:12, 18.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  98%|â–ˆâ–ˆâ–ˆâ–‰| 9.69G/9.90G [00:52<00:11, 17.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  98%|â–ˆâ–ˆâ–ˆâ–‰| 9.70G/9.90G [00:52<00:10, 18.8MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  98%|â–ˆâ–ˆâ–ˆâ–‰| 9.71G/9.90G [00:53<00:10, 18.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  98%|â–ˆâ–ˆâ–ˆâ–‰| 9.72G/9.90G [00:53<00:09, 18.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  98%|â–ˆâ–ˆâ–ˆâ–‰| 9.73G/9.90G [00:54<00:09, 18.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  98%|â–ˆâ–ˆâ–ˆâ–‰| 9.74G/9.90G [00:55<00:08, 19.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  99%|â–ˆâ–ˆâ–ˆâ–‰| 9.75G/9.90G [00:55<00:08, 18.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  99%|â–ˆâ–ˆâ–ˆâ–‰| 9.76G/9.90G [00:56<00:07, 17.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  99%|â–ˆâ–ˆâ–ˆâ–‰| 9.77G/9.90G [00:56<00:06, 19.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  99%|â–ˆâ–ˆâ–ˆâ–‰| 9.78G/9.90G [00:57<00:06, 17.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  99%|â–ˆâ–ˆâ–ˆâ–‰| 9.79G/9.90G [00:57<00:05, 18.1MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  99%|â–ˆâ–ˆâ–ˆâ–‰| 9.80G/9.90G [00:58<00:05, 18.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  99%|â–ˆâ–ˆâ–ˆâ–‰| 9.81G/9.90G [00:59<00:04, 18.6MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  99%|â–ˆâ–ˆâ–ˆâ–‰| 9.83G/9.90G [00:59<00:03, 19.4MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  99%|â–ˆâ–ˆâ–ˆâ–‰| 9.84G/9.90G [01:00<00:03, 18.5MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin:  99%|â–ˆâ–ˆâ–ˆâ–‰| 9.85G/9.90G [01:00<00:02, 18.2MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin: 100%|â–ˆâ–ˆâ–ˆâ–‰| 9.86G/9.90G [01:01<00:02, 18.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin: 100%|â–ˆâ–ˆâ–ˆâ–‰| 9.87G/9.90G [01:02<00:01, 15.9MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin: 100%|â–ˆâ–ˆâ–ˆâ–‰| 9.88G/9.90G [01:02<00:01, 17.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin: 100%|â–ˆâ–ˆâ–ˆâ–‰| 9.89G/9.90G [01:03<00:00, 18.3MB/s]\u001b[A\n",
      "pytorch_model-00001-of-00003.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 9.90G/9.90G [01:03<00:00, 17.9MB/s]\u001b[A\n",
      "Fetching 1 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.15s/it]\n",
      "/root/IDEA-CCNL/Ziya2-13B-Chat\n"
     ]
    }
   ],
   "source": [
    "# ä¸‹è½½æ¨¡å‹\n",
    "!export HF_ENDPOINT=https://hf-mirror.com && huggingface-cli download --resume-download --local-dir-use-symlinks False IDEA-CCNL/Ziya2-13B-Chat --local-dir IDEA-CCNL/Ziya2-13B-Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å®˜æ–¹Demo\n",
    "- [å®˜æ–¹DemoğŸ”—](https://huggingface.co/IDEA-CCNL/Ziya2-13B-Chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:18<00:00,  6.09s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 54.81 MiB is free. Process 136864 has 23.59 GiB memory in use. Of the allocated memory 23.21 GiB is allocated by PyTorch, and 1.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32me:\\project\\DataCleaningByLLM\\Ziya2-13B-Chat.ipynb å•å…ƒæ ¼ 5\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/e%3A/project/DataCleaningByLLM/Ziya2-13B-Chat.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/e%3A/project/DataCleaningByLLM/Ziya2-13B-Chat.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/root/IDEA-CCNL/Ziya2-13B-Chat\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/e%3A/project/DataCleaningByLLM/Ziya2-13B-Chat.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39mfrom_pretrained(model_path,torch_dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat16)\u001b[39m.\u001b[39mto(device) \u001b[39m# å®˜æ–¹ç‰ˆæœ¬æ˜¯ AutoModelForCausalLM bfloat16, float16ç²¾åº¦æ›´ä½\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/e%3A/project/DataCleaningByLLM/Ziya2-13B-Chat.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_path, use_fast\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/lm/lib/python3.11/site-packages/transformers/modeling_utils.py:2271\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m     \u001b[39mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2267\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2268\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2269\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2270\u001b[0m         )\n\u001b[0;32m-> 2271\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mto(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/lm/lib/python3.11/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/envs/lm/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lm/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/lm/lib/python3.11/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         module\u001b[39m.\u001b[39m_apply(fn)\n\u001b[1;32m    812\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lm/lib/python3.11/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/lm/lib/python3.11/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacty of 23.65 GiB of which 54.81 MiB is free. Process 136864 has 23.59 GiB memory in use. Of the allocated memory 23.21 GiB is allocated by PyTorch, and 1.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model_path=\"/root/IDEA-CCNL/Ziya2-13B-Chat\"\n",
    "model = AutoModel.from_pretrained(model_path,torch_dtype=torch.float16).to(device) # å®˜æ–¹ç‰ˆæœ¬æ˜¯ AutoModelForCausalLM bfloat16, float16ç²¾åº¦æ›´ä½\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"æ‰‹æœºå¦‚æœè´´è†œè´´äº†ä¸€å¼ é˜²æŒ‡çº¹çš„é’¢åŒ–è†œï¼Œé‚£å±å¹•æŒ‡çº¹è§£é”è¿˜æœ‰æ•ˆå—ï¼Ÿ\"}]\n",
    "user_prefix = \"<human>:\"\n",
    "assistant_prefix = \"<bot>:\"\n",
    "separator = \"\\n\"\n",
    "prompt = []\n",
    "for item in messages:\n",
    "    prefix = user_prefix if item[\"role\"] == \"user\" else assistant_prefix\n",
    "    prompt.append(f\"{prefix}{item['content']}\")\n",
    "prompt.append(assistant_prefix)\n",
    "prompt = separator.join(prompt)\n",
    "\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "generate_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=512, \n",
    "            do_sample = True, \n",
    "            top_p = 0.9, \n",
    "            temperature = 0.85, \n",
    "            repetition_penalty=1.05, \n",
    "            eos_token_id=tokenizer.encode(\"</s>\"), \n",
    "            )\n",
    "output = tokenizer.batch_decode(generate_ids)[0]\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lm",
   "language": "python",
   "name": "lm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
