{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/V-Sekai-fire/meshgpt-dataset-01/blob/main/MeshGPT_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from re import T\n",
        "is_train_autoencoder = True\n",
        "is_train_autoencoder_iteration = True\n",
        "is_train_mesh_transformer = True\n",
        "is_train_mesh_transformer_iteration = False\n",
        "is_clear_dataset_npz = False"
      ],
      "metadata": {
        "id": "NM_rRocQAcZ_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "aFoYTLhXhTlT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92c0bc88-bc1e-45e6-cf19-904d9848c98c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9tmLTKtUdRmQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d995a3a2-7214-4b90-d22d-55a03d2517d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: typing_extensions 4.9.0\n",
            "Uninstalling typing_extensions-4.9.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/typing_extensions-4.9.0.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/typing_extensions.py\n",
            "Proceed (Y/n)?   Successfully uninstalled typing_extensions-4.9.0\n",
            "Collecting git+https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git\n",
            "  Cloning https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git to /tmp/pip-req-build-z3n8pryd\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git /tmp/pip-req-build-z3n8pryd\n",
            "  Resolved https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git to commit 5c189f32dbc20cd5882be4ef2a132e2aabcb8df5\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beartype in /usr/local/lib/python3.10/dist-packages (from classifier-free-guidance-pytorch==0.5.1) (0.16.4)\n",
            "Requirement already satisfied: einops>=0.7 in /usr/local/lib/python3.10/dist-packages (from classifier-free-guidance-pytorch==0.5.1) (0.7.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from classifier-free-guidance-pytorch==0.5.1) (6.1.3)\n",
            "Requirement already satisfied: open-clip-torch>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from classifier-free-guidance-pytorch==0.5.1) (2.23.0)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from classifier-free-guidance-pytorch==0.5.1) (2.1.0+cu121)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (from classifier-free-guidance-pytorch==0.5.1) (4.35.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.1) (0.16.0+cu121)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.1) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.1) (4.66.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.1) (0.20.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.1) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.1) (3.20.3)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.1) (0.9.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.1) (3.13.1)\n",
            "Collecting typing-extensions (from torch>=2.0->classifier-free-guidance-pytorch==0.5.1)\n",
            "  Using cached typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.1) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.1) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->classifier-free-guidance-pytorch==0.5.1) (2.1.0)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->classifier-free-guidance-pytorch==0.5.1) (0.2.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (0.4.1)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (0.25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->classifier-free-guidance-pytorch==0.5.1) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]->classifier-free-guidance-pytorch==0.5.1) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0->classifier-free-guidance-pytorch==0.5.1) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->open-clip-torch>=2.8.0->classifier-free-guidance-pytorch==0.5.1) (9.4.0)\n",
            "Installing collected packages: typing-extensions\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed typing-extensions-4.9.0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: trimesh in /usr/local/lib/python3.10/dist-packages (4.0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from trimesh) (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "!yes | pip uninstall typing-extensions\n",
        "!yes | pip install git+https://github.com/MarcusLoppe/classifier-free-guidance-pytorch.git\n",
        "!yes | pip install -q git+https://github.com/MarcusLoppe/meshgpt-pytorch.git\n",
        "!yes | pip install trimesh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import trimesh\n",
        "import numpy as np\n",
        "import os\n",
        "import csv\n",
        "from collections import OrderedDict\n",
        "\n",
        "from meshgpt_pytorch import (\n",
        "    MeshTransformerTrainer,\n",
        "    MeshAutoencoderTrainer,\n",
        "    MeshAutoencoder,\n",
        "    MeshTransformer\n",
        ")\n",
        "from meshgpt_pytorch.data import (\n",
        "    derive_face_edges_from_faces\n",
        ")"
      ],
      "metadata": {
        "id": "5ztZ1JUl8zOZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7aW7oUHedRmQ"
      },
      "outputs": [],
      "source": [
        "import trimesh\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "\n",
        "max_faces = 4096\n",
        "\n",
        "def get_mesh(file_path):\n",
        "    mesh = trimesh.load(file_path, force='mesh')\n",
        "\n",
        "    # Center and scale vertices\n",
        "    center = np.mean(mesh.vertices, axis=0)\n",
        "    vertices = mesh.vertices - center\n",
        "    max_abs = np.max(np.abs(vertices))\n",
        "    scale_factor = (1 / 128) / max_abs\n",
        "    vertices *= scale_factor\n",
        "\n",
        "    # Quantize vertices\n",
        "    vertices = np.around(vertices).astype(np.float32)\n",
        "\n",
        "    # Sort vertices by Z, Y, X\n",
        "    sorted_indices = np.lexsort(vertices.T[::-1])\n",
        "    vertices = vertices[sorted_indices]\n",
        "\n",
        "    # Map old indices to new, sorted indices\n",
        "    vertex_map = np.empty(len(sorted_indices), dtype=int)\n",
        "    vertex_map[sorted_indices] = np.arange(len(sorted_indices))\n",
        "\n",
        "    # Reindex faces\n",
        "    reindexed_faces = vertex_map[mesh.faces]\n",
        "    sorted_faces = np.sort(reindexed_faces, axis=1)\n",
        "\n",
        "    return vertices, sorted_faces\n",
        "\n",
        "def augment_mesh(vertices, jitter_strength=0.01):\n",
        "    jitter_amount = np.random.uniform(-jitter_strength, jitter_strength, size=vertices.shape)\n",
        "    vertices += jitter_amount\n",
        "    return vertices\n",
        "\n",
        "def snake_to_sentence_case(snake_str):\n",
        "    components = snake_str.split(\"_\")\n",
        "    return \" \".join(word.capitalize() for word in components)\n",
        "\n",
        "def load_filename(directory, variations):\n",
        "    obj_datas = []\n",
        "\n",
        "    # Get random scale factors within a range\n",
        "    scale_factors = np.random.uniform(0.75, 1.0, size=variations)\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".glb\"):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            vertices, faces = get_mesh(file_path)\n",
        "\n",
        "            if len(faces) > max_faces:\n",
        "                print(f\"Mesh {filename} has {len(faces)} faces which is more than the allowed {max_faces} faces. Rejecting.\")\n",
        "                continue\n",
        "\n",
        "            faces_tensor = torch.tensor(faces, dtype=torch.long).to(\"cuda\")\n",
        "            face_edges = derive_face_edges_from_faces(faces_tensor)\n",
        "\n",
        "            texts, _ = os.path.splitext(filename)\n",
        "            texts = snake_to_sentence_case(texts)\n",
        "            # Run video llava on the image. \"Describe the focus of the photo as a json dictionary.\"\n",
        "            for scale_factor in scale_factors:\n",
        "                aug_vertices = augment_mesh(vertices.copy()) * scale_factor\n",
        "                aug_vertices_tensor = torch.tensor(aug_vertices, dtype=torch.float)\n",
        "\n",
        "                obj_data = {\n",
        "                    \"vertices\": aug_vertices_tensor.to(\"cuda\"),\n",
        "                    \"faces\": faces_tensor,\n",
        "                    \"face_edges\": face_edges,\n",
        "                    \"texts\": texts\n",
        "                }\n",
        "                obj_datas.append(obj_data)\n",
        "\n",
        "    print(f\"[create_mesh_dataset] Returning {len(obj_datas)} meshes\")\n",
        "    return obj_datas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vthmcnU2dRmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85f98085-5321-4c85-d02c-1e06754c9d9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MeshDataset] Loaded 47 entrys\n",
            "[MeshDataset] Created from 47 entrys\n",
            "dict_keys(['vertices', 'faces', 'face_edges', 'texts'])\n",
            "{'S Box', 'S Gui', 'S Primitive Wedge', 'S Chair Sofa Wide', 'S Hmd', 'S Chair Box', 'S Bed Twin', 'S Table Counter', 'S Bone', 'S Table Nightstand', 'S Chair Sofa', 'S Cabinet Dresser 03', 'Mire Clothing', 'Sk Snake 01', 'Sk Cat 01', 'S Door Single', 'S Cabinet Dresser 05', 'S Ziggurat', 'S Table Coffee', 'S Table Sit Circle', 'S Door Double', 'S Phone', 'S Door Double Frame', 'S Tree No Leaves', 'S Bed King', 'S Mask', 'S Table Bedside', 'S Primitive Cylinder Hollow', 'S Table Sit Square', 'S Chair Bar', 'S Chair Modern', 'Sk Horse 01', 'S Table Office', 'S Table Sit Rectangle', 'S Door Single Frame', 'S Primitive Sphere', 'S Stairs Single-6', 'S Table Bar Rectangle', 'S Primitive Pyramid', 'S Chair Stool Mini', 'S Cabinet Bookshelf', 'S Chair Stool', 'S Primitive Cylinder', 'S Bed Full', 'S Tree Bushy', 'S Table Bar Circle', 'S Table Bar'}\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import gc\n",
        "import torch\n",
        "import os\n",
        "from meshgpt_pytorch import MeshDataset\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "project_name = \"demo_mesh\"\n",
        "\n",
        "working_dir = f'/content/drive/MyDrive/{project_name}'\n",
        "\n",
        "working_dir = Path(working_dir)\n",
        "working_dir.mkdir(exist_ok = True, parents = True)\n",
        "dataset_path = working_dir / (project_name + \".npz\")\n",
        "\n",
        "\n",
        "if is_clear_dataset_npz:\n",
        "    data = load_filename(working_dir, 1)\n",
        "    dataset = MeshDataset(data)\n",
        "    dataset.generate_face_edges()\n",
        "    dataset.save(dataset_path)\n",
        "else:\n",
        "    dataset = MeshDataset.load(dataset_path)\n",
        "\n",
        "print(dataset.data[0].keys())\n",
        "print(set(item[\"texts\"] for item in dataset.data)  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzVsRDtzdRmT"
      },
      "source": [
        "### Inspect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vPpalWsbdRmU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "606b6bda-bd10-4f5d-c946-1723dee1b3b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0 complete. Processed 403 vertices and 480 faces.\n",
            "Iteration 1 complete. Processed 403 vertices and 480 faces.\n",
            "Iteration 2 complete. Processed 336 vertices and 168 faces.\n",
            "Iteration 3 complete. Processed 264 vertices and 335 faces.\n",
            "Iteration 4 complete. Processed 168 vertices and 144 faces.\n",
            "Iteration 5 complete. Processed 168 vertices and 144 faces.\n",
            "Iteration 6 complete. Processed 205 vertices and 320 faces.\n",
            "Iteration 7 complete. Processed 216 vertices and 108 faces.\n",
            "Iteration 8 complete. Processed 128 vertices and 256 faces.\n",
            "Iteration 9 complete. Processed 96 vertices and 48 faces.\n",
            "Iteration 10 complete. Processed 96 vertices and 48 faces.\n",
            "Iteration 11 complete. Processed 62 vertices and 48 faces.\n",
            "Iteration 12 complete. Processed 130 vertices and 128 faces.\n",
            "Iteration 13 complete. Processed 56 vertices and 24 faces.\n",
            "Iteration 14 complete. Processed 38 vertices and 44 faces.\n",
            "Iteration 15 complete. Processed 38 vertices and 44 faces.\n",
            "Iteration 16 complete. Processed 24 vertices and 12 faces.\n",
            "Iteration 17 complete. Processed 18 vertices and 8 faces.\n",
            "Iteration 18 complete. Processed 16 vertices and 6 faces.\n",
            "Iteration 19 complete. Processed 1848 vertices and 912 faces.\n",
            "Iteration 20 complete. Processed 1280 vertices and 1306 faces.\n",
            "Iteration 21 complete. Processed 1917 vertices and 2320 faces.\n",
            "Iteration 22 complete. Processed 280 vertices and 556 faces.\n",
            "Iteration 23 complete. Processed 1486 vertices and 1376 faces.\n",
            "Iteration 24 complete. Processed 451 vertices and 626 faces.\n",
            "Iteration 25 complete. Processed 1089 vertices and 2090 faces.\n",
            "Iteration 26 complete. Processed 637 vertices and 564 faces.\n",
            "Iteration 27 complete. Processed 848 vertices and 564 faces.\n",
            "Iteration 28 complete. Processed 1142 vertices and 1464 faces.\n",
            "Iteration 29 complete. Processed 2243 vertices and 4304 faces.\n",
            "Iteration 30 complete. Processed 40 vertices and 20 faces.\n",
            "Iteration 31 complete. Processed 432 vertices and 216 faces.\n",
            "Iteration 32 complete. Processed 552 vertices and 256 faces.\n",
            "Iteration 33 complete. Processed 2179 vertices and 3884 faces.\n",
            "Iteration 34 complete. Processed 385 vertices and 440 faces.\n",
            "Iteration 35 complete. Processed 2388 vertices and 2184 faces.\n",
            "Iteration 36 complete. Processed 796 vertices and 910 faces.\n",
            "Iteration 37 complete. Processed 1197 vertices and 1486 faces.\n",
            "Iteration 38 complete. Processed 2074 vertices and 3638 faces.\n",
            "Iteration 39 complete. Processed 744 vertices and 344 faces.\n",
            "Iteration 40 complete. Processed 904 vertices and 1176 faces.\n",
            "Iteration 41 complete. Processed 3645 vertices and 4447 faces.\n",
            "Iteration 42 complete. Processed 702 vertices and 784 faces.\n",
            "Iteration 43 complete. Processed 1486 vertices and 1376 faces.\n",
            "Iteration 44 complete. Processed 3112 vertices and 1556 faces.\n",
            "Iteration 45 complete. Processed 690 vertices and 512 faces.\n",
            "Iteration 46 complete. Processed 3648 vertices and 4424 faces.\n",
            "Total number of processed meshes: 47\n"
          ]
        }
      ],
      "source": [
        "seen_texts = []  # Keep track of seen texts to avoid processing duplicates\n",
        "mesh_list = []  # List to store individual meshes\n",
        "translation_distance = 1.0  # Distance to translate vertices, set to 1 unit as required\n",
        "\n",
        "# Iterate over each item in the dataset\n",
        "for r, item in enumerate(dataset.data):\n",
        "    texts = item['texts']\n",
        "\n",
        "    # Skip this iteration if there are no texts or if we've already seen these texts\n",
        "    if len(texts) == 0 or texts in seen_texts:\n",
        "        continue\n",
        "\n",
        "    # Add these texts to the list of seen texts\n",
        "    seen_texts.append(texts)\n",
        "\n",
        "    # Get vertices and faces\n",
        "    vertices = np.array(item['vertices'].cpu())\n",
        "    faces = np.array(item['faces'].cpu())\n",
        "\n",
        "    # Translate the vertices copy\n",
        "    translation_vector = np.array([len(seen_texts) * translation_distance, 0, 0])  # Translation along the x-axis\n",
        "    vertices[:, :3] += translation_vector  # Apply translation only to x, y, z\n",
        "\n",
        "    # Create a new mesh object with the translated vertices and original faces\n",
        "    mesh = trimesh.Trimesh(vertices=vertices, faces=faces)\n",
        "\n",
        "    # Append the new mesh to our list of meshes\n",
        "    mesh_list.append(mesh)\n",
        "\n",
        "    print(f\"Iteration {r} complete. Processed {len(vertices)} vertices and {len(faces)} faces.\")\n",
        "\n",
        "# After iterating over all items, print the number of processed meshes\n",
        "print(f\"Total number of processed meshes: {len(mesh_list)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4pHZ20udRmU"
      },
      "source": [
        "### Train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xoAxJWecdRmV"
      },
      "outputs": [],
      "source": [
        "autoencoder = MeshAutoencoder().to(\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlJewvY1dRmV"
      },
      "source": [
        "**Have at least 400-2000 items in the dataset, use this to multiply the dataset**  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JJ9qMRVxdRmW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "940e9db2-e469-485d-a35c-89745daa93a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "423\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(dataset.data)\n",
        "\n",
        "initial_length = len(dataset.data)\n",
        "if initial_length > 0:\n",
        "    replication_factor = max(1, (400 - 1) // initial_length + 1)\n",
        "    dataset.data *= replication_factor\n",
        "    dataset.data = dataset.data[:2000]\n",
        "\n",
        "print(len(dataset.data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9k-OAoBdRmY"
      },
      "source": [
        "**Train to about 0.3 loss if you are using a small dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NKTuiTHvdRmY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "outputId": "296541fc-c5ea-4aec-bb79-7c112368b7ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/380: 100%|██████████| 52/52 [00:14<00:00,  3.57it/s, commit_loss=-.343, loss=0.203, recon_loss=0.238]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 average loss: 0.206714839889453 recon loss: 0.2407: commit_loss -0.3397\n",
            "Stopping training at epoch 0 with average loss 0.206714839889453\n",
            "Training complete\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "autoencoder_trainer = MeshAutoencoderTrainer(model = autoencoder ,warmup_steps = 10, dataset = dataset, num_train_steps=100,\n",
        "                                            batch_size=8,\n",
        "                                            grad_accum_every=2,\n",
        "                                            learning_rate = 4e-3)\n",
        "if is_train_autoencoder:\n",
        "  if is_train_autoencoder_iteration:\n",
        "    autoencoder_trainer.load(f'{working_dir}/mesh-encoder_{project_name}.pt')\n",
        "  loss = autoencoder_trainer.train(380,stop_at_loss = 0.28, diplay_graph= True)\n",
        "  autoencoder_trainer.save(f'{working_dir}/mesh-encoder_{project_name}.pt')\n",
        "else:\n",
        "  autoencoder_trainer.load(f'{working_dir}/mesh-encoder_{project_name}.pt')\n",
        "  autencoder = autoencoder_trainer.model\n",
        "  for param in autoencoder.parameters():\n",
        "      param.requires_grad = True\n",
        "  import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gLCezPnNdRmZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8d8a0d5-3369-4be5-bedf-e5458df1a362"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Highest face count: 4447\n",
            "Max token sequence: 26682\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder total parameters: 94.4M\n",
            "Total parameters: 154.3M\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "max_length =  max(len(d[\"faces\"]) for d in dataset if \"faces\" in d)\n",
        "max_seq = max_length * 6\n",
        "print(\"Highest face count:\" , max_length)\n",
        "print(\"Max token sequence:\" , max_seq)\n",
        "\n",
        "transformer = MeshTransformer(\n",
        "    autoencoder,\n",
        "    dim = 512,\n",
        "    coarse_pre_gateloop_depth = 6, # Better performance using more gateloop layers\n",
        "    fine_pre_gateloop_depth= 4,\n",
        "    #attn_depth = 24, # GPT-2 medium have 24 layer depth, change if needed\n",
        "    max_seq_len = max_seq,\n",
        "    condition_on_text = True,\n",
        "    gateloop_use_heinsen = False,\n",
        "    text_condition_model_types = \"bge\", ## Change or remove this line if you are using:  https://github.com/MarcusLoppe/classifier-free-guidance-pytorch\n",
        "    text_condition_cond_drop_prob = 0.0\n",
        ")\n",
        "\n",
        "total_params = sum(p.numel() for p in transformer.decoder.parameters())\n",
        "total_params = f\"{total_params / 1000000:.1f}M\"\n",
        "print(f\"Decoder total parameters: {total_params}\")\n",
        "total_params = sum(p.numel() for p in transformer.parameters())\n",
        "total_params = f\"{total_params / 1000000:.1f}M\"\n",
        "print(f\"Total parameters: {total_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tun8sv-udRmZ"
      },
      "source": [
        "## **Required!**, embed the text and run generate_codes to save 4-96 GB VRAM (dependant on dataset) ##\n",
        "\n",
        "**If you don't;** <br>\n",
        "During each during each training step the autoencoder will generate the codes and the text encoder will embed the text.\n",
        "<br>\n",
        "After these fields are generate: **they will be deleted and next time it generates the code again:**<br>\n",
        "\n",
        "This is due to the dataloaders nature, it writes this information to a temporary COPY of the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "s--ya8W0dRmZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61c17a19-3891-4a52-fa34-0d5e38952db9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'S Gui', 'S Box', 'S Primitive Wedge', 'S Chair Sofa Wide', 'S Hmd', 'S Chair Box', 'S Bed Twin', 'S Table Counter', 'S Cabinet Dresser 03', 'S Chair Sofa', 'S Bone', 'Mire Clothing', 'Sk Cat 01', 'Sk Snake 01', 'S Table Nightstand', 'S Door Single', 'S Cabinet Dresser 05', 'S Ziggurat', 'S Table Coffee', 'S Table Sit Circle', 'S Door Double', 'S Phone', 'S Door Double Frame', 'S Tree No Leaves', 'S Mask', 'S Bed King', 'S Table Bedside', 'S Chair Bar', 'S Table Sit Square', 'S Primitive Cylinder Hollow', 'S Chair Modern', 'Sk Horse 01', 'S Table Office', 'S Table Sit Rectangle', 'S Door Single Frame', 'S Primitive Sphere', 'S Stairs Single-6', 'S Table Bar Rectangle', 'S Primitive Pyramid', 'S Chair Stool Mini', 'S Cabinet Bookshelf', 'S Chair Stool', 'S Primitive Cylinder', 'S Bed Full', 'S Tree Bushy', 'S Table Bar Circle', 'S Table Bar'}\n",
            "[MeshDataset] Generated 47 text_embeddings\n",
            "[MeshDataset] Generated codes for 423 entrys\n",
            "dict_keys(['vertices', 'faces', 'face_edges', 'text_embeds', 'codes'])\n"
          ]
        }
      ],
      "source": [
        "labels = set(item[\"texts\"] for item in dataset.data)\n",
        "print(labels)\n",
        "dataset.embed_texts(transformer)\n",
        "dataset.generate_codes(autoencoder)\n",
        "print(dataset.data[0].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFa_p1G-dRma"
      },
      "source": [
        "*Load previous saved model if you had to restart session*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXZ0qgV3dRma"
      },
      "source": [
        "**Train to about 0.0001 loss (or less) if you are using a small dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "I9hV_xUQdRma",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "5c0edd86-3b01-4317-a4f3-de16bd617530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/200:  23%|██▎       | 48/211 [00:25<01:26,  1.89it/s, loss=9.48]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 3.26 GiB. GPU 0 has a total capacty of 39.56 GiB of which 2.03 GiB is free. Process 81437 has 37.52 GiB memory in use. Of the allocated memory 29.02 GiB is allocated by PyTorch, and 7.98 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-7b0ffbecb44d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_train_mesh_transformer_iteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{working_dir}/mesh-transformer_{project_name}.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_at_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.00001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/meshgpt_pytorch/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs, stop_at_loss, diplay_graph)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_no_sync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m                         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_accum_every\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/meshgpt_pytorch/meshgpt_pytorch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, vertices, faces, face_edges, codes, cache, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m             )\n\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_on_codes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassifier_free_guidance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/classifier_free_guidance_pytorch/classifier_free_guidance_pytorch.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(self, cond_scale, rescale_phi, cfg_routed_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mcond_scale\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'you cannot do condition scaling when in training mode'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn_maybe_with_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mcond_scale\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'invalid conditioning scale, must be greater or equal to 1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/classifier_free_guidance_pytorch/classifier_free_guidance_pytorch.py\u001b[0m in \u001b[0;36mfn_maybe_with_text\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_text_cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# main classifier free guidance logic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/meshgpt_pytorch/meshgpt_pytorch.py\u001b[0m in \u001b[0;36mforward_on_codes\u001b[0;34m(self, codes, return_loss, return_cache, append_eos, cache, texts, text_embeds, cond_drop_prob)\u001b[0m\n\u001b[1;32m   1576\u001b[0m         \u001b[0;31m# loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1578\u001b[0;31m         ce_loss = F.cross_entropy(\n\u001b[0m\u001b[1;32m   1579\u001b[0m             \u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b n c -> b c n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3053\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.26 GiB. GPU 0 has a total capacty of 39.56 GiB of which 2.03 GiB is free. Process 81437 has 37.52 GiB memory in use. Of the allocated memory 29.02 GiB is allocated by PyTorch, and 7.98 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "if not is_train_mesh_transformer:\n",
        "  trainer = MeshTransformerTrainer(model = transformer, warmup_steps = 10,grad_accum_every=1,num_train_steps=100, dataset = dataset, learning_rate = 1e-1, batch_size=1)\n",
        "  trainer.load(f'{working_dir}/mesh-transformer_{project_name}.pt')\n",
        "  transformer = trainer.model\n",
        "else:\n",
        "  trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=4,num_train_steps=100, dataset = dataset,\n",
        "                                 learning_rate = 5e-4, batch_size=2)\n",
        "  if is_train_mesh_transformer_iteration:\n",
        "    trainer.load(f'{working_dir}/mesh-transformer_{project_name}.pt')\n",
        "  loss = trainer.train(200, stop_at_loss = 0.00001)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the latest model**"
      ],
      "metadata": {
        "id": "TI5IM_Z3K26g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder_trainer = MeshAutoencoderTrainer(model = autoencoder ,warmup_steps = 10, dataset = dataset, num_train_steps=100, batch_size=8,  grad_accum_every=1, learning_rate = 1e-4)\n",
        "autoencoder_trainer.load(f'{working_dir}/mesh-encoder_{project_name}.pt')\n",
        "autencoder = autoencoder_trainer.model\n",
        "for param in autoencoder.parameters():\n",
        "    param.requires_grad = True\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "max_length =  max(len(d[\"faces\"]) for d in dataset if \"faces\" in d)\n",
        "max_seq = max_length * 6\n",
        "print(\"Highest face count:\" , max_length)\n",
        "print(\"Max token sequence:\" , max_seq)\n",
        "\n",
        "transformer = MeshTransformer(\n",
        "    autoencoder,\n",
        "    dim = 512,\n",
        "    coarse_pre_gateloop_depth = 6, # Better performance using more gateloop layers\n",
        "    fine_pre_gateloop_depth= 4,\n",
        "    # attn_depth = 24, # GPT-2 medium have 24 layer depth, change if needed\n",
        "    max_seq_len = max_seq,\n",
        "    condition_on_text = True,\n",
        "    gateloop_use_heinsen = False,\n",
        "    text_condition_model_types = \"bge\", ## Change or remove this line if you are using:  https://github.com/MarcusLoppe/classifier-free-guidance-pytorch\n",
        "    text_condition_cond_drop_prob = 0.0\n",
        ").to(\"cuda\")\n",
        "\n",
        "trainer = MeshTransformerTrainer(model = transformer,warmup_steps = 10,grad_accum_every=1,num_train_steps=100, dataset = dataset, learning_rate = 1e-1, batch_size=2)\n",
        "trainer.load(f'{working_dir}/mesh-transformer_{project_name}.pt')\n",
        "transformer = trainer.model\n"
      ],
      "metadata": {
        "id": "Q8by6SXp4GHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIn8JVsNdRma"
      },
      "source": [
        "## Generate and view mesh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRlzLzYxdRma"
      },
      "outputs": [],
      "source": [
        "trainer.save(f'{working_dir}/mesh-transformer_{project_name}.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tX4RM2-ddRma"
      },
      "outputs": [],
      "source": [
        "def combind_mesh(path, mesh):\n",
        "    all_vertices = []\n",
        "    all_faces = []\n",
        "    vertex_offset = 0\n",
        "    translation_distance = 0.5\n",
        "\n",
        "    for r, faces_coordinates in enumerate(mesh):\n",
        "        numpy_data = faces_coordinates[0].cpu().numpy().reshape(-1, 3)\n",
        "\n",
        "        for vertex in numpy_data:\n",
        "            all_vertices.append(f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\")\n",
        "\n",
        "        for i in range(1, len(numpy_data), 3):\n",
        "            all_faces.append(f\"f {i + vertex_offset} {i + 1 + vertex_offset} {i + 2 + vertex_offset}\\n\")\n",
        "\n",
        "        vertex_offset += len(numpy_data)\n",
        "\n",
        "    obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n",
        "\n",
        "    with open(path , \"w\") as file:\n",
        "        file.write(obj_file_content)\n",
        "\n",
        "def combind_mesh_with_rows(path, meshes):\n",
        "    all_vertices = []\n",
        "    all_faces = []\n",
        "    vertex_offset = 0\n",
        "    translation_distance = 0.5\n",
        "\n",
        "    for row, mesh in enumerate(meshes):\n",
        "        for r, faces_coordinates in enumerate(mesh):\n",
        "            numpy_data = faces_coordinates[0].cpu().numpy().reshape(-1, 3)\n",
        "            numpy_data[:, 0] += translation_distance * (r / 0.2 - 1)\n",
        "            numpy_data[:, 2] += translation_distance * (row / 0.2 - 1)\n",
        "\n",
        "            for vertex in numpy_data:\n",
        "                all_vertices.append(f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\")\n",
        "\n",
        "            for i in range(1, len(numpy_data), 3):\n",
        "                all_faces.append(f\"f {i + vertex_offset} {i + 1 + vertex_offset} {i + 2 + vertex_offset}\\n\")\n",
        "\n",
        "            vertex_offset += len(numpy_data)\n",
        "\n",
        "        obj_file_content = \"\".join(all_vertices) + \"\".join(all_faces)\n",
        "\n",
        "    with open(path , \"w\") as file:\n",
        "        file.write(obj_file_content)\n",
        "\n",
        "\n",
        "def write_mesh_output(path, coords):\n",
        "    numpy_data = faces_coordinates[0].cpu().numpy().reshape(-1, 3)\n",
        "    obj_file_content = \"\"\n",
        "\n",
        "    for vertex in numpy_data:\n",
        "        obj_file_content += f\"v {vertex[0]} {vertex[1]} {vertex[2]}\\n\"\n",
        "\n",
        "    for i in range(1, len(numpy_data), 3):\n",
        "        obj_file_content += f\"f {i} {i + 1} {i + 2}\\n\"\n",
        "\n",
        "    with open(path, \"w\") as file:\n",
        "        file.write(obj_file_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdd-0bMJdRma"
      },
      "source": [
        "**Using only text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzAkhWM7dRmb"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "\n",
        "folder = working_dir / 'renders'\n",
        "obj_file_path = Path(folder)\n",
        "obj_file_path.mkdir(exist_ok = True, parents = True)\n",
        "\n",
        "text_coords = []\n",
        "for text in labels:\n",
        "    print(f\"Generating {text}\")\n",
        "    faces_coordinates = transformer.generate(texts = [text],  temperature = 0.0)\n",
        "    text_coords.append(faces_coordinates)\n",
        "\n",
        "    write_mesh_output(f'{folder}/3d_output_{text}.obj', faces_coordinates)\n",
        "\n",
        "\n",
        "combind_mesh(f'{folder}/3d_models_all.obj', text_coords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oZP_cLvdRmb"
      },
      "source": [
        "**Text + prompt of tokens**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8GVxRnrdRmb"
      },
      "source": [
        "Grab fresh copy of dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68Ni22DzdRmb"
      },
      "outputs": [],
      "source": [
        "dataset = MeshDataset.load(dataset_path)\n",
        "dataset.generate_codes(autoencoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBJxZBNUdRmb"
      },
      "source": [
        "**Prompt with 10% of codes/tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0NTGFLFdRmb"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "token_length_procent = 0.10\n",
        "codes = []\n",
        "texts = []\n",
        "for label in labels:\n",
        "    for item in dataset.data:\n",
        "        if item['texts'] == label:\n",
        "            num_tokens = int(item[\"codes\"].shape[0] * token_length_procent)\n",
        "\n",
        "            texts.append(item['texts'])\n",
        "            codes.append(item[\"codes\"].flatten()[:num_tokens].unsqueeze(0))\n",
        "            break\n",
        "\n",
        "folder = working_dir / f'renders/text+codes'\n",
        "obj_file_path = Path(folder)\n",
        "obj_file_path.mkdir(exist_ok = True, parents = True)\n",
        "\n",
        "coords = []\n",
        "\n",
        "\n",
        "\n",
        "for text, prompt in zip(texts, codes):\n",
        "    print(f\"Generating {text} with {prompt.shape[1]} tokens\")\n",
        "    faces_coordinates = transformer.generate(texts = [text],  prompt = prompt, temperature = 0)\n",
        "    coords.append(faces_coordinates)\n",
        "\n",
        "    obj_file_path = f'{folder}/{text}_{prompt.shape[1]}_tokens.obj'\n",
        "    write_mesh_output(obj_file_path, faces_coordinates)\n",
        "\n",
        "    print(obj_file_path)\n",
        "\n",
        "\n",
        "combind_mesh(f'{folder}/text+prompt_all.obj', coords)\n",
        "\n",
        "if text_coords is not None:\n",
        "    combind_mesh_with_rows(f'{folder}/both_verisons.obj', [text_coords , coords])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhBZsZJtdRmb"
      },
      "source": [
        "**Prompt with 0% to 80% of tokens**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXUYkxcwdRmc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import trimesh\n",
        "from pathlib import Path\n",
        "\n",
        "def convert_to_obj(vertices, faces, output_file_path):\n",
        "    scene = trimesh.Scene()\n",
        "    mesh = trimesh.Trimesh(vertices=vertices, faces=faces)\n",
        "    scene.add_geometry(mesh)\n",
        "\n",
        "    with open(output_file_path, \"w\") as f:\n",
        "        f.write(scene.export(file_type=\"obj\"))\n",
        "\n",
        "def encode_to_pua(codes):\n",
        "    flat_codes = [item for sublist in codes for subsublist in sublist for item in subsublist]\n",
        "    return \"\".join(chr(code + 0xF0000) for code in flat_codes)\n",
        "\n",
        "jsonl_lines = []\n",
        "\n",
        "for token_length_procent in np.arange(0, 0.8, 0.1):\n",
        "    codes = []\n",
        "    texts = []\n",
        "    for label in labels:\n",
        "        for item in dataset.data:\n",
        "            if item['texts'] == label:\n",
        "                num_tokens = int(item[\"codes\"].shape[0] * token_length_procent)\n",
        "\n",
        "                texts.append(item['texts'])\n",
        "                codes.append(item[\"codes\"].flatten()[:num_tokens].unsqueeze(0))\n",
        "                break\n",
        "\n",
        "    coords = []\n",
        "    for text, code in zip(texts, codes):\n",
        "        print(f\"Generating {text} with {code.shape[1]} tokens\")\n",
        "        faces_coordinates = transformer.generate(texts=[text], prompt=code, temperature=0)\n",
        "        coords.append(faces_coordinates)\n",
        "\n",
        "        # Process mesh data inlined here\n",
        "        continuous_coors_list = [np_array.tolist() for np_array in faces_coordinates]\n",
        "        flat_list = [item for sublist in continuous_coors_list for item in sublist]\n",
        "        vertices = [vertex for sublist in flat_list for vertex in sublist]\n",
        "        faces = [[i, i + 1, i + 2] for i in range(0, len(vertices), 3)]\n",
        "\n",
        "        obj_filename = f'{text}_{code.shape[1]}_tokens.obj'\n",
        "        obj_file_path = folder / obj_filename\n",
        "        convert_to_obj(vertices, faces, obj_file_path)\n",
        "\n",
        "        encoded_codes = encode_to_pua(code.cpu().tolist())\n",
        "\n",
        "        with open(obj_file_path, \"r\") as file:\n",
        "            obj_contents = file.read()\n",
        "\n",
        "        # Append line to JSONL structure\n",
        "        jsonl_line = [\n",
        "            {\"role\": \"system\", \"content\": \"This assistant can understand 3D models using the meshgpt-pytorch Unicode plane 15 codebook for 16384 triangles and the .obj 3d format.\"},\n",
        "            {\"role\": \"user\", \"content\": encoded_codes},\n",
        "            {\"role\": \"assistant\", \"content\": obj_contents}\n",
        "        ]\n",
        "        jsonl_lines.append(jsonl_line)\n",
        "\n",
        "        print(obj_file_path)\n",
        "\n",
        "    mesh_rows.append(coords)\n",
        "    combind_mesh(f'{folder}/text+prompt_all_{token_length_procent}.obj', coords)\n",
        "\n",
        "combind_mesh_with_rows(f'{folder}/all.obj', mesh_rows)\n",
        "\n",
        "with open(\"chatml.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for item in jsonl_lines:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30627,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}