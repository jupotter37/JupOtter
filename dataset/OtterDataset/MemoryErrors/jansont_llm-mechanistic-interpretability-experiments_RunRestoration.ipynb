{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1cdb984-1bd1-4c62-afb1-fac9c8677cbe",
   "metadata": {},
   "source": [
    "## Effect of Model Restoration\n",
    "Runs restoration experiment over the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecb18fd7-3c76-4cdd-91ef-a31410244c9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    # %pip install git+https://github.com/neelnanda-io/TransformerLens.git\n",
    "    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
    "    %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
    "    renderer = \"colab\"\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    from IPython import get_ipython\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    renderer = \"jupyterlab\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7850beb2-bf91-4b0f-a597-f69be3275959",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: poetry in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: build<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.10.0)\n",
      "Requirement already satisfied: cachecontrol[filecache]<0.14.0,>=0.13.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.13.1)\n",
      "Requirement already satisfied: cleo<3.0.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (2.0.1)\n",
      "Requirement already satisfied: crashtest<0.5.0,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.4.1)\n",
      "Requirement already satisfied: dulwich<0.22.0,>=0.21.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.21.5)\n",
      "Requirement already satisfied: installer<0.8.0,>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.7.0)\n",
      "Requirement already satisfied: jsonschema<4.18.0,>=4.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (4.17.3)\n",
      "Requirement already satisfied: keyring<25.0.0,>=24.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (24.2.0)\n",
      "Requirement already satisfied: packaging>=20.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (23.1)\n",
      "Requirement already satisfied: pexpect<5.0.0,>=4.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (4.8.0)\n",
      "Requirement already satisfied: pkginfo<2.0.0,>=1.9.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.9.6)\n",
      "Requirement already satisfied: platformdirs<4.0.0,>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (3.8.0)\n",
      "Requirement already satisfied: poetry-core==1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.7.0)\n",
      "Requirement already satisfied: poetry-plugin-export<2.0.0,>=1.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.5.0)\n",
      "Requirement already satisfied: pyproject-hooks<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.18 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (2.31.0)\n",
      "Requirement already satisfied: requests-toolbelt<2,>=0.9.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.0.0)\n",
      "Requirement already satisfied: shellingham<2.0,>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.5.3)\n",
      "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (2.0.1)\n",
      "Requirement already satisfied: tomlkit<1.0.0,>=0.11.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.11.8)\n",
      "Requirement already satisfied: trove-classifiers>=2022.5.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (2023.8.7)\n",
      "Requirement already satisfied: virtualenv<21.0.0,>=20.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (20.24.3)\n",
      "Requirement already satisfied: msgpack>=0.5.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cachecontrol[filecache]<0.14.0,>=0.13.0->poetry) (1.0.5)\n",
      "Requirement already satisfied: filelock>=3.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cachecontrol[filecache]<0.14.0,>=0.13.0->poetry) (3.12.2)\n",
      "Requirement already satisfied: rapidfuzz<3.0.0,>=2.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cleo<3.0.0,>=2.0.0->poetry) (2.15.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dulwich<0.22.0,>=0.21.2->poetry) (2.0.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema<4.18.0,>=4.10.0->poetry) (23.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema<4.18.0,>=4.10.0->poetry) (0.19.3)\n",
      "Requirement already satisfied: jaraco.classes in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring<25.0.0,>=24.0.0->poetry) (3.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.11.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring<25.0.0,>=24.0.0->poetry) (5.2.0)\n",
      "Requirement already satisfied: SecretStorage>=3.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring<25.0.0,>=24.0.0->poetry) (3.3.3)\n",
      "Requirement already satisfied: jeepney>=0.4.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring<25.0.0,>=24.0.0->poetry) (0.8.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pexpect<5.0.0,>=4.7.0->poetry) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0,>=2.18->poetry) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0,>=2.18->poetry) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0,>=2.18->poetry) (2023.5.7)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from virtualenv<21.0.0,>=20.22.0->poetry) (0.3.7)\n",
      "Collecting platformdirs<4.0.0,>=3.0.0 (from poetry)\n",
      "  Using cached platformdirs-3.10.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib-metadata>=4.11.4->keyring<25.0.0,>=24.0.0->poetry) (3.15.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from SecretStorage>=3.2->keyring<25.0.0,>=24.0.0->poetry) (39.0.2)\n",
      "Requirement already satisfied: more-itertools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jaraco.classes->keyring<25.0.0,>=24.0.0->poetry) (10.1.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring<25.0.0,>=24.0.0->poetry) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring<25.0.0,>=24.0.0->poetry) (2.21)\n",
      "Installing collected packages: platformdirs\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 3.8.0\n",
      "    Uninstalling platformdirs-3.8.0:\n",
      "      Successfully uninstalled platformdirs-3.8.0\n",
      "Successfully installed platformdirs-3.10.0\n",
      "Installing dependencies from lock file\n",
      "\n",
      "Package operations: 0 installs, 1 update, 0 removals\n",
      "\n",
      "  â€¢ Downgrading platformdirs (3.10.0 -> 3.8.0)\n",
      "\n",
      "Installing the current project: transformer-lens (0.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker 2.167.0 requires importlib-metadata<5.0,>=1.4.0, but you have importlib-metadata 5.2.0 which is incompatible.\n",
      "sagemaker 2.167.0 requires protobuf<4.0,>=3.1, but you have protobuf 4.23.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ../\n",
    "pip install poetry\n",
    "poetry install\n",
    "cd notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19d3d52e-646f-4715-b40a-bfff0987715e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = renderer\n",
    "\n",
    "\n",
    "# Import stuff\n",
    "import types\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from jaxtyping import Float, Int\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML\n",
    "import ast\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import pickle\n",
    "if IN_COLAB: \n",
    "    import pysvelte\n",
    "\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens.utilities import devices\n",
    "from transformer_lens.past_key_value_caching import HookedTransformerKeyValueCache\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6db405b-b60c-45c5-94e8-33eb312ac4f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def imshow(tensor, renderer=None, **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, **kwargs):\n",
    "    px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)\n",
    "\n",
    "def cuda():\n",
    "    return torch.cuda.is_available()\n",
    "\n",
    "def get_device(): \n",
    "    return \"cuda\" if cuda() else \"cpu\"\n",
    "\n",
    "def save_pickle(obj, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "# Function to load a pickle object from a file\n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj\n",
    "\n",
    "def save_json(data, file_path):\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        loaded_data = json.load(json_file)\n",
    "    return loaded_data\n",
    "\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e4c1d8-eaa7-4dd6-9511-f163b28eea66",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "- fold_ln: Whether to fold in the LayerNorm weights to the subsequent linear layer. This does not change the computation.\n",
    "- center_writing_weights: Whether to center weights writing to the residual stream (ie set mean to be zero). Due to LayerNormthis doesn't change the computation.\n",
    "- center_unembed : Whether to center W_U (ie set mean to be zero). Softmax is translation invariant so this doesn't affect log probs or loss, but does change logits. Defaults to True.\n",
    "- refactor_factored_attn_matrices: Whether to convert the factoredmatrices (W_Q & W_K, and W_O & W_V) to be \"even\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "026991d1-9be3-40cc-b8cf-cbadf7792d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-large into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# MODEL_NAME = \"gpt2-large\"\n",
    "MODEL_NAME = \"gpt2-large\"\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        center_unembed=True,  \n",
    "        center_writing_weights=True,              # Whether to center weights writing to the residual stream (ie set mean to be zero). Due to LayerNorm this doesn't change the computation.      \n",
    "        fold_ln=True,                             # Whether to  fold in the LayerNorm weights to the subsequent linear layer.\n",
    "        refactor_factored_attn_matrices=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a3dc3-172c-4790-9aab-09db6333403a",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We use the CounterFact Dataset. We subsample facts which the model can accurately predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11446ada-fe11-453f-a45a-3ded4f64843d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_json(\"data/fact_dataset.json\")\n",
    "\n",
    "\n",
    "def sample_dataset(dataset, idx = None): \n",
    "    if idx is None: \n",
    "        prompt = \"The {} is located in\"\n",
    "        subject = \"Eiffel Tower\"\n",
    "        target = \"Paris\"\n",
    "    else: \n",
    "        sample = dataset[idx]\n",
    "        prompt = sample[\"requested_rewrite\"][\"prompt\"]\n",
    "        subject = sample[\"requested_rewrite\"][\"subject\"]\n",
    "        target = sample[\"requested_rewrite\"][\"target_true\"][\"str\"]\n",
    "    return prompt, subject, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c608fb-b179-4fe6-9365-7b90d6a6ded4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ablation Methods: \n",
    "- Noise ablation \n",
    "- Resample ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2601b6e-6a51-42bf-9285-ea45b487285f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def noise_ablation(prompt, subject, target, n_noise_samples=5, vx=3):\n",
    "    subject_tokens = model.to_tokens(subject)\n",
    "    \n",
    "    #shape: batch, n_tokens, embedding_dim\n",
    "    subject_embedding = model.embed(subject_tokens)\n",
    "    _, n_tokens, embedding_dim = subject_embedding.shape\n",
    "    \n",
    "    #noise: N(0,v), v = 3*std(embedding)\n",
    "    embedding = model.W_E\n",
    "    v = vx*torch.std(embedding, dim=0) #for each v in V\n",
    "    noise = torch.randn(\n",
    "        (n_noise_samples, n_tokens, embedding_dim)\n",
    "    ).to(device) + v\n",
    "    \n",
    "    subject_embedding_w_noise = subject_embedding + noise\n",
    "    \n",
    "    #shape: batch, n_tokens, vocab_size (logits)\n",
    "    unembedded_subject = model.unembed(subject_embedding_w_noise)\n",
    "\n",
    "    noisy_subject_tokens = torch.argmax(unembedded_subject, dim=-1)\n",
    "    noisy_subject_str = [\n",
    "        model.to_string(nst) for nst in noisy_subject_tokens\n",
    "    ]\n",
    "    true_prompt = prompt.format(subject)\n",
    "    corrupted_prompts = [\n",
    "        prompt.format(nss) for nss in noisy_subject_str\n",
    "    ]\n",
    "    return true_prompt, corrupted_prompts, target\n",
    "\n",
    "def resample_ablation(prompt, subject, target, n_noise_samples=20):\n",
    "    subject_tokens = model.to_tokens(subject)\n",
    "    embedding = model.W_E\n",
    "    #we select n random rows from the embedding matrix\n",
    "    permutations = torch.randperm(embedding.size(0))[:n_noise_samples]\n",
    "    random_samples = embedding[permutations]\n",
    "    #unsqueeze a token dimension between batch and embedding dims\n",
    "    random_samples = random_samples.unsqueeze(dim=1)\n",
    "    #we de-embed these rows\n",
    "    random_embeddings = model.unembed(random_samples)\n",
    "    random_tokens = torch.argmax(random_embeddings, dim=-1)\n",
    "    random_subject_str = [\n",
    "        model.to_string(t) for t in random_tokens\n",
    "    ]\n",
    "    corrupted_facts = [\n",
    "        prompt.format(s) for s in random_subject_str\n",
    "    ]\n",
    "    true_fact = prompt.format(subject)\n",
    "    \n",
    "    fact_tokens = model.to_str_tokens(true_fact)\n",
    "    subject_tokens = model.to_str_tokens(subject)\n",
    "    \n",
    "    fact_tokens = model.to_tokens(true_fact)\n",
    "    subject_tokens = model.to_tokens(subject)[:,1:]\n",
    "\n",
    "    subject_token_mask = torch.zeros_like(fact_tokens, dtype=torch.bool)\n",
    "    for value in subject_tokens[0]:\n",
    "        subject_token_mask |= (fact_tokens == value)\n",
    "\n",
    "    subject_token_mask = subject_token_mask.view(fact_tokens.shape)\n",
    "    \n",
    "    return true_fact, corrupted_facts, target, subject_token_mask\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8f8accf-be5f-4546-9b69-717c8f87bc28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_from_left(tokens : torch.tensor, maxlen:int):\n",
    "    pad_token = model.tokenizer.pad_token_id\n",
    "    padded_tokenized_inputs = torch.zeros(tokens.shape[0], maxlen)\n",
    "    \n",
    "    n_pads = maxlen - tokens.shape[-1]\n",
    "    padded_tokenized_inputs[:,n_pads] = pad_token\n",
    "    padded_tokenized_inputs[:,n_pads:] = tokens\n",
    "    return padded_tokenized_inputs.long()\n",
    "\n",
    "def pad_to_same_length(clean_tokens, corrupted_tokens): \n",
    "    \n",
    "    maxlen = max([clean_tokens.shape[-1], corrupted_tokens.shape[-1]])\n",
    "    \n",
    "    if clean_tokens.shape[-1] > corrupted_tokens.shape[-1]: \n",
    "        corrupted_tokens = pad_from_left(corrupted_tokens, maxlen)\n",
    "    elif clean_tokens.shape[-1] < corrupted_tokens.shape[-1]: \n",
    "        clean_tokens = pad_from_left(clean_tokens, maxlen)\n",
    "    return clean_tokens, corrupted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db31fb8d-4b50-4b1e-8310-f000d1fce71f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unembedding_function(residual_stack, cache, mlp=False) -> float:\n",
    "    #we are only interested in applying the layer norm of the final layer on the final token\n",
    "    #shape: [74, 5, 10, 1280] = n_layers, prompts, tokens, d_model\n",
    "    z = cache.apply_ln_to_stack(residual_stack, layer = -1)\n",
    "    z = z @ model.W_U\n",
    "    return z\n",
    "\n",
    "\n",
    "def decompose_residual(cache,\n",
    "                  mle_token_idx,\n",
    "                  target_token_idx, \n",
    "                return_labels=False):\n",
    "    if return_labels: \n",
    "        residual_stack, labels = cache.decompose_resid(layer=-1, mode=\"all\", return_labels=True)\n",
    "    else: \n",
    "        residual_stack = cache.decompose_resid(layer=-1, mode=\"all\", return_labels=False)\n",
    "    \n",
    "    #shape: [74, 4, 9, 50257] = n_layers, batch, tokens, vocab_size\n",
    "    residual_logits = unembedding_function(residual_stack, cache)\n",
    "    residual_logits = residual_logits[:,:,-1,:]\n",
    "\n",
    "    target_idx_expanded = target_token_idx.repeat(residual_logits.shape[0],1,1)\n",
    "    mle_idx_expanded = mle_token_idx.repeat(residual_logits.shape[0],1,1)\n",
    "\n",
    "    target_residual_logits = residual_logits.gather(index=target_idx_expanded, dim=-1) - residual_logits.mean(dim=-1, keepdim=True)\n",
    "    mle_residual_logits = residual_logits.gather(index=mle_idx_expanded, dim=-1) - residual_logits.mean(dim=-1, keepdim=True)\n",
    "    \n",
    "    target_residual_logits = target_residual_logits.squeeze().mean(dim=-1)\n",
    "    mle_residual_logits = mle_residual_logits.squeeze().mean(dim=-1)\n",
    "    \n",
    "    if return_labels: \n",
    "        return labels, mle_residual_logits, target_residual_logits\n",
    "    return mle_residual_logits, target_residual_logits\n",
    "\n",
    "def patch_layer(\n",
    "    corrupted_residual_component: Float[torch.Tensor, \"batch pos d_model\"],\n",
    "    hook,\n",
    "    cache):\n",
    "    corrupted_residual_component[:, :, :] = cache[hook.name][:, :, :]\n",
    "    \"\"\"\n",
    "    Restore a patch in clean run. \n",
    "    \"\"\"\n",
    "    return corrupted_residual_component\n",
    "\n",
    "def patch_position(\n",
    "    corrupted_residual_component: Float[torch.Tensor, \"batch pos d_model\"],\n",
    "    hook,\n",
    "    pos,\n",
    "    clean_cache):\n",
    "    corrupted_residual_component[:, pos, :] = clean_cache[hook.name][:, pos, :]\n",
    "    \"\"\"\n",
    "    Restore a patch in clean run. \n",
    "    \"\"\"\n",
    "    return corrupted_residual_component\n",
    "\n",
    "def run_all(clean_prompt: str,\n",
    "                         corrupted_prompts: List[str],\n",
    "                         target: str):\n",
    "    \n",
    "    clean_tokens = model.to_tokens(clean_prompt, prepend_bos=True) \n",
    "    corrupted_tokens = model.to_tokens(corrupted_prompts, prepend_bos=True)\n",
    "    target_token_idx = model.to_tokens(target)[:,1] #remember to slice out the start pad token\n",
    "    clean_tokens, corrupted_tokens = pad_to_same_length(clean_tokens, corrupted_tokens)\n",
    "    clean_tokens = clean_tokens.expand(corrupted_tokens.shape[0], -1)\n",
    "    target_token_idx = target_token_idx.expand(corrupted_tokens.shape[0], -1)\n",
    "    n_tokens = clean_tokens.shape[1]\n",
    "    \n",
    "    clean_logits, clean_cache = model.run_with_cache(clean_tokens, return_type=\"logits\")\n",
    "    corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens, return_type=\"logits\")\n",
    "    \n",
    "    mle_token_idx = torch.argmax(clean_logits[:,-1,:], dim=-1).unsqueeze(-1)\n",
    "    \n",
    "    clean_logits = clean_logits[:,-1,:]\n",
    "    clean_mle_logit = (clean_logits.gather(dim=-1, index=mle_token_idx) - clean_logits.mean(dim=-1, keepdim=True)).to(\"cpu\")\n",
    "    clean_target_logit = (clean_logits.gather(dim=-1, index=target_token_idx) - clean_logits.mean(dim=-1, keepdim=True)).to(\"cpu\")\n",
    "    \n",
    "    corrupted_logits = corrupted_logits[:,-1,:]\n",
    "    corrupted_mle_logit = (corrupted_logits.gather(dim=-1, index=mle_token_idx) - corrupted_logits.mean(dim=-1, keepdim=True)).to(\"cpu\")\n",
    "    corrupted_target_logit = (corrupted_logits.gather(dim=-1, index=target_token_idx) - corrupted_logits.mean(dim=-1, keepdim=True)).to(\"cpu\")\n",
    "    \n",
    "    clean_mle_logit = clean_mle_logit.mean(0) ; clean_target_logit = clean_target_logit.mean(0)\n",
    "    corrupted_mle_logit = corrupted_mle_logit.mean(0) ; corrupted_target_logit = corrupted_target_logit.mean(0)\n",
    "    \n",
    "    layer_names, clean_mle_residual_logits, clean_target_residual_logits = decompose_residual(clean_cache,\n",
    "                                                                                          mle_token_idx,\n",
    "                                                                                          target_token_idx, \n",
    "                                                                                         return_labels=True)\n",
    "\n",
    "    corrupted_mle_residual_logits, corrupted_target_residual_logits = decompose_residual(corrupted_cache,\n",
    "                                                                              mle_token_idx,\n",
    "                                                                              target_token_idx)\n",
    "    \n",
    "    total_restored_mle_residual_logits = torch.zeros(model.cfg.n_layers * 2, len(layer_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_restored_target_residual_logits = torch.zeros(model.cfg.n_layers * 2, len(layer_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_restored_mle_logits = torch.zeros(model.cfg.n_layers * 2, device=\"cpu\", dtype=torch.float32)\n",
    "    total_restored_target_logits = torch.zeros(model.cfg.n_layers * 2, device=\"cpu\", dtype=torch.float32)\n",
    "    \n",
    "    total_restored_mle_residual_position_logits = torch.zeros(model.cfg.n_layers * 2, n_tokens, len(layer_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_restored_target_residual_position_logits = torch.zeros(model.cfg.n_layers * 2, n_tokens, len(layer_names), device=\"cpu\", dtype=torch.float32)\n",
    "    total_restored_mle_position_logits = torch.zeros(model.cfg.n_layers * 2, n_tokens, device=\"cpu\", dtype=torch.float32)\n",
    "    total_restored_target_position_logits = torch.zeros(model.cfg.n_layers * 2, n_tokens, device=\"cpu\", dtype=torch.float32)\n",
    "\n",
    "    for layer in range(model.cfg.n_layers * 2):\n",
    "        if layer % 2 == 0: \n",
    "            patch_name =  f\"blocks.{layer//2}.hook_attn_out\"\n",
    "        else: \n",
    "            patch_name = f\"blocks.{layer//2}.hook_mlp_out\"\n",
    "        \n",
    "        hook_fn = partial(patch_layer, cache=clean_cache)            \n",
    "        with model.hooks(\n",
    "            fwd_hooks = [(patch_name, hook_fn)]\n",
    "        ) as hooked_model:\n",
    "            restored_logits, restored_cache = hooked_model.run_with_cache(corrupted_tokens, return_type=\"logits\")\n",
    "\n",
    "            restored_mle_residual_logits, restored_target_residual_logits = decompose_residual(restored_cache, mle_token_idx, target_token_idx)\n",
    "            hooked_model.reset_hooks()\n",
    "            model.reset_hooks()\n",
    "        \n",
    "        restored_logits = restored_logits[:,-1,:]\n",
    "        mle_logit = (restored_logits.gather(dim=-1, index=mle_token_idx) - restored_logits.mean(dim=-1, keepdim=True)).to(\"cpu\")\n",
    "        target_logit = (restored_logits.gather(dim=-1, index=target_token_idx) - restored_logits.mean(dim=-1, keepdim=True)).to(\"cpu\")\n",
    "        mle_logit = mle_logit.mean(0) ; target_logit = target_logit.mean(0)\n",
    "    \n",
    "        total_restored_mle_logits[layer] = mle_logit\n",
    "        total_restored_target_logits[layer] = target_logit\n",
    "                \n",
    "        total_restored_mle_residual_logits[layer] = restored_mle_residual_logits.to(\"cpu\")\n",
    "        total_restored_target_residual_logits[layer] = restored_target_residual_logits.to(\"cpu\")\n",
    "        \n",
    "        \n",
    "        for position in range(n_tokens):\n",
    "        \n",
    "            hook_fn = partial(patch_position, pos=position, clean_cache=clean_cache)\n",
    "            restored_logits = model.run_with_hooks(\n",
    "                                                corrupted_tokens,\n",
    "                                                fwd_hooks = [(patch_name,hook_fn)],\n",
    "                                                return_type=\"logits\"\n",
    "            )\n",
    "            restored_logits = restored_logits[:,-1,:]\n",
    "            mle_logit = (restored_logits.gather(dim=-1, index=mle_token_idx) - restored_logits.mean(dim=-1, keepdim=True)).to(\"cpu\")\n",
    "            target_logit = (restored_logits.gather(dim=-1, index=target_token_idx) - restored_logits.mean(dim=-1, keepdim=True)).to(\"cpu\")\n",
    "            mle_logit = mle_logit.mean(0) ; target_logit = target_logit.mean(0)\n",
    "\n",
    "            total_restored_mle_residual_position_logits[layer, position] = mle_logit\n",
    "            total_restored_target_residual_position_logits[layer, position] = target_logit\n",
    "            total_restored_mle_position_logits[layer, position] = restored_mle_residual_logits.to(\"cpu\")\n",
    "            total_restored_target_position_logits[layer, position] = restored_target_residual_logits.to(\"cpu\")\n",
    "        \n",
    "\n",
    "    return {\n",
    "        \"layer_names\" : layer_names,\n",
    "        \"clean_mle_residual_logits\" : clean_mle_residual_logits.to(\"cpu\"),\n",
    "        \"clean_target_residual_logits\" : clean_target_residual_logits.to(\"cpu\"),\n",
    "        \"corrupted_mle_residual_logits\" : corrupted_mle_residual_logits.to(\"cpu\"),\n",
    "        \"corrupted_target_residual_logits\" : corrupted_target_residual_logits.to(\"cpu\"),\n",
    "        \"restored_mle_residual_logits\" : total_restored_mle_residual_logits.to(\"cpu\"),\n",
    "        \"restored_target_residual_logits\" : total_restored_target_residual_logits.to(\"cpu\"),\n",
    "        \"total_restored_mle_logits\" : total_restored_mle_logits.to(\"cpu\"),\n",
    "        \"total_restored_target_logits\" : total_restored_target_logits.to(\"cpu\"),\n",
    "        \n",
    "        \"total_restored_mle_residual_position_logits\" : total_restored_mle_residual_position_logits.to(\"cpu\"),\n",
    "        \"total_restored_target_residual_position_logits\" : total_restored_target_residual_position_logits.to(\"cpu\"),\n",
    "        \"total_restored_mle_position_logits\" : total_restored_mle_position_logits.to(\"cpu\"),\n",
    "        \"total_restored_target_position_logits\" : total_restored_target_position_logits.to(\"cpu\"),\n",
    "        \n",
    "    \n",
    "         \"clean_mle_logit\":clean_mle_logit,\n",
    "         \"clean_target_logit\":clean_target_logit,\n",
    "         \"corrupted_mle_logit\":corrupted_mle_logit,\n",
    "         \"corrupted_target_logit\":corrupted_target_logit,\n",
    "    \n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9792b17b-a0da-4f86-8149-bfd9e1369763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_experiment(indices, activation_to_ablate): \n",
    "    # indices = list(range(len(dataset)))\n",
    "    # random.shuffle(indices)\n",
    "    # indices = indices[:n]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx in indices: \n",
    "        prompt, subject, target = sample_dataset(dataset, idx=idx)\n",
    "        true_fact, corrupted_facts, target, subject_mask = resample_ablation(prompt, subject, target, n_noise_samples=10)\n",
    "        \n",
    "        print(true_fact)\n",
    "        \n",
    "        result = run_all(\n",
    "            clean_prompt=true_fact, \n",
    "            corrupted_prompts=corrupted_facts, \n",
    "            target=target, \n",
    "        )\n",
    "        result[\"fact\"] = true_fact\n",
    "        result[\"subject_mask\"] = subject_mask\n",
    "        results.append(result)\n",
    "        \n",
    "        \n",
    "        raise\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c729daf-e86d-47bc-9215-fa3e7e64acad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mother tongue of Danielle Darrieux is\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 14.75 GiB total capacity; 13.56 GiB already allocated; 832.00 KiB free; 13.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23637/2672779402.py\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"results/{indices.min()}-{indices.max()}_results.pickle\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_to_ablate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"attn_out\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"saved \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23637/3094530715.py\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(indices, activation_to_ablate)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_fact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         result = run_all(\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mclean_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue_fact\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mcorrupted_prompts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorrupted_facts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_23637/849978898.py\u001b[0m in \u001b[0;36mrun_all\u001b[0;34m(clean_prompt, corrupted_prompts, target)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mfwd_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         ) as hooked_model:\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mrestored_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestored_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhooked_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_with_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrupted_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"logits\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mrestored_mle_residual_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestored_target_residual_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecompose_residual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestored_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmle_token_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_token_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/transformer-circuits/transformer_lens/HookedTransformer.py\u001b[0m in \u001b[0;36mrun_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0mdictionary\u001b[0m \u001b[0mof\u001b[0m \u001b[0mactivations\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mHookedRootModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \"\"\"\n\u001b[0;32m--> 437\u001b[0;31m         out, cache_dict = super().run_with_cache(\n\u001b[0m\u001b[1;32m    438\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_batch_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremove_batch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         )\n",
      "\u001b[0;32m~/SageMaker/transformer-circuits/transformer_lens/hook_points.py\u001b[0m in \u001b[0;36mrun_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0mclear_contexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_contexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         ):\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0mmodel_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mincl_bwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                 \u001b[0mmodel_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/transformer-circuits/transformer_lens/HookedTransformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    364\u001b[0m                 )\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             residual = block(\n\u001b[0m\u001b[1;32m    367\u001b[0m                 \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 \u001b[0mpast_kv_cache_entry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_kv_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/transformer-circuits/transformer_lens/components.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    983\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook_mlp_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresid_mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m             )\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mnormalized_resid_mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             mlp_out = self.hook_mlp_out(\n\u001b[1;32m    987\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_resid_mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/transformer-circuits/transformer_lens/components.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [batch, pos, length]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         scale: Union[\n\u001b[1;32m    225\u001b[0m             \u001b[0mFloat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"batch pos 1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 14.75 GiB total capacity; 13.56 GiB already allocated; 832.00 KiB free; 13.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "index_batches = [np.arange(i, 100+i) for i in [0, 100, 200, 300]]\n",
    "\n",
    "\n",
    "for indices in index_batches: \n",
    "    filename = f\"results/{indices.min()}-{indices.max()}_results.pickle\"\n",
    "    results = run_experiment(indices, activation_to_ablate=\"attn_out\")\n",
    "    save(results, filename)\n",
    "    print(\"saved \", filename)\n",
    "\n",
    "# results_attn_CE = run_experiment_on_delta_ablate(dataset, n=2, activation_to_ablate=\"attn_out\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce27e17-7d7b-4090-b2f5-c6d854662590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
