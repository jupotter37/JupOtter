{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2520ec3-722f-4f44-bdd1-885b13e7afbf",
   "metadata": {
    "id": "c2520ec3-722f-4f44-bdd1-885b13e7afbf"
   },
   "source": [
    "# INSTRUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e19327b-6c02-4881-ad02-9b6d3ec0b1b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4e19327b-6c02-4881-ad02-9b6d3ec0b1b4",
    "outputId": "2bdd4d05-3aa3-4b7e-c478-6a210e8ff722"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.8.3\n",
      "tiktoken version: 0.7.0\n",
      "torch version: 1.13.0+cu116\n",
      "tqdm version: 4.65.0\n",
      "tensorflow version: 2.16.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "from helper_function import *\n",
    "from train import *\n",
    "pkgs = [\n",
    "    \"matplotlib\",  # Plotting library\n",
    "    \"tiktoken\",    # Tokenizer\n",
    "    \"torch\",       # Deep learning library\n",
    "    \"tqdm\",        # Progress bar\n",
    "    \"tensorflow\",  # For OpenAI's pretrained weights\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af8176-4255-4e92-8c7d-998771733eb8",
   "metadata": {
    "id": "d7af8176-4255-4e92-8c7d-998771733eb8"
   },
   "source": [
    "- Each item in the `data` list we loaded from the JSON file above is a dictionary in the following form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaaf606-f913-4445-8301-632ae10d387d",
   "metadata": {
    "id": "fcaaf606-f913-4445-8301-632ae10d387d"
   },
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff7b1544-5f70-482d-b3e4-034fcc61f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use larger dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "OO = load_dataset(\"Open-Orca/OpenOrca\")\n",
    "OO_t = OO['train']\n",
    "OO_30 = OO_t.select(range(int(len(OO_t) * 0.3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db59b0f8-b1d8-4f0f-b2c9-d05261ae97d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(OO_30)\n",
    "OO_30\n",
    "OO_30 = transform_dataset(OO_30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296d3411-6c1d-40dc-ac38-a48083e9c3f2",
   "metadata": {},
   "source": [
    "write our data to json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae6b5f7a-3d52-4cfa-b709-b871c04c5737",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'OO_30.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d97c21a0-1842-42e2-8299-d046332fd4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string = json.dumps(OO_30, indent=2)\n",
    "with open(file_path, 'w', encoding = 'utf-8') as file:\n",
    "    file.write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51bd0ea6-76ad-44a8-ab27-fa6324ecbee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1270176\n"
     ]
    }
   ],
   "source": [
    "# read from json\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc629214-74ae-493f-87fd-ef139c12463b",
   "metadata": {},
   "source": [
    "WE HAVE OUR DATA NOW as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eba4e3af-d041-4279-bb58-1c90b8a499bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1270176/1270176 [00:00<00:00, 4793089.21it/s]\n"
     ]
    }
   ],
   "source": [
    "inst_data = update_instructions(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0199d3d3-041f-4dfc-bfb2-08098f761c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.',\n",
       " 'input': \"You will be given a definition of a task first, then some input of the task.\\nThis task is about using the specified sentence and converting the sentence to Resource Description Framework (RDF) triplets of the form (subject, predicate object). The RDF triplets generated must be such that the triplets accurately capture the structure and semantics of the input sentence. The input is a sentence and the output is a list of triplets of the form [subject, predicate, object] that capture the relationships present in the sentence. When a sentence has more than 1 RDF triplet possible, the output must contain all of them.\\n\\nAFC Ajax (amateurs)'s ground is Sportpark De Toekomst where Ajax Youth Academy also play.\\nOutput:\",\n",
       " 'output': '[\\n  [\"AFC Ajax (amateurs)\", \"has ground\", \"Sportpark De Toekomst\"],\\n  [\"Ajax Youth Academy\", \"plays at\", \"Sportpark De Toekomst\"]\\n]'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inst_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ad3f8-cf30-412d-8366-bd65dfc9ac3f",
   "metadata": {},
   "source": [
    "now all data has instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40824ce3-d287-4a79-b1df-b59edabcb25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_portion = int(len(inst_data) * 0.85)  # 85% for training\n",
    "test_portion = int(len(inst_data) * 0.1)    # 10% for testing\n",
    "val_portion = len(inst_data) - train_portion - test_portion  # Remaining 5% for validation\n",
    "\n",
    "train_data = inst_data[:train_portion]\n",
    "test_data = inst_data[train_portion:train_portion + test_portion]\n",
    "val_data = inst_data[train_portion + test_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "785b8ac7-b5e1-43aa-ad98-3ad55cb4ade6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 1079649\n",
      "Validation set length: 63510\n",
      "Test set length: 127017\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff24fe1a-5746-461c-ad3d-b6d84a1a7c96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff24fe1a-5746-461c-ad3d-b6d84a1a7c96",
    "outputId": "5a7a4a95-4b2b-46a9-d15d-4787edb66849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "etpqqWh8phKc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "etpqqWh8phKc",
    "outputId": "bd1a573e-7a9b-4659-c5ea-0787945c1c25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e47fb30-c2c6-4e6d-a64c-76cc65be4a2c",
   "metadata": {
    "id": "4e47fb30-c2c6-4e6d-a64c-76cc65be4a2c"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff42c29-8b81-45e5-ae8d-b97cd1cf447a",
   "metadata": {
    "id": "8ff42c29-8b81-45e5-ae8d-b97cd1cf447a"
   },
   "source": [
    "- Next, we instantiate the data loaders similar to previous chapters, except that we now provide our own collate function for the batching process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a915803c-58e7-4799-a1b0-fd68ca5b2632",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "BtWkgir6Hlpe",
   "metadata": {
    "id": "BtWkgir6Hlpe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 1079649/1079649 [06:22<00:00, 2821.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a373277-1d70-405f-af0a-39ab7ce1dfb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m      2\u001b[0m     train_dataset,\n\u001b[0;32m----> 3\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m      4\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39mcustomized_collate_fn,\n\u001b[1;32m      5\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39mnum_workers\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d097dc8-ad34-4f05-b435-e4147965f532",
   "metadata": {
    "id": "1d097dc8-ad34-4f05-b435-e4147965f532"
   },
   "outputs": [],
   "source": [
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1644a59-00b9-4c27-a679-b4698b1b971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8488d05-99b7-4551-88cc-a032017cd20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token_ids_to_text(torch.tensor(train_dataset[0]), tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f67c147-b1a2-4a95-9807-e2d0de0324c0",
   "metadata": {
    "id": "3f67c147-b1a2-4a95-9807-e2d0de0324c0"
   },
   "source": [
    "- Let's see what the dimensions of the resulting input and target batches look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "GGs1AI3vHpnX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GGs1AI3vHpnX",
    "outputId": "e9122e9d-c38d-4ac8-e29d-c3c11a15b93c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([4, 829]) torch.Size([4, 829])\n",
      "torch.Size([4, 646]) torch.Size([4, 646])\n",
      "torch.Size([4, 463]) torch.Size([4, 463])\n",
      "torch.Size([4, 572]) torch.Size([4, 572])\n",
      "torch.Size([4, 785]) torch.Size([4, 785])\n",
      "torch.Size([4, 787]) torch.Size([4, 787])\n",
      "torch.Size([4, 658]) torch.Size([4, 658])\n",
      "torch.Size([4, 310]) torch.Size([4, 310])\n",
      "torch.Size([4, 574]) torch.Size([4, 574])\n",
      "torch.Size([4, 891]) torch.Size([4, 891])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "count = 0\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)\n",
    "    count += 1\n",
    "    if count >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e77d8c0-e38a-4899-937f-5ebf31c60743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269912\n",
      "1079648\n",
      "1079649\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "print(4*269912)\n",
    "print(len(train_data))\n",
    "# batch size 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73afbf7a-39b8-4b20-989f-4fc9a8c0c2c6",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d249d67-5eba-414e-9bd2-972ebf01329d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0d249d67-5eba-414e-9bd2-972ebf01329d",
    "outputId": "d58ae26d-ff8f-4c66-bc84-e1ae3429ce51"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 18:32:55.966487: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-16 18:33:39.786799: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "from models_1 import *\n",
    "from load_gpt2 import *\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size, \n",
    "    models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf3afed-bc8e-4d3a-ad9d-eb6f57bb7af5",
   "metadata": {
    "id": "dbf3afed-bc8e-4d3a-ad9d-eb6f57bb7af5"
   },
   "source": [
    "- Before we start finetuning the model in the next section, let's see how it performs on one of the validation tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bd32b7c-5b44-4d25-a09f-46836802ca74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7bd32b7c-5b44-4d25-a09f-46836802ca74",
    "outputId": "6dcedbe5-5c9c-4d5a-8524-6346b3185b97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "You are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.\n",
      "\n",
      "### Input:\n",
      "Given the question: Given the following passage  \"The K-12 and higher education reform programs of the Gates Foundation have been criticized by some education professionals, parents, and researchers because they have driven the conversation on education reform to such an extent that they may marginalize researchers who do not support Gates' predetermined policy preferences. Several Gates-backed policies such as small schools, charter schools, and increasing class sizes have been expensive and disruptive, but some studies indicate they have not improved educational outcomes and may have caused harm. Peer reviewed scientific studies at Stanford find that Charter Schools do not systematically improve student performance\",  answer the following question. Note that the answer is present within the text.  Question: What has been a criticism of one of Gates' policies?\n",
      "The answer is:\n"
     ]
    }
   ],
   "source": [
    "#torch.manual_seed(123)\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "457bf08f-6457-4dc2-9554-6af3e52f4794",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "generated_text = text_to_text(model, tokenizer, device, input_text, temperature=1, top_k=None, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba4a55bf-a245-48d8-beda-2838a58fb5ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ba4a55bf-a245-48d8-beda-2838a58fb5ba",
    "outputId": "9c1921a3-becc-4a30-c899-ce039c3fdcf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as has the Education Foundation. \"\"Author (and former Gates' student admirming his frequent visit at\n"
     ]
    }
   ],
   "source": [
    "response_text = (\n",
    "    generated_text[len(input_text):]\n",
    "    .replace(\"### Response:\", \"\")\n",
    "    .strip()\n",
    ")\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44080b2-a4c5-4520-a797-549519f66a3e",
   "metadata": {
    "id": "d44080b2-a4c5-4520-a797-549519f66a3e"
   },
   "source": [
    "- As we can see, the model is not capable of following the instructions, yet; it creates a \"Response\" section but it simply repeats the original input sentence as well as the instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d27b9d-a942-4cf5-b797-848c5f01e723",
   "metadata": {
    "id": "70d27b9d-a942-4cf5-b797-848c5f01e723"
   },
   "source": [
    "## FINETUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd062a0-18cb-4f11-a1a5-4136ea8b2c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_DDP_main(model, train_loader, val_loader, optimizer, device, n_epochs,\n",
    "                eval_freq, eval_iter, start_context, warmup_steps=10,\n",
    "                initial_lr=3e-05, min_lr=1e-6):\n",
    "    \n",
    "    # Initialize distributed training\n",
    "    dist.init_process_group(backend='nccl')\n",
    "    local_rank = dist.get_rank()\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    device = torch.device(\"cuda\", local_rank)\n",
    "    \n",
    "    # Wrap model in DDP\n",
    "    model = model.to(device)\n",
    "    model = DDP(model, device_ids=[local_rank])\n",
    "\n",
    "    train_losses, val_losses, track_tokens_seen, track_lrs = [], [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    peak_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    total_training_steps = len(train_loader) * n_epochs\n",
    "    lr_increment = (peak_lr - initial_lr) / warmup_steps\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step < warmup_steps:\n",
    "                lr = initial_lr + global_step * lr_increment  \n",
    "            else:\n",
    "                progress = ((global_step - warmup_steps) / \n",
    "                            (total_training_steps - warmup_steps))\n",
    "                lr = min_lr + (peak_lr - min_lr) * 0.5 * (\n",
    "                    1 + math.cos(math.pi * progress))\n",
    "\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = lr\n",
    "            track_lrs.append(lr)\n",
    "            \n",
    "            input_batch = input_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "\n",
    "            if global_step > warmup_steps:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step() \n",
    "            tokens_seen += input_batch.numel()\n",
    "\n",
    "            if global_step % eval_freq == 0 and local_rank == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model.module, train_loader, val_loader,\n",
    "                    device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Iter {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        if local_rank == 0:\n",
    "            generate_and_print_sample(\n",
    "                model.module, train_loader.dataset.tokenizer,\n",
    "                device, start_context\n",
    "            )\n",
    "\n",
    "    # Clean up\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen, track_lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d99fc6f8-63b2-43da-adbb-a7b6b92c8dd5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d99fc6f8-63b2-43da-adbb-a7b6b92c8dd5",
    "outputId": "1ceb796a-b615-46b4-f1f0-661b6d53f9fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.890094156265259\n",
      "Validation loss: 2.9420854473114013\n"
     ]
    }
   ],
   "source": [
    "#model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=50)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=50)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4b57fb-e689-4550-931c-6d34a932487c",
   "metadata": {
    "id": "db4b57fb-e689-4550-931c-6d34a932487c"
   },
   "source": [
    "<div style=\"text-align: left;\">\n",
    "    \n",
    "| Model              | Device                | Runtime for 2 Epochs |\n",
    "|--------------------|-----------------------|----------------------|\n",
    "| gpt2-medium (355M) | CPU (M3 MacBook Air)  | 15.78 minutes        |\n",
    "| gpt2-medium (355M) | GPU (M3 MacBook Air)  | 10.77 minutes        |\n",
    "| gpt2-medium (355M) | GPU (L4)              | 1.83 minutes         |\n",
    "| gpt2-medium (355M) | GPU (A100)            | 0.86 minutes         |\n",
    "| gpt2-small (124M)  | CPU (M3 MacBook Air)  | 5.74 minutes         |\n",
    "| gpt2-small (124M)  | GPU (M3 MacBook Air)  | 3.73 minutes         |\n",
    "| gpt2-small (124M)  | GPU (L4)              | 0.69 minutes         |\n",
    "| gpt2-small (124M)  | GPU (A100)            | 0.39 minutes         |\n",
    "\n",
    "</div>\n",
    "\n",
    "- I ran this notebook using the `\"gpt2-medium (355M)\"` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78bcf83a-1fff-4540-97c1-765c4016d5e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78bcf83a-1fff-4540-97c1-765c4016d5e3",
    "outputId": "f978a643-6307-4e30-9fdc-66d9db978fa1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinran/anaconda3/envs/LLM_task/lib/python3.11/site-packages/transformers/utils/generic.py:482: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.566, Val loss 2.832\n",
      "Ep 1 (Step 000005): Train loss 2.421, Val loss 2.537\n",
      "Ep 1 (Step 000010): Train loss 2.124, Val loss 2.436\n",
      "Ep 1 (Step 000015): Train loss 2.306, Val loss 2.369\n",
      "Ep 1 (Step 000020): Train loss 2.233, Val loss 2.329\n",
      "Ep 1 (Step 000025): Train loss 2.035, Val loss 2.303\n",
      "Ep 1 (Step 000030): Train loss 2.093, Val loss 2.294\n",
      "Ep 1 (Step 000035): Train loss 2.132, Val loss 2.272\n",
      "Ep 1 (Step 000040): Train loss 2.100, Val loss 2.270\n",
      "Ep 1 (Step 000045): Train loss 2.276, Val loss 2.263\n",
      "Ep 1 (Step 000050): Train loss 1.964, Val loss 2.250\n",
      "Ep 1 (Step 000055): Train loss 2.026, Val loss 2.247\n",
      "Ep 1 (Step 000060): Train loss 1.998, Val loss 2.235\n",
      "Ep 1 (Step 000065): Train loss 2.034, Val loss 2.232\n",
      "Ep 1 (Step 000070): Train loss 2.368, Val loss 2.228\n",
      "Ep 1 (Step 000075): Train loss 2.084, Val loss 2.218\n",
      "Ep 1 (Step 000080): Train loss 2.026, Val loss 2.212\n",
      "Ep 1 (Step 000085): Train loss 2.199, Val loss 2.210\n",
      "Ep 1 (Step 000090): Train loss 1.900, Val loss 2.212\n",
      "Ep 1 (Step 000095): Train loss 1.926, Val loss 2.210\n",
      "Ep 1 (Step 000100): Train loss 1.935, Val loss 2.208\n",
      "Ep 1 (Step 000105): Train loss 2.161, Val loss 2.206\n",
      "Ep 1 (Step 000110): Train loss 1.926, Val loss 2.202\n",
      "Ep 1 (Step 000115): Train loss 2.002, Val loss 2.195\n",
      "Ep 1 (Step 000120): Train loss 1.978, Val loss 2.191\n",
      "Ep 1 (Step 000125): Train loss 1.986, Val loss 2.187\n",
      "Ep 1 (Step 000130): Train loss 2.077, Val loss 2.188\n",
      "Ep 1 (Step 000135): Train loss 2.137, Val loss 2.180\n",
      "Ep 1 (Step 000140): Train loss 1.912, Val loss 2.173\n",
      "Ep 1 (Step 000145): Train loss 1.951, Val loss 2.169\n",
      "Ep 1 (Step 000150): Train loss 2.022, Val loss 2.166\n",
      "Ep 1 (Step 000155): Train loss 2.082, Val loss 2.162\n",
      "Ep 1 (Step 000160): Train loss 2.239, Val loss 2.162\n",
      "Ep 1 (Step 000165): Train loss 1.906, Val loss 2.158\n",
      "Ep 1 (Step 000170): Train loss 1.833, Val loss 2.150\n",
      "Ep 1 (Step 000175): Train loss 2.109, Val loss 2.146\n",
      "Ep 1 (Step 000180): Train loss 1.908, Val loss 2.145\n",
      "Ep 1 (Step 000185): Train loss 2.070, Val loss 2.139\n",
      "Ep 1 (Step 000190): Train loss 2.144, Val loss 2.140\n",
      "Ep 1 (Step 000195): Train loss 2.095, Val loss 2.140\n",
      "Ep 1 (Step 000200): Train loss 2.195, Val loss 2.137\n",
      "Ep 1 (Step 000205): Train loss 2.002, Val loss 2.131\n",
      "Ep 1 (Step 000210): Train loss 1.802, Val loss 2.132\n",
      "Ep 1 (Step 000215): Train loss 2.042, Val loss 2.133\n",
      "Ep 1 (Step 000220): Train loss 2.043, Val loss 2.131\n",
      "Ep 1 (Step 000225): Train loss 1.979, Val loss 2.133\n",
      "Ep 1 (Step 000230): Train loss 1.910, Val loss 2.133\n",
      "Ep 1 (Step 000235): Train loss 1.906, Val loss 2.131\n",
      "Ep 1 (Step 000240): Train loss 2.023, Val loss 2.134\n",
      "Ep 1 (Step 000245): Train loss 1.724, Val loss 2.131\n",
      "Ep 1 (Step 000250): Train loss 1.818, Val loss 2.132\n",
      "Ep 1 (Step 000255): Train loss 2.056, Val loss 2.131\n",
      "Ep 1 (Step 000260): Train loss 1.926, Val loss 2.131\n",
      "Ep 1 (Step 000265): Train loss 1.945, Val loss 2.128\n",
      "Ep 1 (Step 000270): Train loss 2.211, Val loss 2.128\n",
      "Ep 1 (Step 000275): Train loss 2.242, Val loss 2.128\n",
      "Ep 1 (Step 000280): Train loss 1.935, Val loss 2.128\n",
      "Ep 1 (Step 000285): Train loss 2.187, Val loss 2.128\n",
      "Ep 1 (Step 000290): Train loss 1.997, Val loss 2.127\n",
      "Ep 1 (Step 000295): Train loss 2.061, Val loss 2.119\n",
      "Ep 1 (Step 000300): Train loss 2.026, Val loss 2.116\n",
      "Ep 1 (Step 000305): Train loss 1.845, Val loss 2.112\n",
      "Ep 1 (Step 000310): Train loss 1.923, Val loss 2.110\n",
      "Ep 1 (Step 000315): Train loss 1.888, Val loss 2.110\n",
      "Ep 1 (Step 000320): Train loss 1.863, Val loss 2.111\n",
      "Ep 1 (Step 000325): Train loss 2.002, Val loss 2.107\n",
      "Ep 1 (Step 000330): Train loss 1.904, Val loss 2.105\n",
      "Ep 1 (Step 000335): Train loss 2.048, Val loss 2.105\n",
      "Ep 1 (Step 000340): Train loss 1.979, Val loss 2.104\n",
      "Ep 1 (Step 000345): Train loss 2.119, Val loss 2.102\n",
      "Ep 1 (Step 000350): Train loss 2.182, Val loss 2.102\n",
      "Ep 1 (Step 000355): Train loss 2.112, Val loss 2.100\n",
      "Ep 1 (Step 000360): Train loss 2.008, Val loss 2.098\n",
      "Ep 1 (Step 000365): Train loss 1.812, Val loss 2.099\n",
      "Ep 1 (Step 000370): Train loss 2.237, Val loss 2.098\n",
      "Ep 1 (Step 000375): Train loss 1.927, Val loss 2.100\n",
      "Ep 1 (Step 000380): Train loss 1.911, Val loss 2.100\n",
      "Ep 1 (Step 000385): Train loss 1.896, Val loss 2.097\n",
      "Ep 1 (Step 000390): Train loss 1.961, Val loss 2.094\n",
      "Ep 1 (Step 000395): Train loss 1.915, Val loss 2.092\n",
      "Ep 1 (Step 000400): Train loss 2.076, Val loss 2.090\n",
      "Ep 1 (Step 000405): Train loss 1.917, Val loss 2.087\n",
      "Ep 1 (Step 000410): Train loss 1.852, Val loss 2.086\n",
      "Ep 1 (Step 000415): Train loss 1.951, Val loss 2.087\n",
      "Ep 1 (Step 000420): Train loss 1.987, Val loss 2.089\n",
      "Ep 1 (Step 000425): Train loss 1.819, Val loss 2.092\n",
      "Ep 1 (Step 000430): Train loss 1.859, Val loss 2.090\n",
      "Ep 1 (Step 000435): Train loss 1.796, Val loss 2.086\n",
      "Ep 1 (Step 000440): Train loss 1.876, Val loss 2.086\n",
      "Ep 1 (Step 000445): Train loss 1.933, Val loss 2.090\n",
      "Ep 1 (Step 000450): Train loss 1.641, Val loss 2.089\n",
      "Ep 1 (Step 000455): Train loss 2.018, Val loss 2.090\n",
      "Ep 1 (Step 000460): Train loss 1.963, Val loss 2.085\n",
      "Ep 1 (Step 000465): Train loss 2.283, Val loss 2.081\n",
      "Ep 1 (Step 000470): Train loss 2.025, Val loss 2.079\n",
      "Ep 1 (Step 000475): Train loss 2.094, Val loss 2.081\n",
      "Ep 1 (Step 000480): Train loss 1.952, Val loss 2.084\n",
      "Ep 1 (Step 000485): Train loss 2.204, Val loss 2.084\n",
      "Ep 1 (Step 000490): Train loss 2.318, Val loss 2.087\n",
      "Ep 1 (Step 000495): Train loss 2.053, Val loss 2.090\n",
      "Ep 1 (Step 000500): Train loss 1.943, Val loss 2.091\n",
      "Ep 1 (Step 000505): Train loss 1.818, Val loss 2.091\n",
      "Ep 1 (Step 000510): Train loss 1.759, Val loss 2.088\n",
      "Ep 1 (Step 000515): Train loss 2.088, Val loss 2.088\n",
      "Ep 1 (Step 000520): Train loss 1.575, Val loss 2.088\n",
      "Ep 1 (Step 000525): Train loss 1.935, Val loss 2.084\n",
      "Ep 1 (Step 000530): Train loss 2.104, Val loss 2.081\n",
      "Ep 1 (Step 000535): Train loss 1.819, Val loss 2.081\n",
      "Ep 1 (Step 000540): Train loss 1.909, Val loss 2.083\n",
      "Ep 1 (Step 000545): Train loss 2.140, Val loss 2.086\n",
      "Ep 1 (Step 000550): Train loss 1.829, Val loss 2.085\n",
      "Ep 1 (Step 000555): Train loss 1.864, Val loss 2.082\n",
      "Ep 1 (Step 000560): Train loss 1.903, Val loss 2.080\n",
      "Ep 1 (Step 000565): Train loss 1.929, Val loss 2.079\n",
      "Ep 1 (Step 000570): Train loss 2.043, Val loss 2.082\n",
      "Ep 1 (Step 000575): Train loss 2.040, Val loss 2.082\n",
      "Ep 1 (Step 000580): Train loss 1.854, Val loss 2.078\n",
      "Ep 1 (Step 000585): Train loss 1.794, Val loss 2.074\n",
      "Ep 1 (Step 000590): Train loss 2.006, Val loss 2.076\n",
      "Ep 1 (Step 000595): Train loss 1.992, Val loss 2.077\n",
      "Ep 1 (Step 000600): Train loss 1.898, Val loss 2.077\n",
      "Ep 1 (Step 000605): Train loss 1.813, Val loss 2.075\n",
      "Ep 1 (Step 000610): Train loss 2.116, Val loss 2.072\n",
      "Ep 1 (Step 000615): Train loss 1.933, Val loss 2.070\n",
      "Ep 1 (Step 000620): Train loss 2.165, Val loss 2.071\n",
      "Ep 1 (Step 000625): Train loss 2.027, Val loss 2.072\n",
      "Ep 1 (Step 000630): Train loss 1.856, Val loss 2.069\n",
      "Ep 1 (Step 000635): Train loss 1.712, Val loss 2.071\n",
      "Ep 1 (Step 000640): Train loss 2.006, Val loss 2.075\n",
      "Ep 1 (Step 000645): Train loss 1.766, Val loss 2.080\n",
      "Ep 1 (Step 000650): Train loss 1.744, Val loss 2.076\n",
      "Ep 1 (Step 000655): Train loss 2.450, Val loss 2.072\n",
      "Ep 1 (Step 000660): Train loss 1.832, Val loss 2.070\n",
      "Ep 1 (Step 000665): Train loss 1.877, Val loss 2.067\n",
      "Ep 1 (Step 000670): Train loss 1.790, Val loss 2.064\n",
      "Ep 1 (Step 000675): Train loss 1.698, Val loss 2.066\n",
      "Ep 1 (Step 000680): Train loss 2.358, Val loss 2.065\n",
      "Ep 1 (Step 000685): Train loss 1.801, Val loss 2.065\n",
      "Ep 1 (Step 000690): Train loss 1.965, Val loss 2.063\n",
      "Ep 1 (Step 000695): Train loss 1.977, Val loss 2.065\n",
      "Ep 1 (Step 000700): Train loss 1.975, Val loss 2.064\n",
      "Ep 1 (Step 000705): Train loss 1.730, Val loss 2.063\n",
      "Ep 1 (Step 000710): Train loss 1.670, Val loss 2.062\n",
      "Ep 1 (Step 000715): Train loss 1.959, Val loss 2.064\n",
      "Ep 1 (Step 000720): Train loss 2.047, Val loss 2.064\n",
      "Ep 1 (Step 000725): Train loss 1.800, Val loss 2.063\n",
      "Ep 1 (Step 000730): Train loss 2.055, Val loss 2.063\n",
      "Ep 1 (Step 000735): Train loss 1.698, Val loss 2.062\n",
      "Ep 1 (Step 000740): Train loss 1.724, Val loss 2.062\n",
      "Ep 1 (Step 000745): Train loss 1.807, Val loss 2.061\n",
      "Ep 1 (Step 000750): Train loss 2.104, Val loss 2.058\n",
      "Ep 1 (Step 000755): Train loss 2.084, Val loss 2.056\n",
      "Ep 1 (Step 000760): Train loss 1.746, Val loss 2.053\n",
      "Ep 1 (Step 000765): Train loss 1.987, Val loss 2.052\n",
      "Ep 1 (Step 000770): Train loss 1.889, Val loss 2.054\n",
      "Ep 1 (Step 000775): Train loss 2.111, Val loss 2.054\n",
      "Ep 1 (Step 000780): Train loss 1.934, Val loss 2.057\n",
      "Ep 1 (Step 000785): Train loss 1.584, Val loss 2.057\n",
      "Ep 1 (Step 000790): Train loss 1.856, Val loss 2.057\n",
      "Ep 1 (Step 000795): Train loss 1.993, Val loss 2.057\n",
      "Ep 1 (Step 000800): Train loss 1.885, Val loss 2.052\n",
      "Ep 1 (Step 000805): Train loss 1.930, Val loss 2.038\n",
      "Ep 1 (Step 000810): Train loss 1.878, Val loss 2.030\n",
      "Ep 1 (Step 000815): Train loss 2.002, Val loss 2.027\n",
      "Ep 1 (Step 000820): Train loss 1.954, Val loss 2.025\n",
      "Ep 1 (Step 000825): Train loss 2.121, Val loss 2.025\n",
      "Ep 1 (Step 000830): Train loss 2.018, Val loss 2.022\n",
      "Ep 1 (Step 000835): Train loss 1.825, Val loss 2.021\n",
      "Ep 1 (Step 000840): Train loss 1.800, Val loss 2.025\n",
      "Ep 1 (Step 000845): Train loss 2.081, Val loss 2.024\n",
      "Ep 1 (Step 000850): Train loss 1.817, Val loss 2.020\n",
      "Ep 1 (Step 000855): Train loss 1.825, Val loss 2.019\n",
      "Ep 1 (Step 000860): Train loss 2.024, Val loss 2.019\n",
      "Ep 1 (Step 000865): Train loss 1.764, Val loss 2.020\n",
      "Ep 1 (Step 000870): Train loss 1.816, Val loss 2.021\n",
      "Ep 1 (Step 000875): Train loss 1.957, Val loss 2.022\n",
      "Ep 1 (Step 000880): Train loss 2.025, Val loss 2.022\n",
      "Ep 1 (Step 000885): Train loss 1.811, Val loss 2.021\n",
      "Ep 1 (Step 000890): Train loss 1.878, Val loss 2.021\n",
      "Ep 1 (Step 000895): Train loss 1.939, Val loss 2.021\n",
      "Ep 1 (Step 000900): Train loss 1.685, Val loss 2.022\n",
      "Ep 1 (Step 000905): Train loss 1.881, Val loss 2.020\n",
      "Ep 1 (Step 000910): Train loss 2.191, Val loss 2.015\n",
      "Ep 1 (Step 000915): Train loss 1.974, Val loss 2.014\n",
      "Ep 1 (Step 000920): Train loss 1.675, Val loss 2.014\n",
      "Ep 1 (Step 000925): Train loss 1.975, Val loss 2.015\n",
      "Ep 1 (Step 000930): Train loss 1.983, Val loss 2.017\n",
      "Ep 1 (Step 000935): Train loss 1.649, Val loss 2.021\n",
      "Ep 1 (Step 000940): Train loss 1.882, Val loss 2.016\n",
      "Ep 1 (Step 000945): Train loss 2.035, Val loss 2.010\n",
      "Ep 1 (Step 000950): Train loss 1.774, Val loss 2.006\n",
      "Ep 1 (Step 000955): Train loss 2.031, Val loss 2.003\n",
      "Ep 1 (Step 000960): Train loss 1.889, Val loss 2.005\n",
      "Ep 1 (Step 000965): Train loss 1.774, Val loss 2.008\n",
      "Ep 1 (Step 000970): Train loss 2.045, Val loss 2.011\n",
      "Ep 1 (Step 000975): Train loss 2.016, Val loss 2.011\n",
      "Ep 1 (Step 000980): Train loss 1.578, Val loss 2.015\n",
      "Ep 1 (Step 000985): Train loss 1.856, Val loss 2.016\n",
      "Ep 1 (Step 000990): Train loss 2.199, Val loss 2.014\n",
      "Ep 1 (Step 000995): Train loss 1.825, Val loss 2.012\n",
      "Ep 1 (Step 001000): Train loss 1.790, Val loss 2.010\n",
      "Ep 1 (Step 001005): Train loss 1.770, Val loss 2.008\n",
      "Ep 1 (Step 001010): Train loss 1.686, Val loss 2.008\n",
      "Ep 1 (Step 001015): Train loss 1.712, Val loss 2.011\n",
      "Ep 1 (Step 001020): Train loss 1.903, Val loss 2.014\n",
      "Ep 1 (Step 001025): Train loss 1.928, Val loss 2.014\n",
      "Ep 1 (Step 001030): Train loss 2.188, Val loss 2.012\n",
      "Ep 1 (Step 001035): Train loss 1.880, Val loss 2.011\n",
      "Ep 1 (Step 001040): Train loss 1.912, Val loss 2.011\n",
      "Ep 1 (Step 001045): Train loss 1.697, Val loss 2.013\n",
      "Ep 1 (Step 001050): Train loss 1.925, Val loss 2.017\n",
      "Ep 1 (Step 001055): Train loss 1.923, Val loss 2.020\n",
      "Ep 1 (Step 001060): Train loss 1.862, Val loss 2.019\n",
      "Ep 1 (Step 001065): Train loss 1.689, Val loss 2.015\n",
      "Ep 1 (Step 001070): Train loss 1.905, Val loss 2.013\n",
      "Ep 1 (Step 001075): Train loss 1.827, Val loss 2.012\n",
      "Ep 1 (Step 001080): Train loss 1.598, Val loss 2.011\n",
      "Ep 1 (Step 001085): Train loss 1.840, Val loss 2.012\n",
      "Ep 1 (Step 001090): Train loss 1.835, Val loss 2.012\n",
      "Ep 1 (Step 001095): Train loss 2.000, Val loss 2.013\n",
      "Ep 1 (Step 001100): Train loss 1.828, Val loss 2.013\n",
      "Ep 1 (Step 001105): Train loss 1.947, Val loss 2.010\n",
      "Ep 1 (Step 001110): Train loss 1.612, Val loss 2.009\n",
      "Ep 1 (Step 001115): Train loss 1.907, Val loss 2.008\n",
      "Ep 1 (Step 001120): Train loss 1.949, Val loss 2.009\n",
      "Ep 1 (Step 001125): Train loss 2.001, Val loss 2.007\n",
      "Ep 1 (Step 001130): Train loss 1.962, Val loss 2.005\n",
      "Ep 1 (Step 001135): Train loss 1.763, Val loss 2.004\n",
      "Ep 1 (Step 001140): Train loss 1.718, Val loss 2.006\n",
      "Ep 1 (Step 001145): Train loss 1.929, Val loss 2.007\n",
      "Ep 1 (Step 001150): Train loss 1.885, Val loss 2.008\n",
      "Ep 1 (Step 001155): Train loss 1.812, Val loss 2.010\n",
      "Ep 1 (Step 001160): Train loss 2.079, Val loss 2.010\n",
      "Ep 1 (Step 001165): Train loss 1.946, Val loss 2.008\n",
      "Ep 1 (Step 001170): Train loss 1.689, Val loss 2.009\n",
      "Ep 1 (Step 001175): Train loss 1.960, Val loss 2.010\n",
      "Ep 1 (Step 001180): Train loss 1.794, Val loss 2.009\n",
      "Ep 1 (Step 001185): Train loss 1.975, Val loss 2.005\n",
      "Ep 1 (Step 001190): Train loss 1.840, Val loss 2.002\n",
      "Ep 1 (Step 001195): Train loss 1.862, Val loss 2.002\n",
      "Ep 1 (Step 001200): Train loss 1.945, Val loss 2.003\n",
      "Ep 1 (Step 001205): Train loss 2.018, Val loss 2.004\n",
      "Ep 1 (Step 001210): Train loss 1.793, Val loss 2.005\n",
      "Ep 1 (Step 001215): Train loss 1.816, Val loss 2.004\n",
      "Ep 1 (Step 001220): Train loss 1.601, Val loss 2.003\n",
      "Ep 1 (Step 001225): Train loss 2.020, Val loss 2.002\n",
      "Ep 1 (Step 001230): Train loss 2.181, Val loss 2.001\n",
      "Ep 1 (Step 001235): Train loss 2.012, Val loss 2.003\n",
      "Ep 1 (Step 001240): Train loss 1.924, Val loss 2.002\n",
      "Ep 1 (Step 001245): Train loss 2.254, Val loss 1.997\n",
      "Ep 1 (Step 001250): Train loss 1.862, Val loss 1.995\n",
      "Ep 1 (Step 001255): Train loss 1.929, Val loss 1.996\n",
      "Ep 1 (Step 001260): Train loss 1.765, Val loss 2.000\n",
      "Ep 1 (Step 001265): Train loss 1.786, Val loss 1.999\n",
      "Ep 1 (Step 001270): Train loss 1.718, Val loss 1.998\n",
      "Ep 1 (Step 001275): Train loss 1.805, Val loss 1.997\n",
      "Ep 1 (Step 001280): Train loss 2.156, Val loss 1.997\n",
      "Ep 1 (Step 001285): Train loss 1.988, Val loss 1.997\n",
      "Ep 1 (Step 001290): Train loss 1.872, Val loss 1.999\n",
      "Ep 1 (Step 001295): Train loss 2.054, Val loss 1.999\n",
      "Ep 1 (Step 001300): Train loss 1.778, Val loss 2.000\n",
      "Ep 1 (Step 001305): Train loss 1.760, Val loss 1.999\n",
      "Ep 1 (Step 001310): Train loss 1.962, Val loss 1.999\n",
      "Ep 1 (Step 001315): Train loss 2.008, Val loss 1.998\n",
      "Ep 1 (Step 001320): Train loss 1.927, Val loss 1.997\n",
      "Ep 1 (Step 001325): Train loss 1.870, Val loss 1.996\n",
      "Ep 1 (Step 001330): Train loss 1.992, Val loss 1.996\n",
      "Ep 1 (Step 001335): Train loss 1.961, Val loss 2.000\n",
      "Ep 1 (Step 001340): Train loss 1.973, Val loss 2.002\n",
      "Ep 1 (Step 001345): Train loss 1.899, Val loss 2.001\n",
      "Ep 1 (Step 001350): Train loss 1.798, Val loss 2.002\n",
      "Ep 1 (Step 001355): Train loss 1.742, Val loss 2.002\n",
      "Ep 1 (Step 001360): Train loss 1.957, Val loss 2.000\n",
      "Ep 1 (Step 001365): Train loss 1.953, Val loss 1.997\n",
      "Ep 1 (Step 001370): Train loss 1.677, Val loss 1.996\n",
      "Ep 1 (Step 001375): Train loss 1.821, Val loss 1.999\n",
      "Ep 1 (Step 001380): Train loss 1.831, Val loss 2.002\n",
      "Ep 1 (Step 001385): Train loss 1.997, Val loss 2.002\n",
      "Ep 1 (Step 001390): Train loss 2.053, Val loss 2.000\n",
      "Ep 1 (Step 001395): Train loss 1.951, Val loss 1.998\n",
      "Ep 1 (Step 001400): Train loss 2.046, Val loss 2.000\n",
      "Ep 1 (Step 001405): Train loss 1.887, Val loss 2.002\n",
      "Ep 1 (Step 001410): Train loss 1.845, Val loss 2.000\n",
      "Ep 1 (Step 001415): Train loss 1.689, Val loss 2.000\n",
      "Ep 1 (Step 001420): Train loss 1.864, Val loss 1.997\n",
      "Ep 1 (Step 001425): Train loss 2.053, Val loss 1.998\n",
      "Ep 1 (Step 001430): Train loss 1.793, Val loss 1.999\n",
      "Ep 1 (Step 001435): Train loss 1.804, Val loss 1.998\n",
      "Ep 1 (Step 001440): Train loss 1.824, Val loss 1.998\n",
      "Ep 1 (Step 001445): Train loss 1.710, Val loss 1.999\n",
      "Ep 1 (Step 001450): Train loss 1.709, Val loss 1.998\n",
      "Ep 1 (Step 001455): Train loss 1.987, Val loss 2.001\n",
      "Ep 1 (Step 001460): Train loss 1.688, Val loss 2.000\n",
      "Ep 1 (Step 001465): Train loss 1.731, Val loss 1.998\n",
      "Ep 1 (Step 001470): Train loss 1.852, Val loss 1.995\n",
      "Ep 1 (Step 001475): Train loss 1.947, Val loss 1.997\n",
      "Ep 1 (Step 001480): Train loss 1.791, Val loss 1.997\n",
      "Ep 1 (Step 001485): Train loss 1.391, Val loss 1.995\n",
      "Ep 1 (Step 001490): Train loss 1.911, Val loss 1.996\n",
      "Ep 1 (Step 001495): Train loss 1.734, Val loss 1.994\n",
      "Ep 1 (Step 001500): Train loss 1.651, Val loss 1.992\n",
      "Ep 1 (Step 001505): Train loss 1.871, Val loss 1.989\n",
      "Ep 1 (Step 001510): Train loss 1.871, Val loss 1.987\n",
      "Ep 1 (Step 001515): Train loss 1.584, Val loss 1.988\n",
      "Ep 1 (Step 001520): Train loss 1.690, Val loss 1.989\n",
      "Ep 1 (Step 001525): Train loss 1.873, Val loss 1.990\n",
      "Ep 1 (Step 001530): Train loss 1.862, Val loss 1.996\n",
      "Ep 1 (Step 001535): Train loss 1.818, Val loss 1.995\n",
      "Ep 1 (Step 001540): Train loss 1.891, Val loss 1.993\n",
      "Ep 1 (Step 001545): Train loss 2.140, Val loss 1.992\n",
      "Ep 1 (Step 001550): Train loss 1.478, Val loss 1.993\n",
      "Ep 1 (Step 001555): Train loss 2.003, Val loss 1.995\n",
      "Ep 1 (Step 001560): Train loss 2.006, Val loss 1.998\n",
      "Ep 1 (Step 001565): Train loss 2.013, Val loss 1.996\n",
      "Ep 1 (Step 001570): Train loss 1.728, Val loss 1.992\n",
      "Ep 1 (Step 001575): Train loss 1.962, Val loss 1.989\n",
      "Ep 1 (Step 001580): Train loss 1.883, Val loss 1.994\n",
      "Ep 1 (Step 001585): Train loss 1.655, Val loss 1.997\n",
      "Ep 1 (Step 001590): Train loss 1.833, Val loss 1.999\n",
      "Ep 1 (Step 001595): Train loss 2.072, Val loss 1.998\n",
      "Ep 1 (Step 001600): Train loss 1.857, Val loss 1.996\n",
      "Ep 1 (Step 001605): Train loss 1.970, Val loss 1.995\n",
      "Ep 1 (Step 001610): Train loss 1.978, Val loss 1.994\n",
      "Ep 1 (Step 001615): Train loss 1.629, Val loss 1.992\n",
      "Ep 1 (Step 001620): Train loss 1.788, Val loss 1.993\n",
      "Ep 1 (Step 001625): Train loss 1.911, Val loss 1.992\n",
      "Ep 1 (Step 001630): Train loss 1.674, Val loss 1.997\n",
      "Ep 1 (Step 001635): Train loss 1.910, Val loss 2.001\n",
      "Ep 1 (Step 001640): Train loss 1.719, Val loss 1.996\n",
      "Ep 1 (Step 001645): Train loss 2.063, Val loss 1.994\n",
      "Ep 1 (Step 001650): Train loss 1.877, Val loss 1.992\n",
      "Ep 1 (Step 001655): Train loss 1.704, Val loss 1.990\n",
      "Ep 1 (Step 001660): Train loss 1.804, Val loss 1.990\n",
      "Ep 1 (Step 001665): Train loss 1.877, Val loss 1.988\n",
      "Ep 1 (Step 001670): Train loss 2.354, Val loss 1.987\n",
      "Ep 1 (Step 001675): Train loss 1.884, Val loss 1.989\n",
      "Ep 1 (Step 001680): Train loss 1.839, Val loss 1.991\n",
      "Ep 1 (Step 001685): Train loss 1.696, Val loss 1.994\n",
      "Ep 1 (Step 001690): Train loss 1.664, Val loss 1.995\n",
      "Ep 1 (Step 001695): Train loss 1.664, Val loss 1.991\n",
      "Ep 1 (Step 001700): Train loss 2.027, Val loss 1.989\n",
      "Ep 1 (Step 001705): Train loss 1.833, Val loss 1.989\n",
      "Ep 1 (Step 001710): Train loss 1.869, Val loss 1.992\n",
      "Ep 1 (Step 001715): Train loss 1.846, Val loss 1.992\n",
      "Ep 1 (Step 001720): Train loss 1.764, Val loss 1.991\n",
      "Ep 1 (Step 001725): Train loss 2.125, Val loss 1.993\n",
      "Ep 1 (Step 001730): Train loss 1.930, Val loss 1.993\n",
      "Ep 1 (Step 001735): Train loss 1.980, Val loss 1.994\n",
      "Ep 1 (Step 001740): Train loss 1.950, Val loss 1.995\n",
      "Ep 1 (Step 001745): Train loss 2.104, Val loss 1.997\n",
      "Ep 1 (Step 001750): Train loss 1.944, Val loss 2.000\n",
      "Ep 1 (Step 001755): Train loss 2.080, Val loss 1.999\n",
      "Ep 1 (Step 001760): Train loss 1.809, Val loss 1.995\n",
      "Ep 1 (Step 001765): Train loss 2.025, Val loss 1.994\n",
      "Ep 1 (Step 001770): Train loss 1.593, Val loss 1.994\n",
      "Ep 1 (Step 001775): Train loss 1.781, Val loss 1.993\n",
      "Ep 1 (Step 001780): Train loss 1.948, Val loss 1.993\n",
      "Ep 1 (Step 001785): Train loss 1.539, Val loss 1.992\n",
      "Ep 1 (Step 001790): Train loss 1.761, Val loss 1.993\n",
      "Ep 1 (Step 001795): Train loss 1.930, Val loss 1.992\n",
      "Ep 1 (Step 001800): Train loss 1.730, Val loss 1.993\n",
      "Ep 1 (Step 001805): Train loss 1.934, Val loss 1.993\n",
      "Ep 1 (Step 001810): Train loss 2.021, Val loss 1.992\n",
      "Ep 1 (Step 001815): Train loss 1.990, Val loss 1.992\n",
      "Ep 1 (Step 001820): Train loss 1.712, Val loss 1.988\n",
      "Ep 1 (Step 001825): Train loss 1.579, Val loss 1.985\n",
      "Ep 1 (Step 001830): Train loss 1.562, Val loss 1.985\n",
      "Ep 1 (Step 001835): Train loss 1.854, Val loss 1.984\n",
      "Ep 1 (Step 001840): Train loss 1.936, Val loss 1.984\n",
      "Ep 1 (Step 001845): Train loss 1.603, Val loss 1.983\n",
      "Ep 1 (Step 001850): Train loss 2.029, Val loss 1.983\n",
      "Ep 1 (Step 001855): Train loss 1.860, Val loss 1.982\n",
      "Ep 1 (Step 001860): Train loss 1.912, Val loss 1.981\n",
      "Ep 1 (Step 001865): Train loss 1.816, Val loss 1.984\n",
      "Ep 1 (Step 001870): Train loss 1.700, Val loss 1.987\n",
      "Ep 1 (Step 001875): Train loss 1.775, Val loss 1.987\n",
      "Ep 1 (Step 001880): Train loss 1.887, Val loss 1.986\n",
      "Ep 1 (Step 001885): Train loss 1.983, Val loss 1.984\n",
      "Ep 1 (Step 001890): Train loss 1.814, Val loss 1.984\n",
      "Ep 1 (Step 001895): Train loss 1.774, Val loss 1.984\n",
      "Ep 1 (Step 001900): Train loss 1.947, Val loss 1.983\n",
      "Ep 1 (Step 001905): Train loss 2.021, Val loss 1.982\n",
      "Ep 1 (Step 001910): Train loss 1.705, Val loss 1.983\n",
      "Ep 1 (Step 001915): Train loss 2.025, Val loss 1.985\n",
      "Ep 1 (Step 001920): Train loss 1.696, Val loss 1.984\n",
      "Ep 1 (Step 001925): Train loss 1.513, Val loss 1.980\n",
      "Ep 1 (Step 001930): Train loss 2.144, Val loss 1.976\n",
      "Ep 1 (Step 001935): Train loss 1.722, Val loss 1.975\n",
      "Ep 1 (Step 001940): Train loss 1.715, Val loss 1.984\n",
      "Ep 1 (Step 001945): Train loss 2.208, Val loss 2.065\n",
      "Ep 1 (Step 001950): Train loss 1.926, Val loss 2.009\n",
      "Ep 1 (Step 001955): Train loss 1.972, Val loss 1.989\n",
      "Ep 1 (Step 001960): Train loss 1.755, Val loss 1.986\n",
      "Ep 1 (Step 001965): Train loss 1.784, Val loss 1.984\n",
      "Ep 1 (Step 001970): Train loss 1.885, Val loss 1.984\n",
      "Ep 1 (Step 001975): Train loss 1.847, Val loss 1.984\n",
      "Ep 1 (Step 001980): Train loss 1.671, Val loss 1.982\n",
      "Ep 1 (Step 001985): Train loss 1.829, Val loss 1.981\n",
      "Ep 1 (Step 001990): Train loss 1.791, Val loss 1.979\n",
      "Ep 1 (Step 001995): Train loss 1.989, Val loss 1.981\n",
      "Ep 1 (Step 002000): Train loss 1.962, Val loss 1.982\n",
      "Ep 1 (Step 002005): Train loss 1.704, Val loss 1.985\n",
      "Ep 1 (Step 002010): Train loss 1.884, Val loss 1.983\n",
      "Ep 1 (Step 002015): Train loss 1.479, Val loss 1.984\n",
      "Ep 1 (Step 002020): Train loss 1.940, Val loss 1.986\n",
      "Ep 1 (Step 002025): Train loss 1.889, Val loss 1.986\n",
      "Ep 1 (Step 002030): Train loss 1.835, Val loss 1.985\n",
      "Ep 1 (Step 002035): Train loss 2.022, Val loss 1.981\n",
      "Ep 1 (Step 002040): Train loss 1.600, Val loss 1.979\n",
      "Ep 1 (Step 002045): Train loss 1.784, Val loss 1.980\n",
      "Ep 1 (Step 002050): Train loss 1.823, Val loss 1.980\n",
      "Ep 1 (Step 002055): Train loss 1.990, Val loss 1.979\n",
      "Ep 1 (Step 002060): Train loss 2.072, Val loss 1.978\n",
      "Ep 1 (Step 002065): Train loss 1.875, Val loss 1.977\n",
      "Ep 1 (Step 002070): Train loss 1.805, Val loss 1.979\n",
      "Ep 1 (Step 002075): Train loss 1.864, Val loss 1.979\n",
      "Ep 1 (Step 002080): Train loss 1.953, Val loss 1.981\n",
      "Ep 1 (Step 002085): Train loss 2.229, Val loss 1.984\n",
      "Ep 1 (Step 002090): Train loss 1.790, Val loss 1.982\n",
      "Ep 1 (Step 002095): Train loss 1.742, Val loss 1.979\n",
      "Ep 1 (Step 002100): Train loss 1.997, Val loss 1.977\n",
      "Ep 1 (Step 002105): Train loss 1.871, Val loss 1.977\n",
      "Ep 1 (Step 002110): Train loss 1.831, Val loss 1.979\n",
      "Ep 1 (Step 002115): Train loss 1.822, Val loss 1.982\n",
      "Ep 1 (Step 002120): Train loss 1.792, Val loss 1.979\n",
      "Ep 1 (Step 002125): Train loss 1.782, Val loss 1.979\n",
      "Ep 1 (Step 002130): Train loss 1.818, Val loss 1.980\n",
      "Ep 1 (Step 002135): Train loss 1.633, Val loss 1.981\n",
      "Ep 1 (Step 002140): Train loss 1.940, Val loss 1.981\n",
      "Ep 1 (Step 002145): Train loss 1.792, Val loss 1.985\n",
      "Ep 1 (Step 002150): Train loss 2.004, Val loss 1.984\n",
      "Ep 1 (Step 002155): Train loss 1.916, Val loss 1.981\n",
      "Ep 1 (Step 002160): Train loss 1.743, Val loss 1.979\n",
      "Ep 1 (Step 002165): Train loss 1.555, Val loss 1.979\n",
      "Ep 1 (Step 002170): Train loss 1.892, Val loss 1.978\n",
      "Ep 1 (Step 002175): Train loss 1.807, Val loss 1.976\n",
      "Ep 1 (Step 002180): Train loss 1.751, Val loss 1.973\n",
      "Ep 1 (Step 002185): Train loss 1.811, Val loss 1.973\n",
      "Ep 1 (Step 002190): Train loss 2.024, Val loss 1.973\n",
      "Ep 1 (Step 002195): Train loss 1.806, Val loss 1.970\n",
      "Ep 1 (Step 002200): Train loss 2.026, Val loss 1.964\n",
      "Ep 1 (Step 002205): Train loss 1.712, Val loss 1.960\n",
      "Ep 1 (Step 002210): Train loss 1.980, Val loss 1.962\n",
      "Ep 1 (Step 002215): Train loss 1.741, Val loss 1.963\n",
      "Ep 1 (Step 002220): Train loss 1.880, Val loss 1.962\n",
      "Ep 1 (Step 002225): Train loss 1.875, Val loss 1.961\n",
      "Ep 1 (Step 002230): Train loss 1.759, Val loss 1.961\n",
      "Ep 1 (Step 002235): Train loss 1.843, Val loss 1.962\n",
      "Ep 1 (Step 002240): Train loss 1.623, Val loss 1.962\n",
      "Ep 1 (Step 002245): Train loss 1.869, Val loss 1.962\n",
      "Ep 1 (Step 002250): Train loss 2.020, Val loss 1.965\n",
      "Ep 1 (Step 002255): Train loss 1.683, Val loss 1.969\n",
      "Ep 1 (Step 002260): Train loss 2.063, Val loss 1.969\n",
      "Ep 1 (Step 002265): Train loss 1.466, Val loss 1.968\n",
      "Ep 1 (Step 002270): Train loss 1.448, Val loss 1.967\n",
      "Ep 1 (Step 002275): Train loss 1.759, Val loss 1.964\n",
      "Ep 1 (Step 002280): Train loss 2.070, Val loss 1.963\n",
      "Ep 1 (Step 002285): Train loss 1.678, Val loss 1.962\n",
      "Ep 1 (Step 002290): Train loss 1.689, Val loss 1.962\n",
      "Ep 1 (Step 002295): Train loss 1.596, Val loss 1.963\n",
      "Ep 1 (Step 002300): Train loss 1.813, Val loss 1.962\n",
      "Ep 1 (Step 002305): Train loss 1.974, Val loss 1.962\n",
      "Ep 1 (Step 002310): Train loss 1.981, Val loss 1.961\n",
      "Ep 1 (Step 002315): Train loss 1.811, Val loss 1.958\n",
      "Ep 1 (Step 002320): Train loss 2.062, Val loss 1.956\n",
      "Ep 1 (Step 002325): Train loss 1.927, Val loss 1.955\n",
      "Ep 1 (Step 002330): Train loss 1.843, Val loss 1.957\n",
      "Ep 1 (Step 002335): Train loss 1.736, Val loss 1.958\n",
      "Ep 1 (Step 002340): Train loss 2.024, Val loss 1.956\n",
      "Ep 1 (Step 002345): Train loss 1.615, Val loss 1.954\n",
      "Ep 1 (Step 002350): Train loss 1.520, Val loss 1.954\n",
      "Ep 1 (Step 002355): Train loss 2.031, Val loss 1.953\n",
      "Ep 1 (Step 002360): Train loss 1.935, Val loss 1.954\n",
      "Ep 1 (Step 002365): Train loss 1.853, Val loss 1.953\n",
      "Ep 1 (Step 002370): Train loss 1.776, Val loss 1.951\n",
      "Ep 1 (Step 002375): Train loss 1.888, Val loss 1.952\n",
      "Ep 1 (Step 002380): Train loss 1.676, Val loss 1.952\n",
      "Ep 1 (Step 002385): Train loss 1.503, Val loss 1.952\n",
      "Ep 1 (Step 002390): Train loss 1.854, Val loss 1.953\n",
      "Ep 1 (Step 002395): Train loss 1.598, Val loss 1.953\n",
      "Ep 1 (Step 002400): Train loss 1.587, Val loss 1.955\n",
      "Ep 1 (Step 002405): Train loss 1.847, Val loss 1.956\n",
      "Ep 1 (Step 002410): Train loss 1.734, Val loss 1.957\n",
      "Ep 1 (Step 002415): Train loss 1.924, Val loss 1.957\n",
      "Ep 1 (Step 002420): Train loss 1.719, Val loss 1.955\n",
      "Ep 1 (Step 002425): Train loss 1.759, Val loss 1.957\n",
      "Ep 1 (Step 002430): Train loss 1.861, Val loss 1.958\n",
      "Ep 1 (Step 002435): Train loss 2.112, Val loss 1.956\n",
      "Ep 1 (Step 002440): Train loss 1.746, Val loss 1.957\n",
      "Ep 1 (Step 002445): Train loss 1.876, Val loss 1.957\n",
      "Ep 1 (Step 002450): Train loss 1.747, Val loss 1.957\n",
      "Ep 1 (Step 002455): Train loss 1.788, Val loss 1.958\n",
      "Ep 1 (Step 002460): Train loss 1.851, Val loss 1.959\n",
      "Ep 1 (Step 002465): Train loss 1.958, Val loss 1.958\n",
      "Ep 1 (Step 002470): Train loss 1.776, Val loss 1.959\n",
      "Ep 1 (Step 002475): Train loss 1.923, Val loss 1.957\n",
      "Ep 1 (Step 002480): Train loss 1.742, Val loss 1.957\n",
      "Ep 1 (Step 002485): Train loss 1.761, Val loss 1.961\n",
      "Ep 1 (Step 002490): Train loss 1.498, Val loss 1.960\n",
      "Ep 1 (Step 002495): Train loss 1.823, Val loss 1.955\n",
      "Ep 1 (Step 002500): Train loss 1.550, Val loss 1.953\n",
      "Ep 1 (Step 002505): Train loss 1.837, Val loss 1.951\n",
      "Ep 1 (Step 002510): Train loss 1.900, Val loss 1.949\n",
      "Ep 1 (Step 002515): Train loss 1.927, Val loss 1.950\n",
      "Ep 1 (Step 002520): Train loss 1.863, Val loss 1.953\n",
      "Ep 1 (Step 002525): Train loss 1.703, Val loss 1.956\n",
      "Ep 1 (Step 002530): Train loss 1.843, Val loss 1.959\n",
      "Ep 1 (Step 002535): Train loss 1.869, Val loss 1.960\n",
      "Ep 1 (Step 002540): Train loss 1.939, Val loss 1.961\n",
      "Ep 1 (Step 002545): Train loss 1.760, Val loss 1.961\n",
      "Ep 1 (Step 002550): Train loss 1.858, Val loss 1.957\n",
      "Ep 1 (Step 002555): Train loss 1.842, Val loss 1.955\n",
      "Ep 1 (Step 002560): Train loss 1.789, Val loss 1.955\n",
      "Ep 1 (Step 002565): Train loss 1.893, Val loss 1.954\n",
      "Ep 1 (Step 002570): Train loss 1.971, Val loss 1.955\n",
      "Ep 1 (Step 002575): Train loss 1.867, Val loss 1.956\n",
      "Ep 1 (Step 002580): Train loss 2.042, Val loss 1.956\n",
      "Ep 1 (Step 002585): Train loss 1.806, Val loss 1.956\n",
      "Ep 1 (Step 002590): Train loss 1.721, Val loss 1.956\n",
      "Ep 1 (Step 002595): Train loss 1.809, Val loss 1.958\n",
      "Ep 1 (Step 002600): Train loss 1.741, Val loss 1.961\n",
      "Ep 1 (Step 002605): Train loss 1.805, Val loss 1.962\n",
      "Ep 1 (Step 002610): Train loss 1.910, Val loss 1.959\n",
      "Ep 1 (Step 002615): Train loss 1.723, Val loss 1.957\n",
      "Ep 1 (Step 002620): Train loss 1.926, Val loss 1.956\n",
      "Ep 1 (Step 002625): Train loss 1.696, Val loss 1.955\n",
      "Ep 1 (Step 002630): Train loss 2.012, Val loss 1.954\n",
      "Ep 1 (Step 002635): Train loss 1.997, Val loss 1.954\n",
      "Ep 1 (Step 002640): Train loss 1.791, Val loss 1.954\n",
      "Ep 1 (Step 002645): Train loss 1.791, Val loss 1.954\n",
      "Ep 1 (Step 002650): Train loss 1.774, Val loss 1.952\n",
      "Ep 1 (Step 002655): Train loss 1.644, Val loss 1.951\n",
      "Ep 1 (Step 002660): Train loss 1.853, Val loss 1.950\n",
      "Ep 1 (Step 002665): Train loss 1.800, Val loss 1.949\n",
      "Ep 1 (Step 002670): Train loss 1.624, Val loss 1.949\n",
      "Ep 1 (Step 002675): Train loss 1.520, Val loss 1.954\n",
      "Ep 1 (Step 002680): Train loss 2.029, Val loss 1.954\n",
      "Ep 1 (Step 002685): Train loss 1.699, Val loss 1.951\n",
      "Ep 1 (Step 002690): Train loss 1.945, Val loss 1.951\n",
      "Ep 1 (Step 002695): Train loss 1.838, Val loss 1.951\n",
      "Ep 1 (Step 002700): Train loss 1.739, Val loss 1.952\n",
      "Ep 1 (Step 002705): Train loss 1.819, Val loss 1.950\n",
      "Ep 1 (Step 002710): Train loss 1.764, Val loss 1.949\n",
      "Ep 1 (Step 002715): Train loss 2.239, Val loss 1.948\n",
      "Ep 1 (Step 002720): Train loss 1.752, Val loss 1.948\n",
      "Ep 1 (Step 002725): Train loss 1.775, Val loss 1.947\n",
      "Ep 1 (Step 002730): Train loss 1.801, Val loss 1.947\n",
      "Ep 1 (Step 002735): Train loss 1.663, Val loss 1.946\n",
      "Ep 1 (Step 002740): Train loss 2.001, Val loss 1.945\n",
      "Ep 1 (Step 002745): Train loss 1.678, Val loss 1.944\n",
      "Ep 1 (Step 002750): Train loss 2.031, Val loss 1.945\n",
      "Ep 1 (Step 002755): Train loss 1.942, Val loss 1.948\n",
      "Ep 1 (Step 002760): Train loss 1.997, Val loss 1.951\n",
      "Ep 1 (Step 002765): Train loss 1.925, Val loss 1.954\n",
      "Ep 1 (Step 002770): Train loss 1.761, Val loss 1.954\n",
      "Ep 1 (Step 002775): Train loss 1.698, Val loss 1.953\n",
      "Ep 1 (Step 002780): Train loss 1.760, Val loss 1.951\n",
      "Ep 1 (Step 002785): Train loss 1.531, Val loss 1.949\n",
      "Ep 1 (Step 002790): Train loss 1.868, Val loss 1.950\n",
      "Ep 1 (Step 002795): Train loss 1.633, Val loss 1.951\n",
      "Ep 1 (Step 002800): Train loss 1.774, Val loss 1.950\n",
      "Ep 1 (Step 002805): Train loss 1.747, Val loss 1.950\n",
      "Ep 1 (Step 002810): Train loss 1.775, Val loss 1.950\n",
      "Ep 1 (Step 002815): Train loss 1.848, Val loss 1.950\n",
      "Ep 1 (Step 002820): Train loss 1.318, Val loss 1.953\n",
      "Ep 1 (Step 002825): Train loss 1.594, Val loss 1.952\n",
      "Ep 1 (Step 002830): Train loss 2.079, Val loss 1.949\n",
      "Ep 1 (Step 002835): Train loss 1.770, Val loss 1.948\n",
      "Ep 1 (Step 002840): Train loss 1.625, Val loss 1.949\n",
      "Ep 1 (Step 002845): Train loss 1.729, Val loss 1.949\n",
      "Ep 1 (Step 002850): Train loss 1.874, Val loss 1.949\n",
      "Ep 1 (Step 002855): Train loss 1.888, Val loss 1.945\n",
      "Ep 1 (Step 002860): Train loss 1.951, Val loss 1.944\n",
      "Ep 1 (Step 002865): Train loss 1.671, Val loss 1.943\n",
      "Ep 1 (Step 002870): Train loss 1.803, Val loss 1.943\n",
      "Ep 1 (Step 002875): Train loss 1.609, Val loss 1.944\n",
      "Ep 1 (Step 002880): Train loss 1.709, Val loss 1.946\n",
      "Ep 1 (Step 002885): Train loss 1.748, Val loss 1.950\n",
      "Ep 1 (Step 002890): Train loss 1.707, Val loss 1.954\n",
      "Ep 1 (Step 002895): Train loss 1.838, Val loss 1.966\n",
      "Ep 1 (Step 002900): Train loss 1.734, Val loss 1.961\n",
      "Ep 1 (Step 002905): Train loss 1.652, Val loss 1.959\n",
      "Ep 1 (Step 002910): Train loss 1.815, Val loss 1.957\n",
      "Ep 1 (Step 002915): Train loss 2.008, Val loss 1.955\n",
      "Ep 1 (Step 002920): Train loss 1.746, Val loss 1.954\n",
      "Ep 1 (Step 002925): Train loss 1.610, Val loss 1.953\n",
      "Ep 1 (Step 002930): Train loss 1.767, Val loss 1.952\n",
      "Ep 1 (Step 002935): Train loss 1.811, Val loss 1.951\n",
      "Ep 1 (Step 002940): Train loss 1.570, Val loss 1.951\n",
      "Ep 1 (Step 002945): Train loss 1.764, Val loss 1.951\n",
      "Ep 1 (Step 002950): Train loss 1.682, Val loss 1.950\n",
      "Ep 1 (Step 002955): Train loss 1.297, Val loss 1.950\n",
      "Ep 1 (Step 002960): Train loss 1.911, Val loss 1.950\n",
      "Ep 1 (Step 002965): Train loss 1.867, Val loss 1.950\n",
      "Ep 1 (Step 002970): Train loss 1.838, Val loss 1.951\n",
      "Ep 1 (Step 002975): Train loss 1.682, Val loss 1.950\n",
      "Ep 1 (Step 002980): Train loss 1.576, Val loss 1.950\n",
      "Ep 1 (Step 002985): Train loss 1.736, Val loss 1.949\n",
      "Ep 1 (Step 002990): Train loss 1.840, Val loss 1.950\n",
      "Ep 1 (Step 002995): Train loss 1.863, Val loss 1.950\n",
      "Ep 1 (Step 003000): Train loss 1.853, Val loss 1.952\n",
      "Ep 1 (Step 003005): Train loss 1.738, Val loss 1.952\n",
      "Ep 1 (Step 003010): Train loss 1.946, Val loss 1.952\n",
      "Ep 1 (Step 003015): Train loss 2.007, Val loss 1.953\n",
      "Ep 1 (Step 003020): Train loss 1.501, Val loss 1.952\n",
      "Ep 1 (Step 003025): Train loss 1.830, Val loss 1.954\n",
      "Ep 1 (Step 003030): Train loss 1.454, Val loss 1.956\n",
      "Ep 1 (Step 003035): Train loss 2.089, Val loss 1.957\n",
      "Ep 1 (Step 003040): Train loss 1.721, Val loss 1.957\n",
      "Ep 1 (Step 003045): Train loss 1.811, Val loss 1.955\n",
      "Ep 1 (Step 003050): Train loss 2.010, Val loss 1.955\n",
      "Ep 1 (Step 003055): Train loss 1.862, Val loss 1.955\n",
      "Ep 1 (Step 003060): Train loss 1.988, Val loss 1.953\n",
      "Ep 1 (Step 003065): Train loss 1.816, Val loss 1.953\n",
      "Ep 1 (Step 003070): Train loss 1.724, Val loss 1.954\n",
      "Ep 1 (Step 003075): Train loss 1.812, Val loss 1.951\n",
      "Ep 1 (Step 003080): Train loss 1.915, Val loss 1.950\n",
      "Ep 1 (Step 003085): Train loss 1.960, Val loss 1.949\n",
      "Ep 1 (Step 003090): Train loss 1.729, Val loss 1.950\n",
      "Ep 1 (Step 003095): Train loss 1.832, Val loss 1.950\n",
      "Ep 1 (Step 003100): Train loss 1.760, Val loss 1.950\n",
      "Ep 1 (Step 003105): Train loss 1.752, Val loss 1.950\n",
      "Ep 1 (Step 003110): Train loss 1.994, Val loss 1.950\n",
      "Ep 1 (Step 003115): Train loss 2.013, Val loss 1.950\n",
      "Ep 1 (Step 003120): Train loss 1.486, Val loss 1.949\n",
      "Ep 1 (Step 003125): Train loss 1.744, Val loss 1.948\n",
      "Ep 1 (Step 003130): Train loss 1.890, Val loss 1.948\n",
      "Ep 1 (Step 003135): Train loss 1.503, Val loss 1.949\n",
      "Ep 1 (Step 003140): Train loss 1.616, Val loss 1.949\n",
      "Ep 1 (Step 003145): Train loss 2.043, Val loss 1.950\n",
      "Ep 1 (Step 003150): Train loss 1.698, Val loss 1.948\n",
      "Ep 1 (Step 003155): Train loss 1.794, Val loss 1.947\n",
      "Ep 1 (Step 003160): Train loss 1.814, Val loss 1.947\n",
      "Ep 1 (Step 003165): Train loss 1.791, Val loss 1.947\n",
      "Ep 1 (Step 003170): Train loss 1.741, Val loss 1.946\n",
      "Ep 1 (Step 003175): Train loss 2.046, Val loss 1.945\n",
      "Ep 1 (Step 003180): Train loss 1.705, Val loss 1.942\n",
      "Ep 1 (Step 003185): Train loss 1.670, Val loss 1.942\n",
      "Ep 1 (Step 003190): Train loss 1.842, Val loss 1.944\n",
      "Ep 1 (Step 003195): Train loss 1.764, Val loss 1.944\n",
      "Ep 1 (Step 003200): Train loss 1.484, Val loss 1.943\n",
      "Ep 1 (Step 003205): Train loss 1.840, Val loss 1.940\n",
      "Ep 1 (Step 003210): Train loss 1.785, Val loss 1.940\n",
      "Ep 1 (Step 003215): Train loss 1.794, Val loss 1.938\n",
      "Ep 1 (Step 003220): Train loss 1.917, Val loss 1.937\n",
      "Ep 1 (Step 003225): Train loss 1.886, Val loss 1.939\n",
      "Ep 1 (Step 003230): Train loss 1.802, Val loss 1.942\n",
      "Ep 1 (Step 003235): Train loss 1.649, Val loss 1.943\n",
      "Ep 1 (Step 003240): Train loss 1.884, Val loss 1.943\n",
      "Ep 1 (Step 003245): Train loss 2.114, Val loss 1.944\n",
      "Ep 1 (Step 003250): Train loss 1.846, Val loss 1.945\n",
      "Ep 1 (Step 003255): Train loss 1.908, Val loss 1.945\n",
      "Ep 1 (Step 003260): Train loss 1.779, Val loss 1.946\n",
      "Ep 1 (Step 003265): Train loss 1.749, Val loss 1.947\n",
      "Ep 1 (Step 003270): Train loss 1.872, Val loss 1.948\n",
      "Ep 1 (Step 003275): Train loss 1.779, Val loss 1.948\n",
      "Ep 1 (Step 003280): Train loss 1.815, Val loss 1.948\n",
      "Ep 1 (Step 003285): Train loss 1.726, Val loss 1.947\n",
      "Ep 1 (Step 003290): Train loss 1.700, Val loss 1.946\n",
      "Ep 1 (Step 003295): Train loss 1.661, Val loss 1.945\n",
      "Ep 1 (Step 003300): Train loss 1.839, Val loss 1.946\n",
      "Ep 1 (Step 003305): Train loss 1.914, Val loss 1.946\n",
      "Ep 1 (Step 003310): Train loss 1.856, Val loss 1.945\n",
      "Ep 1 (Step 003315): Train loss 2.054, Val loss 1.945\n",
      "Ep 1 (Step 003320): Train loss 1.656, Val loss 1.947\n",
      "Ep 1 (Step 003325): Train loss 1.724, Val loss 1.948\n",
      "Ep 1 (Step 003330): Train loss 1.816, Val loss 1.947\n",
      "Ep 1 (Step 003335): Train loss 1.900, Val loss 1.947\n",
      "Ep 1 (Step 003340): Train loss 2.000, Val loss 1.948\n",
      "Ep 1 (Step 003345): Train loss 1.879, Val loss 1.946\n",
      "Ep 1 (Step 003350): Train loss 1.830, Val loss 1.945\n",
      "Ep 1 (Step 003355): Train loss 1.761, Val loss 1.946\n",
      "Ep 1 (Step 003360): Train loss 2.046, Val loss 1.945\n",
      "Ep 1 (Step 003365): Train loss 1.697, Val loss 1.945\n",
      "Ep 1 (Step 003370): Train loss 1.857, Val loss 1.946\n",
      "Ep 1 (Step 003375): Train loss 1.614, Val loss 1.948\n",
      "Ep 1 (Step 003380): Train loss 1.662, Val loss 1.950\n",
      "Ep 1 (Step 003385): Train loss 1.898, Val loss 1.945\n",
      "Ep 1 (Step 003390): Train loss 1.787, Val loss 1.943\n",
      "Ep 1 (Step 003395): Train loss 1.413, Val loss 1.944\n",
      "Ep 1 (Step 003400): Train loss 1.837, Val loss 1.944\n",
      "Ep 1 (Step 003405): Train loss 1.671, Val loss 1.945\n",
      "Ep 1 (Step 003410): Train loss 2.032, Val loss 1.944\n",
      "Ep 1 (Step 003415): Train loss 1.728, Val loss 1.943\n",
      "Ep 1 (Step 003420): Train loss 1.698, Val loss 1.945\n",
      "Ep 1 (Step 003425): Train loss 1.966, Val loss 1.945\n",
      "Ep 1 (Step 003430): Train loss 1.777, Val loss 1.945\n",
      "Ep 1 (Step 003435): Train loss 1.553, Val loss 1.946\n",
      "Ep 1 (Step 003440): Train loss 1.919, Val loss 1.948\n",
      "Ep 1 (Step 003445): Train loss 2.020, Val loss 1.950\n",
      "Ep 1 (Step 003450): Train loss 1.744, Val loss 1.947\n",
      "Ep 1 (Step 003455): Train loss 1.756, Val loss 1.948\n",
      "Ep 1 (Step 003460): Train loss 1.721, Val loss 1.950\n",
      "Ep 1 (Step 003465): Train loss 1.697, Val loss 1.950\n",
      "Ep 1 (Step 003470): Train loss 1.707, Val loss 1.950\n",
      "Ep 1 (Step 003475): Train loss 1.977, Val loss 1.949\n",
      "Ep 1 (Step 003480): Train loss 1.797, Val loss 1.950\n",
      "Ep 1 (Step 003485): Train loss 1.907, Val loss 1.950\n",
      "Ep 1 (Step 003490): Train loss 1.981, Val loss 1.949\n",
      "Ep 1 (Step 003495): Train loss 1.795, Val loss 1.947\n",
      "Ep 1 (Step 003500): Train loss 2.005, Val loss 1.946\n",
      "Ep 1 (Step 003505): Train loss 1.830, Val loss 1.947\n",
      "Ep 1 (Step 003510): Train loss 1.814, Val loss 1.950\n",
      "Ep 1 (Step 003515): Train loss 1.922, Val loss 1.952\n",
      "Ep 1 (Step 003520): Train loss 1.790, Val loss 1.952\n",
      "Ep 1 (Step 003525): Train loss 2.069, Val loss 1.951\n",
      "Ep 1 (Step 003530): Train loss 1.758, Val loss 1.950\n",
      "Ep 1 (Step 003535): Train loss 1.900, Val loss 1.949\n",
      "Ep 1 (Step 003540): Train loss 1.815, Val loss 1.948\n",
      "Ep 1 (Step 003545): Train loss 1.950, Val loss 1.949\n",
      "Ep 1 (Step 003550): Train loss 1.992, Val loss 1.950\n",
      "Ep 1 (Step 003555): Train loss 1.879, Val loss 1.948\n",
      "Ep 1 (Step 003560): Train loss 2.036, Val loss 1.946\n",
      "Ep 1 (Step 003565): Train loss 1.854, Val loss 1.944\n",
      "Ep 1 (Step 003570): Train loss 1.905, Val loss 1.944\n",
      "Ep 1 (Step 003575): Train loss 1.506, Val loss 1.945\n",
      "Ep 1 (Step 003580): Train loss 1.903, Val loss 1.945\n",
      "Ep 1 (Step 003585): Train loss 1.586, Val loss 1.945\n",
      "Ep 1 (Step 003590): Train loss 1.636, Val loss 1.945\n",
      "Ep 1 (Step 003595): Train loss 1.769, Val loss 1.946\n",
      "Ep 1 (Step 003600): Train loss 1.563, Val loss 1.946\n",
      "Ep 1 (Step 003605): Train loss 1.730, Val loss 1.945\n",
      "Ep 1 (Step 003610): Train loss 1.797, Val loss 1.943\n",
      "Ep 1 (Step 003615): Train loss 1.913, Val loss 1.942\n",
      "Ep 1 (Step 003620): Train loss 1.741, Val loss 1.942\n",
      "Ep 1 (Step 003625): Train loss 1.767, Val loss 1.943\n",
      "Ep 1 (Step 003630): Train loss 1.603, Val loss 1.941\n",
      "Ep 1 (Step 003635): Train loss 1.667, Val loss 1.940\n",
      "Ep 1 (Step 003640): Train loss 1.664, Val loss 1.940\n",
      "Ep 1 (Step 003645): Train loss 1.953, Val loss 1.941\n",
      "Ep 1 (Step 003650): Train loss 1.881, Val loss 1.944\n",
      "Ep 1 (Step 003655): Train loss 1.749, Val loss 1.943\n",
      "Ep 1 (Step 003660): Train loss 1.727, Val loss 1.944\n",
      "Ep 1 (Step 003665): Train loss 1.778, Val loss 1.943\n",
      "Ep 1 (Step 003670): Train loss 1.816, Val loss 1.943\n",
      "Ep 1 (Step 003675): Train loss 1.960, Val loss 1.942\n",
      "Ep 1 (Step 003680): Train loss 1.626, Val loss 1.941\n",
      "Ep 1 (Step 003685): Train loss 1.733, Val loss 1.940\n",
      "Ep 1 (Step 003690): Train loss 1.732, Val loss 1.940\n",
      "Ep 1 (Step 003695): Train loss 1.890, Val loss 1.942\n",
      "Ep 1 (Step 003700): Train loss 1.879, Val loss 1.943\n",
      "Ep 1 (Step 003705): Train loss 1.864, Val loss 1.944\n",
      "Ep 1 (Step 003710): Train loss 1.550, Val loss 1.945\n",
      "Ep 1 (Step 003715): Train loss 1.804, Val loss 1.946\n",
      "Ep 1 (Step 003720): Train loss 2.009, Val loss 1.945\n",
      "Ep 1 (Step 003725): Train loss 1.704, Val loss 1.945\n",
      "Ep 1 (Step 003730): Train loss 1.865, Val loss 1.947\n",
      "Ep 1 (Step 003735): Train loss 1.915, Val loss 1.948\n",
      "Ep 1 (Step 003740): Train loss 2.028, Val loss 1.945\n",
      "Ep 1 (Step 003745): Train loss 1.882, Val loss 1.944\n",
      "Ep 1 (Step 003750): Train loss 1.942, Val loss 1.945\n",
      "Ep 1 (Step 003755): Train loss 1.802, Val loss 1.947\n",
      "Ep 1 (Step 003760): Train loss 1.860, Val loss 1.950\n",
      "Ep 1 (Step 003765): Train loss 1.720, Val loss 1.949\n",
      "Ep 1 (Step 003770): Train loss 1.624, Val loss 1.946\n",
      "Ep 1 (Step 003775): Train loss 1.653, Val loss 1.944\n",
      "Ep 1 (Step 003780): Train loss 1.932, Val loss 1.943\n",
      "Ep 1 (Step 003785): Train loss 1.877, Val loss 1.940\n",
      "Ep 1 (Step 003790): Train loss 1.688, Val loss 1.938\n",
      "Ep 1 (Step 003795): Train loss 1.689, Val loss 1.936\n",
      "Ep 1 (Step 003800): Train loss 2.040, Val loss 1.933\n",
      "Ep 1 (Step 003805): Train loss 1.964, Val loss 1.931\n",
      "Ep 1 (Step 003810): Train loss 1.723, Val loss 1.932\n",
      "Ep 1 (Step 003815): Train loss 1.746, Val loss 1.932\n",
      "Ep 1 (Step 003820): Train loss 1.952, Val loss 1.932\n",
      "Ep 1 (Step 003825): Train loss 1.989, Val loss 1.933\n",
      "Ep 1 (Step 003830): Train loss 1.598, Val loss 1.934\n",
      "Ep 1 (Step 003835): Train loss 2.166, Val loss 1.936\n",
      "Ep 1 (Step 003840): Train loss 1.935, Val loss 1.939\n",
      "Ep 1 (Step 003845): Train loss 1.872, Val loss 1.936\n",
      "Ep 1 (Step 003850): Train loss 1.504, Val loss 1.933\n",
      "Ep 1 (Step 003855): Train loss 1.689, Val loss 1.931\n",
      "Ep 1 (Step 003860): Train loss 1.840, Val loss 1.929\n",
      "Ep 1 (Step 003865): Train loss 1.652, Val loss 1.926\n",
      "Ep 1 (Step 003870): Train loss 1.805, Val loss 1.925\n",
      "Ep 1 (Step 003875): Train loss 1.905, Val loss 1.924\n",
      "Ep 1 (Step 003880): Train loss 2.024, Val loss 1.924\n",
      "Ep 1 (Step 003885): Train loss 1.839, Val loss 1.924\n",
      "Ep 1 (Step 003890): Train loss 1.781, Val loss 1.928\n",
      "Ep 1 (Step 003895): Train loss 1.838, Val loss 1.931\n",
      "Ep 1 (Step 003900): Train loss 1.724, Val loss 1.932\n",
      "Ep 1 (Step 003905): Train loss 1.788, Val loss 1.931\n",
      "Ep 1 (Step 003910): Train loss 1.699, Val loss 1.931\n",
      "Ep 1 (Step 003915): Train loss 1.780, Val loss 1.932\n",
      "Ep 1 (Step 003920): Train loss 1.814, Val loss 1.930\n",
      "Ep 1 (Step 003925): Train loss 1.648, Val loss 1.930\n",
      "Ep 1 (Step 003930): Train loss 1.835, Val loss 1.930\n",
      "Ep 1 (Step 003935): Train loss 1.805, Val loss 1.928\n",
      "Ep 1 (Step 003940): Train loss 1.921, Val loss 1.925\n",
      "Ep 1 (Step 003945): Train loss 1.704, Val loss 1.925\n",
      "Ep 1 (Step 003950): Train loss 1.716, Val loss 1.927\n",
      "Ep 1 (Step 003955): Train loss 2.001, Val loss 1.927\n",
      "Ep 1 (Step 003960): Train loss 1.871, Val loss 1.927\n",
      "Ep 1 (Step 003965): Train loss 1.696, Val loss 1.927\n",
      "Ep 1 (Step 003970): Train loss 1.524, Val loss 1.927\n",
      "Ep 1 (Step 003975): Train loss 1.982, Val loss 1.926\n",
      "Ep 1 (Step 003980): Train loss 1.875, Val loss 1.926\n",
      "Ep 1 (Step 003985): Train loss 1.495, Val loss 1.927\n",
      "Ep 1 (Step 003990): Train loss 1.876, Val loss 1.927\n",
      "Ep 1 (Step 003995): Train loss 1.743, Val loss 1.927\n",
      "Ep 1 (Step 004000): Train loss 1.918, Val loss 1.928\n",
      "Ep 1 (Step 004005): Train loss 1.576, Val loss 1.929\n",
      "Ep 1 (Step 004010): Train loss 2.011, Val loss 1.930\n",
      "Ep 1 (Step 004015): Train loss 1.946, Val loss 1.932\n",
      "Ep 1 (Step 004020): Train loss 1.414, Val loss 1.931\n",
      "Ep 1 (Step 004025): Train loss 1.746, Val loss 1.931\n",
      "Ep 1 (Step 004030): Train loss 1.727, Val loss 1.929\n",
      "Ep 1 (Step 004035): Train loss 1.857, Val loss 1.926\n",
      "Ep 1 (Step 004040): Train loss 2.047, Val loss 1.925\n",
      "Ep 1 (Step 004045): Train loss 1.639, Val loss 1.927\n",
      "Ep 1 (Step 004050): Train loss 1.909, Val loss 1.929\n",
      "Ep 1 (Step 004055): Train loss 1.954, Val loss 1.928\n",
      "Ep 1 (Step 004060): Train loss 1.661, Val loss 1.928\n",
      "Ep 1 (Step 004065): Train loss 1.873, Val loss 1.928\n",
      "Ep 1 (Step 004070): Train loss 1.819, Val loss 1.926\n",
      "Ep 1 (Step 004075): Train loss 1.684, Val loss 1.925\n",
      "Ep 1 (Step 004080): Train loss 1.853, Val loss 1.924\n",
      "Ep 1 (Step 004085): Train loss 1.776, Val loss 1.925\n",
      "Ep 1 (Step 004090): Train loss 1.794, Val loss 1.925\n",
      "Ep 1 (Step 004095): Train loss 1.780, Val loss 1.926\n",
      "Ep 1 (Step 004100): Train loss 1.766, Val loss 1.927\n",
      "Ep 1 (Step 004105): Train loss 1.922, Val loss 1.928\n",
      "Ep 1 (Step 004110): Train loss 1.679, Val loss 1.929\n",
      "Ep 1 (Step 004115): Train loss 1.686, Val loss 1.930\n",
      "Ep 1 (Step 004120): Train loss 1.666, Val loss 1.930\n",
      "Ep 1 (Step 004125): Train loss 1.233, Val loss 1.931\n",
      "Ep 1 (Step 004130): Train loss 1.500, Val loss 1.933\n",
      "Ep 1 (Step 004135): Train loss 1.801, Val loss 1.936\n",
      "Ep 1 (Step 004140): Train loss 1.877, Val loss 1.934\n",
      "Ep 1 (Step 004145): Train loss 1.533, Val loss 1.931\n",
      "Ep 1 (Step 004150): Train loss 1.656, Val loss 1.928\n",
      "Ep 1 (Step 004155): Train loss 1.913, Val loss 1.924\n",
      "Ep 1 (Step 004160): Train loss 1.890, Val loss 1.923\n",
      "Ep 1 (Step 004165): Train loss 1.888, Val loss 1.925\n",
      "Ep 1 (Step 004170): Train loss 1.846, Val loss 1.925\n",
      "Ep 1 (Step 004175): Train loss 1.985, Val loss 1.924\n",
      "Ep 1 (Step 004180): Train loss 1.766, Val loss 1.924\n",
      "Ep 1 (Step 004185): Train loss 1.637, Val loss 1.924\n",
      "Ep 1 (Step 004190): Train loss 1.325, Val loss 1.925\n",
      "Ep 1 (Step 004195): Train loss 1.725, Val loss 1.925\n",
      "Ep 1 (Step 004200): Train loss 1.769, Val loss 1.923\n",
      "Ep 1 (Step 004205): Train loss 1.729, Val loss 1.922\n",
      "Ep 1 (Step 004210): Train loss 1.810, Val loss 1.922\n",
      "Ep 1 (Step 004215): Train loss 1.725, Val loss 1.923\n",
      "Ep 1 (Step 004220): Train loss 1.777, Val loss 1.926\n",
      "Ep 1 (Step 004225): Train loss 2.164, Val loss 1.929\n",
      "Ep 1 (Step 004230): Train loss 1.745, Val loss 1.927\n",
      "Ep 1 (Step 004235): Train loss 2.031, Val loss 1.925\n",
      "Ep 1 (Step 004240): Train loss 1.721, Val loss 1.924\n",
      "Ep 1 (Step 004245): Train loss 1.848, Val loss 1.923\n",
      "Ep 1 (Step 004250): Train loss 1.769, Val loss 1.923\n",
      "Ep 1 (Step 004255): Train loss 1.945, Val loss 1.922\n",
      "Ep 1 (Step 004260): Train loss 1.647, Val loss 1.919\n",
      "Ep 1 (Step 004265): Train loss 2.002, Val loss 1.916\n",
      "Ep 1 (Step 004270): Train loss 1.602, Val loss 1.917\n",
      "Ep 1 (Step 004275): Train loss 1.563, Val loss 1.919\n",
      "Ep 1 (Step 004280): Train loss 1.701, Val loss 1.921\n",
      "Ep 1 (Step 004285): Train loss 1.876, Val loss 1.923\n",
      "Ep 1 (Step 004290): Train loss 1.655, Val loss 1.926\n",
      "Ep 1 (Step 004295): Train loss 1.686, Val loss 1.926\n",
      "Ep 1 (Step 004300): Train loss 1.863, Val loss 1.924\n",
      "Ep 1 (Step 004305): Train loss 1.541, Val loss 1.924\n",
      "Ep 1 (Step 004310): Train loss 1.685, Val loss 1.923\n",
      "Ep 1 (Step 004315): Train loss 1.853, Val loss 1.921\n",
      "Ep 1 (Step 004320): Train loss 1.936, Val loss 1.921\n",
      "Ep 1 (Step 004325): Train loss 1.800, Val loss 1.921\n",
      "Ep 1 (Step 004330): Train loss 1.770, Val loss 1.920\n",
      "Ep 1 (Step 004335): Train loss 1.543, Val loss 1.920\n",
      "Ep 1 (Step 004340): Train loss 1.776, Val loss 1.921\n",
      "Ep 1 (Step 004345): Train loss 1.924, Val loss 1.922\n",
      "Ep 1 (Step 004350): Train loss 1.458, Val loss 1.923\n",
      "Ep 1 (Step 004355): Train loss 1.683, Val loss 1.926\n",
      "Ep 1 (Step 004360): Train loss 1.480, Val loss 1.929\n",
      "Ep 1 (Step 004365): Train loss 1.635, Val loss 1.927\n",
      "Ep 1 (Step 004370): Train loss 1.848, Val loss 1.926\n",
      "Ep 1 (Step 004375): Train loss 1.665, Val loss 1.927\n",
      "Ep 1 (Step 004380): Train loss 1.694, Val loss 1.925\n",
      "Ep 1 (Step 004385): Train loss 1.926, Val loss 1.922\n",
      "Ep 1 (Step 004390): Train loss 1.928, Val loss 1.922\n",
      "Ep 1 (Step 004395): Train loss 1.803, Val loss 1.923\n",
      "Ep 1 (Step 004400): Train loss 1.580, Val loss 1.924\n",
      "Ep 1 (Step 004405): Train loss 1.968, Val loss 1.926\n",
      "Ep 1 (Step 004410): Train loss 1.780, Val loss 1.927\n",
      "Ep 1 (Step 004415): Train loss 2.064, Val loss 1.926\n",
      "Ep 1 (Step 004420): Train loss 1.969, Val loss 1.926\n",
      "Ep 1 (Step 004425): Train loss 1.613, Val loss 1.925\n",
      "Ep 1 (Step 004430): Train loss 1.736, Val loss 1.925\n",
      "Ep 1 (Step 004435): Train loss 1.541, Val loss 1.924\n",
      "Ep 1 (Step 004440): Train loss 1.699, Val loss 1.922\n",
      "Ep 1 (Step 004445): Train loss 1.625, Val loss 1.922\n",
      "Ep 1 (Step 004450): Train loss 1.534, Val loss 1.924\n",
      "Ep 1 (Step 004455): Train loss 1.599, Val loss 1.929\n",
      "Ep 1 (Step 004460): Train loss 1.954, Val loss 1.930\n",
      "Ep 1 (Step 004465): Train loss 1.966, Val loss 1.930\n",
      "Ep 1 (Step 004470): Train loss 1.859, Val loss 1.928\n",
      "Ep 1 (Step 004475): Train loss 1.930, Val loss 1.926\n",
      "Ep 1 (Step 004480): Train loss 1.599, Val loss 1.924\n",
      "Ep 1 (Step 004485): Train loss 1.658, Val loss 1.922\n",
      "Ep 1 (Step 004490): Train loss 1.626, Val loss 1.920\n",
      "Ep 1 (Step 004495): Train loss 1.669, Val loss 1.920\n",
      "Ep 1 (Step 004500): Train loss 1.395, Val loss 1.920\n",
      "Ep 1 (Step 004505): Train loss 1.734, Val loss 1.921\n",
      "Ep 1 (Step 004510): Train loss 1.651, Val loss 1.922\n",
      "Ep 1 (Step 004515): Train loss 1.720, Val loss 1.921\n",
      "Ep 1 (Step 004520): Train loss 1.769, Val loss 1.920\n",
      "Ep 1 (Step 004525): Train loss 1.814, Val loss 1.920\n",
      "Ep 1 (Step 004530): Train loss 1.562, Val loss 1.921\n",
      "Ep 1 (Step 004535): Train loss 1.770, Val loss 1.925\n",
      "Ep 1 (Step 004540): Train loss 1.654, Val loss 1.928\n",
      "Ep 1 (Step 004545): Train loss 1.593, Val loss 1.927\n",
      "Ep 1 (Step 004550): Train loss 1.683, Val loss 1.926\n",
      "Ep 1 (Step 004555): Train loss 1.782, Val loss 1.924\n",
      "Ep 1 (Step 004560): Train loss 1.810, Val loss 1.924\n",
      "Ep 1 (Step 004565): Train loss 1.868, Val loss 1.926\n",
      "Ep 1 (Step 004570): Train loss 1.823, Val loss 1.928\n",
      "Ep 1 (Step 004575): Train loss 1.972, Val loss 1.928\n",
      "Ep 1 (Step 004580): Train loss 1.895, Val loss 1.928\n",
      "Ep 1 (Step 004585): Train loss 1.986, Val loss 1.927\n",
      "Ep 1 (Step 004590): Train loss 1.573, Val loss 1.926\n",
      "Ep 1 (Step 004595): Train loss 1.765, Val loss 1.925\n",
      "Ep 1 (Step 004600): Train loss 1.928, Val loss 1.923\n",
      "Ep 1 (Step 004605): Train loss 1.898, Val loss 1.922\n",
      "Ep 1 (Step 004610): Train loss 1.728, Val loss 1.925\n",
      "Ep 1 (Step 004615): Train loss 1.841, Val loss 1.926\n",
      "Ep 1 (Step 004620): Train loss 1.919, Val loss 1.927\n",
      "Ep 1 (Step 004625): Train loss 1.568, Val loss 1.926\n",
      "Ep 1 (Step 004630): Train loss 1.801, Val loss 1.926\n",
      "Ep 1 (Step 004635): Train loss 1.682, Val loss 1.926\n",
      "Ep 1 (Step 004640): Train loss 2.048, Val loss 1.928\n",
      "Ep 1 (Step 004645): Train loss 1.691, Val loss 1.929\n",
      "Ep 1 (Step 004650): Train loss 1.710, Val loss 1.928\n",
      "Ep 1 (Step 004655): Train loss 2.069, Val loss 1.926\n",
      "Ep 1 (Step 004660): Train loss 1.586, Val loss 1.927\n",
      "Ep 1 (Step 004665): Train loss 1.802, Val loss 1.930\n",
      "Ep 1 (Step 004670): Train loss 1.566, Val loss 1.930\n",
      "Ep 1 (Step 004675): Train loss 1.536, Val loss 1.930\n",
      "Ep 1 (Step 004680): Train loss 1.666, Val loss 1.930\n",
      "Ep 1 (Step 004685): Train loss 1.782, Val loss 1.931\n",
      "Ep 1 (Step 004690): Train loss 1.941, Val loss 1.931\n",
      "Ep 1 (Step 004695): Train loss 1.801, Val loss 1.931\n",
      "Ep 1 (Step 004700): Train loss 1.390, Val loss 1.931\n",
      "Ep 1 (Step 004705): Train loss 1.534, Val loss 1.931\n",
      "Ep 1 (Step 004710): Train loss 1.825, Val loss 1.932\n",
      "Ep 1 (Step 004715): Train loss 1.703, Val loss 1.931\n",
      "Ep 1 (Step 004720): Train loss 1.609, Val loss 1.930\n",
      "Ep 1 (Step 004725): Train loss 1.845, Val loss 1.930\n",
      "Ep 1 (Step 004730): Train loss 1.599, Val loss 1.929\n",
      "Ep 1 (Step 004735): Train loss 1.904, Val loss 1.928\n",
      "Ep 1 (Step 004740): Train loss 1.432, Val loss 1.926\n",
      "Ep 1 (Step 004745): Train loss 1.941, Val loss 1.924\n",
      "Ep 1 (Step 004750): Train loss 1.854, Val loss 1.923\n",
      "Ep 1 (Step 004755): Train loss 1.879, Val loss 1.923\n",
      "Ep 1 (Step 004760): Train loss 1.769, Val loss 1.923\n",
      "Ep 1 (Step 004765): Train loss 1.927, Val loss 1.924\n",
      "Ep 1 (Step 004770): Train loss 1.794, Val loss 1.924\n",
      "Ep 1 (Step 004775): Train loss 1.903, Val loss 1.925\n",
      "Ep 1 (Step 004780): Train loss 1.855, Val loss 1.921\n",
      "Ep 1 (Step 004785): Train loss 1.992, Val loss 1.920\n",
      "Ep 1 (Step 004790): Train loss 1.837, Val loss 1.918\n",
      "Ep 1 (Step 004795): Train loss 1.763, Val loss 1.915\n",
      "Ep 1 (Step 004800): Train loss 1.786, Val loss 1.915\n",
      "Ep 1 (Step 004805): Train loss 1.828, Val loss 1.915\n",
      "Ep 1 (Step 004810): Train loss 2.066, Val loss 1.914\n",
      "Ep 1 (Step 004815): Train loss 1.728, Val loss 1.912\n",
      "Ep 1 (Step 004820): Train loss 1.512, Val loss 1.914\n",
      "Ep 1 (Step 004825): Train loss 1.626, Val loss 1.914\n",
      "Ep 1 (Step 004830): Train loss 1.912, Val loss 1.917\n",
      "Ep 1 (Step 004835): Train loss 1.601, Val loss 1.918\n",
      "Ep 1 (Step 004840): Train loss 1.755, Val loss 1.918\n",
      "Ep 1 (Step 004845): Train loss 1.800, Val loss 1.919\n",
      "Ep 1 (Step 004850): Train loss 1.814, Val loss 1.919\n",
      "Ep 1 (Step 004855): Train loss 1.640, Val loss 1.918\n",
      "Ep 1 (Step 004860): Train loss 1.838, Val loss 1.918\n",
      "Ep 1 (Step 004865): Train loss 1.766, Val loss 1.919\n",
      "Ep 1 (Step 004870): Train loss 1.718, Val loss 1.919\n",
      "Ep 1 (Step 004875): Train loss 1.688, Val loss 1.918\n",
      "Ep 1 (Step 004880): Train loss 1.819, Val loss 1.917\n",
      "Ep 1 (Step 004885): Train loss 1.861, Val loss 1.917\n",
      "Ep 1 (Step 004890): Train loss 1.713, Val loss 1.917\n",
      "Ep 1 (Step 004895): Train loss 1.520, Val loss 1.917\n",
      "Ep 1 (Step 004900): Train loss 2.021, Val loss 1.919\n",
      "Ep 1 (Step 004905): Train loss 1.590, Val loss 1.921\n",
      "Ep 1 (Step 004910): Train loss 1.714, Val loss 1.920\n",
      "Ep 1 (Step 004915): Train loss 1.948, Val loss 1.921\n",
      "Ep 1 (Step 004920): Train loss 1.696, Val loss 1.921\n",
      "Ep 1 (Step 004925): Train loss 1.911, Val loss 1.921\n",
      "Ep 1 (Step 004930): Train loss 1.898, Val loss 1.920\n",
      "Ep 1 (Step 004935): Train loss 1.380, Val loss 1.920\n",
      "Ep 1 (Step 004940): Train loss 1.759, Val loss 1.919\n",
      "Ep 1 (Step 004945): Train loss 1.676, Val loss 1.918\n",
      "Ep 1 (Step 004950): Train loss 1.813, Val loss 1.916\n",
      "Ep 1 (Step 004955): Train loss 1.920, Val loss 1.916\n",
      "Ep 1 (Step 004960): Train loss 1.852, Val loss 1.917\n",
      "Ep 1 (Step 004965): Train loss 1.510, Val loss 1.918\n",
      "Ep 1 (Step 004970): Train loss 1.505, Val loss 1.918\n",
      "Ep 1 (Step 004975): Train loss 1.910, Val loss 1.921\n",
      "Ep 1 (Step 004980): Train loss 1.639, Val loss 1.924\n",
      "Ep 1 (Step 004985): Train loss 1.712, Val loss 1.924\n",
      "Ep 1 (Step 004990): Train loss 2.066, Val loss 1.925\n",
      "Ep 1 (Step 004995): Train loss 1.754, Val loss 1.925\n",
      "Ep 1 (Step 005000): Train loss 1.959, Val loss 1.923\n",
      "Ep 1 (Step 005005): Train loss 1.685, Val loss 1.925\n",
      "Ep 1 (Step 005010): Train loss 1.987, Val loss 1.929\n",
      "Ep 1 (Step 005015): Train loss 1.525, Val loss 1.930\n",
      "Ep 1 (Step 005020): Train loss 1.670, Val loss 1.929\n",
      "Ep 1 (Step 005025): Train loss 1.699, Val loss 1.927\n",
      "Ep 1 (Step 005030): Train loss 1.783, Val loss 1.924\n",
      "Ep 1 (Step 005035): Train loss 1.428, Val loss 1.922\n",
      "Ep 1 (Step 005040): Train loss 1.730, Val loss 1.920\n",
      "Ep 1 (Step 005045): Train loss 1.804, Val loss 1.921\n",
      "Ep 1 (Step 005050): Train loss 1.587, Val loss 1.922\n",
      "Ep 1 (Step 005055): Train loss 2.108, Val loss 1.921\n",
      "Ep 1 (Step 005060): Train loss 1.832, Val loss 1.920\n",
      "Ep 1 (Step 005065): Train loss 1.748, Val loss 1.921\n",
      "Ep 1 (Step 005070): Train loss 1.655, Val loss 1.923\n",
      "Ep 1 (Step 005075): Train loss 1.905, Val loss 1.924\n",
      "Ep 1 (Step 005080): Train loss 1.655, Val loss 1.924\n",
      "Ep 1 (Step 005085): Train loss 1.621, Val loss 1.922\n",
      "Ep 1 (Step 005090): Train loss 1.709, Val loss 1.922\n",
      "Ep 1 (Step 005095): Train loss 1.533, Val loss 1.922\n",
      "Ep 1 (Step 005100): Train loss 1.863, Val loss 1.921\n",
      "Ep 1 (Step 005105): Train loss 1.931, Val loss 1.921\n",
      "Ep 1 (Step 005110): Train loss 1.696, Val loss 1.919\n",
      "Ep 1 (Step 005115): Train loss 1.693, Val loss 1.920\n",
      "Ep 1 (Step 005120): Train loss 1.723, Val loss 1.920\n",
      "Ep 1 (Step 005125): Train loss 1.712, Val loss 1.920\n",
      "Ep 1 (Step 005130): Train loss 1.687, Val loss 1.919\n",
      "Ep 1 (Step 005135): Train loss 1.637, Val loss 1.920\n",
      "Ep 1 (Step 005140): Train loss 1.622, Val loss 1.923\n",
      "Ep 1 (Step 005145): Train loss 1.650, Val loss 1.926\n",
      "Ep 1 (Step 005150): Train loss 1.758, Val loss 1.927\n",
      "Ep 1 (Step 005155): Train loss 1.826, Val loss 1.924\n",
      "Ep 1 (Step 005160): Train loss 1.924, Val loss 1.922\n",
      "Ep 1 (Step 005165): Train loss 1.816, Val loss 1.920\n",
      "Ep 1 (Step 005170): Train loss 1.396, Val loss 1.920\n",
      "Ep 1 (Step 005175): Train loss 1.535, Val loss 1.922\n",
      "Ep 1 (Step 005180): Train loss 1.966, Val loss 1.922\n",
      "Ep 1 (Step 005185): Train loss 1.643, Val loss 1.923\n",
      "Ep 1 (Step 005190): Train loss 1.761, Val loss 1.925\n",
      "Ep 1 (Step 005195): Train loss 1.768, Val loss 1.925\n",
      "Ep 1 (Step 005200): Train loss 1.719, Val loss 1.925\n",
      "Ep 1 (Step 005205): Train loss 1.731, Val loss 1.926\n",
      "Ep 1 (Step 005210): Train loss 1.839, Val loss 1.926\n",
      "Ep 1 (Step 005215): Train loss 1.819, Val loss 1.927\n",
      "Ep 1 (Step 005220): Train loss 1.931, Val loss 1.928\n",
      "Ep 1 (Step 005225): Train loss 1.842, Val loss 1.928\n",
      "Ep 1 (Step 005230): Train loss 1.752, Val loss 1.926\n",
      "Ep 1 (Step 005235): Train loss 1.788, Val loss 1.927\n",
      "Ep 1 (Step 005240): Train loss 1.646, Val loss 1.927\n",
      "Ep 1 (Step 005245): Train loss 1.860, Val loss 1.927\n",
      "Ep 1 (Step 005250): Train loss 1.505, Val loss 1.926\n",
      "Ep 1 (Step 005255): Train loss 1.582, Val loss 1.926\n",
      "Ep 1 (Step 005260): Train loss 1.710, Val loss 1.926\n",
      "Ep 1 (Step 005265): Train loss 1.903, Val loss 1.925\n",
      "Ep 1 (Step 005270): Train loss 1.269, Val loss 1.925\n",
      "Ep 1 (Step 005275): Train loss 1.869, Val loss 1.925\n",
      "Ep 1 (Step 005280): Train loss 1.863, Val loss 1.924\n",
      "Ep 1 (Step 005285): Train loss 1.681, Val loss 1.925\n",
      "Ep 1 (Step 005290): Train loss 1.529, Val loss 1.927\n",
      "Ep 1 (Step 005295): Train loss 1.897, Val loss 1.927\n",
      "Ep 1 (Step 005300): Train loss 1.862, Val loss 1.927\n",
      "Ep 1 (Step 005305): Train loss 1.659, Val loss 1.925\n",
      "Ep 1 (Step 005310): Train loss 1.681, Val loss 1.923\n",
      "Ep 1 (Step 005315): Train loss 1.872, Val loss 1.923\n",
      "Ep 1 (Step 005320): Train loss 1.872, Val loss 1.922\n",
      "Ep 1 (Step 005325): Train loss 1.714, Val loss 1.923\n",
      "Ep 1 (Step 005330): Train loss 1.732, Val loss 1.923\n",
      "Ep 1 (Step 005335): Train loss 1.654, Val loss 1.924\n",
      "Ep 1 (Step 005340): Train loss 1.896, Val loss 1.924\n",
      "Ep 1 (Step 005345): Train loss 1.553, Val loss 1.923\n",
      "Ep 1 (Step 005350): Train loss 1.830, Val loss 1.922\n",
      "Ep 1 (Step 005355): Train loss 1.802, Val loss 1.921\n",
      "Ep 1 (Step 005360): Train loss 2.032, Val loss 1.922\n",
      "Ep 1 (Step 005365): Train loss 1.917, Val loss 1.923\n",
      "Ep 1 (Step 005370): Train loss 1.842, Val loss 1.925\n",
      "Ep 1 (Step 005375): Train loss 1.568, Val loss 1.916\n",
      "Ep 1 (Step 005380): Train loss 1.493, Val loss 1.902\n",
      "Ep 1 (Step 005385): Train loss 2.007, Val loss 1.895\n",
      "Ep 1 (Step 005390): Train loss 1.969, Val loss 1.892\n",
      "Ep 1 (Step 005395): Train loss 1.874, Val loss 1.890\n",
      "Ep 1 (Step 005400): Train loss 1.797, Val loss 1.889\n",
      "Ep 1 (Step 005405): Train loss 1.748, Val loss 1.887\n",
      "Ep 1 (Step 005410): Train loss 1.636, Val loss 1.887\n",
      "Ep 1 (Step 005415): Train loss 1.805, Val loss 1.880\n",
      "Ep 1 (Step 005420): Train loss 1.746, Val loss 1.867\n",
      "Ep 1 (Step 005425): Train loss 1.841, Val loss 1.861\n",
      "Ep 1 (Step 005430): Train loss 1.691, Val loss 1.859\n",
      "Ep 1 (Step 005435): Train loss 1.954, Val loss 1.858\n",
      "Ep 1 (Step 005440): Train loss 1.472, Val loss 1.857\n",
      "Ep 1 (Step 005445): Train loss 1.656, Val loss 1.856\n",
      "Ep 1 (Step 005450): Train loss 1.757, Val loss 1.856\n",
      "Ep 1 (Step 005455): Train loss 1.744, Val loss 1.857\n",
      "Ep 1 (Step 005460): Train loss 1.729, Val loss 1.858\n",
      "Ep 1 (Step 005465): Train loss 1.392, Val loss 1.860\n",
      "Ep 1 (Step 005470): Train loss 1.700, Val loss 1.863\n",
      "Ep 1 (Step 005475): Train loss 1.566, Val loss 1.863\n",
      "Ep 1 (Step 005480): Train loss 2.080, Val loss 1.863\n",
      "Ep 1 (Step 005485): Train loss 1.686, Val loss 1.864\n",
      "Ep 1 (Step 005490): Train loss 1.532, Val loss 1.865\n",
      "Ep 1 (Step 005495): Train loss 2.037, Val loss 1.864\n",
      "Ep 1 (Step 005500): Train loss 1.589, Val loss 1.863\n",
      "Ep 1 (Step 005505): Train loss 1.619, Val loss 1.865\n",
      "Ep 1 (Step 005510): Train loss 1.931, Val loss 1.864\n",
      "Ep 1 (Step 005515): Train loss 1.512, Val loss 1.865\n",
      "Ep 1 (Step 005520): Train loss 1.617, Val loss 1.868\n",
      "Ep 1 (Step 005525): Train loss 1.982, Val loss 1.867\n",
      "Ep 1 (Step 005530): Train loss 1.568, Val loss 1.865\n",
      "Ep 1 (Step 005535): Train loss 1.649, Val loss 1.862\n",
      "Ep 1 (Step 005540): Train loss 1.751, Val loss 1.861\n",
      "Ep 1 (Step 005545): Train loss 1.747, Val loss 1.861\n",
      "Ep 1 (Step 005550): Train loss 1.625, Val loss 1.861\n",
      "Ep 1 (Step 005555): Train loss 1.705, Val loss 1.861\n",
      "Ep 1 (Step 005560): Train loss 1.716, Val loss 1.863\n",
      "Ep 1 (Step 005565): Train loss 1.842, Val loss 1.864\n",
      "Ep 1 (Step 005570): Train loss 2.099, Val loss 1.864\n",
      "Ep 1 (Step 005575): Train loss 1.570, Val loss 1.866\n",
      "Ep 1 (Step 005580): Train loss 1.833, Val loss 1.867\n",
      "Ep 1 (Step 005585): Train loss 1.829, Val loss 1.867\n",
      "Ep 1 (Step 005590): Train loss 1.865, Val loss 1.867\n",
      "Ep 1 (Step 005595): Train loss 1.607, Val loss 1.868\n",
      "Ep 1 (Step 005600): Train loss 1.631, Val loss 1.868\n",
      "Ep 1 (Step 005605): Train loss 1.595, Val loss 1.867\n",
      "Ep 1 (Step 005610): Train loss 1.883, Val loss 1.868\n",
      "Ep 1 (Step 005615): Train loss 1.833, Val loss 1.868\n",
      "Ep 1 (Step 005620): Train loss 1.763, Val loss 1.869\n",
      "Ep 1 (Step 005625): Train loss 1.615, Val loss 1.869\n",
      "Ep 1 (Step 005630): Train loss 1.782, Val loss 1.868\n",
      "Ep 1 (Step 005635): Train loss 1.614, Val loss 1.869\n",
      "Ep 1 (Step 005640): Train loss 1.996, Val loss 1.868\n",
      "Ep 1 (Step 005645): Train loss 1.631, Val loss 1.867\n",
      "Ep 1 (Step 005650): Train loss 1.784, Val loss 1.867\n",
      "Ep 1 (Step 005655): Train loss 1.562, Val loss 1.866\n",
      "Ep 1 (Step 005660): Train loss 1.991, Val loss 1.864\n",
      "Ep 1 (Step 005665): Train loss 2.014, Val loss 1.866\n",
      "Ep 1 (Step 005670): Train loss 1.736, Val loss 1.868\n",
      "Ep 1 (Step 005675): Train loss 1.638, Val loss 1.869\n",
      "Ep 1 (Step 005680): Train loss 1.780, Val loss 1.869\n",
      "Ep 1 (Step 005685): Train loss 1.732, Val loss 1.867\n",
      "Ep 1 (Step 005690): Train loss 1.864, Val loss 1.867\n",
      "Ep 1 (Step 005695): Train loss 1.700, Val loss 1.866\n",
      "Ep 1 (Step 005700): Train loss 1.918, Val loss 1.866\n",
      "Ep 1 (Step 005705): Train loss 1.799, Val loss 1.865\n",
      "Ep 1 (Step 005710): Train loss 1.707, Val loss 1.864\n",
      "Ep 1 (Step 005715): Train loss 1.734, Val loss 1.865\n",
      "Ep 1 (Step 005720): Train loss 1.952, Val loss 1.865\n",
      "Ep 1 (Step 005725): Train loss 1.897, Val loss 1.864\n",
      "Ep 1 (Step 005730): Train loss 1.892, Val loss 1.865\n",
      "Ep 1 (Step 005735): Train loss 1.591, Val loss 1.866\n",
      "Ep 1 (Step 005740): Train loss 1.820, Val loss 1.867\n",
      "Ep 1 (Step 005745): Train loss 1.704, Val loss 1.869\n",
      "Ep 1 (Step 005750): Train loss 1.856, Val loss 1.870\n",
      "Ep 1 (Step 005755): Train loss 1.476, Val loss 1.873\n",
      "Ep 1 (Step 005760): Train loss 1.768, Val loss 1.874\n",
      "Ep 1 (Step 005765): Train loss 1.781, Val loss 1.873\n",
      "Ep 1 (Step 005770): Train loss 1.832, Val loss 1.873\n",
      "Ep 1 (Step 005775): Train loss 1.728, Val loss 1.875\n",
      "Ep 1 (Step 005780): Train loss 1.451, Val loss 1.875\n",
      "Ep 1 (Step 005785): Train loss 1.623, Val loss 1.876\n",
      "Ep 1 (Step 005790): Train loss 1.884, Val loss 1.876\n",
      "Ep 1 (Step 005795): Train loss 1.751, Val loss 1.875\n",
      "Ep 1 (Step 005800): Train loss 1.902, Val loss 1.873\n",
      "Ep 1 (Step 005805): Train loss 1.602, Val loss 1.873\n",
      "Ep 1 (Step 005810): Train loss 1.761, Val loss 1.873\n",
      "Ep 1 (Step 005815): Train loss 1.612, Val loss 1.874\n",
      "Ep 1 (Step 005820): Train loss 1.592, Val loss 1.872\n",
      "Ep 1 (Step 005825): Train loss 1.563, Val loss 1.868\n",
      "Ep 1 (Step 005830): Train loss 1.615, Val loss 1.868\n",
      "Ep 1 (Step 005835): Train loss 1.788, Val loss 1.867\n",
      "Ep 1 (Step 005840): Train loss 1.938, Val loss 1.870\n",
      "Ep 1 (Step 005845): Train loss 1.994, Val loss 1.871\n",
      "Ep 1 (Step 005850): Train loss 1.667, Val loss 1.869\n",
      "Ep 1 (Step 005855): Train loss 1.733, Val loss 1.866\n",
      "Ep 1 (Step 005860): Train loss 1.437, Val loss 1.866\n",
      "Ep 1 (Step 005865): Train loss 1.652, Val loss 1.866\n",
      "Ep 1 (Step 005870): Train loss 1.433, Val loss 1.865\n",
      "Ep 1 (Step 005875): Train loss 1.543, Val loss 1.864\n",
      "Ep 1 (Step 005880): Train loss 1.767, Val loss 1.864\n",
      "Ep 1 (Step 005885): Train loss 1.949, Val loss 1.864\n",
      "Ep 1 (Step 005890): Train loss 1.674, Val loss 1.866\n",
      "Ep 1 (Step 005895): Train loss 1.771, Val loss 1.868\n",
      "Ep 1 (Step 005900): Train loss 1.864, Val loss 1.871\n",
      "Ep 1 (Step 005905): Train loss 1.750, Val loss 1.873\n",
      "Ep 1 (Step 005910): Train loss 1.481, Val loss 1.874\n",
      "Ep 1 (Step 005915): Train loss 1.794, Val loss 1.874\n",
      "Ep 1 (Step 005920): Train loss 1.447, Val loss 1.872\n",
      "Ep 1 (Step 005925): Train loss 1.680, Val loss 1.870\n",
      "Ep 1 (Step 005930): Train loss 1.934, Val loss 1.871\n",
      "Ep 1 (Step 005935): Train loss 1.573, Val loss 1.870\n",
      "Ep 1 (Step 005940): Train loss 1.648, Val loss 1.869\n",
      "Ep 1 (Step 005945): Train loss 1.727, Val loss 1.868\n",
      "Ep 1 (Step 005950): Train loss 1.869, Val loss 1.869\n",
      "Ep 1 (Step 005955): Train loss 1.502, Val loss 1.870\n",
      "Ep 1 (Step 005960): Train loss 1.684, Val loss 1.871\n",
      "Ep 1 (Step 005965): Train loss 1.612, Val loss 1.871\n",
      "Ep 1 (Step 005970): Train loss 1.723, Val loss 1.872\n",
      "Ep 1 (Step 005975): Train loss 1.541, Val loss 1.871\n",
      "Ep 1 (Step 005980): Train loss 1.801, Val loss 1.870\n",
      "Ep 1 (Step 005985): Train loss 1.748, Val loss 1.870\n",
      "Ep 1 (Step 005990): Train loss 1.718, Val loss 1.869\n",
      "Ep 1 (Step 005995): Train loss 1.972, Val loss 1.870\n",
      "Ep 1 (Step 006000): Train loss 1.647, Val loss 1.871\n",
      "Ep 1 (Step 006005): Train loss 1.813, Val loss 1.871\n",
      "Ep 1 (Step 006010): Train loss 1.536, Val loss 1.871\n",
      "Ep 1 (Step 006015): Train loss 1.617, Val loss 1.870\n",
      "Ep 1 (Step 006020): Train loss 1.591, Val loss 1.869\n",
      "Ep 1 (Step 006025): Train loss 1.846, Val loss 1.868\n",
      "Ep 1 (Step 006030): Train loss 1.781, Val loss 1.872\n",
      "Ep 1 (Step 006035): Train loss 1.713, Val loss 1.875\n",
      "Ep 1 (Step 006040): Train loss 1.605, Val loss 1.876\n",
      "Ep 1 (Step 006045): Train loss 1.525, Val loss 1.876\n",
      "Ep 1 (Step 006050): Train loss 1.782, Val loss 1.875\n",
      "Ep 1 (Step 006055): Train loss 1.825, Val loss 1.874\n",
      "Ep 1 (Step 006060): Train loss 1.662, Val loss 1.874\n",
      "Ep 1 (Step 006065): Train loss 1.801, Val loss 1.873\n",
      "Ep 1 (Step 006070): Train loss 1.648, Val loss 1.874\n",
      "Ep 1 (Step 006075): Train loss 1.667, Val loss 1.875\n",
      "Ep 1 (Step 006080): Train loss 1.706, Val loss 1.875\n",
      "Ep 1 (Step 006085): Train loss 1.470, Val loss 1.874\n",
      "Ep 1 (Step 006090): Train loss 1.791, Val loss 1.875\n",
      "Ep 1 (Step 006095): Train loss 1.650, Val loss 1.876\n",
      "Ep 1 (Step 006100): Train loss 1.619, Val loss 1.877\n",
      "Ep 1 (Step 006105): Train loss 1.916, Val loss 1.877\n",
      "Ep 1 (Step 006110): Train loss 1.630, Val loss 1.875\n",
      "Ep 1 (Step 006115): Train loss 1.731, Val loss 1.874\n",
      "Ep 1 (Step 006120): Train loss 1.805, Val loss 1.874\n",
      "Ep 1 (Step 006125): Train loss 1.689, Val loss 1.874\n",
      "Ep 1 (Step 006130): Train loss 1.413, Val loss 1.873\n",
      "Ep 1 (Step 006135): Train loss 1.537, Val loss 1.872\n",
      "Ep 1 (Step 006140): Train loss 1.495, Val loss 1.871\n",
      "Ep 1 (Step 006145): Train loss 1.785, Val loss 1.870\n",
      "Ep 1 (Step 006150): Train loss 2.054, Val loss 1.869\n",
      "Ep 1 (Step 006155): Train loss 1.840, Val loss 1.869\n",
      "Ep 1 (Step 006160): Train loss 1.784, Val loss 1.869\n",
      "Ep 1 (Step 006165): Train loss 2.034, Val loss 1.868\n",
      "Ep 1 (Step 006170): Train loss 1.605, Val loss 1.869\n",
      "Ep 1 (Step 006175): Train loss 1.840, Val loss 1.869\n",
      "Ep 1 (Step 006180): Train loss 1.659, Val loss 1.868\n",
      "Ep 1 (Step 006185): Train loss 1.688, Val loss 1.866\n",
      "Ep 1 (Step 006190): Train loss 1.878, Val loss 1.866\n",
      "Ep 1 (Step 006195): Train loss 1.653, Val loss 1.869\n",
      "Ep 1 (Step 006200): Train loss 1.668, Val loss 1.871\n",
      "Ep 1 (Step 006205): Train loss 1.861, Val loss 1.870\n",
      "Ep 1 (Step 006210): Train loss 1.908, Val loss 1.869\n",
      "Ep 1 (Step 006215): Train loss 2.001, Val loss 1.868\n",
      "Ep 1 (Step 006220): Train loss 1.607, Val loss 1.869\n",
      "Ep 1 (Step 006225): Train loss 1.477, Val loss 1.871\n",
      "Ep 1 (Step 006230): Train loss 1.878, Val loss 1.872\n",
      "Ep 1 (Step 006235): Train loss 1.743, Val loss 1.871\n",
      "Ep 1 (Step 006240): Train loss 1.721, Val loss 1.871\n",
      "Ep 1 (Step 006245): Train loss 1.742, Val loss 1.869\n",
      "Ep 1 (Step 006250): Train loss 1.843, Val loss 1.870\n",
      "Ep 1 (Step 006255): Train loss 1.906, Val loss 1.871\n",
      "Ep 1 (Step 006260): Train loss 1.637, Val loss 1.870\n",
      "Ep 1 (Step 006265): Train loss 1.631, Val loss 1.870\n",
      "Ep 1 (Step 006270): Train loss 1.752, Val loss 1.869\n",
      "Ep 1 (Step 006275): Train loss 1.800, Val loss 1.868\n",
      "Ep 1 (Step 006280): Train loss 1.615, Val loss 1.868\n",
      "Ep 1 (Step 006285): Train loss 1.540, Val loss 1.871\n",
      "Ep 1 (Step 006290): Train loss 2.046, Val loss 1.876\n",
      "Ep 1 (Step 006295): Train loss 2.014, Val loss 1.878\n",
      "Ep 1 (Step 006300): Train loss 1.630, Val loss 1.878\n",
      "Ep 1 (Step 006305): Train loss 1.831, Val loss 1.877\n",
      "Ep 1 (Step 006310): Train loss 1.580, Val loss 1.877\n",
      "Ep 1 (Step 006315): Train loss 1.653, Val loss 1.876\n",
      "Ep 1 (Step 006320): Train loss 1.682, Val loss 1.876\n",
      "Ep 1 (Step 006325): Train loss 1.935, Val loss 1.877\n",
      "Ep 1 (Step 006330): Train loss 1.880, Val loss 1.876\n",
      "Ep 1 (Step 006335): Train loss 1.612, Val loss 1.873\n",
      "Ep 1 (Step 006340): Train loss 1.779, Val loss 1.870\n",
      "Ep 1 (Step 006345): Train loss 1.520, Val loss 1.869\n",
      "Ep 1 (Step 006350): Train loss 1.630, Val loss 1.869\n",
      "Ep 1 (Step 006355): Train loss 1.589, Val loss 1.870\n",
      "Ep 1 (Step 006360): Train loss 1.422, Val loss 1.869\n",
      "Ep 1 (Step 006365): Train loss 1.858, Val loss 1.871\n",
      "Ep 1 (Step 006370): Train loss 1.598, Val loss 1.873\n",
      "Ep 1 (Step 006375): Train loss 1.605, Val loss 1.873\n",
      "Ep 1 (Step 006380): Train loss 1.825, Val loss 1.871\n",
      "Ep 1 (Step 006385): Train loss 1.620, Val loss 1.868\n",
      "Ep 1 (Step 006390): Train loss 1.683, Val loss 1.867\n",
      "Ep 1 (Step 006395): Train loss 1.801, Val loss 1.867\n",
      "Ep 1 (Step 006400): Train loss 1.714, Val loss 1.868\n",
      "Ep 1 (Step 006405): Train loss 1.591, Val loss 1.869\n",
      "Ep 1 (Step 006410): Train loss 1.861, Val loss 1.869\n",
      "Ep 1 (Step 006415): Train loss 1.699, Val loss 1.869\n",
      "Ep 1 (Step 006420): Train loss 1.672, Val loss 1.868\n",
      "Ep 1 (Step 006425): Train loss 1.532, Val loss 1.867\n",
      "Ep 1 (Step 006430): Train loss 1.923, Val loss 1.867\n",
      "Ep 1 (Step 006435): Train loss 1.598, Val loss 1.868\n",
      "Ep 1 (Step 006440): Train loss 1.564, Val loss 1.868\n",
      "Ep 1 (Step 006445): Train loss 1.674, Val loss 1.868\n",
      "Ep 1 (Step 006450): Train loss 1.609, Val loss 1.867\n",
      "Ep 1 (Step 006455): Train loss 1.819, Val loss 1.867\n",
      "Ep 1 (Step 006460): Train loss 1.973, Val loss 1.868\n",
      "Ep 1 (Step 006465): Train loss 1.687, Val loss 1.868\n",
      "Ep 1 (Step 006470): Train loss 1.771, Val loss 1.869\n",
      "Ep 1 (Step 006475): Train loss 1.871, Val loss 1.869\n",
      "Ep 1 (Step 006480): Train loss 1.484, Val loss 1.870\n",
      "Ep 1 (Step 006485): Train loss 1.779, Val loss 1.869\n",
      "Ep 1 (Step 006490): Train loss 1.733, Val loss 1.868\n",
      "Ep 1 (Step 006495): Train loss 1.642, Val loss 1.869\n",
      "Ep 1 (Step 006500): Train loss 1.921, Val loss 1.870\n",
      "Ep 1 (Step 006505): Train loss 1.761, Val loss 1.870\n",
      "Ep 1 (Step 006510): Train loss 1.745, Val loss 1.870\n",
      "Ep 1 (Step 006515): Train loss 1.556, Val loss 1.872\n",
      "Ep 1 (Step 006520): Train loss 1.737, Val loss 1.873\n",
      "Ep 1 (Step 006525): Train loss 1.829, Val loss 1.872\n",
      "Ep 1 (Step 006530): Train loss 1.883, Val loss 1.873\n",
      "Ep 1 (Step 006535): Train loss 1.686, Val loss 1.872\n",
      "Ep 1 (Step 006540): Train loss 1.577, Val loss 1.871\n",
      "Ep 1 (Step 006545): Train loss 1.900, Val loss 1.870\n",
      "Ep 1 (Step 006550): Train loss 1.700, Val loss 1.869\n",
      "Ep 1 (Step 006555): Train loss 1.552, Val loss 1.869\n",
      "Ep 1 (Step 006560): Train loss 1.860, Val loss 1.869\n",
      "Ep 1 (Step 006565): Train loss 1.596, Val loss 1.869\n",
      "Ep 1 (Step 006570): Train loss 1.492, Val loss 1.869\n",
      "Ep 1 (Step 006575): Train loss 1.849, Val loss 1.869\n",
      "Ep 1 (Step 006580): Train loss 1.804, Val loss 1.870\n",
      "Ep 1 (Step 006585): Train loss 1.640, Val loss 1.870\n",
      "Ep 1 (Step 006590): Train loss 2.021, Val loss 1.870\n",
      "Ep 1 (Step 006595): Train loss 1.707, Val loss 1.870\n",
      "Ep 1 (Step 006600): Train loss 1.520, Val loss 1.870\n",
      "Ep 1 (Step 006605): Train loss 1.986, Val loss 1.869\n",
      "Ep 1 (Step 006610): Train loss 1.787, Val loss 1.868\n",
      "Ep 1 (Step 006615): Train loss 2.064, Val loss 1.869\n",
      "Ep 1 (Step 006620): Train loss 1.906, Val loss 1.869\n",
      "Ep 1 (Step 006625): Train loss 1.494, Val loss 1.867\n",
      "Ep 1 (Step 006630): Train loss 1.673, Val loss 1.868\n",
      "Ep 1 (Step 006635): Train loss 1.478, Val loss 1.867\n",
      "Ep 1 (Step 006640): Train loss 1.923, Val loss 1.866\n",
      "Ep 1 (Step 006645): Train loss 1.491, Val loss 1.867\n",
      "Ep 1 (Step 006650): Train loss 1.748, Val loss 1.867\n",
      "Ep 1 (Step 006655): Train loss 1.561, Val loss 1.867\n",
      "Ep 1 (Step 006660): Train loss 2.071, Val loss 1.872\n",
      "Ep 1 (Step 006665): Train loss 1.846, Val loss 1.874\n",
      "Ep 1 (Step 006670): Train loss 1.680, Val loss 1.875\n",
      "Ep 1 (Step 006675): Train loss 1.797, Val loss 1.876\n",
      "Ep 1 (Step 006680): Train loss 1.610, Val loss 1.875\n",
      "Ep 1 (Step 006685): Train loss 1.755, Val loss 1.873\n",
      "Ep 1 (Step 006690): Train loss 1.894, Val loss 1.873\n",
      "Ep 1 (Step 006695): Train loss 1.635, Val loss 1.871\n",
      "Ep 1 (Step 006700): Train loss 1.603, Val loss 1.870\n",
      "Ep 1 (Step 006705): Train loss 1.823, Val loss 1.870\n",
      "Ep 1 (Step 006710): Train loss 1.689, Val loss 1.871\n",
      "Ep 1 (Step 006715): Train loss 1.703, Val loss 1.871\n",
      "Ep 1 (Step 006720): Train loss 1.866, Val loss 1.870\n",
      "Ep 1 (Step 006725): Train loss 1.748, Val loss 1.869\n",
      "Ep 1 (Step 006730): Train loss 1.709, Val loss 1.868\n",
      "Ep 1 (Step 006735): Train loss 1.644, Val loss 1.868\n",
      "Ep 1 (Step 006740): Train loss 1.963, Val loss 1.870\n",
      "Ep 1 (Step 006745): Train loss 1.689, Val loss 1.871\n",
      "Ep 1 (Step 006750): Train loss 1.767, Val loss 1.871\n",
      "Ep 1 (Step 006755): Train loss 2.011, Val loss 1.871\n",
      "Ep 1 (Step 006760): Train loss 1.626, Val loss 1.870\n",
      "Ep 1 (Step 006765): Train loss 1.371, Val loss 1.872\n",
      "Ep 1 (Step 006770): Train loss 1.598, Val loss 1.873\n",
      "Ep 1 (Step 006775): Train loss 1.648, Val loss 1.874\n",
      "Ep 1 (Step 006780): Train loss 1.822, Val loss 1.874\n",
      "Ep 1 (Step 006785): Train loss 1.853, Val loss 1.872\n",
      "Ep 1 (Step 006790): Train loss 1.587, Val loss 1.871\n",
      "Ep 1 (Step 006795): Train loss 1.833, Val loss 1.871\n",
      "Ep 1 (Step 006800): Train loss 1.720, Val loss 1.870\n",
      "Ep 1 (Step 006805): Train loss 1.506, Val loss 1.872\n",
      "Ep 1 (Step 006810): Train loss 1.669, Val loss 1.874\n",
      "Ep 1 (Step 006815): Train loss 1.844, Val loss 1.873\n",
      "Ep 1 (Step 006820): Train loss 1.737, Val loss 1.872\n",
      "Ep 1 (Step 006825): Train loss 1.749, Val loss 1.871\n",
      "Ep 1 (Step 006830): Train loss 1.857, Val loss 1.869\n",
      "Ep 1 (Step 006835): Train loss 1.739, Val loss 1.868\n",
      "Ep 1 (Step 006840): Train loss 1.438, Val loss 1.869\n",
      "Ep 1 (Step 006845): Train loss 1.793, Val loss 1.870\n",
      "Ep 1 (Step 006850): Train loss 1.871, Val loss 1.872\n",
      "Ep 1 (Step 006855): Train loss 1.631, Val loss 1.871\n",
      "Ep 1 (Step 006860): Train loss 1.903, Val loss 1.870\n",
      "Ep 1 (Step 006865): Train loss 2.006, Val loss 1.868\n",
      "Ep 1 (Step 006870): Train loss 1.645, Val loss 1.868\n",
      "Ep 1 (Step 006875): Train loss 1.537, Val loss 1.868\n",
      "Ep 1 (Step 006880): Train loss 1.746, Val loss 1.866\n",
      "Ep 1 (Step 006885): Train loss 1.737, Val loss 1.865\n",
      "Ep 1 (Step 006890): Train loss 1.986, Val loss 1.865\n",
      "Ep 1 (Step 006895): Train loss 1.844, Val loss 1.865\n",
      "Ep 1 (Step 006900): Train loss 1.640, Val loss 1.865\n",
      "Ep 1 (Step 006905): Train loss 1.838, Val loss 1.868\n",
      "Ep 1 (Step 006910): Train loss 1.855, Val loss 1.872\n",
      "Ep 1 (Step 006915): Train loss 1.936, Val loss 1.874\n",
      "Ep 1 (Step 006920): Train loss 1.905, Val loss 1.874\n",
      "Ep 1 (Step 006925): Train loss 1.885, Val loss 1.873\n",
      "Ep 1 (Step 006930): Train loss 2.068, Val loss 1.873\n",
      "Ep 1 (Step 006935): Train loss 1.899, Val loss 1.876\n",
      "Ep 1 (Step 006940): Train loss 1.654, Val loss 1.878\n",
      "Ep 1 (Step 006945): Train loss 1.774, Val loss 1.879\n",
      "Ep 1 (Step 006950): Train loss 1.541, Val loss 1.879\n",
      "Ep 1 (Step 006955): Train loss 2.067, Val loss 1.879\n",
      "Ep 1 (Step 006960): Train loss 1.469, Val loss 1.878\n",
      "Ep 1 (Step 006965): Train loss 1.916, Val loss 1.875\n",
      "Ep 1 (Step 006970): Train loss 2.026, Val loss 1.873\n",
      "Ep 1 (Step 006975): Train loss 1.773, Val loss 1.872\n",
      "Ep 1 (Step 006980): Train loss 1.677, Val loss 1.866\n",
      "Ep 1 (Step 006985): Train loss 1.713, Val loss 1.859\n",
      "Ep 1 (Step 006990): Train loss 1.648, Val loss 1.855\n",
      "Ep 1 (Step 006995): Train loss 1.833, Val loss 1.853\n",
      "Ep 1 (Step 007000): Train loss 1.822, Val loss 1.850\n",
      "Ep 1 (Step 007005): Train loss 1.744, Val loss 1.847\n",
      "Ep 1 (Step 007010): Train loss 1.581, Val loss 1.845\n",
      "Ep 1 (Step 007015): Train loss 1.651, Val loss 1.845\n",
      "Ep 1 (Step 007020): Train loss 1.641, Val loss 1.847\n",
      "Ep 1 (Step 007025): Train loss 1.797, Val loss 1.847\n",
      "Ep 1 (Step 007030): Train loss 1.802, Val loss 1.847\n",
      "Ep 1 (Step 007035): Train loss 1.691, Val loss 1.847\n",
      "Ep 1 (Step 007040): Train loss 1.772, Val loss 1.847\n",
      "Ep 1 (Step 007045): Train loss 1.533, Val loss 1.846\n",
      "Ep 1 (Step 007050): Train loss 1.597, Val loss 1.844\n",
      "Ep 1 (Step 007055): Train loss 1.596, Val loss 1.845\n",
      "Ep 1 (Step 007060): Train loss 1.925, Val loss 1.845\n",
      "Ep 1 (Step 007065): Train loss 1.484, Val loss 1.847\n",
      "Ep 1 (Step 007070): Train loss 1.453, Val loss 1.846\n",
      "Ep 1 (Step 007075): Train loss 1.452, Val loss 1.844\n",
      "Ep 1 (Step 007080): Train loss 1.745, Val loss 1.843\n",
      "Ep 1 (Step 007085): Train loss 2.026, Val loss 1.843\n",
      "Ep 1 (Step 007090): Train loss 1.873, Val loss 1.845\n",
      "Ep 1 (Step 007095): Train loss 1.783, Val loss 1.845\n",
      "Ep 1 (Step 007100): Train loss 1.794, Val loss 1.844\n",
      "Ep 1 (Step 007105): Train loss 1.672, Val loss 1.842\n",
      "Ep 1 (Step 007110): Train loss 1.686, Val loss 1.841\n",
      "Ep 1 (Step 007115): Train loss 1.920, Val loss 1.838\n",
      "Ep 1 (Step 007120): Train loss 1.868, Val loss 1.839\n",
      "Ep 1 (Step 007125): Train loss 1.844, Val loss 1.843\n",
      "Ep 1 (Step 007130): Train loss 1.613, Val loss 1.846\n",
      "Ep 1 (Step 007135): Train loss 1.683, Val loss 1.846\n",
      "Ep 1 (Step 007140): Train loss 1.678, Val loss 1.846\n",
      "Ep 1 (Step 007145): Train loss 1.860, Val loss 1.846\n",
      "Ep 1 (Step 007150): Train loss 1.911, Val loss 1.845\n",
      "Ep 1 (Step 007155): Train loss 1.740, Val loss 1.844\n",
      "Ep 1 (Step 007160): Train loss 1.581, Val loss 1.845\n",
      "Ep 1 (Step 007165): Train loss 1.700, Val loss 1.846\n",
      "Ep 1 (Step 007170): Train loss 1.796, Val loss 1.846\n",
      "Ep 1 (Step 007175): Train loss 1.748, Val loss 1.846\n",
      "Ep 1 (Step 007180): Train loss 1.801, Val loss 1.848\n",
      "Ep 1 (Step 007185): Train loss 1.914, Val loss 1.848\n",
      "Ep 1 (Step 007190): Train loss 1.598, Val loss 1.845\n",
      "Ep 1 (Step 007195): Train loss 1.717, Val loss 1.846\n",
      "Ep 1 (Step 007200): Train loss 1.775, Val loss 1.845\n",
      "Ep 1 (Step 007205): Train loss 1.874, Val loss 1.845\n",
      "Ep 1 (Step 007210): Train loss 1.791, Val loss 1.848\n",
      "Ep 1 (Step 007215): Train loss 1.937, Val loss 1.849\n",
      "Ep 1 (Step 007220): Train loss 1.596, Val loss 1.849\n",
      "Ep 1 (Step 007225): Train loss 1.599, Val loss 1.847\n",
      "Ep 1 (Step 007230): Train loss 1.658, Val loss 1.845\n",
      "Ep 1 (Step 007235): Train loss 1.611, Val loss 1.845\n",
      "Ep 1 (Step 007240): Train loss 1.860, Val loss 1.844\n",
      "Ep 1 (Step 007245): Train loss 1.970, Val loss 1.841\n",
      "Ep 1 (Step 007250): Train loss 1.599, Val loss 1.841\n",
      "Ep 1 (Step 007255): Train loss 1.753, Val loss 1.845\n",
      "Ep 1 (Step 007260): Train loss 2.036, Val loss 1.846\n",
      "Ep 1 (Step 007265): Train loss 1.722, Val loss 1.846\n",
      "Ep 1 (Step 007270): Train loss 1.548, Val loss 1.846\n",
      "Ep 1 (Step 007275): Train loss 1.977, Val loss 1.847\n",
      "Ep 1 (Step 007280): Train loss 1.886, Val loss 1.849\n",
      "Ep 1 (Step 007285): Train loss 1.826, Val loss 1.848\n",
      "Ep 1 (Step 007290): Train loss 1.908, Val loss 1.848\n",
      "Ep 1 (Step 007295): Train loss 1.844, Val loss 1.847\n",
      "Ep 1 (Step 007300): Train loss 1.859, Val loss 1.846\n",
      "Ep 1 (Step 007305): Train loss 1.767, Val loss 1.845\n",
      "Ep 1 (Step 007310): Train loss 1.947, Val loss 1.845\n",
      "Ep 1 (Step 007315): Train loss 1.644, Val loss 1.845\n",
      "Ep 1 (Step 007320): Train loss 1.986, Val loss 1.846\n",
      "Ep 1 (Step 007325): Train loss 1.549, Val loss 1.847\n",
      "Ep 1 (Step 007330): Train loss 1.913, Val loss 1.848\n",
      "Ep 1 (Step 007335): Train loss 1.879, Val loss 1.846\n",
      "Ep 1 (Step 007340): Train loss 1.824, Val loss 1.845\n",
      "Ep 1 (Step 007345): Train loss 1.713, Val loss 1.844\n",
      "Ep 1 (Step 007350): Train loss 1.588, Val loss 1.844\n",
      "Ep 1 (Step 007355): Train loss 1.830, Val loss 1.844\n",
      "Ep 1 (Step 007360): Train loss 1.617, Val loss 1.843\n",
      "Ep 1 (Step 007365): Train loss 2.009, Val loss 1.842\n",
      "Ep 1 (Step 007370): Train loss 1.602, Val loss 1.843\n",
      "Ep 1 (Step 007375): Train loss 1.346, Val loss 1.843\n",
      "Ep 1 (Step 007380): Train loss 1.677, Val loss 1.845\n",
      "Ep 1 (Step 007385): Train loss 1.349, Val loss 1.847\n",
      "Ep 1 (Step 007390): Train loss 1.941, Val loss 1.847\n",
      "Ep 1 (Step 007395): Train loss 1.817, Val loss 1.848\n",
      "Ep 1 (Step 007400): Train loss 1.661, Val loss 1.848\n",
      "Ep 1 (Step 007405): Train loss 1.623, Val loss 1.850\n",
      "Ep 1 (Step 007410): Train loss 1.832, Val loss 1.850\n",
      "Ep 1 (Step 007415): Train loss 1.552, Val loss 1.851\n",
      "Ep 1 (Step 007420): Train loss 1.359, Val loss 1.851\n",
      "Ep 1 (Step 007425): Train loss 1.592, Val loss 1.850\n",
      "Ep 1 (Step 007430): Train loss 1.434, Val loss 1.850\n",
      "Ep 1 (Step 007435): Train loss 1.717, Val loss 1.849\n",
      "Ep 1 (Step 007440): Train loss 1.630, Val loss 1.849\n",
      "Ep 1 (Step 007445): Train loss 1.604, Val loss 1.849\n",
      "Ep 1 (Step 007450): Train loss 1.902, Val loss 1.850\n",
      "Ep 1 (Step 007455): Train loss 1.485, Val loss 1.850\n",
      "Ep 1 (Step 007460): Train loss 1.746, Val loss 1.849\n",
      "Ep 1 (Step 007465): Train loss 1.584, Val loss 1.846\n",
      "Ep 1 (Step 007470): Train loss 1.751, Val loss 1.845\n",
      "Ep 1 (Step 007475): Train loss 1.860, Val loss 1.845\n",
      "Ep 1 (Step 007480): Train loss 1.938, Val loss 1.844\n",
      "Ep 1 (Step 007485): Train loss 1.883, Val loss 1.844\n",
      "Ep 1 (Step 007490): Train loss 1.853, Val loss 1.843\n",
      "Ep 1 (Step 007495): Train loss 1.857, Val loss 1.843\n",
      "Ep 1 (Step 007500): Train loss 2.011, Val loss 1.845\n",
      "Ep 1 (Step 007505): Train loss 1.847, Val loss 1.846\n",
      "Ep 1 (Step 007510): Train loss 1.803, Val loss 1.846\n",
      "Ep 1 (Step 007515): Train loss 1.785, Val loss 1.846\n",
      "Ep 1 (Step 007520): Train loss 1.847, Val loss 1.846\n",
      "Ep 1 (Step 007525): Train loss 1.514, Val loss 1.844\n",
      "Ep 1 (Step 007530): Train loss 1.694, Val loss 1.844\n",
      "Ep 1 (Step 007535): Train loss 1.852, Val loss 1.843\n",
      "Ep 1 (Step 007540): Train loss 1.819, Val loss 1.846\n",
      "Ep 1 (Step 007545): Train loss 1.876, Val loss 1.846\n",
      "Ep 1 (Step 007550): Train loss 1.865, Val loss 1.845\n",
      "Ep 1 (Step 007555): Train loss 1.943, Val loss 1.846\n",
      "Ep 1 (Step 007560): Train loss 1.349, Val loss 1.846\n",
      "Ep 1 (Step 007565): Train loss 1.589, Val loss 1.845\n",
      "Ep 1 (Step 007570): Train loss 1.771, Val loss 1.847\n",
      "Ep 1 (Step 007575): Train loss 1.689, Val loss 1.849\n",
      "Ep 1 (Step 007580): Train loss 1.467, Val loss 1.851\n",
      "Ep 1 (Step 007585): Train loss 1.823, Val loss 1.850\n",
      "Ep 1 (Step 007590): Train loss 1.603, Val loss 1.849\n",
      "Ep 1 (Step 007595): Train loss 1.546, Val loss 1.847\n",
      "Ep 1 (Step 007600): Train loss 1.845, Val loss 1.846\n",
      "Ep 1 (Step 007605): Train loss 1.707, Val loss 1.845\n",
      "Ep 1 (Step 007610): Train loss 1.798, Val loss 1.844\n",
      "Ep 1 (Step 007615): Train loss 1.629, Val loss 1.845\n",
      "Ep 1 (Step 007620): Train loss 1.783, Val loss 1.846\n",
      "Ep 1 (Step 007625): Train loss 1.745, Val loss 1.847\n",
      "Ep 1 (Step 007630): Train loss 2.011, Val loss 1.849\n",
      "Ep 1 (Step 007635): Train loss 1.792, Val loss 1.849\n",
      "Ep 1 (Step 007640): Train loss 1.595, Val loss 1.849\n",
      "Ep 1 (Step 007645): Train loss 1.340, Val loss 1.849\n",
      "Ep 1 (Step 007650): Train loss 1.548, Val loss 1.849\n",
      "Ep 1 (Step 007655): Train loss 1.534, Val loss 1.848\n",
      "Ep 1 (Step 007660): Train loss 1.537, Val loss 1.849\n",
      "Ep 1 (Step 007665): Train loss 1.692, Val loss 1.851\n",
      "Ep 1 (Step 007670): Train loss 1.765, Val loss 1.851\n",
      "Ep 1 (Step 007675): Train loss 1.729, Val loss 1.848\n",
      "Ep 1 (Step 007680): Train loss 1.744, Val loss 1.845\n",
      "Ep 1 (Step 007685): Train loss 1.548, Val loss 1.845\n",
      "Ep 1 (Step 007690): Train loss 1.970, Val loss 1.846\n",
      "Ep 1 (Step 007695): Train loss 1.572, Val loss 1.847\n",
      "Ep 1 (Step 007700): Train loss 1.579, Val loss 1.849\n",
      "Ep 1 (Step 007705): Train loss 1.837, Val loss 1.850\n",
      "Ep 1 (Step 007710): Train loss 1.772, Val loss 1.848\n",
      "Ep 1 (Step 007715): Train loss 1.778, Val loss 1.847\n",
      "Ep 1 (Step 007720): Train loss 1.901, Val loss 1.846\n",
      "Ep 1 (Step 007725): Train loss 2.023, Val loss 1.846\n",
      "Ep 1 (Step 007730): Train loss 1.616, Val loss 1.845\n",
      "Ep 1 (Step 007735): Train loss 1.820, Val loss 1.845\n",
      "Ep 1 (Step 007740): Train loss 1.809, Val loss 1.844\n",
      "Ep 1 (Step 007745): Train loss 1.512, Val loss 1.843\n",
      "Ep 1 (Step 007750): Train loss 1.719, Val loss 1.842\n",
      "Ep 1 (Step 007755): Train loss 1.779, Val loss 1.843\n",
      "Ep 1 (Step 007760): Train loss 1.638, Val loss 1.844\n",
      "Ep 1 (Step 007765): Train loss 1.648, Val loss 1.844\n",
      "Ep 1 (Step 007770): Train loss 1.970, Val loss 1.844\n",
      "Ep 1 (Step 007775): Train loss 1.770, Val loss 1.845\n",
      "Ep 1 (Step 007780): Train loss 1.840, Val loss 1.845\n",
      "Ep 1 (Step 007785): Train loss 1.667, Val loss 1.844\n",
      "Ep 1 (Step 007790): Train loss 1.735, Val loss 1.843\n",
      "Ep 1 (Step 007795): Train loss 2.000, Val loss 1.844\n",
      "Ep 1 (Step 007800): Train loss 1.543, Val loss 1.844\n",
      "Ep 1 (Step 007805): Train loss 1.951, Val loss 1.845\n",
      "Ep 1 (Step 007810): Train loss 1.467, Val loss 1.844\n",
      "Ep 1 (Step 007815): Train loss 2.123, Val loss 1.843\n",
      "Ep 1 (Step 007820): Train loss 1.741, Val loss 1.842\n",
      "Ep 1 (Step 007825): Train loss 1.773, Val loss 1.843\n",
      "Ep 1 (Step 007830): Train loss 1.572, Val loss 1.841\n",
      "Ep 1 (Step 007835): Train loss 1.638, Val loss 1.839\n",
      "Ep 1 (Step 007840): Train loss 1.847, Val loss 1.839\n",
      "Ep 1 (Step 007845): Train loss 2.124, Val loss 1.840\n",
      "Ep 1 (Step 007850): Train loss 1.679, Val loss 1.841\n",
      "Ep 1 (Step 007855): Train loss 1.705, Val loss 1.841\n",
      "Ep 1 (Step 007860): Train loss 1.689, Val loss 1.838\n",
      "Ep 1 (Step 007865): Train loss 1.847, Val loss 1.838\n",
      "Ep 1 (Step 007870): Train loss 1.572, Val loss 1.837\n",
      "Ep 1 (Step 007875): Train loss 1.921, Val loss 1.836\n",
      "Ep 1 (Step 007880): Train loss 1.545, Val loss 1.837\n",
      "Ep 1 (Step 007885): Train loss 1.649, Val loss 1.837\n",
      "Ep 1 (Step 007890): Train loss 1.698, Val loss 1.837\n",
      "Ep 1 (Step 007895): Train loss 1.655, Val loss 1.837\n",
      "Ep 1 (Step 007900): Train loss 1.210, Val loss 1.837\n",
      "Ep 1 (Step 007905): Train loss 1.907, Val loss 1.837\n",
      "Ep 1 (Step 007910): Train loss 1.928, Val loss 1.840\n",
      "Ep 1 (Step 007915): Train loss 1.847, Val loss 1.843\n",
      "Ep 1 (Step 007920): Train loss 1.640, Val loss 1.843\n",
      "Ep 1 (Step 007925): Train loss 1.861, Val loss 1.838\n",
      "Ep 1 (Step 007930): Train loss 1.757, Val loss 1.835\n",
      "Ep 1 (Step 007935): Train loss 1.764, Val loss 1.833\n",
      "Ep 1 (Step 007940): Train loss 1.673, Val loss 1.831\n",
      "Ep 1 (Step 007945): Train loss 1.681, Val loss 1.831\n",
      "Ep 1 (Step 007950): Train loss 1.800, Val loss 1.831\n",
      "Ep 1 (Step 007955): Train loss 1.999, Val loss 1.831\n",
      "Ep 1 (Step 007960): Train loss 1.465, Val loss 1.830\n",
      "Ep 1 (Step 007965): Train loss 1.659, Val loss 1.827\n",
      "Ep 1 (Step 007970): Train loss 1.686, Val loss 1.825\n",
      "Ep 1 (Step 007975): Train loss 1.696, Val loss 1.827\n",
      "Ep 1 (Step 007980): Train loss 1.700, Val loss 1.827\n",
      "Ep 1 (Step 007985): Train loss 1.783, Val loss 1.828\n",
      "Ep 1 (Step 007990): Train loss 1.515, Val loss 1.827\n",
      "Ep 1 (Step 007995): Train loss 1.901, Val loss 1.827\n",
      "Ep 1 (Step 008000): Train loss 1.730, Val loss 1.826\n",
      "Ep 1 (Step 008005): Train loss 1.756, Val loss 1.827\n",
      "Ep 1 (Step 008010): Train loss 1.806, Val loss 1.827\n",
      "Ep 1 (Step 008015): Train loss 1.614, Val loss 1.829\n",
      "Ep 1 (Step 008020): Train loss 1.517, Val loss 1.831\n",
      "Ep 1 (Step 008025): Train loss 1.796, Val loss 1.830\n",
      "Ep 1 (Step 008030): Train loss 1.918, Val loss 1.830\n",
      "Ep 1 (Step 008035): Train loss 1.613, Val loss 1.831\n",
      "Ep 1 (Step 008040): Train loss 1.581, Val loss 1.832\n",
      "Ep 1 (Step 008045): Train loss 1.707, Val loss 1.829\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00005\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      9\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 11\u001b[0m train_losses, val_losses, tokens_seen \u001b[38;5;241m=\u001b[39m train_model_simple(\n\u001b[1;32m     12\u001b[0m     model, train_loader, val_loader, optimizer, device,\n\u001b[1;32m     13\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs, eval_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, eval_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     14\u001b[0m     start_context\u001b[38;5;241m=\u001b[39mformat_input(val_data[\u001b[38;5;241m0\u001b[39m]), tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     18\u001b[0m execution_time_minutes \u001b[38;5;241m=\u001b[39m (end_time \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m\n",
      "File \u001b[0;32m~/INSTRUCTION/helper_function.py:300\u001b[0m, in \u001b[0;36mtrain_model_simple\u001b[0;34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    298\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set model to training mode\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m input_batch, target_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    301\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Reset loss gradients from previous batch iteration\u001b[39;00m\n\u001b[1;32m    302\u001b[0m         loss \u001b[38;5;241m=\u001b[39m calc_loss_batch(input_batch, target_batch, model, device)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM_task/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM_task/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM_task/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[0;32m~/INSTRUCTION/helper_function.py:551\u001b[0m, in \u001b[0;36mcustom_collate_fn\u001b[0;34m(batch, pad_token_id, ignore_index, allowed_max_length, device)\u001b[0m\n\u001b[1;32m    548\u001b[0m     inputs_lst\u001b[38;5;241m.\u001b[39mappend(inputs)\n\u001b[1;32m    549\u001b[0m     targets_lst\u001b[38;5;241m.\u001b[39mappend(targets)\n\u001b[0;32m--> 551\u001b[0m inputs_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(inputs_lst)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    552\u001b[0m targets_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(targets_lst)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs_tensor, targets_tensor\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb6ebd33-e911-4a8c-98bc-b1ca8f6148c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26991\n"
     ]
    }
   ],
   "source": [
    "total_training_steps = len(train_loader) * 1\n",
    "warmup_steps = int(0.1 * total_training_steps)  # 10% warmup\n",
    "print(warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29c7ceba-0b96-491f-a8d3-553ff25af092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Iter 000000): Train loss 2.744, Val loss 2.977\n",
      "Ep 1 (Iter 000005): Train loss 2.819, Val loss 2.862\n",
      "Ep 1 (Iter 000010): Train loss 2.514, Val loss 2.795\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 766.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00005\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      9\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 11\u001b[0m train_model(model, train_loader, val_loader, optimizer, device, n_epochs,\n\u001b[1;32m     12\u001b[0m                 \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m, start_context\u001b[38;5;241m=\u001b[39mformat_input(val_data[\u001b[38;5;241m0\u001b[39m]), warmup_steps\u001b[38;5;241m=\u001b[39mwarmup_steps,\n\u001b[1;32m     13\u001b[0m                 initial_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-6\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-7\u001b[39m)\n\u001b[1;32m     15\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     16\u001b[0m execution_time_minutes \u001b[38;5;241m=\u001b[39m (end_time \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m\n",
      "File \u001b[0;32m~/INSTRUCTION/train.py:58\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, device, n_epochs, eval_freq, eval_iter, start_context, warmup_steps, initial_lr, min_lr)\u001b[0m\n\u001b[1;32m     56\u001b[0m track_lrs\u001b[38;5;241m.\u001b[39mappend(lr)\n\u001b[1;32m     57\u001b[0m loss \u001b[38;5;241m=\u001b[39m calc_loss_batch(input_batch, target_batch, model, device)\n\u001b[0;32m---> 58\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_step \u001b[38;5;241m>\u001b[39m warmup_steps:\n\u001b[1;32m     61\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM_task/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    527\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM_task/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[1;32m    268\u001b[0m     tensors,\n\u001b[1;32m    269\u001b[0m     grad_tensors_,\n\u001b[1;32m    270\u001b[0m     retain_graph,\n\u001b[1;32m    271\u001b[0m     create_graph,\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/LLM_task/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 766.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "\n",
    "n_epochs = 1\n",
    "\n",
    "train_model(model, train_loader, val_loader, optimizer, device, n_epochs,\n",
    "                5, 5, start_context=format_input(val_data[0]), warmup_steps=warmup_steps,\n",
    "                initial_lr=5e-6, min_lr=5e-7)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4acd368b-1403-4807-a218-9102e35bfdbb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "4acd368b-1403-4807-a218-9102e35bfdbb",
    "outputId": "026a0b78-fd64-45b5-aafe-09309f7e180c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from previous_chapters import plot_losses\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b77ab2e3-ede4-4836-a6e0-a7ccd21cbd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['instruction', 'input'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry = test_data[0]\n",
    "answer = entry.pop('output')\n",
    "entry.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "89cab635-1d2c-4509-be47-1161c6ccfdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this task, you are given commands (in terms of logical operations) and natural interpretation of the given command to select relevant rows from the given table. Your job is to generate a label \"yes\" if the interpretation is appropriate for the command, otherwise generate label \"no\". \n",
      " Here are the definitions of logical operators: \n",
      " 1. count: returns the number of rows in the view. \n",
      " 2. only: returns whether there is exactly one row in the view. \n",
      " 3. hop: returns the value under the header column of the row. \n",
      " 4. and: returns the boolean operation result of two arguments. \n",
      " 5. max/min/avg/sum: returns the max/min/average/sum of the values under the header column. \n",
      " 6. nth_max/nth_min: returns the n-th max/n-th min of the values under the header column. \n",
      " 7. argmax/argmin: returns the row with the max/min value in header column. \n",
      " 8. nth_argmax/nth_argmin: returns the row with the n-th max/min value in header column. \n",
      " 9. eq/not_eq: returns if the two arguments are equal. \n",
      " 10. round_eq: returns if the two arguments are roughly equal under certain tolerance. \n",
      " 11. greater/less: returns if the first argument is greater/less than the second argument. \n",
      " 12. diff: returns the difference between two arguments. \n",
      " 13. filter_eq/ filter_not_eq: returns the subview whose values under the header column is equal/not equal to the third argument. \n",
      " 14. filter_greater/filter_less: returns the subview whose values under the header column is greater/less than the third argument. \n",
      " 15. filter_greater_eq /filter_less_eq: returns the subview whose values under the header column is greater/less or equal than the third argument. \n",
      " 16. filter_all: returns the view itself for the case of describing the whole table \n",
      " 17. all_eq/not_eq: returns whether all the values under the header column are equal/not equal to the third argument. \n",
      " 18. all_greater/less: returns whether all the values under the header column are greater/less than the third argument. \n",
      " 19. all_greater_eq/less_eq: returns whether all the values under the header column are greater/less or equal to the third argument. \n",
      " 20. most_eq/not_eq: returns whether most of the values under the header column are equal/not equal to the third argument. \n",
      " 21. most_greater/less: returns whether most of the values under the header column are greater/less than the third argument. \n",
      " 22. most_greater_eq/less_eq: returns whether most of the values under the header column are greater/less or equal to the third argument.\n",
      "\n",
      "Command: eq { hop { argmax { all_rows ; location attendance } ; date } ; april 2 }, interpretation: select the rows whose venue record is arbitrary . the number of such rows is 6 .\n"
     ]
    }
   ],
   "source": [
    "print(entry['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "24cb6bf0-0048-44f3-8c15-03e9fcf71635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "since ' april 2 ' is such a small value and you only need 6 rows since the interpret clause tells you the context for the view in this case - every view includes at least one photograph or video in its layout . you do not have to add ' april 2 ' to every view in order to select from nearby views if the command specifies these views only once .\n",
      "Where is the command written in the given input? \n",
      "\n",
      "\n",
      "Let's explain the question step by step:\n",
      "\n",
      "1. Some pieces of context are important in understanding the command:\n",
      "- the view is described as \"approximates, with the third argument\", where \"approximate\" talks about the closest view (an estimate) taking the same position or position of the individual portraits.\n",
      "- location refers to the concept of how the given location can be sized: [attendance, location].\n",
      "\n",
      "2. The view contains parameters:\n",
      "- hops is the value of the view for which the proper command should return a \"hop\", specify the value as the hop variable in the view and the view will then be designated the \"approximate\" view.\n",
      "- location can be the location where the response to this command would take one or two data points, like obey, attend, visit, quit, no etc\n",
      "\n",
      "3. The view contains the interpretation: this command tells us to select the rows whose attendance record is arbitrary and specify the exact position of the individual portraits.\n",
      "\n",
      "4. The view contains a task:\n",
      "1. You must handle the request to select all rows that have attendance record of at least six people each in order to control the number of rows that are selected by adding 6 to the attendance record of at least six people:\n",
      "\n",
      "The field contains exactly 6 rows,... which matches the interpretation: \"attends\", \"approximates\", \"considering\", \"cont, not\", \"conditions\", \"dates\", \"terms\", \"boo, more\"\n",
      "\n",
      "5. The process is repeated multiple times:\n",
      "   a. that the order of the queries matches the interpretation:\n",
      "   b. If there is an alternative, you should specify it first. \n",
      "\n",
      "6. However, none of the the rows in the view are specified in the user's query. If the user wants to specify either hops or location, it would also need to use \"approximates\".\n"
     ]
    }
   ],
   "source": [
    "# larger dataset\n",
    "response = chat(model, tokenizer, device, entry, max_new_tokens=1000)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "49365980-d512-4aaf-8002-ef7653078e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'val_1dot829.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "466381db-427e-4cdc-8426-e81c8cdb4784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the madam cooks the various small meals and the waiter cooks the additional food. when they are together, the chains move guards to prevent mischief. is there any point within the law when individual who hash out frozen meals with the police might become pariah?\n",
      "\n",
      "\n",
      "\n",
      "Some points within the law you might need to consider when considering whether individual who hash out frozen meals with the police might become pariah. Here's the direct answer to this question:\n",
      "\n",
      "1. According to the\n"
     ]
    }
   ],
   "source": [
    "entry = {\n",
    "    \"instruction\": \"Convert the following active sentence to passive.\",\n",
    "    \"input\": 'The chef cooks the meal every day.'\n",
    "}\n",
    "response = chat(model, tokenizer, device, entry)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac02ab4-fe6c-420a-834c-5a169c9c941c",
   "metadata": {},
   "source": [
    "### test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec26e4a6-361f-45db-9021-77669dca2ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 110/110 [00:52<00:00,  2.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    " \n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "    input_text = format_input(entry)\n",
    " \n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    " \n",
    "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
    "    json.dump(test_data, file, indent=4)  # \"indent\" for pretty-printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef30871e-2b66-4fa8-8eb4-22b8a25d0cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
