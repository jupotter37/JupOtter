{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda device\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch \n",
    "import clip\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device,\"device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_bert_path = \"data/Summaries/Summary_Bert_77.csv\"\n",
    "summary_bert = pd.read_csv(summary_bert_path)\n",
    "\n",
    "\n",
    "summary_bert_ingredients_path=\"data/Summaries/export_summary_bert_with_ingredients.csv\"\n",
    "summary_bert_ingredients=pd.read_csv(summary_bert_ingredients_path)\n",
    "\n",
    "data_dir = \"data\"\n",
    "images_dir = \"Food Images/Food Images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sumary recipe  12710\n",
      "we only take 200\n",
      "torch.Size([200, 77])\n",
      "number of images in total  13582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1081346/2356681205.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_bert = torch.tensor(recipe_tensor).to(device)\n"
     ]
    }
   ],
   "source": [
    "recipe = summary_bert[\"Summary_Bert_77\"]\n",
    "print(\"number of sumary recipe \",len(recipe)) #2067\n",
    "recipe = recipe[:200]\n",
    "print(\"we only take\",len(recipe))\n",
    "\n",
    "recipe_tensor = clip.tokenize(recipe, truncate=True).to(device)\n",
    "print(recipe_tensor.shape)\n",
    "\n",
    "tensor_bert = torch.tensor(recipe_tensor).to(device)\n",
    "torch.save(tensor_bert, \"data/tensors/bert_tensor.pt\")\n",
    "\n",
    "\n",
    "liste_images = summary_bert[\"Image_Name\"].tolist()\n",
    "liste_images=liste_images[:200]\n",
    "liste_images = [image + \".jpg\" for image in liste_images]\n",
    "\n",
    "\n",
    "images = os.listdir(os.path.join(data_dir, images_dir)) #13582 images in total \n",
    "print(\"number of images in total \",len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n"
     ]
    }
   ],
   "source": [
    "models = clip.available_models()\n",
    "print(models)\n",
    "model, preprocess = clip.load('RN50', device,jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.str_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;66;03m#images_inputs.append(preprocess(Image.open(im_path)))\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         images_inputs\u001b[38;5;241m.\u001b[39mappend(im_path)\n\u001b[0;32m----> 7\u001b[0m images_inputs_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(images_inputs_tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/Tensors_data/images_inputs_tensor.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(images_inputs))\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.str_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "images_inputs=[]\n",
    "\n",
    "for im in liste_images:\n",
    "        im_path = os.path.join(data_dir, images_dir, im)\n",
    "        #images_inputs.append(preprocess(Image.open(im_path)))\n",
    "        images_inputs.append(im_path)\n",
    "images_inputs_tensor = torch.tensor((images_inputs)).to(device)\n",
    "torch.save(images_inputs_tensor, \"data/Tensors_data/images_inputs_tensor.pt\")\n",
    "print(len(images_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With bert Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text array shape (200,)\n",
      "text_tensor shape torch.Size([200, 77])\n",
      "images array shape 200\n",
      "0    Roast chicken in a large castiron skillet and ...\n",
      "1    Roast the potatoes for 15 to 20 minutes depend...\n",
      "Name: Summary_Bert_77, dtype: object\n",
      "['data/Food Images/Food Images/miso-butter-roast-chicken-acorn-squash-panzanella.jpg', 'data/Food Images/Food Images/crispy-salt-and-pepper-potatoes-dan-kluger.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(\"text array shape\",recipe.shape)\n",
    "print(\"text_tensor shape\", recipe_tensor.shape)\n",
    "#print(\"images_inputs_tensor shape \", images_inputs_tensor.shape)\n",
    "\n",
    "\n",
    "print(\"images array shape\",len(images_inputs))\n",
    "\n",
    "#we ensure that we have the same order for the images and the text\n",
    "print(recipe[:2])\n",
    "print(images_inputs[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: 0.4458 for index : 126\n",
      "Value: 0.1257 for index : 189\n",
      "Value: 0.0979 for index : 78\n",
      "Value: 0.0751 for index : 91\n",
      "Value: 0.04346 for index : 96\n",
      "126\n",
      "Toast pistachios in a dry small skillet over mediumlow heat tossing occasionally until\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "\n",
    "image = preprocess(Image.open(\"data/Food Images/Food Images/-bloody-mary-tomato-toast-with-celery-and-horseradish-56389813.jpg\")).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "#print(\"image\",image.shape) #torch.Size([1, 3, 224, 224])\n",
    "#print(\"text\",recipe_tensor.shape) #torch.Size([200, 77])\n",
    "\n",
    "with torch.no_grad():\n",
    "   \n",
    "    image_features = model.encode_image(image)\n",
    "    #print(\"type image_features\",type(image_features.shape)) #<class 'torch.Size'>\n",
    "    #print(\"shape image_features\",image_features.shape) #torch.Size([1, 1024]\n",
    "    \n",
    "    text_features = model.encode_text(recipe_tensor)\n",
    "    #print(\"type text_features\",type(text_features.shape)) #<class 'torch.Size'>\n",
    "    #print(\"shape text_features\",text_features.shape) #torch.Size([200, 1024]\n",
    "\n",
    "    \n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "#print(similarity[0])\n",
    "\n",
    "values, indices = torch.topk(similarity[0], k=5)\n",
    "\n",
    "# Convert the tensors to numpy arrays\n",
    "values = values.cpu().numpy()\n",
    "indices = indices.cpu().numpy()\n",
    "\n",
    "# Create a list of tuples containing the values and indices\n",
    "values_indices = list(zip(values, indices))\n",
    "\n",
    "# Sort the list in descending order based on the values\n",
    "values_indices.sort(reverse=True)\n",
    "\n",
    "# Extract the sorted values and indices\n",
    "sorted_values, sorted_indices = zip(*values_indices)\n",
    "\n",
    "max_index=sorted_indices[0]\n",
    "\n",
    "\n",
    "# Print the sorted values and indices\n",
    "for value, index in zip(sorted_values, sorted_indices):\n",
    "    print(\"Value:\", value , \"for index :\" ,index)\n",
    "\n",
    "\n",
    "\n",
    "print(max_index)\n",
    "print(recipe[max_index])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Value: 0.3792 for index : 0\n",
      "Value: 0.2229 for index : 18\n",
      "Value: 0.07587 for index : 13\n",
      "Value: 0.03017 for index : 149\n",
      "Value: 0.01888 for index : 82\n",
      "label is Roast chicken in a large castiron skillet and roast on middle rack until an instant\n",
      "model answer Roast chicken in a large castiron skillet and roast on middle rack until an instant\n",
      "Value: 0.4275 for index : 101\n",
      "Value: 0.2324 for index : 1\n",
      "Value: 0.05106 for index : 106\n",
      "Value: 0.04794 for index : 65\n",
      "Value: 0.04233 for index : 129\n",
      "label is To make the meatballs place the breadcrumbs in a small bowl and cover with\n",
      "model answer To make the meatballs place the breadcrumbs in a small bowl and cover with\n",
      "Value: 0.148 for index : 22\n",
      "Value: 0.09705 for index : 116\n",
      "Value: 0.0721 for index : 170\n",
      "Value: 0.0721 for index : 167\n",
      "Value: 0.0721 for index : 117\n",
      "label is Preheat the oven to 350°F Butter a 9inch nonstick metal cake pan\n",
      "model answer Preheat the oven to 350°F Butter a 9inch nonstick metal cake pan\n",
      "Value: 0.4045 for index : 170\n",
      "Value: 0.1536 for index : 167\n",
      "Value: 0.0433 for index : 116\n",
      "Value: 0.04263 for index : 22\n",
      "Value: 0.03268 for index : 117\n",
      "label is Bake for 40 to 45 minutes until the fruit is bubbling around the edges and\n",
      "model answer Bake for 40 to 45 minutes until the fruit is bubbling around the edges and\n",
      "Value: 0.287 for index : 84\n",
      "Value: 0.2167 for index : 147\n",
      "Value: 0.1688 for index : 174\n",
      "Value: 0.0773 for index : 35\n",
      "Value: 0.02713 for index : 168\n",
      "label is This rejuvenating drink combines the mild heat of young ginger and the sourness of tam\n",
      "model answer This rejuvenating drink combines the mild heat of young ginger and the sourness of tam\n",
      "Value: 0.3896 for index : 145\n",
      "Value: 0.1783 for index : 7\n",
      "Value: 0.1755 for index : 5\n",
      "Value: 0.08295 for index : 84\n",
      "Value: 0.02065 for index : 41\n",
      "label is Honey syrup can be made in a small bowl or jar. Mezcal lemon\n",
      "model answer Honey syrup can be made in a small bowl or jar. Mezcal lemon\n",
      "Value: 0.4128 for index : 168\n",
      "Value: 0.07635 for index : 24\n",
      "Value: 0.074 for index : 7\n",
      "Value: 0.0463 for index : 41\n",
      "Value: 0.0442 for index : 145\n",
      "label is Hibiscus ginger cinnamon cloves and orange and lemon zests are the main ingredients\n",
      "model answer Hibiscus ginger cinnamon cloves and orange and lemon zests are the main ingredients\n",
      "Value: 0.6616 for index : 7\n",
      "Value: 0.1572 for index : 145\n",
      "Value: 0.04297 for index : 5\n",
      "Value: 0.00873 for index : 199\n",
      "Value: 0.00833 for index : 110\n",
      "label is For the turmeric syrup combinecup hot water and  cup sugar in a liquid measuring\n",
      "model answer For the turmeric syrup combinecup hot water and  cup sugar in a liquid measuring\n",
      "Value: 0.1885 for index : 123\n",
      "Value: 0.09937 for index : 53\n",
      "Value: 0.0933 for index : 139\n",
      "Value: 0.0786 for index : 27\n",
      "Value: 0.06415 for index : 119\n",
      "label is Bring a medium pot of salted water to a boil Add lentils and cook until\n",
      "model answer Bring a medium pot of salted water to a boil Add lentils and cook until\n",
      "Value: 0.1968 for index : 8\n",
      "Value: 0.1332 for index : 92\n",
      "Value: 0.068 for index : 74\n",
      "Value: 0.04672 for index : 136\n",
      "Value: 0.0453 for index : 123\n",
      "label is Bhuna is the process of repeated sticking and scraping is a Southeast Asian cooking technique\n",
      "model answer Bhuna is the process of repeated sticking and scraping is a Southeast Asian cooking technique\n",
      "Value: 0.2389 for index : 17\n",
      "Value: 0.0935 for index : 68\n",
      "Value: 0.09064 for index : 20\n",
      "Value: 0.0633 for index : 156\n",
      "Value: 0.0396 for index : 40\n",
      "label is Divide yogurt among bowls top with plantains and mangoes then nuts seeds andor\n",
      "model answer Divide yogurt among bowls top with plantains and mangoes then nuts seeds andor\n",
      "good_answer 0\n",
      "bad_answer 11\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "good_answer=0\n",
    "bad_answer=0\n",
    "\n",
    "print(type(images_inputs))\n",
    "\n",
    "for i,img in enumerate(images_inputs):\n",
    "    if i>10:\n",
    "        break\n",
    "\n",
    "    image = preprocess(Image.open(img)).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(recipe_tensor)\n",
    "    \n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "\n",
    "    values, indices = torch.topk(similarity[0], k=5)\n",
    "\n",
    "    # Convert the tensors to numpy arrays\n",
    "    values = values.cpu().numpy()\n",
    "    indices = indices.cpu().numpy()\n",
    "\n",
    "    # Create a list of tuples containing the values and indices\n",
    "    values_indices = list(zip(values, indices))\n",
    "\n",
    "    # Sort the list in descending order based on the values\n",
    "    values_indices.sort(reverse=True)\n",
    "\n",
    "    # Extract the sorted values and indices\n",
    "    sorted_values, sorted_indices = zip(*values_indices)\n",
    "\n",
    "    max_index=sorted_indices[0] #text qui correspond à l'image selon le modèle \n",
    "    \n",
    "    label=recipe[max_index] #rectte\n",
    "\n",
    "\"\"\"\n",
    "    for value, index in zip(sorted_values, sorted_indices):\n",
    "        print(\"Value:\", value , \"for index :\" ,index)\n",
    "\"\"\"\n",
    "\n",
    "    print(\"LABEL is : \",label)\n",
    "    print(\"Model Answer\",recipe[max_index])\n",
    "\n",
    "    if label==sorted_values[0]: \n",
    "        good_answer+=1\n",
    "    else: \n",
    "        bad_answer+=1\n",
    "\n",
    "\n",
    "print(\"good_answer\",good_answer)\n",
    "print(\"bad_answer\",bad_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipe To make the meatballs place the breadcrumbs in a small bowl and cover with\n",
      "image ComputerVision_Data/Food Images/Food Images/chicken-meatballs-with-molokhieh-falastin.jpg\n"
     ]
    }
   ],
   "source": [
    "print(\"recipe\", recipe[101]) \n",
    "print(\"image\",images_inputs[101])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With ONLY the main ingredients (GPT3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With the first five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['minutesWhole chicken', 'whites potatoes', 'cream cheese', 'turkeyBread cubes', 'sugar bourbon']\n",
      "['miso-butter-roast-chicken-acorn-squash-panzanella', 'crispy-salt-and-pepper-potatoes-dan-kluger', 'thanksgiving-mac-and-cheese-erick-williams', 'italian-sausage-and-bread-stuffing-240559', 'newtons-law-apple-bourbon-cocktail']\n"
     ]
    }
   ],
   "source": [
    "# Creation of a tensor for the text \n",
    "\n",
    "liste_ingredients=[]\n",
    "\n",
    "context_length=77\n",
    "#print(device,\"device\")\n",
    "liste_ingredients_tensor = torch.zeros(len(summary_bert_ingredients), context_length, dtype=torch.long)\n",
    "labels = []\n",
    "\n",
    "torch.save(liste_ingredients_tensor, \"data/tensors/ingredients_tensor.pt\")\n",
    "\n",
    "for i, row in enumerate(summary_bert_ingredients.iterrows()):\n",
    "    \n",
    "    summary = row[1][\"summary_with_ingredients\"]\n",
    "    ingredients = summary.split()[-2:]\n",
    "    \n",
    "    ingredients=str(ingredients[0])+ \" \" +str(ingredients[1])\n",
    "\n",
    "    to_supress = string.punctuation + \"—\" + \"–\" + \"()\"\n",
    "    ingredients = ingredients.translate(str.maketrans(\"\", \"\",to_supress)) # Remove punctuation from summary\n",
    "\n",
    "    liste_ingredients.append(ingredients)  # Access row data using integer indices\n",
    "    labels.append(row[1][\"Image_Name\"])  # Access row data using integer indices\n",
    "\n",
    "   \n",
    "print(liste_ingredients[:5])\n",
    "print(labels[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST with the 5 first line\n",
    "\n",
    "main_ingr= ['minutesWhole chicken', 'whites potatoes', 'cream cheese', 'turkeyBread cubes', 'sugar bourbon']\n",
    "label = ['miso-butter-roast-chicken-acorn-squash-panzanella', 'crispy-salt-and-pepper-potatoes-dan-kluger', 'thanksgiving-mac-and-cheese-erick-williams', 'italian-sausage-and-bread-stuffing-240559', 'newtons-law-apple-bourbon-cocktail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([0.1512, 0.2739, 0.0020, 0.5620, 0.0108], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "Value: 0.562 for index : 3\n",
      "Value: 0.274 for index : 1\n",
      "Value: 0.1512 for index : 0\n",
      "Value: 0.01079 for index : 4\n",
      "Value: 0.001995 for index : 2\n",
      "3\n",
      "turkeyBread cubes\n",
      "Bread can be toasted 3 days ahead and kept once cool in a sealed bag at room \n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "image = preprocess(Image.open(\"/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Food Images/Food Images/italian-sausage-and-bread-stuffing-240559.jpg\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize(main_ingr).to(device)\n",
    "print(type(text))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "   \n",
    "    image_features = model.encode_image(image)\n",
    "\n",
    "    \n",
    "    text_features = model.encode_text(text)\n",
    "\n",
    "\n",
    "# Pick the top 5 most similar labels for the image\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "print(similarity[0])\n",
    "\n",
    "values, indices = torch.topk(similarity[0], k=5)\n",
    "\n",
    "# Convert the tensors to numpy arrays\n",
    "values = values.cpu().numpy()\n",
    "indices = indices.cpu().numpy()\n",
    "\n",
    "# Create a list of tuples containing the values and indices\n",
    "values_indices = list(zip(values, indices))\n",
    "\n",
    "# Sort the list in descending order based on the values\n",
    "values_indices.sort(reverse=True)\n",
    "\n",
    "# Extract the sorted values and indices\n",
    "sorted_values, sorted_indices = zip(*values_indices)\n",
    "\n",
    "max_index=sorted_indices[0]\n",
    "\n",
    "\n",
    "# Print the sorted values and indices\n",
    "for value, index in zip(sorted_values, sorted_indices):\n",
    "    print(\"Value:\", value , \"for index :\" ,index)\n",
    "\n",
    "\n",
    "print(max_index)\n",
    "print(liste_ingredients[max_index])\n",
    "print(array_text[max_index])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With all the rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([1.4353e-04, 2.4414e-04, 3.1531e-05,  ..., 6.0737e-05, 2.0325e-05,\n",
      "        1.0118e-03], device='cuda:0', dtype=torch.float16)\n",
      "Value: 0.10645 for index : 58\n",
      "Value: 0.0755 for index : 919\n",
      "Value: 0.03403 for index : 1004\n",
      "Value: 0.03099 for index : 906\n",
      "Value: 0.0305 for index : 116\n",
      "58\n",
      "yolks kefir\n",
      "Melt butter over medium heat in a large skillet Add pine nuts and cook until \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nTo verify the image \\n\\nfor i in range(len(labels)): \\n    if i==index: \\n        image_to_print.append(labels[i])\\n\\n\\n# Display the images using matplotlib\\nfig, axes = plt.subplots(1, len(image_to_print), figsize=(15, 3), squeeze=False)\\n\\nfor i, image_name in enumerate(image_to_print):\\n    image_path = os.path.join(images_dir, image_name)\\n    image = Image.open(image_path)\\n    axes[0, i].imshow(image)\\n    axes[0, i].set_title(image_name)\\n    axes[0, i].axis('off')\\n\\nplt.show()\\n\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "image_to_print=[]\n",
    "\n",
    "image = preprocess(Image.open(\"/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Food Images/Food Images/caramelized-plantain-parfait.jpg\")).unsqueeze(0).to(device)\n",
    "\n",
    "text = clip.tokenize(liste_ingredients).to(device)\n",
    "print(type(text))\n",
    "\n",
    "with torch.no_grad():\n",
    "   \n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "\n",
    "\n",
    "\n",
    "# Pick the top 5 most similar labels for the image\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "print(similarity[0])\n",
    "\n",
    "values, indices = torch.topk(similarity[0], k=5)\n",
    "\n",
    "# Convert the tensors to numpy arrays\n",
    "values = values.cpu().numpy()\n",
    "indices = indices.cpu().numpy()\n",
    "\n",
    "# Create a list of tuples containing the values and indices\n",
    "values_indices = list(zip(values, indices))\n",
    "\n",
    "# Sort the list in descending order based on the values\n",
    "values_indices.sort(reverse=True)\n",
    "\n",
    "# Extract the sorted values and indices\n",
    "sorted_values, sorted_indices = zip(*values_indices)\n",
    "\n",
    "max_index=sorted_indices[0]\n",
    "\n",
    "\n",
    "# Print the sorted values and indices\n",
    "for value, index in zip(sorted_values, sorted_indices):\n",
    "    print(\"Value:\", value , \"for index :\" ,index)\n",
    "\n",
    "\n",
    "print(max_index)\n",
    "print(liste_ingredients[max_index])\n",
    "print(array_text[max_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With all ingredients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['whole chicken kosher salt divided plus moresmall acorn squash about total fin', ' large egg whitespound new potatoes aboutinch in diameterteaspoons kosher sal', ' cup evaporated milkcup whole milk garlic powder onion powder smoked paprika ', 'to pound round Italian loaf cut into inch cubescupstablespoons olive oil divi', ' teaspoon dark brown sugarteaspoon hot water oz bourbonoz fresh lemon juicete']\n",
      "['miso-butter-roast-chicken-acorn-squash-panzanella', 'crispy-salt-and-pepper-potatoes-dan-kluger', 'thanksgiving-mac-and-cheese-erick-williams', 'italian-sausage-and-bread-stuffing-240559', 'newtons-law-apple-bourbon-cocktail']\n"
     ]
    }
   ],
   "source": [
    "ingr_text=[]\n",
    "labels = []\n",
    "context_length=77\n",
    "\n",
    "liste_ingredients_tensor = torch.zeros(len(summary_bert), context_length, dtype=torch.long)\n",
    "\n",
    "\n",
    "for i, row in enumerate(summary_bert.iterrows()):\n",
    "    ingredients = row[1][\"Cleaned_Ingredients\"]\n",
    "\n",
    "    to_supress = string.punctuation + \"—\" + \"–\" + \"()\"\n",
    "    ingredients = ingredients.translate(str.maketrans(\"\", \"\",to_supress)) # Remove punctuation from summary\n",
    "    ingredients = ''.join([char for char in ingredients if not char.isdigit()])\n",
    "    ingredients = ingredients.replace(\"Tbsp\", \"\").replace(\"½\", \"\").replace(\"¾\", \"\").replace(\"lb\", \"\").replace(\"tsp\", \"\").replace(\"⅓\", \"\").replace(\"  \", \"\").replace(\"¼\", \"\")\n",
    "\n",
    "    #bug with ingredients \n",
    "    if len(ingredients)>context_length:\n",
    "        ingredients = ingredients[:context_length]\n",
    "\n",
    "    ingr_text.append(ingredients)  # Access row data using integer indices\n",
    "    labels.append(row[1][\"Image_Name\"])  # Access row data using integer indices\n",
    "\n",
    "    liste_ingredients_tensor[i] = clip.tokenize([ingredients]).to(device)\n",
    "    \n",
    "\n",
    "torch.save(liste_ingredients_tensor, \"data/tensors/All_ingredients_tensor.pt\")\n",
    " \n",
    "\n",
    "ingr=ingr_text[:5]\n",
    "label=labels[:5]\n",
    "\n",
    "print(ingr)\n",
    "print(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([0.0359, 0.0140, 0.0468, 0.8823, 0.0208], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "Value: 0.8823 for index : 3\n",
      "Value: 0.04678 for index : 2\n",
      "Value: 0.03586 for index : 0\n",
      "Value: 0.02075 for index : 4\n",
      "Value: 0.014046 for index : 1\n",
      "3\n",
      "to pound round Italian loaf cut into inch cubescupstablespoons olive oil divi\n",
      "Bread can be toasted 3 days ahead and kept once cool in a sealed bag at room \n"
     ]
    }
   ],
   "source": [
    "image_to_print=[]\n",
    "\n",
    "image = preprocess(Image.open(\"/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Food Images/Food Images/italian-sausage-and-bread-stuffing-240559.jpg\")).unsqueeze(0).to(device)\n",
    "\n",
    "text = clip.tokenize(ingr).to(device)\n",
    "print(type(text))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "   \n",
    "    image_features = model.encode_image(image)\n",
    "   \n",
    "    text_features = model.encode_text(text)\n",
    "\n",
    "# Pick the top 5 most similar labels for the image\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "print(similarity[0])\n",
    "\n",
    "values, indices = torch.topk(similarity[0], k=5)\n",
    "\n",
    "# Convert the tensors to numpy arrays\n",
    "values = values.cpu().numpy()\n",
    "indices = indices.cpu().numpy()\n",
    "\n",
    "# Create a list of tuples containing the values and indices\n",
    "values_indices = list(zip(values, indices))\n",
    "\n",
    "# Sort the list in descending order based on the values\n",
    "values_indices.sort(reverse=True)\n",
    "\n",
    "# Extract the sorted values and indices\n",
    "sorted_values, sorted_indices = zip(*values_indices)\n",
    "\n",
    "max_index=sorted_indices[0]\n",
    "\n",
    "\n",
    "# Print the sorted values and indices\n",
    "for value, index in zip(sorted_values, sorted_indices):\n",
    "    print(\"Value:\", value , \"for index :\" ,index)\n",
    "\n",
    "\n",
    "print(max_index)\n",
    "print(ingr_text[max_index])\n",
    "print(array_text[max_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with all the rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13463, 77])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 152.00 MiB. GPU 0 has a total capacity of 11.74 GiB of which 117.94 MiB is free. Including non-PyTorch memory, this process has 11.51 GiB memory in use. Of the allocated memory 11.17 GiB is allocated by PyTorch, and 214.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     13\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode_image(image)\n\u001b[0;32m---> 15\u001b[0m     text_features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mliste_ingredients_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Pick the top 5 most similar labels for the image\u001b[39;00m\n\u001b[1;32m     18\u001b[0m image_features \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m image_features\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib/python3.9/site-packages/clip/model.py:312\u001b[0m, in \u001b[0;36mCLIP.encode_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    310\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    311\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# NLD -> LND\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# LND -> NLD\u001b[39;00m\n\u001b[1;32m    314\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_final(x)\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib/python3.9/site-packages/clip/model.py:198\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib64/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib/python3.9/site-packages/clip/model.py:185\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 185\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    186\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(x))\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib/python3.9/site-packages/clip/model.py:157\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    156\u001b[0m     orig_type \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m--> 157\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtype(orig_type)\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib64/python3.9/site-packages/torch/nn/modules/normalization.py:201\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Computer-Vision/.venv/lib64/python3.9/site-packages/torch/nn/functional.py:2546\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2544\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2545\u001b[0m     )\n\u001b[0;32m-> 2546\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 152.00 MiB. GPU 0 has a total capacity of 11.74 GiB of which 117.94 MiB is free. Including non-PyTorch memory, this process has 11.51 GiB memory in use. Of the allocated memory 11.17 GiB is allocated by PyTorch, and 214.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "image_to_print=[]\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "image = preprocess(Image.open(\"/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Food Images/Food Images/italian-sausage-and-bread-stuffing-240559.jpg\")).unsqueeze(0).to(device)\n",
    "liste_ingredients_tensor = torch.load(\"/users/eleves-b/2022/hanna.mergui/Computer-Vision/ComputerVision_Data/Tensors_data/All_ingredients_tensor.pt\").to(device)\n",
    "print(liste_ingredients_tensor.shape) #Huge tensor \n",
    "\n",
    "liste_ingredients_tensor=liste_ingredients_tensor[:1000]\n",
    "\n",
    "with torch.no_grad():\n",
    "   \n",
    "    image_features = model.encode_image(image)\n",
    "   \n",
    "    text_features = model.encode_text(liste_ingredients_tensor)\n",
    "\n",
    "# Pick the top 5 most similar labels for the image\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "\n",
    "values, indices = torch.topk(similarity[0], k=5)\n",
    "\n",
    "# Convert the tensors to numpy arrays\n",
    "values = values.cpu().numpy()\n",
    "indices = indices.cpu().numpy()\n",
    "\n",
    "# Create a list of tuples containing the values and indices\n",
    "values_indices = list(zip(values, indices))\n",
    "\n",
    "# Sort the list in descending order based on the values\n",
    "values_indices.sort(reverse=True)\n",
    "\n",
    "# Extract the sorted values and indices\n",
    "sorted_values, sorted_indices = zip(*values_indices)\n",
    "\n",
    "max_index=sorted_indices[0]\n",
    "\n",
    "\n",
    "# Print the sorted values and indices\n",
    "for value, index in zip(sorted_values, sorted_indices):\n",
    "    print(\"Value:\", value , \"for index :\" ,index, \"for recipe : \",labels[index])\n",
    "\n",
    "\n",
    "print(max_index)\n",
    "print(ingr_text[max_index])\n",
    "print(array_text[max_index])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_recipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
