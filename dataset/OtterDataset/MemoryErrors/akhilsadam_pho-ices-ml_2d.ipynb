{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "e3e15b42-766b-4240-8049-a4c01d20e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../core')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "cae80a7b-7a78-48f4-884e-19092c234772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "f36d412b-85bc-45cc-8dfc-0a974366aafc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.34830205e-02,  -1.46624371e-02,  -1.58418529e-02,\n",
       "         -1.70212686e-02,  -1.82006862e-02,  -1.93801019e-02,\n",
       "         -2.05595195e-02,  -2.17389353e-02,  -2.29183510e-02,\n",
       "         -2.40977686e-02,  -2.52771843e-02,  -2.64565982e-02,\n",
       "         -2.76360158e-02,  -2.88154315e-02,  -2.99948491e-02,\n",
       "         -3.11742648e-02,  -3.23536843e-02,  -3.35330963e-02,\n",
       "         -3.47125120e-02,  -3.58919315e-02,  -3.70713472e-02,\n",
       "         -3.82507630e-02,  -3.94301787e-02,  -4.06095982e-02,\n",
       "         -4.17890139e-02,  -4.29684296e-02,  -4.41478454e-02,\n",
       "         -4.53272648e-02],\n",
       "       [ -1.13048600e-02,  -1.24842767e-02,  -1.36636933e-02,\n",
       "         -1.48431081e-02,  -1.60225257e-02,  -1.72019415e-02,\n",
       "         -1.83813572e-02,  -1.95607748e-02,  -2.07401905e-02,\n",
       "         -2.19196081e-02,  -2.30990238e-02,  -2.42784414e-02,\n",
       "         -2.54578553e-02,  -2.66372710e-02,  -2.78166886e-02,\n",
       "         -2.89961044e-02,  -3.01755201e-02,  -3.13549377e-02,\n",
       "         -3.25343534e-02,  -3.37137692e-02,  -3.48931849e-02,\n",
       "         -3.60726044e-02,  -3.72520201e-02,  -3.84314358e-02,\n",
       "         -3.96108516e-02,  -4.07902673e-02,  -4.19696830e-02,\n",
       "         -4.31490988e-02],\n",
       "       [ -9.12670232e-03,  -1.03061181e-02,  -1.14855347e-02,\n",
       "         -1.26649505e-02,  -1.38443671e-02,  -1.50237838e-02,\n",
       "         -1.62032004e-02,  -1.73826162e-02,  -1.85620319e-02,\n",
       "         -1.97414495e-02,  -2.09208652e-02,  -2.21002828e-02,\n",
       "         -2.32796986e-02,  -2.44591143e-02,  -2.56385319e-02,\n",
       "         -2.68179458e-02,  -2.79973615e-02,  -2.91767791e-02,\n",
       "         -3.03561948e-02,  -3.15356143e-02,  -3.27150263e-02,\n",
       "         -3.38944457e-02,  -3.50738615e-02,  -3.62532772e-02,\n",
       "         -3.74326929e-02,  -3.86121087e-02,  -3.97915244e-02,\n",
       "         -4.09709439e-02],\n",
       "       [ -6.94854371e-03,  -8.12796038e-03,  -9.30737704e-03,\n",
       "         -1.04867928e-02,  -1.16662085e-02,  -1.28456252e-02,\n",
       "         -1.40250418e-02,  -1.52044585e-02,  -1.63838733e-02,\n",
       "         -1.75632909e-02,  -1.87427066e-02,  -1.99221242e-02,\n",
       "         -2.11015400e-02,  -2.22809557e-02,  -2.34603733e-02,\n",
       "         -2.46397890e-02,  -2.58192029e-02,  -2.69986223e-02,\n",
       "         -2.81780362e-02,  -2.93574538e-02,  -3.05368695e-02,\n",
       "         -3.17162871e-02,  -3.28957029e-02,  -3.40751186e-02,\n",
       "         -3.52545343e-02,  -3.64339501e-02,  -3.76133658e-02,\n",
       "         -3.87927853e-02],\n",
       "       [ -4.77038603e-03,  -5.94980223e-03,  -7.12921889e-03,\n",
       "         -8.30863416e-03,  -9.48805083e-03,  -1.06674675e-02,\n",
       "         -1.18468842e-02,  -1.30263008e-02,  -1.42057156e-02,\n",
       "         -1.53851332e-02,  -1.65645480e-02,  -1.77439656e-02,\n",
       "         -1.89233813e-02,  -2.01027989e-02,  -2.12822147e-02,\n",
       "         -2.24616304e-02,  -2.36410461e-02,  -2.48204637e-02,\n",
       "         -2.59998795e-02,  -2.71792971e-02,  -2.83587128e-02,\n",
       "         -2.95381304e-02,  -3.07175443e-02,  -3.18969600e-02,\n",
       "         -3.30763794e-02,  -3.42557952e-02,  -3.54352072e-02,\n",
       "         -3.66146266e-02],\n",
       "       [ -2.59222672e-03,  -3.77164315e-03,  -4.95105982e-03,\n",
       "         -6.13047509e-03,  -7.30989175e-03,  -8.48930795e-03,\n",
       "         -9.66872461e-03,  -1.08481403e-02,  -1.20275579e-02,\n",
       "         -1.32069727e-02,  -1.43863894e-02,  -1.55658061e-02,\n",
       "         -1.67452227e-02,  -1.79246385e-02,  -1.91040542e-02,\n",
       "         -2.02834718e-02,  -2.14628875e-02,  -2.26423033e-02,\n",
       "         -2.38217209e-02,  -2.50011366e-02,  -2.61805542e-02,\n",
       "         -2.73599699e-02,  -2.85393838e-02,  -2.97188014e-02,\n",
       "         -3.08982208e-02,  -3.20776328e-02,  -3.32570523e-02,\n",
       "         -3.44364680e-02],\n",
       "       [ -4.14068607e-04,  -1.59348513e-03,  -2.77290167e-03,\n",
       "         -3.95231694e-03,  -5.13173360e-03,  -6.31115027e-03,\n",
       "         -7.49056647e-03,  -8.66998266e-03,  -9.84939933e-03,\n",
       "         -1.10288151e-02,  -1.22082317e-02,  -1.33876484e-02,\n",
       "         -1.45670651e-02,  -1.57464817e-02,  -1.69258974e-02,\n",
       "         -1.81053132e-02,  -1.92847289e-02,  -2.04641446e-02,\n",
       "         -2.16435622e-02,  -2.28229780e-02,  -2.40023956e-02,\n",
       "         -2.51818113e-02,  -2.63612270e-02,  -2.75406446e-02,\n",
       "         -2.87200622e-02,  -2.98994742e-02,  -3.10788918e-02,\n",
       "         -3.22583094e-02],\n",
       "       [  1.76409062e-03,   5.84674126e-04,  -5.94742422e-04,\n",
       "         -1.77415775e-03,  -2.95357429e-03,  -4.13299073e-03,\n",
       "         -5.31240739e-03,  -6.49182312e-03,  -7.67123979e-03,\n",
       "         -8.85065645e-03,  -1.00300722e-02,  -1.12094888e-02,\n",
       "         -1.23889055e-02,  -1.35683222e-02,  -1.47477370e-02,\n",
       "         -1.59271546e-02,  -1.71065703e-02,  -1.82859860e-02,\n",
       "         -1.94654018e-02,  -2.06448194e-02,  -2.18242351e-02,\n",
       "         -2.30036508e-02,  -2.41830684e-02,  -2.53624860e-02,\n",
       "         -2.65419018e-02,  -2.77213175e-02,  -2.89007351e-02,\n",
       "         -3.00801508e-02],\n",
       "       [  3.94224888e-03,   2.76283221e-03,   1.58341555e-03,\n",
       "          4.04000282e-04,  -7.75416265e-04,  -1.95483281e-03,\n",
       "         -3.13424924e-03,  -4.31366544e-03,  -5.49308164e-03,\n",
       "         -6.67249830e-03,  -7.85191450e-03,  -9.03133117e-03,\n",
       "         -1.02107478e-02,  -1.13901636e-02,  -1.25695793e-02,\n",
       "         -1.37489960e-02,  -1.49284126e-02,  -1.61078293e-02,\n",
       "         -1.72872450e-02,  -1.84666626e-02,  -1.96460765e-02,\n",
       "         -2.08254922e-02,  -2.20049098e-02,  -2.31843274e-02,\n",
       "         -2.43637431e-02,  -2.55431589e-02,  -2.67225765e-02,\n",
       "         -2.79019922e-02],\n",
       "       [  6.12040749e-03,   4.94099082e-03,   3.76157416e-03,\n",
       "          2.58215889e-03,   1.40274235e-03,   2.23325827e-04,\n",
       "         -9.56090691e-04,  -2.13550660e-03,  -3.31492326e-03,\n",
       "         -4.49433969e-03,  -5.67375636e-03,  -6.85317209e-03,\n",
       "         -8.03258829e-03,  -9.21200495e-03,  -1.03914216e-02,\n",
       "         -1.15708373e-02,  -1.27502531e-02,  -1.39296697e-02,\n",
       "         -1.51090864e-02,  -1.62885021e-02,  -1.74679197e-02,\n",
       "         -1.86473355e-02,  -1.98267531e-02,  -2.10061669e-02,\n",
       "         -2.21855864e-02,  -2.33650003e-02,  -2.45444160e-02,\n",
       "         -2.57238336e-02],\n",
       "       [  8.29856563e-03,   7.11914897e-03,   5.93973231e-03,\n",
       "          4.76031704e-03,   3.58090037e-03,   2.40148394e-03,\n",
       "          1.22206728e-03,   4.26513798e-05,  -1.13676512e-03,\n",
       "         -2.31618178e-03,  -3.49559798e-03,  -4.67501441e-03,\n",
       "         -5.85443014e-03,  -7.03384681e-03,  -8.21326301e-03,\n",
       "         -9.39267967e-03,  -1.05720954e-02,  -1.17515121e-02,\n",
       "         -1.29309278e-02,  -1.41103445e-02,  -1.52897611e-02,\n",
       "         -1.64691769e-02,  -1.76485945e-02,  -1.88280102e-02,\n",
       "         -2.00074278e-02,  -2.11868435e-02,  -2.23662574e-02,\n",
       "         -2.35456750e-02],\n",
       "       [  1.04767242e-02,   9.29730758e-03,   8.11789092e-03,\n",
       "          6.93847565e-03,   5.75905899e-03,   4.57964232e-03,\n",
       "          3.40022589e-03,   2.22080993e-03,   1.04139349e-03,\n",
       "         -1.38023068e-04,  -1.31743925e-03,  -2.49685557e-03,\n",
       "         -3.67627176e-03,  -4.85568820e-03,  -6.03510439e-03,\n",
       "         -7.21452059e-03,  -8.39393679e-03,  -9.57335345e-03,\n",
       "         -1.07527701e-02,  -1.19321859e-02,  -1.31116034e-02,\n",
       "         -1.42910182e-02,  -1.54704349e-02,  -1.66498516e-02,\n",
       "         -1.78292673e-02,  -1.90086830e-02,  -2.01881006e-02,\n",
       "         -2.13675145e-02],\n",
       "       [  1.26548829e-02,   1.14754653e-02,   1.02960505e-02,\n",
       "          9.11663380e-03,   7.93721806e-03,   6.75780093e-03,\n",
       "          5.57838473e-03,   4.39896854e-03,   3.21955211e-03,\n",
       "          2.04013567e-03,   8.60719359e-04,  -3.18696897e-04,\n",
       "         -1.49811315e-03,  -2.67752935e-03,  -3.85694602e-03,\n",
       "         -5.03636198e-03,  -6.21577865e-03,  -7.39519438e-03,\n",
       "         -8.57461058e-03,  -9.75402724e-03,  -1.09334439e-02,\n",
       "         -1.21128606e-02,  -1.32922763e-02,  -1.44716939e-02,\n",
       "         -1.56511087e-02,  -1.68305244e-02,  -1.80099420e-02,\n",
       "         -1.91893578e-02],\n",
       "       [  1.48330415e-02,   1.36536248e-02,   1.24742081e-02,\n",
       "          1.12947933e-02,   1.01153767e-02,   8.93596001e-03,\n",
       "          7.75654335e-03,   6.57712715e-03,   5.39771095e-03,\n",
       "          4.21829382e-03,   3.03887762e-03,   1.85946142e-03,\n",
       "          6.80045283e-04,  -4.99371090e-04,  -1.67878740e-03,\n",
       "         -2.85820384e-03,  -4.03762003e-03,  -5.21703623e-03,\n",
       "         -6.39645243e-03,  -7.57586863e-03,  -8.75528529e-03,\n",
       "         -9.93470103e-03,  -1.11141177e-02,  -1.22935344e-02,\n",
       "         -1.34729510e-02,  -1.46523658e-02,  -1.58317834e-02,\n",
       "         -1.70111991e-02],\n",
       "       [  1.70111991e-02,   1.58317834e-02,   1.46523658e-02,\n",
       "          1.34729510e-02,   1.22935344e-02,   1.11141177e-02,\n",
       "          9.93470103e-03,   8.75528529e-03,   7.57586863e-03,\n",
       "          6.39645243e-03,   5.21703623e-03,   4.03762003e-03,\n",
       "          2.85820384e-03,   1.67878740e-03,   4.99371090e-04,\n",
       "         -6.80045283e-04,  -1.85946142e-03,  -3.03887762e-03,\n",
       "         -4.21829382e-03,  -5.39771095e-03,  -6.57712715e-03,\n",
       "         -7.75654335e-03,  -8.93596001e-03,  -1.01153767e-02,\n",
       "         -1.12947933e-02,  -1.24742081e-02,  -1.36536248e-02,\n",
       "         -1.48330415e-02],\n",
       "       [  1.91893578e-02,   1.80099420e-02,   1.68305244e-02,\n",
       "          1.56511087e-02,   1.44716939e-02,   1.32922763e-02,\n",
       "          1.21128606e-02,   1.09334439e-02,   9.75402724e-03,\n",
       "          8.57461058e-03,   7.39519438e-03,   6.21577865e-03,\n",
       "          5.03636198e-03,   3.85694602e-03,   2.67752935e-03,\n",
       "          1.49811315e-03,   3.18696897e-04,  -8.60719359e-04,\n",
       "         -2.04013567e-03,  -3.21955211e-03,  -4.39896854e-03,\n",
       "         -5.57838473e-03,  -6.75780093e-03,  -7.93721806e-03,\n",
       "         -9.11663380e-03,  -1.02960505e-02,  -1.14754653e-02,\n",
       "         -1.26548829e-02],\n",
       "       [  2.13675145e-02,   2.01881006e-02,   1.90086830e-02,\n",
       "          1.78292673e-02,   1.66498516e-02,   1.54704349e-02,\n",
       "          1.42910182e-02,   1.31116034e-02,   1.19321859e-02,\n",
       "          1.07527701e-02,   9.57335345e-03,   8.39393679e-03,\n",
       "          7.21452059e-03,   6.03510439e-03,   4.85568820e-03,\n",
       "          3.67627176e-03,   2.49685557e-03,   1.31743925e-03,\n",
       "          1.38023068e-04,  -1.04139349e-03,  -2.22080993e-03,\n",
       "         -3.40022589e-03,  -4.57964232e-03,  -5.75905899e-03,\n",
       "         -6.93847565e-03,  -8.11789092e-03,  -9.29730758e-03,\n",
       "         -1.04767242e-02],\n",
       "       [  2.35456750e-02,   2.23662574e-02,   2.11868435e-02,\n",
       "          2.00074278e-02,   1.88280102e-02,   1.76485945e-02,\n",
       "          1.64691769e-02,   1.52897611e-02,   1.41103445e-02,\n",
       "          1.29309278e-02,   1.17515121e-02,   1.05720954e-02,\n",
       "          9.39267967e-03,   8.21326301e-03,   7.03384681e-03,\n",
       "          5.85443014e-03,   4.67501441e-03,   3.49559798e-03,\n",
       "          2.31618178e-03,   1.13676512e-03,  -4.26513798e-05,\n",
       "         -1.22206728e-03,  -2.40148394e-03,  -3.58090037e-03,\n",
       "         -4.76031704e-03,  -5.93973231e-03,  -7.11914897e-03,\n",
       "         -8.29856563e-03],\n",
       "       [  2.57238336e-02,   2.45444160e-02,   2.33650003e-02,\n",
       "          2.21855864e-02,   2.10061669e-02,   1.98267531e-02,\n",
       "          1.86473355e-02,   1.74679197e-02,   1.62885021e-02,\n",
       "          1.51090864e-02,   1.39296697e-02,   1.27502531e-02,\n",
       "          1.15708373e-02,   1.03914216e-02,   9.21200495e-03,\n",
       "          8.03258829e-03,   6.85317209e-03,   5.67375636e-03,\n",
       "          4.49433969e-03,   3.31492326e-03,   2.13550660e-03,\n",
       "          9.56090691e-04,  -2.23325827e-04,  -1.40274235e-03,\n",
       "         -2.58215889e-03,  -3.76157416e-03,  -4.94099082e-03,\n",
       "         -6.12040749e-03],\n",
       "       [  2.79019922e-02,   2.67225765e-02,   2.55431589e-02,\n",
       "          2.43637431e-02,   2.31843274e-02,   2.20049098e-02,\n",
       "          2.08254922e-02,   1.96460765e-02,   1.84666626e-02,\n",
       "          1.72872450e-02,   1.61078293e-02,   1.49284126e-02,\n",
       "          1.37489960e-02,   1.25695793e-02,   1.13901636e-02,\n",
       "          1.02107478e-02,   9.03133117e-03,   7.85191450e-03,\n",
       "          6.67249830e-03,   5.49308164e-03,   4.31366544e-03,\n",
       "          3.13424924e-03,   1.95483281e-03,   7.75416265e-04,\n",
       "         -4.04000282e-04,  -1.58341555e-03,  -2.76283221e-03,\n",
       "         -3.94224888e-03],\n",
       "       [  3.00801508e-02,   2.89007351e-02,   2.77213175e-02,\n",
       "          2.65419018e-02,   2.53624860e-02,   2.41830684e-02,\n",
       "          2.30036508e-02,   2.18242351e-02,   2.06448194e-02,\n",
       "          1.94654018e-02,   1.82859860e-02,   1.71065703e-02,\n",
       "          1.59271546e-02,   1.47477370e-02,   1.35683222e-02,\n",
       "          1.23889055e-02,   1.12094888e-02,   1.00300722e-02,\n",
       "          8.85065645e-03,   7.67123979e-03,   6.49182312e-03,\n",
       "          5.31240739e-03,   4.13299073e-03,   2.95357429e-03,\n",
       "          1.77415775e-03,   5.94742422e-04,  -5.84674126e-04,\n",
       "         -1.76409062e-03],\n",
       "       [  3.22583094e-02,   3.10788918e-02,   2.98994742e-02,\n",
       "          2.87200622e-02,   2.75406446e-02,   2.63612270e-02,\n",
       "          2.51818113e-02,   2.40023956e-02,   2.28229780e-02,\n",
       "          2.16435622e-02,   2.04641446e-02,   1.92847289e-02,\n",
       "          1.81053132e-02,   1.69258974e-02,   1.57464817e-02,\n",
       "          1.45670651e-02,   1.33876484e-02,   1.22082317e-02,\n",
       "          1.10288151e-02,   9.84939933e-03,   8.66998266e-03,\n",
       "          7.49056647e-03,   6.31115027e-03,   5.13173360e-03,\n",
       "          3.95231694e-03,   2.77290167e-03,   1.59348513e-03,\n",
       "          4.14068607e-04],\n",
       "       [  3.44364680e-02,   3.32570523e-02,   3.20776328e-02,\n",
       "          3.08982208e-02,   2.97188014e-02,   2.85393838e-02,\n",
       "          2.73599699e-02,   2.61805542e-02,   2.50011366e-02,\n",
       "          2.38217209e-02,   2.26423033e-02,   2.14628875e-02,\n",
       "          2.02834718e-02,   1.91040542e-02,   1.79246385e-02,\n",
       "          1.67452227e-02,   1.55658061e-02,   1.43863894e-02,\n",
       "          1.32069727e-02,   1.20275579e-02,   1.08481403e-02,\n",
       "          9.66872461e-03,   8.48930795e-03,   7.30989175e-03,\n",
       "          6.13047509e-03,   4.95105982e-03,   3.77164315e-03,\n",
       "          2.59222672e-03],\n",
       "       [  3.66146266e-02,   3.54352072e-02,   3.42557952e-02,\n",
       "          3.30763794e-02,   3.18969600e-02,   3.07175443e-02,\n",
       "          2.95381304e-02,   2.83587128e-02,   2.71792971e-02,\n",
       "          2.59998795e-02,   2.48204637e-02,   2.36410461e-02,\n",
       "          2.24616304e-02,   2.12822147e-02,   2.01027989e-02,\n",
       "          1.89233813e-02,   1.77439656e-02,   1.65645480e-02,\n",
       "          1.53851332e-02,   1.42057156e-02,   1.30263008e-02,\n",
       "          1.18468842e-02,   1.06674675e-02,   9.48805083e-03,\n",
       "          8.30863416e-03,   7.12921889e-03,   5.94980223e-03,\n",
       "          4.77038603e-03],\n",
       "       [  3.87927853e-02,   3.76133658e-02,   3.64339501e-02,\n",
       "          3.52545343e-02,   3.40751186e-02,   3.28957029e-02,\n",
       "          3.17162871e-02,   3.05368695e-02,   2.93574538e-02,\n",
       "          2.81780362e-02,   2.69986223e-02,   2.58192029e-02,\n",
       "          2.46397890e-02,   2.34603733e-02,   2.22809557e-02,\n",
       "          2.11015400e-02,   1.99221242e-02,   1.87427066e-02,\n",
       "          1.75632909e-02,   1.63838733e-02,   1.52044585e-02,\n",
       "          1.40250418e-02,   1.28456252e-02,   1.16662085e-02,\n",
       "          1.04867928e-02,   9.30737704e-03,   8.12796038e-03,\n",
       "          6.94854371e-03],\n",
       "       [  4.09709439e-02,   3.97915244e-02,   3.86121087e-02,\n",
       "          3.74326929e-02,   3.62532772e-02,   3.50738615e-02,\n",
       "          3.38944457e-02,   3.27150263e-02,   3.15356143e-02,\n",
       "          3.03561948e-02,   2.91767791e-02,   2.79973615e-02,\n",
       "          2.68179458e-02,   2.56385319e-02,   2.44591143e-02,\n",
       "          2.32796986e-02,   2.21002828e-02,   2.09208652e-02,\n",
       "          1.97414495e-02,   1.85620319e-02,   1.73826162e-02,\n",
       "          1.62032004e-02,   1.50237838e-02,   1.38443671e-02,\n",
       "          1.26649505e-02,   1.14855347e-02,   1.03061181e-02,\n",
       "          9.12670232e-03],\n",
       "       [  4.31490988e-02,   4.19696830e-02,   4.07902673e-02,\n",
       "          3.96108516e-02,   3.84314358e-02,   3.72520201e-02,\n",
       "          3.60726044e-02,   3.48931849e-02,   3.37137692e-02,\n",
       "          3.25343534e-02,   3.13549377e-02,   3.01755201e-02,\n",
       "          2.89961044e-02,   2.78166886e-02,   2.66372710e-02,\n",
       "          2.54578553e-02,   2.42784414e-02,   2.30990238e-02,\n",
       "          2.19196081e-02,   2.07401905e-02,   1.95607748e-02,\n",
       "          1.83813572e-02,   1.72019415e-02,   1.60225257e-02,\n",
       "          1.48431081e-02,   1.36636933e-02,   1.24842767e-02,\n",
       "          1.13048600e-02],\n",
       "       [  4.53272648e-02,   4.41478454e-02,   4.29684296e-02,\n",
       "          4.17890139e-02,   4.06095982e-02,   3.94301787e-02,\n",
       "          3.82507630e-02,   3.70713472e-02,   3.58919315e-02,\n",
       "          3.47125120e-02,   3.35330963e-02,   3.23536843e-02,\n",
       "          3.11742648e-02,   2.99948491e-02,   2.88154315e-02,\n",
       "          2.76360158e-02,   2.64565982e-02,   2.52771843e-02,\n",
       "          2.40977686e-02,   2.29183510e-02,   2.17389353e-02,\n",
       "          2.05595195e-02,   1.93801019e-02,   1.82006862e-02,\n",
       "          1.70212686e-02,   1.58418529e-02,   1.46624371e-02,\n",
       "          1.34830205e-02]], dtype=float32)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dojo\n",
    "from display import*\n",
    "# from mnist import load_mnist\n",
    "# device, tra_X, tra_y, tes_X, tes_y = load_mnist(False)\n",
    "# def unique(x, dim=-1):\n",
    "#     unique, inverse = torch.unique(x, return_inverse=True, dim=dim)\n",
    "#     perm = torch.arange(inverse.size(dim), dtype=inverse.dtype, device=inverse.device)\n",
    "#     inverse, perm = inverse.flip([dim]), perm.flip([dim])\n",
    "#     return unique, inverse.new_empty(unique.size(dim)).scatter_(dim, inverse, perm)\n",
    "\n",
    "# un,ids = unique(tra_y,dim=0)\n",
    "# ids = ids[:1]\n",
    "# # print(len(ids))\n",
    "# train_X = tra_y[ids]\n",
    "# train_y = tra_X[ids]\n",
    "# test_X = tra_y[ids]\n",
    "# test_y = tra_X[ids]\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "res=28\n",
    "n = 10000\n",
    "# positioning\n",
    "xr = torch.range(0,res-1,device=device) - (res-1)/2\n",
    "xi,yi = torch.meshgrid(xr,xr, indexing='ij')\n",
    "x = torch.stack((xi,yi),dim=2)\n",
    "# data\n",
    "y = torch.stack((x[:,:,0],x[:,:,1]),dim=2).unsqueeze(2) # res x res x 1 x 2 #,x[:,:,0]**2/2,x[:,:,1]**2/2\n",
    "rc = torch.randn(size=(1,1,2,n),device=device)\n",
    "y = torch.matmul(y,rc).T.squeeze().reshape(n,res**2) # n x im_size\n",
    "# normalize\n",
    "y /= res**2\n",
    "y = (y - y.mean(dim=1,keepdim=True))#/y.std(dim=1,keepdim=True)\n",
    "# to device (and 2D)\n",
    "data = y.to(device)\n",
    "\n",
    "train_X = data[:7500]\n",
    "train_y = data[:7500]\n",
    "test_X = data[7500:]\n",
    "test_y = data[7500:]\n",
    "\n",
    "(y[0].detach().cpu().numpy().reshape(res,res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "66eb7e4c-6add-4e3b-a69b-557476ad92d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DNN import DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "e9f410e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LOSS(yhat,y):\n",
    "    e0 = torch.nn.MSELoss()(yhat,y)\n",
    "    p1 = torch.nn.functional.softmax(yhat,dim=1)\n",
    "    p2 = torch.nn.functional.softmax(y,dim=1)\n",
    "    e1 = 50*torch.nn.KLDivLoss()(p1,p2)\n",
    "    return e0 + e1\n",
    "\n",
    "def make_x(p):\n",
    "    b = p.shape[0]\n",
    "    xr = torch.range(0,res-1,device=device)/res - 0.5\n",
    "    xi,yi = torch.meshgrid(xr,xr, indexing='ij')\n",
    "    return torch.stack((xi,yi),dim=2).unsqueeze(0).expand(b,res,res,2)\n",
    "\n",
    "def reshape_p(p):\n",
    "    d = 2\n",
    "    nv = 6\n",
    "    b = p.shape[0]\n",
    "    n_func = p.shape[1]//(nv)\n",
    "    return torch.reshape(p,(b,nv,n_func)), n_func\n",
    "\n",
    "def gaussian(x, mu=0, sigma=1):\n",
    "    return torch.exp(-((x - mu)**2 / (2 * sigma**2))) / (sigma * np.sqrt(2 * np.pi))\n",
    "\n",
    "def hat(x):\n",
    "    p = torch.sign(x) \n",
    "    return (1-torch.nn.functional.relu(p *x)) * (torch.trunc(x)==0)\n",
    "\n",
    "def forward(x,p):\n",
    "    d=2 # hard coded for now\n",
    "    b = p.shape[0]\n",
    "    p_, n_func = reshape_p(p)\n",
    "    x_ = torch.reshape(x,(b,np.prod(x.size()[1:-1]),d))\n",
    "    # batched mul of every x in x_ with every p in p_[:2,]\n",
    "    # print(torch.bmm(x_,p_[:,:d,:]).size())\n",
    "    # print(torch.bmm(x_[:,:,0].unsqueeze(2),torch.cos(p_[:,0,:]).unsqueeze(1)).size())\n",
    "\n",
    "    # x :batch, image_size, n_func\n",
    "    # p :batch, 3, n_func\n",
    "    # pre-act :batch, image_size, n_func, 2\n",
    "    act = lambda x : hat(x[:,:,:,0])*hat(x[:,:,:,1])#torch.nn.SiLU()\n",
    "    # act :batch, image_size, n_func\n",
    "    angle = 2*torch.pi * torch.frac(p_[:,1,:] + 0.5)\n",
    "    c = torch.cos(angle).unsqueeze(1)\n",
    "    s = torch.sin(angle).unsqueeze(1)\n",
    "    xs = x_[:,:,0].unsqueeze(2)\n",
    "    ys = x_[:,:,1].unsqueeze(2)\n",
    "    xscale = p_[:,2,:].unsqueeze(1).expand(b,x_.shape[1],n_func)\n",
    "    yscale = p_[:,3,:].unsqueeze(1).expand(b,x_.shape[1],n_func)\n",
    "    xshift = p_[:,4,:].unsqueeze(1)\n",
    "    yshift = p_[:,5,:].unsqueeze(1)\n",
    "    a = act( \\\n",
    "                torch.stack(( \\\n",
    "                    torch.mul((torch.bmm(xs,c) - torch.bmm(ys,s)),xscale) + xshift, \\\n",
    "                    torch.mul((torch.bmm(xs,s) + torch.bmm(ys,c)),yscale) + yshift, \\\n",
    "                ),dim=3) \\\n",
    "            )\n",
    "    return torch.bmm((1/n_func + p_[:,0,:]).unsqueeze(1),a.mT)\n",
    "\n",
    "def constraint(x,p,size0):\n",
    "    if p is None:\n",
    "        return torch.zeros(size0,device=device)\n",
    "    p_ = reshape_p(p)[0]\n",
    "    return torch.mean(torch.concatenate((p_[:,0,:]*0.0,torch.trunc(p_[:,1,:])**2,*[p_[:,j,:]*0.0 for j in range(2,6)]),dim=1),dim=0).expand(size0)\n",
    "\n",
    "\n",
    "def criterion(p, y):\n",
    "    # n_func = p.shape[1]//(3)\n",
    "    out = forward(make_x(p),p).reshape(p.shape[0],res**2)\n",
    "    # print(out.mean())\n",
    "    p_ = reshape_p(p)[0]\n",
    "    return LOSS(out, y) + torch.mean(torch.trunc(p_[:,1,:]+0.5)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "3c9edbcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0000, -0.0000, 0.0000, 0.3333, 0.6667, 1.0000, 0.6667, 0.3333, 0.0000,\n",
       "        -0.0000, -0.0000])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hat(torch.Tensor([-5,-4,-3,-2,-1,0,1,2,3,4,5])/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "ad184354",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = dojo.dojo()\n",
    "D.optimizer = lambda x : optim.ASGD(x, lr=0.0001, weight_decay=1e-8) # optim.Adam(net.parameters(), lr=base_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "fffd24bf-642d-4f3f-8b32-42995a98b1da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNN(\n",
       "  (activation): SiLU()\n",
       "  (prob): Identity()\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=904, out_features=1808, bias=False)\n",
       "    (1): Linear(in_features=1808, out_features=120, bias=False)\n",
       "    (2): Linear(in_features=120, out_features=120, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_param=120 # a multiple of 6\n",
    "net = DNN(sizes=[n_param + res**2,2*(n_param + res**2),n_param,n_param],d2_input=False,classification=False,activation = torch.nn.SiLU(),criterion=criterion,bias=False, init=np.sqrt(2.0))\n",
    "net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "1951ac38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [00:54<00:08,  1.58it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 388.00 MiB (GPU 0; 8.00 GiB total capacity; 6.09 GiB already allocated; 0 bytes free; 6.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [307], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m D\u001b[39m.\u001b[39mepochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39m# torch.autograd.set_detect_anomaly(True)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m report \u001b[39m=\u001b[39m D\u001b[39m.\u001b[39;49mtrain(net, train_X, train_y, clip\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, constraint\u001b[39m=\u001b[39;49mconstraint,start_batch_size\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m ecran(net, test_X, test_y, criterion, report, classification\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/mnt/c/Users/sadam/Desktop/Research/PHO_ICES_ML/allegory/../core/dojo.py:73\u001b[0m, in \u001b[0;36mdojo.train\u001b[0;34m(self, net, train_X, train_y, constraint, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mif\u001b[39;00m ro:\n\u001b[1;32m     71\u001b[0m     inp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mconcatenate((inp,constraint(inp,outs[\u001b[39m1\u001b[39m],(cbatch_size,net\u001b[39m.\u001b[39msizes[\u001b[39m0\u001b[39m]\u001b[39m-\u001b[39mtrain_X[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)))),dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m outs \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39;49mtrain(inp, train_y[i:i\u001b[39m+\u001b[39;49mcbatch_size], optimizer,return_outputs\u001b[39m=\u001b[39;49mro, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     74\u001b[0m ls \u001b[39m=\u001b[39m outs \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(outs) \u001b[39m!=\u001b[39m \u001b[39mtuple\u001b[39m \u001b[39melse\u001b[39;00m outs[\u001b[39m0\u001b[39m]\n\u001b[1;32m     76\u001b[0m \u001b[39m# print statistics\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/sadam/Desktop/Research/PHO_ICES_ML/allegory/../core/DNN.py:29\u001b[0m, in \u001b[0;36mDNN.train\u001b[0;34m(self, x, y, optimizer, clip, return_outputs, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(x)\n\u001b[1;32m     28\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(outputs, y)\n\u001b[0;32m---> 29\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     30\u001b[0m \u001b[39mif\u001b[39;00m clip \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameters(), clip)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 388.00 MiB (GPU 0; 8.00 GiB total capacity; 6.09 GiB already allocated; 0 bytes free; 6.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "D.max_batch_size=train_X.size(0)\n",
    "D.epochs=100\n",
    "# torch.autograd.set_detect_anomaly(True)\n",
    "report = D.train(net, train_X, train_y, clip=10, constraint=constraint,start_batch_size=100)\n",
    "ecran(net, test_X, test_y, criterion, report, classification=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7dfeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "report[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d79f217",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X2 = torch.concatenate((test_X,torch.zeros((test_X.size(0),n_param),device=device)),dim=1)\n",
    "\n",
    "p = net(test_X2)\n",
    "outs = forward(make_x(p),p)\n",
    "\n",
    "# plt.plot(report[-1])\n",
    "ni=5\n",
    "si=2\n",
    "fig,axs=plt.subplots(1,5)\n",
    "[axs[i].imshow(outs[si+i].detach().cpu().numpy().reshape(res,res)) for i in range(ni)]\n",
    "[axs[i].axis('off') for i in range(ni)]\n",
    "fig,axs=plt.subplots(1,5)\n",
    "[axs[i].imshow(test_y[si+i].detach().cpu().numpy().reshape(res,res)) for i in range(ni)]\n",
    "[axs[i].axis('off') for i in range(ni)]\n",
    "plt.figure()\n",
    "plt.plot(reshape_p(p)[0][:,1,:].detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38821068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import *\n",
    "def resize_graph(dot, size_per_element=0.15, min_size=12):\n",
    "    \"\"\"Resize the graph according to how much content it contains.\n",
    "    Modify the graph in place.\n",
    "    \"\"\"\n",
    "    # Get the approximate number of nodes and edges\n",
    "    num_rows = len(dot.body)\n",
    "    content_size = num_rows * size_per_element\n",
    "    size = max(min_size, content_size)\n",
    "    size_str = str(size) + \",\" + str(size)\n",
    "    dot.graph_attr.update(size=size_str)\n",
    "\n",
    "\n",
    "dot = make_dot(outs.mean(), params=dict(net.named_parameters()), show_attrs=True)\n",
    "resize_graph(dot,size_per_element=0.15, min_size=24)\n",
    "dot.render(\"graph\", format=\"svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8076a7-2835-4056-804c-48b7fcd0a67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (torch.rand((28,28)) > 0.5).to(device)\n",
    "# plt.imshow(train_X[0].detach().cpu().numpy())\n",
    "mixed = train_X[234]*mask + train_X[234]*(~mask)\n",
    "plt.imshow(mixed.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "3377eebc-247f-4475-8e2e-1f055a63e2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del net, report\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc0903d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
