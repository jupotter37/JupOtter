{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 0s )speaker_00:  olá, sou  davi e estou à disposição para te atender no que você precisar!\n",
      "( 4s )speaker_00:  para registro, o protocolo do seu atendimento é 12192645.\n",
      "( 15s )speaker_00:  olá, raphael! tudo bem? é um prazer poder te atender.\n",
      "( 19s )speaker_01:  olá! td bem e vc?acabei de pagar a fatura do meu cartão mas ele ainda ta bloqueado. conseguiria desoquear?\n",
      "( 41s )speaker_01:  olá! td bem e vc?acabei de pagar a fatura do meu cartão mas ele ainda ta bloqueado. conseguiria desoquear?\n",
      "( 2m 23s )speaker_00:  tudo sim, agradeço por perguntar\n",
      "( 3m 8s )speaker_00:  em sistema nós não temos disponível a reativação ao informar o pagamento, mas vi que ele foi efetuado por débito em conta, por isso o desbloqueio ocorrerá de forma mais veloz\n",
      "( 3m 55s )speaker_00:  ele pode demorar até 1 dia útil, mas, normalmente, é imediato se o pagamento for via débito em conta\n",
      "( 5m 54s )speaker_01:  deu certo aqui\n",
      "( 6m 4s )speaker_01:  não foi por débito em conta mas deu certo\n",
      "( 6m 5s )speaker_01:  valeu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('../chats_fevereiro.csv')\n",
    "print(df['transcript'].iloc[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic Triple extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 175M\n",
      "-rw-rw-r-- 1 kiki kiki  12M Apr 29 16:43 atendimentos_rv_topic_summarization.csv\n",
      "-rw-rw-r-- 1 kiki kiki 417K Jun  4 11:55 BTG_TAGS.ipynb\n",
      "-rw-rw-r-- 1 kiki kiki  70M Apr 24 12:39 chats_fevereiro.csv\n",
      "-rw-rw-r-- 1 kiki kiki  12M May  7 00:52 corpus.pkl\n",
      "-rw-rw-r-- 1 kiki kiki  32M Apr 25 15:56 corpus_preprocessing.pkl\n",
      "-rw-r--r-- 1 kiki kiki  32M May  6 18:20 docs_3m.txt\n",
      "-rw-rw-r-- 1 kiki kiki 1,5M Apr 24 12:54 embeddings_visualization.ipynb\n",
      "drwxrwxr-x 2 kiki kiki 4,0K Jun  3 11:02 graph\n",
      "-rw-rw-r-- 1 kiki kiki    0 Apr 23 13:15 hier_clustering.ipynb\n",
      "-rw-rw-r-- 1 kiki kiki 2,1K May  6 19:17 keywords.txt\n",
      "-rw-rw-r-- 1 kiki kiki 5,9M Apr 30 14:28 resumos_chats_fevereiro2.p\n",
      "-rw-rw-r-- 1 kiki kiki 2,1M Apr 29 20:35 resumos_chats_fevereiro.p\n",
      "-rw-rw-r-- 1 kiki kiki 994K Apr 24 12:44 saida_text.csv\n",
      "-rw-rw-r-- 1 kiki kiki 8,1M Apr 24 12:44 saida_text_v2.csv\n",
      "drwxrwxr-x 5 kiki kiki 4,0K May  6 19:55 taxogen\n",
      "-rw-rw-r-- 1 kiki kiki    0 Apr 24 12:35 test.txt\n",
      "drwxrwxr-x 2 kiki kiki 4,0K Apr 30 15:35 tree-output\n",
      "-rw-rw-r-- 1 kiki kiki  50K Apr 28 23:36 udkg.zip\n",
      "drwxrwxr-x 3 kiki kiki 4,0K Apr  7 21:54 Unsupervised-Deep-Keyphrase-Generation\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25605, 52)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etapa 1- Extracao de entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etapa 2 - Extracao de Relacoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompts(max_knowledge_triplets,text):\n",
    "    prompt_triple_extraction=\"\"\"\n",
    "    A text from bank costumer assistant chat in Portuguese is given. \n",
    "    Given the text, extract up to\"\"\"+ str(max_knowledge_triplets)+\"\"\" knowledge triplets \n",
    "    in the form of (subject, predicate, object). Avoid stopwords.\n",
    "    ---------------------\n",
    "    Examples:\n",
    "    \n",
    "    Text: boa tarde jeison minha fatura está constando como não paga, mas a minha é débito automática.\n",
    "    Triplets:\n",
    "    (fatura, consta, não paga)\n",
    "    (fatura, definida, debito automático)\n",
    "    Text:  renata, como estes dias são considerados feriados, o agendamento está para o próximo dia útil,\n",
    "    ou seja, amanhã.\n",
    "    nesta situação, encaminharei seu caso para o nosso time interno verificar o porquê consta “vencida”, a fim de\n",
    "    realizar essa reparação o mais rápido possível e nisto, peço que mantenha o saldo em conta para realizarmos\n",
    "    o pagamento\n",
    "    Triplets:\n",
    "    (pagamento em feriado, agendado, próximo dia util)\n",
    "    (pagamento, necessário,saldo em conta )\n",
    "    Text:ok então, mas no próprio aplicativo informa que não foi paga a fatura e que irá gerar encargos.\n",
    "    Triplets:\n",
    "    (aplicativo, informa, fatura não paga)\n",
    "    (fatura não paga, ocasiona, encargos )\n",
    "    Text:obrigada por aguardar, as faturas do dia 11/12 e 13 de fevereiro estão isentas de juros e multa se forem pagos até dia 14/11.\n",
    "    Triplets:\n",
    "    (fatura, paga até vencimento, isenta de juros e multa)\n",
    "    (fatura,não pagas até vencimento, geram juros e multa)\n",
    "    \n",
    "    ---------------------\n",
    "    Do not add entities with persons names. Do no add entities with dates and numbers not relevant to the meaning. \n",
    "    Only add entities that are relevant to understand the problems and possible solution to user's questions.\n",
    "    When you encounter entities that look similar but are differently spelled (e.g. cartão and cartões),\n",
    "    use one name for all occurences, when extracting them from text. Try to diversify the entities extracted. \n",
    "    You have the freedom to write entities that do not appear in the text, but are relevant to explain the problem and solution, like in the examples.\n",
    "    \n",
    "    ---------------------\n",
    "    Get triples that explain: The User problem, the solution to the problem, the entities involved in the problem and solution.\n",
    "    Retorne SOMENTE um json com as triplas extraídas na seguinte forma:\n",
    "    {\n",
    "        \"triples\":[\n",
    "            {\"subject\":\"fatura\",\"predicate\":\"consta\",\"object\":\"não paga\"},\n",
    "            {\"subject\":\"fatura\",\"predicate\":\"definida\",\"object\":\"debito automático\"}\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    user_text=f\"\"\"\n",
    "    Text: {text}\n",
    "\n",
    "    Triplets:\n",
    "\n",
    "    \"\"\"\n",
    "    return prompt_triple_extraction,user_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompts(max_knowledge_triplets,text):\n",
    "    prompt_triple_extraction=\"\"\"\n",
    "    A text from bank costumer assistant chat in Portuguese is given. \n",
    "    Given the text, extract up to \"\"\"+str(max_knowledge_triplets)+\"\"\" knowledge triplets \n",
    "    in the form of (subject, predicate, object). Avoid stopwords.\n",
    "\n",
    "    Do not add entities with persons names. Do no add entities with dates and numbers not relevant to the meaning. \n",
    "    Only add entities that are relevant to understand the problems and possible solution to user's questions.\n",
    "    When you encounter entities that look similar but are differently spelled (e.g. cartão and cartões),\n",
    "    use one name for all occurences, when extracting them from text. Try to diversify the entities extracted. \n",
    "    You have the freedom to write entities that do not appear in the text, but are relevant to explain the problem and solution, like in the examples.\n",
    "    \n",
    "    Get triples that explain: The User problem(first triple), the entities involved in the problem and solution, and  the solution to the problem(last triple).\n",
    "    Assure that the triples have connections between them.\n",
    "    Entities relevant: cliente, solução\n",
    "    Relation examples: ter problema, estar, possuir, obter,etc\n",
    "\n",
    "    ---------------------\n",
    "    Examples:\n",
    "    \n",
    "    Chat:\n",
    "    ( 0s )speaker_00:  olá, sou  davi e estou à disposição para te atender no que você precisar!\n",
    "    ( 4s )speaker_00:  para registro, o protocolo do seu atendimento é 12192645.\n",
    "    ( 15s )speaker_00:  olá, raphael! tudo bem? é um prazer poder te atender.\n",
    "    ( 19s )speaker_01:  olá! td bem e vc?acabei de pagar a fatura do meu cartão mas ele ainda ta bloqueado. conseguiria desoquear?\n",
    "    ( 2m 23s )speaker_00:  tudo sim, agradeço por perguntar\n",
    "    ( 3m 8s )speaker_00:  em sistema nós não temos disponível a reativação ao informar o pagamento, mas vi que ele foi efetuado por\n",
    "      débito em conta, por isso o desbloqueio ocorrerá de forma mais veloz\n",
    "    ( 3m 55s )speaker_00:  ele pode demorar até 1 dia útil, mas, normalmente, é imediato se o pagamento for via débito em conta\n",
    "    ( 5m 54s )speaker_01:  deu certo aqui\n",
    "    ( 6m 4s )speaker_01:  não foi por débito em conta mas deu certo\n",
    "    ( 6m 5s )speaker_01:  valeu\n",
    "\n",
    "    \n",
    "    Triplas:\n",
    "    (cliente,tem problema, cartão)\n",
    "    (cartão,esta, bloqueado)\n",
    "    (atendente,não tem, reativação)\n",
    "    (cartão,libera, pagamento)\n",
    "    (pagamento,foi, débito em conta)\n",
    "    (débito em conta,possui, desbloqueio mais rápido)\n",
    "    (solução,obtida por, pagamento)\n",
    "    \n",
    "    Obrigatory: Make it a chain of triples that are connected to each other. \n",
    "    The Relation need to have at least one verb that connects the entities.\n",
    "\n",
    "    Retorne SOMENTE um json com as triplas extraídas na seguinte forma:\n",
    "    {\n",
    "        \"triples\":[\n",
    "            {\"subject\":\"fatura\",\"predicate\":\"consta\",\"object\":\"não paga\"},\n",
    "            {\"subject\":\"fatura\",\"predicate\":\"definida\",\"object\":\"debito automático\"}\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    user_text=f\"\"\"\n",
    "    Text: {text}\n",
    "\n",
    "    Triplets:\n",
    "\n",
    "    \"\"\"\n",
    "    return prompt_triple_extraction,user_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e61b42539545de8406028b91a86504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "/home/kiki/miniconda3/envs/llama3/lib/python3.11/site-packages/transformers/quantizers/auto.py:167: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b686557f23fa4b849379a5e818c906ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/331k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc77afe1fd0b46b39614b956805caa9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a120aa755e3c4566a556ded57faaf8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00006.safetensors:   0%|          | 0.00/7.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d469713511e4ca09bec378efa988726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00006.safetensors:   0%|          | 0.00/6.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3caec6dbe254273b223e581ea64dfdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00006.safetensors:   0%|          | 0.00/6.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096679e6523649c0b14f77369f0dcc8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00006.safetensors:   0%|          | 0.00/6.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b42cfa54ad4388b1db61f92401bc6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00006.safetensors:   0%|          | 0.00/6.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ccddab447354991bbd97020b41de93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00006.safetensors:   0%|          | 0.00/4.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1693c09325b6484c8587a732d4cb7e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 23.66 GiB of which 102.06 MiB is free. Including non-PyTorch memory, this process has 22.27 GiB memory in use. Of the allocated memory 21.89 GiB is allocated by PyTorch, and 62.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# model_id = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsloth/llama-3-70b-Instruct-bnb-4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch_dtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquantization_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mload_in_4bit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlow_cpu_mem_usage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3/lib/python3.11/site-packages/transformers/pipelines/__init__.py:906\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    905\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 906\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    916\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    917\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3/lib/python3.11/site-packages/transformers/pipelines/base.py:283\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to load the model with Tensorflow.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 283\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    285\u001b[0m         model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3/lib/python3.11/site-packages/transformers/modeling_utils.py:3754\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3744\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3745\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3747\u001b[0m     (\n\u001b[1;32m   3748\u001b[0m         model,\n\u001b[1;32m   3749\u001b[0m         missing_keys,\n\u001b[1;32m   3750\u001b[0m         unexpected_keys,\n\u001b[1;32m   3751\u001b[0m         mismatched_keys,\n\u001b[1;32m   3752\u001b[0m         offload_index,\n\u001b[1;32m   3753\u001b[0m         error_msgs,\n\u001b[0;32m-> 3754\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3755\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3758\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3761\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3764\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3765\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3766\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3767\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3768\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3773\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3774\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3/lib/python3.11/site-packages/transformers/modeling_utils.py:4214\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[1;32m   4210\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   4211\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4212\u001b[0m                 )\n\u001b[1;32m   4213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4214\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4215\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4216\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4217\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4218\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4219\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4220\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4221\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4222\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4223\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4224\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4225\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4226\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4227\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4228\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4229\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4230\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4231\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3/lib/python3.11/site-packages/transformers/modeling_utils.py:889\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    887\u001b[0m     set_module_tensor_to_device(model, param_name, param_device, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mset_module_kwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_quantized_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# For quantized modules with FSDP/DeepSpeed Stage 3, we need to quantize the parameter on the GPU\u001b[39;00m\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;66;03m# and then cast it to CPU to avoid excessive memory usage on each GPU\u001b[39;00m\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;66;03m# in comparison to the sharded model across GPUs.\u001b[39;00m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_fsdp_enabled() \u001b[38;5;129;01mor\u001b[39;00m is_deepspeed_zero3_enabled():\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3/lib/python3.11/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:201\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.create_quantized_param\u001b[0;34m(self, model, param_value, param_name, target_device, state_dict, unexpected_keys)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m unexpected_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m unexpected_keys:\n\u001b[1;32m    199\u001b[0m                 unexpected_keys\u001b[38;5;241m.\u001b[39mremove(k)\n\u001b[0;32m--> 201\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParams4bit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_prequantized\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquantized_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantized_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m param_value\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama3/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:278\u001b[0m, in \u001b[0;36mParams4bit.from_prequantized\u001b[0;34m(cls, data, quantized_stats, requires_grad, device, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_prequantized\u001b[39m(\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    277\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 278\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor\u001b[38;5;241m.\u001b[39m_make_subclass(\u001b[38;5;28mcls\u001b[39m, \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m requires_grad\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_state \u001b[38;5;241m=\u001b[39m QuantState\u001b[38;5;241m.\u001b[39mfrom_dict(qs_dict\u001b[38;5;241m=\u001b[39mquantized_stats, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 23.66 GiB of which 102.06 MiB is free. Including non-PyTorch memory, this process has 22.27 GiB memory in use. Of the allocated memory 21.89 GiB is allocated by PyTorch, and 62.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mO Kernel deu pane ao executar o código na célula atual ou em uma célula anterior. \n",
      "\u001b[1;31mAnalise o código nas células para identificar uma possível causa da pane. \n",
      "\u001b[1;31mClique <a href='https://aka.ms/vscodeJupyterKernelCrash'>aqui</a> para obter mais informações. \n",
      "\u001b[1;31mConsulte Jupyter <a href='command:jupyter.viewOutput'>log</a> para obter mais detalhes."
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "# model_id = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
    "model_id = \"unsloth/llama-3-70b-Instruct-bnb-4bit\"\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\n",
    "        \"torch_dtype\": torch.float16,\n",
    "        \"quantization_config\": {\"load_in_4bit\": True},\n",
    "        \"low_cpu_mem_usage\": True,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the extracted knowledge triplets in the form of (subject, predicate, object):\n",
      "\n",
      "{\n",
      "\"triples\": [\n",
      "    {\"subject\": \"cadastro\", \"predicate\": \"gera\", \"object\": \"erro\"},\n",
      "    {\"subject\": \"erro\", \"predicate\": \"para\", \"object\": \"ser finalizado\"},\n",
      "    {\"subject\": \"thiago\", \"predicate\": \"tem\", \"object\": \"cadastro com erro\"},\n",
      "    {\"subject\": \"captura de tela\", \"predicate\": \"envia\", \"object\": \"para o atendente\"},\n",
      "    {\"subject\": \"documento\", \"predicate\": \"consta\", \"object\": \"pendência\"},\n",
      "    {\"subject\": \"selfie\", \"predicate\": \"é\", \"object\": \"requerido\"},\n",
      "    {\"subject\": \"thiago\", \"predicate\": \"envia\", \"object\": \"selfie e documento\"},\n",
      "    {\"subject\": \"análise\", \"predicate\": \"é\", \"object\": \"realizada pelo time de especialistas\"},\n",
      "    {\"subject\": \"informações\", \"predicate\": \"fornece\", \"object\": \"para o atendente\"}\n",
      "]\n",
      "}\n",
      "\n",
      "Note that I extracted entities that are relevant to the problem and solution, and avoided\n"
     ]
    }
   ],
   "source": [
    "max_knowledge_triplets=5\n",
    "text=df['transcript'].iloc[7]\n",
    "prompt_triple_extraction,user_text=get_prompts(max_knowledge_triplets,text)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": prompt_triple_extraction},\n",
    "    {\"role\": \"user\", \"content\":text},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6e2f0a13dc41a59a41d2a65903fc47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c47e7bfc614197b41d0f8426eb6627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387f8c8a5f2247c1b76bd3df65952bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca920c973a3948318b51cc61876f2437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221be4cce30e4221a84f28f484446ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0395c3886a83483da96fcbdb8b7f127a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/464 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[128001, 128009]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "terminators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 0s )speaker_00:  olá, sou  simone e estou à disposição para te atender no que você precisar!\n",
      "( 3s )speaker_00:  para registro, o protocolo do seu atendimento é 12193240.\n",
      "( 18s )speaker_00:  boa noite, thiago. tudo bem? é um prazer ter você aqui. verifiquei que informou no atendimento eletrônico abertura de conta, seria para essa questão que deseja o atendimento?\n",
      "( 29s )speaker_01:  sim\n",
      "( 1m 12s )speaker_00:  entendi!\n",
      "( 1m 20s )speaker_00:  me conta, como posso te ajudar?\n",
      "( 1m 39s )speaker_01:  meu cadastro só gera erro para ser finalizado \n",
      "( 1m 50s )speaker_00:  entendi!\n",
      "( 2m 2s )speaker_00:  por favor, me informa seu nome completo e cpf?\n",
      "( 2m 26s )speaker_01:  thiago de freitas lemos11817448730\n",
      "( 3m 27s )speaker_00:  agradeço as informações!\n",
      "( 3m 41s )speaker_00:  por favor, efetue uma captura de tela dessa mensagem de erro e me envie aqui no chat? assim consigo te orientar melhor. existe um botão para minimizar a aba do chat, desse modo, você consegue sair sem encerrá-lo e não perdemos o contato.\n",
      "( 6m 41s )speaker_00:  agradeço o envio!\n",
      "( 6m 59s )speaker_00:  aqui para nós, consta pendência de documento e selfie.\n",
      "( 7m 30s )speaker_00:  você pode me enviar por aqui? assim, já direciono ao nosso time interno afim de prosseguirmos seu processo.\n",
      "( 10m 28s )speaker_00:  perfeito! preciso, por gentileza, de uma selfie segurando seu documento enviado, tudo bem?\n",
      "( 13m 55s )speaker_00:  thiago, você está aí?\n",
      "( 14m 2s )speaker_01:  sim\n",
      "( 14m 3s )speaker_00:  perfeito!\n",
      "( 14m 41s )speaker_00:  obrigada pelo envio, thiago. estou direcionado para nosso time de especialistas para análise. assim que tivermos mais informações sobre o que ocorreu, vamos te retornar por e-mail.\n",
      "( 14m 48s )speaker_00:  peço gentilmente, que aguarde.\n",
      "( 14m 54s )speaker_00:  referente à sua solicitação, ficou alguma dúvida, lhe ajudo com mais alguma informação?\n",
      "( 15m 17s )speaker_01:  ok obrigado \n",
      "( 15m 21s )speaker_00:  certo, fico feliz em te atender.caso haja alguma dúvida, ou deseje realizar solicitações, não hesite, estamos à sua total disposição para auxiliar sempre que precisar, disponíveis 24h e 7 dias por semana.lembrando que ao final do nosso contato você receberá uma pesquisa por e-mail e aplicativo, referente ao meu atendimento poderia respondê-la? será importante para a manutenção/desenvolvimento do melhor atendimento a você.obrigada pelo contato, conte sempre com a gente!\n"
     ]
    }
   ],
   "source": [
    "print(df['transcript'].iloc[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Text: ( 0s )speaker_00:  olá, sou  simone e estou à disposição para te atender no que você precisar!\n",
      "( 3s )speaker_00:  para registro, o protocolo do seu atendimento é 12193240.\n",
      "( 18s )speaker_00:  boa noite, thiago. tudo bem? é um prazer ter você aqui. verifiquei que informou no atendimento eletrônico abertura de conta, seria para essa questão que deseja o atendimento?\n",
      "( 29s )speaker_01:  sim\n",
      "( 1m 12s )speaker_00:  entendi!\n",
      "( 1m 20s )speaker_00:  me conta, como posso te ajudar?\n",
      "( 1m 39s )speaker_01:  meu cadastro só gera erro para ser finalizado \n",
      "( 1m 50s )speaker_00:  entendi!\n",
      "( 2m 2s )speaker_00:  por favor, me informa seu nome completo e cpf?\n",
      "( 2m 26s )speaker_01:  thiago de freitas le\n"
     ]
    }
   ],
   "source": [
    "max_knowledge_triplets=5\n",
    "text=df['transcript'].iloc[7]\n",
    "prompt_triple_extraction,user_text=get_prompts(max_knowledge_triplets,text)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": prompt_triple_extraction},\n",
    "    {\"role\": \"user\", \"content\": user_text},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "result=outputs[0][\"generated_text\"][-1]\n",
    "print(result['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. (erro, gera, cadastro não finalizado)\n",
    "2. (informação, precisa, nome completo e CPF)\n",
    "3. (documento, precisa, selfie)\n",
    "4. (análise, direcionado, time de especialistas)\n",
    "5. (solicitação, direcionada, time de especialistas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_key=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  organization='',\n",
    "    api_key=api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 0s )speaker_00:  olá, sou  simone e estou à disposição para te atender no que você precisar!\n",
      "( 3s )speaker_00:  para registro, o protocolo do seu atendimento é 12193240.\n",
      "( 18s )speaker_00:  boa noite, thiago. tudo bem? é um prazer ter você aqui. verifiquei que informou no atendimento eletrônico abertura de conta, seria para essa questão que deseja o atendimento?\n",
      "( 29s )speaker_01:  sim\n",
      "( 1m 12s )speaker_00:  entendi!\n",
      "( 1m 20s )speaker_00:  me conta, como posso te ajudar?\n",
      "( 1m 39s )speaker_01:  meu cadastro só gera erro para ser finalizado \n",
      "( 1m 50s )speaker_00:  entendi!\n",
      "( 2m 2s )speaker_00:  por favor, me informa seu nome completo e cpf?\n",
      "( 2m 26s )speaker_01:  thiago de freitas lemos11817448730\n",
      "( 3m 27s )speaker_00:  agradeço as informações!\n",
      "( 3m 41s )speaker_00:  por favor, efetue uma captura de tela dessa mensagem de erro e me envie aqui no chat? assim consigo te orientar melhor. existe um botão para minimizar a aba do chat, desse modo, você consegue sair sem encerrá-lo e não perdemos o contato.\n",
      "( 6m 41s )speaker_00:  agradeço o envio!\n",
      "( 6m 59s )speaker_00:  aqui para nós, consta pendência de documento e selfie.\n",
      "( 7m 30s )speaker_00:  você pode me enviar por aqui? assim, já direciono ao nosso time interno afim de prosseguirmos seu processo.\n",
      "( 10m 28s )speaker_00:  perfeito! preciso, por gentileza, de uma selfie segurando seu documento enviado, tudo bem?\n",
      "( 13m 55s )speaker_00:  thiago, você está aí?\n",
      "( 14m 2s )speaker_01:  sim\n",
      "( 14m 3s )speaker_00:  perfeito!\n",
      "( 14m 41s )speaker_00:  obrigada pelo envio, thiago. estou direcionado para nosso time de especialistas para análise. assim que tivermos mais informações sobre o que ocorreu, vamos te retornar por e-mail.\n",
      "( 14m 48s )speaker_00:  peço gentilmente, que aguarde.\n",
      "( 14m 54s )speaker_00:  referente à sua solicitação, ficou alguma dúvida, lhe ajudo com mais alguma informação?\n",
      "( 15m 17s )speaker_01:  ok obrigado \n",
      "( 15m 21s )speaker_00:  certo, fico feliz em te atender.caso haja alguma dúvida, ou deseje realizar solicitações, não hesite, estamos à sua total disposição para auxiliar sempre que precisar, disponíveis 24h e 7 dias por semana.lembrando que ao final do nosso contato você receberá uma pesquisa por e-mail e aplicativo, referente ao meu atendimento poderia respondê-la? será importante para a manutenção/desenvolvimento do melhor atendimento a você.obrigada pelo contato, conte sempre com a gente!\n"
     ]
    }
   ],
   "source": [
    "max_knowledge_triplets=5\n",
    "text=df['transcript'].iloc[7]\n",
    "prompt_triple_extraction,user_text=get_prompts(max_knowledge_triplets,text)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": prompt_triple_extraction},\n",
    "    {\"role\": \"user\", \"content\": user_text},\n",
    "]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(cliente, tem problema, cadastro)\n",
      "\n",
      "(cadastro, gera, erro)\n",
      "\n",
      "(erro, é informado por, cliente)\n",
      "\n",
      "(cliente, envia, documento e selfie)\n",
      "\n",
      "(solução, é iniciada por, envio de documento e selfie)"
     ]
    }
   ],
   "source": [
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    messages=messages,\n",
    "    stream=True,\n",
    ")\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-pro\n",
      "models/gemini-pro-vision\n"
     ]
    }
   ],
   "source": [
    "for m in genai.list_models():\n",
    "  if 'generateContent' in m.supported_generation_methods:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-1.5-pro-latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 0s )speaker_00:  olá, sou  simone e estou à disposição para te atender no que você precisar!\n",
      "( 3s )speaker_00:  para registro, o protocolo do seu atendimento é 12193240.\n",
      "( 18s )speaker_00:  boa noite, thiago. tudo bem? é um prazer ter você aqui. verifiquei que informou no atendimento eletrônico abertura de conta, seria para essa questão que deseja o atendimento?\n",
      "( 29s )speaker_01:  sim\n",
      "( 1m 12s )speaker_00:  entendi!\n",
      "( 1m 20s )speaker_00:  me conta, como posso te ajudar?\n",
      "( 1m 39s )speaker_01:  meu cadastro só gera erro para ser finalizado \n",
      "( 1m 50s )speaker_00:  entendi!\n",
      "( 2m 2s )speaker_00:  por favor, me informa seu nome completo e cpf?\n",
      "( 2m 26s )speaker_01:  thiago de freitas lemos11817448730\n",
      "( 3m 27s )speaker_00:  agradeço as informações!\n",
      "( 3m 41s )speaker_00:  por favor, efetue uma captura de tela dessa mensagem de erro e me envie aqui no chat? assim consigo te orientar melhor. existe um botão para minimizar a aba do chat, desse modo, você consegue sair sem encerrá-lo e não perdemos o contato.\n",
      "( 6m 41s )speaker_00:  agradeço o envio!\n",
      "( 6m 59s )speaker_00:  aqui para nós, consta pendência de documento e selfie.\n",
      "( 7m 30s )speaker_00:  você pode me enviar por aqui? assim, já direciono ao nosso time interno afim de prosseguirmos seu processo.\n",
      "( 10m 28s )speaker_00:  perfeito! preciso, por gentileza, de uma selfie segurando seu documento enviado, tudo bem?\n",
      "( 13m 55s )speaker_00:  thiago, você está aí?\n",
      "( 14m 2s )speaker_01:  sim\n",
      "( 14m 3s )speaker_00:  perfeito!\n",
      "( 14m 41s )speaker_00:  obrigada pelo envio, thiago. estou direcionado para nosso time de especialistas para análise. assim que tivermos mais informações sobre o que ocorreu, vamos te retornar por e-mail.\n",
      "( 14m 48s )speaker_00:  peço gentilmente, que aguarde.\n",
      "( 14m 54s )speaker_00:  referente à sua solicitação, ficou alguma dúvida, lhe ajudo com mais alguma informação?\n",
      "( 15m 17s )speaker_01:  ok obrigado \n",
      "( 15m 21s )speaker_00:  certo, fico feliz em te atender.caso haja alguma dúvida, ou deseje realizar solicitações, não hesite, estamos à sua total disposição para auxiliar sempre que precisar, disponíveis 24h e 7 dias por semana.lembrando que ao final do nosso contato você receberá uma pesquisa por e-mail e aplicativo, referente ao meu atendimento poderia respondê-la? será importante para a manutenção/desenvolvimento do melhor atendimento a você.obrigada pelo contato, conte sempre com a gente!\n"
     ]
    }
   ],
   "source": [
    "text=df['transcript'].iloc[7]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> Here are some knowledge triplets extracted from the text:\n",
       "> \n",
       "> 1. **(cliente, tem problema, cadastro)** \n",
       ">    * O cliente relata que está com problemas para finalizar seu cadastro.\n",
       "> 2. **(cadastro, apresenta, erro)**\n",
       ">    * O problema do cadastro se manifesta por meio de uma mensagem de erro. \n",
       "> 3. **(erro, impede, finalização)**\n",
       ">    * A mensagem de erro impede que o cliente finalize o cadastro.\n",
       "> 4. **(atendente, solicita, documentos)**\n",
       ">    * Para solucionar o problema, a atendente solicita documentos do cliente:  captura de tela do erro, documento de identificação e selfie com o documento.\n",
       "> 5. **(documentos, serão analisados por, time interno)** \n",
       ">    * Os documentos enviados pelo cliente serão analisados pelo time interno do banco.\n",
       "> 6. **(solução, será enviada por,  e-mail)**\n",
       ">    * A solução para o problema será enviada ao cliente por e-mail após análise. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_knowledge_triplets=6\n",
    "\n",
    "prompt_triple_extraction,user_text=get_prompts(max_knowledge_triplets,text)\n",
    "text=f\"{prompt_triple_extraction} \\n {user_text}\"\n",
    "response = model.generate_content(text)\n",
    "to_markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader, UnstructuredPDFLoader, PyPDFium2Loader\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "import random\n",
    "from helpers.df_helpers import documents2Dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks =  23\n",
      "An extensive literature search was performed, and 56 articles published in peer-reviewed journals between 2005 and 2021 were selected and analyzed. The corresponding authors' experiential knowledge served as the foundation for the analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Input data directory\n",
    "data_dir = \"cureus\"\n",
    "inputdirectory = Path(f\"./data_input/{data_dir}\")\n",
    "loader = DirectoryLoader(inputdirectory, show_progress=True)\n",
    "documents = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "pages = splitter.split_documents(documents)\n",
    "print(\"Number of chunks = \", len(pages))\n",
    "print(pages[3].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>chunk_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abstract India’s health indicators have improv...</td>\n",
       "      <td>data_input/cureus/cureus-0015-00000040274.txt</td>\n",
       "      <td>185959646c7a47c5bc932429131ed527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Categories: Public Health, Epidemiology/Public...</td>\n",
       "      <td>data_input/cureus/cureus-0015-00000040274.txt</td>\n",
       "      <td>67f4e9e125dd4bdc9ddef780a8371117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Introduction And Background India’s health ind...</td>\n",
       "      <td>data_input/cureus/cureus-0015-00000040274.txt</td>\n",
       "      <td>8c4d380f9eb542b1a20a14cf76a1dab8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An extensive literature search was performed, ...</td>\n",
       "      <td>data_input/cureus/cureus-0015-00000040274.txt</td>\n",
       "      <td>dadf1c8e366d447d880a9eae422f67a2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Review Overview of the public and private heal...</td>\n",
       "      <td>data_input/cureus/cureus-0015-00000040274.txt</td>\n",
       "      <td>8498ba38ab4e40b19873d8000b34536a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Abstract India’s health indicators have improv...   \n",
       "1  Categories: Public Health, Epidemiology/Public...   \n",
       "2  Introduction And Background India’s health ind...   \n",
       "3  An extensive literature search was performed, ...   \n",
       "4  Review Overview of the public and private heal...   \n",
       "\n",
       "                                          source  \\\n",
       "0  data_input/cureus/cureus-0015-00000040274.txt   \n",
       "1  data_input/cureus/cureus-0015-00000040274.txt   \n",
       "2  data_input/cureus/cureus-0015-00000040274.txt   \n",
       "3  data_input/cureus/cureus-0015-00000040274.txt   \n",
       "4  data_input/cureus/cureus-0015-00000040274.txt   \n",
       "\n",
       "                           chunk_id  \n",
       "0  185959646c7a47c5bc932429131ed527  \n",
       "1  67f4e9e125dd4bdc9ddef780a8371117  \n",
       "2  8c4d380f9eb542b1a20a14cf76a1dab8  \n",
       "3  dadf1c8e366d447d880a9eae422f67a2  \n",
       "4  8498ba38ab4e40b19873d8000b34536a  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = documents2Dataframe(pages)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "resumos = pickle.load(open(\"../resumos_chats_fevereiro.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44386</td>\n",
       "      <td>- O cliente entrou em contato com o atendimen...</td>\n",
       "      <td>chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44386</td>\n",
       "      <td>- A cliente entrou em contato com o atendimen...</td>\n",
       "      <td>chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44386</td>\n",
       "      <td>- O cliente entrou em contato com o atendimen...</td>\n",
       "      <td>chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44386</td>\n",
       "      <td>- A atendente se identificou como Iracema e i...</td>\n",
       "      <td>chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44386</td>\n",
       "      <td>- O cliente tentou fazer um Pix de R$30,00, m...</td>\n",
       "      <td>chat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chunk_id                                               text source\n",
       "0    44386   - O cliente entrou em contato com o atendimen...   chat\n",
       "1    44386   - A cliente entrou em contato com o atendimen...   chat\n",
       "2    44386   - O cliente entrou em contato com o atendimen...   chat\n",
       "3    44386   - A atendente se identificou como Iracema e i...   chat\n",
       "4    44386   - O cliente tentou fazer um Pix de R$30,00, m...   chat"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "df=pd.read_csv('../chats_fevereiro.csv')\n",
    "text=df['transcript'].values\n",
    "text=resumos\n",
    "chunk_id=[str(random.randint(0,1000000))]*len(text)\n",
    "source=['chat']*len(text)\n",
    "df={'chunk_id':chunk_id,'text':text,'source':source}\n",
    "df=pd.DataFrame(df)\n",
    "df=df.iloc[:10]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function uses the helpers/prompt function to extract concepts from text\n",
    "from helpers.df_helpers import df2Graph\n",
    "from helpers.df_helpers import graph2Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_1</th>\n",
       "      <th>node_2</th>\n",
       "      <th>edge</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>análise</td>\n",
       "      <td>conta</td>\n",
       "      <td>está</td>\n",
       "      <td>44386</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atendimento ao cliente</td>\n",
       "      <td>cliente</td>\n",
       "      <td>entrou em contato com</td>\n",
       "      <td>44386</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>atendente</td>\n",
       "      <td>mensagem de análise</td>\n",
       "      <td>solicitou um print da</td>\n",
       "      <td>44386</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>solicitação interna</td>\n",
       "      <td>análise</td>\n",
       "      <td>abrir uma</td>\n",
       "      <td>44386</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e-mail</td>\n",
       "      <td>cliente</td>\n",
       "      <td>informar o sobre a liberação da conta por</td>\n",
       "      <td>44386</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   node_1               node_2  \\\n",
       "0                 análise                conta   \n",
       "1  atendimento ao cliente              cliente   \n",
       "2               atendente  mensagem de análise   \n",
       "3     solicitação interna              análise   \n",
       "4                  e-mail              cliente   \n",
       "\n",
       "                                        edge chunk_id  count  \n",
       "0                                       está    44386      4  \n",
       "1                      entrou em contato com    44386      4  \n",
       "2                      solicitou um print da    44386      4  \n",
       "3                                  abrir uma    44386      4  \n",
       "4  informar o sobre a liberação da conta por    44386      4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To regenerate the graph with LLM, set this to True\n",
    "regenerate = True\n",
    "outputdirectory=Path(f\"./data_output\")\n",
    "if regenerate:\n",
    "    concepts_list = df2Graph(df, model='llama3')\n",
    "    dfg1 = graph2Df(concepts_list)\n",
    "    if not os.path.exists(outputdirectory):\n",
    "        os.makedirs(outputdirectory)\n",
    "    \n",
    "    dfg1.to_csv(outputdirectory/\"graph.csv\", sep=\"|\", index=False)\n",
    "    df.to_csv(outputdirectory/\"chunks.csv\", sep=\"|\", index=False)\n",
    "else:\n",
    "    dfg1 = pd.read_csv(outputdirectory/\"graph.csv\", sep=\"|\")\n",
    "\n",
    "dfg1.replace(\"\", np.nan, inplace=True)\n",
    "dfg1.dropna(subset=[\"node_1\", \"node_2\", 'edge'], inplace=True)\n",
    "dfg1['count'] = 4 \n",
    "## Increasing the weight of the relation to 4. \n",
    "## We will assign the weight of 1 when later the contextual proximity will be calculated.  \n",
    "print(dfg1.shape)\n",
    "dfg1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_1</th>\n",
       "      <th>node_2</th>\n",
       "      <th>edge</th>\n",
       "      <th>count</th>\n",
       "      <th>chunk_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cliente</td>\n",
       "      <td>conta</td>\n",
       "      <td>tem problema</td>\n",
       "      <td>4</td>\n",
       "      <td>366324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>conta</td>\n",
       "      <td>em análise</td>\n",
       "      <td>esta</td>\n",
       "      <td>4</td>\n",
       "      <td>540512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>atendente</td>\n",
       "      <td>solicitação interna</td>\n",
       "      <td>irá abrir</td>\n",
       "      <td>4</td>\n",
       "      <td>824164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>atendente</td>\n",
       "      <td>e-mail</td>\n",
       "      <td>irá dar retorno</td>\n",
       "      <td>4</td>\n",
       "      <td>703686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cliente</td>\n",
       "      <td>fatura</td>\n",
       "      <td>tem problema</td>\n",
       "      <td>4</td>\n",
       "      <td>853622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      node_1               node_2             edge  count chunk_id\n",
       "0    cliente                conta     tem problema      4   366324\n",
       "1      conta           em análise             esta      4   540512\n",
       "2  atendente  solicitação interna        irá abrir      4   824164\n",
       "3  atendente               e-mail  irá dar retorno      4   703686\n",
       "4    cliente               fatura     tem problema      4   853622"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import json\n",
    "# import random\n",
    "\n",
    "# import pickle\n",
    "# triples=pickle.load(open('../triples.pkl','rb'))\n",
    "# print(len(triples))\n",
    "\n",
    "# dfg1={'node_1':[],'node_2':[],'edge':[],'count':[],'chunk_id':[]}\n",
    "# for tri in triples[:100]:\n",
    "#     try:\n",
    "#         d=json.loads(tri[1])['triples']\n",
    "#         for t in d:\n",
    "#             if ('subject' not in t) or ('object' not in t) or ('relation' not in t):\n",
    "#                 continue\n",
    "#             dfg1['node_1'].append(t['subject'])\n",
    "#             dfg1['node_2'].append(t['object'])\n",
    "#             dfg1['edge'].append(t['relation'])\n",
    "#             dfg1['chunk_id'].append(str(random.randint(0,1000000)))\n",
    "#             dfg1['count'].append(4)\n",
    "#     except KeyboardInterrupt:\n",
    "#         break\n",
    "#     except:\n",
    "#         continue\n",
    "    \n",
    "# dfg1=pd.DataFrame(dfg1)\n",
    "# dfg1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfg1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_1</th>\n",
       "      <th>node_2</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>count</th>\n",
       "      <th>edge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3645</th>\n",
       "      <td>verificação</td>\n",
       "      <td>pagamento</td>\n",
       "      <td>44386,44386</td>\n",
       "      <td>2</td>\n",
       "      <td>contextual proximity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3648</th>\n",
       "      <td>verificação</td>\n",
       "      <td>pix</td>\n",
       "      <td>44386,44386</td>\n",
       "      <td>2</td>\n",
       "      <td>contextual proximity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3652</th>\n",
       "      <td>verificação</td>\n",
       "      <td>senha</td>\n",
       "      <td>44386,44386,44386</td>\n",
       "      <td>3</td>\n",
       "      <td>contextual proximity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3656</th>\n",
       "      <td>verificação</td>\n",
       "      <td>transação</td>\n",
       "      <td>44386,44386</td>\n",
       "      <td>2</td>\n",
       "      <td>contextual proximity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3658</th>\n",
       "      <td>verificação</td>\n",
       "      <td>vencimento</td>\n",
       "      <td>44386,44386</td>\n",
       "      <td>2</td>\n",
       "      <td>contextual proximity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           node_1      node_2           chunk_id  count                  edge\n",
       "3645  verificação   pagamento        44386,44386      2  contextual proximity\n",
       "3648  verificação         pix        44386,44386      2  contextual proximity\n",
       "3652  verificação       senha  44386,44386,44386      3  contextual proximity\n",
       "3656  verificação   transação        44386,44386      2  contextual proximity\n",
       "3658  verificação  vencimento        44386,44386      2  contextual proximity"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def contextual_proximity(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ## Melt the dataframe into a list of nodes\n",
    "    dfg_long = pd.melt(\n",
    "        df, id_vars=[\"chunk_id\"], value_vars=[\"node_1\", \"node_2\"], value_name=\"node\"\n",
    "    )\n",
    "    dfg_long.drop(columns=[\"variable\"], inplace=True)\n",
    "    # Self join with chunk id as the key will create a link between terms occuring in the same text chunk.\n",
    "    dfg_wide = pd.merge(dfg_long, dfg_long, on=\"chunk_id\", suffixes=(\"_1\", \"_2\"))\n",
    "    # drop self loops\n",
    "    self_loops_drop = dfg_wide[dfg_wide[\"node_1\"] == dfg_wide[\"node_2\"]].index\n",
    "    dfg2 = dfg_wide.drop(index=self_loops_drop).reset_index(drop=True)\n",
    "    ## Group and count edges.\n",
    "    dfg2 = (\n",
    "        dfg2.groupby([\"node_1\", \"node_2\"])\n",
    "        .agg({\"chunk_id\": [\",\".join, \"count\"]})\n",
    "        .reset_index()\n",
    "    )\n",
    "    dfg2.columns = [\"node_1\", \"node_2\", \"chunk_id\", \"count\"]\n",
    "    dfg2.replace(\"\", np.nan, inplace=True)\n",
    "    dfg2.dropna(subset=[\"node_1\", \"node_2\"], inplace=True)\n",
    "    # Drop edges with 1 count\n",
    "    dfg2 = dfg2[dfg2[\"count\"] != 1]\n",
    "    dfg2[\"edge\"] = \"contextual proximity\"\n",
    "    return dfg2\n",
    "\n",
    "\n",
    "dfg2 = contextual_proximity(dfg1)\n",
    "dfg2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_1</th>\n",
       "      <th>node_2</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>edge</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11/12</td>\n",
       "      <td>análise</td>\n",
       "      <td>44386,44386,44386</td>\n",
       "      <td>contextual proximity</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11/12</td>\n",
       "      <td>aplicativo</td>\n",
       "      <td>44386,44386</td>\n",
       "      <td>contextual proximity</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11/12</td>\n",
       "      <td>aplicativo do banco</td>\n",
       "      <td>44386,44386</td>\n",
       "      <td>contextual proximity</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11/12</td>\n",
       "      <td>atendente</td>\n",
       "      <td>44386,44386,44386,44386,44386,44386,44386,4438...</td>\n",
       "      <td>contextual proximity</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11/12</td>\n",
       "      <td>atendimento ao cliente</td>\n",
       "      <td>44386,44386,44386,44386,44386,44386,44386,4438...</td>\n",
       "      <td>contextual proximity</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2256</th>\n",
       "      <td>verificação</td>\n",
       "      <td>pagamento</td>\n",
       "      <td>44386,44386</td>\n",
       "      <td>contextual proximity</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2257</th>\n",
       "      <td>verificação</td>\n",
       "      <td>pix</td>\n",
       "      <td>44386,44386</td>\n",
       "      <td>contextual proximity</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2258</th>\n",
       "      <td>verificação</td>\n",
       "      <td>senha</td>\n",
       "      <td>44386,44386,44386</td>\n",
       "      <td>contextual proximity</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>verificação</td>\n",
       "      <td>transação</td>\n",
       "      <td>44386,44386</td>\n",
       "      <td>contextual proximity</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>verificação</td>\n",
       "      <td>vencimento</td>\n",
       "      <td>44386,44386</td>\n",
       "      <td>contextual proximity</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2261 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           node_1                  node_2  \\\n",
       "0           11/12                 análise   \n",
       "1           11/12              aplicativo   \n",
       "2           11/12     aplicativo do banco   \n",
       "3           11/12               atendente   \n",
       "4           11/12  atendimento ao cliente   \n",
       "...           ...                     ...   \n",
       "2256  verificação               pagamento   \n",
       "2257  verificação                     pix   \n",
       "2258  verificação                   senha   \n",
       "2259  verificação               transação   \n",
       "2260  verificação              vencimento   \n",
       "\n",
       "                                               chunk_id                  edge  \\\n",
       "0                                     44386,44386,44386  contextual proximity   \n",
       "1                                           44386,44386  contextual proximity   \n",
       "2                                           44386,44386  contextual proximity   \n",
       "3     44386,44386,44386,44386,44386,44386,44386,4438...  contextual proximity   \n",
       "4     44386,44386,44386,44386,44386,44386,44386,4438...  contextual proximity   \n",
       "...                                                 ...                   ...   \n",
       "2256                                        44386,44386  contextual proximity   \n",
       "2257                                        44386,44386  contextual proximity   \n",
       "2258                                  44386,44386,44386  contextual proximity   \n",
       "2259                                        44386,44386  contextual proximity   \n",
       "2260                                        44386,44386  contextual proximity   \n",
       "\n",
       "      count  \n",
       "0         3  \n",
       "1         2  \n",
       "2         2  \n",
       "3        14  \n",
       "4        12  \n",
       "...     ...  \n",
       "2256      2  \n",
       "2257      2  \n",
       "2258      3  \n",
       "2259      2  \n",
       "2260      2  \n",
       "\n",
       "[2261 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfg = pd.concat([dfg1, dfg2], axis=0)\n",
    "dfg = (\n",
    "    dfg.groupby([\"node_1\", \"node_2\"])\n",
    "    .agg({\"chunk_id\": \",\".join, \"edge\": ','.join, 'count': 'sum'})\n",
    "    .reset_index()\n",
    ")\n",
    "dfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the NetworkX Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes = pd.concat([dfg['node_1'], dfg['node_2']], axis=0).unique()\n",
    "nodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "\n",
    "## Add nodes to the graph\n",
    "for node in nodes:\n",
    "    G.add_node(\n",
    "        str(node)\n",
    "    )\n",
    "\n",
    "## Add edges to the graph\n",
    "for index, row in dfg.iterrows():\n",
    "    G.add_edge(\n",
    "        str(row[\"node_1\"]),\n",
    "        str(row[\"node_2\"]),\n",
    "        title=row[\"edge\"],\n",
    "        weight=row['count']/4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Communities =  3\n"
     ]
    }
   ],
   "source": [
    "communities_generator = nx.community.girvan_newman(G)\n",
    "top_level_communities = next(communities_generator)\n",
    "next_level_communities = next(communities_generator)\n",
    "communities = sorted(map(sorted, next_level_communities))\n",
    "print(\"Number of Communities = \", len(communities))\n",
    "# print(communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node</th>\n",
       "      <th>color</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11/12</td>\n",
       "      <td>#5f57db</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13/02</td>\n",
       "      <td>#57db5f</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14/02</td>\n",
       "      <td>#db5f57</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14/11</td>\n",
       "      <td>#db5f57</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>agendamento do pagamento</td>\n",
       "      <td>#db5f57</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>transação</td>\n",
       "      <td>#db5f57</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>transferência</td>\n",
       "      <td>#db5f57</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>vencimento</td>\n",
       "      <td>#db5f57</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>verificar</td>\n",
       "      <td>#db5f57</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>verificação</td>\n",
       "      <td>#db5f57</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        node    color  group\n",
       "0                      11/12  #5f57db      1\n",
       "1                      13/02  #57db5f      2\n",
       "2                      14/02  #db5f57      3\n",
       "3                      14/11  #db5f57      3\n",
       "4   agendamento do pagamento  #db5f57      3\n",
       "..                       ...      ...    ...\n",
       "56                 transação  #db5f57      3\n",
       "57             transferência  #db5f57      3\n",
       "58                vencimento  #db5f57      3\n",
       "59                 verificar  #db5f57      3\n",
       "60               verificação  #db5f57      3\n",
       "\n",
       "[61 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "palette = \"hls\"\n",
    "\n",
    "## Now add these colors to communities and make another dataframe\n",
    "def colors2Community(communities) -> pd.DataFrame:\n",
    "    ## Define a color palette\n",
    "    p = sns.color_palette(palette, len(communities)).as_hex()\n",
    "    random.shuffle(p)\n",
    "    rows = []\n",
    "    group = 0\n",
    "    for community in communities:\n",
    "        color = p.pop()\n",
    "        group += 1\n",
    "        for node in community:\n",
    "            rows += [{\"node\": node, \"color\": color, \"group\": group}]\n",
    "    df_colors = pd.DataFrame(rows)\n",
    "    return df_colors\n",
    "\n",
    "\n",
    "colors = colors2Community(communities)\n",
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in colors.iterrows():\n",
    "    G.nodes[row['node']]['group'] = row['group']\n",
    "    G.nodes[row['node']]['color'] = row['color']\n",
    "    G.nodes[row['node']]['size'] = G.degree[row['node']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kiki/Projects/BTG/graph/data_output/index.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GFX1-]: More than 2 GPUs detected via PCI, secondary GPU is arbitrary\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "graph_output_directory = \"/home/kiki/Projects/BTG/graph/data_output/index.html\"\n",
    "\n",
    "net = Network(\n",
    "    notebook=False,\n",
    "    # bgcolor=\"#1a1a1a\",\n",
    "    cdn_resources=\"remote\",\n",
    "    height=\"900px\",\n",
    "    width=\"100%\",\n",
    "    select_menu=True,\n",
    "    # font_color=\"#cccccc\",\n",
    "    filter_menu=False,\n",
    ")\n",
    "\n",
    "net.from_nx(G)\n",
    "# net.repulsion(node_distance=150, spring_length=400)\n",
    "net.force_atlas_2based(central_gravity=0.015, gravity=-31)\n",
    "# net.barnes_hut(gravity=-18100, central_gravity=5.05, spring_length=380)\n",
    "net.show_buttons(filter_=[\"physics\"])\n",
    "\n",
    "net.show(graph_output_directory, notebook=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch22",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
