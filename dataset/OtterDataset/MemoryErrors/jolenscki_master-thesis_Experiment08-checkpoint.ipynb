{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12b1a78a-7bec-46ea-84f3-26782ab09084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA']   = \"1\"\n",
    "from os import path as osp\n",
    "# if 'jupyter' in os.getcwd():\n",
    "#     os.chdir(osp.join(os.getcwd(), 'masterarbeit', 'code'))\n",
    "import glob\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdmnotebook\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "from typing import Callable\n",
    "from typing import Optional\n",
    "from typing import Tuple\n",
    "from typing import Union\n",
    "from typing import Dict\n",
    "from typing import List\n",
    "from itertools import cycle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "mpl.rc('axes', unicode_minus=False)\n",
    "preamble = r'\\usepackage{amsmath}'  # LaTeX preamble command\n",
    "mpl.rcParams['text.latex.preamble'] = preamble\n",
    "\n",
    "# import seaborn as sns\n",
    "import networkx as nx\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import gc\n",
    "\n",
    "# pytorch imports\n",
    "import torch\n",
    "from torch import Tensor, nn, cuda\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# pytorch geometric imports\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.transforms import Compose\n",
    "\n",
    "# lightning imports\n",
    "from lightning.pytorch.utilities.combined_loader import CombinedLoader\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "import sys\n",
    "# Add the 'code' directory to sys.path to make the  submodules available\n",
    "# sys.path.append('/home/jupyter/masterarbeit/code')\n",
    "\n",
    "from util.utils import generate_log_name\n",
    "from util.plot_utils import *\n",
    "\n",
    "import logging as log\n",
    "\n",
    "from data.dataset.GraphDataset import GraphDataset\n",
    "\n",
    "from model.transform import CollapseChannels, ExtractSquare\n",
    "from model.autoencoder import Autoencoder\n",
    "from model.predictor import Predictor\n",
    "from model.DAN import GradientReversalLayer, DomainDiscriminator\n",
    "\n",
    "from model.criterions import WeightedMSELoss, MSLELoss, FocalLoss, ZeroInflatedLoss, CustomHuberLoss\n",
    "\n",
    "from fullmodel import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83312120-e8ef-4639-a2b5-91cbe8c8c79a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_id='exp08'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3706c5f2-674c-45d5-be60-93b32640da05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_WORKERS: int = 0\n",
    "BATCH_SIZE: int = 8\n",
    "NUM_CHANNELS: int = 2\n",
    "WDW_LENGTH: list = [12, 6]\n",
    "\n",
    "# Constants that I may change a bit during testing\n",
    "tgt: str = 'MELBOURNE'\n",
    "src_list: list = ['ANTWERP']\n",
    "# src_list: list = ['ANTWERP', 'BANGKOK', 'BARCELONA', 'BERLIN', 'CHICAGO', 'ISTANBUL', 'MOSCOW'] # 7 cities\n",
    "EPOCHS: int = 2\n",
    "tgt_data_limit: int = 1680\n",
    "src_data_limit: int = None\n",
    "LOGGING: int = 1\n",
    "\n",
    "if src_data_limit == -1:\n",
    "    src_data_limit = None\n",
    "\n",
    "# Get data from bucket\n",
    "bucket_name = 'cloud-ai-platform-054ad037-69b6-4c4d-94a1-75d2591213c7'\n",
    "bucket_folder = 'data/graphs'\n",
    "local_folder  = 'data/graphs'\n",
    "download_directory(bucket_name, bucket_folder, local_folder)\n",
    "bucket_folder = 'data/raw'\n",
    "local_folder  = 'data/raw'\n",
    "download_directory(bucket_name, bucket_folder, local_folder)\n",
    "\n",
    "bucket_output = 'output/models/'\n",
    "\n",
    "# Constants that I don't intend to change much\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "TRAIN_VAL_TEST_SPLIT = [0.8, 0.1, 0.1]\n",
    "\n",
    "pre_transform = Compose([\n",
    "    CollapseChannels(),\n",
    "    ExtractSquare(50, 'central'),\n",
    "])\n",
    "\n",
    "static_transform = Compose([\n",
    "    ExtractSquare(50, 'central'),\n",
    "])\n",
    "\n",
    "ds_kwargs = {\n",
    "    'root_dir': 'data/raw',\n",
    "    'device': device,\n",
    "    'pre_transform': pre_transform,\n",
    "    'static_transform': static_transform,\n",
    "}\n",
    "\n",
    "# seed generator for DataLoader\n",
    "torch.manual_seed(2311)\n",
    "\n",
    "# Create datasets for each city\n",
    "ds_dict = {}\n",
    "for city in src_list:\n",
    "    ds_dict[city] = GraphDataset(\n",
    "        cities=[city],\n",
    "        limit=src_data_limit,\n",
    "        **ds_kwargs,\n",
    "    )\n",
    "    \n",
    "temp_tgt = GraphDataset(\n",
    "    cities=[tgt],\n",
    "    limit=None,\n",
    "    **ds_kwargs,\n",
    ")\n",
    "\n",
    "train_tgt, val_tgt, test_tgt = random_split(\n",
    "    temp_tgt, [0.04, 0.04, 0.92]\n",
    ")\n",
    "\n",
    "ds_dict[tgt] = train_tgt\n",
    "\n",
    "test_tgt = DataLoader(test_tgt, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "# Split each dataset into training and test sets\n",
    "train = {}\n",
    "val   = {}\n",
    "test  = {}\n",
    "for city in ds_dict:\n",
    "    train_ds, val_ds, test_ds = random_split(\n",
    "        ds_dict[city], TRAIN_VAL_TEST_SPLIT\n",
    "    )\n",
    "    train[city] = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  drop_last=True)\n",
    "    val[city]   = DataLoader(  val_ds, batch_size=BATCH_SIZE, shuffle=True,  drop_last=True)\n",
    "    test[city]  = DataLoader( test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "# Create dataloader for offline training with source cities\n",
    "source_train = {city: train[city] for city in src_list}\n",
    "source_dataloader = CombinedLoader(source_train, mode='max_size_cycle')\n",
    "\n",
    "source_test = {city: test[city] for city in src_list}\n",
    "sourcetest_dataloader = CombinedLoader(source_test, mode='max_size_cycle')\n",
    "\n",
    "target_dataloader = CombinedLoader({tgt: train[tgt]}, mode='max_size_cycle')\n",
    "targettest_dataloader = CombinedLoader({tgt: test_tgt}, mode='max_size_cycle')\n",
    "\n",
    "# Create dataloader for online training with source and target cities\n",
    "train_dataloader = CombinedLoader(train, mode='max_size_cycle')\n",
    "\n",
    "# Create dataloader for validation with source and target cities\n",
    "val_dataloader = CombinedLoader(val, mode='max_size_cycle')\n",
    "\n",
    "# Create dataloader for testing with source and target cities\n",
    "test_dataloader = CombinedLoader(test, mode='max_size_cycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eca4083-1f0b-4d20-a86d-8c9d13a17c65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2285e516b01d4747a77f37fd44a582ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/2 [00:00<?,?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tqdm.notebook.tqdm_notebook at 0x7f25d5658460>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4f61b2a359402887135904067cf4c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2161 [00:00<?,?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tqdm.notebook.tqdm_notebook at 0x7f25d564e200>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0\n",
      "    Loss: 32337.6992\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 14.56 GiB total capacity; 1.90 GiB already allocated; 5.50 MiB free; 1.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 79\u001b[0m\n\u001b[1;32m     62\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(\n\u001b[1;32m     63\u001b[0m     AE_parameters\u001b[38;5;241m=\u001b[39mAE_parameters,\n\u001b[1;32m     64\u001b[0m     DD_parameters\u001b[38;5;241m=\u001b[39mDD_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m     specs\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlinear_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     75\u001b[0m     tgt\u001b[38;5;241m=\u001b[39mtgt)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     77\u001b[0m model\u001b[38;5;241m.\u001b[39mload_module(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoencoder\u001b[39m\u001b[38;5;124m'\u001b[39m, folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mae_.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred_train\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpretrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m model\u001b[38;5;241m.\u001b[39mpred_train(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinetune\u001b[39m\u001b[38;5;124m'\u001b[39m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     82\u001b[0m test_errors[linear_dim] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpred_test()\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/masterarbeit/code/fullmodel.py:226\u001b[0m, in \u001b[0;36mModel.pred_train\u001b[0;34m(self, mode, save)\u001b[0m\n\u001b[1;32m    224\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(data\u001b[38;5;241m.\u001b[39mptr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m--> 226\u001b[0m     H \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m     H \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfusion_features(H, date_features, batch_size)\n\u001b[1;32m    228\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred(H, edge_index, batch)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/masterarbeit/code/model/autoencoder.py:87\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, edge_index, h, c)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m     86\u001b[0m     x_t \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, t]\u001b[38;5;241m.\u001b[39mreshape(num_nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_channels)\n\u001b[0;32m---> 87\u001b[0m     h, c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     h_t, c_t \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mdetach(), c\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     89\u001b[0m     H\u001b[38;5;241m.\u001b[39mappend(h_t)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch_geometric_temporal/nn/recurrent/gconv_lstm.py:233\u001b[0m, in \u001b[0;36mGConvLSTM.forward\u001b[0;34m(self, X, edge_index, edge_weight, H, C, lambda_max)\u001b[0m\n\u001b[1;32m    231\u001b[0m I \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_input_gate(X, edge_index, edge_weight, H, C, lambda_max)\n\u001b[1;32m    232\u001b[0m F \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_forget_gate(X, edge_index, edge_weight, H, C, lambda_max)\n\u001b[0;32m--> 233\u001b[0m C \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calculate_cell_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m O \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_output_gate(X, edge_index, edge_weight, H, C, lambda_max)\n\u001b[1;32m    235\u001b[0m H \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_hidden_state(O, C)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch_geometric_temporal/nn/recurrent/gconv_lstm.py:184\u001b[0m, in \u001b[0;36mGConvLSTM._calculate_cell_state\u001b[0;34m(self, X, edge_index, edge_weight, H, C, I, F, lambda_max)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_calculate_cell_state\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, edge_index, edge_weight, H, C, I, F, lambda_max):\n\u001b[1;32m    183\u001b[0m     T \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_x_c(X, edge_index, edge_weight, lambda_max\u001b[38;5;241m=\u001b[39mlambda_max)\n\u001b[0;32m--> 184\u001b[0m     T \u001b[38;5;241m=\u001b[39m T \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_h_c\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_max\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m     T \u001b[38;5;241m=\u001b[39m T \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_c\n\u001b[1;32m    186\u001b[0m     T \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(T)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch_geometric/nn/conv/cheb_conv.py:167\u001b[0m, in \u001b[0;36mChebConv.forward\u001b[0;34m(self, x, edge_index, edge_weight, batch, lambda_max)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: Tensor, norm: Tensor)\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlins) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 167\u001b[0m     Tx_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlins[\u001b[38;5;241m1\u001b[39m](Tx_1)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lin \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlins[\u001b[38;5;241m2\u001b[39m:]:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:463\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m         msg_kwargs \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m res\n\u001b[0;32m--> 463\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmsg_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_forward_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    465\u001b[0m     res \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, (msg_kwargs, ), out)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch_geometric/nn/conv/cheb_conv.py:182\u001b[0m, in \u001b[0;36mChebConv.message\u001b[0;34m(self, x_j, norm)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmessage\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_j: Tensor, norm: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_j\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 28.00 MiB (GPU 0; 14.56 GiB total capacity; 1.90 GiB already allocated; 5.50 MiB free; 1.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##############################################################################\n",
    "########################## INSTANTIATING THE MODEL ###########################\n",
    "##############################################################################\n",
    "AE_K_CHEB = 3\n",
    "AE_CONV_DIM = 16\n",
    "AE_LINEAR_DIM = 8\n",
    "AE_DROPOUT = 0.5\n",
    "AE_ACTIVATION = 'tanh'\n",
    "\n",
    "AE_parameters = {\n",
    "    'K_cheb': AE_K_CHEB,\n",
    "    'conv_dim': AE_CONV_DIM,\n",
    "    'linear_dim': AE_LINEAR_DIM,\n",
    "    'dropout': AE_DROPOUT,\n",
    "    'activation': AE_ACTIVATION,\n",
    "    'num_channels': NUM_CHANNELS,\n",
    "    'device': device,\n",
    "}\n",
    "\n",
    "DD_SEQ_LEN = 12\n",
    "DD_FEAT_DIM = AE_LINEAR_DIM\n",
    "DD_LEFT_NODES = 2000\n",
    "DD_parameters = {\n",
    "    'seq_len': DD_SEQ_LEN,\n",
    "    'feat_dim': DD_FEAT_DIM,\n",
    "    'left_nodes': DD_LEFT_NODES,\n",
    "}\n",
    "\n",
    "# autoencoder linear dims + 4 sin-cos time features\n",
    "PRED_FEATURES   = AE_LINEAR_DIM + 4\n",
    "PRED_LINEAR_DIM = 32\n",
    "PRED_PERIODS_IN = 12\n",
    "PRED_PERIODS_OUT = [0, 1, 2, 5, 8, 11]\n",
    "PRED_ACTIVATION  = 'relu'\n",
    "\n",
    "PR_parameters = {\n",
    "    'features': PRED_FEATURES,\n",
    "    'linear_dim': PRED_LINEAR_DIM,\n",
    "    'periods_in': PRED_PERIODS_IN,\n",
    "    'periods_out': PRED_PERIODS_OUT,\n",
    "    'activation': PRED_ACTIVATION,\n",
    "    'num_channels': NUM_CHANNELS,\n",
    "    'device': device,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    \n",
    "}\n",
    "num_epochs = EPOCHS\n",
    "dataloaders = train_dataloader, target_dataloader, targettest_dataloader\n",
    "AE_criterion = nn.MSELoss()\n",
    "PR_criterion = nn.MSELoss()\n",
    "optimizer_parameters = 5e-3, 5e-4\n",
    "BATCH_SIZE = BATCH_SIZE\n",
    "dd_lambda = 0.001\n",
    "\n",
    "folder = osp.join('training logs', 'models', exp_id)\n",
    "check_dir(folder)\n",
    "\n",
    "linear_dims = [8, 16, 32, 64]\n",
    "test_errors = {}\n",
    "for linear_dim in linear_dims:\n",
    "    PR_parameters['linear_dim'] = linear_dim\n",
    "    model = Model(\n",
    "        AE_parameters=AE_parameters,\n",
    "        DD_parameters=DD_parameters,\n",
    "        PR_parameters=PR_parameters,\n",
    "        num_epochs=num_epochs,\n",
    "        dataloaders=dataloaders,\n",
    "        AE_criterion=AE_criterion,\n",
    "        PR_criterion=PR_criterion,\n",
    "        optimizer_parameters=optimizer_parameters,\n",
    "        BATCH_SIZE=BATCH_SIZE,\n",
    "        dd_lambda=dd_lambda,\n",
    "        folder=folder,\n",
    "        specs=f'{linear_dim}',\n",
    "        tgt=tgt).to(device)\n",
    "    \n",
    "    model.load_module('autoencoder', folder, 'ae_.pth')\n",
    "    \n",
    "    model.pred_train('pretrain', save=False)\n",
    "    model.pred_train('finetune', save=True)\n",
    "\n",
    "    test_errors[linear_dim] = model.pred_test().detach()\n",
    "    # gc.collect()\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "check_dir(osp.join('figures', exp_id))\n",
    "plot_losses_boxplot(test_errors, variable=r'Linear Dimension', \n",
    "                    errors=['MAE', 'MSE'], specs=\"\", save=True, show=False,\n",
    "                    exp_id=exp_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f75c548-de9b-4dda-9c78-462edb32c643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
