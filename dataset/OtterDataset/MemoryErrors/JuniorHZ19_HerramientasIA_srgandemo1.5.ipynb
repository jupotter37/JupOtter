{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuniorHZ19/HerramientasIA/blob/main/srgandemo1.5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5OZ5GBZaUYa"
      },
      "source": [
        "!wget http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip\n",
        "!unzip DIV2K_train_HR.zip # This is our dataset link. I will include this command in the description"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI0KcjC3auG4"
      },
      "source": [
        "# Do set your runtime to GPU. You will need it\n",
        "import torch\n",
        "import math\n",
        "from os import listdir\n",
        "import numpy as np\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbIc8Q-Wa0kc"
      },
      "source": [
        "from torchvision.transforms import Compose, RandomCrop, ToTensor, ToPILImage, CenterCrop, Resize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjX6tw0pa61z"
      },
      "source": [
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ARrKwvPdQTp"
      },
      "source": [
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgULVo0cvoqw"
      },
      "source": [
        "from os.path import join"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgr0F6vldlcC",
        "outputId": "0742835d-e4c1-4b8a-f1b4-a203d6e7402e"
      },
      "source": [
        "torch.autograd.set_detect_anomaly(True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f389112d050>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9gCiSoBdnXC"
      },
      "source": [
        "UPSCALE_FACTOR = 4\n",
        "CROP_SIZE = 88"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JxvJbk7dqlY"
      },
      "source": [
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0tpRB2Jdys4"
      },
      "source": [
        "# Now, I will load in some code for the dataset and dataloaders.\n",
        "# Link to this notebook will be in the description, so you can get it from there\n",
        "def is_image_file(filename):\n",
        "    return any(filename.endswith(extension) for extension in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'])\n",
        "\n",
        "\n",
        "def calculate_valid_crop_size(crop_size, upscale_factor):\n",
        "    return crop_size - (crop_size % upscale_factor)\n",
        "\n",
        "\n",
        "def train_hr_transform(crop_size):\n",
        "    return Compose([\n",
        "        RandomCrop(crop_size),\n",
        "        ToTensor(),\n",
        "    ])\n",
        "\n",
        "\n",
        "def train_lr_transform(crop_size, upscale_factor):\n",
        "    return Compose([\n",
        "        ToPILImage(),\n",
        "        Resize(crop_size // upscale_factor, interpolation=Image.BICUBIC),\n",
        "        ToTensor()\n",
        "    ])\n",
        "\n",
        "\n",
        "def display_transform():\n",
        "    return Compose([\n",
        "        ToPILImage(),\n",
        "        Resize(400),\n",
        "        CenterCrop(400),\n",
        "        ToTensor()\n",
        "    ])\n",
        "\n",
        "\n",
        "class TrainDatasetFromFolder(Dataset):\n",
        "    def __init__(self, dataset_dir, crop_size, upscale_factor):\n",
        "        super(TrainDatasetFromFolder, self).__init__()\n",
        "        self.image_filenames = [join(dataset_dir, x) for x in listdir(dataset_dir) if is_image_file(x)]\n",
        "        crop_size = calculate_valid_crop_size(crop_size, upscale_factor)\n",
        "        self.hr_transform = train_hr_transform(crop_size)\n",
        "        self.lr_transform = train_lr_transform(crop_size, upscale_factor)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        hr_image = self.hr_transform(Image.open(self.image_filenames[index]))\n",
        "        lr_image = self.lr_transform(hr_image)\n",
        "        return lr_image, hr_image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHTaB3fTv11X"
      },
      "source": [
        "# That was the dataset code.\n",
        "# Now lets load the train set\n",
        "# Forgot to import join from os.path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re2-OAGQv-aQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0e1d3ed-0754-4374-cbc7-7134cd8138c3"
      },
      "source": [
        "train_set = TrainDatasetFromFolder(\"DIV2K_train_HR\", crop_size=CROP_SIZE,\n",
        "                                   upscale_factor=UPSCALE_FACTOR)\n",
        "trainloader = DataLoader(train_set, batch_size=64, num_workers=4, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA8VHoPTwMaz"
      },
      "source": [
        "from torch import nn, optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQGAwDSGwUnE"
      },
      "source": [
        "# Now we will start implementing the model. We will start in this vid, and co,plee\n",
        "# in the next video\n",
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self, channels):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "    self.bn1 = nn.BatchNorm2d(channels)\n",
        "    self.prelu = nn.PReLU()\n",
        "    self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "    self.bn2 = nn.BatchNorm2d(channels)\n",
        "  def forward(self, x):\n",
        "    residual = self.conv1(x)\n",
        "    residual = self.bn1(residual)\n",
        "    residual = self.prelu(residual)\n",
        "    residual = self.conv2(residual)\n",
        "    residual = self.bn2(residual)\n",
        "    return x + residual\n",
        "\n",
        "# We just implemented a pretty standard residual block here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNpcnPmbw6QQ"
      },
      "source": [
        "class UpsampleBlock(nn.Module):\n",
        "  def __init__(self, in_channels, up_scale):\n",
        "    super(UpsampleBlock, self).__init__()\n",
        "    self.conv = nn.Conv2d(in_channels, in_channels * up_scale ** 2,\n",
        "                          kernel_size=3, padding=1)\n",
        "    self.pixel_shuffle = nn.PixelShuffle(up_scale)\n",
        "    self.prelu = nn.PReLU()\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = self.pixel_shuffle(x)\n",
        "    x = self.prelu(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZnfl5c0uzUI"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, scale_factor):\n",
        "    super(Generator, self).__init__()\n",
        "    upsample_block_num = int(math.log(scale_factor, 2))\n",
        "\n",
        "    self.block1 = nn.Sequential(\n",
        "        nn.Conv2d(3, 64, kernel_size=9, padding=4),\n",
        "        nn.PReLU()\n",
        "    )\n",
        "\n",
        "    self.block2 = ResidualBlock(64)\n",
        "    self.block3 = ResidualBlock(64)\n",
        "    self.block4 = ResidualBlock(64)\n",
        "    self.block5 = ResidualBlock(64)\n",
        "    self.block6 = ResidualBlock(64)\n",
        "    self.block7 = nn.Sequential(\n",
        "        nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(64)\n",
        "    )\n",
        "    block8 = [UpsampleBlock(64, 2) for _ in range(upsample_block_num)]\n",
        "    block8.append(nn.Conv2d(64, 3, kernel_size=9, padding=4))\n",
        "    self.block8 = nn.Sequential(*block8)\n",
        "  def forward(self, x):\n",
        "    block1 = self.block1(x)\n",
        "    block2 = self.block2(block1)\n",
        "    block3 = self.block3(block2)\n",
        "    block4 = self.block4(block3)\n",
        "    block5 = self.block5(block4)\n",
        "    block6 = self.block6(block5)\n",
        "    block7 = self.block7(block6)\n",
        "    block8 = self.block8(block1 + block7)\n",
        "    return (torch.tanh(block8) + 1) / 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue5xbeyUuzWF"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.AdaptiveAvgPool2d(1),\n",
        "        nn.Conv2d(512, 1024, kernel_size=1),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Conv2d(1024, 1, kernel_size=1)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    batch_size=x.size()[0]\n",
        "    return torch.sigmoid(self.net(x).view(batch_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKD-vlkTxe0F"
      },
      "source": [
        "from torchvision.models.vgg import vgg16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RamJwy7yxitq"
      },
      "source": [
        "# Now we got to make the Generator Loss\n",
        "class TVLoss(nn.Module):\n",
        "  def __init__(self, tv_loss_weight=1):\n",
        "    super(TVLoss, self).__init__()\n",
        "    self.tv_loss_weight=tv_loss_weight\n",
        "  def forward(self, x):\n",
        "    batch_size=x.size()[0]\n",
        "    h_x = x.size()[2]\n",
        "    w_x = x.size()[3]\n",
        "\n",
        "    count_h = self.tensor_size(x[:, :, 1:, :])\n",
        "    count_w = self.tensor_size(x[:, :, :, 1:])\n",
        "\n",
        "    h_tv = torch.pow(x[:, :, 1:, :] - x[:, :, :h_x - 1, :], 2).sum()\n",
        "    w_tv = torch.pow(x[:, :, :, 1:] - x[:, :, :, :w_x - 1], 2).sum()\n",
        "    return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n",
        "\n",
        "  # Forgot to implement an important method\n",
        "  @staticmethod # Must add this\n",
        "  def tensor_size(t):\n",
        "    return t.size()[1] * t.size()[2] * t.size()[3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiwFaF3tySU_"
      },
      "source": [
        "class GeneratorLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(GeneratorLoss, self).__init__()\n",
        "    vgg = vgg16(pretrained=True)\n",
        "    loss_network = nn.Sequential(*list(vgg.features)[:31]).eval()\n",
        "    for param in loss_network.parameters():\n",
        "      param.requires_grad = False\n",
        "    self.loss_network = loss_network\n",
        "    self.mse_loss = nn.MSELoss()\n",
        "    self.tv_loss = TVLoss()\n",
        "  def forward(self, out_labels, out_images, target_images):\n",
        "    adversial_loss = torch.mean(1 - out_labels)\n",
        "    perception_loss = self.mse_loss(out_images, target_images)\n",
        "    image_loss = self.mse_loss(out_images, target_images)\n",
        "    tv_loss = self.tv_loss(out_images)\n",
        "    return image_loss + 0.001 * adversial_loss + 0.006 * perception_loss + 2e-8 * tv_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzzSCJjjy7I9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40c0ca85-29f7-4b09-cb70-8a7bc45d4294"
      },
      "source": [
        "device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Standard device selectoin\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oGBnX7PLGJ-"
      },
      "source": [
        "netG = Generator(UPSCALE_FACTOR)\n",
        "netD = Discriminator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n39cIDQeLJFO"
      },
      "source": [
        "generator_criterion = GeneratorLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ynXcO-8LSgN"
      },
      "source": [
        "generator_criterion = generator_criterion.to(device)\n",
        "netG = netG.to(device)\n",
        "netD = netD.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDlYj1loLiD_"
      },
      "source": [
        "optimizerG = optim.Adam(netG.parameters(), lr=0.0002)\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=0.0002)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doBMeLWOLs_0"
      },
      "source": [
        "results = {\n",
        "    \"d_loss\":[],\n",
        "    \"g_loss\":[],\n",
        "    \"d_score\": [],\n",
        "    \"g_score\": []\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVSZCmkvMImY"
      },
      "source": [
        "## Now for training code\n",
        "from tqdm import tqdm\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeTxlp0eML1d"
      },
      "source": [
        "N_EPOCHS = 150 # 150 is good enough for our model. gives decent enough results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHadZtxcMO01"
      },
      "source": [
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "  train_bar = tqdm(trainloader)\n",
        "  running_results = {'batch_sizes':0, 'd_loss':0,\n",
        "                     \"g_loss\":0, \"d_score\":0, \"g_score\":0}\n",
        "\n",
        "  netG.train()\n",
        "  netD.train()\n",
        "  for data, target in train_bar:\n",
        "    g_update_first = True\n",
        "    batch_size = data.size(0)\n",
        "    running_results['batch_sizes'] += batch_size\n",
        "\n",
        "    real_img = Variable(target)\n",
        "    real_img = real_img.to(device)\n",
        "    z = Variable(data)\n",
        "    z = z.to(device)\n",
        "\n",
        "    ## Update Discriminator ##\n",
        "    fake_img = netG(z)\n",
        "    netD.zero_grad()\n",
        "    real_out = netD(real_img).mean()\n",
        "    fake_out = netD(fake_img).mean()\n",
        "    d_loss = 1 - real_out + fake_out\n",
        "    d_loss.backward(retain_graph = True)\n",
        "    optimizerD.step()\n",
        "\n",
        "    ## Now update Generator\n",
        "    fake_img = netG(z)\n",
        "    fake_out = netD(fake_img).mean()\n",
        "    netG.zero_grad()\n",
        "    g_loss = generator_criterion(fake_out, fake_img, real_img)\n",
        "    g_loss.backward()\n",
        "\n",
        "    fake_img = netG(z)\n",
        "    fake_out = netD(fake_img).mean()\n",
        "\n",
        "    optimizerG.step()\n",
        "\n",
        "    running_results['g_loss'] += g_loss.item() * batch_size\n",
        "    running_results['d_loss'] += d_loss.item() * batch_size\n",
        "    running_results['d_score'] += real_out.item() * batch_size\n",
        "    running_results['g_score'] += real_out.item() * batch_size\n",
        "\n",
        "    ## Updating the progress bar\n",
        "    train_bar.set_description(desc=\"[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f\" % (\n",
        "        epoch, N_EPOCHS, running_results['d_loss'] / running_results['batch_sizes'],\n",
        "        running_results['g_loss'] / running_results['batch_sizes'],\n",
        "        running_results['d_score'] / running_results['batch_sizes'],\n",
        "        running_results['g_score'] / running_results['batch_sizes']\n",
        "    ))\n",
        "  netG.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "# Suponiendo que \"Generador\" es la clase de tu generador y \"ruta_modelo\" es la ruta del modelo guardado\n",
        "generador = Generator(4)  # creando generado\n",
        "generador.load_state_dict(torch.load(\"/content/srgan_g.pt\"))\n",
        "generador.to(device)\n",
        "generador.eval()\n",
        "\n",
        "ruta_imagen = \"/content/leyndalucy2.png\"\n",
        "imagen_pil = Image.open(ruta_imagen)\n",
        "\n",
        "transformaciones = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convierte la imagen a un tensor\n",
        "\n",
        "])\n",
        "\n",
        "imagen_transformada = transformaciones(imagen_pil)\n",
        "imagen_transformada=imagen_transformada.to(device)\n",
        "with torch.no_grad():\n",
        "    generador.eval()\n",
        "    generated_image = generador(imagen_transformada.unsqueeze(0)).detach().cpu()\n",
        "\n",
        "# Convertir la imagen al rango [0, 1]\n",
        "generated_image = (generated_image + 1) / 2.0\n",
        "\n",
        "generated_image_np = generated_image.squeeze().permute(1, 2, 0).numpy()\n",
        "\n",
        "print(generated_image.shape)\n",
        "# Visualizar la imagen generada\n",
        "plt.imshow(generated_image_np)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KRHPy45B7rn7",
        "outputId": "fa9e0a27-944c-43c9-aa36-6bbd426e7c77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 25.06 MiB is free. Process 6537 has 14.72 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 397.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-c009a0a0591b>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Suponiendo que \"Generador\" es la clase de tu generador y \"ruta_modelo\" es la ruta del modelo guardado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgenerador\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# creando generado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mgenerador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/srgan_g.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mgenerador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mgenerador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1012\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m                 return _load(opened_zipfile,\n\u001b[0m\u001b[1;32m   1015\u001b[0m                              \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m                              \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1422\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1390\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m             \u001b[0mtyped_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtyped_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1364\u001b[0m         \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m         typed_storage = torch.storage.TypedStorage(\n\u001b[0;32m-> 1366\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1367\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m             _internal=True)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUntypedStorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             untyped_storage = torch.UntypedStorage(\n\u001b[0m\u001b[1;32m    115\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             )\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 25.06 MiB is free. Process 6537 has 14.72 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 397.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW0VrpXKOAJQ"
      },
      "source": [
        "# We will just make sure that this model trains 5 epochs successfuly\n",
        "# This will take around 2 hours to train, please monitor colab\n",
        "# To make sure that it does not time out\n",
        "# There might be few errors, mostly due to typos\n",
        "# The progress bar acts slightly weird\n",
        "# But you see the model is now training\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}