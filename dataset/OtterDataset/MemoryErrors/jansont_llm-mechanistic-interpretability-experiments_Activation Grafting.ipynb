{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1cdb984-1bd1-4c62-afb1-fac9c8677cbe",
   "metadata": {},
   "source": [
    "## Model Grafting\n",
    "Explored the possibility of optimizing over the attention and mlp modules in a gpt2 model to find the minimal set of modules which can recover performance of the full model. \n",
    "\n",
    "Inspired from https://arxiv.org/pdf/2302.06600\n",
    "\n",
    "Did not work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecb18fd7-3c76-4cdd-91ef-a31410244c9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    # %pip install git+https://github.com/neelnanda-io/TransformerLens.git\n",
    "    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
    "    %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
    "    renderer = \"colab\"\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    from IPython import get_ipython\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    renderer = \"jupyterlab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7850beb2-bf91-4b0f-a597-f69be3275959",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: poetry in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: build<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.10.0)\n",
      "Requirement already satisfied: cachecontrol[filecache]<0.14.0,>=0.13.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.13.1)\n",
      "Requirement already satisfied: cleo<3.0.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (2.0.1)\n",
      "Requirement already satisfied: crashtest<0.5.0,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.4.1)\n",
      "Requirement already satisfied: dulwich<0.22.0,>=0.21.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.21.5)\n",
      "Requirement already satisfied: installer<0.8.0,>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.7.0)\n",
      "Requirement already satisfied: jsonschema<4.18.0,>=4.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (4.17.3)\n",
      "Requirement already satisfied: keyring<25.0.0,>=24.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (24.2.0)\n",
      "Requirement already satisfied: packaging>=20.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (23.1)\n",
      "Requirement already satisfied: pexpect<5.0.0,>=4.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (4.8.0)\n",
      "Requirement already satisfied: pkginfo<2.0.0,>=1.9.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.9.6)\n",
      "Requirement already satisfied: platformdirs<4.0.0,>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (3.8.0)\n",
      "Requirement already satisfied: poetry-core==1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.7.0)\n",
      "Requirement already satisfied: poetry-plugin-export<2.0.0,>=1.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.5.0)\n",
      "Requirement already satisfied: pyproject-hooks<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.26 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (2.31.0)\n",
      "Requirement already satisfied: requests-toolbelt<2,>=0.9.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.0.0)\n",
      "Requirement already satisfied: shellingham<2.0,>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (1.5.3)\n",
      "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (2.0.1)\n",
      "Requirement already satisfied: tomlkit<1.0.0,>=0.11.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (0.11.8)\n",
      "Requirement already satisfied: trove-classifiers>=2022.5.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (2023.8.7)\n",
      "Requirement already satisfied: virtualenv<21.0.0,>=20.22.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from poetry) (20.24.3)\n",
      "Requirement already satisfied: msgpack>=0.5.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cachecontrol[filecache]<0.14.0,>=0.13.0->poetry) (1.0.5)\n",
      "Requirement already satisfied: filelock>=3.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cachecontrol[filecache]<0.14.0,>=0.13.0->poetry) (3.12.2)\n",
      "Requirement already satisfied: rapidfuzz<3.0.0,>=2.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cleo<3.0.0,>=2.0.0->poetry) (2.15.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from dulwich<0.22.0,>=0.21.2->poetry) (2.0.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema<4.18.0,>=4.10.0->poetry) (23.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema<4.18.0,>=4.10.0->poetry) (0.19.3)\n",
      "Requirement already satisfied: jaraco.classes in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring<25.0.0,>=24.0.0->poetry) (3.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.11.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring<25.0.0,>=24.0.0->poetry) (5.2.0)\n",
      "Requirement already satisfied: SecretStorage>=3.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring<25.0.0,>=24.0.0->poetry) (3.3.3)\n",
      "Requirement already satisfied: jeepney>=0.4.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from keyring<25.0.0,>=24.0.0->poetry) (0.8.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pexpect<5.0.0,>=4.7.0->poetry) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0,>=2.26->poetry) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0,>=2.26->poetry) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0,>=2.26->poetry) (2023.5.7)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from virtualenv<21.0.0,>=20.22.0->poetry) (0.3.7)\n",
      "Collecting platformdirs<4.0.0,>=3.0.0 (from poetry)\n",
      "  Using cached platformdirs-3.10.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib-metadata>=4.11.4->keyring<25.0.0,>=24.0.0->poetry) (3.15.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from SecretStorage>=3.2->keyring<25.0.0,>=24.0.0->poetry) (39.0.2)\n",
      "Requirement already satisfied: more-itertools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jaraco.classes->keyring<25.0.0,>=24.0.0->poetry) (10.1.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring<25.0.0,>=24.0.0->poetry) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring<25.0.0,>=24.0.0->poetry) (2.21)\n",
      "Installing collected packages: platformdirs\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 3.8.0\n",
      "    Uninstalling platformdirs-3.8.0:\n",
      "      Successfully uninstalled platformdirs-3.8.0\n",
      "Successfully installed platformdirs-3.10.0\n",
      "Installing dependencies from lock file\n",
      "\n",
      "Package operations: 0 installs, 1 update, 0 removals\n",
      "\n",
      "  â€¢ Downgrading platformdirs (3.10.0 -> 3.8.0)\n",
      "\n",
      "Installing the current project: transformer-lens (0.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker 2.167.0 requires importlib-metadata<5.0,>=1.4.0, but you have importlib-metadata 5.2.0 which is incompatible.\n",
      "sagemaker 2.167.0 requires protobuf<4.0,>=3.1, but you have protobuf 4.23.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ../\n",
    "pip install poetry\n",
    "poetry install\n",
    "cd notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "222c65f7-63bf-44c5-8502-802ecc1370b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: numpy<1.24 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.23.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install 'numpy<1.24'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19d3d52e-646f-4715-b40a-bfff0987715e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import types\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.notebook as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "from jaxtyping import Float, Int\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import copy\n",
    "import subprocess\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML\n",
    "from tqdm import tqdm\n",
    "\n",
    "import ast\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import pickle\n",
    "\n",
    "if IN_COLAB: \n",
    "    import pysvelte\n",
    "\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens.utilities import devices\n",
    "from transformer_lens.past_key_value_caching import HookedTransformerKeyValueCache\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce11fc08-03af-4ce0-a2b8-8434f23bbead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6db405b-b60c-45c5-94e8-33eb312ac4f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def imshow(tensor, renderer=None, **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, **kwargs):\n",
    "    px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)\n",
    "\n",
    "def cuda():\n",
    "    return torch.cuda.is_available()\n",
    "\n",
    "def get_device(): \n",
    "    return \"cuda\" if cuda() else \"cpu\"\n",
    "\n",
    "def save_pickle(obj, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "# Function to load a pickle object from a file\n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj\n",
    "\n",
    "def save_json(data, file_path):\n",
    "    with open(file_path, \"w\") as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        loaded_data = json.load(json_file)\n",
    "    return loaded_data\n",
    "\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e4c1d8-eaa7-4dd6-9511-f163b28eea66",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "- fold_ln: Whether to fold in the LayerNorm weights to the subsequent linear layer. This does not change the computation.\n",
    "- center_writing_weights: Whether to center weights writing to the residual stream (ie set mean to be zero). Due to LayerNormthis doesn't change the computation.\n",
    "- center_unembed : Whether to center W_U (ie set mean to be zero). Softmax is translation invariant so this doesn't affect log probs or loss, but does change logits. Defaults to True.\n",
    "- refactor_factored_attn_matrices: Whether to convert the factoredmatrices (W_Q & W_K, and W_O & W_V) to be \"even\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "026991d1-9be3-40cc-b8cf-cbadf7792d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd79a7ddf1c4fba875a91c3de0032bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4e9e9428f541379df5798f83cc190b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37fd796a372a4f4b846ef1f7f003d115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058d2984e32948efa146fd0e101bb7ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ce163303684dbbb5bd18b0f454cdcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af86f08962d94c31948f56549f780443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-large into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"gpt2-large\"\n",
    "model = HookedTransformer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        center_unembed=True,  \n",
    "        center_writing_weights=True,              # Whether to center weights writing to the residual stream (ie set mean to be zero). Due to LayerNorm this doesn't change the computation.      \n",
    "        fold_ln=True,                             # Whether to  fold in the LayerNorm weights to the subsequent linear layer.\n",
    "        refactor_factored_attn_matrices=True,\n",
    "    )\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a3dc3-172c-4790-9aab-09db6333403a",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We use the CounterFact Dataset. We subsample facts which the model can accurately predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11446ada-fe11-453f-a45a-3ded4f64843d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_json(\"data/fact_dataset.json\")\n",
    "\n",
    "\n",
    "def sample_dataset(dataset, idx = None): \n",
    "    if idx is None: \n",
    "        prompt = \"The {} is located in\"\n",
    "        subject = \"Eiffel Tower\"\n",
    "        target = \"Paris\"\n",
    "    else: \n",
    "        sample = dataset[idx]\n",
    "        prompt = sample[\"requested_rewrite\"][\"prompt\"]\n",
    "        subject = sample[\"requested_rewrite\"][\"subject\"]\n",
    "        target = sample[\"requested_rewrite\"][\"target_true\"][\"str\"]\n",
    "    return prompt, subject, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1ece2cd-2127-49af-9efc-69dad43d2bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = sample_dataset(dataset)\n",
    "prompt, subject, target = sample\n",
    "prompt = prompt.format(subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c608fb-b179-4fe6-9365-7b90d6a6ded4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ablation Methods: \n",
    "- Noise ablation \n",
    "- Resample ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2601b6e-6a51-42bf-9285-ea45b487285f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def noise_ablation(prompt, subject, target, n_noise_samples=5, vx=3):\n",
    "    subject_tokens = model.to_tokens(subject)\n",
    "    \n",
    "    #shape: batch, n_tokens, embedding_dim\n",
    "    subject_embedding = model.embed(subject_tokens)\n",
    "    _, n_tokens, embedding_dim = subject_embedding.shape\n",
    "    \n",
    "    #noise: N(0,v), v = 3*std(embedding)\n",
    "    embedding = model.W_E\n",
    "    v = vx*torch.std(embedding, dim=0) #for each v in V\n",
    "    noise = torch.randn(\n",
    "        (n_noise_samples, n_tokens, embedding_dim)\n",
    "    ).to(device) + v\n",
    "    \n",
    "    subject_embedding_w_noise = subject_embedding + noise\n",
    "    \n",
    "    #shape: batch, n_tokens, vocab_size (logits)\n",
    "    unembedded_subject = model.unembed(subject_embedding_w_noise)\n",
    "\n",
    "    noisy_subject_tokens = torch.argmax(unembedded_subject, dim=-1)\n",
    "    noisy_subject_str = [\n",
    "        model.to_string(nst) for nst in noisy_subject_tokens\n",
    "    ]\n",
    "    true_prompt = prompt.format(subject)\n",
    "    corrupted_prompts = [\n",
    "        prompt.format(nss) for nss in noisy_subject_str\n",
    "    ]\n",
    "    return true_prompt, corrupted_prompts, target\n",
    "\n",
    "def resample_ablation(prompt, subject, target, n_noise_samples=20):\n",
    "    subject_tokens = model.to_tokens(subject)\n",
    "    embedding = model.W_E\n",
    "    #we select n random rows from the embedding matrix\n",
    "    permutations = torch.randperm(embedding.size(0))[:n_noise_samples]\n",
    "    random_samples = embedding[permutations]\n",
    "    #unsqueeze a token dimension between batch and embedding dims\n",
    "    random_samples = random_samples.unsqueeze(dim=1)\n",
    "    #we de-embed these rows\n",
    "    random_embeddings = model.unembed(random_samples)\n",
    "    random_tokens = torch.argmax(random_embeddings, dim=-1)\n",
    "    random_subject_str = [\n",
    "        model.to_string(t) for t in random_tokens\n",
    "    ]\n",
    "    corrupted_facts = [\n",
    "        prompt.format(s) for s in random_subject_str\n",
    "    ]\n",
    "    true_fact = prompt.format(subject)\n",
    "    return true_fact, corrupted_facts, target\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca842931-430f-4c96-b05d-397e9e947ad5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Eiffel Tower is located in',\n",
       " ['The  mechanersonocularese Sites is located in',\n",
       "  'The ungle Presenceteinfu STA is located in',\n",
       "  'The apesengedruciatinghidRECT is located in',\n",
       "  'The untaatistre HELillary is located in',\n",
       "  'The icidesetsk Lie aisle Pirates is located in'],\n",
       " 'Paris')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt, subject, target = sample_dataset(dataset)\n",
    "noise_ablation(prompt, subject, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8f8accf-be5f-4546-9b69-717c8f87bc28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_from_left(tokens : torch.tensor, maxlen:int):\n",
    "    pad_token = model.tokenizer.pad_token_id\n",
    "    padded_tokenized_inputs = torch.zeros(tokens.shape[0], maxlen)\n",
    "    \n",
    "    n_pads = maxlen - tokens.shape[-1]\n",
    "    padded_tokenized_inputs[:,n_pads] = pad_token\n",
    "    padded_tokenized_inputs[:,n_pads:] = tokens\n",
    "    return padded_tokenized_inputs.long()\n",
    "\n",
    "def pad_to_same_length(clean_tokens, corrupted_tokens): \n",
    "    \n",
    "    maxlen = max([clean_tokens.shape[-1], corrupted_tokens.shape[-1]])\n",
    "    \n",
    "    if clean_tokens.shape[-1] > corrupted_tokens.shape[-1]: \n",
    "        corrupted_tokens = pad_from_left(corrupted_tokens, maxlen)\n",
    "    elif clean_tokens.shape[-1] < corrupted_tokens.shape[-1]: \n",
    "        clean_tokens = pad_from_left(clean_tokens, maxlen)\n",
    "    return clean_tokens, corrupted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a49d7aa-2980-4f0b-adf0-b32627fd34f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "possible_patches = [\n",
    "    # 'ln1.hook_scale',\n",
    "    # 'ln1.hook_normalized',\n",
    "    # 'attn.hook_q',\n",
    "    # 'attn.hook_k',\n",
    "    # 'attn.hook_v',\n",
    "    # 'attn.hook_attn_scores',\n",
    "    # 'attn.hook_pattern',\n",
    "    # 'attn.hook_z',\n",
    "    'hook_attn_out',\n",
    "    # 'hook_resid_mid',\n",
    "    # 'ln2.hook_scale',\n",
    "    # 'ln2.hook_normalized',\n",
    "    # 'mlp.hook_pre',\n",
    "    # 'mlp.hook_post',\n",
    "    'hook_mlp_out',\n",
    "    # 'hook_resid_post'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "818c416f-efc8-453d-8c94-624023895afe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def patch(\n",
    "    corrupted_residual_component: Float[torch.Tensor, \"batch pos d_model\"],\n",
    "    hook,\n",
    "    pos,\n",
    "    cache):\n",
    "    corrupted_residual_component[:, pos, :] = cache[hook.name][:, pos, :]\n",
    "    \"\"\"\n",
    "    Restore a patch in clean run. \n",
    "    \"\"\"\n",
    "    return corrupted_residual_component\n",
    "\n",
    "def decompose_residual(clean_cache,\n",
    "                  corrupted_cache,\n",
    "                  token_idx,\n",
    "                  target_token_idx): \n",
    "    #shape: [74, 4, 9, 50257] = n_layers, batch, tokens, vocab_size\n",
    "    residual_clean_stack, labels = clean_cache.decompose_resid(layer=-1, return_labels=True)       \n",
    "    residual_corrupted_stack = corrupted_cache.decompose_resid(layer=-1, return_labels=False)\n",
    "   \n",
    "    token_idx_expanded = token_idx.repeat(residual_clean_stack.shape[0],1,1)\n",
    "    \n",
    "    residual_clean_stack = unembedding_function(residual_clean_stack, cache)\n",
    "    residual_clean_stack = residual_clean_stack[:,:,-1,:]\n",
    "    residual_clean_logits = residual_clean_stack.gather(index=target_idx_expanded, dim=-1) - residual_clean_stack.mean(dim=-1, keepdim=True)\n",
    "    \n",
    "    residual_corrupted_stack = unembedding_function(residual_corrupted_stack, cache)\n",
    "    residual_corrupted_stack = residual_corrupted_stack[:,:,-1,:]\n",
    "    #shape: layers, batch\n",
    "    residual_corrupted_logits = residual_corrupted_stack.gather(index=target_idx_expanded, dim=-1) - residual_clean_stack.mean(dim=-1, keepdim=True)\n",
    "    \n",
    "    direct_effect = (residual_clean_logits - residual_corrupted_logits).mean(dim=-1)\n",
    "    print(direct_effect.shape)\n",
    "    raise\n",
    "    return direct_effect\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def get_causal_positions(original_fact, corrupted_facts, target, use_mle_idx=True): \n",
    "    \n",
    "    clean_tokens = model.to_tokens(clean_prompt, prepend_bos=True) \n",
    "    corrupted_tokens = model.to_tokens(corrupted_facts, prepend_bos=True)\n",
    "    #for now, use single corruption\n",
    "    clean_tokens, corrupted_tokens = pad_to_same_length(clean_tokens, corrupted_tokens)\n",
    "    corrupted_tokens = corrupted_tokens[0].unsqueeze(0)\n",
    "    \n",
    "    target_token_idx = target_token_idx.expand(corrupted_tokens.shape[0], -1)\n",
    "    n_tokens = clean_tokens.shape[-1]\n",
    "\n",
    "    clean_logits, clean_cache = model.run_with_cache(clean_tokens, return_type=\"logits\")\n",
    "    _, corrupted_cache = model.run_with_cache(corrupted_tokens, return_type=\"logits\")\n",
    "    \n",
    "    clean_logits = clean_logits[:,-1,:]\n",
    "    \n",
    "    mle_token_idx = clean_logits.argmax(dim=-1)\n",
    "    token_idx = mke_token_idx if use_mle_idx else target_token_idx\n",
    "    \n",
    "   \n",
    "  \n",
    "    layer_names = []\n",
    "    for i in range(n_layers): \n",
    "        patch_name = f\"blocks.{i}.resid_post\"\n",
    "        \n",
    "        hook_fn = partial(patch, pos=tok_pos, cache=clean_cache)\n",
    "        hook = (patch_name, hook_fn)\n",
    "        patched_logits = model.run_with_hooks(\n",
    "            corrupted_tokens,\n",
    "            fwd_hooks=[hook],\n",
    "            return_type=\"logits\"\n",
    "        )   \n",
    "        patched_logits = patched_logits[:,-1,:]\n",
    "        patched_logits = patched_logits - patched_logits.mean(dim=-1, keepdim=True)\n",
    "        patched_loss = F.softmax(patched_logits, dim=-1)\n",
    "\n",
    "        causal_positions_logit_diff[i, tok_pos] = (patched_logits[:,target_idx] - corrupted_logits[:,target_idx]).to(\"cpu\")\n",
    "        causal_positions_prob_diff[i, tok_pos] = (patched_logits[:,target_idx] - corrupted_prob[:,target_idx]).to(\"cpu\")\n",
    "        causal_positions_KL_loss[i, tok_pos] = F.kl_div(patched_logits.log(), corrupted_prob, reduction='batchmean').to(\"cpu\")\n",
    "        causal_positions_CE_loss_diff[i, tok_pos] = F.cross_entropy(patched_logits, corrupted_logits).to(\"cpu\")\n",
    "\n",
    "    return {\n",
    "        \"causal_positions_logit_diff\": causal_positions_logit_diff.to(\"cpu\"), \n",
    "        \"causal_positions_prob_diff\": causal_positions_prob_diff.to(\"cpu\"),\n",
    "        \"causal_positions_KL_loss\": causal_positions_KL_loss.to(\"cpu\"),\n",
    "        \"causal_positions_CE_loss_diff\": causal_positions_CE_loss_diff.to(\"cpu\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e08f8bc0-ffb9-4d23-af18-0e62f3b5a0a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_causal_positions() missing 1 required positional argument: 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12650/92269124.py\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moriginal_fact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrupted_facts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresample_ablation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_noise_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgraft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_causal_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_fact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrupted_facts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: get_causal_positions() missing 1 required positional argument: 'target'"
     ]
    }
   ],
   "source": [
    "clean_prompt, subject, target = sample_dataset(dataset)\n",
    "original_fact, corrupted_facts, target = resample_ablation(clean_prompt, subject, target, n_noise_samples=10)\n",
    "\n",
    "graft = get_causal_positions(original_fact, corrupted_facts, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5ff457-52d5-4606-8f52-ad94653997ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_logits, clean_cache = model.run_with_cache(clean_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65bdaa6-df48-420b-ba19-5cb6014b0559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0e01c6-8ad6-4fa3-8f02-d98a3a80dec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32049b90-0e6b-4a05-a347-ececd6919429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcc2ed8-687a-4685-a3c9-d1baed8b68f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02acd41e-c1b1-4d84-96d0-c53ac7da4825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06540603-5362-4250-92bd-a686e8c44949",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def patch(\n",
    "    corrupted_residual_component: Float[torch.Tensor, \"batch pos d_model\"],\n",
    "    hook,\n",
    "    pos,\n",
    "    cache):\n",
    "    corrupted_residual_component[:, pos, :] = cache[hook.name][:, pos, :]\n",
    "    \"\"\"\n",
    "    Restore a patch in clean run. \n",
    "    \"\"\"\n",
    "    return corrupted_residual_component\n",
    "\n",
    "\n",
    "def get_causal_positions(model, original_fact, corrupted_facts, target): \n",
    "    \n",
    "    clean_tokens = model.to_tokens(clean_prompt, prepend_bos=True) \n",
    "    corrupted_tokens = model.to_tokens(corrupted_facts, prepend_bos=True)\n",
    "    #for now, use single corruption\n",
    "    clean_tokens, corrupted_tokens = pad_to_same_length(clean_tokens, corrupted_tokens)\n",
    "    corrupted_tokens = corrupted_tokens[0].unsqueeze(0)\n",
    "    n_tokens = clean_tokens.shape[-1]\n",
    "\n",
    "    clean_logits, clean_cache = model.run_with_cache(clean_tokens, return_type=\"logits\")\n",
    "    corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens, return_type=\"logits\")\n",
    "    clean_logits = clean_logits[:,-1,:] ; corrupted_logits = corrupted_logits[:,-1,:]\n",
    "    clean_logits = clean_logits - clean_logits.mean(dim=-1, keepdim=True)\n",
    "    corrupted_logits = corrupted_logits - corrupted_logits.mean(dim=-1, keepdim=True)\n",
    "    \n",
    "    mle_idx = torch.argmax(clean_logits, dim=-1)\n",
    "    clean_prob = F.softmax(clean_logits, dim=-1)\n",
    "    corrupted_prob = F.softmax(corrupted_logits, dim=-1)\n",
    "    \n",
    "    #calculate_total_effects\n",
    "    overall_total_effect_logits = clean_logits[:,mle_idx] - corrupted_logits[:,mle_idx]\n",
    "    overall_total_effect_prob = clean_prob[:,mle_idx] - corrupted_prob[:,mle_idx]\n",
    "    overall_total_effect_kl = F.kl_div(clean_prob.log(), corrupted_prob, reduction='batchmean')\n",
    "    overall_total_effect_CE = F.cross_entropy(clean_logits, corrupted_logits)\n",
    "\n",
    "    n_layers = model.cfg.n_layers\n",
    "    \n",
    "    causal_positions_CE_loss_diff = torch.zeros((n_layers*2, n_tokens))\n",
    "    causal_positions_KL_loss = torch.zeros((n_layers*2, n_tokens))\n",
    "    causal_positions_logit_diff = torch.zeros((n_layers*2, n_tokens))\n",
    "    causal_positions_prob_diff = torch.zeros((n_layers*2, n_tokens))\n",
    "    \n",
    "  \n",
    "    layer_names = []\n",
    "    for i in range(n_layers*2): \n",
    "        if i%2==0: \n",
    "            layer_names.append(f\"attn_{i//2}\")\n",
    "            patch_name = f\"blocks.{i//2}.hook_attn_out\"\n",
    "        else: \n",
    "            layer_names.append(f\"mlp_{i//2}\")\n",
    "            patch_name = f\"blocks.{i//2}.hook_mlp_out\"\n",
    "                    \n",
    "\n",
    "        for tok_pos in range(n_tokens): \n",
    "            hook_fn = partial(patch, pos=tok_pos, cache=clean_cache)\n",
    "            hook = (patch_name, hook_fn)\n",
    "            patched_logits = model.run_with_hooks(\n",
    "                corrupted_tokens,\n",
    "                fwd_hooks=[hook],\n",
    "                return_type=\"logits\"\n",
    "            )   \n",
    "            patched_logits = patched_logits[:,-1,:]\n",
    "            patched_logits = patched_logits - patched_logits.mean(dim=-1, keepdim=True)\n",
    "            patched_loss = F.softmax(patched_logits, dim=-1)\n",
    "            \n",
    "         \n",
    "            causal_positions_logit_diff[i, tok_pos] = (patched_logits[:,mle_idx] - corrupted_logits[:,mle_idx]).to(\"cpu\")\n",
    "            causal_positions_prob_diff[i, tok_pos] = (patched_logits[:,mle_idx] - corrupted_prob[:,mle_idx]).to(\"cpu\")\n",
    "            causal_positions_KL_loss[i, tok_pos] = F.kl_div(patched_logits.log(), corrupted_prob, reduction='batchmean').to(\"cpu\")\n",
    "            causal_positions_CE_loss_diff[i, tok_pos] = F.cross_entropy(patched_logits, corrupted_logits).to(\"cpu\")\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"causal_positions_logit_diff\": causal_positions_logit_diff.to(\"cpu\"), \n",
    "        \"causal_positions_prob_diff\": causal_positions_prob_diff.to(\"cpu\"),\n",
    "        \"causal_positions_KL_loss\": causal_positions_KL_loss.to(\"cpu\"),\n",
    "        \"causal_positions_CE_loss_diff\": causal_positions_CE_loss_diff.to(\"cpu\"),\n",
    "        \"overall_total_effect_logits\": overall_total_effect_logits.to(\"cpu\"),\n",
    "        \"overall_total_effect_prob\": overall_total_effect_prob.to(\"cpu\"),\n",
    "        \"overall_total_effect_kl\": overall_total_effect_kl.to(\"cpu\"),\n",
    "        \"overall_total_effect_CE\": overall_total_effect_CE.to(\"cpu\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c3bc00e-be72-462c-99a7-3ac698ab4eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.75 GiB total capacity; 13.28 GiB already allocated; 12.81 MiB free; 13.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7227/92269124.py\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moriginal_fact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrupted_facts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresample_ablation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_noise_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgraft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_causal_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_fact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrupted_facts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_7227/4022284190.py\u001b[0m in \u001b[0;36mget_causal_positions\u001b[0;34m(original_fact, corrupted_facts, target)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mhook_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtok_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclean_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpatch_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             patched_logits = model.run_with_hooks(\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0mcorrupted_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mfwd_hooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/transformer-circuits/transformer_lens/hook_points.py\u001b[0m in \u001b[0;36mrun_with_hooks\u001b[0;34m(self, fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mfwd_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbwd_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_hooks_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_contexts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         ) as hooked_model:\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mhooked_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     def add_caching_hooks(\n",
      "\u001b[0;32m~/SageMaker/transformer-circuits/transformer_lens/HookedTransformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    364\u001b[0m                 )\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             residual = block(\n\u001b[0m\u001b[1;32m    367\u001b[0m                 \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 \u001b[0mpast_kv_cache_entry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_kv_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/transformer-circuits/transformer_lens/components.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    965\u001b[0m             \u001b[0;31m# queries, keys and values, independently.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m             \u001b[0;31m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m             self.attn(\n\u001b[0m\u001b[1;32m    968\u001b[0m                 \u001b[0mquery_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mshortformer_pos_embed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mshortformer_pos_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/transformer-circuits/transformer_lens/components.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         q = self.hook_q(\n\u001b[0;32m--> 508\u001b[0;31m             einsum(\n\u001b[0m\u001b[1;32m    509\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mqkv_einops_string\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_index\u001b[0m \u001b[0md_model\u001b[0m \u001b[0md_head\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m                 \u001b[0;34m->\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0mpos\u001b[0m \u001b[0mhead_index\u001b[0m \u001b[0md_head\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/fancy_einsum/__init__.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mnew_equation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_equation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_equation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/fancy_einsum/__init__.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(self, equation, *operands)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;31m# the path for contracting 0 or 1 time(s) is already optimized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;31m# or the user has disabled using opt_einsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.75 GiB total capacity; 13.28 GiB already allocated; 12.81 MiB free; 13.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "clean_prompt, subject, target = sample_dataset(dataset)\n",
    "original_fact, corrupted_facts, target = resample_ablation(clean_prompt, subject, target, n_noise_samples=10)\n",
    "\n",
    "graft = get_causal_positions(original_fact, corrupted_facts, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3265b486-b515-4308-9e06-d6d89499aff6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'causal_positions_logit_diff': tensor([[1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420],\n",
       "         [1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420],\n",
       "         [1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420],\n",
       "         [1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420],\n",
       "         [1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420],\n",
       "         [1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420],\n",
       "         [1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420],\n",
       "         [1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420],\n",
       "         [1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420],\n",
       "         [1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420],\n",
       "         [1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420],\n",
       "         [1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420],\n",
       "         [1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420],\n",
       "         [1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420],\n",
       "         [1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420],\n",
       "         [1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420],\n",
       "         [1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420],\n",
       "         [1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420],\n",
       "         [1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420],\n",
       "         [1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420],\n",
       "         [1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420],\n",
       "         [1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420],\n",
       "         [1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420],\n",
       "         [1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420, 1.2420]],\n",
       "        grad_fn=<CopySlices>),\n",
       " 'causal_positions_prob_diff': tensor([[0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690],\n",
       "         [0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690],\n",
       "         [0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690],\n",
       "         [0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690],\n",
       "         [0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690],\n",
       "         [0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690],\n",
       "         [0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690],\n",
       "         [0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690],\n",
       "         [0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690],\n",
       "         [0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690],\n",
       "         [0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690],\n",
       "         [0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690],\n",
       "         [0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690],\n",
       "         [0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690],\n",
       "         [0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690],\n",
       "         [0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690],\n",
       "         [0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690],\n",
       "         [0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690],\n",
       "         [0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690],\n",
       "         [0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690],\n",
       "         [0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690],\n",
       "         [0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690],\n",
       "         [0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690],\n",
       "         [0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690, 0.0690]],\n",
       "        grad_fn=<CopySlices>),\n",
       " 'causal_positions_KL_loss': tensor([[0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775],\n",
       "         [0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775],\n",
       "         [0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775],\n",
       "         [0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775],\n",
       "         [0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775],\n",
       "         [0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775],\n",
       "         [0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775],\n",
       "         [0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775],\n",
       "         [0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775],\n",
       "         [0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775],\n",
       "         [0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775],\n",
       "         [0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775],\n",
       "         [0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775],\n",
       "         [0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775],\n",
       "         [0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775],\n",
       "         [0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775],\n",
       "         [0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775],\n",
       "         [0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775],\n",
       "         [0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775],\n",
       "         [0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775],\n",
       "         [0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775],\n",
       "         [0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775],\n",
       "         [0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775],\n",
       "         [0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775, 0.6775]],\n",
       "        grad_fn=<CopySlices>),\n",
       " 'causal_positions_CE_loss_diff': tensor([[-269341.5625, -269341.5625, -269341.5625, -269341.5625, -269341.5625,\n",
       "          -269341.5625, -269341.5625],\n",
       "         [-269341.5625, -269341.5625, -269341.5625, -269341.5625, -269341.5625,\n",
       "          -269341.5625, -269341.5625],\n",
       "         [-269341.5625, -269341.5625, -269341.5625, -269341.5625, -269341.5625,\n",
       "          -269341.5625, -269341.5625],\n",
       "         [-269341.5625, -269341.5625, -269341.5625, -269341.5625, -269341.5625,\n",
       "          -269341.5625, -269341.5625],\n",
       "         [-269341.5625, -269341.5625, -269341.5625, -269341.5625, -269341.5625,\n",
       "          -269341.5625, -269341.5625],\n",
       "         [-269341.5625, -269341.5625, -269341.5625, -269341.5625, -269341.5625,\n",
       "          -269341.5625, -269341.5625],\n",
       "         [-269341.5625, -269341.5625, -269341.5625, -269341.5625, -269341.5625,\n",
       "          -269341.5625, -269341.5625],\n",
       "         [-269341.5625, -269341.5625, -269341.5625, -269341.5625, -269341.5625,\n",
       "          -269341.5625, -269341.5625],\n",
       "         [-269341.5625, -269341.5625, -269341.5625, -269341.5625, -269341.5625,\n",
       "          -269341.5625, -269341.5625],\n",
       "         [-269341.5625, -269341.5625, -269341.5625, -269341.5625, -269341.5625,\n",
       "          -269341.5625, -269341.5625],\n",
       "         [-269341.5625, -269341.5625, -269341.5625, -269341.5625, -269341.5625,\n",
       "          -269341.5625, -269341.5625],\n",
       "         [-269341.5625, -269341.5625, -269341.5625, -269341.5625, -269341.5625,\n",
       "          -269341.5625, -269341.5625],\n",
       "         [-269341.5625, -269341.5625, -269341.5625, -269341.5625, -269341.5625,\n",
       "          -269341.5625, -269341.5625],\n",
       "         [-269341.5625, -269341.5625, -269341.5625, -269341.5625, -269341.5625,\n",
       "          -269341.5625, -269341.5625],\n",
       "         [-269341.5625, -269341.5625, -269341.5625, -269341.5625, -269341.5625,\n",
       "          -269341.5625, -269341.5625],\n",
       "         [-269341.5625, -269341.5625, -269341.5625, -269341.5625, -269341.5625,\n",
       "          -269341.5625, -269341.5625],\n",
       "         [-269341.5625, -269341.5625, -269341.5625, -269341.5625, -269341.5625,\n",
       "          -269341.5625, -269341.5625],\n",
       "         [-269341.5625, -269341.5625, -269341.5625, -269341.5625, -269341.5625,\n",
       "          -269341.5625, -269341.5625],\n",
       "         [-269341.5625, -269341.5625, -269341.5625, -269341.5625, -269341.5625,\n",
       "          -269341.5625, -269341.5625],\n",
       "         [-269341.5625, -269341.5625, -269341.5625, -269341.5625, -269341.5625,\n",
       "          -269341.5625, -269341.5625],\n",
       "         [-269341.5625, -269341.5625, -269341.5625, -269341.5625, -269341.5625,\n",
       "          -269341.5625, -269341.5625],\n",
       "         [-269341.5625, -269341.5625, -269341.5625, -269341.5625, -269341.5625,\n",
       "          -269341.5625, -269341.5625],\n",
       "         [-269341.5625, -269341.5625, -269341.5625, -269341.5625, -269341.5625,\n",
       "          -269341.5625, -269341.5625],\n",
       "         [-269341.5625, -269341.5625, -269341.5625, -269341.5625, -269341.5625,\n",
       "          -269341.5625, -269341.5625]], grad_fn=<CopySlices>),\n",
       " 'overall_total_effect_logits': tensor([[1.2420]], grad_fn=<ToCopyBackward0>),\n",
       " 'overall_total_effect_prob': tensor([[0.0690]], grad_fn=<ToCopyBackward0>),\n",
       " 'overall_total_effect_kl': tensor(0.6775, grad_fn=<ToCopyBackward0>),\n",
       " 'overall_total_effect_CE': tensor(-269341.5625, grad_fn=<ToCopyBackward0>)}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3a223d-7e6f-4ee1-b5f3-569e133bda2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e367a84-c848-469b-9734-5fab7a65ed4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67647d20-f6fd-488d-b1f6-1a5ab7071513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cadf48-66b7-428c-8fd2-0af7134b1d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161268d8-2bc0-4334-bdb2-ede355d08fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a100b-851c-4e95-a078-38f5d739dc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d93af1-eb23-4193-ba4e-a26073f3fb24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfde73e6-ec8a-4761-b040-251001837473",
   "metadata": {},
   "source": [
    "## Bayesian Optimization with Probabilistic Reparametrization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2222d62c-74ff-436c-93d3-797ca63816fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: scikit-optimize in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.9.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-optimize) (1.2.0)\n",
      "Requirement already satisfied: pyaml>=16.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-optimize) (23.7.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-optimize) (1.23.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-optimize) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-optimize) (1.2.2)\n",
      "Requirement already satisfied: PyYAML in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pyaml>=16.9->scikit-optimize) (6.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f8d6663-1346-438e-bea3-071c643cfe41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skopt import gp_minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "17619d02-1f75-4bbf-80eb-c2c15a6dbfd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def patch(\n",
    "    corrupted_residual_component: Float[torch.Tensor, \"batch pos d_model\"],\n",
    "    hook,\n",
    "    pos,\n",
    "    cache):\n",
    "    corrupted_residual_component[:, pos, :] = cache[hook.name][:, pos, :]\n",
    "    \"\"\"\n",
    "    Restore a patch in clean run. \n",
    "    \"\"\"\n",
    "    return corrupted_residual_component\n",
    "\n",
    "\n",
    "def unembedding_function(residual_stack, cache, mlp=False) -> float:\n",
    "    #we are only interested in applying the layer norm of the final layer on the final token\n",
    "    #shape: [74, 5, 10, 1280] = n_layers, prompts, tokens, d_model\n",
    "    z = cache.apply_ln_to_stack(residual_stack, layer = -1, mlp_input=mlp)\n",
    "    z = z @ model.W_U\n",
    "    return z\n",
    "\n",
    "\n",
    "def similarity_loss(original_logits, patched_logits, alpha=1.0, beta=1.0):\n",
    "    probs_original = F.softmax(original_logits, dim=-1)\n",
    "    probs_grafted = F.softmax(patched_logits, dim=-1)\n",
    "    kl_loss = F.kl_div(probs_grafted.log(), probs_original, reduction='batchmean')\n",
    "    \n",
    "    argmax_index = torch.argmax(original_logits)\n",
    "    truth = torch.zeros_like(original_logits)\n",
    "    truth[0, argmax_index] = 1\n",
    "    \n",
    "    ce = F.cross_entropy(patched_logits, truth)\n",
    "    \n",
    "    total_loss = alpha * kl_loss + beta * ce\n",
    "    \n",
    "    return total_loss\n",
    "        \n",
    "    \n",
    "def continuous_objective(continuous_graft,\n",
    "                         model,\n",
    "                         clean_cache,\n",
    "                         corrupted_tokens,\n",
    "                         original_logits, \n",
    "                         all_activation_keys, \n",
    "                         l1_lambda = 0.01):\n",
    "    \n",
    "    continuous_graft = np.array(continuous_graft)\n",
    "    continuous_graft = continuous_graft.reshape(len(all_activation_keys),\n",
    "                                                corrupted_tokens.shape[-1])\n",
    "\n",
    "    THRESH = 0.5  # Adjust the threshold based on your problem's characteristics\n",
    "\n",
    "    binary_graft = np.where(continuous_graft > THRESH, 1, 0)\n",
    "    # binary_graft = np.zeros_like(binary_graft)\n",
    "    reference_loss = similarity_loss(original_logits, original_logits).item()\n",
    "\n",
    "    fwd_hooks = []   \n",
    "        \n",
    "    for i in range(binary_graft.shape[0]):\n",
    "        patch_name = all_activation_keys[i]\n",
    "        for tok_pos in range(binary_graft.shape[1]):\n",
    "            graft_val = binary_graft[i, tok_pos]\n",
    "            if graft_val > 0:\n",
    "                hook_fn = partial(patch, pos=tok_pos, cache=clean_cache)\n",
    "                hook = (patch_name, hook_fn)\n",
    "                fwd_hooks.append(hook)\n",
    "\n",
    "    patched_logits = model.run_with_hooks(\n",
    "        corrupted_tokens,\n",
    "        fwd_hooks=fwd_hooks,\n",
    "        return_type=\"logits\"\n",
    "    )\n",
    "    patched_logits = patched_logits[:, -1, :]\n",
    "\n",
    "    loss = similarity_loss(original_logits=original_logits, patched_logits=patched_logits)\n",
    "    loss = loss.item()\n",
    "    loss = loss - reference_loss\n",
    "    # L1 regularization term to encourage sparsity\n",
    "    l1_reg = l1_lambda * np.sum(np.abs(continuous_graft))\n",
    "    total_loss = loss + l1_reg\n",
    "    print(total_loss)\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "    \n",
    "def learn_graft(\n",
    "             clean_prompt: str,\n",
    "             corrupted_prompts: List[str],\n",
    "             target: str):\n",
    "    \n",
    "    clean_tokens = model.to_tokens(clean_prompt, prepend_bos=True) \n",
    "    corrupted_tokens = model.to_tokens(corrupted_prompts, prepend_bos=True)\n",
    "    corrupted_tokens = corrupted_tokens[0].unsqueeze(0)\n",
    "    \n",
    "    target_token_idx = model.to_tokens(target)[:,1] \n",
    "    clean_tokens, corrupted_tokens = pad_to_same_length(clean_tokens, corrupted_tokens)\n",
    "    \n",
    "    clean_tokens = clean_tokens.expand(corrupted_tokens.shape[0], -1)\n",
    "    target_token_idx = target_token_idx.expand(corrupted_tokens.shape[0], -1)\n",
    "    \n",
    "    corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens, return_type=\"logits\")\n",
    "    clean_logits, clean_cache = model.run_with_cache(clean_tokens, return_type=\"logits\")\n",
    "    corrupted_logits = corrupted_logits[:,-1,:]\n",
    "    \n",
    "    \n",
    "    #define all possible activations to ablate\n",
    "    token_positions = torch.arange(corrupted_tokens.shape[-1], device=device)\n",
    "    all_activation_keys = []\n",
    "    for layer in range(model.cfg.n_layers): \n",
    "        for activation_patch in possible_patches:\n",
    "            patch_name = f\"blocks.{layer}.{activation_patch}\"\n",
    "            all_activation_keys.append(patch_name)\n",
    "\n",
    "    # Define the bounds for the continuous relaxation (between 0 and 1 for each value)\n",
    "    all_interventions = len(all_activation_keys) * clean_tokens.shape[-1]\n",
    "    bounds = [(0, 1) for _ in range(all_interventions)]\n",
    "\n",
    "    # Run Bayesian optimization\n",
    "    result = gp_minimize(\n",
    "        partial(continuous_objective, model=model, clean_cache=clean_cache,\n",
    "                corrupted_tokens=corrupted_tokens, original_logits=corrupted_logits, \n",
    "                all_activation_keys=all_activation_keys),\n",
    "        dimensions=bounds,\n",
    "        n_calls=25  # Adjust the number of optimization calls as needed\n",
    "    )\n",
    "\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "39fe50ed-e7b9-4285-a016-cde0ff685641",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.30443917274475\n",
      "19.8521035861969\n",
      "3.887194986343384\n",
      "20.53722764015198\n",
      "15.270883264541625\n",
      "20.43558289527893\n",
      "21.0517959690094\n",
      "21.339812631607057\n",
      "20.631496057510375\n",
      "19.860386056900026\n",
      "0.8797665023803711\n",
      "21.006300287246702\n",
      "20.064836587905884\n",
      "21.168011331558226\n",
      "19.868252267837523\n",
      "4.523120489120483\n",
      "21.106985177993774\n",
      "6.567327432632446\n",
      "21.653884439468385\n",
      "21.36880478858948\n",
      "21.413832559585572\n",
      "2.254520254135132\n",
      "20.59296280860901\n",
      "20.88110520362854\n",
      "21.072228021621704\n"
     ]
    }
   ],
   "source": [
    "clean_prompt, subject, target = sample_dataset(dataset)\n",
    "original_fact, corrupted_facts, target = resample_ablation(clean_prompt, subject, target, n_noise_samples=10)\n",
    "\n",
    "graft = learn_graft(original_fact, corrupted_facts, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3e538211-7562-4993-a102-a647bf10c81a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 1, 0, 0, 1, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 1, 1, 1],\n",
       "       [0, 1, 0, 1, 1, 1, 0, 1, 0],\n",
       "       [0, 1, 1, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 1, 1, 1, 0],\n",
       "       [1, 0, 0, 1, 0, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 1, 0, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 1, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 0, 1],\n",
       "       [0, 1, 1, 0, 0, 1, 1, 0, 0],\n",
       "       [0, 1, 1, 1, 1, 0, 1, 0, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 1, 1, 1, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 1, 0, 0, 1, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 0, 0, 1],\n",
       "       [1, 1, 0, 1, 0, 1, 0, 1, 0],\n",
       "       [1, 1, 1, 0, 1, 1, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 1, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(graft[\"x\"]).reshape(24,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d35fc0e-6385-41aa-8bae-3c3f1f760058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
