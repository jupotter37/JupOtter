{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda, number of workers: 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam, SGD, RMSprop\n",
    "from torchvision import disable_beta_transforms_warning\n",
    "disable_beta_transforms_warning()\n",
    "import torchvision.transforms.v2 as tf\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from fixcap import FixCapsNet\n",
    "from torchsummary import summary\n",
    "\n",
    "workers = os.cpu_count()\n",
    "pu = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "assert pu == 'cuda', 'Connect to the GPU'\n",
    "\n",
    "print(f'using {pu}, number of workers: {workers}')\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_folder = '/home/user'\n",
    "WORKSPACE = f'{home_folder}/Skin_Diseases_Detection'\n",
    "DATASET_FOLDER = f\"{home_folder}/skdi_dataset/base_dir\"\n",
    "TRAINING_FOLDER = f'{DATASET_FOLDER}/train_dir'\n",
    "TESTING_FOLDER = f'{DATASET_FOLDER}/val_dir'\n",
    "VALIDATION_FOLDER = f'{DATASET_FOLDER}/val_dir'\n",
    "CHECKPOINT_FOLDER = f'{WORKSPACE}/checkpoints'\n",
    "STATUS_FOLDER = f'{WORKSPACE}/status'\n",
    "PLOT_FOLDER = f'{WORKSPACE}/plots'\n",
    "PARAM_FOLDER = f'{WORKSPACE}/param'\n",
    "CONFIG_FOLDER = f'{WORKSPACE}/config'\n",
    "MODEL_FOLDER = f'{WORKSPACE}/model'\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, train_trans, test_trans, train_batch_size = 32):\n",
    "        self.train_ds = ImageFolder(TRAINING_FOLDER, train_trans)\n",
    "        self.test_ds = ImageFolder(TESTING_FOLDER, test_trans)\n",
    "        self.val_ds = ImageFolder(VALIDATION_FOLDER, test_trans)\n",
    "\n",
    "        self.train_dl = DataLoader(self.train_ds, batch_size=train_batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
    "        self.test_dl = DataLoader(self.test_ds, batch_size=train_batch_size, shuffle=True, num_workers=workers)\n",
    "        self.val_dl = DataLoader(self.val_ds, batch_size=train_batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
    "\n",
    "def make_cnn(dataset: ImageFolder, hid_layers = [64, 64],\n",
    "            act_fn='relu', max_pool = None, pooling_after_layers = 2, dropout = 0.2, batch_norm=True,\n",
    "            groups =1, bias=False, conv_layers=[[32, 3, 1],\n",
    "                                                [16, 3, 1]]):\n",
    "    \n",
    "    img = dataset.__getitem__(0)[0]\n",
    "    input_shape = img.shape\n",
    "    num_of_classes = len(dataset.classes)\n",
    "    layers = []\n",
    "    activation_fun = {'relu': nn.ReLU(inplace=True), 'softplus':nn.Softplus(), 'tanh':nn.Tanh(), 'elu': nn.ELU()}\n",
    "\n",
    "    assert pooling_after_layers <= len(conv_layers), 'exceeding the number conv layers..'\n",
    "\n",
    "    in_chann, inp_h, inp_w = input_shape\n",
    "    for ind, conv in enumerate(conv_layers):\n",
    "        out_chann, filter_size, stride = conv\n",
    "        layers.append(nn.Conv2d(in_chann, out_chann, filter_size, stride, groups=groups, bias=bias))\n",
    "        if batch_norm:\n",
    "            layers.append(nn.BatchNorm2d(out_chann))\n",
    "        layers.append(activation_fun[act_fn])\n",
    "\n",
    "        out_h = (inp_h - filter_size)//stride + 1\n",
    "        out_w = (inp_w - filter_size)//stride + 1\n",
    "        inp_h = out_h\n",
    "        inp_w = out_w\n",
    "\n",
    "        if max_pool is not None and ((ind+1) % pooling_after_layers == 0 or ind == (len(conv_layers) - 1)):\n",
    "            layers.append(nn.MaxPool2d(max_pool[0], max_pool[1]))\n",
    "            out_h = (inp_h - max_pool[0])//max_pool[1] + 1\n",
    "            out_w = (inp_w - max_pool[0])//max_pool[1] + 1\n",
    "            inp_h = out_h\n",
    "            inp_w = out_w\n",
    "        in_chann = out_chann\n",
    "\n",
    "    layers.append(nn.Flatten())\n",
    "    layers.append(nn.Linear(inp_h*inp_w*in_chann, hid_layers[0]))\n",
    "    layers.append(activation_fun[act_fn])\n",
    "    if len(hid_layers) > 1:\n",
    "        dim_pairs = zip(hid_layers[:-1], hid_layers[1:])\n",
    "        for in_dim, out_dim in list(dim_pairs):\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if dropout is not None:\n",
    "                layers.append(nn.Dropout(p=dropout))\n",
    "            layers.append(activation_fun[act_fn])\n",
    "\n",
    "    layers.append(nn.Linear(hid_layers[-1], num_of_classes))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def convert(**kwargs):\n",
    "    return kwargs\n",
    "\n",
    "class Utils:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.model_file = ''\n",
    "        self.plot_file = ''\n",
    "        self.min_loss = 0\n",
    "\n",
    "    def read_file(self, path):\n",
    "        file = open(path, 'r')\n",
    "        file.seek(0)\n",
    "        info = file.readline()\n",
    "        file.close()\n",
    "        return info\n",
    "\n",
    "    def write_file(self, path, content):\n",
    "        mode = 'w'\n",
    "        if path == self.plot_file:\n",
    "            mode = '+a'\n",
    "        file = open(path, mode=mode)\n",
    "        file.write(content)\n",
    "        file.close()\n",
    "\n",
    "    def create_file(self, path):\n",
    "        with open(path, 'w') as file:\n",
    "            pass\n",
    "        file.close()\n",
    "\n",
    "    def create_checkpoint_file(self, num):\n",
    "        path = f'{CHECKPOINT_FOLDER}/checkpoint_{num}.pth'\n",
    "        file = open(path, 'w')\n",
    "        file.close()\n",
    "        return path\n",
    "    \n",
    "    def save_config(self, args: dict):\n",
    "        if not os.path.exists(self.config_file):\n",
    "            self.create_file(self.config_file)\n",
    "        with open(self.config_file, 'w') as file:\n",
    "            yaml.safe_dump(args, file)\n",
    "        file.close()\n",
    "\n",
    "    def check_status_file(self):\n",
    "        if not os.path.exists(self.status_file):\n",
    "            self.create_file(self.status_file)\n",
    "        checkpath = self.read_file(self.status_file)\n",
    "        epoch = 0\n",
    "        if checkpath != '':\n",
    "            epoch = self.load_checkpoint(checkpath)\n",
    "            file = open(self.plot_file, 'r')\n",
    "            lines = file.readlines()\n",
    "            file = open(self.plot_file, 'w')\n",
    "            file.writelines(lines[:epoch+1])\n",
    "            file.close()\n",
    "        else:\n",
    "            file = open(self.plot_file, 'w')\n",
    "            file.close()\n",
    "            self.write_file(self.plot_file,'Train_loss,Train_acc,Valid_loss,Valid_acc\\n')\n",
    "            self.model.train()\n",
    "        return epoch\n",
    "\n",
    "    def write_plot_data(self, data:list):\n",
    "        str_data = ','.join(map(str, data))\n",
    "        self.write_file(self.plot_file, f'{str_data}\\n')\n",
    "\n",
    "    def save_checkpoint(self, epoch, checkpath):\n",
    "        checkpoint = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optim_state_dict': self.optim.state_dict(),\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        file = open(self.status_file, 'w')\n",
    "        file.write(checkpath)\n",
    "        file.close()\n",
    "        torch.save(checkpoint, checkpath)\n",
    "        print('checkpoint saved..')\n",
    "    \n",
    "    def load_checkpoint(self, checkpath):\n",
    "        print('loading checkpoint..')\n",
    "        checkpoint = torch.load(checkpath)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optim.load_state_dict(checkpoint['optim_state_dict'])\n",
    "        self.model.train()\n",
    "        print('checkpoint loaded...')\n",
    "        return checkpoint['epoch']\n",
    "    \n",
    "    def save_check_interval(self, epoch, interval=50):\n",
    "        num_checks = 7\n",
    "        if not(epoch % interval) and epoch > 0:\n",
    "            checkpath = self.create_checkpoint_file(epoch)\n",
    "            self.save_checkpoint(epoch, checkpath)\n",
    "            files = len(os.listdir(CHECKPOINT_FOLDER))\n",
    "            if files > num_checks+1:\n",
    "                os.remove(f'{CHECKPOINT_FOLDER}/checkpoint_{epoch-num_checks}.pth')\n",
    "\n",
    "    \n",
    "    def load_model(self):\n",
    "        print('loading model...')\n",
    "        self.model.load_state_dict(torch.load(self.model_file))\n",
    "        self.model.eval()\n",
    "        print('model loaded...')\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(self.model.state_dict(), self.model_file)\n",
    "        print('model saved...')\n",
    "\n",
    "    def save_best_model(self, param, acc_param=True):\n",
    "        if acc_param:\n",
    "           param_ = max(param, self.param)\n",
    "        else:\n",
    "            param_ = min(param, self.param)\n",
    "        if self.param != param_:\n",
    "            self.param = param_\n",
    "            self.write_file(self.param_file, f'{param_}')\n",
    "            self.save_model()\n",
    "\n",
    "    def check_param_file(self):\n",
    "        if os.path.exists(self.param_file):\n",
    "            param = float(self.read_file(self.param_file))\n",
    "        else:\n",
    "            self.create_file(self.param_file)\n",
    "            param = -1000.0\n",
    "            self.write_file(self.param_file, f'{param}')\n",
    "        return param\n",
    "\n",
    "class skdi_detector(Utils):\n",
    "    def __init__(self,params):\n",
    "        self.params = params\n",
    "        self.dataset = Dataset(train_trans=params.train_trans, test_trans=params.test_trans,\n",
    "                               train_batch_size=params.batch_size)\n",
    "        self.loss_fn = CrossEntropyLoss()\n",
    "        self.name = params.name\n",
    "        self.model_file = f'{MODEL_FOLDER}/{self.name}_model.pth'\n",
    "        self.status_file = f'{STATUS_FOLDER}/{self.name}_status.txt'\n",
    "        self.plot_file = f'{PLOT_FOLDER}/{self.name}_plot.txt'\n",
    "        self.param_file = f'{PARAM_FOLDER}/{self.name}_param.txt'\n",
    "        self.config_file = f'{CONFIG_FOLDER}/{self.name}_config.yaml'\n",
    "        self.param = self.check_param_file()\n",
    "        self.metric_param = params.metric_param\n",
    "        self.clip_grad = params.clip_grad\n",
    "        self.acc_param = False\n",
    "        if self.metric_param in ['train_acc', 'val_acc']:\n",
    "            self.acc_param = True\n",
    "    \n",
    "    def to_one_hot(self, x, length):\n",
    "        batch_size = x.size(0)\n",
    "        x_one_hot = torch.zeros(batch_size, length)\n",
    "        for i in range(batch_size):\n",
    "            x_one_hot[i, x[i]] = 1.0\n",
    "        return x_one_hot\n",
    "\n",
    "    def train_step(self):\n",
    "        self.model.train()\n",
    "        train_loss, train_acc = 0.0, 0.0\n",
    "        for _, (data, target) in enumerate(self.dataset.train_dl):\n",
    "            tar_indices = target\n",
    "            tar_one_hot = self.to_one_hot(tar_indices, 7)\n",
    "            data, target = data.to(pu), tar_one_hot.to(pu)\n",
    "\n",
    "            self.optim.zero_grad()\n",
    "            output = self.model(data)\n",
    "\n",
    "            v_mag = torch.sqrt(torch.sum(output**2, dim=2, keepdim=True)) \n",
    "            pred = v_mag.data.max(1, keepdim=True)[1].cpu().squeeze()\n",
    "            train_acc += pred.eq(tar_indices.view_as(pred)).squeeze().sum().item()\n",
    "            \n",
    "            loss = self.model.loss(output, target)\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad.clip_grad_norm_(self.model.parameters(), self.clip_grad)\n",
    "            self.optim.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(self.dataset.train_dl)\n",
    "        train_acc /= len(self.dataset.train_dl)\n",
    "\n",
    "        return train_loss, train_acc\n",
    "    \n",
    "    def validate_step(self):\n",
    "        self.model.eval()\n",
    "        val_loss, val_acc = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for _, (data, target) in enumerate(self.dataset.val_dl):\n",
    "                tar_indices = target\n",
    "                tar_one_hot = self.to_one_hot(tar_indices, 7)\n",
    "                \n",
    "                data, target = data.to(pu), tar_one_hot.to(pu)\n",
    "\n",
    "                output = self.model(data)\n",
    "                v_mag = torch.sqrt(torch.sum(output**2, dim=2, keepdim=True)) \n",
    "                pred = v_mag.data.max(1, keepdim=True)[1].cpu().squeeze()\n",
    "                val_acc += pred.eq(tar_indices.view_as(pred)).squeeze().sum().item()\n",
    "\n",
    "                loss = self.model.loss(output, target)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(self.dataset.val_dl)\n",
    "        val_acc /= len(self.dataset.val_dl)\n",
    "        return val_loss, val_acc\n",
    "    \n",
    "    def create_model(self):\n",
    "        if self.params.custom_model:\n",
    "            conv_outputs = 128 #128_Feature_map\n",
    "            num_primary_units = 8\n",
    "            primary_unit_size = 16 * 6 * 6\n",
    "            output_unit_size = 16\n",
    "            img_size = 299\n",
    "            self.model = FixCapsNet(conv_inputs= 3,\n",
    "                                conv_outputs=conv_outputs,\n",
    "                                num_primary_units=num_primary_units,\n",
    "                                primary_unit_size=primary_unit_size,\n",
    "                                output_unit_size=output_unit_size,\n",
    "                                num_classes=7,\n",
    "                                init_weights=True,mode=\"128\").to(pu)\n",
    "        else:\n",
    "            self.model = make_cnn(dataset=self.dataset.train_ds, hid_layers=self.params.hid_layers, act_fn=self.params.act_fn,\n",
    "                                max_pool=self.params.max_pool, pooling_after_layers=self.params.pool_after_layers,\n",
    "                                batch_norm=self.params.batch_norm, conv_layers=self.params.conv_layers, dropout=self.params.dropout).to(pu)\n",
    "        \n",
    "        self.optim = Adam(self.model.parameters(), lr=self.params.lr)\n",
    "        print(f'Model: {self.model}')\n",
    "        print(f'Number of classes: {self.dataset.train_ds.classes}')\n",
    "        print(f'Input image size: {self.dataset.train_ds.__getitem__(0)[0][0].shape}')\n",
    "        print(f'total number of parameters: {sum([p.numel() for p in self.model.parameters()])}')\n",
    "        \n",
    "    def plot_images(self, n_imgs):\n",
    "        fig, axes = plt.subplots(1, n_imgs, figsize=(10, 10))\n",
    "        for i in range(n_imgs):\n",
    "            ind = random.randint(0, len(self.dataset.train_ds)-1)\n",
    "            img, label = self.dataset.train_ds.__getitem__(ind)\n",
    "            img = img.numpy()\n",
    "            img = img.transpose((1, 2, 0))\n",
    "            axes.flat[i].set_title(label)\n",
    "            axes.flat[i].imshow(img)\n",
    "            axes.flat[i].axis('off')\n",
    "\n",
    "    def train(self):\n",
    "        epochs = self.params.epochs\n",
    "        epoch = 0\n",
    "        epoch = self.check_status_file()\n",
    "\n",
    "        print(f'training for {epochs - epoch} epochs....')\n",
    "        for ep in tqdm(range(epoch, epochs+1)):\n",
    "            train_loss, train_acc = self.train_step()\n",
    "            val_loss, val_acc = self.validate_step()\n",
    "\n",
    "            metric_param = {'train_loss': train_loss, 'train_acc': train_acc,\n",
    "                            'val_loss': val_loss, 'val_acc': val_acc}\n",
    "            \n",
    "            print(f'epochs: {ep}\\t{train_loss = :.4f}\\t{train_acc = :.4f}\\t{val_loss = :.4f}\\t{val_acc = :.4f}')\n",
    "            self.write_plot_data([train_loss, train_acc, val_loss, val_acc])\n",
    "            self.save_check_interval(epoch=ep, interval=1)\n",
    "            self.save_best_model(acc_param=self.acc_param, param=metric_param[self.metric_param])\n",
    "        \n",
    "        print('Finished Training....')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    def __init__(self):\n",
    "        self.name = 'model_2'\n",
    "        self.custom_model = True\n",
    "        self.conv_layers = [[128, 11, 2],\n",
    "                            [128, 3, 2],\n",
    "                            [512, 3, 2],\n",
    "                            [128, 5, 1],\n",
    "                            [128, 3, 1],\n",
    "                            [512, 3, 1]]\n",
    "                            # [512, 5, 1]]\n",
    "                            # [128, 3, 1],\n",
    "                            # [256, 3, 1],\n",
    "                            # [512, 3, 1]]\n",
    "                            # [512, 3, 1],\n",
    "                            # [512, 3, 1]]\n",
    "        self.max_pool = [2, 2]\n",
    "        self.pool_after_layers = 3\n",
    "        self.act_fn = 'relu'\n",
    "        self.batch_norm = False\n",
    "        self.dropout = None\n",
    "        self.hid_layers = [256, 256]\n",
    "        self.lr = 0.00001\n",
    "        self.epochs = 100\n",
    "        self.clip_grad = 0.5\n",
    "        self.metric_param = 'val_acc'\n",
    "        self.batch_size = 168\n",
    "        self.test_trans = tf.Compose([\n",
    "            tf.Resize(size =(299, 299)),\n",
    "            tf.ToTensor(),\n",
    "            tf.Normalize([0.5, 0.5, 0.5],\n",
    "                         [0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        self.train_trans = tf.Compose([\n",
    "            tf.Resize(size=(299, 299)),\n",
    "            tf.ToTensor(),\n",
    "            tf.Normalize([0.5, 0.5, 0.5],\n",
    "                         [0.5, 0.5, 0.5])\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: FixCapsNet(\n",
      "  (Convolution): Sequential(\n",
      "    (0): Conv2d(3, 128, kernel_size=(31, 31), stride=(2, 2), bias=False)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): FractionalMaxPool2d()\n",
      "  )\n",
      "  (CBAM): Conv_CBAM(\n",
      "    (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): Hardswish()\n",
      "    (ca): ChannelAttention(\n",
      "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "      (fc1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (relu1): ReLU()\n",
      "      (fc2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (sigmoid): Sigmoid()\n",
      "    )\n",
      "    (sa): SpatialAttention(\n",
      "      (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (sigmoid): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (primary): Primary_Caps(\n",
      "    (Caps_0): ConvUnit(\n",
      "      (Cpas): Sequential(\n",
      "        (0): Conv2d(128, 16, kernel_size=(9, 9), stride=(2, 2), groups=16)\n",
      "      )\n",
      "    )\n",
      "    (Caps_1): ConvUnit(\n",
      "      (Cpas): Sequential(\n",
      "        (0): Conv2d(128, 16, kernel_size=(9, 9), stride=(2, 2), groups=16)\n",
      "      )\n",
      "    )\n",
      "    (Caps_2): ConvUnit(\n",
      "      (Cpas): Sequential(\n",
      "        (0): Conv2d(128, 16, kernel_size=(9, 9), stride=(2, 2), groups=16)\n",
      "      )\n",
      "    )\n",
      "    (Caps_3): ConvUnit(\n",
      "      (Cpas): Sequential(\n",
      "        (0): Conv2d(128, 16, kernel_size=(9, 9), stride=(2, 2), groups=16)\n",
      "      )\n",
      "    )\n",
      "    (Caps_4): ConvUnit(\n",
      "      (Cpas): Sequential(\n",
      "        (0): Conv2d(128, 16, kernel_size=(9, 9), stride=(2, 2), groups=16)\n",
      "      )\n",
      "    )\n",
      "    (Caps_5): ConvUnit(\n",
      "      (Cpas): Sequential(\n",
      "        (0): Conv2d(128, 16, kernel_size=(9, 9), stride=(2, 2), groups=16)\n",
      "      )\n",
      "    )\n",
      "    (Caps_6): ConvUnit(\n",
      "      (Cpas): Sequential(\n",
      "        (0): Conv2d(128, 16, kernel_size=(9, 9), stride=(2, 2), groups=16)\n",
      "      )\n",
      "    )\n",
      "    (Caps_7): ConvUnit(\n",
      "      (Cpas): Sequential(\n",
      "        (0): Conv2d(128, 16, kernel_size=(9, 9), stride=(2, 2), groups=16)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (digits): Digits_Caps()\n",
      ")\n",
      "Number of classes: ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
      "Input image size: torch.Size([299, 299])\n",
      "total number of parameters: 1017618\n",
      "training dataset size: 38569\n",
      "validation dataset size: 938\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "params = Params()\n",
    "\n",
    "agent = skdi_detector(params)\n",
    "\n",
    "agent.create_model()\n",
    "print(f'training dataset size: {len(agent.dataset.train_ds)}')\n",
    "print(f'validation dataset size: {len(agent.dataset.val_ds)}')\n",
    "\n",
    "# agent.plot_images(3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training for 100 epochs....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/101 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.46 GiB (GPU 0; 5.62 GiB total capacity; 355.67 MiB already allocated; 491.62 MiB free; 388.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 315\u001b[0m, in \u001b[0;36mskdi_detector.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs....\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epoch, epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m--> 315\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_step()\n\u001b[1;32m    318\u001b[0m     metric_param \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: train_loss, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: train_acc,\n\u001b[1;32m    319\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: val_loss, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m: val_acc}\n",
      "Cell \u001b[0;32mIn[6], line 233\u001b[0m, in \u001b[0;36mskdi_detector.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    230\u001b[0m data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(pu), tar_one_hot\u001b[38;5;241m.\u001b[39mto(pu)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 233\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m v_mag \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39msum(output\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)) \n\u001b[1;32m    236\u001b[0m pred \u001b[38;5;241m=\u001b[39m v_mag\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Skin_Diseases_Detection/fixcap.py:43\u001b[0m, in \u001b[0;36mFixCapsNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 43\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConvolution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCBAM(x)\n\u001b[1;32m     45\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdigits(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprimary(x))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.46 GiB (GPU 0; 5.62 GiB total capacity; 355.67 MiB already allocated; 491.62 MiB free; 388.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
