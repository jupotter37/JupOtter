{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, board_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.board_size = board_size\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=5, stride=1, padding=2)    # First convolutional layer with input channels=1 and output channels=32\n",
    "        self.conv2 = nn.Conv2d(8, 4, kernel_size=3, stride=1, padding=1)   # Second convolutional layer with input channels=32 and output channels=64\n",
    "        self.fc1 = nn.Linear(4 * (board_size ** 2), 64)                    # Fully connected layer with input features= flatten output of conv2 which is 64*(board_size^2) and output features=256; original code was (prev)//4\n",
    "        self.fc2 = nn.Linear(64, board_size ** 2)                           # Fully connected layer with input features=256 and output features=board_size^2\n",
    "        self.activation = torch.sigmoid\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))        # Apply ReLU activation to the output of the first convolutional layer\n",
    "        x = torch.relu(self.conv2(x))        # Apply ReLU activation to the output of the second convolutional layer\n",
    "        x = x.view(x.size(0), -1)            # Reshape x into a 2D matrix with size (batch_size, -1)\n",
    "        x = torch.relu(self.fc1(x))          # Apply ReLU activation to the output of the first fully connected layer\n",
    "        x = self.fc2(x)                     # Output the final logits from the second fully connected layer\n",
    "        return self.activation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, board_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.board_size = board_size\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2)    # First convolutional layer with input channels=1 and output channels=32\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)   # Second convolutional layer with input channels=32 and output channels=64\n",
    "        self.fc1 = nn.Linear(64 * (board_size ** 2), 128)                    # Fully connected layer with input features= flatten output of conv2 which is 64*(board_size^2) and output features=256; original code was (prev)//4\n",
    "        self.fc2 = nn.Linear(128, board_size ** 2)                           # Fully connected layer with input features=256 and output features=board_size^2\n",
    "        self.activation = torch.sigmoid\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))        # Apply ReLU activation to the output of the first convolutional layer\n",
    "        x = torch.relu(self.conv2(x))        # Apply ReLU activation to the output of the second convolutional layer\n",
    "        x = x.view(x.size(0), -1)            # Reshape x into a 2D matrix with size (batch_size, -1)\n",
    "        x = torch.relu(self.fc1(x))          # Apply ReLU activation to the output of the first fully connected layer\n",
    "        x = self.fc2(x)                     # Output the final logits from the second fully connected layer\n",
    "        return self.activation(x)\n",
    "\n",
    "class RenjuGame:\n",
    "    def __init__(self, board_size):\n",
    "        self.board_size = board_size\n",
    "        self.board = np.zeros((board_size, board_size), dtype=np.int32)\n",
    "        self.current_player = 1                                             # current_player = 1 or 2\n",
    "        self.winner = None\n",
    "        self.last_move_col = 0\n",
    "        self.last_move_row = 0\n",
    "\n",
    "    def is_valid_move(self, row, col):\n",
    "        if row < 0 or row >= self.board_size or col < 0 or col >= self.board_size:\n",
    "            return False\n",
    "        return self.board[row][col] == 0\n",
    "\n",
    "    def make_move(self, row, col):\n",
    "        if self.is_valid_move(row, col):\n",
    "            self.board[row][col] = self.current_player\n",
    "            self.check_winner()\n",
    "            self.current_player = 3 - self.current_player\n",
    "            self.last_move_row = row\n",
    "            self.last_move_col = col\n",
    "        else:\n",
    "            self.current_player = 3 - self.current_player\n",
    "\n",
    "\n",
    "    def check_winner(self):\n",
    "        directions = [(0, 1), (1, 0), (1, 1), (-1, 1)]  # horizontal, vertical, diagonal, anti-diagonal\n",
    "        for dr, dc in directions:\n",
    "            for row in range(self.board_size):\n",
    "                for col in range(self.board_size):\n",
    "                    if self.board[row][col] != 0:\n",
    "                        color = self.board[row][col]\n",
    "                        win = True\n",
    "                        for i in range(5):\n",
    "                            if row + i * dr < 0 or row + i * dr >= self.board_size or col + i * dc < 0 or col + i * dc >= self.board_size or self.board[row + i * dr][col + i * dc] != color:\n",
    "                                win = False\n",
    "                                break\n",
    "                        if win:\n",
    "                            self.winner = color\n",
    "                            return\n",
    " \n",
    "    \n",
    "\n",
    "    def is_game_over(self):\n",
    "        return np.count_nonzero(self.board) == self.board_size * self.board_size or self.winner is not None\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.board.copy()\n",
    "\n",
    "    def get_invalid_moves(self):\n",
    "        invalid_moves = []\n",
    "        for row in range(self.board_size):\n",
    "            for col in range(self.board_size):\n",
    "                if not self.is_valid_move(row, col):\n",
    "                    invalid_moves.append(row * self.board_size + col)\n",
    "                    #valid_moves.append((row, col))\n",
    "        return np.array(invalid_moves)\n",
    "\n",
    "    def print_board(self):\n",
    "        for row in self.board:\n",
    "            print(row)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n",
      "epoch 0\n",
      "0.001484006643295288 49.64668273925781 0.49646682 23 2\n",
      "[1 1 1 1 2 2 1]\n",
      "[1 2 1 1 1 1 2]\n",
      "[2 1 1 1 2 1 2]\n",
      "[0 2 2 2 2 2 2]\n",
      "[1 2 0 1 0 2 1]\n",
      "[2 2 2 1 1 2 2]\n",
      "[1 1 2 1 2 1 2]\n",
      "\n",
      "epoch 400\n",
      "0.001684549976797665 50.34335708618164 0.5034336 18 2\n",
      "[1 1 0 1 1 0 1]\n",
      "[2 0 2 1 2 0 0]\n",
      "[0 1 0 2 2 1 1]\n",
      "[2 0 1 2 2 0 1]\n",
      "[1 0 2 1 2 2 2]\n",
      "[1 2 2 1 1 0 2]\n",
      "[1 2 2 1 0 2 0]\n",
      "\n",
      "epoch 800\n",
      "0.0019396367398175325 0.0 0.4890086 23 1\n",
      "[1 2 2 1 2 1 2]\n",
      "[2 2 2 1 1 0 2]\n",
      "[2 1 2 1 2 1 1]\n",
      "[2 2 1 1 2 1 1]\n",
      "[1 2 2 1 2 2 2]\n",
      "[1 0 1 1 1 2 2]\n",
      "[1 1 1 1 0 2 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "dtype = torch.float32\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "\n",
    "# Create game and network instances\n",
    "board_size = 7\n",
    "game = RenjuGame(board_size)\n",
    "net = PolicyNetwork(board_size).to(device)\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-2)\n",
    "\n",
    "loss_f = nn.BCELoss()\n",
    "# Training loop\n",
    "loss_mean_lst = []\n",
    "\n",
    "for i in range(10000):  # Number of games to play\n",
    "    game.__init__(board_size)  # Reset the game\n",
    "    action_prob_prev = None\n",
    "    count = 0\n",
    "    loss_lst = []\n",
    "    while not game.is_game_over():\n",
    "        count+=1\n",
    "        # Convert the current game state to a tensor\n",
    "        state = torch.Tensor(game.get_state()).unsqueeze(0).unsqueeze(0).to(device)  # Adds extra dimensions for batch and channels\n",
    "\n",
    "        # Get the action probabilities from the network\n",
    "        action_probs = net(state).view(game.board_size, game.board_size)\n",
    "\n",
    "        # Select an action\n",
    "        action_probs = action_probs.flatten()\n",
    "        action_probs[game.get_invalid_moves()] = -1\n",
    "        action = torch.argmax(action_probs) #torch.multinomial(action_probs.flatten(), 1)[0]\n",
    "        \n",
    "        \n",
    "        # Convert the action back to row, col\n",
    "        row, col = action // game.board_size, action % game.board_size\n",
    "\n",
    "        # Make the move\n",
    "        game.make_move(row.item(), col.item())\n",
    "\n",
    "        #for train\n",
    "        action_prob = action_probs.flatten()[action, None]\n",
    "\n",
    "        if action_prob_prev is not None:\n",
    "            optimizer.zero_grad()\n",
    "            # Use a separate copy of action_prob for the loss calculation\n",
    "            action_prob_for_loss = action_prob.clone().detach().requires_grad_(True)\n",
    "            loss = torch.nn.ReLU()(action_prob_prev - action_prob_for_loss) #loss_f(action_prob_prev, action_prob_for_loss)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            loss_lst.append(loss.item())\n",
    "            \n",
    "            # Store action_probs\n",
    "        action_prob_prev = action_prob.clone().detach().requires_grad_(True).to(device)\n",
    "        \n",
    "        # Let the opponent (which is also the network) make a move\n",
    "        \n",
    "        if not game.is_game_over():\n",
    "            # Opponent's turn\n",
    "            with torch.no_grad():\n",
    "                state = torch.Tensor(game.get_state()).unsqueeze(0).unsqueeze(0).to(device)\n",
    "                action_probs = net(state).view(game.board_size, game.board_size)\n",
    "                action_probs = action_probs.flatten()\n",
    "                action_probs[game.get_invalid_moves()] = 0\n",
    "                action = torch.argmax(action_probs) #torch.multinomial(action_probs.flatten(), 1)[0]\n",
    "                \n",
    "                action = torch.multinomial(action_probs.flatten(), 1)[0]\n",
    "                row, col = action // game.board_size, action % game.board_size\n",
    "                game.make_move(row.item(), col.item())\n",
    "\n",
    "    # Now, the game is over, so we can compute the reward\n",
    "    reward = 1 if game.winner == 1 else 0  # Assuming the network plays as player 1\n",
    "    if game.winner == None:\n",
    "        reward = 0.5\n",
    "    \n",
    "    # Update the network\n",
    "    optimizer.zero_grad()\n",
    "    loss = 100*torch.nn.ReLU()(action_prob_prev - torch.tensor([reward], dtype=dtype).to(device)) #loss = loss_f(action_prob, torch.tensor([reward], dtype=dtype).to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i%400 ==0:\n",
    "        print('epoch', i)\n",
    "        print(np.mean(loss_lst), loss.item(), action_prob.detach()[0].cpu().numpy() ,count, game.winner)\n",
    "        game.print_board()\n",
    "        torch.save(net.state_dict(), 'policy_net.pth')\n",
    "    loss_mean_lst.append(np.sum(loss_lst) + loss.item())\n",
    "\n",
    "# Save the trained network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 2 2 0 2 0]\n",
      "[2 0 2 2 0 2 2]\n",
      "[0 0 0 0 2 0 0]\n",
      "[0 2 2 2 2 0 0]\n",
      "[0 0 0 0 0 2 0]\n",
      "[2 2 2 2 0 0 2]\n",
      "[2 2 0 2 2 2 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "game.print_board()\n",
    "torch.nn.Re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_move(game, net):\n",
    "    net.eval()\n",
    "    state = torch.Tensor(game.get_state()).unsqueeze(0).unsqueeze(0).to(device) \n",
    "    action_probs = net(state).view(game.board_size, game.board_size)\n",
    "    action = torch.multinomial(action_probs.flatten(), 1)\n",
    "    row, col = action // game.board_size, action % game.board_size\n",
    "    game.make_move(row.item(), col.item())\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your move:  4,2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0]\n",
      "[0 0 0 2 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 2 0 0]\n",
      "[0 0 1 1 1 1 1 0]\n",
      "[0 2 0 0 0 0 0 0]\n",
      "[0 0 0 2 0 0 0 0]\n",
      "[2 0 0 0 0 0 0 0]\n",
      "\n",
      "winner is:  1\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "game.__init__(board_size)  # Reset the game\n",
    "\n",
    "while not game.is_game_over():\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    user_input = input(\"Please enter your move: \")\n",
    "    numbers = user_input.split(',')\n",
    "    row = int(numbers[0])\n",
    "    col = int(numbers[1])\n",
    "\n",
    "    game.make_move(row,col)\n",
    "    cpu_move(game,net)\n",
    "    game.print_board()\n",
    "\n",
    "print('winner is: ',game.winner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "167.86912536621094 -1\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 3; 31.74 GiB total capacity; 1.18 GiB already allocated; 3.62 MiB free; 1.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(game\u001b[38;5;241m.\u001b[39mget_state())\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Adds extra dimensions for batch and channels\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Get the action probabilities from the network\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m action_probs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(game\u001b[38;5;241m.\u001b[39mboard_size, game\u001b[38;5;241m.\u001b[39mboard_size)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Select an action\u001b[39;00m\n\u001b[1;32m     28\u001b[0m action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(action_probs\u001b[38;5;241m.\u001b[39mflatten(), \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/ohpc/pub/python/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[108], line 14\u001b[0m, in \u001b[0;36mPolicyNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))        \u001b[38;5;66;03m# Apply ReLU activation to the output of the second convolutional layer\u001b[39;00m\n\u001b[1;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)            \u001b[38;5;66;03m# Reshape x into a 2D matrix with size (batch_size, -1)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)          \u001b[38;5;66;03m# Apply ReLU activation to the output of the first fully connected layer\u001b[39;00m\n\u001b[1;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)                     \u001b[38;5;66;03m# Output the final logits from the second fully connected layer\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(x)\n",
      "File \u001b[0;32m/opt/ohpc/pub/python/3.11.2/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/ohpc/pub/python/3.11.2/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 3; 31.74 GiB total capacity; 1.18 GiB already allocated; 3.62 MiB free; 1.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "dtype = torch.float32\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create game and network instances\n",
    "board_size = 9\n",
    "game = RenjuGame(board_size)\n",
    "net = PolicyNetwork(board_size).to(device)\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-2)\n",
    "\n",
    "loss_f = nn.BCELoss()\n",
    "# Training loop\n",
    "\n",
    "for i in range(100):  # Number of games to play\n",
    "    game.__init__(board_size)  # Reset the game\n",
    "    action_lst = []\n",
    "    while not game.is_game_over():\n",
    "        # Convert the current game state to a tensor\n",
    "        state = torch.Tensor(game.get_state()).unsqueeze(0).unsqueeze(0).to(device)  # Adds extra dimensions for batch and channels\n",
    "\n",
    "        # Get the action probabilities from the network\n",
    "        action_probs = net(state).view(game.board_size, game.board_size)\n",
    "\n",
    "        # Select an action\n",
    "        action = torch.multinomial(action_probs.flatten(), 1)\n",
    "\n",
    "        # Convert the action back to row, col\n",
    "        row, col = action // game.board_size, action % game.board_size\n",
    "\n",
    "        # Make the move\n",
    "        game.make_move(row.item(), col.item())\n",
    "\n",
    "        #for train\n",
    "        action_prob = action_probs.flatten()[action, None]\n",
    "\n",
    "        # Store action_probs\n",
    "        action_lst.append( action_prob.clone()) #.detach().requires_grad_(True).to(device)\n",
    "        \n",
    "        # Let the opponent (which is also the network) make a move\n",
    "        if not game.is_game_over():\n",
    "            # Opponent's turn\n",
    "            state = torch.Tensor(game.get_state()).unsqueeze(0).unsqueeze(0).to(device)\n",
    "            action_probs = net(state).view(game.board_size, game.board_size)\n",
    "            action = torch.multinomial(action_probs.flatten(), 1)\n",
    "            row, col = action // game.board_size, action % game.board_size\n",
    "            game.make_move(row.item(), col.item())\n",
    "\n",
    "    # Now, the game is over, so we can compute the reward\n",
    "    reward = 1 if game.winner == 1 else -1  # Assuming the network plays as player 1\n",
    "\n",
    "    # Update the network\n",
    "    optimizer.zero_grad()\n",
    "    action_tensor = torch.concat(action_lst).to(device)\n",
    "    # Update the network to increase the probability of actions that led to winning, and decrease the probability of actions that led to losing\n",
    "    loss = torch.sum(torch.log(action_tensor) * reward )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i%5 ==0:\n",
    "        print('epoch', i)\n",
    "        print(loss.item(), reward)\n",
    "\n",
    "# Save the trained network\n",
    "#torch.save(net.state_dict(), 'policy_net.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5002]], device='cuda:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 0 0 0]\n",
      "[0 1 1 1 1 0 0 0 0 0]\n",
      "[0 2 2 2 2 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bchild = testnode.select_best_child()\n",
    "\n",
    "#print(bchild.visits)\n",
    "#print(testnode.visits)\n",
    "#print(testnode.children[11].state.winner)\n",
    "#print(testnode.children[11].player)\n",
    "\n",
    "#print(testnode.children[11].wins)\n",
    "#testnode.children[11].state.print_board()\n",
    "\n",
    "testnode.select_best_child().state.print_board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
