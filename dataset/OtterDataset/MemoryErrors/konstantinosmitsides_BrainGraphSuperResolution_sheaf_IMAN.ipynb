{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Sequential, Linear, ReLU, Sigmoid, Tanh, Dropout, Upsample\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import NNConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import BatchNorm\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.distributions import normal, kl\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, GAE, VGAE, InnerProductDecoder, ARGVA\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from MatrixVectorizer import MatrixVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global variables\n",
    "N_SUBJECTS = 167\n",
    "\n",
    "N_LR_NODES = 160\n",
    "\n",
    "N_HR_NODES = 268\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "N_LR_NODES_F = int(N_LR_NODES * (N_LR_NODES-1) / 2)\n",
    "N_HR_NODES_F = int(N_HR_NODES * (N_HR_NODES-1) / 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SheafConvLayer(nn.Module):\n",
    "    def __init__(self, n_nodes, d, f_in, f_out=None):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.n_nodes = n_nodes\n",
    "        self.f_in = f_in\n",
    "        self.f_out = f_out\n",
    "        # random init weight matrices\n",
    "        if f_out is None:\n",
    "            f_out = f_in \n",
    "        self.weight1 = nn.Parameter(torch.randn((d, d), device=DEVICE))\n",
    "        self.weight2 = nn.Parameter(torch.randn((f_in, f_out), device=DEVICE))\n",
    "        self.edge_weights = nn.Parameter(torch.randn((n_nodes, n_nodes, d, 2*d), device=DEVICE))\n",
    "\n",
    "\n",
    "    def forward(self, X, adj):\n",
    "        kron_prod = torch.kron(torch.eye(self.n_nodes).to(DEVICE), self.weight1)\n",
    "        L = self.sheaf_laplacian(X, adj)\n",
    "        if self.f_out is None:\n",
    "            return X - F.elu(L @ kron_prod @ X @ self.weight2), L\n",
    "        else:\n",
    "            return F.elu(L @ kron_prod @ X @ self.weight2), L\n",
    "\n",
    "\n",
    "    def sheaf_laplacian(self, X, adj, epsilon=1e-6):\n",
    "        X_reshaped = X.reshape(self.n_nodes, self.d, -1)\n",
    "        idx_pairs = torch.cartesian_prod(torch.arange(self.n_nodes), torch.arange(self.n_nodes))\n",
    "        all_stacked_features = X_reshaped[idx_pairs].reshape(self.n_nodes, self.n_nodes, 2*self.d, -1).to(DEVICE)\n",
    "        lin_trans = F.elu(torch.matmul(self.edge_weights, all_stacked_features))\n",
    "        inner_transpose = torch.transpose(lin_trans, -1, -2)\n",
    "        L_v = -1 * torch.matmul(lin_trans, torch.transpose(inner_transpose, 0, 1))\n",
    "        row_cond = torch.isclose(torch.sum(adj, dim=1), torch.zeros_like(torch.sum(adj, dim=1)))\n",
    "        col_cond = torch.isclose(torch.sum(adj, dim=0), torch.zeros_like(torch.sum(adj, dim=0)))\n",
    "        adj_row_weights = adj / (torch.sum(adj, dim=1)[:, None] + epsilon)\n",
    "        adj_col_weights = adj / (torch.sum(adj, dim=0)[:, None] + epsilon)\n",
    "        # adj_col_weights = torch.where(col_cond[None, :], 0., adj / torch.sum(adj, dim=0)[None, :])\n",
    "        adj_weights = torch.maximum(adj_row_weights * adj_col_weights, torch.zeros_like(adj_row_weights))\n",
    "\n",
    "        adj_diag_weights = adj_row_weights ** 2\n",
    "        diag_blocks = torch.sum(adj_diag_weights[:, :, None, None] * torch.matmul(lin_trans, inner_transpose), dim=1)\n",
    "        L_v[range(self.n_nodes), range(self.n_nodes)] = diag_blocks\n",
    "        return L_v.view(-1, self.n_nodes * self.d)\n",
    "        ### NOTE IGNORE MATRIX NORMALISATION FOR NOW #####\n",
    "        # inv_root_diag_blocks = torch.pow(diag_blocks+epsilon, -1/2)\n",
    "        # normalise_mat = torch.block_diag(*inv_root_diag_blocks)\n",
    "\n",
    "        # return normalise_mat @ L_v.view(-1, self.n_nodes * self.d) @ normalise_mat\n",
    "        ################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SheafAligner(nn.Module):\n",
    "    \n",
    "    def __init__(self, d, f):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d = d\n",
    "        self.f = f\n",
    "\n",
    "        self.sheafconv1 = SheafConvLayer(N_LR_NODES, d, f)\n",
    "        self.batchnorm1 = BatchNorm(f)\n",
    "\n",
    "        self.sheafconv2 = SheafConvLayer(N_LR_NODES, d, f)\n",
    "        self.batchnorm2 = BatchNorm(f)\n",
    "\n",
    "        self.sheafconv3 = SheafConvLayer(N_LR_NODES, d, f)\n",
    "        self.batchnorm3 = BatchNorm(f)\n",
    "\n",
    "    def forward(self, X, adj):\n",
    "\n",
    "        x1, L1 = self.sheafconv1(X, adj)\n",
    "        x1 = F.sigmoid(self.batchnorm1(x1))\n",
    "        x1 = F.dropout(x1, training=self.training)\n",
    "\n",
    "        mean_x1 = x1.reshape(N_LR_NODES, self.d, self.f).mean(dim=-1)\n",
    "        adj1 = torch.matmul(mean_x1[:,None, None, :], L1.reshape(N_LR_NODES, N_LR_NODES, self.d, self.d))\n",
    "        adj1 = torch.matmul(adj1, mean_x1[None, :, :, None])\n",
    "        adj1 = F.sigmoid(adj1.squeeze())\n",
    "\n",
    "        x2, L2 = self.sheafconv2(x1, adj1)\n",
    "        x2 = F.sigmoid(self.batchnorm2(x2))\n",
    "        x2 = F.dropout(x2, training=self.training)\n",
    "        \n",
    "        mean_x2 = x2.reshape(N_LR_NODES, self.d, self.f).mean(dim=-1)\n",
    "        adj2 = torch.matmul(mean_x2[:,None, None, :], L2.reshape(N_LR_NODES, N_LR_NODES, self.d, self.d))\n",
    "        adj2 = torch.matmul(adj2, mean_x2[None, :, :, None])\n",
    "        adj2 = F.sigmoid(adj2.squeeze())\n",
    "\n",
    "        x3, L3 = self.sheafconv3(x2, adj2)\n",
    "        x3 = F.sigmoid(self.batchnorm3(x3))\n",
    "\n",
    "        mean_x3 = x3.reshape(N_LR_NODES, self.d, self.f).mean(dim=-1)\n",
    "        adj3 = torch.matmul(mean_x3[:,None, None, :], L3.reshape(N_LR_NODES, N_LR_NODES, self.d, self.d))\n",
    "        adj3 = torch.matmul(adj3, mean_x3[None, :, :, None])\n",
    "        adj3 = (adj3 + torch.t(adj3)) / 2 # to ensure the matrix is symmetric\n",
    "        adj3 = F.tanh(F.relu(adj3)) # to ensure there is sparsity\n",
    "        \n",
    "        return x3, adj3\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SheafGenerator(nn.Module):\n",
    "    def __init__(self, d, f):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d = d\n",
    "        self.f = f\n",
    "        \n",
    "        self.sheafconv1 = SheafConvLayer(N_LR_NODES, d, f)\n",
    "        self.batchnorm1 = BatchNorm(f)\n",
    "\n",
    "        self.sheafconv2 = SheafConvLayer(N_LR_NODES, d, f)\n",
    "        self.batchnorm2 = BatchNorm(f)\n",
    "\n",
    "        self.sheafconv3 = SheafConvLayer(N_LR_NODES, d, f, N_HR_NODES)\n",
    "        self.batchnorm3 = BatchNorm(N_HR_NODES)\n",
    "\n",
    "        self.out_mat = nn.Parameter(torch.randn((N_LR_NODES, 2*N_LR_NODES), device=DEVICE))\n",
    "\n",
    "        self.out_sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, X, adj):\n",
    "        x1, L1 = self.sheafconv1(X, adj) # returns (d*lr_n) * f\n",
    "        x1 = F.sigmoid(self.batchnorm1(x1))\n",
    "        x1 = F.dropout(x1, p=0.1, training=self.training)\n",
    "\n",
    "        mean_x1 = x1.reshape(N_LR_NODES, self.d, self.f).mean(dim=-1)\n",
    "        adj1 = torch.matmul(mean_x1[:,None, None, :], L1.reshape(N_LR_NODES, N_LR_NODES, self.d, self.d))\n",
    "        adj1 = torch.matmul(adj1, mean_x1[None, :, :, None])\n",
    "        adj1 = F.sigmoid(adj1.squeeze())\n",
    "\n",
    "        x2, L2 = self.sheafconv2(x1, adj1) # returns (d*lr_n) * f\n",
    "        x2 = F.sigmoid(self.batchnorm2(x2))\n",
    "        x2 = F.dropout(x2, p=0.1, training=self.training)\n",
    "\n",
    "        mean_x2 = x2.reshape(N_LR_NODES, self.d, self.f).mean(dim=-1)\n",
    "        adj2 = torch.matmul(mean_x2[:,None, None, :], L2.reshape(N_LR_NODES, N_LR_NODES, self.d, self.d))\n",
    "        adj2 = torch.matmul(adj2, mean_x2[None, :, :, None])\n",
    "        adj2 = F.sigmoid(adj2.squeeze())\n",
    "\n",
    "        x3, L3 = self.sheafconv3(x2, adj2) # returns (d*lr_n) * hr_n\n",
    "        x3 = F.sigmoid(self.batchnorm3(x3))\n",
    "\n",
    "        x3 = torch.matmul(self.out_mat, x3)\n",
    "        adj3 = torch.t(x3) @ adj2 @ x3\n",
    "        adj3 = (adj3 + torch.t(adj3)) / 2 # to ensure the matrix is symmetric\n",
    "        adj3 = F.tanh(F.relu(adj3)) # to ensure there is sparsity\n",
    "\n",
    "        return adj3\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SheafDiscriminator(nn.Module):\n",
    "    def __init__(self, d, f):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d = d\n",
    "        self.f = f\n",
    "\n",
    "        self.sheafconv1 = SheafConvLayer(N_HR_NODES, d, f)\n",
    "        self.sheafconv2 = SheafConvLayer(N_HR_NODES, d, f, 1)\n",
    "        self.out = torch.nn.Linear(2*N_HR_NODES, 1)\n",
    "\n",
    "    def forward(self, X, adj):\n",
    "        x1, L1 = self.sheafconv1(X, adj)\n",
    "        x1 = F.sigmoid(x1)\n",
    "        x1 = F.dropout(x1, p=0.1, training=self.training)\n",
    "\n",
    "        mean_x1 = x1.reshape(N_HR_NODES, self.d, self.f).mean(dim=-1)\n",
    "        adj1 = torch.matmul(mean_x1[:,None, None, :], L1.reshape(N_HR_NODES, N_HR_NODES, self.d, self.d))\n",
    "        adj1 = torch.matmul(adj1, mean_x1[None, :, :, None])\n",
    "        adj1 = F.sigmoid(adj1.squeeze())\n",
    "\n",
    "\n",
    "        x2, L2 = self.sheafconv2(x1, adj)\n",
    "        x2 = F.sigmoid(x2).flatten()\n",
    "        x3 = F.sigmoid(self.out(x2))\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_coor(input, target, epsilon=1e-7):\n",
    "    vx = input - torch.mean(input, dim=(1, 2))[:, None, None]\n",
    "    vy = target - torch.mean(target, dim=(1, 2))[:, None, None]\n",
    "    cost = torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)+epsilon) * torch.sqrt(torch.sum(vy ** 2)+epsilon)+epsilon)\n",
    "    return cost\n",
    "\n",
    "def GT_loss(target, predicted):\n",
    "\n",
    "    # l1_loss\n",
    "    l1_loss = torch.nn.L1Loss()\n",
    "    loss_pix2pix = l1_loss(target, predicted)\n",
    "\n",
    "    # topological_loss\n",
    "    target_n = target.detach().cpu().clone().numpy()\n",
    "    predicted_n = predicted.detach().cpu().clone().numpy()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    topo_loss = []\n",
    "    \n",
    "\n",
    "    for i in range(len(target_n)):\n",
    "\n",
    "        cur_target = target_n[i]\n",
    "        cur_predicted = predicted_n[i]\n",
    "\n",
    "        target_t = eigen_centrality(cur_target)\n",
    "        real_topology = torch.tensor(target_t[0])\n",
    "        predicted_t = eigen_centrality(cur_predicted)\n",
    "        fake_topology = torch.tensor(predicted_t[0])\n",
    "        topo_loss.append(l1_loss(real_topology, fake_topology))\n",
    "\n",
    "    topo_loss = torch.sum(torch.stack(topo_loss))\n",
    "\n",
    "    pc_loss = pearson_coor(target, predicted).to(DEVICE)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    G_loss = loss_pix2pix + (1 - pc_loss) + topo_loss\n",
    "\n",
    "    return G_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# put it back into a 2D symmetric array\n",
    "\n",
    "\n",
    "def topological_measures(data):\n",
    "    # ROI is the number of brain regions (i.e.,35 in our case)\n",
    "    ROI = 160\n",
    "\n",
    "    topology = []\n",
    "\n",
    "\n",
    "\n",
    "    # A = to_2d(data)\n",
    "    np.fill_diagonal(data, 0)\n",
    "\n",
    "    # create a graph from similarity matrix\n",
    "    G = nx.from_numpy_matrix(np.absolute(data))\n",
    "    U = G.to_undirected()\n",
    "\n",
    "    # Centrality #\n",
    "\n",
    "    # compute closeness centrality and transform the output to vector\n",
    "    cc = nx.closeness_centrality(U, distance=\"weight\")\n",
    "    closeness_centrality = np.array([cc[g] for g in U])\n",
    "    # compute betweeness centrality and transform the output to vector\n",
    "    # bc = nx.betweenness_centrality(U, weight='weight')\n",
    "    # bc = (nx.betweenness_centrality(U))\n",
    "    betweenness_centrality = np.array([cc[g] for g in U])\n",
    "    # # compute egeinvector centrality and transform the output to vector\n",
    "    ec = nx.eigenvector_centrality_numpy(U)\n",
    "    eigenvector_centrality = np.array([ec[g] for g in U])\n",
    "\n",
    "\n",
    "    topology.append(closeness_centrality)  # 0\n",
    "    topology.append(betweenness_centrality)  # 1\n",
    "    topology.append(eigenvector_centrality)  # 2\n",
    "\n",
    "    return topology\n",
    "# put it back into a 2D symmetric array\n",
    "\n",
    "def eigen_centrality(data):\n",
    "    # ROI is the number of brain regions (i.e.,35 in our case)\n",
    "    ROI = 160\n",
    "\n",
    "    topology_eigen = []\n",
    "\n",
    "    G = nx.from_numpy_array(np.absolute(data))\n",
    "    U = G.to_undirected()\n",
    "\n",
    "    # A = to_2d(data)\n",
    "    np.fill_diagonal(data, 0)\n",
    "\n",
    "    # create a graph frL2\n",
    "    # # compute egeinvector centrality and transform the output to vector\n",
    "    ec = nx.eigenvector_centrality_numpy(U)\n",
    "    \n",
    "    eigenvector_centrality = np.array([ec[g] for g in U])\n",
    "\n",
    "\n",
    "\n",
    "    topology_eigen.append(eigenvector_centrality)  # 2\n",
    "\n",
    "    return topology_eigen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def unfreeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempt Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preparation import load_data_tensor\n",
    "\n",
    "lr_train, lr_test, hr_train = load_data_tensor(\"dgl-icl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_X_dim1 = torch.load('model_autoencoder/encode_lr_1.pt')\n",
    "lr_X_dim2 = torch.load('model_autoencoder/encode_lr_2.pt')\n",
    "hr_X_dim1 = torch.load('model_autoencoder/encode_hr_1.pt')\n",
    "hr_X_dim2 = torch.load('model_autoencoder/encode_hr_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_X_all = torch.empty((167, 320, 32))\n",
    "for i in range(len(lr_X_dim1)):\n",
    "    a, b = lr_X_dim1[i], lr_X_dim2[i]\n",
    "    lr_X_all[i] = torch.cat([a, b], dim=-1).view(-1, a.shape[-1])\n",
    "\n",
    "hr_X_all = torch.empty((167, 536, 32))\n",
    "for i in range(len(hr_X_dim1)):\n",
    "    a, b = hr_X_dim1[i], hr_X_dim2[i]\n",
    "    hr_X_all[i] = torch.cat([a, b], dim=-1).view(-1, a.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2 # number of dimensions in each node\n",
    "f = 32 # length of node encoding\n",
    "BATCHSIZE = 1\n",
    "N_TRAIN_SAMPLES = len(lr_train)\n",
    "EPOCHS = 30\n",
    "\n",
    "\n",
    "aligner = SheafAligner(d, f).to(DEVICE)\n",
    "generator = SheafGenerator(d, f).to(DEVICE)\n",
    "discriminator = SheafDiscriminator(d, f).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2445361"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in aligner.parameters()) + sum(p.numel() for p in generator.parameters()) + sum(p.numel() for p in discriminator.parameters()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aligner_optimizer = torch.optim.AdamW(aligner.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "generator_optimizer = torch.optim.AdamW(list(aligner.parameters()) + list(generator.parameters()), lr=0.001, betas=(0.5, 0.999))\n",
    "# generator_optimizer = torch.optim.AdamW(generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "discriminator_optimizer = torch.optim.AdamW(discriminator.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
    "\n",
    "adversarial_loss = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:28,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7965999516898287, generator_loss = 3.248340706285146, discriminator = 0.7017677319263984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:29,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.795399045159003, generator_loss = 3.2420367845093234, discriminator = 0.6954997767231421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:30,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7946944914892048, generator_loss = 3.2320239408576374, discriminator = 0.7060076954835903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:27,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7943154891094047, generator_loss = 3.2412356868407497, discriminator = 0.6986124486980324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:27,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7940994900857618, generator_loss = 3.236818812950267, discriminator = 0.6986538696431828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:27,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7939687406945372, generator_loss = 3.2583425432554076, discriminator = 0.6936831602793254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:27,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7938886256275063, generator_loss = 3.2553153082928072, discriminator = 0.707389393609441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:27,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7938321603986318, generator_loss = 3.2445529628231666, discriminator = 0.7056346560666661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:27,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7937915210952302, generator_loss = 3.2354528117254637, discriminator = 0.7104832041049431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:28,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7937619768216938, generator_loss = 3.2482992944535196, discriminator = 0.6995087708541733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:28,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7937398613569979, generator_loss = 3.240412697849498, discriminator = 0.7070539797137597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:28,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7937230867540052, generator_loss = 3.245352510592519, discriminator = 0.7069193089079714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:29,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7937108210460868, generator_loss = 3.2279776563476474, discriminator = 0.6955548592670235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:28,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7937004180725463, generator_loss = 3.247601592462558, discriminator = 0.7114887594462869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:29,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7936909819791417, generator_loss = 3.2283712816741468, discriminator = 0.6942692263397628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:28,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7936854676572148, generator_loss = 3.2243179143834784, discriminator = 0.717961979126502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:28,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7936796774407346, generator_loss = 3.245733324489532, discriminator = 0.7009783854741536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:29,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7936732883224944, generator_loss = 3.23987036874942, discriminator = 0.7022154052814323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:28,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.79366909743783, generator_loss = 3.2322180630136206, discriminator = 0.7037931345180123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:28,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7936656378700347, generator_loss = 3.252268266028611, discriminator = 0.7014898832686647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:28,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7936611828689804, generator_loss = 3.2624085676102395, discriminator = 0.6998482285859342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:29,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7936598683545689, generator_loss = 3.258113860755962, discriminator = 0.6992874559528099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:28,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.793655875200283, generator_loss = 3.2425749328967868, discriminator = 0.7007992210502396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:28,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7936541812862464, generator_loss = 3.2397737062551832, discriminator = 0.7021619845293239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:28,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7936508352171161, generator_loss = 3.237967924829353, discriminator = 0.7061702416328612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:28,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7936492901362345, generator_loss = 3.2300402563421127, discriminator = 0.70190732314915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:28,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7936463331034084, generator_loss = 3.237903223630535, discriminator = 0.7085740248600166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:29,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7936441776995173, generator_loss = 3.241238585875044, discriminator = 0.7045286287090735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:29,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7936414005513677, generator_loss = 3.2406666709209446, discriminator = 0.6925859258560363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "167it [02:29,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 166: align_loss = 0.7936385648693153, generator_loss = 3.248549182822229, discriminator = 0.6992024600862743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aligner.train()\n",
    "generator.train()\n",
    "discriminator.train()\n",
    "    \n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    alignment_loss_ls = []\n",
    "    generator_loss_ls = []\n",
    "    disciriminator_loss_ls = []\n",
    "\n",
    "    for i, sample in tqdm(enumerate(zip(lr_X_all, lr_train, hr_X_all, hr_train))):\n",
    "\n",
    "        # aligner_optimizer.zero_grad()\n",
    "        generator_optimizer.zero_grad()\n",
    "        discriminator_optimizer.zero_grad()\n",
    "\n",
    "        X_lr, adj_lr, X_hr, adj_hr = sample\n",
    "\n",
    "        aligned_X_lr, aligned_adj_lr = aligner(X_lr.to(DEVICE), adj_lr.to(DEVICE))\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        hr_mean = torch.mean(X_hr)\n",
    "        hr_std = torch.std(X_hr)\n",
    "\n",
    "        adj_hr_sampled = torch.normal(hr_mean, hr_std, size=(N_LR_NODES, N_LR_NODES)).to(DEVICE)\n",
    "        # hr_X_sampled = torch.Tensor(MatrixVectorizer().anti_vectorize(hr_X_sampled, N_HR_NODES))\n",
    "\n",
    "\n",
    "        alignment_loss = torch.abs(F.kl_div(F.softmax(adj_hr_sampled, dim=-1), F.softmax(aligned_adj_lr, dim=-1), None, None, 'sum'))\n",
    "        alignment_loss = alignment_loss / 1000\n",
    "\n",
    "        alignment_loss_ls.append(alignment_loss.detach().item())\n",
    "\n",
    "        # generate hr adjacency\n",
    "        generated_adj_hr = generator(aligned_X_lr.to(DEVICE), aligned_adj_lr.to(DEVICE))\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        freeze_model(generator)\n",
    "        freeze_model(aligner)\n",
    "        unfreeze_model(discriminator)\n",
    "\n",
    "        d_real = discriminator(X_hr.to(DEVICE), adj_hr.to(DEVICE))\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        d_fake = discriminator(X_hr.to(DEVICE), generated_adj_hr.to(DEVICE))\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        d_real_loss = adversarial_loss(d_real, torch.ones_like(d_real, requires_grad=False))\n",
    "        torch.cuda.empty_cache()\n",
    "        d_fake_loss = adversarial_loss(d_fake, torch.zeros_like(d_fake, requires_grad=False))\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        d_loss.backward(retain_graph=True)\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        disciriminator_loss_ls.append(d_loss.detach().item())\n",
    "\n",
    "\n",
    "        unfreeze_model(generator)\n",
    "        unfreeze_model(aligner)\n",
    "        freeze_model(discriminator)\n",
    "\n",
    "        ### NOTE TEMPORARY MEASURE BECAUSE THEY TAKE IN (BATCHSIZE, xx, xx) shape ####\n",
    "        temp_adj_hr = adj_hr.reshape(1, *adj_hr.shape)\n",
    "        temp_generated_adj_hr = generated_adj_hr.reshape(1, *generated_adj_hr.shape)\n",
    "        ##########################################################\n",
    "\n",
    "        g_topology_loss = GT_loss(temp_adj_hr.to(DEVICE), temp_generated_adj_hr.to(DEVICE))\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        d_fake = discriminator(X_hr.to(DEVICE), generated_adj_hr.to(DEVICE))\n",
    "\n",
    "        g_adversarial_loss = adversarial_loss(d_fake, (torch.ones_like(d_fake)))\n",
    "        g_loss = g_adversarial_loss + g_topology_loss + alignment_loss\n",
    "      \n",
    "        g_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        generator_loss_ls.append(g_loss.detach().item())\n",
    "\n",
    "\n",
    "    avg_align_loss = np.mean(alignment_loss_ls)\n",
    "    avg_generator_loss = np.mean(generator_loss_ls)\n",
    "    avg_discriminator_loss = np.mean(disciriminator_loss_ls)\n",
    "\n",
    "    print(f'sample {i}: align_loss = {avg_align_loss}, generator_loss = {avg_generator_loss}, discriminator = {avg_discriminator_loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(aligner, generator, discriminator, train_X_lr, train_adj_lr, train_X_hr, train_adj_hr, generator_optimizer, discriminator_optimizer, adversarial_loss=torch.nn.BCELoss()):\n",
    "    aligner.train()\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    train_data = (train_X_lr, train_adj_lr, train_X_hr, train_adj_hr)\n",
    "        \n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        alignment_loss_ls = []\n",
    "        generator_loss_ls = []\n",
    "        disciriminator_loss_ls = []\n",
    "\n",
    "\n",
    "\n",
    "        for i, sample in tqdm(enumerate(zip(*train_data))):\n",
    "\n",
    "            generator_optimizer.zero_grad()\n",
    "            discriminator_optimizer.zero_grad()\n",
    "\n",
    "            X_lr, adj_lr, X_hr, adj_hr = sample\n",
    "\n",
    "            aligned_X_lr, aligned_adj_lr = aligner(X_lr.to(DEVICE), adj_lr.to(DEVICE))\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            hr_mean = torch.mean(X_hr)\n",
    "            hr_std = torch.std(X_hr)\n",
    "\n",
    "            adj_hr_sampled = torch.normal(hr_mean, hr_std, size=(N_LR_NODES, N_LR_NODES)).to(DEVICE)\n",
    "\n",
    "\n",
    "            alignment_loss = torch.abs(F.kl_div(F.softmax(adj_hr_sampled, dim=-1), F.softmax(aligned_adj_lr, dim=-1), None, None, 'sum'))\n",
    "            alignment_loss = alignment_loss / 1000\n",
    "\n",
    "            alignment_loss_ls.append(alignment_loss.detach().item())\n",
    "\n",
    "            # generate hr adjacency\n",
    "            generated_adj_hr = generator(aligned_X_lr.to(DEVICE), aligned_adj_lr.to(DEVICE))\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            freeze_model(generator)\n",
    "            freeze_model(aligner)\n",
    "            unfreeze_model(discriminator)\n",
    "\n",
    "            d_real = discriminator(X_hr.to(DEVICE), adj_hr.to(DEVICE))\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            d_fake = discriminator(X_hr.to(DEVICE), generated_adj_hr.to(DEVICE))\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            d_real_loss = adversarial_loss(d_real, torch.ones_like(d_real, requires_grad=False))\n",
    "            torch.cuda.empty_cache()\n",
    "            d_fake_loss = adversarial_loss(d_fake, torch.zeros_like(d_fake, requires_grad=False))\n",
    "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            d_loss.backward(retain_graph=True)\n",
    "            discriminator_optimizer.step()\n",
    "\n",
    "            disciriminator_loss_ls.append(d_loss.detach().item())\n",
    "\n",
    "\n",
    "            unfreeze_model(generator)\n",
    "            unfreeze_model(aligner)\n",
    "            freeze_model(discriminator)\n",
    "\n",
    "            ### NOTE TEMPORARY MEASURE BECAUSE THEY TAKE IN (BATCHSIZE, xx, xx) shape ####\n",
    "            temp_adj_hr = adj_hr.reshape(1, *adj_hr.shape)\n",
    "            temp_generated_adj_hr = generated_adj_hr.reshape(1, *generated_adj_hr.shape)\n",
    "            ##########################################################\n",
    "\n",
    "            g_topology_loss = GT_loss(temp_adj_hr.to(DEVICE), temp_generated_adj_hr.to(DEVICE))\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            d_fake = discriminator(X_hr.to(DEVICE), generated_adj_hr.to(DEVICE))\n",
    "\n",
    "            g_adversarial_loss = adversarial_loss(d_fake, (torch.ones_like(d_fake)))\n",
    "            g_loss = g_adversarial_loss + g_topology_loss + alignment_loss\n",
    "        \n",
    "            g_loss.backward(retain_graph=True)\n",
    "            generator_optimizer.step()\n",
    "        \n",
    "            generator_loss_ls.append(g_loss.detach().item())\n",
    "\n",
    "\n",
    "        avg_align_loss = np.mean(alignment_loss_ls)\n",
    "        avg_generator_loss = np.mean(generator_loss_ls)\n",
    "        avg_discriminator_loss = np.mean(disciriminator_loss_ls)\n",
    "\n",
    "        print(f'sample {i}: align_loss = {avg_align_loss}, generator_loss = {avg_generator_loss}, discriminator = {avg_discriminator_loss}')\n",
    "\n",
    "    return aligner, generator, discriminator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_fn import evaluate_predictions\n",
    "\n",
    "\n",
    "def validation(aligner, generator, val_X_lr, val_adj_lr, val_adj_hr):\n",
    "    print('begin validation')\n",
    "    aligner.eval()\n",
    "    generator.eval()\n",
    "\n",
    "    all_predictions = torch.empty((len(val_X_lr), N_HR_NODES, N_HR_NODES), requires_grad=False).cpu()\n",
    "\n",
    "    for i in range(len(val_X_lr)):\n",
    "                    \n",
    "        aligned_X_lr, aligned_adj_lr = aligner(val_X_lr[i].to(DEVICE), val_adj_lr[i].to(DEVICE))\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        generated_adj_hr = generator(aligned_X_lr.detach(), aligned_adj_lr.detach()).cpu()\n",
    "\n",
    "        all_predictions[i] = generated_adj_hr\n",
    "\n",
    "    return evaluate_predictions(all_predictions, val_adj_hr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 30.75 MiB is free. Process 4068805 has 2.50 GiB memory in use. Including non-PyTorch memory, this process has 12.71 GiB memory in use. Of the allocated memory 11.51 GiB is allocated by PyTorch, and 933.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvalidation\u001b[49m\u001b[43m(\u001b[49m\u001b[43maligner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_X_all\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhr_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 12\u001b[0m, in \u001b[0;36mvalidation\u001b[0;34m(aligner, generator, val_X_lr, val_adj_lr, val_adj_hr)\u001b[0m\n\u001b[1;32m      8\u001b[0m all_predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;28mlen\u001b[39m(val_X_lr), N_HR_NODES, N_HR_NODES))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(val_X_lr)):\n\u001b[0;32m---> 12\u001b[0m     aligned_X_lr, aligned_adj_lr \u001b[38;5;241m=\u001b[39m \u001b[43maligner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_X_lr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_adj_lr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     15\u001b[0m     generated_adj_hr \u001b[38;5;241m=\u001b[39m generator(aligned_X_lr\u001b[38;5;241m.\u001b[39mto(DEVICE), aligned_adj_lr\u001b[38;5;241m.\u001b[39mto(DEVICE))\n",
      "File \u001b[0;32m/vol/bitbucket/tcwong/deep_graph_learning/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/tcwong/deep_graph_learning/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m, in \u001b[0;36mSheafAligner.forward\u001b[0;34m(self, X, adj)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, adj):\n\u001b[0;32m---> 20\u001b[0m     x1, L1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msheafconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatchnorm1(x1))\n\u001b[1;32m     22\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x1, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[0;32m/vol/bitbucket/tcwong/deep_graph_learning/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/tcwong/deep_graph_learning/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m, in \u001b[0;36mSheafConvLayer.forward\u001b[0;34m(self, X, adj)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, adj):\n\u001b[1;32m     17\u001b[0m     kron_prod \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mkron(torch\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_nodes)\u001b[38;5;241m.\u001b[39mto(DEVICE), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight1)\n\u001b[0;32m---> 18\u001b[0m     L \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msheaf_laplacian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m X \u001b[38;5;241m-\u001b[39m F\u001b[38;5;241m.\u001b[39melu(L \u001b[38;5;241m@\u001b[39m kron_prod \u001b[38;5;241m@\u001b[39m X \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight2), L\n",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m, in \u001b[0;36mSheafConvLayer.sheaf_laplacian\u001b[0;34m(self, X, adj, epsilon)\u001b[0m\n\u001b[1;32m     27\u001b[0m idx_pairs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcartesian_prod(torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_nodes), torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_nodes))\n\u001b[1;32m     28\u001b[0m all_stacked_features \u001b[38;5;241m=\u001b[39m X_reshaped[idx_pairs]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_nodes, \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 29\u001b[0m lin_trans \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_stacked_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m inner_transpose \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtranspose(lin_trans, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     31\u001b[0m L_v \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(lin_trans, torch\u001b[38;5;241m.\u001b[39mtranspose(inner_transpose, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/vol/bitbucket/tcwong/deep_graph_learning/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/functional.py:1564\u001b[0m, in \u001b[0;36melu\u001b[0;34m(input, alpha, inplace)\u001b[0m\n\u001b[1;32m   1562\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39melu_(\u001b[38;5;28minput\u001b[39m, alpha)\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1564\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 30.75 MiB is free. Process 4068805 has 2.50 GiB memory in use. Including non-PyTorch memory, this process has 12.71 GiB memory in use. Of the allocated memory 11.51 GiB is allocated by PyTorch, and 933.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "validation(aligner, generator, lr_X_all[:20], lr_train[:20], hr_train[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(n_fold, X_lr, adj_lr, X_hr, adj_hr, d=2, f=32):\n",
    "    kf = KFold(n_fold, shuffle=True, random_state=99)\n",
    "    runs_results = []\n",
    "    for train_idx, val_idx in kf.split(X_lr):\n",
    "        train_X_lr, val_X_lr = X_lr[train_idx], X_lr[val_idx]\n",
    "        train_adj_lr, val_adj_lr = adj_lr[train_idx], adj_lr[val_idx]\n",
    "        train_X_hr = X_hr[train_idx]\n",
    "        train_adj_hr, val_adj_hr = adj_hr[train_idx], adj_hr[val_idx]\n",
    "\n",
    "        aligner = SheafAligner(d, f).to(DEVICE)\n",
    "        generator = SheafGenerator(d, f).to(DEVICE)\n",
    "        discriminator = SheafDiscriminator(d, f).to(DEVICE)\n",
    "\n",
    "        generator_optimizer = torch.optim.AdamW(list(aligner.parameters()) + list(generator.parameters()), lr=0.001, betas=(0.5, 0.999))\n",
    "        discriminator_optimizer = torch.optim.AdamW(discriminator.parameters(), lr=0.001, betas=(0.5, 0.999))        \n",
    "        \n",
    "        aligner, generator, discriminator = train(aligner, generator, discriminator, train_X_lr, train_adj_lr, train_X_hr, train_adj_hr, generator_optimizer, discriminator_optimizer)\n",
    "        val_metrics = validation(aligner, generator, val_X_lr, val_adj_lr, val_adj_hr)\n",
    "        runs_results.append(val_metrics)\n",
    "\n",
    "    return runs_results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [01:36,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.7963261384147782, generator_loss = 3.2505935683239473, discriminator = 0.7009271485311491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [01:36,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.794862018512176, generator_loss = 3.2396781119822613, discriminator = 0.6988674165966274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [01:37,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.794393320341368, generator_loss = 3.2476265712387633, discriminator = 0.7013840122265859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [01:36,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.7941779794993701, generator_loss = 3.2512970190887756, discriminator = 0.7027396054955216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [01:37,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.7940458429826273, generator_loss = 3.2383339633299126, discriminator = 0.7025974129771327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [01:37,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.7939558593002526, generator_loss = 3.2416893662656454, discriminator = 0.703324428549758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [01:36,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.7938910298519306, generator_loss = 3.2330873981812758, discriminator = 0.7070250446731979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [01:36,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.7938442316141214, generator_loss = 3.2418197925049683, discriminator = 0.6990944141740197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [01:36,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.7938065373145782, generator_loss = 3.250571236956412, discriminator = 0.6999446985957859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [01:36,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.7937809057063885, generator_loss = 3.229642957182803, discriminator = 0.6936066757451307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [01:36,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.7937571180833353, generator_loss = 3.2338481250133304, discriminator = 0.7096576572538497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [01:37,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.7937413647368148, generator_loss = 3.245629262819132, discriminator = 0.7080226433169734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [01:37,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.7937282141264494, generator_loss = 3.2437831064088627, discriminator = 0.7040401144070668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [01:38,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.7937172398910867, generator_loss = 3.2378315581955057, discriminator = 0.6923432956944715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [01:38,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.7937090767396463, generator_loss = 3.2495518933568692, discriminator = 0.7002628338229548\n",
      "begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/tcwong/deep_graph_learning/DGL24-Group-Project/evaluation_fn.py:90: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  pcc = pearsonr(pred_1d, gt_1d)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.73091316\n",
      "PCC:  nan\n",
      "Jensen-Shannon Distance:  0.34707408553065305\n",
      "Average MAE betweenness centrality: 0.01531607274276826\n",
      "Average MAE eigenvector centrality: 0.01719175730917245\n",
      "Average MAE PageRank centrality: 0.0007483549436318534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [01:05,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.7975988425650038, generator_loss = 2.9283703311348703, discriminator = 0.7027852776888255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [01:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.7975799860181035, generator_loss = 2.856796785376231, discriminator = 0.6976501087884646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [01:01,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.797562643214389, generator_loss = 2.871366634334486, discriminator = 0.6970356122867482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [00:56,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.7975556082553692, generator_loss = 2.8213571084144835, discriminator = 0.6879967510163247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [00:56,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.79754989533811, generator_loss = 2.816919800746125, discriminator = 0.7061627985120894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [00:56,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.7975462288469881, generator_loss = 2.8275692238801526, discriminator = 0.7042101111497965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [00:56,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.7975517994648701, generator_loss = 2.817022312757365, discriminator = 0.6997884076994818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [00:56,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.7975492515005507, generator_loss = 2.833361236328846, discriminator = 0.6977764931884972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [00:56,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.7975660197369687, generator_loss = 2.8247117884353186, discriminator = 0.6931395149445748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [00:56,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.7975582501909755, generator_loss = 2.8283984846019474, discriminator = 0.698121477891733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [00:56,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.7975570785032736, generator_loss = 2.830449732224929, discriminator = 0.694651693374187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [00:55,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.7975729311908688, generator_loss = 2.819719614164132, discriminator = 0.6989114295254957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [00:55,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.7975733532561912, generator_loss = 2.844377701696241, discriminator = 0.6973009114866858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [00:56,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.7975785077155173, generator_loss = 2.7946865476071863, discriminator = 0.7028823038479229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [00:56,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 110: align_loss = 0.7975819422318055, generator_loss = 2.8289459368331467, discriminator = 0.6984498098089889\n",
      "begin validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/tcwong/deep_graph_learning/DGL24-Group-Project/evaluation_fn.py:90: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  pcc = pearsonr(pred_1d, gt_1d)[0]\n",
      "/vol/bitbucket/tcwong/deep_graph_learning/DGL24-Group-Project/venv/lib/python3.10/site-packages/scipy/spatial/distance.py:1262: RuntimeWarning: invalid value encountered in divide\n",
      "  p = p / np.sum(p, axis=axis, keepdims=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.25090793\n",
      "PCC:  nan\n",
      "Jensen-Shannon Distance:  nan\n",
      "Average MAE betweenness centrality: 0.015412674840715746\n",
      "Average MAE eigenvector centrality: 0.01734299046230713\n",
      "Average MAE PageRank centrality: 0.0007402635545339637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [01:03,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 111: align_loss = 0.7976914133344378, generator_loss = 2.894211779893698, discriminator = 0.7025277258030006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [00:56,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 111: align_loss = 0.7976698407105037, generator_loss = 2.808177985624804, discriminator = 0.6933394920613084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [00:56,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 111: align_loss = 0.7976588542972293, generator_loss = 2.821406282014415, discriminator = 0.6941365775253091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [00:56,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 111: align_loss = 0.797634192343269, generator_loss = 2.8176566381403854, discriminator = 0.6997857716466699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [00:56,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 111: align_loss = 0.7976143594299044, generator_loss = 2.8093899675968754, discriminator = 0.6922769205910819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [00:56,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 111: align_loss = 0.7975747452250549, generator_loss = 2.829363536366324, discriminator = 0.6857746876776218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [00:56,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 111: align_loss = 0.797548172729356, generator_loss = 2.82663897312177, discriminator = 0.7000684988285814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [00:56,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 111: align_loss = 0.7974959622536387, generator_loss = 2.8181051241059625, discriminator = 0.7024704196623394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [00:56,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 111: align_loss = 0.7974584209067481, generator_loss = 2.8227147002557293, discriminator = 0.7060143351554871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [00:56,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 111: align_loss = 0.7973908952304295, generator_loss = 2.816436887253166, discriminator = 0.6976693235337734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [00:56,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 111: align_loss = 0.7973480810012136, generator_loss = 2.831261266968121, discriminator = 0.7017229438892433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [00:56,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 111: align_loss = 0.7972805249903884, generator_loss = 2.8391023763928627, discriminator = 0.7093329557350704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [00:56,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 111: align_loss = 0.7972132680671555, generator_loss = 2.8190940282649706, discriminator = 0.7068851121834346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [00:56,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 111: align_loss = 0.797135978937149, generator_loss = 2.8211140230765595, discriminator = 0.7014998947935445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [00:56,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 111: align_loss = 0.7970623964709895, generator_loss = 2.815761498750056, discriminator = 0.6986792923084327\n",
      "begin validation\n",
      "MAE:  0.25977245\n",
      "PCC:  nan\n",
      "Jensen-Shannon Distance:  nan\n",
      "Average MAE betweenness centrality: 0.015483741598290088\n",
      "Average MAE eigenvector centrality: 0.01720620378716123\n",
      "Average MAE PageRank centrality: 0.0007260023412142038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/tcwong/deep_graph_learning/DGL24-Group-Project/evaluation_fn.py:90: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  pcc = pearsonr(pred_1d, gt_1d)[0]\n",
      "/vol/bitbucket/tcwong/deep_graph_learning/DGL24-Group-Project/venv/lib/python3.10/site-packages/scipy/spatial/distance.py:1262: RuntimeWarning: invalid value encountered in divide\n",
      "  p = p / np.sum(p, axis=axis, keepdims=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'mae': 0.73091316,\n",
       "  'pcc': nan,\n",
       "  'js_dis': 0.34707408553065305,\n",
       "  'avg_mae_bc': 0.01531607274276826,\n",
       "  'avg_mae_ec': 0.01719175730917245,\n",
       "  'avg_mae_pc': 0.0007483549436318534},\n",
       " {'mae': 0.25090793,\n",
       "  'pcc': nan,\n",
       "  'js_dis': nan,\n",
       "  'avg_mae_bc': 0.015412674840715746,\n",
       "  'avg_mae_ec': 0.01734299046230713,\n",
       "  'avg_mae_pc': 0.0007402635545339637},\n",
       " {'mae': 0.25977245,\n",
       "  'pcc': nan,\n",
       "  'js_dis': nan,\n",
       "  'avg_mae_bc': 0.015483741598290088,\n",
       "  'avg_mae_ec': 0.01720620378716123,\n",
       "  'avg_mae_pc': 0.0007260023412142038}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 15\n",
    "cross_validate(3, lr_X_all, lr_train, hr_X_all, hr_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(aligner.state_dict(), 'tim_files/aligner.pth')\n",
    "torch.save(generator.state_dict(), 'tim_files/generator.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SheafGenerator:\n\tMissing key(s) in state_dict: \"out_mat\". \n\tsize mismatch for sheafconv3.weight2: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([32, 268]).\n\tsize mismatch for batchnorm3.module.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([268]).\n\tsize mismatch for batchnorm3.module.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([268]).\n\tsize mismatch for batchnorm3.module.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([268]).\n\tsize mismatch for batchnorm3.module.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([268]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m aligner\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtim_files/aligner.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtim_files/generator.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/tcwong/deep_graph_learning/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SheafGenerator:\n\tMissing key(s) in state_dict: \"out_mat\". \n\tsize mismatch for sheafconv3.weight2: copying a param with shape torch.Size([32, 32]) from checkpoint, the shape in current model is torch.Size([32, 268]).\n\tsize mismatch for batchnorm3.module.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([268]).\n\tsize mismatch for batchnorm3.module.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([268]).\n\tsize mismatch for batchnorm3.module.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([268]).\n\tsize mismatch for batchnorm3.module.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([268])."
     ]
    }
   ],
   "source": [
    "aligner.load_state_dict(torch.load('tim_files/aligner.pth'))\n",
    "generator.load_state_dict(torch.load('tim_files/generator.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.69 GiB of which 7.94 MiB is free. Process 2720755 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 11.47 GiB memory in use. Of the allocated memory 10.59 GiB is allocated by PyTorch, and 733.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m all_predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;28mlen\u001b[39m(lr_test), N_HR_NODES, N_HR_NODES), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(lr_test)):\n\u001b[0;32m---> 11\u001b[0m     aligned_X_lr, aligned_adj_lr \u001b[38;5;241m=\u001b[39m \u001b[43maligner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr_X_all\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     14\u001b[0m     generated_adj_hr \u001b[38;5;241m=\u001b[39m generator(aligned_X_lr\u001b[38;5;241m.\u001b[39mdetach(), aligned_adj_lr\u001b[38;5;241m.\u001b[39mdetach())\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[0;32m/vol/bitbucket/tcwong/deep_graph_learning/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/tcwong/deep_graph_learning/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m, in \u001b[0;36mSheafAligner.forward\u001b[0;34m(self, X, adj)\u001b[0m\n\u001b[1;32m     26\u001b[0m adj1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(adj1, mean_x1[\u001b[38;5;28;01mNone\u001b[39;00m, :, :, \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[1;32m     27\u001b[0m adj1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msigmoid(adj1\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[0;32m---> 29\u001b[0m x2, L2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msheafconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m x2 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatchnorm2(x2))\n\u001b[1;32m     31\u001b[0m x2 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x2, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[0;32m/vol/bitbucket/tcwong/deep_graph_learning/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/tcwong/deep_graph_learning/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m, in \u001b[0;36mSheafConvLayer.forward\u001b[0;34m(self, X, adj)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, adj):\n\u001b[1;32m     17\u001b[0m     kron_prod \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mkron(torch\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_nodes)\u001b[38;5;241m.\u001b[39mto(DEVICE), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight1)\n\u001b[0;32m---> 18\u001b[0m     L \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msheaf_laplacian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m X \u001b[38;5;241m-\u001b[39m F\u001b[38;5;241m.\u001b[39melu(L \u001b[38;5;241m@\u001b[39m kron_prod \u001b[38;5;241m@\u001b[39m X \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight2), L\n",
      "Cell \u001b[0;32mIn[3], line 29\u001b[0m, in \u001b[0;36mSheafConvLayer.sheaf_laplacian\u001b[0;34m(self, X, adj, epsilon)\u001b[0m\n\u001b[1;32m     27\u001b[0m idx_pairs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcartesian_prod(torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_nodes), torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_nodes))\n\u001b[1;32m     28\u001b[0m all_stacked_features \u001b[38;5;241m=\u001b[39m X_reshaped[idx_pairs]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_nodes, \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 29\u001b[0m lin_trans \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_stacked_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m inner_transpose \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtranspose(lin_trans, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     31\u001b[0m L_v \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(lin_trans, torch\u001b[38;5;241m.\u001b[39mtranspose(inner_transpose, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/vol/bitbucket/tcwong/deep_graph_learning/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/functional.py:1564\u001b[0m, in \u001b[0;36melu\u001b[0;34m(input, alpha, inplace)\u001b[0m\n\u001b[1;32m   1562\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39melu_(\u001b[38;5;28minput\u001b[39m, alpha)\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1564\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.69 GiB of which 7.94 MiB is free. Process 2720755 has 178.00 MiB memory in use. Including non-PyTorch memory, this process has 11.47 GiB memory in use. Of the allocated memory 10.59 GiB is allocated by PyTorch, and 733.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# predictions = torch.empty((len(lr_test), N_HR_NODES, N_HR_NODES))\n",
    "# for i in range(len(lr_test)):\n",
    "#     X_lr = lr_X_all[i]\n",
    "#     adj_lr = lr_test[i]\n",
    "#     aligned aligner\n",
    "\n",
    "all_predictions = torch.empty((len(lr_test), N_HR_NODES, N_HR_NODES), requires_grad=False).cpu()\n",
    "\n",
    "for i in range(len(lr_test)):\n",
    "                \n",
    "    aligned_X_lr, aligned_adj_lr = aligner(lr_X_all[i].to(DEVICE), lr_test[i].to(DEVICE))\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    generated_adj_hr = generator(aligned_X_lr.detach(), aligned_adj_lr.detach()).cpu()\n",
    "\n",
    "    all_predictions[i] = generated_adj_hr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preparation import generate_submission_file\n",
    "\n",
    "generate_submission_file(all_predictions, 'average_IMAN_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin validation\n",
      "MAE:  0.7193633\n",
      "PCC:  0.007882498734272114\n",
      "Jensen-Shannon Distance:  0.36142108855438787\n",
      "Average MAE betweenness centrality: 0.015426310551565453\n",
      "Average MAE eigenvector centrality: 0.019239364692123003\n",
      "Average MAE PageRank centrality: 0.0008014847652272805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mae': 0.7193633,\n",
       " 'pcc': 0.007882498734272114,\n",
       " 'js_dis': 0.36142108855438787,\n",
       " 'avg_mae_bc': 0.015426310551565453,\n",
       " 'avg_mae_ec': 0.019239364692123003,\n",
       " 'avg_mae_pc': 0.0008014847652272805}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(aligner, generator, lr_X_all[:20], lr_train[:20], hr_train[:20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
