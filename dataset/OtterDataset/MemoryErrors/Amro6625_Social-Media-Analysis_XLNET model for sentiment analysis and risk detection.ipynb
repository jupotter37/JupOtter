{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a8058fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d619edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers requests beautifulsoup4 pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eaeb0c1-ca7d-4622-8638-f702c4764f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf406a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import plotly.express as px\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "import re \n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "lb = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac50aba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94cb498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c71658e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 74681 entries, 0 to 74680\n",
      "Data columns (total 4 columns):\n",
      " #   Column                                                 Non-Null Count  Dtype \n",
      "---  ------                                                 --------------  ----- \n",
      " 0   2401                                                   74681 non-null  int64 \n",
      " 1   Borderlands                                            74681 non-null  object\n",
      " 2   Positive                                               74681 non-null  object\n",
      " 3   im getting on borderlands and i will murder you all ,  73995 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a33118e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive', 'Neutral', 'Negative', 'Irrelevant'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Positive'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b04f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['im getting on borderlands and i will murder you all ,'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f861efa9-2072-4e27-a2c0-c5dd56a95b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80916300",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.rename(columns={\"Borderlands\":\"Feature2\",\"im getting on borderlands and i will murder you all ,\":\"Feature1\",\"Positive\": \"labels\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf97e817",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tweets\"]= df[\"Feature1\"].astype(str) +\" \"+ df[\"Feature2\"].astype(str)\n",
    "df= df.drop([\"Feature1\",\"Feature2\"],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3cb7dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_len'] = [len(text.split()) for text in df.tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f35aba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~(df['tweet_len'] < 5) & ~(df['tweet_len'] > 60)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9802d184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Irrelevant': 0, 'Negative': 1, 'Neutral': 2, 'Positive': 3}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels = {key : value for value , key in enumerate(np.unique(df['labels']))}\n",
    "df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "423e8363-39a0-4660-8076-a9fd66778c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'labels' is the column name in your DataFrame\n",
    "df = df[df['labels'] != 'Irrelevant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "413385b5-2e33-40a8-bcb7-8dc5fbe0fd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "Negative    9805\n",
       "Positive    8738\n",
       "Neutral     8080\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d7a7039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2401</th>\n",
       "      <th>labels</th>\n",
       "      <th>tweets</th>\n",
       "      <th>tweet_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38713</th>\n",
       "      <td>5442</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Thanks to @ Kain0025 for the raid. Thanks to @...</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>4692</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>How not to get bored about every damn thing in...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45892</th>\n",
       "      <td>11877</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>This comes as Facebook faces major criticism f...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68307</th>\n",
       "      <td>3697</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>I'and d rather a delayed game than a broken di...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19912</th>\n",
       "      <td>12608</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>yeah its alright for fighting some raid here i...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57886</th>\n",
       "      <td>11526</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>You guys are missing both not seeing my squad ...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14818</th>\n",
       "      <td>2938</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I've been at a lot of unranked in DotA 2. (SEA...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640</th>\n",
       "      <td>2688</td>\n",
       "      <td>Positive</td>\n",
       "      <td>All 3 of these are me! I wore Tannis and Hands...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38544</th>\n",
       "      <td>5411</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Ouch, the Pain Zone powered by The Nuclear Arc...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24951</th>\n",
       "      <td>4684</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>For @narendramodi i will am unable to exclusiv...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26623 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        2401    labels                                             tweets  \\\n",
       "38713   5442  Positive  Thanks to @ Kain0025 for the raid. Thanks to @...   \n",
       "24996   4692   Neutral  How not to get bored about every damn thing in...   \n",
       "45892  11877   Neutral  This comes as Facebook faces major criticism f...   \n",
       "68307   3697   Neutral  I'and d rather a delayed game than a broken di...   \n",
       "19912  12608   Neutral  yeah its alright for fighting some raid here i...   \n",
       "...      ...       ...                                                ...   \n",
       "57886  11526   Neutral  You guys are missing both not seeing my squad ...   \n",
       "14818   2938  Positive  I've been at a lot of unranked in DotA 2. (SEA...   \n",
       "1640    2688  Positive  All 3 of these are me! I wore Tannis and Hands...   \n",
       "38544   5411   Neutral  Ouch, the Pain Zone powered by The Nuclear Arc...   \n",
       "24951   4684   Neutral  For @narendramodi i will am unable to exclusiv...   \n",
       "\n",
       "       tweet_len  \n",
       "38713         44  \n",
       "24996         12  \n",
       "45892         19  \n",
       "68307         12  \n",
       "19912         32  \n",
       "...          ...  \n",
       "57886         15  \n",
       "14818         35  \n",
       "1640          14  \n",
       "38544         22  \n",
       "24951         18  \n",
       "\n",
       "[26623 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ed78032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the tweets\n",
    "def preprocess_tweet(tweet):\n",
    "\n",
    "    # Removing the mentions\n",
    "    tweet = re.sub(r'@[\\w]+', '', tweet)\n",
    "\n",
    "    # Removing hashtags\n",
    "    tweet = re.sub(r'#\\w+', '', tweet)\n",
    "    \n",
    "    # Removing URLs\n",
    "    tweet = re.sub(r'https?://\\S+', '', tweet)\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    tweet = re.sub(r'[^a-zA-Z\\s]', '', tweet)\n",
    "    \n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in stopwords]\n",
    "\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['Cleaned_Tweet'] = df['tweets'].apply(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f28b0b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LabelEncoder\n",
    "lb = LabelEncoder()\n",
    "\n",
    "# Fit and transform labels\n",
    "df['labels'] = lb.fit_transform(df['labels'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f03be29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['labels'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da646f47-c1b2-4e47-8240-88c98f1ee138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])  # Ensure labels are already converted to numerical format\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f85ae4e9-1f4e-458d-b005-a010a9264d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction = 0.8\n",
    "torch.cuda.set_per_process_memory_fraction(fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bca2aeca-8caa-48cf-a4fa-cdd5dd635423",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer and model\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e0cd928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data into train and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['Cleaned_Tweet'].values,\n",
    "    df['labels'].values,\n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f80561d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and DataLoader for training\n",
    "train_dataset = CustomDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = CustomDataset(val_texts, val_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b571a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1476d3ad-eeb4-484a-acd9-71ca7413914b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████████████████████████████████████████████████████████████████| 5990/5990 [23:58<00:00,  4.16it/s]\n",
      "Epoch 1: 100%|█████████████████████████████████████████████████████████████████████| 5990/5990 [23:49<00:00,  4.19it/s]\n",
      "Epoch 2: 100%|█████████████████████████████████████████████████████████████████████| 5990/5990 [23:47<00:00,  4.20it/s]\n",
      "Epoch 3: 100%|█████████████████████████████████████████████████████████████████████| 5990/5990 [23:48<00:00,  4.19it/s]\n",
      "Epoch 4: 100%|█████████████████████████████████████████████████████████████████████| 5990/5990 [23:48<00:00,  4.19it/s]\n",
      "Epoch 5: 100%|█████████████████████████████████████████████████████████████████████| 5990/5990 [23:48<00:00,  4.19it/s]\n",
      "Epoch 6: 100%|█████████████████████████████████████████████████████████████████████| 5990/5990 [23:48<00:00,  4.19it/s]\n",
      "Epoch 7: 100%|█████████████████████████████████████████████████████████████████████| 5990/5990 [23:47<00:00,  4.20it/s]\n",
      "Epoch 8: 100%|█████████████████████████████████████████████████████████████████████| 5990/5990 [23:47<00:00,  4.20it/s]\n",
      "Epoch 9: 100%|█████████████████████████████████████████████████████████████████████| 5990/5990 [23:48<00:00,  4.19it/s]\n",
      "Epoch 10: 100%|████████████████████████████████████████████████████████████████████| 5990/5990 [23:48<00:00,  4.19it/s]\n",
      "Epoch 11: 100%|████████████████████████████████████████████████████████████████████| 5990/5990 [23:47<00:00,  4.20it/s]\n",
      "Epoch 12: 100%|████████████████████████████████████████████████████████████████████| 5990/5990 [23:47<00:00,  4.20it/s]\n",
      "Epoch 13: 100%|████████████████████████████████████████████████████████████████████| 5990/5990 [23:47<00:00,  4.20it/s]\n",
      "Epoch 14: 100%|████████████████████████████████████████████████████████████████████| 5990/5990 [23:48<00:00,  4.19it/s]\n",
      "Epoch 15: 100%|████████████████████████████████████████████████████████████████████| 5990/5990 [23:51<00:00,  4.18it/s]\n",
      "Epoch 16: 100%|████████████████████████████████████████████████████████████████████| 5990/5990 [23:50<00:00,  4.19it/s]\n",
      "Epoch 17: 100%|████████████████████████████████████████████████████████████████████| 5990/5990 [23:49<00:00,  4.19it/s]\n",
      "Epoch 18: 100%|████████████████████████████████████████████████████████████████████| 5990/5990 [23:50<00:00,  4.19it/s]\n",
      "Epoch 19: 100%|████████████████████████████████████████████████████████████████████| 5990/5990 [23:48<00:00,  4.19it/s]\n",
      "Epoch 20: 100%|████████████████████████████████████████████████████████████████████| 5990/5990 [23:47<00:00,  4.20it/s]\n",
      "Epoch 21: 100%|████████████████████████████████████████████████████████████████████| 5990/5990 [23:48<00:00,  4.19it/s]\n",
      "Epoch 22: 100%|████████████████████████████████████████████████████████████████████| 5990/5990 [23:48<00:00,  4.19it/s]\n",
      "Epoch 23: 100%|████████████████████████████████████████████████████████████████████| 5990/5990 [23:47<00:00,  4.19it/s]\n",
      "Epoch 24: 100%|████████████████████████████████████████████████████████████████████| 5990/5990 [23:47<00:00,  4.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# Move the model to the GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "epochs = 25\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d928cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|████████████████████████████████████████████████████████████████████| 666/666 [00:52<00:00, 12.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92       987\n",
      "           1       0.88      0.91      0.89       820\n",
      "           2       0.91      0.88      0.90       856\n",
      "\n",
      "    accuracy                           0.90      2663\n",
      "   macro avg       0.90      0.90      0.90      2663\n",
      "weighted avg       0.90      0.90      0.90      2663\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Validation\n",
    "model.eval()\n",
    "val_preds = []\n",
    "val_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        val_preds.extend(preds.cpu().numpy())\n",
    "        val_true.extend(labels.cpu().numpy())\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(val_true, val_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "728eea93-1fc5-41dd-89b4-c1809a9ac81c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('XLNET_sentiment_tokenizer\\\\tokenizer_config.json',\n",
       " 'XLNET_sentiment_tokenizer\\\\special_tokens_map.json',\n",
       " 'XLNET_sentiment_tokenizer\\\\spiece.model',\n",
       " 'XLNET_sentiment_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save_pretrained('XLNET_sentiment_model')\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained('XLNET_sentiment_tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1728301-1640-4d54-81fa-65830a52f16a",
   "metadata": {},
   "source": [
    "# XlNET Model for Risk Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2b14bd9-e303-4557-9afe-7fb097548bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Excel file\n",
    "df_new = pd.read_excel('negative_sentiments.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6f64776-d886-423f-a1c3-d10986a69090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating labeled and unlabeled data\n",
    "predict_df_new = df_new[df_new['Label'].isna()]\n",
    "train_df_new = df_new.dropna(subset=['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee2d3697-6d04-4798-a665-18daa427408c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer and model loading\n",
    "tokenizer_new = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "model_new = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e021e39-02ef-4e52-81bc-72813ecd5013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing and preparing DataLoader for training\n",
    "class CustomDatasetNew(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "77754c88-cf28-42ad-9a46-0aec2e810cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the training data into train and validation sets\n",
    "train_texts_new, val_texts_new, train_labels_new, val_labels_new = train_test_split(\n",
    "    train_df_new['Cleaned_Tweet'].values,\n",
    "    train_df_new['Label'].values,\n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0261a82a-d0bf-4301-922b-a5abedaad7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating datasets and DataLoader for training\n",
    "train_dataset_new = CustomDatasetNew(train_texts_new, train_labels_new, tokenizer_new)\n",
    "val_dataset_new = CustomDatasetNew(val_texts_new, val_labels_new, tokenizer_new)\n",
    "\n",
    "train_loader_new = DataLoader(train_dataset_new, batch_size=4, shuffle=True)\n",
    "val_loader_new = DataLoader(val_dataset_new, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "867bf987-add8-4c80-b2d9-6a28c77d63af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                  | 0/34 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of the allocated memory 3.03 GiB is allocated by PyTorch, and 170.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m labels_new \u001b[38;5;241m=\u001b[39m batch_new[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device_new)\n\u001b[0;32m     15\u001b[0m optimizer_new\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 17\u001b[0m outputs_new \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels_new\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m loss_new \u001b[38;5;241m=\u001b[39m outputs_new\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     19\u001b[0m loss_new\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:1545\u001b[0m, in \u001b[0;36mXLNetForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, labels, use_mems, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1540\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1541\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1543\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1545\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m   1546\u001b[0m     input_ids,\n\u001b[0;32m   1547\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1548\u001b[0m     mems\u001b[38;5;241m=\u001b[39mmems,\n\u001b[0;32m   1549\u001b[0m     perm_mask\u001b[38;5;241m=\u001b[39mperm_mask,\n\u001b[0;32m   1550\u001b[0m     target_mapping\u001b[38;5;241m=\u001b[39mtarget_mapping,\n\u001b[0;32m   1551\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1552\u001b[0m     input_mask\u001b[38;5;241m=\u001b[39minput_mask,\n\u001b[0;32m   1553\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1554\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1555\u001b[0m     use_mems\u001b[38;5;241m=\u001b[39muse_mems,\n\u001b[0;32m   1556\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1557\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1558\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1559\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1560\u001b[0m )\n\u001b[0;32m   1561\u001b[0m output \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1563\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_summary(output)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:1204\u001b[0m, in \u001b[0;36mXLNetModel.forward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, use_mems, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1202\u001b[0m \u001b[38;5;66;03m# Positional encoding\u001b[39;00m\n\u001b[0;32m   1203\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_positional_encoding(qlen, klen, bsz\u001b[38;5;241m=\u001b[39mbsz)\n\u001b[1;32m-> 1204\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[43mpos_emb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_h\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1205\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pos_emb)\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1209\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer)\u001b[39;00m\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head]\u001b[39;00m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of the allocated memory 3.03 GiB is allocated by PyTorch, and 170.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "device_new = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_new.to(device_new)\n",
    "\n",
    "optimizer_new = torch.optim.AdamW(model_new.parameters(), lr=2e-5)\n",
    "epochs_new = 100\n",
    "\n",
    "for epoch_new in range(epochs_new):\n",
    "    model_new.train()\n",
    "    for batch_new in tqdm(train_loader_new, desc=f\"Epoch {epoch_new}\"):\n",
    "        input_ids_new = batch_new['input_ids'].to(device)\n",
    "        attention_mask_new = batch_new['attention_mask'].to(device_new)\n",
    "        labels_new = batch_new['label'].to(device_new)\n",
    "\n",
    "        optimizer_new.zero_grad()\n",
    "\n",
    "        outputs_new = model_new(input_ids_new, attention_mask=attention_mask_new, labels=labels_new)\n",
    "        loss_new = outputs_new.loss\n",
    "        loss_new.backward()\n",
    "\n",
    "        optimizer_new.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da89fd8b-ae1a-44f3-ae88-6f96c30aaf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                  | 0/34 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of the allocated memory 3.03 GiB is allocated by PyTorch, and 174.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m labels_new \u001b[38;5;241m=\u001b[39m batch_new[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device_new)\n\u001b[0;32m     20\u001b[0m optimizer_new\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 22\u001b[0m outputs_new \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels_new\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m loss_new \u001b[38;5;241m=\u001b[39m outputs_new\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     24\u001b[0m loss_new\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:1545\u001b[0m, in \u001b[0;36mXLNetForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, labels, use_mems, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1540\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1541\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1543\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1545\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m   1546\u001b[0m     input_ids,\n\u001b[0;32m   1547\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1548\u001b[0m     mems\u001b[38;5;241m=\u001b[39mmems,\n\u001b[0;32m   1549\u001b[0m     perm_mask\u001b[38;5;241m=\u001b[39mperm_mask,\n\u001b[0;32m   1550\u001b[0m     target_mapping\u001b[38;5;241m=\u001b[39mtarget_mapping,\n\u001b[0;32m   1551\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1552\u001b[0m     input_mask\u001b[38;5;241m=\u001b[39minput_mask,\n\u001b[0;32m   1553\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1554\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1555\u001b[0m     use_mems\u001b[38;5;241m=\u001b[39muse_mems,\n\u001b[0;32m   1556\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1557\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1558\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1559\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1560\u001b[0m )\n\u001b[0;32m   1561\u001b[0m output \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1563\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_summary(output)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:1237\u001b[0m, in \u001b[0;36mXLNetModel.forward\u001b[1;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, use_mems, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m   1235\u001b[0m     hidden_states\u001b[38;5;241m.\u001b[39mappend((output_h, output_g) \u001b[38;5;28;01mif\u001b[39;00m output_g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m output_h)\n\u001b[1;32m-> 1237\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1238\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_g\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask_h\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_tgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask_g\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseg_mat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseg_mat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmems\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmems\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1249\u001b[0m output_h, output_g \u001b[38;5;241m=\u001b[39m outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:508\u001b[0m, in \u001b[0;36mXLNetLayer.forward\u001b[1;34m(self, output_h, output_g, attn_mask_h, attn_mask_g, r, seg_mat, mems, target_mapping, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    497\u001b[0m     output_h,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    506\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    507\u001b[0m ):\n\u001b[1;32m--> 508\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_g\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask_g\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseg_mat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmems\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmems\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    520\u001b[0m     output_h, output_g \u001b[38;5;241m=\u001b[39m outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:439\u001b[0m, in \u001b[0;36mXLNetRelativeAttention.forward\u001b[1;34m(self, h, g, attn_mask_h, attn_mask_g, r, seg_mat, mems, target_mapping, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    436\u001b[0m k_head_r \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mibh,hnd->ibnd\u001b[39m\u001b[38;5;124m\"\u001b[39m, r\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr\u001b[38;5;241m.\u001b[39mdtype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr)\n\u001b[0;32m    438\u001b[0m \u001b[38;5;66;03m# core attention ops\u001b[39;00m\n\u001b[1;32m--> 439\u001b[0m attn_vec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_attn_core\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq_head_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_head_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv_head_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_head_r\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseg_mat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseg_mat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    451\u001b[0m     attn_vec, attn_prob \u001b[38;5;241m=\u001b[39m attn_vec\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:281\u001b[0m, in \u001b[0;36mXLNetRelativeAttention.rel_attn_core\u001b[1;34m(self, q_head, k_head_h, v_head_h, k_head_r, seg_mat, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;66;03m# position based attention score\u001b[39;00m\n\u001b[0;32m    280\u001b[0m bd \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mibnd,jbnd->bnij\u001b[39m\u001b[38;5;124m\"\u001b[39m, q_head \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr_r_bias, k_head_r)\n\u001b[1;32m--> 281\u001b[0m bd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_shift_bnij\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mklen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# segment based attention score\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seg_mat \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\transformers\\models\\xlnet\\modeling_xlnet.py:258\u001b[0m, in \u001b[0;36mXLNetRelativeAttention.rel_shift_bnij\u001b[1;34m(x, klen)\u001b[0m\n\u001b[0;32m    254\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(x_size[\u001b[38;5;241m0\u001b[39m], x_size[\u001b[38;5;241m1\u001b[39m], x_size[\u001b[38;5;241m2\u001b[39m], x_size[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# Note: the tensor-slice form was faster in my testing than torch.index_select\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m#       However, tracing doesn't like the nature of the slice, and if klen changes\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m#       during the run then it'll fail, whereas index_select will be fine.\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mklen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# x = x[:, :, :, :klen]\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of the allocated memory 3.03 GiB is allocated by PyTorch, and 174.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "device_new = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_new.to(device_new)\n",
    "\n",
    "optimizer_new = torch.optim.AdamW(model_new.parameters(), lr=2e-5)\n",
    "epochs_new = 100\n",
    "\n",
    "best_val_loss_new = float('inf')\n",
    "patience_new = 5  \n",
    "counter_new = 0\n",
    "\n",
    "for epoch_new in range(epochs_new):\n",
    "    # Training loop\n",
    "    model_new.train()\n",
    "    for batch_new in tqdm(train_loader_new, desc=f\"Epoch {epoch_new}\"):\n",
    "        input_ids_new = batch_new['input_ids'].to(device_new)\n",
    "        attention_mask_new = batch_new['attention_mask'].to(device_new)\n",
    "        labels_new = batch_new['label'].to(device_new)\n",
    "\n",
    "        optimizer_new.zero_grad()\n",
    "\n",
    "        outputs_new = model_new(input_ids_new, attention_mask=attention_mask_new, labels=labels_new)\n",
    "        loss_new = outputs_new.loss\n",
    "        loss_new.backward()\n",
    "\n",
    "        optimizer_new.step()\n",
    "\n",
    "    # Validation loop\n",
    "    model_new.eval()\n",
    "    val_loss_new = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_new in tqdm(val_loader_new, desc=\"Validation\"):\n",
    "            input_ids_new = batch_new['input_ids'].to(device_new)\n",
    "            attention_mask_new = batch_new['attention_mask'].to(device_new)\n",
    "            labels_new = batch_new['label'].to(device_new)\n",
    "\n",
    "            outputs_new = model_new(input_ids_new, attention_mask=attention_mask_new)\n",
    "            logits_new = outputs_new.logits\n",
    "            preds_new = torch.argmax(logits_new, dim=1)\n",
    "            loss_new = torch.nn.functional.cross_entropy(logits_new, labels_new)\n",
    "\n",
    "            val_loss_new += loss_new.item()\n",
    "\n",
    "    # Calculating average validation loss\n",
    "    avg_val_loss_new = val_loss_new / len(val_loader_new)\n",
    "\n",
    "    # Checking for early stopping\n",
    "    if avg_val_loss_new < best_val_loss_new:\n",
    "        best_val_loss_new = avg_val_loss_new\n",
    "        counter_new = 0\n",
    "    else:\n",
    "        counter_new += 1\n",
    "\n",
    "    if counter_new >= patience_new:\n",
    "        print(f'Early stopping after {epoch_new + 1} epochs without improvement.')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df4651d-bbb1-4878-8b76-ddf9942ac895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation of the model\n",
    "model_new.eval()\n",
    "val_preds_new = []\n",
    "val_true_new = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_new in tqdm(val_loader_new, desc=\"Validation\"):\n",
    "        input_ids_new = batch_new['input_ids'].to(device_new)\n",
    "        attention_mask_new = batch_new['attention_mask'].to(device_new)\n",
    "        labels_new = batch_new['label'].to(device_new)\n",
    "\n",
    "        outputs_new = model_new(input_ids_new, attention_mask=attention_mask_new)\n",
    "        logits_new = outputs_new.logits\n",
    "        preds_new = torch.argmax(logits_new, dim=1)\n",
    "\n",
    "        val_preds_new.extend(preds_new.cpu().numpy())\n",
    "        val_true_new.extend(labels_new.cpu().numpy())\n",
    "\n",
    "print(classification_report(val_true_new, val_preds_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee297e-069f-4ac8-9c4f-24582f4957c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Mean Squared Error (MSE)\n",
    "mse_new = mean_squared_error(val_true_new, val_preds_new)\n",
    "print(f\"Mean Squared Error (MSE): {mse_new:.4f}\")\n",
    "\n",
    "# Calculating Mean Absolute Error (MAE)\n",
    "mae_new = mean_absolute_error(val_true_new, val_preds_new)\n",
    "print(f\"Mean Absolute Error (MAE): {mae_new:.4f}\")\n",
    "\n",
    "# Calculating R-squared (R2) score\n",
    "r2_new = r2_score(val_true_new, val_preds_new)\n",
    "print(f\"R-squared (R2) Score: {r2_new:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8ad43f-3317-4649-badb-7b24b5da5a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample list of sentences\n",
    "sentences_new = [\"Had a wonderful time in hull today\",\"there is a fire in the south street we need the your assistance @HumbersideFire\", \"The kids are lighting fireworks in pearson park it is really dangerous\", \"I see smoke coming from the paragon station\", \"Some teenager are jumping of the bridge into the water\",\"There is no incident in the beverly road\"]\n",
    "\n",
    "# Tokenize the list of sentences\n",
    "inputs_new = tokenizer_new(sentences_new, padding=True, truncation=True, return_tensors='pt').to('cuda')\n",
    "\n",
    "# Forward pass through the model\n",
    "outputs_new = model_new(**inputs_new)\n",
    "\n",
    "# Apply softmax to get predictions\n",
    "predictions_new = torch.nn.functional.softmax(outputs_new.logits, dim=-1)\n",
    "\n",
    "# Convert predictions to numpy array\n",
    "predictions_new = predictions_new.cpu().detach().numpy()\n",
    "\n",
    "# Get the predicted labels\n",
    "predicted_labels_new = [np.argmax(pred) for pred in predictions_new]\n",
    "\n",
    "# Get the corresponding probabilities\n",
    "probs_new = [pred[label] for pred, label in zip(predictions_new, predicted_labels_new)]\n",
    "\n",
    "# Print results\n",
    "for sentence_new, label_new, prob_new in zip(sentences_new, predicted_labels_new, probs_new):\n",
    "    print(f\"Sentence: {sentence_new}\")\n",
    "    print(f\"Predicted Label: {label_new}\")\n",
    "    print(f\"Probability: {prob_new:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc849a4d-0c26-49f2-962b-e2ff1cebbb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            label = int(self.labels[idx])\n",
    "        else:\n",
    "            label = None\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long) if label is not None else None\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8148234-f2be-4829-845f-082d41899309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading file with data for prediction\n",
    "new_df = pd.read_excel('Labeled_Tweets.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ffeb2d-f6b6-4d40-b225-866ba4606d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'Cleaned_Tweet' is the column containing the text data for prediction\n",
    "new_texts = new_df['Cleaned_Tweet'].values\n",
    "\n",
    "# Tokenize and prepare DataLoader for prediction\n",
    "new_dataset = CustomDataset(new_texts, labels=None, tokenizer=tokenizer_new, max_len=128)\n",
    "new_loader = DataLoader(new_dataset, batch_size=8, shuffle=False, collate_fn=lambda x: x)\n",
    "\n",
    "# Make predictions\n",
    "model_new.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(new_loader, desc=\"Predicting\"):\n",
    "        input_ids = torch.stack([item['input_ids'] for item in batch]).to(device_new)\n",
    "        attention_mask = torch.stack([item['attention_mask'] for item in batch]).to(device_new)\n",
    "\n",
    "        outputs = model_new(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        predictions.extend(preds.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa82243-a6ac-4f62-8759-05396a5fc706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to the DataFrame\n",
    "new_df['XLNET_Prediction'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144023e5-714f-4f40-9c7a-6d8e3fd2340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d39a79-cd49-4b04-bcc6-b6d82273de99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
