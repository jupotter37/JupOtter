{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use t-SNE to visualize the forground and background features of the images.\n",
    "\n",
    "# Set the root path of the dataset to be data/images/CUB_200_2011/images\n",
    "# Set the root path of the dataset to be data/images/ILSVRC2012\n",
    "ROOT_CUB = '/home/billymicoder/Documents/GitHub/BillyCCAM/CCAM/data/images/CUB_200_2011/images'\n",
    "ROOT_ILSVRC = '/home/billymicoder/Documents/GitHub/BillyCCAM/CCAM/data/images/ILSVRC2012'\n",
    "\n",
    "# Read and load all the images in the Black Footed Albatross folder and print the number of images\n",
    "# in the folder, as well as the first image name.\n",
    "# The images are loaded as PIL images.\n",
    "from PIL import Image\n",
    "# if pillow is not installed, use pip install pillow\n",
    "# run the following code in the terminal\n",
    "# pip install pillow\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in Black Footed Albatross folder: 60\n",
      "First image name In Albatros: Black_Footed_Albatross_0076_417.jpg\n",
      "Number of images in CUB_200_2011/images/ folder: 11788\n",
      "First image name in Cub Images: <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x357 at 0x7F536B85B220>\n",
      "Number of labels in CUB_200_2011/images/ folder: 11788\n",
      "First label in Cub Images: 175.Pine_Warbler\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set the path of the Black Footed Albatross folder\n",
    "albatros_path = os.path.join(ROOT_CUB, '001.Black_footed_Albatross')\n",
    "\n",
    "# Get the list of all the images\n",
    "albatros_list = os.listdir(albatros_path)\n",
    "\n",
    "# Use pillow.Image to load all the images into a list of albatros_images\n",
    "albatros_images = [Image.open(os.path.join(albatros_path, img)) for img in albatros_list]\n",
    "\n",
    "def load_images(root_folder):\n",
    "    # Load all the images in CUB_200_2011/images/ with the labels as the folder names\n",
    "    # and store them in a list of images\n",
    "    cub_images = []\n",
    "    cub_labels = []\n",
    "\n",
    "    # Iterate through all the folders in the CUB_200_2011/images/ folder\n",
    "    for folder in os.listdir(root_folder):\n",
    "        # Get the path of the folder\n",
    "        folder_path = os.path.join(root_folder, folder)\n",
    "        # Get the list of all the images in the folder\n",
    "        image_list = os.listdir(folder_path)\n",
    "        # Iterate through all the images in the folder\n",
    "        for img in image_list:\n",
    "            # Get the path of the image\n",
    "            img_path = os.path.join(folder_path, img)\n",
    "            # Load the image and append it to the list of images\n",
    "            cub_images.append(Image.open(img_path))\n",
    "            # Append the label\n",
    "            cub_labels.append(folder)\n",
    "\n",
    "    return cub_images, cub_labels\n",
    "\n",
    "# Load all the images in CUB_200_2011/images/ with the labels as the folder names\n",
    "# and store them in a list of images\n",
    "cub_images, cub_labels = load_images(ROOT_CUB)\n",
    "\n",
    "\n",
    "# Print the number of images in the Black Footed Albatross folder\n",
    "print('Number of images in Black Footed Albatross folder:', len(albatros_list))\n",
    "\n",
    "# Print the first image name\n",
    "print('First image name In Albatros:', albatros_list[0])\n",
    "\n",
    "# Print the number of images in the CUB_200_2011/images/ folder\n",
    "print('Number of images in CUB_200_2011/images/ folder:', len(cub_images))\n",
    "\n",
    "# Print the first image name\n",
    "print('First image name in Cub Images:', cub_images[0])\n",
    "\n",
    "# Print the number of labels in the CUB_200_2011/images/ folder\n",
    "print('Number of labels in CUB_200_2011/images/ folder:', len(cub_labels))\n",
    "\n",
    "# Print the first label\n",
    "print('First label in Cub Images:', cub_labels[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear teh memory cache to run the code faster\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 119/200 [00:24<00:16,  4.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# IMport dataloader from torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define a transform to convert the PIL images to tensors\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Define a function to load the images and apply the transform\n",
    "def load_images_with_transform(root_folder, transform):\n",
    "    # Load all the images in CUB_200_2011/images/ with the labels as the folder names\n",
    "    # and store them in a list of images\n",
    "    cub_images_transform = []\n",
    "    cub_labels_transform = []\n",
    "\n",
    "    # Create a coutner to count the number of images loaded and limit the number of images to be loaded to 120\n",
    "    counter = 0\n",
    "    \n",
    "    # Iterate through all the folders in the CUB_200_2011/images/ folder\n",
    "    # and load the images and apply the transform. use tqdm to show the progress bar\n",
    "    for folder in tqdm(os.listdir(root_folder)):\n",
    "        # Get the path of the folder\n",
    "        folder_path = os.path.join(root_folder, folder)\n",
    "        # Get the list of all the images in the folder\n",
    "        image_list = os.listdir(folder_path)\n",
    "        # Iterate through all the images in the folder\n",
    "        for img in image_list:\n",
    "            # Get the path of the image\n",
    "            img_path = os.path.join(folder_path, img)\n",
    "            # Load the image and apply the transform\n",
    "            cub_images_transform.append(transform(Image.open(img_path)))\n",
    "            # Append the label\n",
    "            cub_labels_transform.append(folder)\n",
    "\n",
    "        # Increment the counter\n",
    "        counter += 1\n",
    "        # Break the loop if the counter is 120\n",
    "        if counter == 120:\n",
    "            break\n",
    "\n",
    "\n",
    "    return cub_images_transform, cub_labels_transform\n",
    "    \n",
    "\n",
    "cub_images_transform, cub_labels_transform = load_images_with_transform(ROOT_CUB, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of the loaded dataloader: <class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset from the list of images and labels\n",
    "dataset = list(zip(cub_images_transform, cub_labels_transform))\n",
    "\n",
    "# Create a dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "# Import torch\n",
    "import torch\n",
    "\n",
    "# Define a function to visualize the images\n",
    "def show_images(images, labels, ncols=4):\n",
    "    # Create a figure with the given number of columns\n",
    "    fig, axes = plt.subplots(figsize=(12, 12), ncols=ncols)\n",
    "\n",
    "    # Iterate through the images and labels\n",
    "    for i, (image, label) in enumerate(zip(images, labels)):\n",
    "        # Get the corresponding axis\n",
    "        ax = axes[i % ncols]\n",
    "        # Plot the image on the axis\n",
    "        ax.imshow(image)\n",
    "        # Set the title of the image as the label\n",
    "        ax.set_title(label)\n",
    "        # Remove the ticks\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "print(f\"Type of the loaded dataloader: {type(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"next\" in dir(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/billymicoder/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/billymicoder/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/billymicoder/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/billymicoder/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/billymicoder/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/billymicoder/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/billymicoder/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 162, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [3, 332, 500] at entry 0 and [3, 396, 500] at entry 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Get the first batch of images and labels\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m images, labels \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(dataloader))\n\u001b[1;32m      4\u001b[0m \u001b[39m# Show the images and labels\u001b[39;00m\n\u001b[1;32m      5\u001b[0m show_images(images, labels)\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1372\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/billymicoder/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/billymicoder/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/billymicoder/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/home/billymicoder/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/billymicoder/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/home/billymicoder/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/home/billymicoder/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 162, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [3, 332, 500] at entry 0 and [3, 396, 500] at entry 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the first batch of images and labels\n",
    "images, labels = next(iter(dataloader))\n",
    "\n",
    "# Show the images and labels\n",
    "show_images(images, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torch.nn\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a class for the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "\n",
    "        # Define the pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Define the linear layers\n",
    "        self.fc1 = nn.Linear(32 * 56 * 56, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 200)\n",
    "\n",
    "        # Define the dropout layer\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through the convolutional layers\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "\n",
    "        # Flatten the input tensor\n",
    "        x = x.view(-1, 32 * 56 * 56)\n",
    "\n",
    "        # Pass the tensor through the linear layers\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# Create an instance of the model\n",
    "model = CNN()\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n",
    "\n",
    "# Import torch.optim\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the number of epochs to train for\n",
    "n_epochs = 10\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.cuda()\n",
    "\n",
    "# Set the model to train mode\n",
    "model.train()\n",
    "\n",
    "# Define a dictionary to store the training statistics\n",
    "training_stats = {'epoch': [], 'Training Loss': [], 'Training Acc': []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use t-SNE to visualise the features\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Define a function to visualise the features\n",
    "def visualize_features(features, labels):\n",
    "    # Create a TSNE instance\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "\n",
    "    # Use TSNE to get the reduced dimensionality features\n",
    "    reduced_features = tsne.fit_transform(features)\n",
    "\n",
    "    # Define a list of markers\n",
    "    markers = ['o', 'x', 's', '*', '+']\n",
    "\n",
    "    # Get the unique labels\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    # Create a figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "    # Iterate through the unique labels\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        # Get the indices where the label is equal to i\n",
    "        indices = np.where(labels == i)\n",
    "        # Scatter plot the points with reduced dimensions\n",
    "        ax.scatter(reduced_features[indices, 0], reduced_features[indices, 1], marker=markers[i], label=label)\n",
    "\n",
    "    # Add a legend\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to train the model\n",
    "def train(model, criterion, optimizer, dataloader, n_epochs=10):\n",
    "    # Move the model to the GPU\n",
    "    model.cuda()\n",
    "\n",
    "    # Set the model to train mode\n",
    "    model.train()\n",
    "\n",
    "    # Define a dictionary to store the training statistics\n",
    "    training_stats = {'epoch': [], 'Training Loss': [], 'Training Acc': []}\n",
    "\n",
    "    # Iterate through the epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # Print the epoch number\n",
    "        print('Epoch:', epoch + 1)\n",
    "\n",
    "        # Keep track of the training loss\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # Keep track of the number of correct predictions\n",
    "        num_correct = 0\n",
    "\n",
    "        # Keep track of the number of data points\n",
    "        num_data = 0\n",
    "\n",
    "        # Iterate through the training data\n",
    "        for data, target in tqdm(dataloader):\n",
    "            # Move the data to the GPU\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "            # Clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Pass the data through the model\n",
    "            output = model(data)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the training loss\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "            # Get the predicted class with the highest score\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "            # Get the number of correct predictions\n",
    "            num_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            # Update the total number of data points\n",
    "            num_data += data.size(0)\n",
    "\n",
    "        # Calculate the training accuracy\n",
    "        train_acc = num_correct / num_data\n",
    "\n",
    "        # Print the training loss and accuracy\n",
    "        print('Training Loss:', train_loss / num_data)\n",
    "        print('Training Accuracy:', train_acc)\n",
    "\n",
    "        # Add the training loss and accuracy to the dictionary\n",
    "        training_stats['epoch'].append(epoch + 1)\n",
    "        training_stats['Training Loss'].append(train_loss / num_data)\n",
    "        training_stats['Training Acc'].append(train_acc)\n",
    "\n",
    "    # Return the training statistics\n",
    "    return training_stats\n",
    "\n",
    "# Train the model\n",
    "training_stats = train(model, criterion, optimizer, dataloader, n_epochs)\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(training_stats['epoch'], training_stats['Training Loss'])\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot the training accuracy\n",
    "plt.plot(training_stats['epoch'], training_stats['Training Acc'])\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the features from a pre-trained model\n",
    "def get_features(model, dataloader):\n",
    "    # Create a list to store the features and the labels\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate through the dataloader\n",
    "    for data, target in tqdm(dataloader):\n",
    "        # Move the data to the GPU\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "        # Pass the data through the model\n",
    "        output = model(data)\n",
    "\n",
    "        # Get the features and the labels\n",
    "        features.extend(output.cpu().detach().numpy())\n",
    "        labels.extend(target.cpu().detach().numpy())\n",
    "\n",
    "    # Return the features and the labels\n",
    "    return features, labels\n",
    "\n",
    "# Get the features from the pre-trained model\n",
    "features, labels = get_features(model, dataloader)\n",
    "\n",
    "# visualise the features\n",
    "visualize_features(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from WSOL.models.resnet import ResNet, resnet101, resnet50\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Set the seed for reproducible results\n",
    "seed = 10\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Set the device to be cuda if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pretrained ResNet50 model\n",
    "model = resnet50(pretrained=True)\n",
    "# model.eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      | 100073 KiB | 100073 KiB | 100073 KiB |      0 B   |\n",
      "|       from large pool |  82240 KiB |  82240 KiB |  82240 KiB |      0 B   |\n",
      "|       from small pool |  17833 KiB |  17833 KiB |  17833 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         | 100073 KiB | 100073 KiB | 100073 KiB |      0 B   |\n",
      "|       from large pool |  82240 KiB |  82240 KiB |  82240 KiB |      0 B   |\n",
      "|       from small pool |  17833 KiB |  17833 KiB |  17833 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      | 100040 KiB | 100040 KiB | 100040 KiB |      0 B   |\n",
      "|       from large pool |  82240 KiB |  82240 KiB |  82240 KiB |      0 B   |\n",
      "|       from small pool |  17800 KiB |  17800 KiB |  17800 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   | 120832 KiB | 120832 KiB | 120832 KiB |      0 B   |\n",
      "|       from large pool | 102400 KiB | 102400 KiB | 102400 KiB |      0 B   |\n",
      "|       from small pool |  18432 KiB |  18432 KiB |  18432 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  20758 KiB |  28795 KiB |  86171 KiB |  65412 KiB |\n",
      "|       from large pool |  20160 KiB |  28160 KiB |  74496 KiB |  54336 KiB |\n",
      "|       from small pool |    598 KiB |   2011 KiB |  11675 KiB |  11076 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     320    |     320    |     320    |       0    |\n",
      "|       from large pool |      18    |      18    |      18    |       0    |\n",
      "|       from small pool |     302    |     302    |     302    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     320    |     320    |     320    |       0    |\n",
      "|       from large pool |      18    |      18    |      18    |       0    |\n",
      "|       from small pool |     302    |     302    |     302    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      14    |      14    |      14    |       0    |\n",
      "|       from large pool |       5    |       5    |       5    |       0    |\n",
      "|       from small pool |       9    |       9    |       9    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       7    |       7    |      14    |       7    |\n",
      "|       from large pool |       5    |       5    |       5    |       0    |\n",
      "|       from small pool |       2    |       5    |       9    |       7    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# detach the images from the graph and move them to the device\n",
    "# cub_images = [img.detach().to(device) for img in cub_images]\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 56/11788 [00:39<2:18:02,  1.42it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 6.00 GiB total capacity; 19.09 GiB already allocated; 0 bytes free; 19.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 45\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m features\n\u001b[1;32m     43\u001b[0m \u001b[39m# Extract the features of the images\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m# albatros_features = extract_features(model, albatros_images)\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m cub_images_features \u001b[39m=\u001b[39m extract_features(model, cub_images)\n",
      "Cell \u001b[0;32mIn[3], line 35\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(model, images)\u001b[0m\n\u001b[1;32m     30\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m \u001b[39m# Extract the features\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m# The feature size should be (1, 2048, 7, 7)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# inputs, labels = inputs.to(device), labels.to(device)\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m feature \u001b[39m=\u001b[39m model(img)\n\u001b[1;32m     37\u001b[0m \u001b[39m# Remove the batch dimension and append the feature to the list\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m# The feature size should be (2048, 7, 7)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m features\u001b[39m.\u001b[39mappend(feature\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m))\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/WSOL/models/resnet.py:212\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    209\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m    210\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n\u001b[0;32m--> 212\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer1(x)\n\u001b[1;32m    213\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x)\n\u001b[1;32m    214\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/WSOL/models/resnet.py:117\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    114\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[1;32m    116\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(out)\n\u001b[0;32m--> 117\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn3(out)\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     residual \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample(x)\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/BillyCCAM/CCAM/venv/lib/python3.10/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 6.00 GiB total capacity; 19.09 GiB already allocated; 0 bytes free; 19.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# !pip install tqdm\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define a function to extract the features of the images\n",
    "def extract_features(model, images):\n",
    "    '''\n",
    "    Extract the features of the images using the pretrained ResNet50 model.\n",
    "    The features are extracted from the last convolutional layer.\n",
    "    '''\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Define a list to store the features\n",
    "    features = []\n",
    "\n",
    "    # Iterate over all the images\n",
    "    for img in tqdm(images):\n",
    "        # Convert the image to a torch.Tensor and normalize the image\n",
    "        # The image size should be (3, 224, 224)\n",
    "        img = transforms.ToTensor()(img)\n",
    "        img = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])(img)\n",
    "        \n",
    "        # Add a batch dimension to the image\n",
    "        # The image size should be (1, 3, 224, 224)\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "        \n",
    "        # Move the image to the device\n",
    "        img = img.to(device)\n",
    "\n",
    "        # Extract the features\n",
    "        # The feature size should be (1, 2048, 7, 7)\n",
    "        # inputs, labels = inputs.to(device), labels.to(device)\n",
    "        feature = model(img)\n",
    "\n",
    "        # Remove the batch dimension and append the feature to the list\n",
    "        # The feature size should be (2048, 7, 7)\n",
    "        features.append(feature.squeeze(0))\n",
    "\n",
    "    return features\n",
    "\n",
    "# Extract the features of the images\n",
    "# albatros_features = extract_features(model, albatros_images)\n",
    "cub_images_features = extract_features(model, cub_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature maps: 60\n",
      "Shape of the first feature map: torch.Size([1000])\n",
      "Shape of the last feature map: torch.Size([1000])\n",
      "Number of parameters: 25557032\n",
      "Number of trainable parameters: 25557032\n"
     ]
    }
   ],
   "source": [
    "# Print the number of features extracted\n",
    "print('Number of feature maps:', len(albatros_features))\n",
    "\n",
    "# Print the shape of the first feature map\n",
    "print('Shape of the first feature map:', albatros_features[0].shape)\n",
    "\n",
    "# Print the shape of the last feature map\n",
    "print('Shape of the last feature map:', albatros_features[-1].shape)\n",
    "\n",
    "# Print the number of parameters of the model\n",
    "print('Number of parameters:', sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# Print the number of trainable parameters of the model\n",
    "print('Number of trainable parameters:', sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of Albatros features: <class 'list'>\n",
      "Shape of the tensor: torch.Size([60, 1000])\n",
      "Type of the tensor: torch.float32\n",
      "Shape of the labels: (60,)\n",
      "Type of the labels: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Type of Albatros features: {type(albatros_features)}\")\n",
    "\n",
    "# Convert the list of features to a torch.Tensor\n",
    "albatros_features_tensor = torch.stack(albatros_features)\n",
    "\n",
    "# Print the shape of the tensor\n",
    "print('Shape of the tensor:', albatros_features_tensor.shape)\n",
    "\n",
    "# Print the type of the tensor\n",
    "print('Type of the tensor:', albatros_features_tensor.dtype)\n",
    "\n",
    "# Get the labels of the images\n",
    "albatros_labels_numpy = np.array([0 for _ in range(len(albatros_images))])\n",
    "print('Shape of the labels:', albatros_labels_numpy.shape)\n",
    "print('Type of the labels:', albatros_labels_numpy.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'cpu' in dir(albatros_features_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20468/2562659186.py:35: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n",
      "  axes.scatter(x_embedded[:, 0], x_embedded[:, 1], label=c, cmap=cmap)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import tsne and matplotlib\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to visualize the features\n",
    "def visualize_features(features, labels, num_classes, title):\n",
    "    '''\n",
    "    Visualize the features using t-SNE.\n",
    "    '''\n",
    "    # Get the color map\n",
    "    cmap = plt.get_cmap('tab20')\n",
    "\n",
    "    # Define the figure\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    axes = plt.axes()\n",
    "\n",
    "    # Get the list of all the classes\n",
    "    classes = [i for i in range(num_classes)]\n",
    "\n",
    "    # Iterate over all the classes\n",
    "    for i, c in enumerate(classes):\n",
    "        # Get the indexes of the images of the current class\n",
    "        indexes = np.where(labels == c)[0]\n",
    "\n",
    "        # Extract the features of the images of the current class\n",
    "        x = [features[index] for index in indexes]\n",
    "\n",
    "        # convert the features to a numpy array\n",
    "        x = np.array(x)\n",
    "\n",
    "        # Apply t-SNE to the features\n",
    "        x_embedded = TSNE(n_components=2).fit_transform(x)\n",
    "\n",
    "        # Scatter plot the points\n",
    "        axes.scatter(x_embedded[:, 0], x_embedded[:, 1], label=c, cmap=cmap)\n",
    "\n",
    "    # Set the title\n",
    "    axes.set_title(title)\n",
    "\n",
    "    # Set the legend\n",
    "    axes.legend(loc='best')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Convert the list of features to a numpy array\n",
    "# albatros_features_numpy = np.array(albatros_features_tensor.cpu().detach())\n",
    "albatros_features_numpy = albatros_features_tensor.cpu().detach().numpy()\n",
    "\n",
    "# # Convert the list of images to a numpy array\n",
    "# albatros_images_numpy = np.array(albatros_images)\n",
    "\n",
    "# # Get the labels of the images\n",
    "# albatros_labels_numpy = np.array([0 for _ in range(len(albatros_images))])\n",
    "\n",
    "# Visualize the features\n",
    "visualize_features(albatros_features_numpy, albatros_labels_numpy, 1, 'Black Footed Albatross')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# scale and move the coordinates so they fit [0; 1] range\n",
    "def scale_to_01_range(x):\n",
    "    # compute the distribution range\n",
    "    value_range = (np.max(x) - np.min(x))\n",
    " \n",
    "    # move the distribution so that it starts from zero\n",
    "    # by extracting the minimal value from all its values\n",
    "    starts_from_zero = x - np.min(x)\n",
    " \n",
    "    # make the distribution fit [0; 1] by dividing by its range\n",
    "    return starts_from_zero / value_range\n",
    " \n",
    "# extract x and y coordinates representing the positions of the images on T-SNE plot\n",
    "tx = tsne[:, 0]\n",
    "ty = tsne[:, 1]\n",
    " \n",
    "tx = scale_to_01_range(tx)\n",
    "ty = scale_to_01_range(ty)\n",
    "\n",
    "# Compute the coordinates of the image on the plot\n",
    "def compute_plot_coordinates(image, x, y, image_centers_area_size, offset):\n",
    "    image_height, image_width, _ = image.shape\n",
    " \n",
    "    # compute the image center coordinates on the plot\n",
    "    center_x = int(image_centers_area_size * x) + offset\n",
    " \n",
    "    # in matplotlib, the y axis is directed upward\n",
    "    # to have the same here, we need to mirror the y coordinate\n",
    "    center_y = int(image_centers_area_size * (1 - y)) + offset\n",
    " \n",
    "    # knowing the image center,\n",
    "    # compute the coordinates of the top left and bottom right corner\n",
    "    tl_x = center_x - int(image_width / 2)\n",
    "    tl_y = center_y - int(image_height / 2)\n",
    " \n",
    "    br_x = tl_x + image_width\n",
    "    br_y = tl_y + image_height\n",
    " \n",
    "    return tl_x, tl_y, br_x, br_y\n",
    " \n",
    "# we'll put the image centers in the central area of the plot\n",
    "# and use offsets to make sure the images fit the plot\n",
    " \n",
    "# init the plot as white canvas\n",
    "tsne_plot = 255 * np.ones((plot_size, plot_size, 3), np.uint8)\n",
    " \n",
    "# now we'll put a small copy of every image to its corresponding T-SNE coordinate\n",
    "for image_path, label, x, y in tqdm(\n",
    "        zip(albatros_labels_numpy, albatros_labels_numpy, tx, ty),\n",
    "        desc='Building the T-SNE plot',\n",
    "        total=len(images)\n",
    "):\n",
    "    image = cv2.imread(image_path)\n",
    " \n",
    "    # scale the image to put it to the plot\n",
    "    image = scale_image(image, max_image_size)\n",
    " \n",
    "    # draw a rectangle with a color corresponding to the image class\n",
    "    image = draw_rectangle_by_class(image, label)\n",
    " \n",
    "    # compute the coordinates of the image on the scaled plot visualization\n",
    "    tl_x, tl_y, br_x, br_y = compute_plot_coordinates(image, x, y, image_centers_area_size, offset)\n",
    " \n",
    "    # put the image to its t-SNE coordinates using numpy sub-array indices\n",
    "    tsne_plot[tl_y:br_y, tl_x:br_x, :] = image\n",
    " \n",
    "cv2.imshow('t-SNE', tsne_plot)\n",
    "cv2.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
