{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Arshad221b/MedSeg/blob/main/second_ty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4CHv0U1wPCG0"
   },
   "outputs": [],
   "source": [
    "import random \n",
    "import torch \n",
    "from torch.utils import data \n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rud7WqEHb8Ve",
    "outputId": "dd5d2c59-f066-4924-db66-f89a0b2af607"
   },
   "outputs": [],
   "source": [
    "# ! pip install nibabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9Q0x1-i3cEU4"
   },
   "outputs": [],
   "source": [
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mrdqqcEFPNP8"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7727guIDvSg"
   },
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ioZcoim2PRdO"
   },
   "outputs": [],
   "source": [
    "# ! unzip /content/drive/MyDrive/AMOS/amos22.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sw0PdPCnDyZJ"
   },
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "U9DyYcKammlA"
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 128\n",
    "BATCH_SIZE = 1\n",
    "NUM_CLASS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KRWGRtEvZbZK"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_7YVKIA-InO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UoQ6S3nUPGJG"
   },
   "outputs": [],
   "source": [
    "class AmosDataLoader(data.Dataset):\n",
    "  def __init__(\n",
    "      self, \n",
    "      input_paths: list, \n",
    "      target_paths: list, \n",
    "      transform_input = None, \n",
    "      transform_target = None\n",
    "  ): \n",
    "\n",
    "    self.input_paths      = input_paths\n",
    "    self.target_paths     = target_paths\n",
    "    self.transform_input  = transform_input\n",
    "    self.transform_target = transform_target\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.input_paths)\n",
    "\n",
    "  def preprocess_img_input(self, input_im):\n",
    "    # z_factor_input      = input_im.shape[2]*int(input_im.shape[0]/IMAGE_SIZE)**2\n",
    "    input_im            = np.stack((input_im,)*3, axis=-1)\n",
    "    input_im            = torch.tensor(input_im).float()/255\n",
    "    # print(input_im.shape)\n",
    "    input_im            = input_im.permute(3,2,0,1)\n",
    "    \n",
    "    input_im            = input_im.unsqueeze(0)\n",
    "    # print('input_shape before inter',input_im.shape)\n",
    "    output_size_input   = (82, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    input_im            = F.interpolate(input_im, size=output_size_input, mode='trilinear', align_corners=False)\n",
    "    input_im            = input_im#.squeeze(0)\n",
    "    return input_im\n",
    "\n",
    "  def preprocess_output(self, output_im):\n",
    "    mask_cat              = np.zeros((NUM_CLASS, *output_im.shape), dtype=np.float32)\n",
    "    for i in range(NUM_CLASS):\n",
    "        mask_cat[i][output_im == i] = 1\n",
    "    output_im             = torch.tensor(mask_cat).float()/255\n",
    "    output_im             = output_im.permute(0,2,3,1)\n",
    "    output_im             = output_im.unsqueeze(0)#.unsqueeze(0)\n",
    "    output_size_input     = (82, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    output_im             = F.interpolate(output_im, size=output_size_input, mode='trilinear', align_corners=False)\n",
    "    output_im             = output_im#.squeeze(0)\n",
    "    return output_im\n",
    "\n",
    "  def __getitem__(self,x):\n",
    "    input_image   = self.input_paths[x]\n",
    "    target_image  = self.target_paths[x]\n",
    "    input_im      = nib.load(input_image).get_fdata()\n",
    "    target_im     = nib.load(target_image).get_fdata()\n",
    "\n",
    "    return input_im, target_im\n",
    "    \n",
    "  def collate_fn(self, batch):\n",
    "    # print(len(batch[0][0]))\n",
    "    im_ins, im_outs = [], []\n",
    "    for im_in, im_out  in batch: \n",
    "      im_in = self.preprocess_img_input(im_in)\n",
    "      im_out = self.preprocess_output(im_out)\n",
    "      im_ins.append(im_in)\n",
    "      im_outs.append(im_out)\n",
    "\n",
    "    # print(torch.tensor(im_ins).shape)\n",
    "    return torch.cat(im_ins, dim = 0).to(device), torch.cat(im_outs, dim= 0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hrVS5thiZtF_"
   },
   "outputs": [],
   "source": [
    "path = '/home/arshad/Downloads/amos22/amos22'\n",
    "\n",
    "\n",
    "input_paths   = sorted(glob(os.path.join(path, \"imagesTr\",\"*.nii.gz\")))[:25]\n",
    "target_paths  = sorted(glob(os.path.join(path, \"labelsTr\",\"*.nii.gz\")))[:25]\n",
    "\n",
    "val_input = sorted(glob(os.path.join(path, \"imagesVa\",\"*.nii.gz\")))[:5]\n",
    "val_target = sorted(glob(os.path.join(path, \"labelsVa\",\"*.nii.gz\")))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_paths = input_paths[:25]\n",
    "# target_paths = target_paths[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labelsTs',\n",
       " 'imagesTr',\n",
       " 'imagesVa',\n",
       " 'labelsVa',\n",
       " 'dataset.json',\n",
       " 'readme.md',\n",
       " '.DS_Store',\n",
       " 'imagesTs',\n",
       " 'labelsTr']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fioLA0sqaUru"
   },
   "outputs": [],
   "source": [
    "train_dl      = AmosDataLoader(input_paths, target_paths)\n",
    "train_loader  = DataLoader(train_dl, batch_size = BATCH_SIZE, drop_last= True, collate_fn=train_dl.collate_fn)\n",
    "\n",
    "val_dl      = AmosDataLoader(val_input, val_target)\n",
    "val_loader  = DataLoader(val_dl, batch_size = BATCH_SIZE, drop_last= True, collate_fn=train_dl.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGsVE4joDrAp"
   },
   "source": [
    "# Model Building\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tAXk92Ooa0YO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Down3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool3d(2),\n",
    "            DoubleConv3D(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mpconv(x)\n",
    "\n",
    "class Up3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose3d(in_channels, in_channels//2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv3D(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # print(x1.shape, x2.shape)\n",
    "        x1    = self.up(x1)\n",
    "        # print(x1.shape)\n",
    "        diffZ = x2.size()[2] - x1.size()[2]\n",
    "        diffY = x2.size()[3] - x1.size()[3]\n",
    "        diffX = x2.size()[4] - x1.size()[4]\n",
    "        x1    = F.pad(x1, (diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2,\n",
    "                        diffZ // 2, diffZ - diffZ // 2))\n",
    "        x     = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class OutConv3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv3D, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=False):\n",
    "        super().__init__()\n",
    "        self.in_channels  = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.bilinear     = bilinear\n",
    "\n",
    "        self.conv1    = DoubleConv3D(in_channels, 64)\n",
    "        self.down1    = Down3D(64, 128)\n",
    "        self.down2    = Down3D(128, 256)\n",
    "        self.down3    = Down3D(256, 512)\n",
    "        self.down4    = Down3D(512, 1024)\n",
    "        self.up1      = Up3D(1024, 512, bilinear)\n",
    "        self.up2      = Up3D(512, 256, bilinear)\n",
    "        self.up3      = Up3D(256, 128, bilinear)\n",
    "        self.up4      = Up3D(128, 64, bilinear)\n",
    "        self.outconv  = OutConv3D(64, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        # x = x.unsqueeze(1)\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        # print(x5.shape, x4.shape)\n",
    "        x6 = self.up1(x5, x4)\n",
    "        x7 = self.up2(x6, x3)\n",
    "        x8 = self.up3(x7, x2) \n",
    "        x9 = self.up4(x8, x1)\n",
    "        output= self.outconv(x9)\n",
    "        # print(x6.shape)\n",
    "        # up network\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WYdfm81hGCRD"
   },
   "outputs": [],
   "source": [
    "model = UNet3D(3, 15).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUNIpQ1h0NrO"
   },
   "source": [
    "# Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(input, target):\n",
    "    smooth = 1.0\n",
    "\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "\n",
    "    return 1 - ((2. * intersection + smooth) /\n",
    "              (iflat.sum() + tflat.sum() + smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "avXePzyVKpha"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_batch(data, model, optimizer, criteria):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    ims_in, ims_out = data\n",
    "    optimizer.zero_grad()\n",
    "    pred_img = model(ims_in)\n",
    "    # total_loss = criteria(pred_img, ims_out)\n",
    "    dice_train = dice_loss(pred_img, ims_out)\n",
    "    dice_train.backward()\n",
    "    optimizer.step()\n",
    "    return total_loss, dice_train\n",
    "\n",
    "\n",
    "def validation_batch(data, model, criteria):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ims_in, ims_out = data\n",
    "    with torch.no_grad():\n",
    "      pred_img = model(ims_in)\n",
    "      # torch.empty_cache()\n",
    "    # total_loss = criteria(pred_img, ims_out)\n",
    "    dice_val = dice_loss(pred_img, ims_out)\n",
    "    return total_loss, dice_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ekD7Nc3cSoar"
   },
   "outputs": [],
   "source": [
    "n_epoch = 5\n",
    "criteria = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WoSuiFsNTs76",
    "outputId": "372dea96-5e63-4b1e-9bd2-949c827373fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "train_avg_loss tensor(0.1944, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "train_dice_loss tensor(1.0008, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "val_avg_loss tensor(0.1415, device='cuda:0')\n",
      "val_dice_loss tensor(0.9995, device='cuda:0')\n",
      "1\n",
      "train_avg_loss tensor(0.2141, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "train_dice_loss tensor(0.9973, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "val_avg_loss tensor(0.2846, device='cuda:0')\n",
      "val_dice_loss tensor(1.0043, device='cuda:0')\n",
      "2\n",
      "train_avg_loss tensor(0.1915, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "train_dice_loss tensor(0.9912, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "val_avg_loss tensor(0.1591, device='cuda:0')\n",
      "val_dice_loss tensor(1.0031, device='cuda:0')\n",
      "3\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.65 GiB total capacity; 21.07 GiB already allocated; 315.31 MiB free; 22.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m val_dice_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ix, ims \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m---> 13\u001b[0m   loss, dice_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriteria\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m   train_epoch_loss\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     15\u001b[0m   train_dice_loss\u001b[38;5;241m.\u001b[39mappend(dice_train)\n",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m, in \u001b[0;36mtrain_batch\u001b[0;34m(data, model, optimizer, criteria)\u001b[0m\n\u001b[1;32m      7\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m criteria(pred_img, ims_out)\n\u001b[1;32m      8\u001b[0m dice_train \u001b[38;5;241m=\u001b[39m dice_loss(pred_img, ims_out)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mdice_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss, dice_train\n",
      "File \u001b[0;32m~/Downloads/venv/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 328.00 MiB (GPU 0; 23.65 GiB total capacity; 21.07 GiB already allocated; 315.31 MiB free; 22.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "train_dices = []\n",
    "val_dice = []\n",
    "val_loss = []\n",
    "for epoch in range(n_epoch):\n",
    "  print(epoch)\n",
    "  train_epoch_loss = []\n",
    "  train_dice_loss = []\n",
    "  val_epoch_loss = []\n",
    "  val_dice_loss = []\n",
    "    \n",
    "  for ix, ims in enumerate(train_loader):\n",
    "    loss, dice_train = train_batch(ims, model, optimizer, criteria)\n",
    "    train_epoch_loss.append(loss)\n",
    "    train_dice_loss.append(dice_train)\n",
    "    \n",
    "  print('train_avg_loss', sum(train_epoch_loss)/len(train_epoch_loss))\n",
    "  train_loss.append(sum(train_epoch_loss)/len(train_epoch_loss))\n",
    "    \n",
    "  print('train_dice_loss', sum(train_dice_loss)/len(train_dice_loss))\n",
    "  train_dices.append(sum(train_dice_loss)/len(train_dice_loss))\n",
    "    \n",
    "  for ix, ims in enumerate(val_loader):\n",
    "    loss, dice_val = validation_batch(ims, model,  criteria)\n",
    "    val_epoch_loss.append(loss)\n",
    "    val_dice_loss.append(dice_val)\n",
    "  \n",
    "  print('val_avg_loss', sum(val_epoch_loss)/len(val_epoch_loss))\n",
    "  print('val_dice_loss', sum(val_dice_loss)/len(val_dice_loss))\n",
    "\n",
    "  val_loss.append(sum(val_epoch_loss)/len(val_epoch_loss))\n",
    "  val_dice.append(sum(val_dice_loss)/len(val_dice_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtnNM1vRVEKx"
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'models')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPauRa/mx8GFCDYnEHvCNI0",
   "collapsed_sections": [
    "B7727guIDvSg"
   ],
   "include_colab_link": true,
   "mount_file_id": "15Fdb5Qh4O08i6gsGbw8ZCiMwIXiRtFH5",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
