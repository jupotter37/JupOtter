{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12d400e8-a290-4fdf-bd0b-cf08984a6d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import _pickle as cPickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "from torchvision.models import ResNet50_Weights\n",
    "\n",
    "torch.set_default_device('cpu') \n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "IMAGENET_PATH = \"../data/Imagenet32_val/\"\n",
    "PATH = \"../2.NeuralHashing/mainModel.pth\"\n",
    "HASH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b7b8042-281e-4295-92c5-0b9bb479aad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu124\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7dc06768-b862-4546-b29e-90ee790b4dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = cPickle.load(fo)\n",
    "    return dict\n",
    "\n",
    "def load_databatch(data_folder, idx, img_size=32):\n",
    "    data_file = os.path.join(data_folder, 'val_data')\n",
    "\n",
    "    dict_ = unpickle(data_file) #+ str(idx))\n",
    "\n",
    "    images = dict_['data']\n",
    "    labels = dict_['labels']\n",
    "\n",
    "    ## images_without_mean = dict_['mean']   ??? check is there are better results or not\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15876c35-6c5d-4a30-9492-1757d9466190",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_databatch(IMAGENET_PATH, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e29bac35-c72f-4d75-ad29-e51688d8adf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of images: 50000\n",
      "372\n"
     ]
    }
   ],
   "source": [
    "print( \"size of images:\", len(labels) )\n",
    "print(labels[0])\n",
    "\n",
    "images2 = images.astype(np.float32).reshape(50000,3,32,32)\n",
    "dataset = CustomDataset(torch.from_numpy(images2), torch.Tensor(labels) )\n",
    "ImageNetLoader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6395beb4-dc6c-42f0-bd90-db692db4aa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest = torch.Tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6de9504a-5d47-4872-ab85-567709eaa3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(372.)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e8402e38-e0f7-4172-823d-80638ff131ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, bits):\n",
    "        super().__init__()\n",
    "        self.hash1 = nn.Linear(1000, 64)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        hash_ = self.hash1(x)\n",
    "        #hash_ = hash_.softmax(1)\n",
    "        hash_ = self.sigmoid(hash_)\n",
    "        hash_ = torch.round(hash_)\n",
    "        return x.softmax(1), hash_\n",
    "\n",
    "net = Net( HASH_SIZE )\n",
    "net.to(device)\n",
    "\n",
    "resnet50 = models.resnet50( pretrained = True ) #weights=ResNet50_Weights.DEFAULT)\n",
    "resnet50 = resnet50.to(device)\n",
    "\n",
    "model = nn.Sequential(resnet50, net)\n",
    "#model = resnet50\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2386c105-9155-420c-ad86-defac3771658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(PATH))\n",
    "model = model.to(device)\n",
    "\n",
    "#criterion2 = nn.CrossEntropyLoss()\n",
    "#optimizer2 = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "#iterations = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30ecf7dc-af77-4127-a07c-742474ad4a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range( 10 ):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i, data in enumerate(ImageNetLoader, 0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        #optimizer2.zero_grad()\n",
    "        #outputs, hash_ = model(inputs)\n",
    "        \n",
    "        \n",
    "        #print(hash_[0])\n",
    "        #print(hash_[1])\n",
    "\n",
    "        break;\n",
    "\n",
    "    #print(f'[{epoch + 1}] loss: {running_loss} accurancy: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e817054e-4207-462c-a5b3-13afc14aefb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(982, device='cuda:0')\n",
      "tensor(356., device='cuda:0')\n",
      "Accuracy of the network on the 10000 test images: 0 %\n"
     ]
    }
   ],
   "source": [
    "#model = model.eval()\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in ImageNetLoader:\n",
    "\n",
    "        img, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs, hash_ = model(img)\n",
    "\n",
    "        #print(labels)\n",
    "        #print(outputs)\n",
    "        \n",
    "        target, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        #print(target)\n",
    "        #print(predicted)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        plt.imshow(img[0].cpu().type(torch.int8).T, interpolation='nearest')\n",
    "        plt.show()\n",
    "\n",
    "        print(predicted[0])\n",
    "        print(labels[0])\n",
    "\n",
    "        break;\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd463c1-33d8-4fa8-bb06-e05217f2cadc",
   "metadata": {},
   "source": [
    "### CuNN with ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e825d54b-b7bb-4a78-a0f5-79cd00e29f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class cuNN:\n",
    "    def __init__(self, size, hash_size, images, size_images ):\n",
    "        self.buckets = []\n",
    "        self.bandsNumber = size\n",
    "        self.rows = int(hash_size**2 / self.bandsNumber)\n",
    "        self.hash_buckets_list: List[Dict[int, List[str]]] = [dict() for _ in range(self.bandsNumber)]\n",
    "        self.signatures = np.zeros([len(images), hash_size ]).astype(np.float32)\n",
    "        self.hash_size = hash_size\n",
    "        self.images = images;\n",
    "        self.size_ = size_images\n",
    "\n",
    "        for i in range(size):\n",
    "            self.buckets.append({})\n",
    "            \n",
    "    def fill_buckets(self, signature, idx):\n",
    "        for i in range(self.bandsNumber):\n",
    "            signature_band = signature[i*self.rows:(i+1)*self.rows]\n",
    "            signature_band_bytes = signature_band.tobytes()\n",
    "            if signature_band_bytes not in self.hash_buckets_list[i]:\n",
    "                self.hash_buckets_list[i][signature_band_bytes] = list()\n",
    "            self.hash_buckets_list[i][signature_band_bytes].append(idx)\n",
    "\n",
    "    def compute_hash(self):\n",
    "        \n",
    "        tensor_image = torch.FloatTensor(self.images).to(\"cuda:0\")\n",
    "        label, hash_ = model(tensor_image)\n",
    "        signatures = torch.round(hash_).cpu().detach().numpy().astype(np.int8)\n",
    "        \n",
    "        idx = 0\n",
    "        for signature in signatures:\n",
    "            self.signatures[idx] = np.packbits( signature ).astype(np.float32)\n",
    "            self.fill_buckets(signature, idx)\n",
    "            idx+=1\n",
    "        return self.signatures\n",
    "    \n",
    "    def compute_candidate(self):\n",
    "        candidate_pairs = set()\n",
    "\n",
    "        for hash_buckets in self.hash_buckets_list:\n",
    "            for hash_bucket in hash_buckets.values():\n",
    "                if len(hash_bucket) > 1:\n",
    "                    hash_bucket = sorted(hash_bucket)\n",
    "                    for i in range(len(hash_bucket)):\n",
    "                        for j in range(i+1, len(hash_bucket)):\n",
    "                            candidate_pairs.add(\n",
    "                               tuple([hash_bucket[i], hash_bucket[j]])\n",
    "                            )\n",
    "        return candidate_pairs\n",
    "    \n",
    "    def check_candidate(self, candidate_pairs, threshold=0.9):\n",
    "        near_duplicates = list()\n",
    "        for cpa, cpb in candidate_pairs:\n",
    "            item_cpa = np.array(self.signatures[cpa]).astype(np.uint8)\n",
    "            item_cpb = np.array(self.signatures[cpb]).astype(np.uint8)\n",
    "            hd = sum(np.bitwise_xor(\n",
    "                    np.unpackbits(item_cpa), \n",
    "                    np.unpackbits(item_cpb)\n",
    "            ))\n",
    "            similarity = (self.hash_size**2 - hd) / self.hash_size**2\n",
    "            if similarity > threshold:\n",
    "                near_duplicates.append((cpa, cpb, similarity))\n",
    "                \n",
    "        # Sort near-duplicates by descending similarity and return\n",
    "        near_duplicates.sort(key=lambda x:x[2], reverse=True)\n",
    "        return near_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "56365fd8-8426-4702-ab32-1a74a381d0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "IMAGES = images[0:5000].reshape(5000,3,32,32)\n",
    "print(IMAGES.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d34ec3d7-203d-4941-94d8-3a5ab9432d5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 314.00 MiB. GPU 0 has a total capacity of 5.92 GiB of which 4.06 MiB is free. Process 8038 has 84.00 MiB memory in use. Process 59807 has 5.79 GiB memory in use. Of the allocated memory 5.63 GiB is allocated by PyTorch, and 85.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m nnLSH \u001b[38;5;241m=\u001b[39m cuNN(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m8\u001b[39m, IMAGES, \u001b[38;5;241m5000\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m signatures \u001b[38;5;241m=\u001b[39m \u001b[43mnnLSH\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_hash\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m pairs \u001b[38;5;241m=\u001b[39m nnLSH\u001b[38;5;241m.\u001b[39mcompute_candidate()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(pairs))\n",
      "Cell \u001b[0;32mIn[39], line 28\u001b[0m, in \u001b[0;36mcuNN.compute_hash\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_hash\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     27\u001b[0m     tensor_image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m     label, hash_ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     signatures \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mround(hash_)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint8)\n\u001b[1;32m     31\u001b[0m     idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[1;32m    270\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py:79\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 314.00 MiB. GPU 0 has a total capacity of 5.92 GiB of which 4.06 MiB is free. Process 8038 has 84.00 MiB memory in use. Process 59807 has 5.79 GiB memory in use. Of the allocated memory 5.63 GiB is allocated by PyTorch, and 85.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "nnLSH = cuNN(16, 8, IMAGES, 5000)\n",
    "signatures = nnLSH.compute_hash()\n",
    "\n",
    "pairs = nnLSH.compute_candidate()\n",
    "\n",
    "print(len(pairs))\n",
    "duplicates = nnLSH.check_candidate(pairs, 0.9)\n",
    "print(len(duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297d63f7-8118-4d86-9db7-42a8d46577a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_row = 10\n",
    "num_col = 6\n",
    "\n",
    "idx_start = 0;\n",
    "\n",
    "fig, axes = plt.subplots(num_row, num_col, figsize=(3*num_col,3*num_row))\n",
    "for i in range(0, num_row*num_col, 2):\n",
    "    ax = axes[i//num_col, i%num_col]\n",
    "    ax.imshow(IMAGES[duplicates[i//2 + idx_start][0]].astype(np.uint8).T)\n",
    "    ax.set_title(f\"\"\"Label: {round(duplicates[i//2 + idx_start][2], 4)}, {labels[duplicates[i//2 + idx_start][0]]}, {duplicates[i//2 + idx_start][0]}\"\"\")\n",
    "\n",
    "    ax = axes[(i+1)//num_col, (i+1)%num_col]\n",
    "    ax.imshow(IMAGES[duplicates[i//2 + idx_start][1]].astype(np.uint8).T)\n",
    "    ax.set_title(f\"\"\"Label: {round(duplicates[i//2 + idx_start][2], 4)}, {labels[duplicates[i//2 + idx_start][1]]}, {duplicates[i//2 + idx_start][1]}\"\"\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338176ab-9661-4f26-ae6e-c4ca461d8c56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1eed94-9ff7-472e-b43e-cdc586e4ea98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe82d3a0-1d88-4d47-a9f6-5a27dfd8e460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e908134-2618-4ae7-8470-f3656a1aa6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
