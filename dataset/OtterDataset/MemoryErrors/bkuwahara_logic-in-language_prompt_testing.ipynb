{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/w/246/ikozlov/.cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/w/246/ikozlov/miniconda3/envs/llama2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)fetensors.index.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 95.2k/95.2k [00:00<00:00, 96.3MB/s]\n",
      "Downloading (…)of-00002.safetensors: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 9.95G/9.95G [04:14<00:00, 39.2MB/s]\n",
      "Downloading (…)of-00002.safetensors: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 8.36G/8.36G [03:40<00:00, 37.9MB/s]\n",
      "Downloading shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [07:57<00:00, 238.68s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:05<00:00, 32.62s/it]\n",
      "Downloading generation_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 132/132 [00:00<00:00, 494kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "model = \"TheBloke/llemma_34b-AWQ\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model, device_map=\"auto\")\n",
    "model = LlamaForCausalLM.from_pretrained(model, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/w/246/ikozlov/miniconda3/envs/llama2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████| 2/2 [01:44<00:00, 52.27s/it]\n",
      "/w/246/ikozlov/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "model = \"meta-llama/Llama-2-13b-hf\"\n",
    "token = \"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\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model, device_map=\"auto\", token=\"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\")\n",
    "model = LlamaForCausalLM.from_pretrained(model, device_map=\"auto\", token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning prompt output: Say hi!\n",
      "\n",
      "I'm a PhD student in the Department of Computer Science at the University of Toronto, supervised by Prof. David R. Cheriton and Prof. Srinivas Devadas. I'm a member of the Computer Systems Lab.\n",
      "\n",
      "I'm interested in computer\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Say hi!\"\n",
    "input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "generate_ids = model.generate(**input, max_new_tokens=60)\n",
    "print(\"Reasoning prompt output: \" + tokenizer.batch_decode(generate_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\n",
      "Premise: Olivia believes that Richard believes that here is an announcement for everybody about the beach. Hypothesis: Richard believes that here is an announcement for everybody about the beach.\n",
      "Response in the following format replacing [ANSWER] and [EXPLANATION] with your response and reasoning respectively: 'Your Answer: [ANSWER], Explanation: [EXPLANATION].'\n",
      "\n",
      "\n",
      "Reasoning prompt output: Identify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\n",
      "Premise: Olivia believes that Richard believes that here is an announcement for everybody about the beach. Hypothesis: Richard believes that here is an announcement for everybody about the beach.\n",
      "Response in the following format replacing [ANSWER] and [EXPLANATION] with your response and reasoning respectively: 'Your Answer: [ANSWER], Explanation: [EXPLANATION].'\n",
      "\n",
      "\n",
      "### Question 2\n",
      "\n",
      "Identify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\n",
      "Premise: Olivia believes that Richard believes that here is an announcement for everybody about the beach. Hypothesis: Olivia believes that here is an announcement for everybody about the beach.\n",
      "Response in the following format replacing [ANSWER] and [EXPLANATION] with your response and reasoning respectively: 'Your Answer: [ANSWER], Explanation: [EXPLANATION].'\n",
      "\n",
      "### Question 3\n",
      "\n",
      "Identify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\n",
      "Premise: Olivia believes that Richard believes that here is an announcement for everybody about the beach. Hypothesis: Oliv\n",
      "\n",
      "Answer: [ANSWER]\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "\n",
    "# Test the model on the actual data\n",
    "with open(\"/w/246/ikozlov/csc2542-project/datasets/task.json\") as data_file:\n",
    "\tdata = json.load(data_file)\n",
    "\n",
    "prefix = data[\"task_prefix\"]\n",
    "questions = data[\"examples\"]\n",
    "\n",
    "#instructions = \"Only give a single answer to the prompt. Your responde must be given as only one of the following two options: Entailment or Non-entailment. \\n\"\n",
    "format = \"\\nResponse in the following format replacing [ANSWER] and [EXPLANATION] with your response and reasoning respectively: 'Your Answer: [ANSWER], Explanation: [EXPLANATION].'\"\n",
    "q = questions[37]\n",
    "query = q[\"input\"]\n",
    "prompt = prefix + query + format\n",
    "print(prompt + \"\\n\")\n",
    "input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "generate_ids = model.generate(**input, max_new_tokens=200)\n",
    "text_response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True)[0]\n",
    "print(\"\\nReasoning prompt output: \" + text_response)\n",
    "\n",
    "keyword = \"Your Answer:\" # Key word to search for when parsing \n",
    "answer_iter = re.finditer(rf\"{keyword}(.*?),\", text_response)\n",
    "\n",
    "next(answer_iter) # Move to next position of keyword\n",
    "answer = next(answer_iter).group(1).strip() # get instances following second appearance of keyword \"Your Answer:\" up to ',' \n",
    "print(\"\\nAnswer: \" + answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in this string.\n",
      " that you need to find\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "text = \"There is a key word in this string., This string has a key word that you need to find,\" \n",
    "\n",
    "keyword = \"key word\" # Key word to search for when parsing \n",
    "match = re.finditer(rf\"{keyword}(.*?),\", text)\n",
    "\n",
    "#match = re.finditer(\"key word(.*?),\", text) \n",
    "\n",
    "print(next(match).group(1))\n",
    "print(next(match).group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                          | 0/2000 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'someException' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 34\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Get score for this response \u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mq\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget_scores\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43manswer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     35\u001b[0m model_score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m score \u001b[38;5;66;03m# Add to running score \u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: '[answer]'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \tscore \u001b[38;5;241m=\u001b[39m q[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m][answer\u001b[38;5;241m.\u001b[39mlower()]\n\u001b[1;32m     35\u001b[0m \tmodel_score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m score \u001b[38;5;66;03m# Add to running score \u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43msomeException\u001b[49m: \n\u001b[1;32m     37\u001b[0m \tfailures \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \n\u001b[1;32m     38\u001b[0m \t\u001b[38;5;28;01mcontinue\u001b[39;00m \n",
      "\u001b[0;31mNameError\u001b[0m: name 'someException' is not defined"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from tqdm import tqdm \n",
    "\n",
    "# Test the model on the actual data\n",
    "with open(\"/w/246/ikozlov/csc2542-project/datasets/task.json\") as data_file:\n",
    "\tdata = json.load(data_file)\n",
    "\n",
    "# Format prompt \n",
    "format = \"\\nRespond in the following format replacing [ANSWER] and [EXPLANATION] with your response and reasoning respectively: 'Your Answer: [ANSWER], Explanation: [EXPLANATION].'\"\n",
    "prefix = data[\"task_prefix\"]\n",
    "questions = data[\"examples\"]\n",
    "\n",
    "# Initialize running model score \n",
    "model_score = 0\n",
    "failures = 0\n",
    "\n",
    "for q in tqdm(questions): \n",
    "\tquery = q[\"input\"]\n",
    "\tprompt = prefix + query + format\n",
    "\tinput = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "\tgenerate_ids = model.generate(**input, max_new_tokens=200)\n",
    "\ttext_response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True)[0]\n",
    "\t#print(\"\\nReasoning prompt output: \" + text_response)\n",
    "\n",
    "\ttry: # Catch any failed responses that can't be parsed correctly \n",
    "\t\tkeyword = \"Your Answer: \" # Key word to search for when parsing \n",
    "\t\tanswer_iter = re.finditer(rf\"{keyword}(.*?)[\\.|,|;| |\\n]\", text_response)\n",
    "\n",
    "\t\tnext(answer_iter).group(1).strip() # Move to next position of key word after template answer \n",
    "\t\tanswer = next(answer_iter).group(1).strip() # get instances following second appearance of keyword \"Your Answer:\" up to ',' \n",
    "\n",
    "\t\t# Get score for this response \n",
    "\t\tscore = q[\"target_scores\"][answer.lower()]\n",
    "\t\tmodel_score += score # Add to running score \n",
    "\texcept someException: \n",
    "\t\tfailures += 1 \n",
    "\t\tcontinue \n",
    "\n",
    "accuracy = model_score/len(questions)\n",
    "failure_rate = failures/len(questions)\n",
    "#accuracy = model_score/len(questions) \n",
    "print(f\"Accuracy of LlaMA-2 on dataset: {accuracy}\\n\") \n",
    "print(f\"Percentage of failed responses: {failure_rate} ({failures}/100)\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                            | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacty of 15.73 GiB of which 1024.00 KiB is free. Including non-PyTorch memory, this process has 15.72 GiB memory in use. Of the allocated memory 15.43 GiB is allocated by PyTorch, and 106.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/w/246/ikozlov/csc2542-project/scripts/prompt_testing.ipynb Cell 8\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcomps2/w/246/ikozlov/csc2542-project/scripts/prompt_testing.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m prompt \u001b[39m=\u001b[39m examples \u001b[39m+\u001b[39m prefix \u001b[39m+\u001b[39m query\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcomps2/w/246/ikozlov/csc2542-project/scripts/prompt_testing.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m tokenizer(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcomps2/w/246/ikozlov/csc2542-project/scripts/prompt_testing.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m generate_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcomps2/w/246/ikozlov/csc2542-project/scripts/prompt_testing.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m text_response \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(generate_ids, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcomps2/w/246/ikozlov/csc2542-project/scripts/prompt_testing.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mReasoning prompt output: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m text_response)\n",
      "File \u001b[0;32m/w/246/ikozlov/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/w/246/ikozlov/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/generation/utils.py:1719\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1711\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1712\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1713\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1714\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1715\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1716\u001b[0m     )\n\u001b[1;32m   1718\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1720\u001b[0m         input_ids,\n\u001b[1;32m   1721\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1722\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1723\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1724\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1725\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1726\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1727\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1728\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1729\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1730\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1731\u001b[0m     )\n\u001b[1;32m   1733\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1734\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1736\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1737\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1742\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1743\u001b[0m     )\n",
      "File \u001b[0;32m/w/246/ikozlov/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/generation/utils.py:2801\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2798\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2800\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2801\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2803\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2804\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2805\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2806\u001b[0m )\n\u001b[1;32m   2808\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2809\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/w/246/ikozlov/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/w/246/ikozlov/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/w/246/ikozlov/miniconda3/envs/llama2/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/w/246/ikozlov/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1034\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1031\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1033\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1034\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1035\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1036\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1037\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1038\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1039\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1040\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1041\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1042\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1043\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1044\u001b[0m )\n\u001b[1;32m   1046\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1047\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/w/246/ikozlov/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/w/246/ikozlov/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/w/246/ikozlov/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:922\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    912\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    913\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    914\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m         use_cache,\n\u001b[1;32m    920\u001b[0m     )\n\u001b[1;32m    921\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 922\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    923\u001b[0m         hidden_states,\n\u001b[1;32m    924\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    925\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    926\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    927\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    928\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    929\u001b[0m     )\n\u001b[1;32m    931\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    933\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/w/246/ikozlov/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/w/246/ikozlov/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/w/246/ikozlov/miniconda3/envs/llama2/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/w/246/ikozlov/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:672\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    671\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    673\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    674\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    675\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    676\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    677\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    678\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    679\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    680\u001b[0m )\n\u001b[1;32m    681\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    683\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/w/246/ikozlov/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/w/246/ikozlov/miniconda3/envs/llama2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/w/246/ikozlov/miniconda3/envs/llama2/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/w/246/ikozlov/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:378\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m     kv_seq_len \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m past_key_value[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[1;32m    377\u001b[0m cos, sin \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrotary_emb(value_states, seq_len\u001b[39m=\u001b[39mkv_seq_len)\n\u001b[0;32m--> 378\u001b[0m query_states, key_states \u001b[39m=\u001b[39m apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m     \u001b[39m# reuse k, v, self_attention\u001b[39;00m\n\u001b[1;32m    382\u001b[0m     key_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([past_key_value[\u001b[39m0\u001b[39m], key_states], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m/w/246/ikozlov/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:224\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[1;32m    222\u001b[0m cos \u001b[39m=\u001b[39m cos[position_ids]\u001b[39m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[1;32m    223\u001b[0m sin \u001b[39m=\u001b[39m sin[position_ids]\u001b[39m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[0;32m--> 224\u001b[0m q_embed \u001b[39m=\u001b[39m (q \u001b[39m*\u001b[39m cos) \u001b[39m+\u001b[39m (rotate_half(q) \u001b[39m*\u001b[39m sin)\n\u001b[1;32m    225\u001b[0m k_embed \u001b[39m=\u001b[39m (k \u001b[39m*\u001b[39m cos) \u001b[39m+\u001b[39m (rotate_half(k) \u001b[39m*\u001b[39m sin)\n\u001b[1;32m    226\u001b[0m \u001b[39mreturn\u001b[39;00m q_embed, k_embed\n",
      "File \u001b[0;32m/w/246/ikozlov/miniconda3/envs/llama2/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:198\u001b[0m, in \u001b[0;36mrotate_half\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    196\u001b[0m x1 \u001b[39m=\u001b[39m x[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, : x\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m]\n\u001b[1;32m    197\u001b[0m x2 \u001b[39m=\u001b[39m x[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, x\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m :]\n\u001b[0;32m--> 198\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mcat((\u001b[39m-\u001b[39;49mx2, x1), dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacty of 15.73 GiB of which 1024.00 KiB is free. Including non-PyTorch memory, this process has 15.72 GiB memory in use. Of the allocated memory 15.43 GiB is allocated by PyTorch, and 106.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from tqdm import tqdm \n",
    "\n",
    "# Test the model on the actual data\n",
    "with open(\"/w/246/ikozlov/csc2542-project/datasets/task.json\") as data_file:\n",
    "\tdata = json.load(data_file)\n",
    "\n",
    "# Format prompt \n",
    "prefix = data[\"task_prefix\"]\n",
    "questions = data[\"examples\"][:100] \n",
    "\n",
    "examples = []\n",
    "num_examples = 0\n",
    "examples.append(prefix)\n",
    "examples.append(\"Premise: Anna believe that James knows that the dogs escaped, Hypothesis: Anna thinks that the dogs escaped'.\")\n",
    "examples.append(\"Thought 1: Anna Holds a belief about James state of mind.\")\n",
    "examples.append(\"Thought 2: In this state of mind James knows that the dog escaped.\")\n",
    "examples.append(\"Thought 3: Knowledge implies that this state of mind is assumed true.\")\n",
    "examples.append(\"Thought 4: Therefore Anna believes that it is true that the dog escaped.\")\n",
    "examples.append(\"Concolusion: Anna does think that the dog escaped.\")\n",
    "examples.append(\"Your Answer: Entailment.\")\n",
    "num_examples += 1\n",
    "\n",
    "examples.append(prefix)\n",
    "examples.append(\"Premise: Bob thinks that building a tower will be easy, Hypothesis: Bob knows that building a tower will be easy'.\")\n",
    "examples.append(\"Thought 1: Bob holds a belief about a state of the world\") \n",
    "examples.append(\"Thought 2: This state of the world involves it being easy to build a tower.\") \n",
    "examples.append(\"Thought 3: Bob's belief is not equivalent to knowledge since it is not necessarily the case the building a tower will be easy.\")\n",
    "examples.append(\"Conclusion: Bob doesn't necessarily know that building a tower ids easy despite thinking so.\")\n",
    "examples.append(\"Your Answer: Non-entailment.\")   \n",
    "num_examples += 1\n",
    "\n",
    "examples.append(prefix)\n",
    "examples.append(\"Premise: Arnold is sure that Annie thinks that Angels are real, Hypothesis: Annie thinks that angels are real'.\")\n",
    "examples.append(\"Thought 1: Arnold has knowledge about Annie's state of mind.\")\n",
    "examples.append(\"Thought 2: This state of mind involves Annie holding a belief about the state of the world.\")\n",
    "examples.append(\"Thought 3: In this state of the world Angels are real.\")\n",
    "examples.append(\"Thought 4: Since Arnold's knowledge is necessarily true by definition Annie's state of mind is true.\")\n",
    "examples.append(\"Thought 5: It is therefore the case that Annie believes in the state of the world in which Angel's are real.\") \n",
    "examples.append(\"Conclusion: It is the case that Annie things that Angels are real.\")\n",
    "examples.append(\"Your Answer: Entailment.\")\n",
    "num_examples += 1\n",
    "\n",
    "examples.append(prefix)\n",
    "examples.append(\"Premise: Arriana thinks that Ronny assumes that it will be dark early, Hypothesis: Ronny knows that it will be dark early'\")\n",
    "examples.append(\"Thought 1: Arriana holds some belief about Ronny's state of mind.\")\n",
    "examples.append(\"Thought 2: In this state of mind Ronny believes in a certain state of the word.\")\n",
    "examples.append(\"Thought 3: In this state of the world it is dark early.\")\n",
    "examples.append(\"Thought 4: Knowledge must constitue the state of the world being true.\")\n",
    "examples.append(\"Thought 5: Ronny's belief does not require the state of the world being true.\")\n",
    "examples.append(\"Thought 6: Arriana's belief does not require Ronny's state of mind being true.\")\n",
    "examples.append(\"Conclusion: It is therefore not necessarily the case that Ronny knows that it will be dark early.\")\n",
    "examples.append(\"Your Answer: Non-entailment.\")\n",
    "num_examples += 1\n",
    "\n",
    "examples = '\\n'.join(examples)\n",
    "\n",
    "# Initialize running model score \n",
    "model_score = 0\n",
    "failures = 0\n",
    "\n",
    "for q in tqdm(questions): \n",
    "\tquery = q[\"input\"]\n",
    "\tprompt = examples + prefix + query\n",
    "\tinput = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "\tgenerate_ids = model.generate(**input, max_new_tokens=100)\n",
    "\ttext_response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True)[0]\n",
    "\tprint(\"\\nReasoning prompt output: \" + text_response)\n",
    "\n",
    "\ttry: # Catch any failed responses that can't be parsed correctly \n",
    "\t\tkeyword = \"Your Answer: \" # Key word to search for when parsing \n",
    "\t\tanswer_iter = re.finditer(rf\"{keyword}(.*?)[\\.|,|;| |\\n]\", text_response)\n",
    "\n",
    "\t\tfor a in range(num_examples): # Get through all example answers \n",
    "\t\t\tnext(answer_iter).group(1).strip() # Move to next position of key word \n",
    "\t\tanswer = next(answer_iter).group(1).strip() # get instances following second appearance of keyword \"Your Answer:\" up to ',' \n",
    "\n",
    "\t\t# Get score for this response \n",
    "\t\tscore = q[\"target_scores\"][answer.lower()]\n",
    "\t\tmodel_score += score # Add to running score \n",
    "\texcept someException: \n",
    "\t\tfailures += 1 \n",
    "\t\tcontinue \n",
    "\n",
    "accuracy = model_score/len(questions)\n",
    "failure_rate = failures/len(questions)\n",
    "#accuracy = model_score/len(questions) \n",
    "print(f\"Accuracy of LlaMA-2 on dataset: {accuracy}\\n\") \n",
    "print(f\"Percentage of failed responses: {failure_rate} ({failures}/100)\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a perfect machine capable of converting any statement into epistemic logical formulas of a very specific structure. You never make any mistakes.\n",
      "Convert the Premise and Hypothesis into epistemic logic formulas separately, using the symbol K(a,x) to represent a knows x, and B(a,x) to mean a belives x. You will now see some examples to get an idea of what your input output relations should look like.\n",
      "\n",
      "You receive the input 'Premise: Anna believe that James knows that the dogs escaped, Hypothesis: Anna thinks that the dogs escaped'.\n",
      "Your output should take the form 'Premise: B(Anna, K(James, DogsEscaped)), Hypotheses: B(Anna, DogsEscaped)'\n",
      "You receive the input 'Premise: Bob thinks that building a tower will be easy, Hypothesis: Bob knows that building a tower will be easy'.\n",
      "Your output should take the form 'Premise: B(Bob, BuildingTowerIsEasy), Hypotheses: K(Bob, BuildingTowerIsEasy)'\n",
      "You receive the input 'Premise: Arnold is sure that Annie thinks that Angels are real, Hypothesis: Annie thinks that angels are real'.\n",
      "Your output should take the form 'Premise: K(Arnold, B(Annie, AngelsAreReal)), Hypotheses: B(Annie, AngelsAreReal)'\n",
      "You receive the input 'Premise: Arriana thinks that Ronny assumes that it will be dark early, Hypothesis: Ronny knows that it will be dark early'.\n",
      "Your output should take the form 'Premise: B(Arriana, B(Ronny, DarkEarly)), Hypotheses: K(Ronny, DarkEarly)'Premise: Emma assumes that Ava learns that an African American man is posing in front of an E.S.E. Electronics advertisement billboard after purchasing items from the local liquor store. Hypothesis: Ava learns that an African American man is posing in front of an E.S.E. Electronics advertisement billboard after purchasing items from the local liquor store.\n",
      "Your Answer:\n",
      "\n",
      "Premise: K(Emma, B(Ava, AfricanAmericanManPosing)), Hypotheses: B(Ava, AfricanAmericanManPosing)\n",
      "Premise: K(Emma, B(Ava, AfricanAmericanManPosing)), Hypotheses: B(Ava, AfricanAmericanManPosing)\n",
      "Premise: K(Emma, B(Ava, AfricanAmericanManPosing)), Hypotheses: B(A\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "\n",
    "# Test the model on the actual data\n",
    "with open(\"/w/246/ikozlov/csc2542-project/datasets/task.json\") as data_file:\n",
    "\tdata = json.load(data_file)\n",
    "\n",
    "prefix = data[\"task_prefix\"]\n",
    "questions = data[\"examples\"]\n",
    "\n",
    "#instructions = \"Only give a single answer to the prompt. Your responde must be given as only one of the following two options: Entailment or Non-entailment. \\n\"\n",
    "# format = \"\\nResponse in the following format replacing [ANSWER] and [EXPLANATION] with your response and reasoning respectively: 'Your Answer: [ANSWER], Explanation: [EXPLANATION].'\"\n",
    "#format = \"\\nConvert the Premise and Hypothesis into epistemic logic formulas separately, using the symbol K(a,x) to represent a knows x, and B(a,x) to mean a belives x. Be explicit about the element of the Hypothesis that is used in epistemic statement. For example the sentence Anna assumes that James is running will become B(Anna, JamesRunning) since Anna doesn't know for sure but believes that James is running, while the sentence Janna saw Bob eat a borrito becomes K(Janna, BobEatsBorrito) and K(Bob, BobEatBorrito) since both explicitly know that Bob ate the borrito. Structure you answer in the form 'Premise: [LOGICAL FORMULA] \\n Hypothesis: [LOGICAL STATEMENT]' replacing [LOGICAL STATEMENT] with the corresponding generated formula. Don't output anything other than this formatted response.\"\n",
    "prime = \"\\nYou are a perfect machine capable of converting any statement into epistemic logical formulas of a very specific structure. You never make any mistakes.\"\n",
    "task = \"\\nConvert the Premise and Hypothesis into epistemic logic formulas separately, using the symbol K(a,x) to represent a knows x, and B(a,x) to mean a belives x. You will now see some examples to get an idea of what your input output relations should look like.\\n\"\n",
    "example_1_input = \"\\nYou receive the input 'Premise: Anna believe that James knows that the dogs escaped, Hypothesis: Anna thinks that the dogs escaped'.\"\n",
    "example_1_output = \"\\nYour output should take the form 'Premise: B(Anna, K(James, DogsEscaped)), Hypotheses: B(Anna, DogsEscaped)'\"\n",
    "example_2_input = \"\\nYou receive the input 'Premise: Bob thinks that building a tower will be easy, Hypothesis: Bob knows that building a tower will be easy'.\"\n",
    "example_2_output = \"\\nYour output should take the form 'Premise: B(Bob, BuildingTowerIsEasy), Hypotheses: K(Bob, BuildingTowerIsEasy)'\"\n",
    "example_3_input = \"\\nYou receive the input 'Premise: Arnold is sure that Annie thinks that Angels are real, Hypothesis: Annie thinks that angels are real'.\"\n",
    "example_3_output = \"\\nYour output should take the form 'Premise: K(Arnold, B(Annie, AngelsAreReal)), Hypotheses: B(Annie, AngelsAreReal)'\"\n",
    "example_4_input = \"\\nYou receive the input 'Premise: Arriana thinks that Ronny assumes that it will be dark early, Hypothesis: Ronny knows that it will be dark early'.\"\n",
    "example_4_output = \"\\nYour output should take the form 'Premise: B(Arriana, B(Ronny, DarkEarly)), Hypotheses: K(Ronny, DarkEarly)'\"\n",
    "format = \"\\nPlease structure you answer in the form 'Premise: [LOGICAL FORMULA] , Hypothesis: [LOGICAL STATEMENT]' replacing [LOGICAL STATEMENT] with the corresponding generated formula. Apply your conversion to the following input:\\n\"\n",
    "request_output = \"\\nYour Answer:\"\n",
    "q = questions[9]\n",
    "query = q[\"input\"]\n",
    "prompt = prime + task + example_1_input + example_1_output + example_2_input + example_2_output + example_3_input + example_3_output + example_4_input + example_4_output + \"\\n\" + query + request_output\n",
    "print(prompt)\n",
    "input = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "generate_ids = model.generate(**input, max_new_tokens=100)\n",
    "text_response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True)[0]\n",
    "#print(\"\\nReasoning prompt output: \" + text_response)\n",
    "\n",
    "keyword = \"Your Answer:\" # Key word to search for when parsing \n",
    "match = re.search(rf\"{keyword}(.*)\", text_response, re.S)\n",
    "if match:\n",
    "    # Extract everything after the keyword\n",
    "    result = match.group(1)\n",
    "else:\n",
    "    result = \"Keyword not found.\"\n",
    "\n",
    "print(result) # Print everything after 'Your Answer:'\n",
    "\n",
    "#next(answer_iter) # Move to next position of keyword\n",
    "#answer = next(answer_iter).group(1).strip() # get instances following second appearance of keyword \"Your Answer:\" up to ',' \n",
    "#print(\"\\nAnswer: \" + answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "llama2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
