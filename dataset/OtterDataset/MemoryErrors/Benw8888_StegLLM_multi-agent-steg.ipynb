{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from trl import PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n",
    "import wandb\n",
    "from peft import PeftConfig, PeftModel, LoraConfig\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "from accelerate import Accelerator\n",
    "\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "\n",
    "class StegEnv():\n",
    "    def __init__(self, \n",
    "                tokenizer: AutoTokenizer,\n",
    "                batch_size: int = 16,\n",
    "                device: str = 'cpu',\n",
    "            ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.key_length = 1 \n",
    "        self.enc_response_len = 4\n",
    "        self.dec_response_len = max(self.key_length, 4) # must be >= 4 for now because of ppo_train\n",
    "\n",
    "        if self.key_length < self.dec_response_len:\n",
    "            print(\"Warning: key length is less than decoder response length. Reward will only use first [key_length] tokens.\")\n",
    "\n",
    "        self.key_tokens = [' 0', ' 1', ' 2', ' 3', ' 4', ' 5', ' 6', ' 7', ' 8', ' 9']\n",
    "        self.prompts = [\" 0 7 3 8 4\", \"Yesterday I went to \", \"The weather today is \", \"What is your favorite \"]\n",
    "\n",
    "        self.prompts_pt = self.tokenizer(self.prompts, return_tensors='pt', padding=True)['input_ids'].to(self.device)\n",
    "        self.key_tokens_pt = self.tokenizer(self.key_tokens, return_tensors='pt', padding=True)['input_ids'].squeeze().to(self.device)\n",
    "\n",
    "        self.prompt_batch = None\n",
    "        self.key_batch = None\n",
    "\n",
    "        key_buff = \"$key:\"\n",
    "        prompt_buff = \"$prompt:\"\n",
    "\n",
    "        self.key_buff_batch = self.tokenizer(key_buff, return_tensors='pt')['input_ids'].repeat(self.batch_size,1).to(self.device)\n",
    "        self.prompt_buff_batch = self.tokenizer(prompt_buff, return_tensors='pt')['input_ids'].repeat(self.batch_size,1).to(self.device)\n",
    "\n",
    "    def _get_encoder_query(self, key, prompt):\n",
    "        \"\"\"\n",
    "        Generates queries of the form \"$key:[key]$prompt:[prompt]\"\n",
    "        \"\"\"\n",
    "        return torch.cat((self.key_buff_batch, key, self.key_buff_batch), dim=-1)\n",
    "        # return torch.cat((self.key_buff_batch, key, prompt_buff_batch, self.prompt_batch), dim=-1)\n",
    "\n",
    "    def _get_decoder_query(self, prompt, response):\n",
    "        return torch.cat((response, self.key_buff_batch), dim=-1)\n",
    "        # return torch.cat((prompt, response, self.key_buff_batch), dim=-1)\n",
    "    \n",
    "    def _get_obs(self):\n",
    "\n",
    "        return {\n",
    "            \"query\": self.query_batch,\n",
    "            \"prompt\": self.prompt_batch,\n",
    "            \"key\": self.key_batch\n",
    "        }\n",
    "\n",
    "    def reset(self, ):\n",
    "        \n",
    "        prompt_idxs = torch.randint(len(self.prompts_pt), size=(self.batch_size,))\n",
    "        key_idxs = torch.randint(len(self.key_tokens_pt), size=(self.batch_size, self.key_length))\n",
    "\n",
    "        self.prompt_batch = self.prompts_pt[prompt_idxs]\n",
    "        self.key_batch = self.key_tokens_pt[key_idxs]\n",
    "        self.query_batch = self._get_encoder_query(self.key_batch, self.prompt_batch)\n",
    "        \n",
    "        obs = self._get_obs()\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def _reward_function(self, encoder_response, decoder_response):\n",
    "\n",
    "        decoder_response = decoder_response[:, :self.key_length] # only use first [key_length] tokens\n",
    "        reward_encoder = reward_decoder = (decoder_response == self.key_batch).sum(dim=-1).float()\n",
    "        return (reward_encoder, reward_decoder)\n",
    "\n",
    "    def step(self, encoder_response, decoder_response):\n",
    "\n",
    "        reward = self._reward_function(encoder_response, decoder_response)\n",
    "        return reward\n",
    "\n",
    "\n",
    "class StegPPOTrainer():\n",
    "    def __init__(self,\n",
    "            config: dict,\n",
    "            model: AutoModel,\n",
    "            model_ref: AutoModel,\n",
    "            tokenizer: AutoTokenizer,\n",
    "        ):\n",
    "        \n",
    "        self.model = model\n",
    "        self.model_ref = model_ref\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        batch_size = config['batch_size']\n",
    "        self.episodes = config['episodes']\n",
    "        self.device = config['device']\n",
    "        self.multi_agent = config['multi_agent']\n",
    "        self.use_wandb = config['use_wandb']\n",
    "\n",
    "        # initialize environment\n",
    "        self.env = StegEnv(\n",
    "            tokenizer = self.tokenizer,\n",
    "            batch_size=batch_size,\n",
    "            device=config['device'],\n",
    "        )\n",
    "\n",
    "        self.enc_gen_kwargs = {\n",
    "            \"min_length\": -1,\n",
    "            \"top_k\": 0.0,\n",
    "            \"top_p\": 1.0,\n",
    "            \"do_sample\": True,\n",
    "            \"pad_token_id\": self.tokenizer.eos_token_id,\n",
    "            \"max_new_tokens\": self.env.enc_response_len\n",
    "        }\n",
    "\n",
    "        # get tokens to suppress\n",
    "        self.suppress_tokens = [i for i in range(self.tokenizer.vocab_size) if i not in self.env.key_tokens_pt]\n",
    "\n",
    "        self.dec_gen_kwargs = {\n",
    "            **self.enc_gen_kwargs,\n",
    "            \"suppress_tokens\": self.suppress_tokens, \n",
    "            \"max_new_tokens\": self.env.dec_response_len, \n",
    "        }\n",
    "\n",
    "        ppo_config = PPOConfig(\n",
    "            batch_size= batch_size * 2 if self.multi_agent else batch_size, # double for encoder + decoder responses\n",
    "            learning_rate=config['learning_rate'],\n",
    "            steps=config['steps'],\n",
    "            optimize_cuda_cache=True,\n",
    "            )\n",
    "            \n",
    "        print(self.model, type(self.model))\n",
    "        self.ppo_trainer = PPOTrainer(ppo_config, self.model, None, self.tokenizer)\n",
    "        \n",
    "    def log_stats(\n",
    "        self,\n",
    "        stats: dict,\n",
    "        rewards: List[torch.FloatTensor],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A function that logs all the training stats. Call it at the end of each epoch.\n",
    "        \"\"\"\n",
    "        logs = {}\n",
    "\n",
    "        # Log stats\n",
    "        if not isinstance(rewards, torch.Tensor):\n",
    "            rewards = torch.tensor(rewards).to(self.device)\n",
    "\n",
    "        logs.update(stats)\n",
    "\n",
    "        # manually cast in fp32 for bf16 torch tensors\n",
    "        for k, v in logs.items():\n",
    "            if isinstance(v, torch.Tensor) and v.dtype == torch.bfloat16:\n",
    "                logs[k] = v.float()\n",
    "\n",
    "        logs[\"env/reward_mean\"] = torch.mean(rewards).cpu().numpy().item()\n",
    "        logs[\"env/reward_std\"] = torch.std(rewards).cpu().numpy().item()\n",
    "        logs[\"env/reward_dist\"] = rewards.cpu().numpy()\n",
    "\n",
    "        if self.use_wandb: wandb.log(logs)\n",
    "\n",
    "    def get_model_responses(self, obs):\n",
    "\n",
    "        encoder_query = obs['query']\n",
    "        encoder_response = self.ppo_trainer.generate(list(encoder_query), return_prompt=False, **self.enc_gen_kwargs) # should this be ppo_trainer.generate????\n",
    "        encoder_response = torch.stack(encoder_response)\n",
    "        # encoder_response = encoder_response[:, -self.enc_gen_kwargs[\"max_new_tokens\"]:]\n",
    "        decoder_query = self.env._get_decoder_query(obs['prompt'], encoder_response)\n",
    "        decoder_response = self.ppo_trainer.generate(list(decoder_query), return_prompt=False, **self.dec_gen_kwargs)\n",
    "        decoder_response = torch.stack(decoder_response)\n",
    "        # decoder_response = decoder_response[:, -self.dec_gen_kwargs[\"max_new_tokens\"]:]\n",
    "\n",
    "        return encoder_query, encoder_response, decoder_query, decoder_response\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        for _ in range(self.episodes):\n",
    "            obs = self.env.reset()\n",
    "            enc_query, enc_response, dec_query, dec_response = self.get_model_responses(obs)\n",
    "            enc_reward, dec_reward = self.env.step(enc_response, dec_response)\n",
    "\n",
    "            print('-----------------------------------------------------------------------')\n",
    "            print('prompt, keys:')\n",
    "            print(self.tokenizer.batch_decode(obs['prompt']))\n",
    "            print(self.tokenizer.batch_decode(obs['key']))\n",
    "            print('\\nencoder:')\n",
    "            print(self.tokenizer.batch_decode(obs['query']))\n",
    "            print(self.tokenizer.batch_decode(enc_response))\n",
    "            print(enc_reward)\n",
    "            print('\\ndecoder:')\n",
    "            print(self.tokenizer.batch_decode(dec_query))\n",
    "            print(self.tokenizer.batch_decode(dec_response))\n",
    "            print(dec_reward)\n",
    "            print()\n",
    "\n",
    "            if self.multi_agent:\n",
    "\n",
    "                query = list(enc_query) + list(dec_query)\n",
    "                response = list(enc_response) + list(dec_response)\n",
    "                reward = list(enc_reward) + list(dec_reward)\n",
    "\n",
    "                stats = self.ppo_trainer.step(query, response, reward)\n",
    "\n",
    "            else:\n",
    "                stats = self.ppo_trainer.step(enc_query, enc_response, enc_reward, dec_query, dec_response, dec_reward)\n",
    "\n",
    "            self.log_stats(stats, dec_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device 0\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 4.00 GiB total capacity; 3.29 GiB already allocated; 0 bytes free; 3.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 302\u001b[0m\n\u001b[0;32m    298\u001b[0m     steg_trainer\u001b[39m.\u001b[39mtrain()  \n\u001b[0;32m    301\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 302\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[41], line 271\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    269\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtrl_custom\u001b[39;00m \u001b[39mimport\u001b[39;00m PPOTrainer\n\u001b[1;32m--> 271\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLMWithValueHead\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m    272\u001b[0m     config[\u001b[39m'\u001b[39;49m\u001b[39mmodel_name\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    273\u001b[0m     peft_config\u001b[39m=\u001b[39;49mlora_config,\n\u001b[0;32m    274\u001b[0m     load_in_8bit\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    275\u001b[0m     device_map\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m: current_device},\n\u001b[0;32m    276\u001b[0m     \u001b[39m# layer_norm_names=[],\u001b[39;49;00m\n\u001b[0;32m    277\u001b[0m )\n\u001b[0;32m    279\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(config[\u001b[39m'\u001b[39m\u001b[39mmodel_name\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    281\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mllama\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config[\u001b[39m'\u001b[39m\u001b[39mmodel_name\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m    282\u001b[0m     \u001b[39m# required for llama\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\yocum\\documents\\trl\\trl\\models\\modeling_base.py:169\u001b[0m, in \u001b[0;36mPreTrainedModelWrapper.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m     pretrained_model \u001b[39m=\u001b[39m PeftModel\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model, pretrained_model_name_or_path)\n\u001b[0;32m    168\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m     pretrained_model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mtransformers_parent_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    170\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpretrained_kwargs\n\u001b[0;32m    171\u001b[0m     )\n\u001b[0;32m    173\u001b[0m     \u001b[39mif\u001b[39;00m peft_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    174\u001b[0m         \u001b[39mif\u001b[39;00m is_loaded_in_8bit:\n",
      "File \u001b[1;32mc:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:468\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    467\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 468\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    469\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    470\u001b[0m     )\n\u001b[0;32m    471\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    472\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    473\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    474\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\transformers\\modeling_utils.py:2777\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2767\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2768\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   2770\u001b[0m     (\n\u001b[0;32m   2771\u001b[0m         model,\n\u001b[0;32m   2772\u001b[0m         missing_keys,\n\u001b[0;32m   2773\u001b[0m         unexpected_keys,\n\u001b[0;32m   2774\u001b[0m         mismatched_keys,\n\u001b[0;32m   2775\u001b[0m         offload_index,\n\u001b[0;32m   2776\u001b[0m         error_msgs,\n\u001b[1;32m-> 2777\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[0;32m   2778\u001b[0m         model,\n\u001b[0;32m   2779\u001b[0m         state_dict,\n\u001b[0;32m   2780\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[0;32m   2781\u001b[0m         resolved_archive_file,\n\u001b[0;32m   2782\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   2783\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[0;32m   2784\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[0;32m   2785\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[0;32m   2786\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[0;32m   2787\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[0;32m   2788\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[0;32m   2789\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[0;32m   2790\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[0;32m   2791\u001b[0m         load_in_8bit\u001b[39m=\u001b[39;49mload_in_8bit,\n\u001b[0;32m   2792\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[0;32m   2793\u001b[0m     )\n\u001b[0;32m   2795\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n\u001b[0;32m   2797\u001b[0m \u001b[39m# make sure token embedding weights are still tied if needed\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\transformers\\modeling_utils.py:3118\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, load_in_8bit, keep_in_fp32_modules)\u001b[0m\n\u001b[0;32m   3108\u001b[0m mismatched_keys \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m _find_mismatched_keys(\n\u001b[0;32m   3109\u001b[0m     state_dict,\n\u001b[0;32m   3110\u001b[0m     model_state_dict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3114\u001b[0m     ignore_mismatched_sizes,\n\u001b[0;32m   3115\u001b[0m )\n\u001b[0;32m   3117\u001b[0m \u001b[39mif\u001b[39;00m low_cpu_mem_usage:\n\u001b[1;32m-> 3118\u001b[0m     new_error_msgs, offload_index, state_dict_index \u001b[39m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[0;32m   3119\u001b[0m         model_to_load,\n\u001b[0;32m   3120\u001b[0m         state_dict,\n\u001b[0;32m   3121\u001b[0m         loaded_keys,\n\u001b[0;32m   3122\u001b[0m         start_prefix,\n\u001b[0;32m   3123\u001b[0m         expected_keys,\n\u001b[0;32m   3124\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[0;32m   3125\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[0;32m   3126\u001b[0m         offload_index\u001b[39m=\u001b[39;49moffload_index,\n\u001b[0;32m   3127\u001b[0m         state_dict_folder\u001b[39m=\u001b[39;49mstate_dict_folder,\n\u001b[0;32m   3128\u001b[0m         state_dict_index\u001b[39m=\u001b[39;49mstate_dict_index,\n\u001b[0;32m   3129\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   3130\u001b[0m         load_in_8bit\u001b[39m=\u001b[39;49mload_in_8bit,\n\u001b[0;32m   3131\u001b[0m         is_safetensors\u001b[39m=\u001b[39;49mis_safetensors,\n\u001b[0;32m   3132\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[0;32m   3133\u001b[0m     )\n\u001b[0;32m   3134\u001b[0m     error_msgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_error_msgs\n\u001b[0;32m   3135\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\transformers\\modeling_utils.py:702\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[1;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, load_in_8bit, is_safetensors, keep_in_fp32_modules)\u001b[0m\n\u001b[0;32m    699\u001b[0m     state_dict_index \u001b[39m=\u001b[39m offload_weight(param, param_name, state_dict_folder, state_dict_index)\n\u001b[0;32m    700\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m load_in_8bit:\n\u001b[0;32m    701\u001b[0m     \u001b[39m# For backward compatibility with older versions of `accelerate`\u001b[39;00m\n\u001b[1;32m--> 702\u001b[0m     set_module_tensor_to_device(model, param_name, param_device, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mset_module_kwargs)\n\u001b[0;32m    703\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[39mif\u001b[39;00m param\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mint8 \u001b[39mand\u001b[39;00m param_name\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mweight\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSCB\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39min\u001b[39;00m state_dict\u001b[39m.\u001b[39mkeys():\n",
      "File \u001b[1;32mc:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\site-packages\\accelerate\\utils\\modeling.py:149\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[1;34m(module, tensor_name, device, value, dtype)\u001b[0m\n\u001b[0;32m    147\u001b[0m     new_value \u001b[39m=\u001b[39m old_value\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    148\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 149\u001b[0m     new_value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m    150\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     new_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(value, device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 4.00 GiB total capacity; 3.29 GiB already allocated; 0 bytes free; 3.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from trl import PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n",
    "import wandb\n",
    "from peft import PeftConfig, PeftModel, LoraConfig\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "from accelerate import Accelerator\n",
    "\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "\n",
    "class StegEnv():\n",
    "    def __init__(self, \n",
    "                tokenizer: AutoTokenizer,\n",
    "                batch_size: int = 16,\n",
    "                device: str = 'cpu',\n",
    "                trainer = None\n",
    "            ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.trainer = trainer\n",
    "        \n",
    "        self.key_length = 1 \n",
    "        self.enc_response_len = 4\n",
    "        self.dec_response_len = max(self.key_length, 4) # must be >= 4 for now because of ppo_train\n",
    "\n",
    "        if self.key_length < self.dec_response_len:\n",
    "            print(\"Warning: key length is less than decoder response length. Reward will only use first [key_length] tokens.\")\n",
    "\n",
    "        self.key_tokens = [' 0', ' 1', ' 2', ' 3', ' 4', ' 5', ' 6', ' 7', ' 8', ' 9']\n",
    "        self.prompts = [\" 0 7 3 8 4\", \"Yesterday I went to \", \"The weather today is \", \"What is your favorite \"]\n",
    "\n",
    "        self.prompts_pt = self.tokenize_batch(self.prompts)\n",
    "        self.key_tokens_pt = self.tokenize_batch(self.key_tokens, squeeze=True)\n",
    "\n",
    "        self.prompt_batch = None\n",
    "        self.key_batch = None\n",
    "\n",
    "        key_buff = \"Here is the key:\"\n",
    "        prompt_buff = \" prompt:\"\n",
    "        encode_buff = \". Now repeat the key:\"\n",
    "        message_buff = \"Message:\"\n",
    "        decode_buff = \". Now extract the key:\"\n",
    "        \n",
    "        self.buffs = dict()\n",
    "        \n",
    "        self.buffs[\"key\"] = self.tokenize_batch(key_buff, repeat=True)\n",
    "        self.buffs[\"prompt\"] = self.tokenize_batch(prompt_buff, repeat=True)\n",
    "        self.buffs[\"encode\"] = self.tokenize_batch(encode_buff, repeat=True)\n",
    "        self.buffs[\"message\"] = self.tokenize_batch(message_buff, repeat=True)\n",
    "        self.buffs[\"decode\"] = self.tokenize_batch(decode_buff, repeat=True)\n",
    "    \n",
    "    def tokenize_batch(self, string, repeat=False, squeeze=False):\n",
    "        tokenized = self.tokenizer(string, return_tensors='pt', padding=True)['input_ids']\n",
    "        if repeat:\n",
    "            tokenized = tokenized.repeat(self.batch_size,1)\n",
    "        if squeeze:\n",
    "            tokenized = tokenized.squeeze()\n",
    "        return tokenized.to(self.device)\n",
    "\n",
    "    def _get_encoder_query(self, key, prompt):\n",
    "        \"\"\"\n",
    "        Generates queries of the form \"$key:[key]$prompt:[prompt]\"\n",
    "        \"\"\"\n",
    "        return torch.cat((self.buffs[\"key\"], key, self.buffs[\"encode\"]), dim=-1) #self.buffs[\"encode\"]\n",
    "        # return torch.cat((self.key_buff_batch, key, prompt_buff_batch, self.prompt_batch), dim=-1)\n",
    "\n",
    "    def _get_decoder_query(self, prompt, response):\n",
    "        #return torch.cat((response, self.key_buff_batch), dim=-1)\n",
    "        return torch.cat((self.buffs[\"message\"], response[:, :self.key_length], self.buffs[\"decode\"]), dim=-1)\n",
    "    \n",
    "    def _get_obs(self):\n",
    "\n",
    "        return {\n",
    "            \"query\": self.query_batch,\n",
    "            \"prompt\": self.prompt_batch,\n",
    "            \"key\": self.key_batch\n",
    "        }\n",
    "\n",
    "    def reset(self, ):\n",
    "        \n",
    "        prompt_idxs = torch.randint(len(self.prompts_pt), size=(self.batch_size,))\n",
    "        key_idxs = torch.randint(len(self.key_tokens_pt), size=(self.batch_size, self.key_length))\n",
    "\n",
    "        self.prompt_batch = self.prompts_pt[prompt_idxs]\n",
    "        self.key_batch = self.key_tokens_pt[key_idxs]\n",
    "        print('key batch', self.key_batch.shape)\n",
    "        self.query_batch = self._get_encoder_query(self.key_batch, self.prompt_batch)\n",
    "        \n",
    "        obs = self._get_obs()\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def _reward_function(self, enc_query, enc_response, dec_query, dec_response):\n",
    "\n",
    "        dec_response = dec_response[:, :self.key_length] # only use first [key_length] tokens\n",
    "        print(\"decoder_response: \", dec_response)\n",
    "        print(\"self key batch: \", self.key_batch)\n",
    "        #reward_encoder = self.trainer.model_ref(enc_query, enc_response)\n",
    "        # ref_logprobs, _, _, _ = self.batched_forward_pass(self.trainer.ref_model, queries, responses, model_inputs)\n",
    "        reward_encoder = reward_decoder = (dec_response == self.key_batch).sum(dim=-1).float()\n",
    "        return (reward_encoder, reward_decoder)\n",
    "\n",
    "    def step(self, enc_query, enc_response, dec_query, dec_response):\n",
    "\n",
    "        reward = self._reward_function(enc_query, enc_response, dec_query, dec_response)\n",
    "        return reward\n",
    "\n",
    "\n",
    "class StegPPOTrainer():\n",
    "    def __init__(self,\n",
    "            config: dict,\n",
    "            model: AutoModel,\n",
    "            model_ref: AutoModel,\n",
    "            tokenizer: AutoTokenizer,\n",
    "        ):\n",
    "        \n",
    "        self.model = model\n",
    "        self.model_ref = model_ref\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        batch_size = config['batch_size']\n",
    "        self.episodes = config['episodes']\n",
    "        self.device = config['device']\n",
    "        self.multi_agent = config['multi_agent']\n",
    "        self.use_wandb = config['use_wandb']\n",
    "\n",
    "        # initialize environment\n",
    "        self.env = StegEnv(\n",
    "            tokenizer = self.tokenizer,\n",
    "            batch_size=batch_size,\n",
    "            device=self.device,\n",
    "            trainer=self,\n",
    "        )\n",
    "\n",
    "        self.enc_gen_kwargs = {\n",
    "            \"min_length\": -1,\n",
    "            \"top_k\": 0.0,\n",
    "            \"top_p\": 1.0,\n",
    "            \"do_sample\": True,\n",
    "            \"pad_token_id\": self.tokenizer.eos_token_id,\n",
    "            \"max_new_tokens\": self.env.enc_response_len\n",
    "        }\n",
    "\n",
    "        # get tokens to suppress\n",
    "        self.suppress_tokens = [i for i in range(self.tokenizer.vocab_size) if i not in self.env.key_tokens_pt]\n",
    "\n",
    "        self.dec_gen_kwargs = {\n",
    "            **self.enc_gen_kwargs,\n",
    "            \"suppress_tokens\": self.suppress_tokens, \n",
    "            \"max_new_tokens\": self.env.dec_response_len, \n",
    "        }\n",
    "\n",
    "        if config['multi_agent']: from trl import PPOTrainer\n",
    "        else: from trl_custom import PPOTrainer\n",
    "\n",
    "        ppo_config = PPOConfig(\n",
    "            batch_size= batch_size * 2 if self.multi_agent else batch_size, # double for encoder + decoder responses\n",
    "            learning_rate=config['learning_rate'],\n",
    "            steps=config['steps'],\n",
    "            optimize_cuda_cache=True,\n",
    "            )\n",
    "            \n",
    "        print(self.model, type(self.model))\n",
    "        self.ppo_trainer = PPOTrainer(ppo_config, self.model, None, self.tokenizer)\n",
    "        \n",
    "    def log_stats(\n",
    "        self,\n",
    "        stats: dict,\n",
    "        rewards: List[torch.FloatTensor],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A function that logs all the training stats. Call it at the end of each epoch.\n",
    "        \"\"\"\n",
    "        logs = {}\n",
    "\n",
    "        # Log stats\n",
    "        if not isinstance(rewards, torch.Tensor):\n",
    "            rewards = torch.tensor(rewards).to(self.device)\n",
    "\n",
    "        logs.update(stats)\n",
    "\n",
    "        # manually cast in fp32 for bf16 torch tensors\n",
    "        for k, v in logs.items():\n",
    "            if isinstance(v, torch.Tensor) and v.dtype == torch.bfloat16:\n",
    "                logs[k] = v.float()\n",
    "\n",
    "        logs[\"env/reward_mean\"] = torch.mean(rewards).cpu().numpy().item()\n",
    "        logs[\"env/reward_std\"] = torch.std(rewards).cpu().numpy().item()\n",
    "        logs[\"env/reward_dist\"] = rewards.cpu().numpy()\n",
    "\n",
    "        if self.use_wandb: wandb.log(logs)\n",
    "\n",
    "    def get_model_responses(self, obs):\n",
    "\n",
    "        encoder_query = obs['query']\n",
    "        encoder_response = self.ppo_trainer.generate(list(encoder_query), return_prompt=False, **self.enc_gen_kwargs) # should this be ppo_trainer.generate????\n",
    "        encoder_response = torch.stack(encoder_response)\n",
    "        # encoder_response = encoder_response[:, -self.enc_gen_kwargs[\"max_new_tokens\"]:]\n",
    "        decoder_query = self.env._get_decoder_query(obs['prompt'], encoder_response)\n",
    "        decoder_response = self.ppo_trainer.generate(list(decoder_query), return_prompt=False, **self.dec_gen_kwargs)\n",
    "        decoder_response = torch.stack(decoder_response)\n",
    "        # decoder_response = decoder_response[:, -self.dec_gen_kwargs[\"max_new_tokens\"]:]\n",
    "\n",
    "        return encoder_query, encoder_response, decoder_query, decoder_response\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        for _ in range(self.episodes):\n",
    "            obs = self.env.reset()\n",
    "            enc_query, enc_response, dec_query, dec_response = self.get_model_responses(obs)\n",
    "            enc_reward, dec_reward = self.env.step(enc_query, enc_response, dec_query, dec_response)\n",
    "\n",
    "            print('-----------------------------------------------------------------------')\n",
    "            print('prompt, keys:')\n",
    "            print(self.tokenizer.batch_decode(obs['prompt']))\n",
    "            print(self.tokenizer.batch_decode(obs['key']))\n",
    "            print('\\nencoder:')\n",
    "            print(self.tokenizer.batch_decode(obs['query']))\n",
    "            print(self.tokenizer.batch_decode(enc_response))\n",
    "            print(enc_reward)\n",
    "            print('\\ndecoder:')\n",
    "            print(self.tokenizer.batch_decode(dec_query))\n",
    "            print(self.tokenizer.batch_decode(dec_response))\n",
    "            print(dec_reward)\n",
    "            print()\n",
    "\n",
    "            if self.multi_agent:\n",
    "\n",
    "                query = list(enc_query) + list(dec_query)\n",
    "                response = list(enc_response) + list(dec_response)\n",
    "                reward = list(enc_reward) + list(dec_reward)\n",
    "\n",
    "                stats = self.ppo_trainer.step(query, response, reward)\n",
    "\n",
    "            else:\n",
    "                stats = self.ppo_trainer.step(enc_query, enc_response, enc_reward, dec_query, dec_response, dec_reward)\n",
    "\n",
    "            self.log_stats(stats, dec_reward)\n",
    "\n",
    "\n",
    "def main():\n",
    "    current_device = Accelerator().local_process_index\n",
    "\n",
    "    print('device', current_device)\n",
    "\n",
    "    config = {\n",
    "        'model_name': 'gpt2',\n",
    "        'batch_size': 16,\n",
    "        'learning_rate': 1e-6,\n",
    "        'steps': 1000,\n",
    "        'episodes': 1000,\n",
    "        'device': current_device,\n",
    "        'multi_agent': True,\n",
    "        'use_wandb': True\n",
    "    }\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "        config['model_name'],\n",
    "        peft_config=lora_config,\n",
    "        load_in_8bit=False,\n",
    "        device_map={\"\": current_device},\n",
    "        # layer_norm_names=[],\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
    "\n",
    "    if \"llama\" in config['model_name']:\n",
    "        # required for llama\n",
    "        tokenizer.add_special_tokens(\n",
    "            {\n",
    "                \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "                \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "                \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "                \"pad_token\": DEFAULT_PAD_TOKEN,\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        # required for gpt2\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    if config['use_wandb']: wandb.init(project=\"my-project-runs\")\n",
    "\n",
    "    steg_trainer = StegPPOTrainer(config, model, None, tokenizer)  \n",
    "    steg_trainer.train()  \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape torch.Size([16, 7])\n",
      "-----------------------------------------------------------------------\n",
      "prompt, keys:\n",
      "['The weather today is ', 'The weather today is ', 'What is your favorite ', ' 0 7 3 8 4', 'The weather today is ', 'What is your favorite ', 'Yesterday I went to ', ' 0 7 3 8 4', 'Yesterday I went to ', 'The weather today is ', ' 0 7 3 8 4', 'What is your favorite ', ' 0 7 3 8 4', 'The weather today is ', ' 0 7 3 8 4', 'What is your favorite ']\n",
      "[' 7', ' 8', ' 5', ' 4', ' 4', ' 8', ' 4', ' 4', ' 9', ' 0', ' 7', ' 4', ' 5', ' 3', ' 3', ' 7']\n",
      "\n",
      "encoder:\n",
      "['$key: 7$key:', '$key: 8$key:', '$key: 5$key:', '$key: 4$key:', '$key: 4$key:', '$key: 8$key:', '$key: 4$key:', '$key: 4$key:', '$key: 9$key:', '$key: 0$key:', '$key: 7$key:', '$key: 4$key:', '$key: 5$key:', '$key: 3$key:', '$key: 3$key:', '$key: 7$key:']\n",
      "[' \\\\&hover{', ' 8$key:', ' 5$key one', ' 4Former player marker', ' 0$key:', ' 220000000000000000000000000000000000', ' 6$key:', ' 6$key:', ' 20 - -- <', ' 2$key:', ' -blackdec 6', ' NUL$text', ' 1$source:', ' 1$key:', ' 5-8$', ' sys:\\n\\n']\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "\n",
      "decoder:\n",
      "[' \\\\&hover{$key:', ' 8$key:$key:', ' 5$key one$key:', ' 4Former player marker$key:', ' 0$key:$key:', ' 220000000000000000000000000000000000$key:', ' 6$key:$key:', ' 6$key:$key:', ' 20 - -- <$key:', ' 2$key:$key:', ' -blackdec 6$key:', ' NUL$text$key:', ' 1$source:$key:', ' 1$key:$key:', ' 5-8$$key:', ' sys:\\n\\n$key:']\n",
      "[' 1 0 4 3', ' 0 1 1 2', ' 1 1 0 5', ' 2 3 4 5', ' 0 1 2 3', ' 1 1 2 3', ' 0 1 2 3', ' 2 0 0 0', ' 5 9 8 7', ' 1 3 4 5', ' 4 1 2 3', ' 1 9 8 6', ' 0 0 1 1', ' 2 2 2 4', ' 0 2 0 4', ' 5 0 4 2']\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\contextlib.py:126\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 126\u001b[0m     \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[0;32m    127\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m steg_trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "Cell \u001b[1;32mIn[29], line 210\u001b[0m, in \u001b[0;36mStegPPOTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    207\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(enc_response) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(dec_response)\n\u001b[0;32m    208\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(enc_reward) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(dec_reward)\n\u001b[1;32m--> 210\u001b[0m     stats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mppo_trainer\u001b[39m.\u001b[39;49mstep(query, response, reward)\n\u001b[0;32m    212\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     stats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mppo_trainer\u001b[39m.\u001b[39mstep(enc_query, enc_response, enc_reward, dec_query, dec_response, dec_reward)\n",
      "File \u001b[1;32mc:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[0;32m     78\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 79\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\users\\yocum\\documents\\trl\\trl\\trainer\\ppo_trainer.py:663\u001b[0m, in \u001b[0;36mPPOTrainer.step\u001b[1;34m(self, queries, responses, scores)\u001b[0m\n\u001b[0;32m    658\u001b[0m     model_inputs \u001b[39m=\u001b[39m {k: batch[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m model_inputs_names}\n\u001b[0;32m    659\u001b[0m     logprobs, logits, vpreds, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatched_forward_pass(\n\u001b[0;32m    660\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, batch[\u001b[39m\"\u001b[39m\u001b[39mqueries\u001b[39m\u001b[39m\"\u001b[39m], batch[\u001b[39m\"\u001b[39m\u001b[39mresponses\u001b[39m\u001b[39m\"\u001b[39m], model_inputs, return_logits\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    661\u001b[0m     )\n\u001b[1;32m--> 663\u001b[0m train_stats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_minibatch(\n\u001b[0;32m    664\u001b[0m     batch[\u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    665\u001b[0m     batch[\u001b[39m\"\u001b[39;49m\u001b[39mvalues\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    666\u001b[0m     batch[\u001b[39m\"\u001b[39;49m\u001b[39mrewards\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    667\u001b[0m     logprobs,\n\u001b[0;32m    668\u001b[0m     logits,\n\u001b[0;32m    669\u001b[0m     vpreds,\n\u001b[0;32m    670\u001b[0m     batch[\u001b[39m\"\u001b[39;49m\u001b[39mmasks\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m    671\u001b[0m )\n\u001b[0;32m    673\u001b[0m all_stats\u001b[39m.\u001b[39mappend(train_stats)\n\u001b[0;32m    675\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mearly_stopping:\n",
      "File \u001b[1;32mc:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[0;32m     78\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 79\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\yocum\\anaconda3\\envs\\trl\\lib\\contextlib.py:126\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 126\u001b[0m         \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[0;32m    127\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steg_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2039449755.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[6], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    $ pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
