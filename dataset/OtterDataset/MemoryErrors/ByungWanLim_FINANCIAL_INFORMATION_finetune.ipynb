{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "from faiss_module import make_db\n",
    "import pandas as pd\n",
    "def format_docs(docs):\n",
    "    \"\"\"ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…\"\"\"\n",
    "    context = \"\"\n",
    "    # context = \"<|start_header_id|>system<|end_header_id|>\\nContext\\n\"\n",
    "    for i, doc in enumerate(docs):\n",
    "        #context += f\"Document {i+1}\\n\"\n",
    "        context += doc.page_content\n",
    "        context += '\\n\\n'\n",
    "    # context += \"<|eot_id|>\"\n",
    "    return context \n",
    "\n",
    "def format_docs_wnum(docs):\n",
    "    \"\"\"ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…\"\"\"\n",
    "    context = \"\"\n",
    "    # context = \"<|start_header_id|>system<|end_header_id|>\\nContext\\n\"\n",
    "    for i, doc in enumerate(docs):\n",
    "        context += f\"Document {i+1}\\n\"\n",
    "        context += doc.page_content\n",
    "        context += '\\n\\n'\n",
    "    # context += \"<|eot_id|>\"\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FAISS DB from: ./train_faiss_db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:796: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.76\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:796: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.76\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:796: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.76\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:796: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.76\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:796: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.76\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:796: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.76\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:796: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.76\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:796: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.76\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:796: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.76\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:796: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.76\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:796: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.76\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:796: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.76\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:796: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.76\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "train_db = make_db(train_df,'./train_faiss_db')\n",
    "answer_list = []\n",
    "context_list = []\n",
    "context_list_wnum = []\n",
    "for i, entry in enumerate(train_df.to_dict(orient='records')):\n",
    "    question = entry['Question']\n",
    "    answer = entry['Answer']+\"<|eot_id|>\"\n",
    "    # print(question)\n",
    "    # print(answer)\n",
    "    train_retriever = train_db.as_retriever(search_type=\"similarity_score_threshold\",\n",
    "                search_kwargs={'filter': {'source':entry['Source_path']},'score_threshold': 0.76,'k':3})\n",
    "    docs = train_retriever.invoke(question)\n",
    "    if len(docs) == 0:\n",
    "        context_list.append(\"None\")\n",
    "        context_list_wnum.append(\"None\")\n",
    "    else:\n",
    "        #print(format_docs(docs))\n",
    "        context_list.append(format_docs(docs))\n",
    "        context_list_wnum.append(format_docs_wnum(docs))\n",
    "    answer_list.append(answer)\n",
    "    \n",
    "train_df['Answer'] = answer_list\n",
    "train_df['context'] = context_list\n",
    "train_df['context_wnum'] = context_list_wnum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "def create_prompt(row):\n",
    "    context = row['context']\n",
    "    question = row['Question']\n",
    "    prompt  =f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> \n",
    "You are a Korean Q&A Assistant.<|eot_id|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "{context}\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{question}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "train_df['prompt'] = train_df.apply(create_prompt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAMPLE_ID</th>\n",
       "      <th>Source</th>\n",
       "      <th>Source_path</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>context</th>\n",
       "      <th>context_wnum</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_000</td>\n",
       "      <td>1-1 2024 ì£¼ìš” ì¬ì •í†µê³„ 1ê¶Œ</td>\n",
       "      <td>./train_source/1-1 2024 ì£¼ìš” ì¬ì •í†µê³„ 1ê¶Œ.pdf</td>\n",
       "      <td>2024ë…„ ì¤‘ì•™ì •ë¶€ ì¬ì •ì²´ê³„ëŠ” ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆë‚˜ìš”?</td>\n",
       "      <td>2024ë…„ ì¤‘ì•™ì •ë¶€ ì¬ì •ì²´ê³„ëŠ” ì˜ˆì‚°(ì¼ë°˜Â·íŠ¹ë³„íšŒê³„)ê³¼ ê¸°ê¸ˆìœ¼ë¡œ êµ¬ë¶„ë˜ë©°, 2024ë…„...</td>\n",
       "      <td>ì£¼ìš” ì¬ì •í†µê³„â—\\nâ… .\\n201ì¬ì •ì²´ê³„\\nâ–¸ì¤‘ì•™ì •ë¶€ ì¬ì •ì²´ê³„ëŠ” ì˜ˆì‚°(ì¼ë°˜ ï½¥íŠ¹ë³„íšŒê³„...</td>\n",
       "      <td>Document 1\\nì£¼ìš” ì¬ì •í†µê³„â—\\nâ… .\\n201ì¬ì •ì²´ê³„\\nâ–¸ì¤‘ì•™ì •ë¶€ ì¬ì •ì²´ê³„ëŠ”...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_001</td>\n",
       "      <td>1-1 2024 ì£¼ìš” ì¬ì •í†µê³„ 1ê¶Œ</td>\n",
       "      <td>./train_source/1-1 2024 ì£¼ìš” ì¬ì •í†µê³„ 1ê¶Œ.pdf</td>\n",
       "      <td>2024ë…„ ì¤‘ì•™ì •ë¶€ì˜ ì˜ˆì‚° ì§€ì¶œì€ ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆë‚˜ìš”?</td>\n",
       "      <td>2024ë…„ ì¤‘ì•™ì •ë¶€ì˜ ì˜ˆì‚° ì§€ì¶œì€ ì¼ë°˜íšŒê³„ 356.5ì¡°ì›, 21ê°œ íŠ¹ë³„íšŒê³„ 81.7...</td>\n",
       "      <td>ì£¼ìš” ì¬ì •í†µê³„â—\\nâ… .\\n201ì¬ì •ì²´ê³„\\nâ–¸ì¤‘ì•™ì •ë¶€ ì¬ì •ì²´ê³„ëŠ” ì˜ˆì‚°(ì¼ë°˜ ï½¥íŠ¹ë³„íšŒê³„...</td>\n",
       "      <td>Document 1\\nì£¼ìš” ì¬ì •í†µê³„â—\\nâ… .\\n201ì¬ì •ì²´ê³„\\nâ–¸ì¤‘ì•™ì •ë¶€ ì¬ì •ì²´ê³„ëŠ”...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_002</td>\n",
       "      <td>1-1 2024 ì£¼ìš” ì¬ì •í†µê³„ 1ê¶Œ</td>\n",
       "      <td>./train_source/1-1 2024 ì£¼ìš” ì¬ì •í†µê³„ 1ê¶Œ.pdf</td>\n",
       "      <td>ê¸°ê¸ˆì´ ì˜ˆì‚°ê³¼ ë‹¤ë¥¸ ì ì€?</td>\n",
       "      <td>ê¸°ê¸ˆì€ ì˜ˆì‚°ê³¼ êµ¬ë¶„ë˜ëŠ” ì¬ì •ìˆ˜ë‹¨ìœ¼ë¡œì„œ ì¬ì •ìš´ì˜ì˜ ì‹ ì¶•ì„±ì„ ê¸°í•  í•„ìš”ê°€ ìˆì„ ë•Œ, ì •...</td>\n",
       "      <td>(1.4) (1.2) (1.5) (3.1) (3.4) (3.1) (3.6) (2.3...</td>\n",
       "      <td>Document 1\\n(1.4) (1.2) (1.5) (3.1) (3.4) (3.1...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_003</td>\n",
       "      <td>1-1 2024 ì£¼ìš” ì¬ì •í†µê³„ 1ê¶Œ</td>\n",
       "      <td>./train_source/1-1 2024 ì£¼ìš” ì¬ì •í†µê³„ 1ê¶Œ.pdf</td>\n",
       "      <td>ì¼ë°˜íšŒê³„, íŠ¹ë³„íšŒê³„, ê¸°ê¸ˆ ê°„ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?</td>\n",
       "      <td>ì¼ë°˜íšŒê³„ëŠ” íŠ¹ì • ì‚¬ì—… ìš´ì˜ ë° íŠ¹ì • ì„¸ì…ìœ¼ë¡œ íŠ¹ì • ì„¸ì¶œì„ ì¶©ë‹¹í•˜ëŠ”ë° ì‚¬ìš©ë˜ê³ , íŠ¹ë³„...</td>\n",
       "      <td>íŠ¹ë³„íšŒê³„ 68.0 69.4 69.7 0.3 71.5 66.4 92.9 3.4 1.3...</td>\n",
       "      <td>Document 1\\níŠ¹ë³„íšŒê³„ 68.0 69.4 69.7 0.3 71.5 66.4 ...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_004</td>\n",
       "      <td>1-1 2024 ì£¼ìš” ì¬ì •í†µê³„ 1ê¶Œ</td>\n",
       "      <td>./train_source/1-1 2024 ì£¼ìš” ì¬ì •í†µê³„ 1ê¶Œ.pdf</td>\n",
       "      <td>2024ë…„ ì´ìˆ˜ì…ì€ ì–¼ë§ˆì´ë©°, ì˜ˆì‚°ìˆ˜ì…ê³¼ ê¸°ê¸ˆìˆ˜ì…ì€ ê°ê° ëª‡ ì¡°ì›ì¸ê°€ìš”?</td>\n",
       "      <td>2024ë…„ ì´ìˆ˜ì…ì€ 612.2ì¡°ì›ì´ë©°, ì˜ˆì‚°ìˆ˜ì…ì€ 395.5ì¡°ì›, ê¸°ê¸ˆìˆ˜ì…ì€ 216...</td>\n",
       "      <td>ì£¼ìš” ì¬ì •í†µê³„â—\\nâ… .\\n402ì¬ì •ìˆ˜ì…\\nâ–¸2024ë…„ë„ ì´ìˆ˜ì…ì€ 612.2ì¡°ì›ì´ë©°,...</td>\n",
       "      <td>Document 1\\nì£¼ìš” ì¬ì •í†µê³„â—\\nâ… .\\n402ì¬ì •ìˆ˜ì…\\nâ–¸2024ë…„ë„ ì´ìˆ˜ì…...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SAMPLE_ID               Source                             Source_path  \\\n",
       "0  TRAIN_000  1-1 2024 ì£¼ìš” ì¬ì •í†µê³„ 1ê¶Œ  ./train_source/1-1 2024 ì£¼ìš” ì¬ì •í†µê³„ 1ê¶Œ.pdf   \n",
       "1  TRAIN_001  1-1 2024 ì£¼ìš” ì¬ì •í†µê³„ 1ê¶Œ  ./train_source/1-1 2024 ì£¼ìš” ì¬ì •í†µê³„ 1ê¶Œ.pdf   \n",
       "2  TRAIN_002  1-1 2024 ì£¼ìš” ì¬ì •í†µê³„ 1ê¶Œ  ./train_source/1-1 2024 ì£¼ìš” ì¬ì •í†µê³„ 1ê¶Œ.pdf   \n",
       "3  TRAIN_003  1-1 2024 ì£¼ìš” ì¬ì •í†µê³„ 1ê¶Œ  ./train_source/1-1 2024 ì£¼ìš” ì¬ì •í†µê³„ 1ê¶Œ.pdf   \n",
       "4  TRAIN_004  1-1 2024 ì£¼ìš” ì¬ì •í†µê³„ 1ê¶Œ  ./train_source/1-1 2024 ì£¼ìš” ì¬ì •í†µê³„ 1ê¶Œ.pdf   \n",
       "\n",
       "                                   Question  \\\n",
       "0            2024ë…„ ì¤‘ì•™ì •ë¶€ ì¬ì •ì²´ê³„ëŠ” ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆë‚˜ìš”?   \n",
       "1          2024ë…„ ì¤‘ì•™ì •ë¶€ì˜ ì˜ˆì‚° ì§€ì¶œì€ ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆë‚˜ìš”?   \n",
       "2                            ê¸°ê¸ˆì´ ì˜ˆì‚°ê³¼ ë‹¤ë¥¸ ì ì€?   \n",
       "3             ì¼ë°˜íšŒê³„, íŠ¹ë³„íšŒê³„, ê¸°ê¸ˆ ê°„ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?   \n",
       "4  2024ë…„ ì´ìˆ˜ì…ì€ ì–¼ë§ˆì´ë©°, ì˜ˆì‚°ìˆ˜ì…ê³¼ ê¸°ê¸ˆìˆ˜ì…ì€ ê°ê° ëª‡ ì¡°ì›ì¸ê°€ìš”?   \n",
       "\n",
       "                                              Answer  \\\n",
       "0  2024ë…„ ì¤‘ì•™ì •ë¶€ ì¬ì •ì²´ê³„ëŠ” ì˜ˆì‚°(ì¼ë°˜Â·íŠ¹ë³„íšŒê³„)ê³¼ ê¸°ê¸ˆìœ¼ë¡œ êµ¬ë¶„ë˜ë©°, 2024ë…„...   \n",
       "1  2024ë…„ ì¤‘ì•™ì •ë¶€ì˜ ì˜ˆì‚° ì§€ì¶œì€ ì¼ë°˜íšŒê³„ 356.5ì¡°ì›, 21ê°œ íŠ¹ë³„íšŒê³„ 81.7...   \n",
       "2  ê¸°ê¸ˆì€ ì˜ˆì‚°ê³¼ êµ¬ë¶„ë˜ëŠ” ì¬ì •ìˆ˜ë‹¨ìœ¼ë¡œì„œ ì¬ì •ìš´ì˜ì˜ ì‹ ì¶•ì„±ì„ ê¸°í•  í•„ìš”ê°€ ìˆì„ ë•Œ, ì •...   \n",
       "3  ì¼ë°˜íšŒê³„ëŠ” íŠ¹ì • ì‚¬ì—… ìš´ì˜ ë° íŠ¹ì • ì„¸ì…ìœ¼ë¡œ íŠ¹ì • ì„¸ì¶œì„ ì¶©ë‹¹í•˜ëŠ”ë° ì‚¬ìš©ë˜ê³ , íŠ¹ë³„...   \n",
       "4  2024ë…„ ì´ìˆ˜ì…ì€ 612.2ì¡°ì›ì´ë©°, ì˜ˆì‚°ìˆ˜ì…ì€ 395.5ì¡°ì›, ê¸°ê¸ˆìˆ˜ì…ì€ 216...   \n",
       "\n",
       "                                             context  \\\n",
       "0  ì£¼ìš” ì¬ì •í†µê³„â—\\nâ… .\\n201ì¬ì •ì²´ê³„\\nâ–¸ì¤‘ì•™ì •ë¶€ ì¬ì •ì²´ê³„ëŠ” ì˜ˆì‚°(ì¼ë°˜ ï½¥íŠ¹ë³„íšŒê³„...   \n",
       "1  ì£¼ìš” ì¬ì •í†µê³„â—\\nâ… .\\n201ì¬ì •ì²´ê³„\\nâ–¸ì¤‘ì•™ì •ë¶€ ì¬ì •ì²´ê³„ëŠ” ì˜ˆì‚°(ì¼ë°˜ ï½¥íŠ¹ë³„íšŒê³„...   \n",
       "2  (1.4) (1.2) (1.5) (3.1) (3.4) (3.1) (3.6) (2.3...   \n",
       "3  íŠ¹ë³„íšŒê³„ 68.0 69.4 69.7 0.3 71.5 66.4 92.9 3.4 1.3...   \n",
       "4  ì£¼ìš” ì¬ì •í†µê³„â—\\nâ… .\\n402ì¬ì •ìˆ˜ì…\\nâ–¸2024ë…„ë„ ì´ìˆ˜ì…ì€ 612.2ì¡°ì›ì´ë©°,...   \n",
       "\n",
       "                                        context_wnum  \\\n",
       "0  Document 1\\nì£¼ìš” ì¬ì •í†µê³„â—\\nâ… .\\n201ì¬ì •ì²´ê³„\\nâ–¸ì¤‘ì•™ì •ë¶€ ì¬ì •ì²´ê³„ëŠ”...   \n",
       "1  Document 1\\nì£¼ìš” ì¬ì •í†µê³„â—\\nâ… .\\n201ì¬ì •ì²´ê³„\\nâ–¸ì¤‘ì•™ì •ë¶€ ì¬ì •ì²´ê³„ëŠ”...   \n",
       "2  Document 1\\n(1.4) (1.2) (1.5) (3.1) (3.4) (3.1...   \n",
       "3  Document 1\\níŠ¹ë³„íšŒê³„ 68.0 69.4 69.7 0.3 71.5 66.4 ...   \n",
       "4  Document 1\\nì£¼ìš” ì¬ì •í†µê³„â—\\nâ… .\\n402ì¬ì •ìˆ˜ì…\\nâ–¸2024ë…„ë„ ì´ìˆ˜ì…...   \n",
       "\n",
       "                                              prompt  \n",
       "0  <|begin_of_text|><|start_header_id|>system<|en...  \n",
       "1  <|begin_of_text|><|start_header_id|>system<|en...  \n",
       "2  <|begin_of_text|><|start_header_id|>system<|en...  \n",
       "3  <|begin_of_text|><|start_header_id|>system<|en...  \n",
       "4  <|begin_of_text|><|start_header_id|>system<|en...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ë°ì´í„°ì…‹ì„ í•™ìŠµì…‹ê³¼ ê²€ì¦ì…‹ìœ¼ë¡œ ë‚˜ëˆ„ê¸°\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=52)\n",
    "\n",
    "# train_dfëŠ” í•™ìŠµì…‹, val_dfëŠ” ê²€ì¦ì…‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì…‹ ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:07<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 851,968 || all params: 8,031,113,216 || trainable%: 0.0106\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# ì–‘ìí™” ì„¤ì •\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_use_double_quant=True, \n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "#ë§Œì•½ GPUì—ì„œ amp(Automatic Mixed Precision)ë¥¼ ì§€ì›í•œë‹¤ë©´, fp16 ëŒ€ì‹  bfloat16ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë”ìš± ì•ˆì •ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ ë° ì–‘ìí™” ì„¤ì • ì ìš©\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# LoRA ì–´ëŒ‘í„° ì„¤ì • ì¶”ê°€\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    inference_mode=False, \n",
    "    r=2, \n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.1\n",
    ")\n",
    "# task_type: ì ìš©í•  ì‘ì—…ì˜ ìœ í˜•(GPT ê³„ì—´ì˜ ê²½ìš° CAUSAL_LM).\n",
    "# inference_mode: í•™ìŠµ ëª¨ë“œ(False) ë˜ëŠ” ì¶”ë¡  ëª¨ë“œ(True).\n",
    "# r: LoRAì˜ ë³‘ëª© ì°¨ì›.\n",
    "# lora_alpha: ìŠ¤ì¼€ì¼ë§ ì¸ì.\n",
    "# lora_dropout: ë“œë¡­ì•„ì›ƒ í™•ë¥ .\n",
    "\n",
    "# ëª¨ë¸ì— LoRA ì–´ëŒ‘í„° ì ìš©\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 396/396 [00:00<00:00, 1791.62 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 396/396 [00:00<00:00, 10555.33 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 2325.40 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 8333.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from datasets import Dataset\n",
    "# í† í¬ë‚˜ì´ì¦ˆ í•¨ìˆ˜ ì •ì˜\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['prompt'], padding='max_length', truncation=True, max_length=512)\n",
    "# ëª¨ë¸ì˜ ì •ë‹µ ë¼ë²¨ ì„¤ì •\n",
    "def add_labels(examples):\n",
    "    labels = tokenizer(examples['Answer'], padding='max_length', truncation=True, max_length=512).input_ids\n",
    "    examples['labels'] = labels\n",
    "    return examples\n",
    "\n",
    "\n",
    "# ì˜ˆì‹œë¡œ, ê¸°ì¡´ ë°ì´í„°ì…‹ ìƒì„±\n",
    "train_dataset = Dataset.from_pandas(train_df[['prompt', 'Answer']])\n",
    "val_dataset = Dataset.from_pandas(val_df[['prompt', 'Answer']])\n",
    "\n",
    "# í† í¬ë‚˜ì´ì§•ê³¼ ë¼ë²¨ ì¶”ê°€ëŠ” ë™ì¼í•˜ê²Œ ì ìš©\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "train_dataset = train_dataset.map(add_labels, batched=True)\n",
    "\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(add_labels, batched=True)\n",
    "\n",
    "# # DistributedSampler ìƒì„±\n",
    "# train_sampler = DistributedSampler(train_dataset)\n",
    "# val_sampler = DistributedSampler(val_dataset)\n",
    "\n",
    "# # DataLoaderì— ìƒ˜í”ŒëŸ¬ ì ìš©\n",
    "# train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=1)\n",
    "# val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ëª¨ë¸ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "# F1 ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜\n",
    "def calculate_f1_score(true_sentence, predicted_sentence, sum_mode=True):\n",
    "    true_sentence = ''.join(true_sentence.split())\n",
    "    predicted_sentence = ''.join(predicted_sentence.split())\n",
    "    \n",
    "    true_counter = Counter(true_sentence)\n",
    "    predicted_counter = Counter(predicted_sentence)\n",
    "    \n",
    "    if sum_mode:\n",
    "        true_positive = sum((true_counter & predicted_counter).values())\n",
    "        predicted_positive = sum(predicted_counter.values())\n",
    "        actual_positive = sum(true_counter.values())\n",
    "    else:\n",
    "        true_positive = len((true_counter & predicted_counter).values())\n",
    "        predicted_positive = len(predicted_counter.values())\n",
    "        actual_positive = len(true_counter.values())\n",
    "\n",
    "    precision = true_positive / predicted_positive if predicted_positive > 0 else 0\n",
    "    recall = true_positive / actual_positive if actual_positive > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "# compute_metrics í•¨ìˆ˜ ì •ì˜\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    predicted_sentences = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions]\n",
    "    true_sentences = [tokenizer.decode(label, skip_special_tokens=True) for label in labels]\n",
    "    \n",
    "    f1_scores = [calculate_f1_score(true, pred)[2] for true, pred in zip(true_sentences, predicted_sentences)]\n",
    "    avg_f1_score = np.mean(f1_scores)\n",
    "    \n",
    "    return {\"f1\": avg_f1_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–      | 396/1188 [03:41<07:19,  1.80it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 9.05 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 29\u001b[0m\n\u001b[0;32m     19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     20\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,                             \u001b[38;5;66;03m# í•™ìŠµí•  ëª¨ë¸\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,                      \u001b[38;5;66;03m# ìœ„ì—ì„œ ì„¤ì •í•œ TrainingArguments\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics          \u001b[38;5;66;03m# í‰ê°€ ë©”íŠ¸ë¦­ í•¨ìˆ˜ (ì„ íƒ ì‚¬í•­)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# í•™ìŠµ ìˆ˜í–‰\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\trainer.py:1948\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1946\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1948\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1949\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1953\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\trainer.py:2386\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2383\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2385\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2386\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   2389\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[0;32m   2390\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\trainer.py:2814\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2812\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[1;32m-> 2814\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m   2817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_checkpoint(model, trial, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\trainer.py:2771\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[1;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[0;32m   2770\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m-> 2771\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2772\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2774\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\trainer.py:3676\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3673\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3675\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3676\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3677\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3679\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   3680\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   3681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3684\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3686\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\trainer.py:3892\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3890\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((logits))\n\u001b[0;32m   3891\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_eval_metrics \u001b[38;5;129;01mor\u001b[39;00m description \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 3892\u001b[0m         \u001b[43mall_preds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3894\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((labels))\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\trainer_pt_utils.py:322\u001b[0m, in \u001b[0;36mEvalLoopContainer.add\u001b[1;34m(self, tensors)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_nested_concat \u001b[38;5;28;01melse\u001b[39;00m [tensors]\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_nested_concat:\n\u001b[1;32m--> 322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m \u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mappend(tensors)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\trainer_pt_utils.py:136\u001b[0m, in \u001b[0;36mnested_concat\u001b[1;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_pad_and_concatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, Mapping):\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\n\u001b[0;32m    139\u001b[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    140\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\trainer_pt_utils.py:94\u001b[0m, in \u001b[0;36mtorch_pad_and_concatenate\u001b[1;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[0;32m     91\u001b[0m tensor2 \u001b[38;5;241m=\u001b[39m atleast_1d(tensor2)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Let's figure out the new shape\u001b[39;00m\n\u001b[0;32m     97\u001b[0m new_shape \u001b[38;5;241m=\u001b[39m (tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mmax\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 9.05 GiB. GPU "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# TrainingArguments ì„¤ì •\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                  # ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬\n",
    "    evaluation_strategy=\"epoch\",             # ì—í¬í¬ë§ˆë‹¤ í‰ê°€\n",
    "    per_device_train_batch_size=1,           # GPU í•˜ë‚˜ë‹¹ ë°°ì¹˜ í¬ê¸° (DataLoaderì—ì„œ ì„¤ì •í•œ ê²ƒê³¼ ì¼ì¹˜ì‹œí‚´)\n",
    "    per_device_eval_batch_size=1,            # GPU í•˜ë‚˜ë‹¹ í‰ê°€ ë°°ì¹˜ í¬ê¸° (DataLoaderì—ì„œ ì„¤ì •í•œ ê²ƒê³¼ ì¼ì¹˜ì‹œí‚´)\n",
    "    num_train_epochs=3,                      # í•™ìŠµí•  ì—í¬í¬ ìˆ˜\n",
    "    learning_rate=5e-5,                      # í•™ìŠµë¥ \n",
    "    weight_decay=0.01,                       # ê°€ì¤‘ì¹˜ ê°ì†Œ\n",
    "    fp16=True,                               # Mixed Precision ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½ ë° í•™ìŠµ ì†ë„ ì¦ê°€)\n",
    "    dataloader_pin_memory=True,              # DataLoaderê°€ ë©”ëª¨ë¦¬ í•€ ì„¤ì •ì„ í•˜ë„ë¡ ì„¤ì •\n",
    "    dataloader_drop_last=False,              # ë°°ì¹˜ê°€ ë‚˜ëˆ„ì–´ ë–¨ì–´ì§€ì§€ ì•Šì„ ê²½ìš° ë§ˆì§€ë§‰ ë°°ì¹˜ ë“œë¡­ ì—¬ë¶€\n",
    "    report_to=\"none\",                        # ë¡œê¹…ì´ë‚˜ ê¸°íƒ€ ë¦¬í¬íŒ… ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (í•„ìš”ì‹œ ë³€ê²½)\n",
    ")\n",
    "\n",
    "# Trainer ì„¤ì •\n",
    "trainer = Trainer(\n",
    "    model=model,                             # í•™ìŠµí•  ëª¨ë¸\n",
    "    args=training_args,                      # ìœ„ì—ì„œ ì„¤ì •í•œ TrainingArguments\n",
    "    train_dataset=train_dataset,       # ì»¤ìŠ¤í…€ DataLoader (DistributedSampler ì‚¬ìš©)\n",
    "    eval_dataset=val_dataset,          # ì»¤ìŠ¤í…€ DataLoader (DistributedSampler ì‚¬ìš©)\n",
    "    tokenizer=tokenizer,                     # ì‚¬ìš© ì¤‘ì¸ í† í¬ë‚˜ì´ì €\n",
    "    compute_metrics=compute_metrics          # í‰ê°€ ë©”íŠ¸ë¦­ í•¨ìˆ˜ (ì„ íƒ ì‚¬í•­)\n",
    ")\n",
    "\n",
    "# í•™ìŠµ ìˆ˜í–‰\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì €ì¥\n",
    "model.save_pretrained(\"./saved_model\")\n",
    "tokenizer.save_pretrained(\"./saved_model\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# ì–‘ìí™”ëœ ëª¨ë¸ê³¼ LoRA ì–´ëŒ‘í„°ê°€ ì ìš©ëœ ëª¨ë¸ ë¡œë“œ\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"./saved_model\", quantization_config=bnb_config)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"./saved_model\", quantization_config=bnb_config)\n",
    "# model = PeftModel.from_pretrained(model, \"./saved_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
