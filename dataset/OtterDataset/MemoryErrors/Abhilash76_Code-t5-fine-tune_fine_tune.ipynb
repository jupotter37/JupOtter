{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baab97a6874e53ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T15:17:52.017697Z",
     "start_time": "2024-02-27T15:17:52.014771Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77b33a64157a9cce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T15:17:52.049322Z",
     "start_time": "2024-02-27T15:17:52.045010Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T15:17:54.688751Z",
     "start_time": "2024-02-27T15:17:52.089172Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19573,
     "status": "ok",
     "timestamp": 1708978777884,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "initial_id",
    "outputId": "d41a9e9d-cec7-40c5-cd24-528955aa500f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/abhilash/.local/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: torchtext in /home/abhilash/.local/lib/python3.10/site-packages (0.17.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.1)\n",
      "Requirement already satisfied: sentencepiece in /home/abhilash/.local/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
      "Requirement already satisfied: datasets in /home/abhilash/.local/lib/python3.10/site-packages (2.17.1)\n",
      "Requirement already satisfied: accelerate in /home/abhilash/.local/lib/python3.10/site-packages (0.26.1)\n",
      "Requirement already satisfied: bitsandbytes in /home/abhilash/.local/lib/python3.10/site-packages (0.42.0)\n",
      "Requirement already satisfied: peft in /home/abhilash/.local/lib/python3.10/site-packages (0.10.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/abhilash/.local/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/abhilash/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.26.3)\n",
      "Requirement already satisfied: torchdata==0.7.1 in /home/abhilash/.local/lib/python3.10/site-packages (from torchtext) (0.7.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in /usr/lib/python3/dist-packages (from torchdata==0.7.1->torchtext) (1.26.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/abhilash/.local/lib/python3.10/site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/abhilash/.local/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/abhilash/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /home/abhilash/.local/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/abhilash/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /home/abhilash/.local/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: psutil in /home/abhilash/.local/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: scipy in /home/abhilash/.local/lib/python3.10/site-packages (from bitsandbytes) (1.12.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/abhilash/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/abhilash/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/abhilash/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/abhilash/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/abhilash/.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchtext) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchtext) (2020.6.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/abhilash/.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/abhilash/.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install torch torchtext transformers sentencepiece pandas tqdm datasets accelerate bitsandbytes peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34702c45c363d328",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T15:17:55.418834Z",
     "start_time": "2024-02-27T15:17:54.690396Z"
    },
    "executionInfo": {
     "elapsed": 567,
     "status": "ok",
     "timestamp": 1708978778445,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "34702c45c363d328"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/numpy/lib/scimath.py:46: RuntimeWarning: divide by zero encountered in log\n",
      "  _ln2 = nx.log(2.0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import pandas as pd\n",
    "import ast\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f304f37f66bed64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T15:17:56.061183Z",
     "start_time": "2024-02-27T15:17:55.421835Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "6f6ac04385994affbd66e11501711f04",
      "e8d0e0100b1e48639ba2e1bc981331d7",
      "fa4b4c785e274c52ba4994f9500f12e1",
      "f2689721c574404d96290323d7cd946c",
      "5de36102a2c04919a5b1edad5282829e",
      "fd857a7c201447fc956d199deea6f407",
      "426bfaa9a8ed427fa20d13adf24f6353",
      "615c1ae1332d44d59ac8e9ac76c6545b",
      "9694d9914b0546b29fba99a0a4e2bde2",
      "f23672ed896046979ab9c31901cf82fe",
      "5e3404b6278c430198105fc11c78695e"
     ]
    },
    "executionInfo": {
     "elapsed": 21843,
     "status": "ok",
     "timestamp": 1708978800284,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "9f304f37f66bed64",
    "outputId": "80844ed2-600d-43d6-a4b0-a53c1acab26c"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "data = load_dataset(\"csv\", data_files=\"merged_file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a34b53a14d41b48c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T15:17:56.086976Z",
     "start_time": "2024-02-27T15:17:56.062763Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1708978800284,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "a34b53a14d41b48c",
    "outputId": "5d5846fe-d03c-4591-8377-b60efe39a56b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'cell_id_x', 'notebook_id', 'code_imports', 'defined_functions', 'source_x', 'cell_id_y', 'source_y'],\n",
       "        num_rows: 828122\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "289b592c37c45d5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T15:18:26.512669Z",
     "start_time": "2024-02-27T15:17:56.088060Z"
    },
    "executionInfo": {
     "elapsed": 77710,
     "status": "ok",
     "timestamp": 1708978877988,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "289b592c37c45d5d"
   },
   "outputs": [],
   "source": [
    "updated_data = [{'Code': item['source_x'], 'Description': item['source_y']} for item in data[\"train\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc4929fb267d4965",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T15:18:26.741399Z",
     "start_time": "2024-02-27T15:18:26.513794Z"
    },
    "executionInfo": {
     "elapsed": 680,
     "status": "ok",
     "timestamp": 1708978878654,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "cc4929fb267d4965"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(updated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4886ab76dc27ef30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T15:18:26.777830Z",
     "start_time": "2024-02-27T15:18:26.758041Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1708978878655,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "4886ab76dc27ef30",
    "outputId": "f964c535-5acf-468c-9e6b-0578bb86304f",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>import os\\nPROJECT = \"cloud-training-demos\" # ...</td>\n",
       "      <td># MNIST Image Classification with TensorFlow o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>%%bash\\ngcloud config set project $PROJECT\\ngc...</td>\n",
       "      <td># MNIST Image Classification with TensorFlow o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>%%bash\\nrm -rf mnistmodel.tar.gz mnist_trained...</td>\n",
       "      <td>## Run as a Python module\\n\\nIn the previous n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>%%bash\\nOUTDIR=gs://${BUCKET}/mnist/trained_${...</td>\n",
       "      <td>**Now, let's do it on Cloud ML Engine so we ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>from google.datalab.ml import TensorBoard\\nTen...</td>\n",
       "      <td>## Monitoring training with TensorBoard\\n\\nUse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>for pid in TensorBoard.list()[\"pid\"]:\\n    Ten...</td>\n",
       "      <td>## Monitoring training with TensorBoard\\n\\nUse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>%%bash\\nMODEL_NAME=\"mnist\"\\nMODEL_VERSION=${MO...</td>\n",
       "      <td>## Deploying and predicting with model\\n\\nDepl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>import json, codecs\\nimport matplotlib.pyplot ...</td>\n",
       "      <td>To predict with the model, let's take one of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>%%bash\\ngcloud ml-engine predict \\\\n    --mode...</td>\n",
       "      <td>Send it to the prediction service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>trainingInput:\\n    scaleTier: CUSTOM\\n    mas...</td>\n",
       "      <td>## DO NOT RUN anything beyond this point\\n\\nTh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>%%bash\\nOUTDIR=gs://${BUCKET}/mnist/trained_${...</td>\n",
       "      <td>This takes &lt;b&gt;13 hours and 250 ML Units&lt;/b&gt;, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>from subprocess import check_output\\nprint(che...</td>\n",
       "      <td># Keras - Bidirectional LSTM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>import numpy as np\\nimport pandas as pd</td>\n",
       "      <td># Keras - Bidirectional LSTM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>embedding_matrix</td>\n",
       "      <td>### Read Glove Word Vectors : word -&gt; vector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>from keras.models import Model\\nfrom keras.lay...</td>\n",
       "      <td>### Import keras Libs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>EMBEDDING_FILE = f'glove.6B.50d.txt'\\nTRAIN_DA...</td>\n",
       "      <td>### Import keras Libs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>max_features = 20000\\nmaxlen = 100\\nembed_size...</td>\n",
       "      <td>### Import keras Libs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>train = pd.read_csv(TRAIN_DATA_FILE)</td>\n",
       "      <td>### Import keras Libs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>train.head()</td>\n",
       "      <td>### Import keras Libs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>#train = train.sample(frac=1)\\ntest = pd.read_...</td>\n",
       "      <td>### Import keras Libs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>test.head()</td>\n",
       "      <td>### Import keras Libs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>list_sentences_train = train['comment_text'].f...</td>\n",
       "      <td>### Import keras Libs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>#print(list_sentences_train)</td>\n",
       "      <td>### Import keras Libs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>list_classes = [\"toxic\", \"severe_toxic\", \"obsc...</td>\n",
       "      <td>### Import keras Libs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>list_classes</td>\n",
       "      <td>### Import keras Libs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>y = train[list_classes].values</td>\n",
       "      <td>### Import keras Libs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>y</td>\n",
       "      <td>### Import keras Libs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>y.shape</td>\n",
       "      <td>### Import keras Libs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>list_sentences_test = test['comment_text'].fil...</td>\n",
       "      <td>### Import keras Libs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>#print(list_sentences_test)</td>\n",
       "      <td>### Import keras Libs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>tokenizer = text.Tokenizer(num_words=max_featu...</td>\n",
       "      <td>### Import keras Libs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>print(tokenizer)</td>\n",
       "      <td>### Import keras Libs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>tokenizer.fit_on_texts(list(list_sentences_tra...</td>\n",
       "      <td>### Import keras Libs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>list_tokenized_train = tokenizer.texts_to_sequ...</td>\n",
       "      <td>### Import keras Libs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>print(list_tokenized_train)</td>\n",
       "      <td>### Import keras Libs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>X_t = sequence.pad_sequences(list_tokenized_tr...</td>\n",
       "      <td>### Import keras Libs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>X_t.shape</td>\n",
       "      <td>### Import keras Libs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>X_te.shape</td>\n",
       "      <td>### Import keras Libs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>def get_coefs(word,*arr): \\n    return word, n...</td>\n",
       "      <td>### Read Glove Word Vectors : word -&gt; vector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>all_embs = np.stack(embeddings_index.values())</td>\n",
       "      <td>### Read Glove Word Vectors : word -&gt; vector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>emb_mean,emb_std = all_embs.mean(), all_embs.s...</td>\n",
       "      <td>### Read Glove Word Vectors : word -&gt; vector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>emb_mean,emb_std</td>\n",
       "      <td>### Read Glove Word Vectors : word -&gt; vector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>word_index = tokenizer.word_index</td>\n",
       "      <td>### Read Glove Word Vectors : word -&gt; vector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>print(word_index)</td>\n",
       "      <td>### Read Glove Word Vectors : word -&gt; vector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>nb_words = min(max_features, len(word_index))</td>\n",
       "      <td>### Read Glove Word Vectors : word -&gt; vector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>nb_words</td>\n",
       "      <td>### Read Glove Word Vectors : word -&gt; vector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>embedding_matrix = np.random.normal(emb_mean, ...</td>\n",
       "      <td>### Read Glove Word Vectors : word -&gt; vector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>embedding_matrix</td>\n",
       "      <td>### Read Glove Word Vectors : word -&gt; vector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>for word, i in word_index.items():\\n    if i &gt;...</td>\n",
       "      <td>### Read Glove Word Vectors : word -&gt; vector</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>def get_model():\\n    inp = Input(shape=(maxle...</td>\n",
       "      <td>### Read Glove Word Vectors : word -&gt; vector</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Code  \\\n",
       "0   import os\\nPROJECT = \"cloud-training-demos\" # ...   \n",
       "1   %%bash\\ngcloud config set project $PROJECT\\ngc...   \n",
       "2   %%bash\\nrm -rf mnistmodel.tar.gz mnist_trained...   \n",
       "3   %%bash\\nOUTDIR=gs://${BUCKET}/mnist/trained_${...   \n",
       "4   from google.datalab.ml import TensorBoard\\nTen...   \n",
       "5   for pid in TensorBoard.list()[\"pid\"]:\\n    Ten...   \n",
       "6   %%bash\\nMODEL_NAME=\"mnist\"\\nMODEL_VERSION=${MO...   \n",
       "7   import json, codecs\\nimport matplotlib.pyplot ...   \n",
       "8   %%bash\\ngcloud ml-engine predict \\\\n    --mode...   \n",
       "9   trainingInput:\\n    scaleTier: CUSTOM\\n    mas...   \n",
       "10  %%bash\\nOUTDIR=gs://${BUCKET}/mnist/trained_${...   \n",
       "11  from subprocess import check_output\\nprint(che...   \n",
       "12            import numpy as np\\nimport pandas as pd   \n",
       "13                                   embedding_matrix   \n",
       "14  from keras.models import Model\\nfrom keras.lay...   \n",
       "15  EMBEDDING_FILE = f'glove.6B.50d.txt'\\nTRAIN_DA...   \n",
       "16  max_features = 20000\\nmaxlen = 100\\nembed_size...   \n",
       "17               train = pd.read_csv(TRAIN_DATA_FILE)   \n",
       "18                                       train.head()   \n",
       "19  #train = train.sample(frac=1)\\ntest = pd.read_...   \n",
       "20                                        test.head()   \n",
       "21  list_sentences_train = train['comment_text'].f...   \n",
       "22                       #print(list_sentences_train)   \n",
       "23  list_classes = [\"toxic\", \"severe_toxic\", \"obsc...   \n",
       "24                                       list_classes   \n",
       "25                     y = train[list_classes].values   \n",
       "26                                                  y   \n",
       "27                                            y.shape   \n",
       "28  list_sentences_test = test['comment_text'].fil...   \n",
       "29                        #print(list_sentences_test)   \n",
       "30  tokenizer = text.Tokenizer(num_words=max_featu...   \n",
       "31                                   print(tokenizer)   \n",
       "32  tokenizer.fit_on_texts(list(list_sentences_tra...   \n",
       "33  list_tokenized_train = tokenizer.texts_to_sequ...   \n",
       "34                        print(list_tokenized_train)   \n",
       "35  X_t = sequence.pad_sequences(list_tokenized_tr...   \n",
       "36                                          X_t.shape   \n",
       "37                                         X_te.shape   \n",
       "38  def get_coefs(word,*arr): \\n    return word, n...   \n",
       "39     all_embs = np.stack(embeddings_index.values())   \n",
       "40  emb_mean,emb_std = all_embs.mean(), all_embs.s...   \n",
       "41                                   emb_mean,emb_std   \n",
       "42                  word_index = tokenizer.word_index   \n",
       "43                                  print(word_index)   \n",
       "44      nb_words = min(max_features, len(word_index))   \n",
       "45                                           nb_words   \n",
       "46  embedding_matrix = np.random.normal(emb_mean, ...   \n",
       "47                                   embedding_matrix   \n",
       "48  for word, i in word_index.items():\\n    if i >...   \n",
       "49  def get_model():\\n    inp = Input(shape=(maxle...   \n",
       "\n",
       "                                          Description  \n",
       "0   # MNIST Image Classification with TensorFlow o...  \n",
       "1   # MNIST Image Classification with TensorFlow o...  \n",
       "2   ## Run as a Python module\\n\\nIn the previous n...  \n",
       "3   **Now, let's do it on Cloud ML Engine so we ca...  \n",
       "4   ## Monitoring training with TensorBoard\\n\\nUse...  \n",
       "5   ## Monitoring training with TensorBoard\\n\\nUse...  \n",
       "6   ## Deploying and predicting with model\\n\\nDepl...  \n",
       "7   To predict with the model, let's take one of t...  \n",
       "8                   Send it to the prediction service  \n",
       "9   ## DO NOT RUN anything beyond this point\\n\\nTh...  \n",
       "10  This takes <b>13 hours and 250 ML Units</b>, s...  \n",
       "11                      # Keras - Bidirectional LSTM   \n",
       "12                      # Keras - Bidirectional LSTM   \n",
       "13       ### Read Glove Word Vectors : word -> vector  \n",
       "14                              ### Import keras Libs  \n",
       "15                              ### Import keras Libs  \n",
       "16                              ### Import keras Libs  \n",
       "17                              ### Import keras Libs  \n",
       "18                              ### Import keras Libs  \n",
       "19                              ### Import keras Libs  \n",
       "20                              ### Import keras Libs  \n",
       "21                              ### Import keras Libs  \n",
       "22                              ### Import keras Libs  \n",
       "23                              ### Import keras Libs  \n",
       "24                              ### Import keras Libs  \n",
       "25                              ### Import keras Libs  \n",
       "26                              ### Import keras Libs  \n",
       "27                              ### Import keras Libs  \n",
       "28                              ### Import keras Libs  \n",
       "29                              ### Import keras Libs  \n",
       "30                              ### Import keras Libs  \n",
       "31                              ### Import keras Libs  \n",
       "32                              ### Import keras Libs  \n",
       "33                              ### Import keras Libs  \n",
       "34                              ### Import keras Libs  \n",
       "35                              ### Import keras Libs  \n",
       "36                              ### Import keras Libs  \n",
       "37                              ### Import keras Libs  \n",
       "38       ### Read Glove Word Vectors : word -> vector  \n",
       "39       ### Read Glove Word Vectors : word -> vector  \n",
       "40       ### Read Glove Word Vectors : word -> vector  \n",
       "41       ### Read Glove Word Vectors : word -> vector  \n",
       "42       ### Read Glove Word Vectors : word -> vector  \n",
       "43       ### Read Glove Word Vectors : word -> vector  \n",
       "44       ### Read Glove Word Vectors : word -> vector  \n",
       "45       ### Read Glove Word Vectors : word -> vector  \n",
       "46       ### Read Glove Word Vectors : word -> vector  \n",
       "47       ### Read Glove Word Vectors : word -> vector  \n",
       "48       ### Read Glove Word Vectors : word -> vector  \n",
       "49       ### Read Glove Word Vectors : word -> vector  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "020ce0ed-efc7-47b3-9139-f77adedf57bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:182\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ''",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3805\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3806\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3807\u001b[0m     ):\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "df['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3a3ea3ec52da81f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T15:18:29.362602Z",
     "start_time": "2024-02-27T15:18:26.778768Z"
    },
    "executionInfo": {
     "elapsed": 4874,
     "status": "ok",
     "timestamp": 1708978883522,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "a3a3ea3ec52da81f"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ecaf6999b2ee235",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T15:18:29.414194Z",
     "start_time": "2024-02-27T15:18:29.363486Z"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1708978883522,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "6ecaf6999b2ee235"
   },
   "outputs": [],
   "source": [
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device('cuda')\n",
    "\n",
    "#else:\n",
    "#    try:\n",
    "#        device = torch.device('mps')\n",
    "#    except Exception:\n",
    "#        device = torch.device('cpu')\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5e9f9e5ee08071b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T15:18:29.421730Z",
     "start_time": "2024-02-27T15:18:29.416069Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1708978883522,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "e5e9f9e5ee08071b",
    "outputId": "204458ef-850f-4651-a88e-04a0814f2bcb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bf0fdfb52d66246",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-27T15:18:53.923566Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345,
     "referenced_widgets": [
      "0338cff46db84141bbd3e448a6cec1c6",
      "c1311766e96a42a2a6afa46ae0900cc2",
      "d9252aee504c4211afe12e2f149530f3",
      "4b6dab05933c4a61ae3cb97beb70fc8d",
      "648f7401a38842119b87040393042eda",
      "f014cca49d0441f69499cc98923c84b7",
      "c4af812000dd4470b65fff7b7694680b",
      "1094659a432b45ba8230444cd526cb8f",
      "4432a666276a496cb1bf04354ad9a417",
      "84f9e4b99afd4264bb89cb7629598546",
      "957b4117bdb145a5900bec2a0501ea99",
      "c95e7a7edaff4dc39c8a31b70c5567e4",
      "0403eaa45933488d9a62399c454ff30d",
      "5550f3c40eca4c46864c6c713bf501d6",
      "50735bd82c34493e8ed909dc37ee1028",
      "f994d30e8606483a8c783beaa6935bf1",
      "8d129dfba3b14ab6a7e9b4bf23009716",
      "b02fb084d07e469d9e828ed634364b1a",
      "c71b5073cc764a52882d58064c3a1854",
      "1b345a04dae547c69cfdb8abfc1fc788",
      "ebc45a8d90ee4c3f8edf9954ded85a03",
      "cb793974256c42f78c89f66152fc63ba",
      "af962879d466466b8bfdb57f293d1d09",
      "f3b6ba638ffe47eda7dfccccdb87ea4e",
      "253c3d9f966a4c769877833da4b16d7e",
      "cf95923d61a743128c6c89328c0c527b",
      "d7e60c80882242cbbdfaac49e3b31287",
      "5480cf12eba446f9b8fab6073a6cc5ef",
      "f7f3735f60e548419d4a380208b9e530",
      "cd91b20f621f4e7faffc4bb8725dc063",
      "4f4e35c989a14f8a823e1692f1969a3b",
      "90eaacb26aba4ee38f37e8c005cffaa1",
      "839f8c1724cc440b9c4bb69d0876171f",
      "ca400e55b65a4cd0a5b1fdf6df39a23e",
      "51acb6ef3155414dadd217959e68798d",
      "c88a701108f140ad9296e1935132e19b",
      "04df562ce2f84059819dabe3dbaffe46",
      "1c5fe8caec364d84bad4cc30fca2e0a0",
      "704c69c39b884796ac85838099a0ecf4",
      "ebd62dd8188442749a8d42eaeb3d6e94",
      "06efaa4c02024b1f8d9cd16a2e921983",
      "123f85eea3e74d04b009472051c5658b",
      "3032f12569bf4fedac696f9a89fa2190",
      "5d7019f6667242ef8006e2cd31339ce5",
      "003a1240316f4c999311e4eb4c94947a",
      "c24b31ec3a4843258b079e96b748ea6a",
      "c0ac8000f2e2464294ed348f3e4375e3",
      "44b7101b2ae34f97ba7d017453d95a20",
      "3331119af7d648868a3a800013b5ce9d",
      "4f61077e74a94d0b9101dc12573395a0",
      "23a8404240c64008990c86002aaa1b62",
      "5b0d8c8ed8f54cb7add26b53469716cb",
      "06ffc5179dcc45cb945d5d13dc80b611",
      "30b039505b2c4f468a836605f764863f",
      "bdfb5661cd2d4bac9e43a9304461df5a",
      "37dfa295b7d34ad691c5692d42d2ee54",
      "a2a180b43818401fb65704b7daa32a2f",
      "432709c15d844f5c863fabeb9ae93052",
      "f531d74111a64e03bcdff13b52408708",
      "e24f145940d94f228e9dbee0223765d0",
      "7cf08471a81c4f42b71adf62b2e65ff0",
      "7c3b452280c448d8bcae01a58b818336",
      "0fcca77c266b4d54a5ca4a4bccd62ff3",
      "f429dc73d4a24c59a81d0fd28a47ced6",
      "8e246e16f77a41c3adf4fb1e85728dc3",
      "a885164d7791437d93eb228a19334e81",
      "dde08ca3738a459c8f3a246168eaf932",
      "8a083bc0ae744eb9812b2007483ff5c5",
      "f5ed685cdfa5497cab9f2fc26f1aefc0",
      "3f2d1d994cb14b02a4be12cd2a9d95fd",
      "158a7d4e6acf49f2b78a3c8171e5aed4",
      "c2f26422fb944968a3b3f41741b733cb",
      "d02dfa1fc5304b959b11a8c128ae1382",
      "b1937b66e7c24180b5c7cfa98fb53859",
      "62a9e5956847481cab3f24dc9519a0d6",
      "2b4919a2fd03451b80a8f275f01362f2",
      "273d2e752f7540509bbf4e46d6e230da"
     ]
    },
    "executionInfo": {
     "elapsed": 177695,
     "status": "ok",
     "timestamp": 1708979061210,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "6bf0fdfb52d66246",
    "is_executing": true,
    "outputId": "e3b4b0e2-361d-413e-c57a-e04b1808ee54"
   },
   "outputs": [],
   "source": [
    "model_name = \"sagard21/python-code-explainer\"\n",
    "\n",
    "# Adding the bitsandbytes configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, config=bnb_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e686b7be01c3d56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1708979061210,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "5e686b7be01c3d56",
    "outputId": "8001f81f-5322-4b63-9fc3-70937d0ae769"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32100, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bQM59C1bv4JR",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1708979061210,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "bQM59C1bv4JR"
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3kbhT0HiwPUO",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1708979061210,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "3kbhT0HiwPUO"
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "qISerBXrx2C5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3727,
     "status": "ok",
     "timestamp": 1708979064933,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "qISerBXrx2C5",
    "outputId": "64e5b303-fc22-4c27-906e-0195a29a76f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4718592 || all params: 742358016 || trainable%: 0.6356221524251716\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"k\", \"v\", \"o\", \"up_proj\", \"down_proj\", \"gate_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a178ff7ec8382d4",
   "metadata": {
    "executionInfo": {
     "elapsed": 1084,
     "status": "ok",
     "timestamp": 1708979171993,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "a178ff7ec8382d4"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a222f783535b926e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "executionInfo": {
     "elapsed": 1476,
     "status": "ok",
     "timestamp": 1708979175162,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "a222f783535b926e",
    "outputId": "4d98a135-4eed-4bb9-bc3b-f42a4f233f4a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>789629</td>\n",
       "      <td>826331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>552574</td>\n",
       "      <td>237424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>df.head()</td>\n",
       "      <td># Vertical Scan with Space Charge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1028</td>\n",
       "      <td>1015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Code                        Description\n",
       "count      789629                             826331\n",
       "unique     552574                             237424\n",
       "top     df.head()  # Vertical Scan with Space Charge\n",
       "freq         1028                               1015"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22ef70d26f566522",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1708979177265,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "22ef70d26f566522"
   },
   "outputs": [],
   "source": [
    "# Dataset preparation\n",
    "class LanguageDataset(Dataset):\n",
    "    def __init__(self, _df, _tokenizer, max_length=1024):\n",
    "        self.labels = _df.columns\n",
    "        self.data = _df.to_dict(orient='records')\n",
    "        #print(self.data[:10])\n",
    "        self.tokenizer = _tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx][self.labels[0]]\n",
    "        y = self.data[idx][self.labels[1]]\n",
    "        text = f\"{x} | {y}\"\n",
    "        print(text)\n",
    "        tokens = self.tokenizer.encode_plus(text,\n",
    "                                            return_tensors='pt',\n",
    "                                            max_length=1024,\n",
    "                                            truncation=True,\n",
    "                                            padding='max_length')\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d8d2cf9ef7d7492",
   "metadata": {
    "executionInfo": {
     "elapsed": 2417,
     "status": "ok",
     "timestamp": 1708979183276,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "5d8d2cf9ef7d7492"
   },
   "outputs": [],
   "source": [
    "data_sample = LanguageDataset(df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46a42d361847991a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1708979185808,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "46a42d361847991a",
    "outputId": "5ad08593-e1c0-4c13-a48e-8c88ccde4084"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "PROJECT = \"cloud-training-demos\" # REPLACE WITH YOUR PROJECT ID\n",
      "BUCKET = \"cloud-training-demos-ml\" # REPLACE WITH YOUR BUCKET NAME\n",
      "REGION = \"us-central1\" # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
      "MODEL_TYPE = \"dnn\"  # \"linear\", \"dnn\", \"dnn_dropout\", or \"cnn\"\n",
      "\n",
      "# Do not change these\n",
      "os.environ[\"PROJECT\"] = PROJECT\n",
      "os.environ[\"BUCKET\"] = BUCKET\n",
      "os.environ[\"REGION\"] = REGION\n",
      "os.environ[\"MODEL_TYPE\"] = MODEL_TYPE\n",
      "os.environ[\"TFVERSION\"] = \"1.13\"  # Tensorflow version | # MNIST Image Classification with TensorFlow on Cloud ML Engine\n",
      "\n",
      "This notebook demonstrates how to implement different image models on MNIST using Estimator. \n",
      "\n",
      "Note the MODEL_TYPE; change it to try out different models\n",
      "{'input_ids': tensor([[   1, 5666, 1140,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])}\n",
      "%%bash\n",
      "gcloud config set project $PROJECT\n",
      "gcloud config set compute/region $REGION | # MNIST Image Classification with TensorFlow on Cloud ML Engine\n",
      "\n",
      "This notebook demonstrates how to implement different image models on MNIST using Estimator. \n",
      "\n",
      "Note the MODEL_TYPE; change it to try out different models\n",
      "{'input_ids': tensor([[    1, 11438, 28734,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])}\n",
      "%%bash\n",
      "rm -rf mnistmodel.tar.gz mnist_trained\n",
      "gcloud ml-engine local train \\\n",
      "    --module-name=trainer.task \\\n",
      "    --package-path=${PWD}/mnistmodel/trainer \\\n",
      "    -- \\\n",
      "    --output_dir=${PWD}/mnist_trained \\\n",
      "    --train_steps=100 \\\n",
      "    --learning_rate=0.01 \\\n",
      "    --model=$MODEL_TYPE | ## Run as a Python module\n",
      "\n",
      "In the previous notebook (mnist_linear.ipynb) we ran our code directly from the notebook.\n",
      "\n",
      "Now since we want to run our code on Cloud ML Engine, we've packaged it as a python module.\n",
      "\n",
      "The `model.py` and `task.py` containing the model code is in <a href=\"mnistmodel/trainer\">mnistmodel/trainer</a>\n",
      "\n",
      "**Let's first run it locally for a few steps to test the code.** \n",
      "{'input_ids': tensor([[    1, 11438, 28734,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])}\n",
      "%%bash\n",
      "OUTDIR=gs://${BUCKET}/mnist/trained_${MODEL_TYPE}\n",
      "JOBNAME=mnist_${MODEL_TYPE}_$(date -u +%y%m%d_%H%M%S)\n",
      "echo $OUTDIR $REGION $JOBNAME\n",
      "gsutil -m rm -rf $OUTDIR\n",
      "gcloud ml-engine jobs submit training $JOBNAME \\\n",
      "    --region=$REGION \\\n",
      "    --module-name=trainer.task \\\n",
      "    --package-path=${PWD}/mnistmodel/trainer \\\n",
      "    --job-dir=$OUTDIR \\\n",
      "    --staging-bucket=gs://$BUCKET \\\n",
      "    --scale-tier=BASIC_GPU \\\n",
      "    --runtime-version=$TFVERSION \\\n",
      "    -- \\\n",
      "    --output_dir=$OUTDIR \\\n",
      "    --train_steps=10000 --learning_rate=0.01 --train_batch_size=512 \\\n",
      "    --model=$MODEL_TYPE --batch_norm | **Now, let's do it on Cloud ML Engine so we can train on GPU (`--scale-tier=BASIC_GPU`)**\n",
      "\n",
      "Note the GPU speed up depends on the model type. You'll notice the more complex CNN model trains significantly faster on GPU, however the speed up on the simpler models is not as pronounced.\n",
      "{'input_ids': tensor([[    1, 11438, 28734,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])}\n",
      "from google.datalab.ml import TensorBoard\n",
      "TensorBoard().start(\"gs://{}/mnist/trained_{}\".format(BUCKET, MODEL_TYPE)) | ## Monitoring training with TensorBoard\n",
      "\n",
      "Use this cell to launch tensorboard\n",
      "{'input_ids': tensor([[   1, 2080, 5200,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])}\n",
      "for pid in TensorBoard.list()[\"pid\"]:\n",
      "    TensorBoard().stop(pid)\n",
      "    print(\"Stopped TensorBoard with pid {}\".format(pid)) | ## Monitoring training with TensorBoard\n",
      "\n",
      "Use this cell to launch tensorboard\n",
      "{'input_ids': tensor([[   1, 1884, 4231,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])}\n",
      "%%bash\n",
      "MODEL_NAME=\"mnist\"\n",
      "MODEL_VERSION=${MODEL_TYPE}\n",
      "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/mnist/trained_${MODEL_TYPE}/export/exporter | tail -1)\n",
      "echo \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\n",
      "#gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
      "#gcloud ml-engine models delete ${MODEL_NAME}\n",
      "gcloud ml-engine models create ${MODEL_NAME} --regions $REGION\n",
      "gcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version=$TFVERSION | ## Deploying and predicting with model\n",
      "\n",
      "Deploy the model:\n",
      "{'input_ids': tensor([[    1, 11438, 28734,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])}\n",
      "import json, codecs\n",
      "import matplotlib.pyplot as plt\n",
      "import tensorflow as tf\n",
      "\n",
      "HEIGHT = 28\n",
      "WIDTH = 28\n",
      "\n",
      "# Get mnist data\n",
      "mnist = tf.keras.datasets.mnist\n",
      "\n",
      "(_, _), (x_test, _) = mnist.load_data()\n",
      "\n",
      "# Scale our features between 0 and 1\n",
      "x_test = x_test / 255.0 \n",
      "\n",
      "IMGNO = 5 # CHANGE THIS to get different images\n",
      "jsondata = {\"image\": x_test[IMGNO].reshape(HEIGHT, WIDTH).tolist()}\n",
      "json.dump(jsondata, codecs.open(\"test.json\", 'w', encoding = \"utf-8\"))\n",
      "plt.imshow(x_test[IMGNO].reshape(HEIGHT, WIDTH)); | To predict with the model, let's take one of the example images.\n",
      "{'input_ids': tensor([[   1, 5666, 1163,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])}\n",
      "%%bash\n",
      "gcloud ml-engine predict \\\n",
      "    --model=mnist \\\n",
      "    --version=${MODEL_TYPE} \\\n",
      "    --json-instances=./test.json | Send it to the prediction service\n",
      "{'input_ids': tensor([[    1, 11438, 28734,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])}\n",
      "trainingInput:\n",
      "    scaleTier: CUSTOM\n",
      "    masterType: complex_model_m_gpu\n",
      "    hyperparameters:\n",
      "        goal: MAXIMIZE\n",
      "        maxTrials: 30\n",
      "        maxParallelTrials: 2\n",
      "        hyperparameterMetricTag: accuracy\n",
      "        params:\n",
      "        - parameterName: train_batch_size\n",
      "            type: INTEGER\n",
      "            minValue: 32\n",
      "            maxValue: 512\n",
      "            scaleType: UNIT_LINEAR_SCALE\n",
      "        - parameterName: learning_rate\n",
      "            type: DOUBLE\n",
      "            minValue: 0.001\n",
      "            maxValue: 0.1\n",
      "            scaleType: UNIT_LOG_SCALE\n",
      "        - parameterName: nfil1\n",
      "            type: INTEGER\n",
      "            minValue: 5\n",
      "            maxValue: 20\n",
      "            scaleType: UNIT_LINEAR_SCALE\n",
      "        - parameterName: nfil2\n",
      "            type: INTEGER\n",
      "            minValue: 10\n",
      "            maxValue: 30\n",
      "            scaleType: UNIT_LINEAR_SCALE\n",
      "        - parameterName: dprob\n",
      "            type: DOUBLE\n",
      "            minValue: 0.1\n",
      "            maxValue: 0.6\n",
      "            scaleType: UNIT_LINEAR_SCALE | ## DO NOT RUN anything beyond this point\n",
      "\n",
      "This shows you what I did, but trying to repeat this will take several hours.\n",
      "\n",
      "<br/>\n",
      "\n",
      "## Hyperparameter tuning\n",
      "This is what `hyperparam.yaml` looked like:\n",
      "{'input_ids': tensor([[    1, 17584,  1210,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "    print(data_sample.__getitem__(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "498e9417-1571-49a3-a17a-920c22abe567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "828122"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3076057f74959a6",
   "metadata": {
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1708979188006,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "d3076057f74959a6"
   },
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(df))\n",
    "val_size = len(df) - train_size\n",
    "\n",
    "train_data, val_data = random_split(data_sample, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7035655-b3d5-47eb-902c-6f53965310a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "662497"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "841cf089-ab23-439b-bc0f-7e9f86b1e362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165625"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cdf85e9219d0c770",
   "metadata": {
    "executionInfo": {
     "elapsed": 446,
     "status": "ok",
     "timestamp": 1708979191196,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "cdf85e9219d0c770"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ec827ef7ce67437",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1708979194656,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "8ec827ef7ce67437"
   },
   "outputs": [],
   "source": [
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e1368e8970c9ddf",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1708979197438,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "5e1368e8970c9ddf"
   },
   "outputs": [],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "model_name = 'sagard21/python-code'\n",
    "gpu = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36fe719fa83bf8f3",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1708979199183,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "36fe719fa83bf8f3"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = tokenizer.pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1e466ea1c2ecb44",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1708979201662,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "b1e466ea1c2ecb44"
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['epoch', 'transformer', 'batch_size', 'gpu', 'training_loss', 'validation_loss', 'epoch_duration_sec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf12ff96-584f-49c9-890d-713ba1670408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>transformer</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>gpu</th>\n",
       "      <th>training_loss</th>\n",
       "      <th>validation_loss</th>\n",
       "      <th>epoch_duration_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [epoch, transformer, batch_size, gpu, training_loss, validation_loss, epoch_duration_sec]\n",
       "Index: []"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "203b0284-a5a5-4c55-b3ba-31465078c232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "662497"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74985b11-5b97-4f40-87d0-553c6c1f47d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165625"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "967f9080-7c13-4793-95fa-bb6fe850c184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "828122"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data) + len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08ca6144-feb3-4063-b465-22b412705168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neuron_id = np.random.randint(0, config.rnn_dim)\n",
      "\n",
      "sentence_plot(some_test_sentences_contra_pad,\n",
      "              news_contra[:, :, neuron_id],\n",
      "              title=\"New candidate, neuron {} (only contradictory sentences)\".format(neuron_id)) | - ### New candidate for hidden state\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   1, 4644,  295,  ...,    2,    2,    2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "772fec25-9be1-44fc-8118-aac181d71466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "css_c = coord.SkyCoord(ra=css['RA']*u.degree, \n",
      "                       dec=css['Dec']*u.degree) | ## CSS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   1, 5212,   67,  ...,    2,    2,    2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "15503712-4372-4b8d-bc47-4aa553c8b0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#descriptives of missing\n",
      "vrs = ['retail','food_drugs','work','homes','pct_social_distancing']\n",
      "for x in vrs:\n",
      "    df_all['miss_{}'.format(x)] = np.where(df_all[x].isnull(),1,0)\n",
      "    print('\\nMissing Comparison for -{}\\n{}'.format(x,df_all.drop_duplicates(['FIPS','miss_{}'.format(x)]).groupby(['miss_{}'.format(x)])['POP_ESTIMATE_2018'].describe())) | # Models - data transformation\n",
      "Printing the 0 th row here: {'input_ids': tensor([[ 1,  7, 72,  ...,  2,  2,  2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])} \n",
      "\n",
      "#descriptives of missing\n",
      "vrs = ['retail','food_drugs','work','homes','pct_social_distancing']\n",
      "for x in vrs:\n",
      "    df_all['miss_{}'.format(x)] = np.where(df_all[x].isnull(),1,0)\n",
      "    print('\\nMissing Comparison for -{}\\n{}'.format(x,df_all.drop_duplicates(['FIPS','miss_{}'.format(x)]).groupby(['miss_{}'.format(x)])['POP_ESTIMATE_2018'].describe())) | # Models - data transformation\n",
      "Also printing the input_ids: 1024\n",
      "def logistic_regression(X, y, initializer=None, seed=42, learning_rate=0.01):\n",
      "    n_inputs_including_bias = int(X.get_shape()[1])\n",
      "    with tf.name_scope(\"logistic_regression\"):\n",
      "        with tf.name_scope(\"model\"):\n",
      "            if initializer is None:\n",
      "                initializer = tf.random_uniform([n_inputs_including_bias, 1], -1.0, 1.0, seed=seed)\n",
      "            theta = tf.Variable(initializer, name=\"theta\")\n",
      "            logits = tf.matmul(X, theta, name=\"logits\")\n",
      "            y_proba = tf.sigmoid(logits)\n",
      "        with tf.name_scope(\"train\"):\n",
      "            loss = tf.losses.log_loss(y, y_proba, scope=\"loss\")\n",
      "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
      "            training_op = optimizer.minimize(loss)\n",
      "            loss_summary = tf.summary.scalar('log_loss', loss)\n",
      "        with tf.name_scope(\"init\"):\n",
      "            init = tf.global_variables_initializer()\n",
      "        with tf.name_scope(\"save\"):\n",
      "            saver = tf.train.Saver()\n",
      "    return y_proba, loss, training_op, loss_summary, init, saver | Now let's define the `logistic_regression()` function to create the graph. We will leave out the definition of the inputs `X` and the targets `y`. We could include them here, but leaving them out will make it easier to use this function in a wide range of use cases (e.g. perhaps we will want to add some preprocessing steps for the inputs before we feed them to the Logistic Regression model).\n",
      "Printing the 1 th row here: {'input_ids': tensor([[  1, 536, 613,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])} \n",
      "\n",
      "def logistic_regression(X, y, initializer=None, seed=42, learning_rate=0.01):\n",
      "    n_inputs_including_bias = int(X.get_shape()[1])\n",
      "    with tf.name_scope(\"logistic_regression\"):\n",
      "        with tf.name_scope(\"model\"):\n",
      "            if initializer is None:\n",
      "                initializer = tf.random_uniform([n_inputs_including_bias, 1], -1.0, 1.0, seed=seed)\n",
      "            theta = tf.Variable(initializer, name=\"theta\")\n",
      "            logits = tf.matmul(X, theta, name=\"logits\")\n",
      "            y_proba = tf.sigmoid(logits)\n",
      "        with tf.name_scope(\"train\"):\n",
      "            loss = tf.losses.log_loss(y, y_proba, scope=\"loss\")\n",
      "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
      "            training_op = optimizer.minimize(loss)\n",
      "            loss_summary = tf.summary.scalar('log_loss', loss)\n",
      "        with tf.name_scope(\"init\"):\n",
      "            init = tf.global_variables_initializer()\n",
      "        with tf.name_scope(\"save\"):\n",
      "            saver = tf.train.Saver()\n",
      "    return y_proba, loss, training_op, loss_summary, init, saver | Now let's define the `logistic_regression()` function to create the graph. We will leave out the definition of the inputs `X` and the targets `y`. We could include them here, but leaving them out will make it easier to use this function in a wide range of use cases (e.g. perhaps we will want to add some preprocessing steps for the inputs before we feed them to the Logistic Regression model).\n",
      "Also printing the input_ids: 1024\n",
      "int_text = [i for i in range(20)]\n",
      "x,y = next(get_batches(int_text, batch_size=4, window_size=5))\n",
      "\n",
      "print('x\\n', x)\n",
      "print('y\\n', y) | ### Generating Batches \n",
      "\n",
      "Here's a generator function that returns batches of input and target data for our model, using the `get_target` function from above. The idea is that it grabs `batch_size` words from a words list. Then for each of those batches, it gets the target words in a window.\n",
      "Printing the 2 th row here: {'input_ids': tensor([[  1, 474,  67,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])} \n",
      "\n",
      "int_text = [i for i in range(20)]\n",
      "x,y = next(get_batches(int_text, batch_size=4, window_size=5))\n",
      "\n",
      "print('x\\n', x)\n",
      "print('y\\n', y) | ### Generating Batches \n",
      "\n",
      "Here's a generator function that returns batches of input and target data for our model, using the `get_target` function from above. The idea is that it grabs `batch_size` words from a words list. Then for each of those batches, it gets the target words in a window.\n",
      "Also printing the input_ids: 1024\n",
      "c =complex('5+2j')\n",
      "print(c)\n",
      "print(abs(c)) | **int( )** can also be used to get only the integer value of a float number or can be used to convert a number which is of type string to integer format. Similarly, the function **str( )** can be used to convert the integer back to string format\n",
      "Printing the 3 th row here: {'input_ids': tensor([[  1,  71, 273,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])} \n",
      "\n",
      "c =complex('5+2j')\n",
      "print(c)\n",
      "print(abs(c)) | **int( )** can also be used to get only the integer value of a float number or can be used to convert a number which is of type string to integer format. Similarly, the function **str( )** can be used to convert the integer back to string format\n",
      "Also printing the input_ids: 1024\n",
      "neuron_id = np.random.randint(0, config.rnn_dim)\n",
      "\n",
      "sentence_plot(some_test_sentences_contra_pad,\n",
      "              news_contra[:, :, neuron_id],\n",
      "              title=\"New candidate, neuron {} (only contradictory sentences)\".format(neuron_id)) | - ### New candidate for hidden state\n",
      "Printing the 4 th row here: {'input_ids': tensor([[   1, 4644,  295,  ...,    2,    2,    2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])} \n",
      "\n",
      "neuron_id = np.random.randint(0, config.rnn_dim)\n",
      "\n",
      "sentence_plot(some_test_sentences_contra_pad,\n",
      "              news_contra[:, :, neuron_id],\n",
      "              title=\"New candidate, neuron {} (only contradictory sentences)\".format(neuron_id)) | - ### New candidate for hidden state\n",
      "Also printing the input_ids: 1024\n",
      "n_neighbors = # TODO: Cantidad de vecinos a tener en cuenta\n",
      "metric =  # TODO: Medida de distancia. Algunas opciones: cosine, euclidean, manhattan.\n",
      "\n",
      "model = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric)\n",
      "model.fit(X_train_feature, y_train)\n",
      "\n",
      "print('Exactitud para entrenamiento: %.2f' %  accuracy_score(y_train, model.predict(X_train_feature)))\n",
      "print('Exactitud para validacin: %.2f' % accuracy_score(y_val, model.predict(X_val_feature))) | ### Vecinos ms cercanos\n",
      "\n",
      "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
      "Printing the 5 th row here: {'input_ids': tensor([[ 1, 82, 67,  ...,  2,  2,  2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])} \n",
      "\n",
      "n_neighbors = # TODO: Cantidad de vecinos a tener en cuenta\n",
      "metric =  # TODO: Medida de distancia. Algunas opciones: cosine, euclidean, manhattan.\n",
      "\n",
      "model = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric)\n",
      "model.fit(X_train_feature, y_train)\n",
      "\n",
      "print('Exactitud para entrenamiento: %.2f' %  accuracy_score(y_train, model.predict(X_train_feature)))\n",
      "print('Exactitud para validacin: %.2f' % accuracy_score(y_val, model.predict(X_val_feature))) | ### Vecinos ms cercanos\n",
      "\n",
      "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
      "Also printing the input_ids: 1024\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"aapl_no_dates.csv\")            # Dataset with no date\n",
      "df.head() | # Date_Range\n",
      "Printing the 6 th row here: {'input_ids': tensor([[    1,  5666, 12037,  ...,     2,     2,     2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])} \n",
      "\n",
      "import pandas as pd\n",
      "df = pd.read_csv(\"aapl_no_dates.csv\")            # Dataset with no date\n",
      "df.head() | # Date_Range\n",
      "Also printing the input_ids: 1024\n",
      "tips.sex.str.lower().str[0].head() | #### String operations\n",
      "Printing the 7 th row here: {'input_ids': tensor([[   1,   88, 7146,  ...,    2,    2,    2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])} \n",
      "\n",
      "tips.sex.str.lower().str[0].head() | #### String operations\n",
      "Also printing the input_ids: 1024\n",
      "# Create pandas dataframe from june_temps_list\n",
      "june_df = pd.DataFrame(june_temps_list, columns=['temps'])\n",
      "june_df.describe() | # Challenge\n",
      "Printing the 8 th row here: {'input_ids': tensor([[   1,    7, 1788,  ...,    2,    2,    2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])} \n",
      "\n",
      "# Create pandas dataframe from june_temps_list\n",
      "june_df = pd.DataFrame(june_temps_list, columns=['temps'])\n",
      "june_df.describe() | # Challenge\n",
      "Also printing the input_ids: 1024\n",
      "print(saludar_py.r_repr()) | # Juntando R y Python\n",
      "Printing the 9 th row here: {'input_ids': tensor([[   1, 1188,   12,  ...,    2,    2,    2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])} \n",
      "\n",
      "print(saludar_py.r_repr()) | # Juntando R y Python\n",
      "Also printing the input_ids: 1024\n",
      "env.action_space.low | You can find the meaning of each of these 24 numbers in the [documentation](https://github.com/openai/gym/wiki/BipedalWalker-v2).\n",
      "Printing the 10 th row here: {'input_ids': tensor([[   1, 3074,   18,  ...,    2,    2,    2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])} \n",
      "\n",
      "env.action_space.low | You can find the meaning of each of these 24 numbers in the [documentation](https://github.com/openai/gym/wiki/BipedalWalker-v2).\n",
      "Also printing the input_ids: 1024\n",
      "%reload_ext autoreload\n",
      "%autoreload 2\n",
      "%matplotlib inline | ## Planet Kaggle competition\n",
      "Printing the 11 th row here: {'input_ids': tensor([[    1,     9, 17517,  ...,     2,     2,     2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])} \n",
      "\n",
      "%reload_ext autoreload\n",
      "%autoreload 2\n",
      "%matplotlib inline | ## Planet Kaggle competition\n",
      "Also printing the input_ids: 1024\n",
      "# RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES\n",
      "import requests\n",
      "from IPython.core.display import HTML\n",
      "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\").text\n",
      "HTML(styles) | # <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS-109B Introduction to Data Science\n",
      "## Lab 5: Convolutional Neural Networks\n",
      "\n",
      "**Harvard University**<br>\n",
      "**Spring 2019**<br>\n",
      "**Lab instructor:** Eleni Kaxiras<br>\n",
      "**Instructors:** Pavlos Protopapas and Mark Glickman<br>\n",
      "**Authors:** Eleni Kaxiras, Pavlos Protopapas, Patrick Ohiomoba, and Davis Sontag\n",
      "Printing the 12 th row here: {'input_ids': tensor([[    1,     7, 15484,  ...,     2,     2,     2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])} \n",
      "\n",
      "# RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES\n",
      "import requests\n",
      "from IPython.core.display import HTML\n",
      "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\").text\n",
      "HTML(styles) | # <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS-109B Introduction to Data Science\n",
      "## Lab 5: Convolutional Neural Networks\n",
      "\n",
      "**Harvard University**<br>\n",
      "**Spring 2019**<br>\n",
      "**Lab instructor:** Eleni Kaxiras<br>\n",
      "**Instructors:** Pavlos Protopapas and Mark Glickman<br>\n",
      "**Authors:** Eleni Kaxiras, Pavlos Protopapas, Patrick Ohiomoba, and Davis Sontag\n",
      "Also printing the input_ids: 1024\n",
      "predictions = model.predict(input_fn=predict_input_fn) | ### Predictions\n",
      "\n",
      "The trained model can also be used to make predictions on new data.\n",
      "\n",
      "Note that the TensorFlow graph is recreated and the checkpoint is reloaded every time we make predictions on new data. If the model is very large then this could add a significant overhead.\n",
      "\n",
      "It is unclear why the Estimator is designed this way, possibly because it will always use the latest checkpoint and it can also be distributed easily for use on multiple computers.\n",
      "Printing the 13 th row here: {'input_ids': tensor([[    1, 25499,   273,  ...,     2,     2,     2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])} \n",
      "\n",
      "predictions = model.predict(input_fn=predict_input_fn) | ### Predictions\n",
      "\n",
      "The trained model can also be used to make predictions on new data.\n",
      "\n",
      "Note that the TensorFlow graph is recreated and the checkpoint is reloaded every time we make predictions on new data. If the model is very large then this could add a significant overhead.\n",
      "\n",
      "It is unclear why the Estimator is designed this way, possibly because it will always use the latest checkpoint and it can also be distributed easily for use on multiple computers.\n",
      "Also printing the input_ids: 1024\n",
      "l = [1, 2, '3', 4, 5]\n",
      "list(squares(l, 10)) | As you can see that exception was handled just like the index exception. That's probably not what I want - so it would be much better to write it this way:\n",
      "Printing the 14 th row here: {'input_ids': tensor([[  1,  80, 273,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])} \n",
      "\n",
      "l = [1, 2, '3', 4, 5]\n",
      "list(squares(l, 10)) | As you can see that exception was handled just like the index exception. That's probably not what I want - so it would be much better to write it this way:\n",
      "Also printing the input_ids: 1024\n",
      "from sklearn.model_selection import cross_val_score\n",
      "\n",
      "scores = cross_val_score(model, X_full, y_full, cv=5)\n",
      "scores | - Status primarily available after 2m and 3yr, looks like it would be most useful to consider these two points in time\n",
      "Printing the 15 th row here: {'input_ids': tensor([[   1, 2080, 4343,  ...,    2,    2,    2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])} \n",
      "\n",
      "from sklearn.model_selection import cross_val_score\n",
      "\n",
      "scores = cross_val_score(model, X_full, y_full, cv=5)\n",
      "scores | - Status primarily available after 2m and 3yr, looks like it would be most useful to consider these two points in time\n",
      "Also printing the input_ids: 1024\n",
      "learn.fit_flat_cos(16, lr=slice(3e-4)) | \n",
      "Printing the 16 th row here: {'input_ids': tensor([[    1, 21346,    18,  ...,     2,     2,     2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])} \n",
      "\n",
      "learn.fit_flat_cos(16, lr=slice(3e-4)) | \n",
      "Also printing the input_ids: 1024\n",
      "batch_size = 64\n",
      "\n",
      "data_size = data_train.data.shape[0]\n",
      "validation_split = .2\n",
      "split = int(np.floor(validation_split * data_size))\n",
      "indices = list(range(data_size))\n",
      "np.random.shuffle(indices)\n",
      "\n",
      "train_indices, val_indices = indices[split:], indices[:split]\n",
      "\n",
      "train_sampler = SubsetRandomSampler(train_indices)\n",
      "val_sampler = SubsetRandomSampler(val_indices)\n",
      "\n",
      "train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, \n",
      "                                           sampler=train_sampler)\n",
      "val_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size,\n",
      "                                         sampler=val_sampler) |    training  validation.\n",
      "\n",
      "     - https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
      "Printing the 17 th row here: {'input_ids': tensor([[   1, 5303,   67,  ...,    2,    2,    2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])} \n",
      "\n",
      "batch_size = 64\n",
      "\n",
      "data_size = data_train.data.shape[0]\n",
      "validation_split = .2\n",
      "split = int(np.floor(validation_split * data_size))\n",
      "indices = list(range(data_size))\n",
      "np.random.shuffle(indices)\n",
      "\n",
      "train_indices, val_indices = indices[split:], indices[:split]\n",
      "\n",
      "train_sampler = SubsetRandomSampler(train_indices)\n",
      "val_sampler = SubsetRandomSampler(val_indices)\n",
      "\n",
      "train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, \n",
      "                                           sampler=train_sampler)\n",
      "val_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size,\n",
      "                                         sampler=val_sampler) |    training  validation.\n",
      "\n",
      "     - https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
      "Also printing the input_ids: 1024\n",
      "tokens_pad = pad_sequences(tokens, maxlen=max_tokens,\n",
      "                           padding=pad, truncating=pad)\n",
      "tokens_pad.shape | To input texts with different lengths into the model, we also need to pad and truncate them.\n",
      "Printing the 18 th row here: {'input_ids': tensor([[   1, 7860,   67,  ...,    2,    2,    2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])} \n",
      "\n",
      "tokens_pad = pad_sequences(tokens, maxlen=max_tokens,\n",
      "                           padding=pad, truncating=pad)\n",
      "tokens_pad.shape | To input texts with different lengths into the model, we also need to pad and truncate them.\n",
      "Also printing the input_ids: 1024\n",
      "name_gen = 'image_gen'\n",
      "path_gen = path/name_gen | ## Save generated images\n",
      "Printing the 19 th row here: {'input_ids': tensor([[  1, 529,  67,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])} \n",
      "\n",
      "name_gen = 'image_gen'\n",
      "path_gen = path/name_gen | ## Save generated images\n",
      "Also printing the input_ids: 1024\n"
     ]
    }
   ],
   "source": [
    "for idx in range(20):\n",
    "    print(\"Printing the\", idx,\"th row here:\", train_data[idx],\"\\n\")\n",
    "    print(\"Also printing the input_ids:\", train_data[idx]['input_ids'].numpy().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "165fe330-f8bd-4528-b069-fc222b4b6ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Model\n",
    "\n",
    "class CustomT5Model(T5Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None):\n",
    "        # Custom logic for handling input_ids with length 1024\n",
    "        if input_ids.size(1) > 1024:\n",
    "            # Truncate or process input_ids as needed\n",
    "            input_ids = input_ids[:, :1024]\n",
    "        \n",
    "        # Call the base T5Model's forward method with modified input_ids\n",
    "        outputs = super().forward(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, decoder_attention_mask=decoder_attention_mask)\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "keRPegz17wTK",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "aborted",
     "timestamp": 1708979069963,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "keRPegz17wTK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 12:04:40.694887: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-03 12:04:41.252607: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-03 12:04:41.253757: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-03 12:04:41.341498: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-03 12:04:41.521426: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-03 12:04:42.874054: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from sklearn.utils import shuffle\n",
      "\n",
      "def prepare_imdb_data(data, labels):\n",
      "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
      "    \n",
      "    #Combine positive and negative reviews and labels\n",
      "    data_train = data['train']['pos'] + data['train']['neg']\n",
      "    data_test = data['test']['pos'] + data['test']['neg']\n",
      "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
      "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
      "    \n",
      "    #Shuffle reviews and corresponding labels within training and test sets\n",
      "    data_train, labels_train = shuffle(data_train, labels_train)\n",
      "    data_test, labels_test = shuffle(data_test, labels_test)\n",
      "    \n",
      "    # Return a unified training data, test data, training labels, test labets\n",
      "    return data_train, data_test, labels_train, labels_test | Now that we've read the raw training and testing data from the downloaded dataset, we will combine the positive and negative reviews and shuffle the resulting records.\n",
      "df.drop(['Name','Ticket'],axis=1,inplace=True)\n",
      "df.head(2) | ## Not Considering Name And Ticket\n",
      "None | # Relu_vs_Selu_Celu_Mish_1\n",
      "\n",
      "> compare Relu_vs_Selu_Celu_Mish_1\n",
      "with plt.rc_context(dict(sns.axes_style(\"darkgrid\"),\n",
      "                         **sns.plotting_context(\"notebook\", font_scale=1.5))):\n",
      "    \n",
      "    plt.figure(figsize=(12, 8))\n",
      "    ax = sns.violinplot(x=\"Run\", y=\"MeanAlpha\", data=per_1_md.sort_values(\"Run\"), palette= [colors[e] for e in [99, 0, 50]])\n",
      "    ax.set_xticklabels(['Run 1', 'Run 3', 'Run 4']) | #Alpha diversity plots\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 21\u001b[0m\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      5\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1869\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1866\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1869\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1872\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1873\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1874\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1875\u001b[0m ):\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1877\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2768\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2765\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2767\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2768\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2771\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2791\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2789\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2790\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2791\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2792\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2793\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/utils/operations.py:687\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/utils/operations.py:675\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/peft_model.py:1129\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1128\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1140\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1142\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:161\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:1706\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# Encode if needed (training, first prediction pass)\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1705\u001b[0m     \u001b[38;5;66;03m# Convert encoder inputs in embeddings if needed\u001b[39;00m\n\u001b[0;32m-> 1706\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n\u001b[1;32m   1716\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1717\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1718\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1719\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1720\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:1018\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to initialize the model with valid token embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1016\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens(input_ids)\n\u001b[0;32m-> 1018\u001b[0m batch_size, seq_length \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;66;03m# required mask seq length can be calculated via length of past\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m mask_seq_length \u001b[38;5;241m=\u001b[39m past_key_values[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m seq_length \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m seq_length\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        warmup_steps=2,\n",
    "        max_steps=2000,\n",
    "        learning_rate=5e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = True  # silence the warnings. Please re-enable for inference!\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "76da2829-1392-4dfc-9523-60c131c6a106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#descriptives of missing\n",
      "vrs = ['retail','food_drugs','work','homes','pct_social_distancing']\n",
      "for x in vrs:\n",
      "    df_all['miss_{}'.format(x)] = np.where(df_all[x].isnull(),1,0)\n",
      "    print('\\nMissing Comparison for -{}\\n{}'.format(x,df_all.drop_duplicates(['FIPS','miss_{}'.format(x)]).groupby(['miss_{}'.format(x)])['POP_ESTIMATE_2018'].describe())) | # Models - data transformation\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m train_data:\n\u001b[0;32m----> 4\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_data:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for val_inputs, val_targets in val_loader:\n",
    "            val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss += criterion(val_outputs, val_targets).item()\n",
    "\n",
    "            _, predicted = torch.max(val_outputs, 1)\n",
    "            correct_predictions += (predicted == val_targets).sum().item()\n",
    "            total_samples += val_targets.size(0)\n",
    "\n",
    "        val_accuracy = correct_predictions / total_samples\n",
    "        average_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {average_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fcc869e0-965e-4dcc-8cb3-dc667a1e653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output_dir\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    logging_dir=\"./logs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e309af-6d58-46c4-964b-67520293cc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,  # Your training dataset\n",
    "    eval_dataset=val_data,     # Your validation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eff2ed-7134-4392-93ac-991b8fd3973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a497c22d85b57713",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "error",
     "timestamp": 1708979203179,
     "user": {
      "displayName": "Abhilash Kandarpa",
      "userId": "06086554660814846667"
     },
     "user_tz": 0
    },
    "id": "a497c22d85b57713",
    "outputId": "d7dd6846-cf34-4012-952b-8ef5f82f48d8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch1/5 Batch size: 9, Transformer: sagard21/python-code:   0%| | 0/73"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "import os | # Talks markdown generator for academicpages\n",
      "\n",
      "Takes a TSV of talks with metadata and converts them for use with [academicpages.github.io](academicpages.github.io). This is an interactive Jupyter notebook ([see more info here](http://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html)). The core python code is also in `talks.py`. Run either from the `markdown_generator` folder after replacing `talks.tsv` with one containing your data.\n",
      "\n",
      "TODO: Make this work with BibTex and other databases, rather than Stuart's non-standard TSV format and citation style.\n",
      "//export\n",
      "import Path\n",
      "import TensorFlow\n",
      "import Python | # A CNN Mnist Model\n",
      "import seaborn as sns\n",
      "sns.set(context='talk', style='ticks', font_scale=1.6)\n",
      "%matplotlib inline | # LSLGA/DR8 QA\n",
      "model_2.score(Xlin, y) | We can check the $ R^2 $ model score. By linearizing the normalised ENTHRU energy, we have improved our $ R^2 $ score and are thus fitting a model which better describes our data.\n",
      "#@test {\"timeout\": 90}\n",
      "with context.eager_mode():\n",
      "  durations = []\n",
      "  for t in range(burn_ins + trials):\n",
      "    hp = tf.contrib.training.HParams(\n",
      "        learning_rate=0.05,\n",
      "        max_steps=max_steps,\n",
      "    )\n",
      "    train_ds = setup_mnist_data(True, hp, 500)\n",
      "    test_ds = setup_mnist_data(False, hp, 100)\n",
      "    ds = tf.data.Dataset.zip((train_ds, test_ds))\n",
      "    start = time.time()\n",
      "    (train_losses, test_losses, train_accuracies,\n",
      "     test_accuracies) = train(ds, hp)\n",
      "    \n",
      "    train_losses[-1].numpy()\n",
      "    test_losses[-1].numpy()\n",
      "    train_accuracies[-1].numpy()\n",
      "    test_accuracies[-1].numpy()\n",
      "\n",
      "    if t < burn_ins:\n",
      "      continue\n",
      "\n",
      "    duration = time.time() - start\n",
      "    durations.append(duration)\n",
      "    print('Duration:', duration)\n",
      "\n",
      "  print('Mean duration:', np.mean(durations), '+/-', np.std(durations))\n",
      " | # Eager\n",
      "import datetime\n",
      "\n",
      "start_time = int(datetime.datetime.utcnow().timestamp() * 1000)\n",
      "\n",
      "route_layer = network.RouteLayer(route_service_url, gis=my_gis)\n",
      "result = route_layer.solve(stops=stops, \n",
      "                           directions_language='en-US', return_routes=False,\n",
      "                           return_stops=False, return_directions=True,\n",
      "                           directions_length_units='esriNAUKilometers',\n",
      "                           return_barriers=False, return_polygon_barriers=False,\n",
      "                           return_polyline_barriers=False, start_time=start_time,\n",
      "                           start_time_is_utc=True) | As the ArcGIS Online routing service is traffic aware, one can specify the start time of the departure to get the most accurate travel time. In this example, we will pass as the start time the current time (UTC).\n",
      "facebook.network.q\n",
      " | # Activity #2: Networked data\n",
      "\n",
      "estimator = LinearRegression()\n",
      "estimator.fit(X_train, y_train)\n",
      "\n",
      "plot_regression(estimator, X_train, y_train, X_test, y_test)\n",
      "\n",
      "print('MSE on train data :', round(mean_squared_error(y_train, estimator.predict(X_train)), 2))\n",
      "print('MSE on test data :', round(mean_squared_error(y_test, estimator.predict(X_test)), 2))\n",
      "print('R2-score :', round(r2_score(y_test, estimator.predict(X_test)), 2)) | # Predictions\n",
      "import mimetypes\n",
      "import re\n",
      "import urllib\n",
      " \n",
      " \n",
      "MIMETYPE_REGEX = r'[\\w]+\\/[\\w\\-\\+\\.]+'\n",
      "_MIMETYPE_RE = re.compile('^{}$'.format(MIMETYPE_REGEX))\n",
      " \n",
      "CHARSET_REGEX = r'[\\w\\-\\+\\.]+'\n",
      "_CHARSET_RE = re.compile('^{}$'.format(CHARSET_REGEX))\n",
      " \n",
      "DATA_URI_REGEX = (\n",
      "    r'data:' +\n",
      "    r'(?P<mimetype>{})?'.format(MIMETYPE_REGEX) +\n",
      "    r'(?:\\;charset\\=(?P<charset>{}))?'.format(CHARSET_REGEX) +\n",
      "    r'(?P<base64>\\;base64)?' +\n",
      "    r',(?P<data>.*)')\n",
      "_DATA_URI_RE = re.compile(r'^{}$'.format(DATA_URI_REGEX), re.DOTALL)\n",
      " \n",
      " \n",
      "class DataURI(str):\n",
      " \n",
      "    @classmethod\n",
      "    def make(cls, mimetype, charset, base64, data):\n",
      "        parts = ['data:']\n",
      "        if mimetype is not None:\n",
      "            if not _MIMETYPE_RE.match(mimetype):\n",
      "                raise ValueError(\"Invalid mimetype: %r\" % mimetype)\n",
      "            parts.append(mimetype)\n",
      "        if charset is not None:\n",
      "            if not _CHARSET_RE.match(charset):\n",
      "                raise ValueError(\"Invalid charset: %r\" % charset)\n",
      "            parts.extend([';charset=', charset])\n",
      "        if base64:\n",
      "            parts.append(';base64')\n",
      "            encoded_data = data.encode('base64').replace('\\n', '')\n",
      "        else:\n",
      "            encoded_data = urllib.quote(data)\n",
      "        parts.extend([',', encoded_data])\n",
      "        return cls(''.join(parts))\n",
      " \n",
      "    @classmethod\n",
      "    def from_file(cls, filename, charset=None, base64=True):\n",
      "        mimetype, _ = mimetypes.guess_type(filename, strict=False)\n",
      "        with open(filename) as fp:\n",
      "            data = fp.read()\n",
      "        return cls.make(mimetype, charset, base64, data)\n",
      " \n",
      "    def __new__(cls, *args, **kwargs):\n",
      "        uri = super(DataURI, cls).__new__(cls, *args, **kwargs)\n",
      "        uri._parse  # Trigger any ValueErrors on instantiation.\n",
      "        return uri\n",
      " \n",
      "    def __repr__(self):\n",
      "        return 'DataURI(%s)' % (super(DataURI, self).__repr__(),)\n",
      " \n",
      "    def wrap(self, width=76):\n",
      "        return type(self)('\\n'.join(textwrap.wrap(self, width)))\n",
      " \n",
      "    @property\n",
      "    def mimetype(self):\n",
      "        return self._parse[0]\n",
      " \n",
      "    @property\n",
      "    def charset(self):\n",
      "        return self._parse[1]\n",
      " \n",
      "    @property\n",
      "    def is_base64(self):\n",
      "        return self._parse[2]\n",
      " \n",
      "    @property\n",
      "    def data(self):\n",
      "        return self._parse[3]\n",
      " \n",
      "    @property\n",
      "    def _parse(self):\n",
      "        match = _DATA_URI_RE.match(self)\n",
      "        if not match:\n",
      "            raise ValueError(\"Not a valid data URI: %r\" % self)\n",
      "        mimetype = match.group('mimetype') or None\n",
      "        charset = match.group('charset') or None\n",
      "        if match.group('base64'):\n",
      "            data = match.group('data').decode('base64')\n",
      "        else:\n",
      "            data = urllib.unquote(match.group('data'))\n",
      "        return mimetype, charset, bool(match.group('base64')), data | This button is just hiding the code, so non-programmers are not scared away by some lines of code.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch1/5 Batch size: 9, Transformer: sagard21/python-code:   0%| | 0/73\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacity of 15.81 GiB of which 475.38 MiB is free. Including non-PyTorch memory, this process has 11.73 GiB memory in use. Process 5718 has 3.21 GiB memory in use. Of the allocated memory 10.95 GiB is allocated by PyTorch, and 144.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids \u001b[38;5;241m=\u001b[39m inputs, labels \u001b[38;5;241m=\u001b[39m targets)\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     15\u001b[0m train_iterator\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem()})\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/function.py:289\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:302\u001b[0m, in \u001b[0;36mCheckpointFunction.backward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    297\u001b[0m     device_autocast_ctx \u001b[38;5;241m=\u001b[39m device_module\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mctx\u001b[38;5;241m.\u001b[39mdevice_autocast_kwargs\n\u001b[1;32m    299\u001b[0m     ) \u001b[38;5;28;01mif\u001b[39;00m _supports_autocast(ctx\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext()\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad(), device_autocast_ctx, \\\n\u001b[1;32m    301\u001b[0m          torch\u001b[38;5;241m.\u001b[39mcpu\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mctx\u001b[38;5;241m.\u001b[39mcpu_autocast_kwargs):\n\u001b[0;32m--> 302\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdetached_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    305\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (outputs,)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:724\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    722\u001b[0m     query_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 724\u001b[0m cross_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    737\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:635\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    624\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    632\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    633\u001b[0m ):\n\u001b[1;32m    634\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 635\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEncDecAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    646\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    647\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py:561\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    558\u001b[0m     position_bias_masked \u001b[38;5;241m=\u001b[39m position_bias\n\u001b[1;32m    560\u001b[0m scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_bias_masked\n\u001b[0;32m--> 561\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtype_as(\n\u001b[1;32m    562\u001b[0m     scores\n\u001b[1;32m    563\u001b[0m )  \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[1;32m    564\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(\n\u001b[1;32m    565\u001b[0m     attn_weights, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m    566\u001b[0m )  \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# Mask heads if we want to\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:1858\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1856\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1858\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1860\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacity of 15.81 GiB of which 475.38 MiB is free. Including non-PyTorch memory, this process has 11.73 GiB memory in use. Process 5718 has 3.21 GiB memory in use. Of the allocated memory 10.95 GiB is allocated by PyTorch, and 144.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    epoch_training_loss = 0\n",
    "    train_iterator = tqdm(train_loader, desc=f\"Training epoch{epoch+1}/{num_epochs} Batch size: {batch_size}, Transformer: {model_name}\")\n",
    "    for batch in train_iterator:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = batch['input_ids'].squeeze(1).to(device)\n",
    "        targets = inputs.clone()\n",
    "        outputs = model(input_ids = inputs, labels = targets)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_iterator.set_postfix({'Training Loss': loss.item()})\n",
    "        epoch_training_loss += loss.item()\n",
    "        torch.cuda.empty_cache()\n",
    "    avg_epoch_training_loss = epoch_training_loss / len(train_iterator)\n",
    "\n",
    "    #validation\n",
    "    model.eval()\n",
    "    epoch_validation_loss = 0\n",
    "    total_loss = 0\n",
    "    valid_iterator = tqdm(val_loader, desc=f\"Validating epoch {epoch+1}/{num_epochs}\")\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_iterator:\n",
    "            inputs = batch['input_ids'].squeeze(1).to(device)\n",
    "            targets = inputs.clone()\n",
    "            outputs = model(input_ids=inputs, labels=targets)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss\n",
    "            valid_iterator.set_postfix({'Validation Loss': loss.item()})\n",
    "            epoch_validation_loss += loss.item()\n",
    "\n",
    "    avg_epoch_validation_loss = epoch_validation_loss / len(valid_iterator)\n",
    "\n",
    "    end_time = time.time()  # End the timer for the epoch\n",
    "    epoch_duration_sec = end_time - start_time  # Calculate the duration in seconds\n",
    "\n",
    "    new_row = {'transformer': model_name,\n",
    "               'batch_size': batch_size,\n",
    "               'gpu': gpu,\n",
    "               'epoch': epoch+1,\n",
    "               'training_loss': avg_epoch_training_loss,\n",
    "               'validation_loss': avg_epoch_validation_loss,\n",
    "               'epoch_duration_sec': epoch_duration_sec}  # Add epoch_duration to the dataframe\n",
    "\n",
    "    results.loc[len(results)] = new_row\n",
    "    print(f\"Epoch: {epoch+1}, Validation Loss: {total_loss/len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9a94a239-17ed-4b5d-ad8e-8f3c1347aece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tqdm.std.tqdm at 0x7f0899f07f10>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cd228309-29e8-4ae0-84f7-7ee742672aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch1/5 Batch size: 9, Transformer: sagard21/python-code:   0%| | 0/73"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Plotting the confusion matrix on a heat map\n",
      "\n",
      "plot_confusion_matrix(confusion_matrix(y_test, y_pred), figsize = (15,12))\n",
      "plt.ylabel('True Label', fontsize = 20)\n",
      "plt.xlabel('Predicted Label',  fontsize = 20)\n",
      "plt.title('Confusion Matrix Heat Map', fontsize = 20)\n",
      "plt.show() | ### Adding another RNN layer\n",
      "print (\"test: \" + test) | ## About iPython Notebooks ##\n",
      "\n",
      "iPython Notebooks are interactive coding environments embedded in a webpage. You will be using iPython notebooks in this class. You only need to write code between the ### START CODE HERE ### and ### END CODE HERE ### comments. After writing your code, you can run the cell by either pressing \"SHIFT\"+\"ENTER\" or by clicking on \"Run Cell\" (denoted by a play symbol) in the upper bar of the notebook. \n",
      "\n",
      "We will often specify \"( X lines of code)\" in the comments to tell you about how much code you need to write. It is just a rough estimate, so don't feel bad if your code is longer or shorter.\n",
      "\n",
      "**Exercise**: Set test to `\"Hello World\"` in the cell below to print \"Hello World\" and run the two cells below.\n",
      "challenge_output = 'extra.mp4'\n",
      "clip2 = VideoFileClip('challenge.mp4')\n",
      "challenge_clip = clip2.fl_image(process_image)\n",
      "%time challenge_clip.write_videofile(challenge_output, audio=False) | ## Optional Challenge\n",
      "\n",
      "Try your lane finding pipeline on the video below.  Does it still work?  Can you figure out a way to make it more robust?  If you're up for the challenge, modify your pipeline so it works with this video and submit it along with the rest of your project!\n",
      "tmp = T.jacobian(dq)\n",
      "H = tmp.jacobian(dq)\n",
      "C = tmp.jacobian(q) * dq - T.jacobian(q).T\n",
      "G = U.jacobian(q).T | # Equation of motion\n",
      "def plot_history(histories, key='binary_crossentropy'):\n",
      "  plt.figure(figsize=(16,10))\n",
      "    \n",
      "  for name, history in histories:\n",
      "    val = plt.plot(history.epoch, history.history['val_'+key],\n",
      "                   '--', label=name.title()+' Val')\n",
      "    plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
      "             label=name.title()+' Train')\n",
      "\n",
      "  plt.xlabel('Epochs')\n",
      "  plt.ylabel(key.replace('_',' ').title())\n",
      "  plt.legend()\n",
      "\n",
      "  plt.xlim([0,max(history.epoch)])\n",
      "\n",
      "\n",
      "plot_history([('baseline', baseline_history),\n",
      "              ('smaller', smaller_history),\n",
      "              ('bigger', bigger_history)]) | The solid lines show the training loss, and the dashed lines show the validation loss (remember: a lower validation loss indicates a better model). Here, the smaller network begins overfitting later than the baseline model (after 6 epochs rather than 4) and its performance degrades much more slowly once it starts overfitting. \n",
      "loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)\n",
      "\n",
      "print(\"Testing set Mean Abs Error: {:5.2f} MPG\".format(mae)) |  +/- 2 MPG  \n",
      "\n",
      " **** \n",
      "# archivo yearly-data-with-zone.txt es generado por compute_statistics.py\n",
      "df_22 = pd.read_csv(\"./yearly-data-with-natural-zone.txt\", sep=\"\\t\", header=None, names=[\"code\", \"year\", \"precip\", \"zone\"])\n",
      "df_22 = df_22.sort_values(by=[\"zone\", \"year\"])\n",
      "df_22.head() | # plots para division por zona natural\n",
      "# Second element\n",
      "tup[1] | And indexing tuples is exactly like indexing lists\n",
      "None | # Now You Code 2: IP Addresses\n",
      "\n",
      "For this Now You Code, you will complete a very common task in data analytics: converting an IP address to an approximate location.\n",
      "\n",
      "Write a program to read the IP Addresses from the File `NYC2-IP-Addresses.txt` and for each IP address determine the approximate location (City and State) for the origin of that IP Address. This is usually done as part of analytics to determine the origins of website visitors. \n",
      "\n",
      "To perform the lookups, use the http://freegeoip.net API. You'll have to read through the API documentation first and understand how to use the API before you write the entire program.  \n",
      "\n",
      "As a first step I highly recommed that you get just the IP lookup working, perhaps writing it as a function,  and then try to read from the file and perform the lookups for each IP address in the file.\n",
      "\n",
      "Here's a sample of a geoip lookup of the IP Address `'128.230.182.217'`\n",
      "\n",
      "```\n",
      "{'city': 'Syracuse',\n",
      " 'country_code': 'US',\n",
      " 'country_name': 'United States',\n",
      " 'ip': '128.230.182.217',\n",
      " 'latitude': 43.0377,\n",
      " 'longitude': -76.1396,\n",
      " 'metro_code': 555,\n",
      " 'region_code': 'NY',\n",
      " 'region_name': 'New York',\n",
      " 'time_zone': 'America/New_York',\n",
      " 'zip_code': '13244'}\n",
      "```\n",
      "\n",
      "In this example the city and state would be `Syracuse, NY`\n",
      "\n",
      "\n",
      "Final Program Output:\n",
      "\n",
      "```\n",
      "IP: 128.122.140.238 LOCATION: New York,NY\n",
      "IP: 23.112.232.160 LOCATION: Green Bay,WI\n",
      "IP: 23.192.215.44 LOCATION: Cambridge,MA\n",
      "IP: 23.224.160.4 LOCATION: Cheyenne,WY\n",
      "IP: 23.230.12.5 LOCATION: San Jose,CA\n",
      "IP: 23.80.125.101 LOCATION: Phoenix,AZ\n",
      "IP: 23.83.132.200 LOCATION: Phoenix,AZ\n",
      "IP: 23.88.15.5 LOCATION: Los Angeles,CA\n",
      "IP: 24.0.14.56 LOCATION: Iselin,NJ\n",
      "IP: 24.1.25.140 LOCATION: Chicago,IL\n",
      "IP: 24.11.125.10 LOCATION: Orem,UT\n",
      "IP: 24.38.114.105 LOCATION: Matawan,NJ\n",
      "IP: 24.38.224.161 LOCATION: Darien,CT\n",
      "IP: 56.216.127.219 LOCATION: Raleigh,NC\n",
      "IP: 68.199.40.156 LOCATION: Elmont,NY\n",
      "IP: 74.111.18.59 LOCATION: Auburn,NY\n",
      "IP: 74.111.6.173 LOCATION: Liverpool,NY\n",
      "IP: 98.29.25.44 LOCATION: Dayton,OH\n",
      "```\n",
      "\n",
      "{'input_ids': tensor([[[    1,     7, 15211,  ...,     2,     2,     2]],\n",
      "\n",
      "        [[    1,  1188,  7566,  ...,     2,     2,     2]],\n",
      "\n",
      "        [[    1, 25092,    67,  ...,     2,     2,     2]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    1,     7,  6637,  ...,     2,     2,     2]],\n",
      "\n",
      "        [[    1,     7,  7631,  ...,     2,     2,     2]],\n",
      "\n",
      "        [[    1,  7036,   571,  ...,     2,     2,     2]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model.train()\n",
    "epoch_training_loss = 0\n",
    "train_iterator = tqdm(train_loader, desc=f\"Training epoch{epoch+1}/{num_epochs} Batch size: {BATCH_SIZE}, Transformer: {model_name}\")\n",
    "for batch in train_iterator:\n",
    "    optimizer.zero_grad()\n",
    "    print(batch)t\n",
    "    inputs = batch['input_ids'].squeeze(1).to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "46fbedf7-735e-47d1-a5fb-8d38810f0fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_string_from_code(node, lines):\n",
      "  \"\"\"Get string from code.\n",
      "1. Get start and end point of the node as a list.\n",
      "2. Add to the code_list a variable called \"lines\" that is stored in the code object.\n",
      "3. Iterate through all nodes and traverse them into a dictionary and append their respective lists.\n",
      "4. For each child node, we get its value from node's start position and char positions by using split() so we can use the Split method on it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing the output of the model\n",
    "input_str = \"def get_string_from_code(node, lines):\\n  line_start = node.start_point[0]\\n  line_end = node.end_point[0]\\n  char_start = node.start_point[1]\\n  char_end = node.end_point[1]\\n  if line_start != line_end:\\n    code_list.append(' '.join([lines[line_start][char_start:]] + lines[line_start+1:line_end] + [lines[line_end][:char_end]]))\\n  else:\\n    code_list.append(lines[line_start][char_start:char_end])\\n\\ndef my_traverse(node, code_list):\\n  lines = code.split('\\n')\\n  if node.child_count == 0:\\n    get_string_from_code(node, lines)\\n  elif node.type == 'string':\\n    get_string_from_code(node, lines)\\n  else:\\n    for n in node.children:\\n      my_traverse(n, code_list)\\n \\n  return ' '.join(code_list)\"\n",
    "input_ids = tokenizer.encode(input_str, return_tensors='pt').to(device)\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_length=1024,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,\n",
    "    top_k=8,\n",
    "    top_p=0.95,\n",
    "    temperature=0.5,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a017dcda5152c54c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model, \"JupyterLM.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b7ec9d8-aae0-416c-afbc-4a27b5288cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"model\", from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e482e7e6b7b2bdc0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data = data.map(lambda samples: tokenizer(samples[\"source_x\"]), batched=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf01f2cad9048fb",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "model.config.use_cache = True\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "003a1240316f4c999311e4eb4c94947a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c24b31ec3a4843258b079e96b748ea6a",
       "IPY_MODEL_c0ac8000f2e2464294ed348f3e4375e3",
       "IPY_MODEL_44b7101b2ae34f97ba7d017453d95a20"
      ],
      "layout": "IPY_MODEL_3331119af7d648868a3a800013b5ce9d"
     }
    },
    "0338cff46db84141bbd3e448a6cec1c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c1311766e96a42a2a6afa46ae0900cc2",
       "IPY_MODEL_d9252aee504c4211afe12e2f149530f3",
       "IPY_MODEL_4b6dab05933c4a61ae3cb97beb70fc8d"
      ],
      "layout": "IPY_MODEL_648f7401a38842119b87040393042eda"
     }
    },
    "0403eaa45933488d9a62399c454ff30d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d129dfba3b14ab6a7e9b4bf23009716",
      "placeholder": "",
      "style": "IPY_MODEL_b02fb084d07e469d9e828ed634364b1a",
      "value": "vocab.json:100%"
     }
    },
    "04df562ce2f84059819dabe3dbaffe46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3032f12569bf4fedac696f9a89fa2190",
      "placeholder": "",
      "style": "IPY_MODEL_5d7019f6667242ef8006e2cd31339ce5",
      "value": "1.37M/1.37M[00:01&lt;00:00,1.09MB/s]"
     }
    },
    "06efaa4c02024b1f8d9cd16a2e921983": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "06ffc5179dcc45cb945d5d13dc80b611": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0fcca77c266b4d54a5ca4a4bccd62ff3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1094659a432b45ba8230444cd526cb8f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "123f85eea3e74d04b009472051c5658b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "158a7d4e6acf49f2b78a3c8171e5aed4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b345a04dae547c69cfdb8abfc1fc788": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1c5fe8caec364d84bad4cc30fca2e0a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23a8404240c64008990c86002aaa1b62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "253c3d9f966a4c769877833da4b16d7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd91b20f621f4e7faffc4bb8725dc063",
      "max": 294402,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4f4e35c989a14f8a823e1692f1969a3b",
      "value": 294402
     }
    },
    "273d2e752f7540509bbf4e46d6e230da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2b4919a2fd03451b80a8f275f01362f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3032f12569bf4fedac696f9a89fa2190": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30b039505b2c4f468a836605f764863f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3331119af7d648868a3a800013b5ce9d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37dfa295b7d34ad691c5692d42d2ee54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a2a180b43818401fb65704b7daa32a2f",
       "IPY_MODEL_432709c15d844f5c863fabeb9ae93052",
       "IPY_MODEL_f531d74111a64e03bcdff13b52408708"
      ],
      "layout": "IPY_MODEL_e24f145940d94f228e9dbee0223765d0"
     }
    },
    "3f2d1d994cb14b02a4be12cd2a9d95fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b4919a2fd03451b80a8f275f01362f2",
      "placeholder": "",
      "style": "IPY_MODEL_273d2e752f7540509bbf4e46d6e230da",
      "value": "2.95G/2.95G[02:43&lt;00:00,17.7MB/s]"
     }
    },
    "426bfaa9a8ed427fa20d13adf24f6353": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "432709c15d844f5c863fabeb9ae93052": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fcca77c266b4d54a5ca4a4bccd62ff3",
      "max": 1499,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f429dc73d4a24c59a81d0fd28a47ced6",
      "value": 1499
     }
    },
    "4432a666276a496cb1bf04354ad9a417": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "44b7101b2ae34f97ba7d017453d95a20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_30b039505b2c4f468a836605f764863f",
      "placeholder": "",
      "style": "IPY_MODEL_bdfb5661cd2d4bac9e43a9304461df5a",
      "value": "15.5k/15.5k[00:00&lt;00:00,1.20MB/s]"
     }
    },
    "4b6dab05933c4a61ae3cb97beb70fc8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_84f9e4b99afd4264bb89cb7629598546",
      "placeholder": "",
      "style": "IPY_MODEL_957b4117bdb145a5900bec2a0501ea99",
      "value": "1.51k/1.51k[00:00&lt;00:00,82.5kB/s]"
     }
    },
    "4f4e35c989a14f8a823e1692f1969a3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4f61077e74a94d0b9101dc12573395a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "50735bd82c34493e8ed909dc37ee1028": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ebc45a8d90ee4c3f8edf9954ded85a03",
      "placeholder": "",
      "style": "IPY_MODEL_cb793974256c42f78c89f66152fc63ba",
      "value": "511k/511k[00:00&lt;00:00,16.1MB/s]"
     }
    },
    "51acb6ef3155414dadd217959e68798d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_704c69c39b884796ac85838099a0ecf4",
      "placeholder": "",
      "style": "IPY_MODEL_ebd62dd8188442749a8d42eaeb3d6e94",
      "value": "tokenizer.json:100%"
     }
    },
    "5480cf12eba446f9b8fab6073a6cc5ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5550f3c40eca4c46864c6c713bf501d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c71b5073cc764a52882d58064c3a1854",
      "max": 510846,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1b345a04dae547c69cfdb8abfc1fc788",
      "value": 510846
     }
    },
    "5b0d8c8ed8f54cb7add26b53469716cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d7019f6667242ef8006e2cd31339ce5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5de36102a2c04919a5b1edad5282829e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e3404b6278c430198105fc11c78695e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "615c1ae1332d44d59ac8e9ac76c6545b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "62a9e5956847481cab3f24dc9519a0d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "648f7401a38842119b87040393042eda": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f6ac04385994affbd66e11501711f04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e8d0e0100b1e48639ba2e1bc981331d7",
       "IPY_MODEL_fa4b4c785e274c52ba4994f9500f12e1",
       "IPY_MODEL_f2689721c574404d96290323d7cd946c"
      ],
      "layout": "IPY_MODEL_5de36102a2c04919a5b1edad5282829e"
     }
    },
    "704c69c39b884796ac85838099a0ecf4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c3b452280c448d8bcae01a58b818336": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7cf08471a81c4f42b71adf62b2e65ff0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "839f8c1724cc440b9c4bb69d0876171f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "84f9e4b99afd4264bb89cb7629598546": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a083bc0ae744eb9812b2007483ff5c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2f26422fb944968a3b3f41741b733cb",
      "placeholder": "",
      "style": "IPY_MODEL_d02dfa1fc5304b959b11a8c128ae1382",
      "value": "model.safetensors:100%"
     }
    },
    "8d129dfba3b14ab6a7e9b4bf23009716": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e246e16f77a41c3adf4fb1e85728dc3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90eaacb26aba4ee38f37e8c005cffaa1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "957b4117bdb145a5900bec2a0501ea99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9694d9914b0546b29fba99a0a4e2bde2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a2a180b43818401fb65704b7daa32a2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7cf08471a81c4f42b71adf62b2e65ff0",
      "placeholder": "",
      "style": "IPY_MODEL_7c3b452280c448d8bcae01a58b818336",
      "value": "config.json:100%"
     }
    },
    "a885164d7791437d93eb228a19334e81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "af962879d466466b8bfdb57f293d1d09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f3b6ba638ffe47eda7dfccccdb87ea4e",
       "IPY_MODEL_253c3d9f966a4c769877833da4b16d7e",
       "IPY_MODEL_cf95923d61a743128c6c89328c0c527b"
      ],
      "layout": "IPY_MODEL_d7e60c80882242cbbdfaac49e3b31287"
     }
    },
    "b02fb084d07e469d9e828ed634364b1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b1937b66e7c24180b5c7cfa98fb53859": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bdfb5661cd2d4bac9e43a9304461df5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c0ac8000f2e2464294ed348f3e4375e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b0d8c8ed8f54cb7add26b53469716cb",
      "max": 15484,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_06ffc5179dcc45cb945d5d13dc80b611",
      "value": 15484
     }
    },
    "c1311766e96a42a2a6afa46ae0900cc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f014cca49d0441f69499cc98923c84b7",
      "placeholder": "",
      "style": "IPY_MODEL_c4af812000dd4470b65fff7b7694680b",
      "value": "tokenizer_config.json:100%"
     }
    },
    "c24b31ec3a4843258b079e96b748ea6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f61077e74a94d0b9101dc12573395a0",
      "placeholder": "",
      "style": "IPY_MODEL_23a8404240c64008990c86002aaa1b62",
      "value": "special_tokens_map.json:100%"
     }
    },
    "c2f26422fb944968a3b3f41741b733cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4af812000dd4470b65fff7b7694680b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c71b5073cc764a52882d58064c3a1854": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c88a701108f140ad9296e1935132e19b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06efaa4c02024b1f8d9cd16a2e921983",
      "max": 1368535,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_123f85eea3e74d04b009472051c5658b",
      "value": 1368535
     }
    },
    "c95e7a7edaff4dc39c8a31b70c5567e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0403eaa45933488d9a62399c454ff30d",
       "IPY_MODEL_5550f3c40eca4c46864c6c713bf501d6",
       "IPY_MODEL_50735bd82c34493e8ed909dc37ee1028"
      ],
      "layout": "IPY_MODEL_f994d30e8606483a8c783beaa6935bf1"
     }
    },
    "ca400e55b65a4cd0a5b1fdf6df39a23e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_51acb6ef3155414dadd217959e68798d",
       "IPY_MODEL_c88a701108f140ad9296e1935132e19b",
       "IPY_MODEL_04df562ce2f84059819dabe3dbaffe46"
      ],
      "layout": "IPY_MODEL_1c5fe8caec364d84bad4cc30fca2e0a0"
     }
    },
    "cb793974256c42f78c89f66152fc63ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd91b20f621f4e7faffc4bb8725dc063": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf95923d61a743128c6c89328c0c527b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90eaacb26aba4ee38f37e8c005cffaa1",
      "placeholder": "",
      "style": "IPY_MODEL_839f8c1724cc440b9c4bb69d0876171f",
      "value": "294k/294k[00:00&lt;00:00,14.9MB/s]"
     }
    },
    "d02dfa1fc5304b959b11a8c128ae1382": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d7e60c80882242cbbdfaac49e3b31287": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9252aee504c4211afe12e2f149530f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1094659a432b45ba8230444cd526cb8f",
      "max": 1515,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4432a666276a496cb1bf04354ad9a417",
      "value": 1515
     }
    },
    "dde08ca3738a459c8f3a246168eaf932": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8a083bc0ae744eb9812b2007483ff5c5",
       "IPY_MODEL_f5ed685cdfa5497cab9f2fc26f1aefc0",
       "IPY_MODEL_3f2d1d994cb14b02a4be12cd2a9d95fd"
      ],
      "layout": "IPY_MODEL_158a7d4e6acf49f2b78a3c8171e5aed4"
     }
    },
    "e24f145940d94f228e9dbee0223765d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8d0e0100b1e48639ba2e1bc981331d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd857a7c201447fc956d199deea6f407",
      "placeholder": "",
      "style": "IPY_MODEL_426bfaa9a8ed427fa20d13adf24f6353",
      "value": "Generatingtrainsplit:"
     }
    },
    "ebc45a8d90ee4c3f8edf9954ded85a03": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ebd62dd8188442749a8d42eaeb3d6e94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f014cca49d0441f69499cc98923c84b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f23672ed896046979ab9c31901cf82fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2689721c574404d96290323d7cd946c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f23672ed896046979ab9c31901cf82fe",
      "placeholder": "",
      "style": "IPY_MODEL_5e3404b6278c430198105fc11c78695e",
      "value": "828122/0[00:18&lt;00:00,53840.22examples/s]"
     }
    },
    "f3b6ba638ffe47eda7dfccccdb87ea4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5480cf12eba446f9b8fab6073a6cc5ef",
      "placeholder": "",
      "style": "IPY_MODEL_f7f3735f60e548419d4a380208b9e530",
      "value": "merges.txt:100%"
     }
    },
    "f429dc73d4a24c59a81d0fd28a47ced6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f531d74111a64e03bcdff13b52408708": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8e246e16f77a41c3adf4fb1e85728dc3",
      "placeholder": "",
      "style": "IPY_MODEL_a885164d7791437d93eb228a19334e81",
      "value": "1.50k/1.50k[00:00&lt;00:00,112kB/s]"
     }
    },
    "f5ed685cdfa5497cab9f2fc26f1aefc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1937b66e7c24180b5c7cfa98fb53859",
      "max": 2950619850,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_62a9e5956847481cab3f24dc9519a0d6",
      "value": 2950619850
     }
    },
    "f7f3735f60e548419d4a380208b9e530": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f994d30e8606483a8c783beaa6935bf1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa4b4c785e274c52ba4994f9500f12e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_615c1ae1332d44d59ac8e9ac76c6545b",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9694d9914b0546b29fba99a0a4e2bde2",
      "value": 1
     }
    },
    "fd857a7c201447fc956d199deea6f407": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
