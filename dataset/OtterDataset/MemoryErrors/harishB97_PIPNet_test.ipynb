{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "/home/harishbabu/.conda/envs/hpnet1/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from pipnet.pipnet import PIPNet, get_network\n",
    "from util.data import get_dataloaders\n",
    "from util.vis_pipnet import get_img_coordinates\n",
    "from util.func import get_patch_size\n",
    "from util.eval_cub_csv import get_topk_cub\n",
    "from PIL import ImageFont, Image, ImageDraw as D\n",
    "from pipnet.train import test_pipnet, train_pipnet\n",
    "from omegaconf import OmegaConf\n",
    "from util.phylo_utils import construct_phylo_tree, construct_discretized_phylo_tree\n",
    "from util.args import get_args, save_args, get_optimizer_nn\n",
    "import wandb\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_path = '/home/harishbabu/projects/PIPNet/runs/004-CUB-27-imgnet_cnext26_img=224_nprotos=200'\n",
    "run_path = '/home/harishbabu/projects/PIPNet/runs/005-CUB-27-imgnet_cnext26_img=224_nprotos=50'\n",
    "\n",
    "device = torch.device('cuda')\n",
    "device_ids = [torch.cuda.current_device()]\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "# device_ids = []\n",
    "\n",
    "args_file = open(os.path.join(run_path, 'metadata', 'args.pickle'), 'rb')\n",
    "args = pickle.load(args_file)\n",
    "\n",
    "ckpt_path = os.path.join(run_path, 'checkpoints', 'net_trained_last')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- No discretization -------------------------\n"
     ]
    }
   ],
   "source": [
    "if args.phylo_config:\n",
    "    phylo_config = OmegaConf.load(args.phylo_config)\n",
    "\n",
    "if args.phylo_config:\n",
    "    # construct the phylo tree\n",
    "    if phylo_config.phyloDistances_string == 'None':\n",
    "        root = construct_phylo_tree(phylo_config.phylogeny_path)\n",
    "        print('-'*25 + ' No discretization ' + '-'*25)\n",
    "    else:\n",
    "        root = construct_discretized_phylo_tree(phylo_config.phylogeny_path, phylo_config.phyloDistances_string)\n",
    "        print('-'*25 + ' Discretized ' + '-'*25)\n",
    "else:\n",
    "    # construct the tree (original hierarchy as described in the paper)\n",
    "    root = Node(\"root\")\n",
    "    root.add_children(['animal','vehicle','everyday_object','weapon','scuba_diver'])\n",
    "    root.add_children_to('animal',['non_primate','primate'])\n",
    "    root.add_children_to('non_primate',['African_elephant','giant_panda','lion'])\n",
    "    root.add_children_to('primate',['capuchin','gibbon','orangutan'])\n",
    "    root.add_children_to('vehicle',['ambulance','pickup','sports_car'])\n",
    "    root.add_children_to('everyday_object',['laptop','sandal','wine_bottle'])\n",
    "    root.add_children_to('weapon',['assault_rifle','rifle'])\n",
    "    # flat root\n",
    "    # root.add_children(['scuba_diver','African_elephant','giant_panda','lion','capuchin','gibbon','orangutan','ambulance','pickup','sports_car','laptop','sandal','wine_bottle','assault_rifle','rifle'])\n",
    "root.assign_all_descendents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes (k) =  27 ['cub_001_Black_footed_Albatross', 'cub_011_Rusty_Blackbird', 'cub_016_Painted_Bunting', 'cub_019_Gray_Catbird', 'cub_030_Fish_Crow'] etc.\n",
      "Number of prototypes:  50\n"
     ]
    }
   ],
   "source": [
    "# Obtain the dataset and dataloaders\n",
    "trainloader, trainloader_pretraining, trainloader_normal, trainloader_normal_augment, projectloader, testloader, test_projectloader, classes = get_dataloaders(args, device)\n",
    "if len(classes)<=20:\n",
    "    if args.validation_size == 0.:\n",
    "        print(\"Classes: \", testloader.dataset.class_to_idx, flush=True)\n",
    "    else:\n",
    "        print(\"Classes: \", str(classes), flush=True)\n",
    "\n",
    "# Create a convolutional network based on arguments and add 1x1 conv layer\n",
    "feature_net, add_on_layers, pool_layer, classification_layers, num_prototypes = get_network(len(classes), args, root=root)\n",
    "   \n",
    "# Create a PIP-Net\n",
    "net = PIPNet(num_classes=len(classes),\n",
    "                    num_prototypes=num_prototypes,\n",
    "                    feature_net = feature_net,\n",
    "                    args = args,\n",
    "                    add_on_layers = add_on_layers,\n",
    "                    pool_layer = pool_layer,\n",
    "                    classification_layers = classification_layers,\n",
    "                    num_parent_nodes = len(root.nodes_with_children()),\n",
    "                    root = root\n",
    "                    )\n",
    "net = net.to(device=device)\n",
    "net = nn.DataParallel(net, device_ids = device_ids)    \n",
    "net.load_state_dict(checkpoint['model_state_dict'],strict=True)\n",
    "criterion = nn.NLLLoss(reduction='mean').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mharishbabu\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/harishbabu/projects/PIPNet/wandb/run-20230626_153429-1cuwitzx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/harishbabu/PIPNet/runs/1cuwitzx\" target=\"_blank\">005-CUB-27-imgnet_cnext26_img=224_nprotos=50</a></strong> to <a href=\"https://wandb.ai/harishbabu/PIPNet\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch0: 100% 13/13 [00:07<00:00,  1.69it/s, L:21.442,LC:0.031, LA:0.01, LT:0.411]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFine accuracy: 1 2\n",
      "\tNode name: root, acc: 99.36, f1:99.34, samples: 1562, 113+001+068=1500/1502=1.0, cub_090_Red_breasted_Merganser=52/60=0.87\n",
      "\tNode name: 113+001+068, acc: 98.14, f1:98.13, samples: 1502, 113+060=1252/1264=0.99, 001+052=166/178=0.93, cub_068_Ruby_throated_Hummingbird=56/60=0.93\n",
      "\tNode name: 113+060, acc: 99.21, f1:99.21, samples: 1264, 113+187=1082/1086=1.0, 060+071=172/178=0.97\n",
      "\tNode name: 001+052, acc: 100.0, f1:100.0, samples: 178, 001+033=118/118=1.0, cub_052_Pied_billed_Grebe=60/60=1.0\n",
      "\tNode name: 113+187, acc: 99.26, f1:99.28, samples: 1086, 113+037=978/986=0.99, 187+079=100/100=1.0\n",
      "\tNode name: 060+071, acc: 92.13, f1:91.94, samples: 178, 060+143=116/118=0.98, cub_071_Long_tailed_Jaeger=48/60=0.8\n",
      "\tNode name: 001+033, acc: 96.61, f1:96.61, samples: 118, cub_001_Black_footed_Albatross=58/60=0.97, cub_033_Yellow_billed_Cuckoo=56/58=0.97\n",
      "\tNode name: 113+037, acc: 98.99, f1:98.97, samples: 986, 113+030=868/868=1.0, 037+077=108/118=0.92\n",
      "\tNode name: 187+079, acc: 92.0, f1:92.0, samples: 100, cub_187_American_Three_toed_Woodpecker=36/40=0.9, cub_079_Belted_Kingfisher=56/60=0.93\n",
      "\tNode name: 060+143, acc: 96.61, f1:96.6, samples: 118, cub_060_Glaucous_winged_Gull=54/58=0.93, cub_143_Caspian_Tern=60/60=1.0\n",
      "\tNode name: 113+030, acc: 97.0, f1:97.04, samples: 868, 113+085=732/748=0.98, 030+156=110/120=0.92\n",
      "\tNode name: 037+077, acc: 96.61, f1:96.61, samples: 118, cub_037_Acadian_Flycatcher=58/58=1.0, cub_077_Tropical_Kingbird=56/60=0.93\n",
      "\tNode name: 113+085, acc: 98.4, f1:98.31, samples: 748, 113+194=688/688=1.0, cub_085_Horned_Lark=48/60=0.8\n",
      "\tNode name: 030+156, acc: 91.67, f1:91.66, samples: 120, cub_030_Fish_Crow=54/60=0.9, cub_156_White_eyed_Vireo=56/60=0.93\n",
      "\tNode name: 113+194, acc: 99.42, f1:99.42, samples: 688, 113+118=566/570=0.99, 194+019=118/118=1.0\n",
      "\tNode name: 113+118, acc: 97.19, f1:97.15, samples: 570, 113+034=504/510=0.99, cub_118_House_Sparrow=50/60=0.83\n",
      "\tNode name: 194+019, acc: 94.92, f1:94.92, samples: 118, cub_194_Cactus_Wren=56/60=0.93, cub_019_Gray_Catbird=56/58=0.97\n",
      "\tNode name: 113+034, acc: 99.22, f1:99.2, samples: 510, 113+016=452/452=1.0, cub_034_Gray_crowned_Rosy_Finch=54/58=0.93\n",
      "\tNode name: 113+016, acc: 98.67, f1:98.66, samples: 452, 113+165=394/396=0.99, cub_016_Painted_Bunting=52/56=0.93\n",
      "\tNode name: 113+165, acc: 97.47, f1:97.47, samples: 396, 113+011=214/218=0.98, 165+181=172/178=0.97\n",
      "\tNode name: 113+011, acc: 99.08, f1:99.08, samples: 218, 113+122=100/100=1.0, 011+097=116/118=0.98\n",
      "\tNode name: 165+181, acc: 96.63, f1:96.58, samples: 178, 165+161=120/120=1.0, cub_181_Worm_eating_Warbler=52/58=0.9\n",
      "\tNode name: 113+122, acc: 90.0, f1:89.95, samples: 100, cub_113_Baird_Sparrow=34/40=0.85, cub_122_Harris_Sparrow=56/60=0.93\n",
      "\tNode name: 011+097, acc: 89.83, f1:89.77, samples: 118, cub_011_Rusty_Blackbird=58/60=0.97, cub_097_Orchard_Oriole=48/58=0.83\n",
      "\tNode name: 165+161, acc: 70.0, f1:68.27, samples: 120, cub_165_Chestnut_sided_Warbler=28/60=0.47, cub_161_Blue_winged_Warbler=56/60=0.93\n",
      "test 0.8501920614596671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch0: 100% 13/13 [00:08<00:00,  1.56it/s, L:3.514,LC:0.005, LA:0.01, LT:0.041]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFine accuracy: 1 2\n",
      "\tNode name: root, acc: 100.0, f1:100.0, samples: 1620, 113+001+068=1560/1560=1.0, cub_090_Red_breasted_Merganser=60/60=1.0\n",
      "\tNode name: 113+001+068, acc: 100.0, f1:100.0, samples: 1560, 113+060=1320/1320=1.0, 001+052=180/180=1.0, cub_068_Ruby_throated_Hummingbird=60/60=1.0\n",
      "\tNode name: 113+060, acc: 100.0, f1:100.0, samples: 1320, 113+187=1140/1140=1.0, 060+071=180/180=1.0\n",
      "\tNode name: 001+052, acc: 100.0, f1:100.0, samples: 180, 001+033=120/120=1.0, cub_052_Pied_billed_Grebe=60/60=1.0\n",
      "\tNode name: 113+187, acc: 100.0, f1:100.0, samples: 1140, 113+037=1020/1020=1.0, 187+079=120/120=1.0\n",
      "\tNode name: 060+071, acc: 98.89, f1:98.88, samples: 180, 060+143=120/120=1.0, cub_071_Long_tailed_Jaeger=58/60=0.97\n",
      "\tNode name: 001+033, acc: 98.33, f1:98.33, samples: 120, cub_001_Black_footed_Albatross=58/60=0.97, cub_033_Yellow_billed_Cuckoo=60/60=1.0\n",
      "\tNode name: 113+037, acc: 100.0, f1:100.0, samples: 1020, 113+030=900/900=1.0, 037+077=120/120=1.0\n",
      "\tNode name: 187+079, acc: 94.17, f1:94.16, samples: 120, cub_187_American_Three_toed_Woodpecker=59/60=0.98, cub_079_Belted_Kingfisher=54/60=0.9\n",
      "\tNode name: 060+143, acc: 95.83, f1:95.83, samples: 120, cub_060_Glaucous_winged_Gull=58/60=0.97, cub_143_Caspian_Tern=57/60=0.95\n",
      "\tNode name: 113+030, acc: 100.0, f1:100.0, samples: 900, 113+085=780/780=1.0, 030+156=120/120=1.0\n",
      "\tNode name: 037+077, acc: 98.33, f1:98.33, samples: 120, cub_037_Acadian_Flycatcher=60/60=1.0, cub_077_Tropical_Kingbird=58/60=0.97\n",
      "\tNode name: 113+085, acc: 100.0, f1:100.0, samples: 780, 113+194=720/720=1.0, cub_085_Horned_Lark=60/60=1.0\n",
      "\tNode name: 030+156, acc: 100.0, f1:100.0, samples: 120, cub_030_Fish_Crow=60/60=1.0, cub_156_White_eyed_Vireo=60/60=1.0\n",
      "\tNode name: 113+194, acc: 100.0, f1:100.0, samples: 720, 113+118=600/600=1.0, 194+019=120/120=1.0\n",
      "\tNode name: 113+118, acc: 100.0, f1:100.0, samples: 600, 113+034=540/540=1.0, cub_118_House_Sparrow=60/60=1.0\n",
      "\tNode name: 194+019, acc: 93.33, f1:93.3, samples: 120, cub_194_Cactus_Wren=52/60=0.87, cub_019_Gray_Catbird=60/60=1.0\n",
      "\tNode name: 113+034, acc: 100.0, f1:100.0, samples: 540, 113+016=480/480=1.0, cub_034_Gray_crowned_Rosy_Finch=60/60=1.0\n",
      "\tNode name: 113+016, acc: 100.0, f1:100.0, samples: 480, 113+165=420/420=1.0, cub_016_Painted_Bunting=60/60=1.0\n",
      "\tNode name: 113+165, acc: 100.0, f1:100.0, samples: 420, 113+011=240/240=1.0, 165+181=180/180=1.0\n",
      "\tNode name: 113+011, acc: 100.0, f1:100.0, samples: 240, 113+122=120/120=1.0, 011+097=120/120=1.0\n",
      "\tNode name: 165+181, acc: 100.0, f1:100.0, samples: 180, 165+161=120/120=1.0, cub_181_Worm_eating_Warbler=60/60=1.0\n",
      "\tNode name: 113+122, acc: 90.0, f1:89.99, samples: 120, cub_113_Baird_Sparrow=52/60=0.87, cub_122_Harris_Sparrow=56/60=0.93\n",
      "\tNode name: 011+097, acc: 91.67, f1:91.61, samples: 120, cub_011_Rusty_Blackbird=60/60=1.0, cub_097_Orchard_Oriole=50/60=0.83\n",
      "\tNode name: 165+161, acc: 79.17, f1:79.05, samples: 120, cub_165_Chestnut_sided_Warbler=43/60=0.72, cub_161_Blue_winged_Warbler=52/60=0.87\n",
      "train 0.9549382716049383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "run = wandb.init(project=\"pipnet\", name=os.path.basename(args.log_dir), config=vars(args), reinit=False)\n",
    "info = test_pipnet(net, testloader, criterion, epoch, device, progress_prefix= 'Test Epoch', wandb_logging=True, wandb_log_subdir = 'test')\n",
    "print('test', info['fine_accuracy'])\n",
    "info = test_pipnet(net, trainloader, criterion, epoch, device, progress_prefix= 'Train Epoch', wandb_logging=False, wandb_log_subdir = 'train')\n",
    "print('train', info['fine_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosen network is convnext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Train Epoch0:   0% 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters that require gradient:  206\n",
      "Align weight:  5.0 , U_tanh weight:  2.0 Class weight: 2.0\n",
      "Pretrain? False Finetune? False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch0:   0% 0/13 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 154.00 MiB (GPU 0; 15.78 GiB total capacity; 14.51 GiB already allocated; 117.69 MiB free; 14.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     scheduler_classifier \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mCosineAnnealingWarmRestarts(optimizer_classifier, T_0\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, eta_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, T_mult\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m scheduler_net \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mCosineAnnealingLR(optimizer_net, T_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(trainloader)\u001b[38;5;241m*\u001b[39margs\u001b[38;5;241m.\u001b[39mepochs, eta_min\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlr_net\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m100.\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m train_info \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_pipnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_classifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_classifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinetune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/PIPNet/pipnet/train.py:94\u001b[0m, in \u001b[0;36mtrain_pipnet\u001b[0;34m(net, train_loader, optimizer_net, optimizer_classifier, scheduler_net, scheduler_classifier, criterion, epoch, nr_epochs, device, pretrain, finetune, progress_prefix, wandb_logging)\u001b[0m\n\u001b[1;32m     91\u001b[0m optimizer_net\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Perform a forward pass through the network\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m proto_features, pooled, out \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mxs1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m loss, class_loss, a_loss_pf, tanh_loss, avg_class_loss, avg_a_loss_pf, avg_tanh_loss, acc \u001b[38;5;241m=\u001b[39m calculate_loss(proto_features, pooled, out, ys, align_pf_weight, t_weight, unif_weight, cl_weight, \\\n\u001b[1;32m     97\u001b[0m                                                                                                             net\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39m_multiplier, pretrain, finetune, criterion, \\\n\u001b[1;32m     98\u001b[0m                                                                                                                 train_iter, \u001b[38;5;28mprint\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, EPS\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m, root\u001b[38;5;241m=\u001b[39mroot, label2name\u001b[38;5;241m=\u001b[39mlabel2name, node_accuracy\u001b[38;5;241m=\u001b[39mnode_accuracy)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Compute the gradient\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/hpnet1/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/hpnet1/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:169\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m ({},)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[1;32m    171\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_apply(replicas, inputs, kwargs)\n",
      "File \u001b[0;32m~/.conda/envs/hpnet1/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/PIPNet/pipnet/pipnet.py:42\u001b[0m, in \u001b[0;36mPIPNet.forward\u001b[0;34m(self, xs, inference)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, xs,  inference\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 42\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     43\u001b[0m     proto_features_all_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_on(features)\n\u001b[1;32m     44\u001b[0m     proto_features \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/.conda/envs/hpnet1/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/hpnet1/lib/python3.8/site-packages/torchvision/models/convnext.py:176\u001b[0m, in \u001b[0;36mConvNeXt.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/hpnet1/lib/python3.8/site-packages/torchvision/models/convnext.py:170\u001b[0m, in \u001b[0;36mConvNeXt._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 170\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[1;32m    172\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n",
      "File \u001b[0;32m~/.conda/envs/hpnet1/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/hpnet1/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/hpnet1/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/hpnet1/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/hpnet1/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/hpnet1/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/hpnet1/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 154.00 MiB (GPU 0; 15.78 GiB total capacity; 14.51 GiB already allocated; 117.69 MiB free; 14.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "optimizer_net, optimizer_classifier, params_to_freeze, params_to_train, params_backbone = get_optimizer_nn(net, args)            \n",
    "if args.epochs<=30:\n",
    "    scheduler_classifier = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer_classifier, T_0=5, eta_min=0.001, T_mult=1, verbose=False)\n",
    "else:\n",
    "    scheduler_classifier = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer_classifier, T_0=10, eta_min=0.001, T_mult=1, verbose=False)\n",
    "\n",
    "scheduler_net = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_net, T_max=len(trainloader)*args.epochs, eta_min=args.lr_net/100.)\n",
    "train_info = train_pipnet(net, trainloader, optimizer_net, optimizer_classifier, scheduler_net, scheduler_classifier, criterion, epoch, args.epochs, device, pretrain=False, finetune=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "\t113+001+068\n",
      "\t\t113+060\n",
      "\t\t\t113+187\n",
      "\t\t\t\t113+037\n",
      "\t\t\t\t\t113+030\n",
      "\t\t\t\t\t\t113+085\n",
      "\t\t\t\t\t\t\t113+194\n",
      "\t\t\t\t\t\t\t\t113+118\n",
      "\t\t\t\t\t\t\t\t\t113+034\n",
      "\t\t\t\t\t\t\t\t\t\t113+016\n",
      "\t\t\t\t\t\t\t\t\t\t\t113+165\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t113+011\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t113+122\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\tcub_113_Baird_Sparrow\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\tcub_122_Harris_Sparrow\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t011+097\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\tcub_011_Rusty_Blackbird\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\tcub_097_Orchard_Oriole\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t165+181\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t165+161\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\tcub_165_Chestnut_sided_Warbler\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\tcub_161_Blue_winged_Warbler\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\tcub_181_Worm_eating_Warbler\n",
      "\t\t\t\t\t\t\t\t\t\t\tcub_016_Painted_Bunting\n",
      "\t\t\t\t\t\t\t\t\t\tcub_034_Gray_crowned_Rosy_Finch\n",
      "\t\t\t\t\t\t\t\t\tcub_118_House_Sparrow\n",
      "\t\t\t\t\t\t\t\t194+019\n",
      "\t\t\t\t\t\t\t\t\tcub_194_Cactus_Wren\n",
      "\t\t\t\t\t\t\t\t\tcub_019_Gray_Catbird\n",
      "\t\t\t\t\t\t\tcub_085_Horned_Lark\n",
      "\t\t\t\t\t\t030+156\n",
      "\t\t\t\t\t\t\tcub_030_Fish_Crow\n",
      "\t\t\t\t\t\t\tcub_156_White_eyed_Vireo\n",
      "\t\t\t\t\t037+077\n",
      "\t\t\t\t\t\tcub_037_Acadian_Flycatcher\n",
      "\t\t\t\t\t\tcub_077_Tropical_Kingbird\n",
      "\t\t\t\t187+079\n",
      "\t\t\t\t\tcub_187_American_Three_toed_Woodpecker\n",
      "\t\t\t\t\tcub_079_Belted_Kingfisher\n",
      "\t\t\t060+071\n",
      "\t\t\t\t060+143\n",
      "\t\t\t\t\tcub_060_Glaucous_winged_Gull\n",
      "\t\t\t\t\tcub_143_Caspian_Tern\n",
      "\t\t\t\tcub_071_Long_tailed_Jaeger\n",
      "\t\t001+052\n",
      "\t\t\t001+033\n",
      "\t\t\t\tcub_001_Black_footed_Albatross\n",
      "\t\t\t\tcub_033_Yellow_billed_Cuckoo\n",
      "\t\t\tcub_052_Pied_billed_Grebe\n",
      "\t\tcub_068_Ruby_throated_Hummingbird\n",
      "\tcub_090_Red_breasted_Merganser\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
