{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import dcMinMaxFunctions as dc\n",
        "# import dcor\n",
        "from scipy.misc import derivative\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from scipy import stats\n",
        "import wandb\n",
        "from cov_help import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = nn.Sequential(\n",
        "            nn.Linear(54, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 7),\n",
        "            nn.Softmax(dim=1)\n",
        "\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "49479\n"
          ]
        }
      ],
      "source": [
        "#find number of parameters in model\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(count_parameters(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Elevation</th>\n",
              "      <th>Aspect</th>\n",
              "      <th>Slope</th>\n",
              "      <th>Horizontal_Distance_To_Hydrology</th>\n",
              "      <th>Vertical_Distance_To_Hydrology</th>\n",
              "      <th>Horizontal_Distance_To_Roadways</th>\n",
              "      <th>Hillshade_9am</th>\n",
              "      <th>Hillshade_Noon</th>\n",
              "      <th>Hillshade_3pm</th>\n",
              "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
              "      <th>...</th>\n",
              "      <th>Soil_Type32</th>\n",
              "      <th>Soil_Type33</th>\n",
              "      <th>Soil_Type34</th>\n",
              "      <th>Soil_Type35</th>\n",
              "      <th>Soil_Type36</th>\n",
              "      <th>Soil_Type37</th>\n",
              "      <th>Soil_Type38</th>\n",
              "      <th>Soil_Type39</th>\n",
              "      <th>Soil_Type40</th>\n",
              "      <th>Cover_Type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2596</td>\n",
              "      <td>51</td>\n",
              "      <td>3</td>\n",
              "      <td>258</td>\n",
              "      <td>0</td>\n",
              "      <td>510</td>\n",
              "      <td>221</td>\n",
              "      <td>232</td>\n",
              "      <td>148</td>\n",
              "      <td>6279</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5180</td>\n",
              "      <td>112</td>\n",
              "      <td>4</td>\n",
              "      <td>424</td>\n",
              "      <td>-12</td>\n",
              "      <td>780</td>\n",
              "      <td>440</td>\n",
              "      <td>470</td>\n",
              "      <td>302</td>\n",
              "      <td>12450</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2804</td>\n",
              "      <td>139</td>\n",
              "      <td>9</td>\n",
              "      <td>268</td>\n",
              "      <td>65</td>\n",
              "      <td>3180</td>\n",
              "      <td>234</td>\n",
              "      <td>238</td>\n",
              "      <td>135</td>\n",
              "      <td>6121</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2785</td>\n",
              "      <td>155</td>\n",
              "      <td>18</td>\n",
              "      <td>242</td>\n",
              "      <td>118</td>\n",
              "      <td>3090</td>\n",
              "      <td>238</td>\n",
              "      <td>238</td>\n",
              "      <td>122</td>\n",
              "      <td>6211</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2595</td>\n",
              "      <td>45</td>\n",
              "      <td>2</td>\n",
              "      <td>153</td>\n",
              "      <td>-1</td>\n",
              "      <td>391</td>\n",
              "      <td>220</td>\n",
              "      <td>234</td>\n",
              "      <td>150</td>\n",
              "      <td>6172</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581007</th>\n",
              "      <td>2396</td>\n",
              "      <td>153</td>\n",
              "      <td>20</td>\n",
              "      <td>85</td>\n",
              "      <td>17</td>\n",
              "      <td>108</td>\n",
              "      <td>240</td>\n",
              "      <td>237</td>\n",
              "      <td>118</td>\n",
              "      <td>837</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581008</th>\n",
              "      <td>2391</td>\n",
              "      <td>152</td>\n",
              "      <td>19</td>\n",
              "      <td>67</td>\n",
              "      <td>12</td>\n",
              "      <td>95</td>\n",
              "      <td>240</td>\n",
              "      <td>237</td>\n",
              "      <td>119</td>\n",
              "      <td>845</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581009</th>\n",
              "      <td>2386</td>\n",
              "      <td>159</td>\n",
              "      <td>17</td>\n",
              "      <td>60</td>\n",
              "      <td>7</td>\n",
              "      <td>90</td>\n",
              "      <td>236</td>\n",
              "      <td>241</td>\n",
              "      <td>130</td>\n",
              "      <td>854</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581010</th>\n",
              "      <td>2384</td>\n",
              "      <td>170</td>\n",
              "      <td>15</td>\n",
              "      <td>60</td>\n",
              "      <td>5</td>\n",
              "      <td>90</td>\n",
              "      <td>230</td>\n",
              "      <td>245</td>\n",
              "      <td>143</td>\n",
              "      <td>864</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581011</th>\n",
              "      <td>2383</td>\n",
              "      <td>165</td>\n",
              "      <td>13</td>\n",
              "      <td>60</td>\n",
              "      <td>4</td>\n",
              "      <td>67</td>\n",
              "      <td>231</td>\n",
              "      <td>244</td>\n",
              "      <td>141</td>\n",
              "      <td>875</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>581012 rows × 55 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
              "0            2596      51      3                               258   \n",
              "1            5180     112      4                               424   \n",
              "2            2804     139      9                               268   \n",
              "3            2785     155     18                               242   \n",
              "4            2595      45      2                               153   \n",
              "...           ...     ...    ...                               ...   \n",
              "581007       2396     153     20                                85   \n",
              "581008       2391     152     19                                67   \n",
              "581009       2386     159     17                                60   \n",
              "581010       2384     170     15                                60   \n",
              "581011       2383     165     13                                60   \n",
              "\n",
              "        Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
              "0                                    0                              510   \n",
              "1                                  -12                              780   \n",
              "2                                   65                             3180   \n",
              "3                                  118                             3090   \n",
              "4                                   -1                              391   \n",
              "...                                ...                              ...   \n",
              "581007                              17                              108   \n",
              "581008                              12                               95   \n",
              "581009                               7                               90   \n",
              "581010                               5                               90   \n",
              "581011                               4                               67   \n",
              "\n",
              "        Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
              "0                 221             232            148   \n",
              "1                 440             470            302   \n",
              "2                 234             238            135   \n",
              "3                 238             238            122   \n",
              "4                 220             234            150   \n",
              "...               ...             ...            ...   \n",
              "581007            240             237            118   \n",
              "581008            240             237            119   \n",
              "581009            236             241            130   \n",
              "581010            230             245            143   \n",
              "581011            231             244            141   \n",
              "\n",
              "        Horizontal_Distance_To_Fire_Points  ...  Soil_Type32  Soil_Type33  \\\n",
              "0                                     6279  ...            0            0   \n",
              "1                                    12450  ...            0            0   \n",
              "2                                     6121  ...            0            0   \n",
              "3                                     6211  ...            0            0   \n",
              "4                                     6172  ...            0            0   \n",
              "...                                    ...  ...          ...          ...   \n",
              "581007                                 837  ...            0            0   \n",
              "581008                                 845  ...            0            0   \n",
              "581009                                 854  ...            0            0   \n",
              "581010                                 864  ...            0            0   \n",
              "581011                                 875  ...            0            0   \n",
              "\n",
              "        Soil_Type34  Soil_Type35  Soil_Type36  Soil_Type37  Soil_Type38  \\\n",
              "0                 0            0            0            0            0   \n",
              "1                 0            0            0            0            0   \n",
              "2                 0            0            0            0            0   \n",
              "3                 0            0            0            0            0   \n",
              "4                 0            0            0            0            0   \n",
              "...             ...          ...          ...          ...          ...   \n",
              "581007            0            0            0            0            0   \n",
              "581008            0            0            0            0            0   \n",
              "581009            0            0            0            0            0   \n",
              "581010            0            0            0            0            0   \n",
              "581011            0            0            0            0            0   \n",
              "\n",
              "        Soil_Type39  Soil_Type40  Cover_Type  \n",
              "0                 0            0           5  \n",
              "1                 0            0          10  \n",
              "2                 0            0           2  \n",
              "3                 0            0           2  \n",
              "4                 0            0           5  \n",
              "...             ...          ...         ...  \n",
              "581007            0            0           3  \n",
              "581008            0            0           3  \n",
              "581009            0            0           3  \n",
              "581010            0            0           3  \n",
              "581011            0            0           3  \n",
              "\n",
              "[581012 rows x 55 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df=pd.read_csv(\"data/covtype.csv\")\n",
        "# multiply second row by 2\n",
        "df.iloc[1] = df.iloc[1]*2\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df.drop(\"Cover_Type\",axis=1).values\n",
        "Y= df[\"Cover_Type\"].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize(x):\n",
        "    x_normed = 1*x / x.max(0, keepdim=True)[0]\n",
        "    return x_normed\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = torch.Tensor(X)\n",
        "Y = torch.Tensor(Y)\n",
        "X = normalize(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# max_dist = torch.cdist(X, X).max()\n",
        "# print(max_dist)\n",
        "# for i in range(len(X)):\n",
        "#     if(i%1000==0):\n",
        "#         print(i)\n",
        "#         print\n",
        "#     for j in range(i,len(X)):\n",
        "#         dist = torch.dist(X[i],X[j])\n",
        "#         if(dist>max_dist):\n",
        "#             max_dist = dist\n",
        "            \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Write code to convert Y from 1,2,3,4,5,6,7 to 0,1,2,3,4,5,6\n",
        "Y = Y-1\n",
        "Y = Y.type(torch.LongTensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = X[::10]\n",
        "Y= Y[::10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(2.7668)\n"
          ]
        }
      ],
      "source": [
        "data_path = \"data/covtype.csv\"\n",
        "norm=1\n",
        "X,Y = cov_data_loader(data_path,norm=norm)\n",
        "max_dist = torch.cdist(X, X).max()\n",
        "print(max_dist) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([4, 4, 4, 4, 0, 4, 4, 1, 0, 1])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RSikHVAlO3MY"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#write a code to choose evenly sampled subset of X and Y so that only 10% data remains\n",
        "# X_train = X_train[::10]\n",
        "# Y_train = Y_train[::10]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estimating Data Densities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Gaussian Kernel Density Estimation & Derivative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gau_ker(u):\n",
        "    return torch.pow(2*torch.tensor(torch.pi),u.shape[1]/(-2))*torch.exp(torch.bmm(u.view(u.shape[0], 1, u.shape[1]), u.view(u.shape[0],  u.shape[1],1))/(-2)).cuda()\n",
        "\n",
        "\n",
        "def py_kde(x,X_t,h):\n",
        "    norm = X_t.shape[0]*(h**x.shape[1])\n",
        "    prob = torch.zeros(x.shape[0]).cuda()\n",
        "    for i in range(len(X_t)):\n",
        "        prob+= (torch.squeeze(gau_ker((x - X_t[i])/h))/norm).cuda()\n",
        "    return(prob)\n",
        "\n",
        "\n",
        "def py_kde_der(p_x,x):\n",
        "    # x.requires_grad = True\n",
        "    # p_x = py_kde(x,X_t,h)\n",
        "    return (torch.autograd.grad(p_x,x,torch.ones_like(p_x),allow_unused=True,create_graph=True)[0]).cuda()\n",
        "\n",
        "\n",
        "def gau_ker_der(X,h):\n",
        "    N= X.shape[0]\n",
        "    d = X.shape[1]\n",
        "    grad = torch.zeros(X.shape)\n",
        "    for n in range(N):\n",
        "        for i in range(d):\n",
        "            for j in range(N):\n",
        "                grad[n][i]+= torch.exp(-1*torch.dot((X[n]-X[j]),(X[n]-X[j]))/(2*h*h))*(X[n][i] -X[j][i]) /(N*(h**(d+2))*((2*math.pi)**(d/2)))\n",
        "\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Confidence Intervals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def CI_KDE(p_x,n,h,d,alpha):\n",
        "    return( stats.norm.ppf(1-alpha/2)*torch.sqrt(p_x/((2**d)*math.sqrt(torch.pi**d)*n*h**(d))).cuda() )\n",
        "\n",
        "def CI_KDE_der(p_x_der,p_x,n,h,d,alpha):\n",
        "    return( p_x_der*stats.norm.ppf(1-alpha/2)*torch.sqrt(1/(p_x.unsqueeze(dim=1)*(2**d)*math.sqrt(torch.pi**d)*n*h**(d))).cuda() )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Example on Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = len(X)\n",
        "d = X.shape[1]\n",
        "alpha=0.01\n",
        "h=n**(-1./(d+4))\n",
        "x = X[0:1000].detach().cuda()\n",
        "x.requires_grad = True\n",
        "f = py_kde(x,x,h)\n",
        "f_der = py_kde_der(f,x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8276729740613151"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.2429,  0.5807,  0.4880,  ...,  0.0749,  0.0479,  0.0276],\n",
              "        [ 0.1770, -0.4430,  0.4479,  ...,  0.1349,  0.0908,  0.0512],\n",
              "        [ 0.2547,  0.4702,  0.2407,  ...,  0.1305,  0.0856,  0.0466],\n",
              "        ...,\n",
              "        [-0.2687,  0.9611, -0.1838,  ...,  0.1539, -2.2256,  0.0312],\n",
              "        [-0.1069,  0.2870, -0.3550,  ...,  0.1578,  0.1586,  0.0356],\n",
              "        [ 0.0683,  0.6880, -0.3939,  ...,  0.1432,  0.1589,  0.0285]],\n",
              "       device='cuda:0', grad_fn=<DivBackward0>)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f_der/f.unsqueeze(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "ci = CI_KDE(f,n,h,d,alpha)\n",
        "ci_der = CI_KDE_der(f_der,f,n,h,d,alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Net(nn.Module):\n",
        "    def __init__(self,p):\n",
        "        super(Net, self ).__init__()\n",
        "        \n",
        "        self.loss_reg = 0\n",
        "        self.p =p \n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        self.H_net1 = nn.Sequential(\n",
        "            nn.Linear(54, 128),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(256, 54*54).cuda()\n",
        "        )\n",
        "        self.X_net = nn.Sequential(\n",
        "            nn.Linear(54, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 7),\n",
        "            nn.Softmax(dim=2)\n",
        "\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        def H_mul(z):\n",
        "            H12 = self.H_net1(z)\n",
        "            H12= H12.reshape(z.shape[0],d,d)\n",
        "            x12 = torch.matmul(z,H12)\n",
        "            return(x12)\n",
        "    \n",
        "        \n",
        "        def batch_jacobian(func, z, create_graph=False):\n",
        "            # x in shape (Batch, Length)\n",
        "            def _func_sum(z):\n",
        "                return func(z).sum(dim=0)\n",
        "            return torch.squeeze(torch.autograd.functional.jacobian(_func_sum, z, create_graph=create_graph)).permute(1,0,2)\n",
        "        \n",
        "        x.requires_grad =True\n",
        "        p = self.p\n",
        "        self.x = x\n",
        "        d = x.shape[1]\n",
        "        bs = x.shape[0]\n",
        "        x= torch.unsqueeze(x,1)\n",
        "        z = x.cuda()\n",
        "        loss_reg = torch.zeros(bs,d).cuda()\n",
        "        for i in range(p):\n",
        "            H = self.H_net1(z).cuda()\n",
        "            H = H.reshape(bs,d,d)\n",
        "            z = torch.matmul(z,H).cuda()\n",
        "            J = batch_jacobian(H_mul, z, create_graph=True)\n",
        "            J_int =-torch.log(torch.abs(torch.det(J)))\n",
        "            loss_reg = loss_reg + torch.squeeze(torch.autograd.grad(J_int, x,torch.ones_like(J_int),allow_unused=True,create_graph= True)[0]).cuda()\n",
        "        self.loss_reg = loss_reg\n",
        "        self.y = z\n",
        "        y = self.X_net(z)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Net2(nn.Module):\n",
        "    def __init__(self,p):\n",
        "        super(Net2, self ).__init__()\n",
        "        \n",
        "        self.loss_reg = 0\n",
        "        self.p =p \n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        self.H_net1 = nn.Sequential(\n",
        "            nn.Linear(18, 128),\n",
        "            # nn.Sigmoid(),\n",
        "            # nn.Linear(128, 256),\n",
        "            # nn.Sigmoid(),\n",
        "            nn.Linear(128, 18*18).cuda()\n",
        "        )\n",
        "        self.H_net2 = nn.Sequential(\n",
        "            nn.Linear(18, 128),\n",
        "            # nn.Sigmoid(),\n",
        "            # nn.Linear(128, 256),\n",
        "            # nn.Sigmoid(),\n",
        "            nn.Linear(128, 18*18).cuda()\n",
        "        )\n",
        "        self.H_net3 = nn.Sequential(\n",
        "            nn.Linear(18, 128),\n",
        "            # nn.Sigmoid(),\n",
        "            # nn.Linear(128, 256),\n",
        "            # nn.Sigmoid(),\n",
        "            nn.Linear(128, 18*18).cuda()\n",
        "        )\n",
        "        self.X_net = nn.Sequential(\n",
        "            nn.Linear(54, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 7),\n",
        "            nn.Softmax(dim=2)\n",
        "\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        def H_mul(z):\n",
        "            H11 = self.H_net1(z[:,:,0:18])\n",
        "            H12 = self.H_net2(z[:,:,18:36])\n",
        "            H13 = self.H_net3(z[:,:,36:54])\n",
        "            H11= H11.reshape(z.shape[0],18,18)\n",
        "            H12= H12.reshape(z.shape[0],18,18)\n",
        "            H13= H13.reshape(z.shape[0],18,18)\n",
        "            x11 = torch.matmul(z[:,:,0:18],H11)\n",
        "            x12 = torch.matmul(z[:,:,18:36],H12)\n",
        "            x13 = torch.matmul(z[:,:,36:54],H13)\n",
        "            x = torch.cat((x11,x12,x13),dim=2)\n",
        "            # x12 = torch.matmul(z,H12)\n",
        "            return(x)\n",
        "    \n",
        "        \n",
        "        def batch_jacobian(func, z, create_graph=False):\n",
        "            # x in shape (Batch, Length)\n",
        "            def _func_sum(z):\n",
        "                return func(z).sum(dim=0)\n",
        "            return torch.squeeze(torch.autograd.functional.jacobian(_func_sum, z, create_graph=create_graph)).permute(1,0,2)\n",
        "        \n",
        "        x.requires_grad =True\n",
        "        p = self.p\n",
        "        self.x = x\n",
        "        d = x.shape[1]\n",
        "        bs = x.shape[0]\n",
        "        x= torch.unsqueeze(x,1)\n",
        "        z = x.cuda()\n",
        "        z1 = z[:,:,0:18]\n",
        "        z2 = z[:,:,18:36]\n",
        "        z3 = z[:,:,36:54]\n",
        "        loss_reg = torch.zeros(bs,d).cuda()\n",
        "        for i in range(p):\n",
        "            \n",
        "            H1 = self.H_net1(z1).cuda()\n",
        "            H2 = self.H_net2(z2).cuda()\n",
        "            H3 = self.H_net3(z3).cuda()\n",
        "            H1 = H1.reshape(bs,18,18)\n",
        "            H2 = H2.reshape(bs,18,18)\n",
        "            H3 = H3.reshape(bs,18,18)\n",
        "            z1 = torch.matmul(z1,H1).cuda()\n",
        "            z2 = torch.matmul(z2,H2).cuda()\n",
        "            z3 = torch.matmul(z3,H3).cuda()\n",
        "            # z = torch.cat((z1,z2,z3),dim=2).cuda()\n",
        "            \n",
        "            # H = H.reshape(bs,d,d)\n",
        "            # z = torch.matmul(z,H).cuda()\n",
        "            J1 = batch_jacobian(H_mul1, z1, create_graph=True)\n",
        "            J2 = batch_jacobian(H_mul2, z2, create_graph=True)\n",
        "            J3 = batch_jacobian(H_mul3, z3, create_graph=True)\n",
        "            J_int =-torch.log(torch.abs(torch.det(J)))\n",
        "            loss_reg = loss_reg + torch.squeeze(torch.autograd.grad(J_int, x,torch.ones_like(J_int),allow_unused=True,create_graph= True)[0]).cuda()\n",
        "           \n",
        "        self.loss_reg = loss_reg\n",
        "        self.y = z\n",
        "        y = self.X_net(z)\n",
        "        return y.squeeze(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.9795e+00, -5.2436e+00, -3.2072e-02, -4.6823e+00,  6.9529e-01,\n",
              "         -1.5260e+00, -1.4645e+00, -1.6959e+00, -1.5445e+00, -8.1698e-01,\n",
              "          2.0707e+00, -1.7900e+00, -9.8698e-01, -3.2681e+00,  5.1768e+00,\n",
              "         -5.8131e+00,  4.1742e+00,  2.0826e-01, -9.8656e-01, -2.3452e+00,\n",
              "         -1.4194e+00, -2.4600e+00, -1.3039e+00,  7.7651e-01,  4.9003e-01,\n",
              "         -1.3757e+00, -3.4262e+00,  4.1262e+00, -1.2681e+00,  1.4706e+00,\n",
              "         -3.9701e+00, -3.4830e-01, -1.5868e+00, -7.7940e-01, -8.0162e-01,\n",
              "         -3.7394e-01,  8.7634e-01,  2.7341e+00, -4.4930e+00, -3.5222e+00,\n",
              "          3.0263e-01,  2.3162e-02,  1.5849e+00, -1.4334e+00, -3.2295e+00,\n",
              "         -5.8185e+00,  1.5277e+00,  6.1460e+00, -2.6870e+00,  4.0574e+00,\n",
              "         -2.0177e-01, -6.2287e+00, -7.9765e-01,  1.2575e+00],\n",
              "        [-1.4813e-01, -1.0657e+00, -1.5682e-01, -8.5582e-01,  1.8674e-01,\n",
              "         -3.7624e-01,  7.7718e-03, -2.7648e-01, -3.5015e-01, -2.4804e-01,\n",
              "          4.1152e-01, -4.5987e-01, -2.1486e-01, -7.6695e-01,  9.0514e-01,\n",
              "         -1.0380e+00,  6.8894e-01,  5.9867e-02, -2.1697e-01, -5.6785e-01,\n",
              "         -3.6820e-01, -4.2830e-01, -2.8536e-01,  1.3351e-01, -1.3388e-02,\n",
              "         -3.2835e-01, -6.5187e-01,  7.6071e-01, -3.5777e-01,  3.9216e-01,\n",
              "         -8.1374e-01,  9.1498e-02, -1.4303e-01, -1.0299e-01, -1.3094e-01,\n",
              "          8.2090e-02,  1.5934e-01,  7.2301e-01, -6.6315e-01, -6.5877e-01,\n",
              "          1.8573e-02,  1.4745e-02,  3.4362e-01,  3.6462e-02, -5.9366e-01,\n",
              "         -1.2194e+00,  3.3200e-01,  1.0753e+00, -5.1404e-01,  5.6476e-01,\n",
              "         -4.3284e-02, -1.1056e+00, -1.0707e-01,  1.7119e-01],\n",
              "        [-4.0477e-01, -2.0952e+00, -1.9666e-01, -1.9164e+00,  4.9718e-01,\n",
              "         -6.0741e-01, -4.0310e-01, -3.5964e-01, -5.1948e-01, -2.5614e-01,\n",
              "          7.0460e-01, -8.4408e-01, -5.5534e-01, -1.4914e+00,  1.9736e+00,\n",
              "         -1.9021e+00,  1.3905e+00,  1.3818e-01, -4.5844e-01, -1.1091e+00,\n",
              "         -7.3411e-01, -9.1892e-01, -4.6081e-01,  3.3686e-01, -1.2498e-01,\n",
              "         -6.3313e-01, -1.4032e+00,  1.4159e+00, -8.7757e-01,  5.9369e-01,\n",
              "         -1.5488e+00,  2.6484e-02, -4.0958e-01, -3.3321e-01, -2.9099e-01,\n",
              "          5.3888e-02,  4.4627e-01,  1.2811e+00, -1.5634e+00, -1.3777e+00,\n",
              "         -4.6367e-02,  1.6491e-01,  7.3549e-01, -3.2927e-03, -1.2065e+00,\n",
              "         -2.4430e+00,  6.5216e-01,  2.0440e+00, -9.4130e-01,  1.2246e+00,\n",
              "         -2.0609e-02, -2.0727e+00, -2.1368e-01,  2.9980e-01],\n",
              "        [-2.9888e-01, -1.1772e+00, -1.0292e-01, -7.1281e-01, -2.9921e-02,\n",
              "         -3.7169e-01, -1.5562e-02, -4.6171e-01, -3.6388e-01, -3.7391e-01,\n",
              "          6.8480e-01, -4.7302e-01, -1.1321e-02, -5.9053e-01,  9.1822e-01,\n",
              "         -1.0337e+00,  8.2743e-01,  3.0994e-02, -1.4022e-01, -5.1630e-01,\n",
              "         -3.4791e-01, -4.2740e-01, -2.7445e-01,  1.9437e-01,  1.2061e-01,\n",
              "         -1.9202e-01, -7.7836e-01,  7.8614e-01, -1.0200e-01,  3.4514e-01,\n",
              "         -6.9712e-01,  9.5985e-02, -1.7381e-01, -1.1810e-01, -2.0368e-01,\n",
              "          7.3395e-02,  3.5215e-04,  7.5332e-01, -6.0874e-01, -6.7017e-01,\n",
              "         -3.5764e-02, -9.8490e-02,  2.3282e-01, -1.6753e-01, -6.3807e-01,\n",
              "         -1.0519e+00,  2.7480e-01,  1.2521e+00, -4.9057e-01,  6.1596e-01,\n",
              "         -8.6526e-02, -1.2008e+00, -1.4868e-01,  2.7432e-01]], device='cuda:0',\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = Net(1).to(torch.device(\"cuda\"))\n",
        "net(X[0:4])\n",
        "net.loss_reg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "cCPV_zl0O3Mc"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# criterion = nn.BCELoss(reduction= 'none')\n",
        "def my_loss(y_pred,y_train,reg_loss):\n",
        "    loss = criterion(y_pred,y_train) +reg_loss\n",
        "    return loss\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "torch.autograd.set_detect_anomaly(False)\n",
        "net_5 = Net(1)\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "#loss function for multiclass classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model_priv(net,trainloader,optimizer,epochs,h,rate=10,device= torch.device('cpu'),print_cond = True,only_reg_flag=0):\n",
        "    # scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "    net = net.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "        # scheduler.step()\n",
        "        running_loss = 0.0\n",
        "        \n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            \n",
        "            inputs = data[0].to(device)\n",
        "            inputs.requires_grad = True\n",
        "            labels = data[1].to(device)\n",
        "            f = py_kde(inputs,inputs,h)\n",
        "            f_der = py_kde_der(f,inputs)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            if(only_reg_flag):\n",
        "                loss = torch.norm(f_der/f.view(f.shape[0],1)+ net.loss_reg,dim=1).sum()\n",
        "                \n",
        "            else:\n",
        "                loss = criterion(torch.squeeze(outputs),torch.squeeze(labels)) + torch.norm(f_der/f.view(f.shape[0],1)+ net.loss_reg,dim=1).sum()\n",
        "            loss.backward(retain_graph=True)\n",
        "            optimizer.step()\n",
        "          \n",
        "\n",
        "      \n",
        "            # print statistics\n",
        "            # print(loss.sum().shape)\n",
        "            running_loss += loss.sum().detach().cpu()\n",
        "            # if i % 100 == 99:    # print every 2000 mini-batches\n",
        "            if((i+1)%10==0):\n",
        "                if(print_cond):\n",
        "                    print('%d loss: %.10f' %\n",
        "                            (epoch + 1, running_loss /(rate*10*trainloader.batch_size)))\n",
        "                    running_loss = 0.0\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_priv = torch.utils.data.TensorDataset(X,Y)\n",
        "\n",
        "trainloader_priv = torch.utils.data.DataLoader(train_priv, batch_size=3000,\n",
        "                                          shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 loss: 3.0203502178\n",
            "1 loss: 2.6952309608\n",
            "1 loss: 2.6554479599\n",
            "1 loss: 2.6397027969\n",
            "1 loss: 2.6396825314\n",
            "2 loss: 2.6125872135\n",
            "2 loss: 2.6190793514\n",
            "2 loss: 2.6090741158\n",
            "2 loss: 2.6050832272\n",
            "2 loss: 2.5892179012\n",
            "3 loss: 2.6847579479\n",
            "3 loss: 2.6296780109\n",
            "3 loss: 2.6157927513\n",
            "3 loss: 2.6220626831\n",
            "3 loss: 2.6027367115\n",
            "4 loss: 2.6036062241\n",
            "4 loss: 2.5878219604\n",
            "4 loss: 2.5710530281\n",
            "4 loss: 2.5730116367\n",
            "4 loss: 2.5733473301\n",
            "5 loss: 2.5422060490\n",
            "5 loss: 2.5912210941\n",
            "5 loss: 2.5702314377\n",
            "5 loss: 2.5445830822\n",
            "5 loss: 2.5285699368\n",
            "6 loss: 2.5017125607\n",
            "6 loss: 2.5775489807\n",
            "6 loss: 2.5178527832\n",
            "6 loss: 2.4715538025\n",
            "6 loss: 2.4923946857\n",
            "7 loss: 2.4434075356\n",
            "7 loss: 2.4138438702\n",
            "7 loss: 2.4062497616\n",
            "7 loss: 2.4662039280\n",
            "7 loss: 2.4044909477\n",
            "8 loss: 2.3660125732\n",
            "8 loss: 2.3306806087\n",
            "8 loss: 2.3420295715\n",
            "8 loss: 2.3207480907\n",
            "8 loss: 2.3021810055\n",
            "9 loss: 2.3315722942\n",
            "9 loss: 2.2613692284\n",
            "9 loss: 2.2387456894\n",
            "9 loss: 2.2918112278\n",
            "9 loss: 2.2852385044\n",
            "10 loss: 2.2373528481\n",
            "10 loss: 2.2767724991\n",
            "10 loss: 2.1983895302\n",
            "10 loss: 2.1884825230\n",
            "10 loss: 2.1373984814\n",
            "11 loss: 2.5458960533\n",
            "11 loss: 2.3566055298\n",
            "11 loss: 2.2712588310\n",
            "11 loss: 2.1830906868\n",
            "11 loss: 2.1366863251\n",
            "12 loss: 2.1440956593\n",
            "12 loss: 2.0955538750\n",
            "12 loss: 2.1002376080\n",
            "12 loss: 2.0758450031\n",
            "12 loss: 2.0435068607\n",
            "13 loss: 2.1267471313\n",
            "13 loss: 2.0625669956\n",
            "13 loss: 1.9989869595\n",
            "13 loss: 1.9781730175\n",
            "13 loss: 1.9722805023\n",
            "14 loss: 2.1146175861\n",
            "14 loss: 2.0444211960\n",
            "14 loss: 2.0928862095\n",
            "14 loss: 1.9789642096\n",
            "14 loss: 2.0069403648\n",
            "15 loss: 2.0024170876\n",
            "15 loss: 1.9261988401\n",
            "15 loss: 1.8953697681\n",
            "15 loss: 1.8769781590\n",
            "15 loss: 1.9236146212\n",
            "16 loss: 1.9468578100\n",
            "16 loss: 1.8419458866\n",
            "16 loss: 1.8358001709\n",
            "16 loss: 1.7831208706\n",
            "16 loss: 1.8130097389\n",
            "17 loss: 2.1277787685\n",
            "17 loss: 1.8897691965\n",
            "17 loss: 1.7773382664\n",
            "17 loss: 1.7373800278\n",
            "17 loss: 1.7269198895\n",
            "18 loss: 1.8023781776\n",
            "18 loss: 1.7224539518\n",
            "18 loss: 1.6930154562\n",
            "18 loss: 1.6539995670\n",
            "18 loss: 1.6666593552\n",
            "19 loss: 1.7384574413\n",
            "19 loss: 1.6923929453\n",
            "19 loss: 1.6549582481\n",
            "19 loss: 1.6048779488\n",
            "19 loss: 1.6280783415\n",
            "20 loss: 1.7924948931\n",
            "20 loss: 1.6884402037\n",
            "20 loss: 1.6220358610\n",
            "20 loss: 1.5908206701\n",
            "20 loss: 1.6057683229\n",
            "21 loss: 1.7374103069\n",
            "21 loss: 1.6413611174\n",
            "21 loss: 1.5870825052\n",
            "21 loss: 1.5428800583\n",
            "21 loss: 1.5238415003\n",
            "22 loss: 1.8645662069\n",
            "22 loss: 1.7154333591\n",
            "22 loss: 1.5855607986\n",
            "22 loss: 1.5234764814\n",
            "22 loss: 1.5169032812\n",
            "23 loss: 1.5966129303\n",
            "23 loss: 1.5708754063\n",
            "23 loss: 1.5715968609\n",
            "23 loss: 1.5202702284\n",
            "23 loss: 1.4863542318\n",
            "24 loss: 1.4996705055\n",
            "24 loss: 1.4506282806\n",
            "24 loss: 1.4438203573\n",
            "24 loss: 1.4661310911\n",
            "24 loss: 1.4743653536\n",
            "25 loss: 1.5295352936\n",
            "25 loss: 1.4626700878\n",
            "25 loss: 1.4174320698\n",
            "25 loss: 1.4064760208\n",
            "25 loss: 1.4104745388\n",
            "26 loss: 1.5567613840\n",
            "26 loss: 1.6117193699\n",
            "26 loss: 2.0922458172\n",
            "26 loss: nan\n",
            "26 loss: nan\n",
            "27 loss: nan\n",
            "27 loss: nan\n",
            "27 loss: nan\n",
            "27 loss: nan\n",
            "27 loss: nan\n",
            "28 loss: nan\n",
            "28 loss: nan\n",
            "28 loss: nan\n",
            "28 loss: nan\n",
            "28 loss: nan\n",
            "29 loss: nan\n",
            "29 loss: nan\n",
            "29 loss: nan\n",
            "29 loss: nan\n",
            "29 loss: nan\n",
            "30 loss: nan\n",
            "30 loss: nan\n",
            "30 loss: nan\n",
            "30 loss: nan\n",
            "30 loss: nan\n"
          ]
        }
      ],
      "source": [
        "train_priv = torch.utils.data.TensorDataset(X,Y)\n",
        "\n",
        "trainloader_priv = torch.utils.data.DataLoader(train_priv, batch_size=1000,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "net_5 = Net(1)\n",
        "optim = torch.optim.Adam(net_5.parameters(),lr=0.003)\n",
        "train_model_priv(net_5,trainloader_priv,optim,30,h=0.82,rate=1,device=torch.device('cuda'),only_reg_flag=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# # criterion = nn.BCELoss(reduction= 'none')\n",
        "# def my_loss(y_pred,y_train,reg_loss):\n",
        "#     loss = criterion(y_pred,y_train) +reg_loss\n",
        "#     return loss\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "torch.autograd.set_detect_anomaly(False)\n",
        "net_5 = Net(1)\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "#loss function for multiclass classification\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch number: 0   Iteration number: 0  LR : 1e-05  Loss: tensor(3.6211, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(3.6211, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 400  LR : 1e-05  Loss: tensor(2.9169, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.9169, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 800  LR : 1e-05  Loss: tensor(3.3790, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(3.3790, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 1200  LR : 1e-05  Loss: tensor(3.1163, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(3.1163, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 1600  LR : 1e-05  Loss: tensor(2.4735, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4735, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 2000  LR : 1e-05  Loss: tensor(2.5010, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5010, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 2400  LR : 1e-05  Loss: tensor(2.4700, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4700, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 2800  LR : 1e-05  Loss: tensor(2.4567, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4567, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 3200  LR : 1e-05  Loss: tensor(2.5962, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5962, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 3600  LR : 1e-05  Loss: tensor(2.5803, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5803, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 4000  LR : 1e-05  Loss: tensor(2.6602, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6602, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 4400  LR : 1e-05  Loss: tensor(2.7162, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.7162, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 4800  LR : 1e-05  Loss: tensor(2.6594, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6594, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 5200  LR : 1e-05  Loss: tensor(2.4527, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4527, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 5600  LR : 1e-05  Loss: tensor(2.4650, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4650, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 6000  LR : 1e-05  Loss: tensor(2.3843, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3843, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 6400  LR : 1e-05  Loss: tensor(2.4107, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4107, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 6800  LR : 1e-05  Loss: tensor(2.3968, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3968, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 7200  LR : 1e-05  Loss: tensor(2.3809, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3809, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 7600  LR : 1e-05  Loss: tensor(2.3657, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3657, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 8000  LR : 1e-05  Loss: tensor(2.3813, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3813, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 8400  LR : 1e-05  Loss: tensor(2.4650, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4650, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 8800  LR : 1e-05  Loss: tensor(2.3099, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3099, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 9200  LR : 1e-05  Loss: tensor(2.3177, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3177, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 9600  LR : 1e-05  Loss: tensor(2.3640, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3640, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 10000  LR : 1e-05  Loss: tensor(2.4694, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4694, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 10400  LR : 1e-05  Loss: tensor(2.4158, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4158, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 10800  LR : 1e-05  Loss: tensor(2.3031, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3031, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 11200  LR : 1e-05  Loss: tensor(2.2900, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2900, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 11600  LR : 1e-05  Loss: tensor(2.4129, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4129, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 12000  LR : 1e-05  Loss: tensor(2.5405, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5405, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 12400  LR : 1e-05  Loss: tensor(2.5626, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5626, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 12800  LR : 1e-05  Loss: tensor(2.3830, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3830, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 13200  LR : 1e-05  Loss: tensor(2.4045, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4045, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 13600  LR : 1e-05  Loss: tensor(2.2581, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2581, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 14000  LR : 1e-05  Loss: tensor(2.2886, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2886, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 14400  LR : 1e-05  Loss: tensor(2.3779, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3779, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 14800  LR : 1e-05  Loss: tensor(2.3267, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3267, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 15200  LR : 1e-05  Loss: tensor(2.4159, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4159, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 15600  LR : 1e-05  Loss: tensor(2.3234, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3234, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 16000  LR : 1e-05  Loss: tensor(2.1369, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.1369, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 16400  LR : 1e-05  Loss: tensor(2.3436, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3436, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 16800  LR : 1e-05  Loss: tensor(2.4763, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4763, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 17200  LR : 1e-05  Loss: tensor(2.5376, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5376, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 17600  LR : 1e-05  Loss: tensor(2.5094, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5094, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 18000  LR : 1e-05  Loss: tensor(2.2512, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2512, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 18400  LR : 1e-05  Loss: tensor(2.2605, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2605, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 18800  LR : 1e-05  Loss: tensor(2.3562, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3562, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 19200  LR : 1e-05  Loss: tensor(2.4564, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4564, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 19600  LR : 1e-05  Loss: tensor(2.3694, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3694, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 20000  LR : 1e-05  Loss: tensor(2.3740, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3740, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 20400  LR : 1e-05  Loss: tensor(2.3181, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3181, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 20800  LR : 1e-05  Loss: tensor(2.3157, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3157, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 21200  LR : 1e-05  Loss: tensor(2.2916, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2916, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 21600  LR : 1e-05  Loss: tensor(2.4217, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4217, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 22000  LR : 1e-05  Loss: tensor(2.4132, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4132, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 22400  LR : 1e-05  Loss: tensor(2.3316, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3316, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 22800  LR : 1e-05  Loss: tensor(2.4140, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4140, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 23200  LR : 1e-05  Loss: tensor(2.5107, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5107, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 23600  LR : 1e-05  Loss: tensor(2.4195, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4195, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 24000  LR : 1e-05  Loss: tensor(2.3443, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3443, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 24400  LR : 1e-05  Loss: tensor(2.2295, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2295, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 24800  LR : 1e-05  Loss: tensor(2.2825, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2825, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 25200  LR : 1e-05  Loss: tensor(2.4105, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4105, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 25600  LR : 1e-05  Loss: tensor(2.3179, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3179, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 26000  LR : 1e-05  Loss: tensor(2.3005, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3005, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 26400  LR : 1e-05  Loss: tensor(2.3313, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3313, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 26800  LR : 1e-05  Loss: tensor(2.2632, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2632, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 27200  LR : 1e-05  Loss: tensor(2.2130, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2130, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 27600  LR : 1e-05  Loss: tensor(2.3560, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3560, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 28000  LR : 1e-05  Loss: tensor(2.3889, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3889, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 28400  LR : 1e-05  Loss: tensor(2.4668, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4668, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 28800  LR : 1e-05  Loss: tensor(2.4252, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4252, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 29200  LR : 1e-05  Loss: tensor(2.3155, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3155, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 29600  LR : 1e-05  Loss: tensor(2.3193, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3193, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 30000  LR : 1e-05  Loss: tensor(2.2895, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2895, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 30400  LR : 1e-05  Loss: tensor(2.3043, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3043, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 30800  LR : 1e-05  Loss: tensor(2.3253, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3253, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 31200  LR : 1e-05  Loss: tensor(2.4515, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4515, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 31600  LR : 1e-05  Loss: tensor(2.5382, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5382, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 32000  LR : 1e-05  Loss: tensor(2.4832, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4832, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 32400  LR : 1e-05  Loss: tensor(2.4461, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4461, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 32800  LR : 1e-05  Loss: tensor(2.3972, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3972, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 33200  LR : 1e-05  Loss: tensor(2.5027, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5027, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 33600  LR : 1e-05  Loss: tensor(2.4967, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4967, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 34000  LR : 1e-05  Loss: tensor(2.5514, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5514, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 34400  LR : 1e-05  Loss: tensor(2.5942, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5942, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 34800  LR : 1e-05  Loss: tensor(2.5942, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5942, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 35200  LR : 1e-05  Loss: tensor(2.6402, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6402, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 35600  LR : 1e-05  Loss: tensor(2.6710, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6710, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 36000  LR : 1e-05  Loss: tensor(2.6494, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6494, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 36400  LR : 1e-05  Loss: tensor(2.7251, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.7251, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 36800  LR : 1e-05  Loss: tensor(2.6732, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6732, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 37200  LR : 1e-05  Loss: tensor(2.6929, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6929, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 37600  LR : 1e-05  Loss: tensor(2.5853, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5853, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 38000  LR : 1e-05  Loss: tensor(2.5936, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5936, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 38400  LR : 1e-05  Loss: tensor(2.5418, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5418, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 38800  LR : 1e-05  Loss: tensor(2.6643, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6643, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 39200  LR : 1e-05  Loss: tensor(2.6820, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6820, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 39600  LR : 1e-05  Loss: tensor(2.6550, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6550, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 40000  LR : 1e-05  Loss: tensor(2.6392, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6392, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 40400  LR : 1e-05  Loss: tensor(2.6219, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6219, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 40800  LR : 1e-05  Loss: tensor(2.5538, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5538, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 41200  LR : 1e-05  Loss: tensor(2.4655, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4655, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 41600  LR : 1e-05  Loss: tensor(2.4325, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4325, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 42000  LR : 1e-05  Loss: tensor(2.5342, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5342, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 42400  LR : 1e-05  Loss: tensor(2.4876, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4876, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 42800  LR : 1e-05  Loss: tensor(2.3579, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3579, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 43200  LR : 1e-05  Loss: tensor(2.4139, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4139, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 43600  LR : 1e-05  Loss: tensor(2.1795, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.1795, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 44000  LR : 1e-05  Loss: tensor(2.1863, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.1863, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 44400  LR : 1e-05  Loss: tensor(2.2244, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2244, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 44800  LR : 1e-05  Loss: tensor(2.2052, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2052, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 45200  LR : 1e-05  Loss: tensor(2.2604, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2604, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 45600  LR : 1e-05  Loss: tensor(2.3427, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3427, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 46000  LR : 1e-05  Loss: tensor(2.3614, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3614, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 46400  LR : 1e-05  Loss: tensor(2.3918, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3918, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 46800  LR : 1e-05  Loss: tensor(2.3933, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3933, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 47200  LR : 1e-05  Loss: tensor(2.2065, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2065, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 47600  LR : 1e-05  Loss: tensor(2.0622, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.0622, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 48000  LR : 1e-05  Loss: tensor(2.1929, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.1929, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 48400  LR : 1e-05  Loss: tensor(2.3367, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3367, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 48800  LR : 1e-05  Loss: tensor(2.2794, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2794, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 49200  LR : 1e-05  Loss: tensor(2.4613, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4613, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 49600  LR : 1e-05  Loss: tensor(2.4714, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4714, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 50000  LR : 1e-05  Loss: tensor(2.5241, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5241, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 50400  LR : 1e-05  Loss: tensor(2.4812, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4812, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 50800  LR : 1e-05  Loss: tensor(2.3896, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3896, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 51200  LR : 1e-05  Loss: tensor(2.3953, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3953, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 51600  LR : 1e-05  Loss: tensor(2.3290, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3290, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 52000  LR : 1e-05  Loss: tensor(2.3253, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3253, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 52400  LR : 1e-05  Loss: tensor(2.3935, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3935, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 52800  LR : 1e-05  Loss: tensor(2.3453, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3453, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 53200  LR : 1e-05  Loss: tensor(2.2445, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2445, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 53600  LR : 1e-05  Loss: tensor(2.0642, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.0642, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 54000  LR : 1e-05  Loss: tensor(2.0794, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.0794, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 54400  LR : 1e-05  Loss: tensor(1.9739, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(1.9739, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 54800  LR : 1e-05  Loss: tensor(1.9035, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(1.9035, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 55200  LR : 1e-05  Loss: tensor(2.0700, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.0700, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 55600  LR : 1e-05  Loss: tensor(2.0080, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.0080, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 56000  LR : 1e-05  Loss: tensor(1.9298, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(1.9298, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 56400  LR : 1e-05  Loss: tensor(2.2591, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2591, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 56800  LR : 1e-05  Loss: tensor(1.8942, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(1.8942, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 57200  LR : 1e-05  Loss: tensor(1.8745, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(1.8745, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 57600  LR : 1e-05  Loss: tensor(1.8795, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(1.8795, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 0   Iteration number: 58000  LR : 1e-05  Loss: tensor(0.4406, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.4406, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 0  LR : 1e-05  Loss: tensor(2.9545, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.9545, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 400  LR : 1e-05  Loss: tensor(2.7791, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.7791, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 800  LR : 1e-05  Loss: tensor(3.2933, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(3.2933, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 1200  LR : 1e-05  Loss: tensor(3.0239, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(3.0239, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 1600  LR : 1e-05  Loss: tensor(2.3913, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3913, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 2000  LR : 1e-05  Loss: tensor(2.4198, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4198, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 2400  LR : 1e-05  Loss: tensor(2.3949, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3949, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 2800  LR : 1e-05  Loss: tensor(2.3794, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3794, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 3200  LR : 1e-05  Loss: tensor(2.5302, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5302, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 3600  LR : 1e-05  Loss: tensor(2.5181, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5181, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 4000  LR : 1e-05  Loss: tensor(2.6032, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6032, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 4400  LR : 1e-05  Loss: tensor(2.6601, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6601, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 4800  LR : 1e-05  Loss: tensor(2.6057, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6057, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 5200  LR : 1e-05  Loss: tensor(2.4011, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4011, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 5600  LR : 1e-05  Loss: tensor(2.4152, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4152, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 6000  LR : 1e-05  Loss: tensor(2.3357, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3357, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 6400  LR : 1e-05  Loss: tensor(2.3630, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3630, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 6800  LR : 1e-05  Loss: tensor(2.3480, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3480, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 7200  LR : 1e-05  Loss: tensor(2.3307, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3307, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 7600  LR : 1e-05  Loss: tensor(2.3194, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3194, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 8000  LR : 1e-05  Loss: tensor(2.3349, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3349, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 8400  LR : 1e-05  Loss: tensor(2.4199, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4199, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 8800  LR : 1e-05  Loss: tensor(2.2650, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2650, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 9200  LR : 1e-05  Loss: tensor(2.2729, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2729, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 9600  LR : 1e-05  Loss: tensor(2.3190, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3190, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 10000  LR : 1e-05  Loss: tensor(2.4261, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4261, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 10400  LR : 1e-05  Loss: tensor(2.3712, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3712, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 10800  LR : 1e-05  Loss: tensor(2.2615, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2615, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 11200  LR : 1e-05  Loss: tensor(2.2436, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2436, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 11600  LR : 1e-05  Loss: tensor(2.3714, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3714, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 12000  LR : 1e-05  Loss: tensor(2.4998, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4998, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 12400  LR : 1e-05  Loss: tensor(2.5239, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5239, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 12800  LR : 1e-05  Loss: tensor(2.3388, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3388, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 13200  LR : 1e-05  Loss: tensor(2.3597, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3597, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 13600  LR : 1e-05  Loss: tensor(2.2140, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2140, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 14000  LR : 1e-05  Loss: tensor(2.2428, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2428, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 14400  LR : 1e-05  Loss: tensor(2.3330, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3330, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 14800  LR : 1e-05  Loss: tensor(2.2867, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2867, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 15200  LR : 1e-05  Loss: tensor(2.3749, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3749, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 15600  LR : 1e-05  Loss: tensor(2.2723, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2723, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 16000  LR : 1e-05  Loss: tensor(2.0911, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.0911, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 16400  LR : 1e-05  Loss: tensor(2.2956, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2956, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 16800  LR : 1e-05  Loss: tensor(2.4172, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4172, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 17200  LR : 1e-05  Loss: tensor(2.4780, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4780, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 17600  LR : 1e-05  Loss: tensor(2.4611, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4611, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 18000  LR : 1e-05  Loss: tensor(2.2143, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2143, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 18400  LR : 1e-05  Loss: tensor(2.2155, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2155, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 18800  LR : 1e-05  Loss: tensor(2.2905, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2905, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 19200  LR : 1e-05  Loss: tensor(2.5088, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5088, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 19600  LR : 1e-05  Loss: tensor(2.4040, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4040, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 20000  LR : 1e-05  Loss: tensor(2.4272, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4272, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 20400  LR : 1e-05  Loss: tensor(2.3609, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3609, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 20800  LR : 1e-05  Loss: tensor(2.3536, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3536, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 21200  LR : 1e-05  Loss: tensor(2.3249, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3249, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 21600  LR : 1e-05  Loss: tensor(2.4504, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4504, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 22000  LR : 1e-05  Loss: tensor(2.4398, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4398, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 22400  LR : 1e-05  Loss: tensor(2.3574, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3574, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 22800  LR : 1e-05  Loss: tensor(2.4333, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4333, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 23200  LR : 1e-05  Loss: tensor(2.5281, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5281, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 23600  LR : 1e-05  Loss: tensor(2.4370, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4370, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 24000  LR : 1e-05  Loss: tensor(2.3625, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3625, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 24400  LR : 1e-05  Loss: tensor(2.2538, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2538, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 24800  LR : 1e-05  Loss: tensor(2.3067, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3067, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 25200  LR : 1e-05  Loss: tensor(2.4313, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4313, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 25600  LR : 1e-05  Loss: tensor(2.3363, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3363, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 26000  LR : 1e-05  Loss: tensor(2.3225, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3225, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 26400  LR : 1e-05  Loss: tensor(2.3507, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3507, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 26800  LR : 1e-05  Loss: tensor(2.2874, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2874, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 27200  LR : 1e-05  Loss: tensor(2.2392, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2392, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 27600  LR : 1e-05  Loss: tensor(2.3802, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3802, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 28000  LR : 1e-05  Loss: tensor(2.4121, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4121, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 28400  LR : 1e-05  Loss: tensor(2.4901, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4901, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 28800  LR : 1e-05  Loss: tensor(2.4425, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4425, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 29200  LR : 1e-05  Loss: tensor(2.3329, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3329, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 29600  LR : 1e-05  Loss: tensor(2.3336, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3336, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 30000  LR : 1e-05  Loss: tensor(2.3019, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3019, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 30400  LR : 1e-05  Loss: tensor(2.3174, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3174, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 30800  LR : 1e-05  Loss: tensor(2.3349, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3349, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 31200  LR : 1e-05  Loss: tensor(2.4588, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4588, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 31600  LR : 1e-05  Loss: tensor(2.5464, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5464, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 32000  LR : 1e-05  Loss: tensor(2.4914, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4914, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 32400  LR : 1e-05  Loss: tensor(2.4539, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4539, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 32800  LR : 1e-05  Loss: tensor(2.4049, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4049, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 33200  LR : 1e-05  Loss: tensor(2.5113, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5113, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 33600  LR : 1e-05  Loss: tensor(2.5052, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5052, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 34000  LR : 1e-05  Loss: tensor(2.5597, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5597, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 34400  LR : 1e-05  Loss: tensor(2.6033, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6033, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 34800  LR : 1e-05  Loss: tensor(2.6033, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6033, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 35200  LR : 1e-05  Loss: tensor(2.6508, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6508, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 35600  LR : 1e-05  Loss: tensor(2.6804, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6804, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 36000  LR : 1e-05  Loss: tensor(2.6573, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6573, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 36400  LR : 1e-05  Loss: tensor(2.7307, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.7307, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 36800  LR : 1e-05  Loss: tensor(2.6803, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6803, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 37200  LR : 1e-05  Loss: tensor(2.7022, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.7022, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 37600  LR : 1e-05  Loss: tensor(2.5953, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5953, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 38000  LR : 1e-05  Loss: tensor(2.6006, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6006, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 38400  LR : 1e-05  Loss: tensor(2.5474, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5474, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 38800  LR : 1e-05  Loss: tensor(2.6700, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6700, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 39200  LR : 1e-05  Loss: tensor(2.6886, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6886, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 39600  LR : 1e-05  Loss: tensor(2.6614, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6614, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 40000  LR : 1e-05  Loss: tensor(2.6454, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6454, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 40400  LR : 1e-05  Loss: tensor(2.6291, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6291, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 40800  LR : 1e-05  Loss: tensor(2.5622, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5622, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 41200  LR : 1e-05  Loss: tensor(2.4736, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4736, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 41600  LR : 1e-05  Loss: tensor(2.4416, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4416, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 42000  LR : 1e-05  Loss: tensor(2.5407, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5407, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 42400  LR : 1e-05  Loss: tensor(2.4955, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4955, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 42800  LR : 1e-05  Loss: tensor(2.3704, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3704, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 43200  LR : 1e-05  Loss: tensor(2.4262, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4262, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 43600  LR : 1e-05  Loss: tensor(2.1890, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.1890, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 44000  LR : 1e-05  Loss: tensor(2.1950, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.1950, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 44400  LR : 1e-05  Loss: tensor(2.2327, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2327, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 44800  LR : 1e-05  Loss: tensor(2.2157, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2157, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 45200  LR : 1e-05  Loss: tensor(2.2702, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2702, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 45600  LR : 1e-05  Loss: tensor(2.3542, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3542, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 46000  LR : 1e-05  Loss: tensor(2.3747, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3747, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 46400  LR : 1e-05  Loss: tensor(2.4061, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4061, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 46800  LR : 1e-05  Loss: tensor(2.4067, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4067, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 47200  LR : 1e-05  Loss: tensor(2.2242, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2242, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 47600  LR : 1e-05  Loss: tensor(2.0780, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.0780, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 48000  LR : 1e-05  Loss: tensor(2.1987, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.1987, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 48400  LR : 1e-05  Loss: tensor(2.3469, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3469, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 48800  LR : 1e-05  Loss: tensor(2.2907, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2907, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 49200  LR : 1e-05  Loss: tensor(2.4742, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4742, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 49600  LR : 1e-05  Loss: tensor(2.4851, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4851, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 50000  LR : 1e-05  Loss: tensor(2.5377, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5377, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 50400  LR : 1e-05  Loss: tensor(2.4948, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4948, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 50800  LR : 1e-05  Loss: tensor(2.4021, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4021, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 51200  LR : 1e-05  Loss: tensor(2.4097, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4097, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 51600  LR : 1e-05  Loss: tensor(2.3425, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3425, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 52000  LR : 1e-05  Loss: tensor(2.3396, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3396, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 52400  LR : 1e-05  Loss: tensor(2.4090, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4090, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 52800  LR : 1e-05  Loss: tensor(2.3607, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3607, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 53200  LR : 1e-05  Loss: tensor(2.2567, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2567, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 53600  LR : 1e-05  Loss: tensor(2.0742, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.0742, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 54000  LR : 1e-05  Loss: tensor(2.0904, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.0904, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 54400  LR : 1e-05  Loss: tensor(1.9865, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(1.9865, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 54800  LR : 1e-05  Loss: tensor(1.9127, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(1.9127, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 55200  LR : 1e-05  Loss: tensor(2.0811, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.0811, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 55600  LR : 1e-05  Loss: tensor(2.0159, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.0159, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 56000  LR : 1e-05  Loss: tensor(1.9382, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(1.9382, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 56400  LR : 1e-05  Loss: tensor(2.2649, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2649, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 56800  LR : 1e-05  Loss: tensor(1.8995, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(1.8995, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 57200  LR : 1e-05  Loss: tensor(1.8784, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(1.8784, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 57600  LR : 1e-05  Loss: tensor(1.8846, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(1.8846, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 1   Iteration number: 58000  LR : 1e-05  Loss: tensor(0.4413, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.4413, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 0  LR : 1e-05  Loss: tensor(2.9626, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.9626, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 400  LR : 1e-05  Loss: tensor(2.7905, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.7905, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 800  LR : 1e-05  Loss: tensor(3.3086, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(3.3086, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 1200  LR : 1e-05  Loss: tensor(3.0367, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(3.0367, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 1600  LR : 1e-05  Loss: tensor(2.3905, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3905, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 2000  LR : 1e-05  Loss: tensor(2.4220, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4220, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 2400  LR : 1e-05  Loss: tensor(2.3994, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3994, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 2800  LR : 1e-05  Loss: tensor(2.3854, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3854, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 3200  LR : 1e-05  Loss: tensor(2.5369, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5369, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 3600  LR : 1e-05  Loss: tensor(2.5247, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5247, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 4000  LR : 1e-05  Loss: tensor(2.6095, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6095, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 4400  LR : 1e-05  Loss: tensor(2.6659, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6659, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 4800  LR : 1e-05  Loss: tensor(2.6116, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6116, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 5200  LR : 1e-05  Loss: tensor(2.4062, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4062, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 5600  LR : 1e-05  Loss: tensor(2.4209, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4209, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 6000  LR : 1e-05  Loss: tensor(2.3406, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3406, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 6400  LR : 1e-05  Loss: tensor(2.3679, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3679, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 6800  LR : 1e-05  Loss: tensor(2.3561, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3561, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 7200  LR : 1e-05  Loss: tensor(2.3399, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3399, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 7600  LR : 1e-05  Loss: tensor(2.3277, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3277, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 8000  LR : 1e-05  Loss: tensor(2.3452, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3452, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 8400  LR : 1e-05  Loss: tensor(2.4298, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4298, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 8800  LR : 1e-05  Loss: tensor(2.2753, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2753, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 9200  LR : 1e-05  Loss: tensor(2.2845, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2845, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 9600  LR : 1e-05  Loss: tensor(2.3317, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3317, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 10000  LR : 1e-05  Loss: tensor(2.4390, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4390, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 10400  LR : 1e-05  Loss: tensor(2.3849, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3849, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 10800  LR : 1e-05  Loss: tensor(2.2737, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2737, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 11200  LR : 1e-05  Loss: tensor(2.2596, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2596, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 11600  LR : 1e-05  Loss: tensor(2.3853, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3853, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 12000  LR : 1e-05  Loss: tensor(2.5155, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5155, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 12400  LR : 1e-05  Loss: tensor(2.5395, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5395, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 12800  LR : 1e-05  Loss: tensor(2.3580, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3580, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 13200  LR : 1e-05  Loss: tensor(2.3807, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3807, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 13600  LR : 1e-05  Loss: tensor(2.2335, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2335, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 14000  LR : 1e-05  Loss: tensor(2.2647, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2647, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 14400  LR : 1e-05  Loss: tensor(2.3542, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3542, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 14800  LR : 1e-05  Loss: tensor(2.3035, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3035, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 15200  LR : 1e-05  Loss: tensor(2.3944, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3944, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 15600  LR : 1e-05  Loss: tensor(2.3013, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3013, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 16000  LR : 1e-05  Loss: tensor(2.1139, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.1139, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 16400  LR : 1e-05  Loss: tensor(2.3227, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3227, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 16800  LR : 1e-05  Loss: tensor(2.4586, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4586, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 17200  LR : 1e-05  Loss: tensor(2.5199, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5199, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 17600  LR : 1e-05  Loss: tensor(2.4934, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4934, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 18000  LR : 1e-05  Loss: tensor(2.2366, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2366, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 18400  LR : 1e-05  Loss: tensor(2.2452, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2452, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 18800  LR : 1e-05  Loss: tensor(2.3401, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3401, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 19200  LR : 1e-05  Loss: tensor(2.4408, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4408, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 19600  LR : 1e-05  Loss: tensor(2.3538, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3538, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 20000  LR : 1e-05  Loss: tensor(2.3595, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3595, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 20400  LR : 1e-05  Loss: tensor(2.3050, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3050, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 20800  LR : 1e-05  Loss: tensor(2.3029, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3029, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 21200  LR : 1e-05  Loss: tensor(2.2802, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2802, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 21600  LR : 1e-05  Loss: tensor(2.4080, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4080, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 22000  LR : 1e-05  Loss: tensor(2.4007, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4007, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 22400  LR : 1e-05  Loss: tensor(2.3180, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3180, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 22800  LR : 1e-05  Loss: tensor(2.4006, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4006, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 23200  LR : 1e-05  Loss: tensor(2.4984, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4984, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 23600  LR : 1e-05  Loss: tensor(2.4067, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4067, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 24000  LR : 1e-05  Loss: tensor(2.3310, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3310, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 24400  LR : 1e-05  Loss: tensor(2.2178, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2178, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 24800  LR : 1e-05  Loss: tensor(2.2723, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2723, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 25200  LR : 1e-05  Loss: tensor(2.4000, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4000, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 25600  LR : 1e-05  Loss: tensor(2.3070, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3070, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 26000  LR : 1e-05  Loss: tensor(2.2906, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2906, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 26400  LR : 1e-05  Loss: tensor(2.3217, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3217, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 26800  LR : 1e-05  Loss: tensor(2.2556, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2556, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 27200  LR : 1e-05  Loss: tensor(2.2062, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2062, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 27600  LR : 1e-05  Loss: tensor(2.3496, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3496, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 28000  LR : 1e-05  Loss: tensor(2.3820, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3820, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 28400  LR : 1e-05  Loss: tensor(2.4610, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4610, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 28800  LR : 1e-05  Loss: tensor(2.4179, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4179, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 29200  LR : 1e-05  Loss: tensor(2.3074, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3074, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 29600  LR : 1e-05  Loss: tensor(2.3098, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3098, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 30000  LR : 1e-05  Loss: tensor(2.2796, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2796, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 30400  LR : 1e-05  Loss: tensor(2.2957, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2957, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 30800  LR : 1e-05  Loss: tensor(2.3160, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3160, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 31200  LR : 1e-05  Loss: tensor(2.4428, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4428, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 31600  LR : 1e-05  Loss: tensor(2.5309, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5309, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 32000  LR : 1e-05  Loss: tensor(2.4763, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4763, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 32400  LR : 1e-05  Loss: tensor(2.4390, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4390, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 32800  LR : 1e-05  Loss: tensor(2.3899, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3899, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 33200  LR : 1e-05  Loss: tensor(2.4967, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4967, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 33600  LR : 1e-05  Loss: tensor(2.4908, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4908, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 34000  LR : 1e-05  Loss: tensor(2.5462, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5462, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 34400  LR : 1e-05  Loss: tensor(2.5898, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5898, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 34800  LR : 1e-05  Loss: tensor(2.5896, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5896, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 35200  LR : 1e-05  Loss: tensor(2.6367, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6367, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 35600  LR : 1e-05  Loss: tensor(2.6679, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6679, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 36000  LR : 1e-05  Loss: tensor(2.6449, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6449, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 36400  LR : 1e-05  Loss: tensor(2.7200, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.7200, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 36800  LR : 1e-05  Loss: tensor(2.6696, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6696, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 37200  LR : 1e-05  Loss: tensor(2.6905, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6905, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 37600  LR : 1e-05  Loss: tensor(2.5826, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5826, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 38000  LR : 1e-05  Loss: tensor(2.5890, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5890, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 38400  LR : 1e-05  Loss: tensor(2.5364, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5364, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 38800  LR : 1e-05  Loss: tensor(2.6599, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6599, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 39200  LR : 1e-05  Loss: tensor(2.6789, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6789, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 39600  LR : 1e-05  Loss: tensor(2.6522, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6522, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 40000  LR : 1e-05  Loss: tensor(2.6365, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6365, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 40400  LR : 1e-05  Loss: tensor(2.6202, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6202, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 40800  LR : 1e-05  Loss: tensor(2.5517, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5517, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 41200  LR : 1e-05  Loss: tensor(2.4628, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4628, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 41600  LR : 1e-05  Loss: tensor(2.4305, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4305, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 42000  LR : 1e-05  Loss: tensor(2.5307, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5307, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 42400  LR : 1e-05  Loss: tensor(2.4849, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4849, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 42800  LR : 1e-05  Loss: tensor(2.3585, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3585, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 43200  LR : 1e-05  Loss: tensor(2.4151, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4151, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 43600  LR : 1e-05  Loss: tensor(2.1779, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.1779, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 44000  LR : 1e-05  Loss: tensor(2.1851, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.1851, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 44400  LR : 1e-05  Loss: tensor(2.2229, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2229, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 44800  LR : 1e-05  Loss: tensor(2.2054, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2054, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 45200  LR : 1e-05  Loss: tensor(2.2603, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2603, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 45600  LR : 1e-05  Loss: tensor(2.3438, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3438, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 46000  LR : 1e-05  Loss: tensor(2.3633, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3633, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 46400  LR : 1e-05  Loss: tensor(2.3947, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3947, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 46800  LR : 1e-05  Loss: tensor(2.3959, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3959, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 47200  LR : 1e-05  Loss: tensor(2.2118, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2118, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 47600  LR : 1e-05  Loss: tensor(2.0665, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.0665, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 48000  LR : 1e-05  Loss: tensor(2.1909, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.1909, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 48400  LR : 1e-05  Loss: tensor(2.3380, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3380, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 48800  LR : 1e-05  Loss: tensor(2.2824, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2824, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 49200  LR : 1e-05  Loss: tensor(2.4659, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4659, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 49600  LR : 1e-05  Loss: tensor(2.4760, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4760, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 50000  LR : 1e-05  Loss: tensor(2.5288, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5288, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 50400  LR : 1e-05  Loss: tensor(2.4854, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4854, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 50800  LR : 1e-05  Loss: tensor(2.3924, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3924, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 51200  LR : 1e-05  Loss: tensor(2.4000, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4000, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 51600  LR : 1e-05  Loss: tensor(2.3331, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3331, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 52000  LR : 1e-05  Loss: tensor(2.3297, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3297, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 52400  LR : 1e-05  Loss: tensor(2.3989, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3989, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 52800  LR : 1e-05  Loss: tensor(2.3514, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3514, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 53200  LR : 1e-05  Loss: tensor(2.2483, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2483, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 53600  LR : 1e-05  Loss: tensor(2.0649, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.0649, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 54000  LR : 1e-05  Loss: tensor(2.0817, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.0817, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 54400  LR : 1e-05  Loss: tensor(1.9780, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(1.9780, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 54800  LR : 1e-05  Loss: tensor(1.9044, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(1.9044, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 55200  LR : 1e-05  Loss: tensor(2.0745, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.0745, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 55600  LR : 1e-05  Loss: tensor(2.0102, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.0102, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 56000  LR : 1e-05  Loss: tensor(1.9291, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(1.9291, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 56400  LR : 1e-05  Loss: tensor(2.2550, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2550, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 56800  LR : 1e-05  Loss: tensor(1.8907, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(1.8907, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 57200  LR : 1e-05  Loss: tensor(1.8711, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(1.8711, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 57600  LR : 1e-05  Loss: tensor(1.8767, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(1.8767, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 2   Iteration number: 58000  LR : 1e-05  Loss: tensor(0.4396, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(0.4396, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 0  LR : 1e-05  Loss: tensor(2.9557, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.9557, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 400  LR : 1e-05  Loss: tensor(2.7828, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.7828, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 800  LR : 1e-05  Loss: tensor(3.3008, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(3.3008, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 1200  LR : 1e-05  Loss: tensor(3.0285, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(3.0285, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 1600  LR : 1e-05  Loss: tensor(2.3825, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3825, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 2000  LR : 1e-05  Loss: tensor(2.4140, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4140, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 2400  LR : 1e-05  Loss: tensor(2.3918, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3918, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 2800  LR : 1e-05  Loss: tensor(2.3781, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3781, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 3200  LR : 1e-05  Loss: tensor(2.5299, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5299, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 3600  LR : 1e-05  Loss: tensor(2.5174, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5174, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 4000  LR : 1e-05  Loss: tensor(2.6025, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6025, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 4400  LR : 1e-05  Loss: tensor(2.6584, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6584, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 4800  LR : 1e-05  Loss: tensor(2.6041, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.6041, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 5200  LR : 1e-05  Loss: tensor(2.3987, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3987, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 5600  LR : 1e-05  Loss: tensor(2.4135, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4135, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 6000  LR : 1e-05  Loss: tensor(2.3331, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3331, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 6400  LR : 1e-05  Loss: tensor(2.3602, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3602, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 6800  LR : 1e-05  Loss: tensor(2.3478, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3478, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 7200  LR : 1e-05  Loss: tensor(2.3318, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3318, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 7600  LR : 1e-05  Loss: tensor(2.3201, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3201, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 8000  LR : 1e-05  Loss: tensor(2.3373, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3373, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 8400  LR : 1e-05  Loss: tensor(2.4214, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4214, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 8800  LR : 1e-05  Loss: tensor(2.2666, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2666, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 9200  LR : 1e-05  Loss: tensor(2.2757, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2757, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 9600  LR : 1e-05  Loss: tensor(2.3227, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3227, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 10000  LR : 1e-05  Loss: tensor(2.4301, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4301, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 10400  LR : 1e-05  Loss: tensor(2.3765, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3765, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 10800  LR : 1e-05  Loss: tensor(2.2658, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2658, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 11200  LR : 1e-05  Loss: tensor(2.2499, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2499, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 11600  LR : 1e-05  Loss: tensor(2.3768, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3768, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 12000  LR : 1e-05  Loss: tensor(2.5070, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5070, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 12400  LR : 1e-05  Loss: tensor(2.5310, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5310, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 12800  LR : 1e-05  Loss: tensor(2.3490, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3490, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 13200  LR : 1e-05  Loss: tensor(2.3718, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3718, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 13600  LR : 1e-05  Loss: tensor(2.2242, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2242, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 14000  LR : 1e-05  Loss: tensor(2.2553, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2553, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 14400  LR : 1e-05  Loss: tensor(2.3448, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3448, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 14800  LR : 1e-05  Loss: tensor(2.2944, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2944, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 15200  LR : 1e-05  Loss: tensor(2.3854, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3854, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 15600  LR : 1e-05  Loss: tensor(2.2920, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2920, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 16000  LR : 1e-05  Loss: tensor(2.1042, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.1042, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 16400  LR : 1e-05  Loss: tensor(2.3136, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3136, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 16800  LR : 1e-05  Loss: tensor(2.4488, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4488, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 17200  LR : 1e-05  Loss: tensor(2.5097, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5097, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 17600  LR : 1e-05  Loss: tensor(2.4850, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4850, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 18000  LR : 1e-05  Loss: tensor(2.2290, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2290, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 18400  LR : 1e-05  Loss: tensor(2.2367, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2367, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 18800  LR : 1e-05  Loss: tensor(2.3299, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3299, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 19200  LR : 1e-05  Loss: tensor(2.4295, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4295, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 19600  LR : 1e-05  Loss: tensor(2.3429, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3429, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 20000  LR : 1e-05  Loss: tensor(2.3481, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3481, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 20400  LR : 1e-05  Loss: tensor(2.2949, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2949, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 20800  LR : 1e-05  Loss: tensor(2.2903, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2903, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 21200  LR : 1e-05  Loss: tensor(2.2686, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2686, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 21600  LR : 1e-05  Loss: tensor(2.3938, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3938, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 22000  LR : 1e-05  Loss: tensor(2.3883, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3883, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 22400  LR : 1e-05  Loss: tensor(2.3052, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3052, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 22800  LR : 1e-05  Loss: tensor(2.3876, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3876, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 23200  LR : 1e-05  Loss: tensor(2.4843, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4843, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 23600  LR : 1e-05  Loss: tensor(2.3927, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3927, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 24000  LR : 1e-05  Loss: tensor(2.3156, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3156, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 24400  LR : 1e-05  Loss: tensor(2.2042, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2042, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 24800  LR : 1e-05  Loss: tensor(2.2593, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2593, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 25200  LR : 1e-05  Loss: tensor(2.3864, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3864, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 25600  LR : 1e-05  Loss: tensor(2.2947, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2947, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 26000  LR : 1e-05  Loss: tensor(2.2780, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2780, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 26400  LR : 1e-05  Loss: tensor(2.3099, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3099, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 26800  LR : 1e-05  Loss: tensor(2.2434, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2434, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 27200  LR : 1e-05  Loss: tensor(2.1946, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.1946, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 27600  LR : 1e-05  Loss: tensor(2.3351, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3351, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 28000  LR : 1e-05  Loss: tensor(2.3677, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.3677, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 28400  LR : 1e-05  Loss: tensor(2.4471, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4471, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 28800  LR : 1e-05  Loss: tensor(2.4013, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4013, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 29200  LR : 1e-05  Loss: tensor(2.2900, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2900, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 29600  LR : 1e-05  Loss: tensor(2.2940, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2940, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 30000  LR : 1e-05  Loss: tensor(2.2636, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2636, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 30400  LR : 1e-05  Loss: tensor(2.2795, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2795, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 30800  LR : 1e-05  Loss: tensor(2.2997, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.2997, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 31200  LR : 1e-05  Loss: tensor(2.4264, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4264, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 31600  LR : 1e-05  Loss: tensor(2.5182, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.5182, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 32000  LR : 1e-05  Loss: tensor(2.4634, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4634, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Epoch number: 3   Iteration number: 32400  LR : 1e-05  Loss: tensor(2.4240, device='cuda:0', grad_fn=<DivBackward0>) Reg Loss: tensor(2.4240, device='cuda:0', grad_fn=<SumBackward0>)\n"
          ]
        }
      ],
      "source": [
        "net_5= net_5.cuda()\n",
        "data=X\n",
        "batch_size = 1\n",
        "epochs = 10\n",
        "Y = Y.cuda()\n",
        "opt1 = torch.optim.Adam(net_5.parameters(),lr=0.00001)\n",
        "# scheduler = lr_scheduler.LinearLR(opt1, start_factor=1.0, end_factor=0.1, total_iters=10)\n",
        "net_5.train()\n",
        "bs = 400\n",
        "for epoch in range(epochs):\n",
        "    for ct in range(0,len(X),bs):\n",
        "        x = data[ct:bs+ct].detach().cuda()\n",
        "\n",
        "        opt1.zero_grad()\n",
        "        x_hat = net_5(x).cuda()\n",
        "        # f = py_kde(x,x,0.65)\n",
        "        f_x = py_kde(x,x,0.65)\n",
        "        f_x_der = py_kde_der(f_x,x)\n",
        "    \n",
        "        loss = torch.linalg.norm(f_x_der/f_x.view(f_x.shape[0],1)+ net_5.loss_reg,dim=1)\n",
        "        loss.backward(torch.ones_like(loss), retain_graph=True)\n",
        "        opt1.step()\n",
        "        print(\"Epoch number:\",epoch,\"  Iteration number:\",ct,\" LR :\",opt1.param_groups[0][\"lr\"], \" Loss:\",torch.sum(loss)/bs, \"Reg Loss:\",torch.sum(torch.linalg.norm(f_x_der/f_x.view(f_x.shape[0],1)+net_5.loss_reg,dim=1)/bs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.BCELoss(reduction= 'none')\n",
        "def my_loss(y_pred,y_train,reg_loss):\n",
        "    loss = criterion(y_pred,y_train) +reg_loss\n",
        "    return loss\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "torch.autograd.set_detect_anomaly(False)\n",
        "net_10 = Net(10)\n",
        "import torch.optim.lr_scheduler as lr_scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.73 GiB total capacity; 3.13 GiB already allocated; 44.75 MiB free; 3.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[1;32m/home/kaustubh/Code/PowMechNB_copy3.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code/PowMechNB_copy3.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m x \u001b[39m=\u001b[39m data[ct:bs\u001b[39m+\u001b[39mct]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code/PowMechNB_copy3.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m opt1\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code/PowMechNB_copy3.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m x_hat \u001b[39m=\u001b[39m net_10(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code/PowMechNB_copy3.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# f = py_kde(x,x,0.65)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code/PowMechNB_copy3.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m f_x \u001b[39m=\u001b[39m py_kde(x,x,\u001b[39m0.65\u001b[39m)\n",
            "File \u001b[0;32m~/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32m/home/kaustubh/Code/PowMechNB_copy3.ipynb Cell 33\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code/PowMechNB_copy3.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m H \u001b[39m=\u001b[39m H\u001b[39m.\u001b[39mreshape(bs,d,d)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code/PowMechNB_copy3.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m z \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(z,H)\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code/PowMechNB_copy3.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m J \u001b[39m=\u001b[39m batch_jacobian(H_mul, z,create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code/PowMechNB_copy3.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m J_int \u001b[39m=\u001b[39m\u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mlog(torch\u001b[39m.\u001b[39mabs(torch\u001b[39m.\u001b[39mdet(J)))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code/PowMechNB_copy3.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m loss_reg \u001b[39m=\u001b[39m loss_reg \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39msqueeze(torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgrad(J_int, x,torch\u001b[39m.\u001b[39mones_like(J_int),allow_unused\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,create_graph\u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mcuda()\n",
            "\u001b[1;32m/home/kaustubh/Code/PowMechNB_copy3.ipynb Cell 33\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code/PowMechNB_copy3.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_func_sum\u001b[39m(z):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code/PowMechNB_copy3.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(z)\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code/PowMechNB_copy3.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msqueeze(torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mjacobian(_func_sum, z, create_graph\u001b[39m=\u001b[39;49mcreate_graph))\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m)\n",
            "File \u001b[0;32m~/llm/lib/python3.10/site-packages/torch/autograd/functional.py:686\u001b[0m, in \u001b[0;36mjacobian\u001b[0;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[1;32m    684\u001b[0m jac_i: Tuple[List[torch\u001b[39m.\u001b[39mTensor]] \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m([] \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(inputs)))  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    685\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(out\u001b[39m.\u001b[39mnelement()):\n\u001b[0;32m--> 686\u001b[0m     vj \u001b[39m=\u001b[39m _autograd_grad((out\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)[j],), inputs,\n\u001b[1;32m    687\u001b[0m                         retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, create_graph\u001b[39m=\u001b[39;49mcreate_graph)\n\u001b[1;32m    689\u001b[0m     \u001b[39mfor\u001b[39;00m el_idx, (jac_i_el, vj_el, inp_el) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(jac_i, vj, inputs)):\n\u001b[1;32m    690\u001b[0m         \u001b[39mif\u001b[39;00m vj_el \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/llm/lib/python3.10/site-packages/torch/autograd/functional.py:167\u001b[0m, in \u001b[0;36m_autograd_grad\u001b[0;34m(outputs, inputs, grad_outputs, create_graph, retain_graph, is_grads_batched)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mNone\u001b[39;00m,) \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(inputs)\n\u001b[1;32m    166\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(new_outputs, inputs, new_grad_outputs, allow_unused\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    168\u001b[0m                                create_graph\u001b[39m=\u001b[39;49mcreate_graph, retain_graph\u001b[39m=\u001b[39;49mretain_graph,\n\u001b[1;32m    169\u001b[0m                                is_grads_batched\u001b[39m=\u001b[39;49mis_grads_batched)\n",
            "File \u001b[0;32m~/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    305\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.73 GiB total capacity; 3.13 GiB already allocated; 44.75 MiB free; 3.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "net_10= net_10.cuda()\n",
        "data=X.cuda()\n",
        "batch_size = 1\n",
        "epochs = 10\n",
        "Y = Y.cuda()\n",
        "opt1 = torch.optim.Adam(net_10.parameters(),lr=0.001)\n",
        "# scheduler = lr_scheduler.LinearLR(opt1, start_factor=1.0, end_factor=0.1, total_iters=10)\n",
        "net_10.train()\n",
        "bs = 500\n",
        "for epoch in range(epochs):\n",
        "    for ct in range(0,len(X),bs):\n",
        "        x = data[ct:bs+ct].detach().cuda()\n",
        "\n",
        "        opt1.zero_grad()\n",
        "        x_hat = net_10(x)\n",
        "        # f = py_kde(x,x,0.65)\n",
        "        f_x = py_kde(x,x,0.65)\n",
        "        f_x_der = py_kde_der(f_x,x)\n",
        "        loss = criterion(torch.squeeze(x_hat[0:,0:,0]),torch.squeeze(Y[ct:bs+ct])) +torch.linalg.norm(f_x_der/f_x.view(f_x.shape[0],1)+ net_10.loss_reg,dim=1)\n",
        "        loss.backward(torch.ones_like(loss), retain_graph=True)\n",
        "        opt1.step()\n",
        "        print(\"Epoch number:\",epoch,\"  Iteration number:\",ct,\" LR :\",opt1.param_groups[0][\"lr\"], \" Loss:\",torch.sum(loss)/bs, \"Reg Loss:\",torch.sum(torch.linalg.norm(f_x_der/f_x.view(f_x.shape[0],1)+net_10.loss_reg,dim=1)/bs))\n",
        "    # if(epoch%20==19):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upper bound on Epsilon\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data=X\n",
        "\n",
        "\n",
        "\n",
        "losses= torch.zeros(X.shape[0])\n",
        "bs = 1000\n",
        "n = bs\n",
        "h = 0.65\n",
        "d = X.shape[1]\n",
        "alpha= 0.01\n",
        "for ct in range(0,len(X),bs):\n",
        "    x = data[ct:bs+ct].detach()\n",
        "    x_hat = net_5(x)\n",
        "    f = py_kde(x,x,0.65)\n",
        "    f_der = py_kde_der(f,x)\n",
        "    ci = CI_KDE(f,n,h,d,alpha)\n",
        "    loss =torch.max(torch.linalg.norm(f_der/(f-ci).view(f.shape[0],1)+net_5.loss_reg,dim=1),torch.linalg.norm(f_der/(f+ci).view(f.shape[0],1)+net_5.loss_reg,dim=1)) \n",
        "    losses[ct:bs+ct] =loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# net = Net(1)\n",
        "net = torch.load(\"Models/net_1_cov\")\n",
        "# net.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_priv = torch.utils.data.TensorDataset(X,Y)\n",
        "\n",
        "trainloader_priv = torch.utils.data.DataLoader(train_priv, batch_size=1000,\n",
        "                                        shuffle=True, num_workers=2,drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "optim = torch.optim.Adam(net.parameters(),lr=0.00003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/home/kaustubh/Code3/PowMechNB_copy3.ipynb Cell 44\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m net\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m net\u001b[39m.\u001b[39meval()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m test_model(net,trainloader_priv)\n",
            "File \u001b[0;32m~/Code3/cov_help.py:330\u001b[0m, in \u001b[0;36mtest_model\u001b[0;34m(model, test_loader)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m test_loader:\n\u001b[1;32m    329\u001b[0m     inputs, labels \u001b[39m=\u001b[39m data\n\u001b[0;32m--> 330\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m    331\u001b[0m     _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n\u001b[1;32m    332\u001b[0m     total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
            "File \u001b[0;32m~/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32m/home/kaustubh/Code3/PowMechNB_copy3.ipynb Cell 44\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     J \u001b[39m=\u001b[39m batch_jacobian(H_mul, z, create_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m     J_int \u001b[39m=\u001b[39m\u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mlog(torch\u001b[39m.\u001b[39mabs(torch\u001b[39m.\u001b[39mdet(J)))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m     loss_reg \u001b[39m=\u001b[39m loss_reg \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39msqueeze(torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(J_int, x,torch\u001b[39m.\u001b[39;49mones_like(J_int),allow_unused\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,create_graph\u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_reg \u001b[39m=\u001b[39m loss_reg\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#X61sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my \u001b[39m=\u001b[39m z\n",
            "File \u001b[0;32m~/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    305\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ],
      "source": [
        "net.to(torch.device('cuda'))\n",
        "net.eval()\n",
        "test_model(net,trainloader_priv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/home/kaustubh/Code3/PowMechNB_copy3.ipynb Cell 45\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m test_loader:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     inputs, labels \u001b[39m=\u001b[39m data\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     outputs \u001b[39m=\u001b[39m net(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(torch\u001b[39m.\u001b[39msqueeze(outputs),torch\u001b[39m.\u001b[39msqueeze(labels))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     losses\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mloss\u001b[39m.\u001b[39msum()\n",
            "File \u001b[0;32m~/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32m/home/kaustubh/Code3/PowMechNB_copy3.ipynb Cell 45\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     J \u001b[39m=\u001b[39m batch_jacobian(H_mul, z, create_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m     J_int \u001b[39m=\u001b[39m\u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mlog(torch\u001b[39m.\u001b[39mabs(torch\u001b[39m.\u001b[39mdet(J)))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m     loss_reg \u001b[39m=\u001b[39m loss_reg \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39msqueeze(torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(J_int, x,torch\u001b[39m.\u001b[39;49mones_like(J_int),allow_unused\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,create_graph\u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_reg \u001b[39m=\u001b[39m loss_reg\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#Y115sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my \u001b[39m=\u001b[39m z\n",
            "File \u001b[0;32m~/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    305\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ],
      "source": [
        "net.eval()\n",
        "test_loader = trainloader_priv\n",
        "correct = 0\n",
        "total = 0\n",
        "losses =0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        inputs, labels = data\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(torch.squeeze(outputs),torch.squeeze(labels))\n",
        "        losses+=loss.sum()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the test images: %d %%' % (\n",
        "    100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.0021)"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "losses/len(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Net(\n",
              "  (H_net1): Sequential(\n",
              "    (0): Linear(in_features=54, out_features=128, bias=True)\n",
              "    (1): Sigmoid()\n",
              "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
              "    (3): Sigmoid()\n",
              "    (4): Linear(in_features=256, out_features=2916, bias=True)\n",
              "  )\n",
              "  (X_net): Sequential(\n",
              "    (0): Linear(in_features=54, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=128, out_features=7, bias=True)\n",
              "    (5): Softmax(dim=2)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[6.6744e-02, 9.2670e-01, 2.1241e-03, 1.3546e-03, 9.6444e-04,\n",
              "          1.2291e-03, 8.8563e-04]],\n",
              "\n",
              "        [[4.8453e-02, 9.4747e-01, 1.3700e-03, 8.5512e-04, 5.7667e-04,\n",
              "          7.4947e-04, 5.2757e-04]],\n",
              "\n",
              "        [[5.4603e-02, 9.3788e-01, 2.3995e-03, 1.5850e-03, 1.1224e-03,\n",
              "          1.3911e-03, 1.0192e-03]]], device='cuda:0',\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net(X[0:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:wtr3pi52) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59e96e20c38f40ce8a10d74c216eba84",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.008 MB of 0.014 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.529151…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▄▄▄▆▃▄▄▇▆▄▄█▃▆▇▂▁▄▅▃▂▃▃▁▅▄▃▂▃▄▆▂▂▂▂▅▂▁▃</td></tr><tr><td>loss_reg</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▃▂▂▂▁▂▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.00157</td></tr><tr><td>loss_reg</td><td>37.95282</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ghostly-spirit-5</strong> at: <a href='https://wandb.ai/ponkshekaustubh11/covertype%202/runs/wtr3pi52' target=\"_blank\">https://wandb.ai/ponkshekaustubh11/covertype%202/runs/wtr3pi52</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20231031_151633-wtr3pi52/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:wtr3pi52). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.12 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/kaustubh/Code3/wandb/run-20231031_151737-r3tvdm7v</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ponkshekaustubh11/covertype%202/runs/r3tvdm7v' target=\"_blank\">chilling-treat-6</a></strong> to <a href='https://wandb.ai/ponkshekaustubh11/covertype%202' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ponkshekaustubh11/covertype%202' target=\"_blank\">https://wandb.ai/ponkshekaustubh11/covertype%202</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ponkshekaustubh11/covertype%202/runs/r3tvdm7v' target=\"_blank\">https://wandb.ai/ponkshekaustubh11/covertype%202/runs/r3tvdm7v</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/kaustubh/Code3/PowMechNB_copy3.ipynb Cell 46\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m run \u001b[39m=\u001b[39m wandb\u001b[39m.\u001b[39minit(project\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcovertype 2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m lr_schedule \u001b[39m=\u001b[39m LearnerRateScheduler(\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m, base_learning_rate\u001b[39m=\u001b[39m\u001b[39m0.00003\u001b[39m, decay_rate \u001b[39m=\u001b[39m \u001b[39m0.99\u001b[39m, decay_steps\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.198.63.24/home/kaustubh/Code3/PowMechNB_copy3.ipynb#Y106sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m train_model_priv(net,trainloader_priv,optim,\u001b[39m4\u001b[39;49m,h\u001b[39m=\u001b[39;49m\u001b[39m0.82\u001b[39;49m,rate\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,device\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m),only_reg_flag\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,lr_schedular\u001b[39m=\u001b[39;49mlr_schedule)\n",
            "File \u001b[0;32m~/Code3/cov_help.py:161\u001b[0m, in \u001b[0;36mtrain_model_priv\u001b[0;34m(net, trainloader, optimizer, epochs, h, rate, device, print_cond, only_reg_flag, lr_schedular)\u001b[0m\n\u001b[1;32m    159\u001b[0m labels \u001b[39m=\u001b[39m data[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    160\u001b[0m f \u001b[39m=\u001b[39m py_kde(inputs,inputs,h)\n\u001b[0;32m--> 161\u001b[0m f_der \u001b[39m=\u001b[39m py_kde_der(f,inputs)\n\u001b[1;32m    163\u001b[0m \u001b[39m# zero the parameter gradients\u001b[39;00m\n\u001b[1;32m    164\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
            "File \u001b[0;32m~/Code3/cov_help.py:90\u001b[0m, in \u001b[0;36mpy_kde_der\u001b[0;34m(p_x, x)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpy_kde_der\u001b[39m(p_x,x):\n\u001b[1;32m     88\u001b[0m     \u001b[39m# x.requires_grad = True\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     \u001b[39m# p_x = py_kde(x,X_t,h)\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m     \u001b[39mreturn\u001b[39;00m (torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(p_x,x,torch\u001b[39m.\u001b[39;49mones_like(p_x),allow_unused\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mcuda()\n",
            "File \u001b[0;32m~/llm/lib/python3.10/site-packages/torch/autograd/__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    304\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    305\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "run = wandb.init(project=\"covertype 2\")\n",
        "\n",
        "lr_schedule = LearnerRateScheduler('step', base_learning_rate=0.00003, decay_rate = 0.99, decay_steps=1)\n",
        "train_model_priv(net,trainloader_priv,optim,4,h=0.82,rate=10,device=torch.device('cuda'),only_reg_flag=2,lr_schedular=lr_schedule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "net =  torch.load(\"Models/net_1_cov\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_emb,losses = create_model_embs(net,trainloader_priv,device= torch.device('cuda'),l=len(X),h=0.82)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2.3786)"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(losses).sum()/len(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        " \n",
        " \n",
        "# Creating dataset\n",
        "a = losses.detach()\n",
        " \n",
        "# Creating histogram\n",
        "fig, ax = plt.subplots()\n",
        "ax.hist(a, bins = np.arange(0.1,5,0.03))\n",
        " \n",
        "# Show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_emb_train, X_emb_test, Y_train, Y_test = train_test_split(X_emb, Y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_emb_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_emb_train,Y_train), batch_size=1000,\n",
        "                                        shuffle=True, num_workers=2)\n",
        "test_emb_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_emb_test,Y_test), batch_size=1000,\n",
        "                                        shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10], loss: 1.677\n",
            "Epoch [20], loss: 1.676\n",
            "Epoch [30], loss: 1.676\n",
            "Epoch [40], loss: 1.676\n",
            "Epoch [50], loss: 1.676\n",
            "Epoch [60], loss: 1.676\n",
            "Epoch [70], loss: 1.676\n",
            "Epoch [80], loss: 1.676\n",
            "Epoch [90], loss: 1.677\n",
            "Epoch [100], loss: 1.676\n"
          ]
        }
      ],
      "source": [
        "net.cpu()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = nn.Sequential(\n",
        "            nn.Linear(54, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 7),\n",
        "            nn.Softmax(dim=1)\n",
        "\n",
        "        )\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.01,weight_decay=1e-4)\n",
        "train_emb(model, train_emb_loader, criterion, optimizer, num_epochs=100,device=torch.device('cpu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "o = autoencoder1(X_train.detach())\n",
        "X_emb_train = torch.squeeze(autoencoder1.y).detach()\n",
        "o = autoencoder1(X_test.detach())\n",
        "X_emb_test = torch.squeeze(autoencoder1.y).detach()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test images: 49 %\n"
          ]
        }
      ],
      "source": [
        "test_model(model,test_emb_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy 0.8339999914169312\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# o = autoencoder1(X)\n",
        "# X_embs = torch.squeeze(autoencoder1.y.detach())\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(12, 48),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(48, 32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, 1),\n",
        "    nn.Sigmoid(\n",
        "\n",
        "    )\n",
        ")\n",
        "loss_fn = nn.BCELoss()  # binary cross entropy\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "n_epochs = 2000\n",
        "batch_size = 1000\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for i in range(0, len(X_emb_train), batch_size):\n",
        "        Xbatch = X_emb_train[i:i+batch_size]\n",
        "        y_pred = model(Xbatch)\n",
        "        ybatch = Y_train[i:i+batch_size]\n",
        "        loss = loss_fn(y_pred, ybatch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    #     print(loss)\n",
        "    # print(f'Finished epoch {epoch}, latest loss {loss}')\n",
        "# compute accuracy (no_grad is optional)\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_emb_test)\n",
        " \n",
        "accuracy = (y_pred.round() == Y_test).float().mean()\n",
        "print(f\"Accuracy {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy 0.871999979019165\n"
          ]
        }
      ],
      "source": [
        "model1 = nn.Sequential(\n",
        "    nn.Linear(12, 48),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(48, 32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, 1),\n",
        "    nn.Sigmoid(\n",
        "\n",
        "    )\n",
        ")\n",
        "loss_fn = nn.BCELoss()  # binary cross entropy\n",
        "optimizer1 = optim.Adam(model1.parameters(), lr=0.001)\n",
        "n_epochs = 1000\n",
        "batch_size = 1000\n",
        " \n",
        "for epoch in range(n_epochs):\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        Xbatch = X_train[i:i+batch_size]\n",
        "        y_pred = model1(Xbatch)\n",
        "        ybatch = Y_train[i:i+batch_size]\n",
        "        loss = loss_fn(y_pred, ybatch)\n",
        "        optimizer1.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer1.step()\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_pred = model1(X_test)\n",
        " \n",
        "accuracy = (y_pred.round() == Y_test).float().mean()\n",
        "print(f\"Accuracy {accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
