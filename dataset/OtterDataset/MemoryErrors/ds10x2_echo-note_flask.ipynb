{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44f507c8-5893-44e1-8a47-a807703150fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ffmpeg\n",
      "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: ffmpeg\n",
      "  Building wheel for ffmpeg (pyproject.toml): started\n",
      "  Building wheel for ffmpeg (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6089 sha256=c294b79a3d37b062cf790243e0cb233fb93a3e3c79c48744f74c9d5b3bf0e32a\n",
      "  Stored in directory: c:\\users\\ssafy\\appdata\\local\\pip\\cache\\wheels\\26\\21\\0c\\c26e09dff860a9071683e279445262346e008a9a1d2142c4ad\n",
      "Successfully built ffmpeg\n",
      "Installing collected packages: ffmpeg\n",
      "Successfully installed ffmpeg-1.4\n"
     ]
    }
   ],
   "source": [
    "#!pip install flask\n",
    "#!pip install -U openai-whisper\n",
    "#!pip install pytorch\n",
    "!pip install ffmpeg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50995688-2279-4fb6-b85b-906938af9dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Worker 0 starting, loading Whisper model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://70.12.246.80:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "INFO:__main__:Received STT request\n",
      "WARNING:__main__:No audio file provided in the request\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Sep/2024 10:44:14] \"\u001b[31m\u001b[1mPOST /stt HTTP/1.1\u001b[0m\" 400 -\n",
      "C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "INFO:__main__:Received STT request\n",
      "WARNING:__main__:No audio file provided in the request\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Sep/2024 10:44:21] \"\u001b[31m\u001b[1mPOST /stt HTTP/1.1\u001b[0m\" 400 -\n",
      "Exception in thread Thread-5 (worker):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Temp\\ipykernel_3712\\4036696997.py\", line 27, in worker\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\__init__.py\", line 156, in load_model\n",
      "    return model.to(device)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1174, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 780, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 805, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"C:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1160, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 2.96 GiB is allocated by PyTorch, and 7.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "INFO:__main__:Received STT request\n",
      "WARNING:__main__:No audio file provided in the request\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Sep/2024 10:44:36] \"\u001b[31m\u001b[1mPOST /stt HTTP/1.1\u001b[0m\" 400 -\n"
     ]
    }
   ],
   "source": [
    "import os  # 운영 체제와 상호작용하기 위한 os 모듈 임포트\n",
    "import tempfile  # 임시 파일 생성을 위한 tempfile 모듈 임포트\n",
    "import logging  # 로그 메시지를 위한 logging 모듈 임포트\n",
    "from flask import Flask, request, jsonify  # 웹 애플리케이션 생성을 위한 Flask와 유틸리티 임포트\n",
    "import whisper  # Whisper ASR 모델 임포트\n",
    "import torch  # 텐서 연산을 위한 PyTorch 임포트\n",
    "import threading  # 스레드를 생성하기 위한 threading 모듈 임포트\n",
    "import queue  # 작업 큐 생성을 위한 queue 모듈 임포트\n",
    "import uuid  # 고유한 작업 ID 생성을 위한 uuid 모듈 임포트\n",
    "\n",
    "# 로그 메시지를 info 레벨로 설정\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)  # 이 모듈을 위한 로거 생성\n",
    "\n",
    "app = Flask(__name__)  # Flask 애플리케이션 인스턴스 생성\n",
    "\n",
    "# 작업 큐와 결과 저장을 위한 딕셔너리 초기화\n",
    "task_queue = queue.Queue()\n",
    "results = {}\n",
    "\n",
    "# 오디오 파일을 처리할 작업자 스레드 수 설정\n",
    "num_worker_threads = 1\n",
    "\n",
    "# 오디오 파일을 처리할 작업자 함수 정의\n",
    "def worker(thread_id):\n",
    "    logger.info(f\"Worker {thread_id} starting, loading Whisper model...\")\n",
    "    whisper_model = whisper.load_model(\"medium\")  # Whisper ASR 모델 로드\n",
    "    logger.info(f\"Worker {thread_id} loaded Whisper model.\")\n",
    "\n",
    "    while True:  # 작업자가 계속 실행되도록 무한 루프\n",
    "        task_id, audio_path = task_queue.get()  # 큐에서 작업 가져오기\n",
    "        if task_id is None:  # 종료 신호 확인\n",
    "            break  # 종료 신호를 받으면 루프 종료\n",
    "        \n",
    "        logger.info(f\"Worker {thread_id} processing task {task_id}\")\n",
    "        \n",
    "        try:\n",
    "            # Whisper 모델을 사용하여 오디오 파일을 전사\n",
    "            results[task_id] = {\"status\": \"processing\"}\n",
    "            result = whisper_model.transcribe(audio_path)\n",
    "            # segment에서 id, start, end, text \n",
    "            # 비동기 \n",
    "            # API통신.uri(result, \"spring url\")\n",
    "            \n",
    "            \n",
    "            results[task_id] = {\"status\": \"completed\", \"result\": result}  # 결과를 딕셔너리에 저장\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Worker {thread_id} encountered an error: {str(e)}\")  # 발생한 오류 로그\n",
    "            results[task_id] = {\"status\": \"failed\", \"error\": str(e)}  # 오류를 결과에 저장\n",
    "        finally:\n",
    "            os.remove(audio_path)  # 임시 오디오 파일 제거\n",
    "            if torch.cuda.is_available():  # CUDA 사용 가능 여부 확인\n",
    "                torch.cuda.empty_cache()  # 필요 시 CUDA 메모리 캐시 지우기\n",
    "        \n",
    "        task_queue.task_done()  # 작업 완료 표시\n",
    "\n",
    "# 작업자 스레드 시작\n",
    "threads = []\n",
    "for i in range(num_worker_threads):\n",
    "    t = threading.Thread(target=worker, args=(i,))  # 작업자 함수에 대한 새 스레드 생성\n",
    "    t.start()  # 스레드 시작\n",
    "    threads.append(t)  # 스레드 목록에 추가\n",
    "\n",
    "@app.route('/', methods=['GET'])  # 홈 페이지를 위한 라우트 정의\n",
    "def home():\n",
    "    return \"Welcome to the Asynchronous Flask Server with Whisper!\"  # 환영 메시지 반환\n",
    "\n",
    "@app.route('/stt', methods=['POST'])  # STT 요청을 위한 라우트 정의\n",
    "def stt_request():\n",
    "    logger.info(\"Received STT request\")\n",
    "    \n",
    "    if 'audio' not in request.files:  # 오디오 파일이 제공되었는지 확인\n",
    "        logger.warning(\"No audio file provided in the request\")\n",
    "        return jsonify({'error': 'No audio file provided'}), 400  # 제공되지 않으면 오류 반환\n",
    "    \n",
    "    audio_data = request.files['audio']  # 요청에서 오디오 파일 가져오기\n",
    "    \n",
    "    if audio_data.filename == '':  # 파일 이름이 비어 있는지 확인\n",
    "        logger.warning(\"Empty filename provided\")\n",
    "        return jsonify({'error': 'No selected file'}), 400  # 비어 있으면 오류 반환\n",
    "    \n",
    "    logger.info(f\"Processing audio file: {audio_data.filename}\")\n",
    "    \n",
    "    # 오디오 데이터를 저장할 임시 파일 생성\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as temp_audio:\n",
    "        audio_data.save(temp_audio.name)  # 오디오 데이터를 임시 파일에 저장\n",
    "        temp_audio_path = temp_audio.name  # 임시 파일 경로 저장\n",
    "    \n",
    "    # 고유한 작업 ID를 생성하고 작업을 큐에 추가\n",
    "    task_id = str(audio_data.filename)  # 작업을 위한 고유 식별자 생성\n",
    "    task_queue.put((task_id, temp_audio_path))  # 큐에 작업 추가\n",
    "    \n",
    "    logger.info(f\"Task {task_id} added to the queue\")\n",
    "    \n",
    "    return jsonify({\"task_id\": task_id, \"status\": \"processing\"}), 202  # 작업 ID와 상태 반환\n",
    "\n",
    "@app.route('/stt_result/<task_id>', methods=['GET'])  # 작업 상태를 확인하기 위한 라우트 정의\n",
    "def get_status(task_id):\n",
    "    if task_id not in results:  # 작업 ID가 결과에 없으면\n",
    "        return jsonify({\"status\": \"No_result\"}), 404  # 처리 중 상태 반환\n",
    "    \n",
    "    result = results[task_id]  # 주어진 작업 ID에 대한 결과 가져오기\n",
    "    \n",
    "    if result[\"status\"] == \"completed\":  # 작업이 완료되었는지 확인\n",
    "        del results[task_id]  # 메모리 해제를 위해 결과 딕셔너리에서 제거\n",
    "        return jsonify(result), 200  # 완료된 결과 반환\n",
    "    elif result[\"status\"] == \"failed\":  # 작업이 실패했는지 확인\n",
    "        del results[task_id]  # 메모리 해제를 위해 결과 딕셔너리에서 제거\n",
    "        return jsonify(result), 500  # 실패한 결과 반환\n",
    "    else:\n",
    "        return jsonify({\"status\": \"processing\"}), 202  # 여전히 처리 중인 상태 반환\n",
    "\n",
    "@app.errorhandler(500)  # 500 내부 서버 오류 처리\n",
    "def internal_error(error):\n",
    "    logger.error(f\"Internal Server Error: {str(error)}\")  # 오류 로그\n",
    "    return jsonify({\"error\": \"Internal Server Error\"}), 500  # 오류 메시지 반환\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        app.run(host='0.0.0.0', port=5000)  # 포트 5000에서 Flask 앱 시작\n",
    "    finally:\n",
    "        # 애플리케이션 종료 시 작업자 스레드 중지\n",
    "        torch.cuda.empty_cache()  # 메모리 캐시 비우기\n",
    "        for _ in range(num_worker_threads):\n",
    "            task_queue.put((None, None))  # 스레드에 중지 신호 전송\n",
    "        for t in threads:\n",
    "            t.join()  # 모든 스레드가 종료될 때까지 대기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d58b28d-5334-4835-b8bc-95707fba51a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Worker 0 starting, loading Whisper model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://localhost:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "INFO:__main__:Worker 0 loaded Whisper model.\n",
      "INFO:__main__:Received STT request\n",
      "INFO:__main__:Downloading audio file from URI: https://timeisnullnull.s3.ap-northeast-2.amazonaws.com/codeapple.mp3\n",
      "INFO:__main__:Audio file downloaded and saved to C:\\Users\\SSAFY\\AppData\\Local\\Temp\\tmpp2oe6hlm\n",
      "INFO:__main__:Task 1 added to the queue\n",
      "INFO:__main__:Worker 0 processing task 1\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Sep/2024 12:45:27] \"\u001b[35m\u001b[1mPOST /stt HTTP/1.1\u001b[0m\" 202 -\n",
      "INFO:__main__:Received STT request\n",
      "INFO:__main__:Downloading audio file from URI: https://timeisnullnull.s3.ap-northeast-2.amazonaws.com/codeapple.mp3\n",
      "INFO:__main__:Audio file downloaded and saved to C:\\Users\\SSAFY\\AppData\\Local\\Temp\\tmphwhp2eps\n",
      "INFO:__main__:Task 1 added to the queue\n",
      "INFO:__main__:Worker 0 processing task 1\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Sep/2024 13:07:19] \"\u001b[35m\u001b[1mPOST /stt HTTP/1.1\u001b[0m\" 202 -\n",
      "INFO:__main__:Received STT request\n",
      "INFO:__main__:Downloading audio file from URI: https://timeisnullnull.s3.ap-northeast-2.amazonaws.com/codeapple.mp3\n",
      "INFO:__main__:Audio file downloaded and saved to C:\\Users\\SSAFY\\AppData\\Local\\Temp\\tmp1_x236gf\n",
      "INFO:__main__:Task 1 added to the queue\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Sep/2024 13:07:28] \"\u001b[35m\u001b[1mPOST /stt HTTP/1.1\u001b[0m\" 202 -\n",
      "INFO:__main__:Worker 0 processing task 1\n",
      "INFO:__main__:Received STT request\n",
      "INFO:__main__:Downloading audio file from URI: https://timeisnullnull.s3.ap-northeast-2.amazonaws.com/codeapple.mp3\n",
      "INFO:__main__:Audio file downloaded and saved to C:\\Users\\SSAFY\\AppData\\Local\\Temp\\tmpdf59_g9x\n",
      "INFO:__main__:Task 1 added to the queue\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Sep/2024 13:07:41] \"\u001b[35m\u001b[1mPOST /stt HTTP/1.1\u001b[0m\" 202 -\n",
      "INFO:__main__:Worker 0 processing task 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과 전송 오류: HTTPConnectionPool(host='localhost', port=8080): Max retries exceeded with url: /voice/stt (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000200E1435370>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Received STT request\n",
      "INFO:__main__:Downloading audio file from URI: https://timeisnullnull.s3.ap-northeast-2.amazonaws.com/codeapple.mp3\n",
      "INFO:__main__:Audio file downloaded and saved to C:\\Users\\SSAFY\\AppData\\Local\\Temp\\tmpsv206id8\n",
      "INFO:__main__:Task 1 added to the queue\n",
      "INFO:__main__:Worker 0 processing task 1\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Sep/2024 13:08:12] \"\u001b[35m\u001b[1mPOST /stt HTTP/1.1\u001b[0m\" 202 -\n",
      "INFO:__main__:Received STT request\n",
      "INFO:__main__:Downloading audio file from URI: https://timeisnullnull.s3.ap-northeast-2.amazonaws.com/codeapple.mp3\n",
      "INFO:__main__:Audio file downloaded and saved to C:\\Users\\SSAFY\\AppData\\Local\\Temp\\tmpet8n92e3\n",
      "INFO:__main__:Task 1 added to the queue\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Sep/2024 13:08:17] \"\u001b[35m\u001b[1mPOST /stt HTTP/1.1\u001b[0m\" 202 -\n",
      "INFO:__main__:Received STT request\n",
      "INFO:__main__:Downloading audio file from URI: https://timeisnullnull.s3.ap-northeast-2.amazonaws.com/codeapple.mp3\n",
      "INFO:__main__:Audio file downloaded and saved to C:\\Users\\SSAFY\\AppData\\Local\\Temp\\tmpnon169gs\n",
      "INFO:__main__:Task 1 added to the queue\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Sep/2024 13:08:21] \"\u001b[35m\u001b[1mPOST /stt HTTP/1.1\u001b[0m\" 202 -\n",
      "INFO:__main__:Received STT request\n",
      "INFO:__main__:Downloading audio file from URI: https://timeisnullnull.s3.ap-northeast-2.amazonaws.com/codeapple.mp3\n",
      "INFO:__main__:Audio file downloaded and saved to C:\\Users\\SSAFY\\AppData\\Local\\Temp\\tmpajs8avjo\n",
      "INFO:__main__:Task 1 added to the queue\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Sep/2024 13:08:26] \"\u001b[35m\u001b[1mPOST /stt HTTP/1.1\u001b[0m\" 202 -\n",
      "INFO:__main__:Received STT request\n",
      "INFO:__main__:Downloading audio file from URI: https://timeisnullnull.s3.ap-northeast-2.amazonaws.com/codeapple.mp3\n",
      "INFO:__main__:Worker 0 processing task 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과 전송 오류: HTTPConnectionPool(host='localhost', port=8080): Max retries exceeded with url: /voice/stt (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000200B95F6900>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Audio file downloaded and saved to C:\\Users\\SSAFY\\AppData\\Local\\Temp\\tmppc7byoq7\n",
      "INFO:__main__:Task 1 added to the queue\n",
      "INFO:werkzeug:127.0.0.1 - - [27/Sep/2024 13:08:31] \"\u001b[35m\u001b[1mPOST /stt HTTP/1.1\u001b[0m\" 202 -\n",
      "INFO:__main__:Worker 0 processing task 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과 전송 오류: HTTPConnectionPool(host='localhost', port=8080): Max retries exceeded with url: /voice/stt (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000200E1436C30>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Worker 0 processing task 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과 전송 오류: HTTPConnectionPool(host='localhost', port=8080): Max retries exceeded with url: /voice/stt (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002014AB563C0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Worker 0 processing task 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과 전송 오류: HTTPConnectionPool(host='localhost', port=8080): Max retries exceeded with url: /voice/stt (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002012403C1D0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))\n",
      "결과 전송 오류: HTTPConnectionPool(host='localhost', port=8080): Max retries exceeded with url: /voice/stt (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000200B95F7FE0>: Failed to establish a new connection: [WinError 10061] 대상 컴퓨터에서 연결을 거부했으므로 연결하지 못했습니다'))\n"
     ]
    }
   ],
   "source": [
    "import os  # 운영 체제와 상호작용하기 위한 os 모듈 임포트\n",
    "import tempfile  # 임시 파일 생성을 위한 tempfile 모듈 임포트\n",
    "import logging  # 로그 메시지를 위한 logging 모듈 임포트\n",
    "from flask import Flask, request, jsonify  # 웹 애플리케이션 생성을 위한 Flask와 유틸리티 임포트\n",
    "import whisper  # Whisper ASR 모델 임포트\n",
    "import torch  # 텐서 연산을 위한 PyTorch 임포트\n",
    "import threading  # 스레드를 생성하기 위한 threading 모듈 임포트\n",
    "import queue  # 작업 큐 생성을 위한 queue 모듈 임포트\n",
    "import uuid  # 고유한 작업 ID 생성을 위한 uuid 모듈 임포트\n",
    "import requests\n",
    "\n",
    "# 로그 메시지를 info 레벨로 설정\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)  # 이 모듈을 위한 로거 생성\n",
    "\n",
    "app = Flask(__name__)  # Flask 애플리케이션 인스턴스 생성\n",
    "\n",
    "# 작업 큐와 결과 저장을 위한 딕셔너리 초기화\n",
    "task_queue = queue.Queue()\n",
    "results = {}\n",
    "\n",
    "# 오디오 파일을 처리할 작업자 스레드 수 설정\n",
    "num_worker_threads = 1\n",
    "\n",
    "# 오디오 파일을 처리할 작업자 함수 정의\n",
    "def worker(thread_id):\n",
    "    logger.info(f\"Worker {thread_id} starting, loading Whisper model...\")\n",
    "    model = whisper.load_model(\"base\")  # Whisper ASR 모델 로드\n",
    "    logger.info(f\"Worker {thread_id} loaded Whisper model.\")\n",
    "\n",
    "    while True:  # 작업자가 계속 실행되도록 무한 루프\n",
    "        note_id, audio_path = task_queue.get()  # 큐에서 작업 가져오기\n",
    "        if note_id is None:  # 종료 신호 확인\n",
    "            break  # 종료 신호를 받으면 루프 종료\n",
    "        \n",
    "        logger.info(f\"Worker {thread_id} processing task {note_id}\")\n",
    "        \n",
    "        try:\n",
    "            # Whisper 모델을 사용하여 오디오 파일을 전사\n",
    "            # results[note_id] = {\"status\": \"processing\"}\n",
    "            result = model.transcribe(audio_path)\n",
    "            \n",
    "            send_result_to_spring(note_id, result['segments'])\n",
    "            \n",
    "            # results[note_id] = {\"status\": \"completed\", \"result\": result}  # 결과를 딕셔너리에 저장\n",
    "               \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Worker {thread_id} encountered an error: {str(e)}\")  # 발생한 오류 로그\n",
    "            results[note_id] = {\"status\": \"failed\", \"error\": str(e)}  # 오류를 결과에 저장\n",
    "        \n",
    "        finally:\n",
    "            os.remove(audio_path)  # 임시 오디오 파일 제거\n",
    "            if torch.cuda.is_available():  # CUDA 사용 가능 여부 확인\n",
    "                torch.cuda.empty_cache()  # 필요 시 CUDA 메모리 캐시 지우기\n",
    "        \n",
    "        task_queue.task_done()  # 작업 완료 표시\n",
    "\n",
    "def send_result_to_spring(note_id, result):\n",
    "    # 결과를 Spring 서버에 비동기로 전송\n",
    "    data = {\n",
    "        \"id\": note_id,  # note_id를 문자열로 변환하지 않음\n",
    "        \"result\": result  # result는 JSON 형식으로 예상\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\"http://localhost:8080/voice/stt\", json=data)\n",
    "        response.raise_for_status()  # 잘못된 응답에 대한 에러 발생\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"결과 전송 오류: {e}\")\n",
    "\n",
    "\n",
    "# 작업자 스레드 시작\n",
    "threads = []\n",
    "for i in range(num_worker_threads):\n",
    "    t = threading.Thread(target=worker, args=(i,))  # 작업자 함수에 대한 새 스레드 생성\n",
    "    t.start()  # 스레드 시작\n",
    "    threads.append(t)  # 스레드 목록에 추가\n",
    "\n",
    "@app.route('/', methods=['GET'])  # 홈 페이지를 위한 라우트 정의\n",
    "def home():\n",
    "    return \"Welcome to the Asynchronous Flask Server with Whisper!\"  # 환영 메시지 반환\n",
    "\n",
    "@app.route('/stt', methods=['POST'])  # STT 요청을 위한 라우트 정의\n",
    "def stt_request():\n",
    "    logger.info(\"Received STT request\")\n",
    "    \n",
    "    if not request.json.get('presigned_url'):  # 오디오 파일이 제공되었는지 확인\n",
    "        logger.warning(\"No audio file provided in the request\")\n",
    "        return jsonify({'error': 'No audio file provided'}), 400  # 제공되지 않으면 오류 반환\n",
    "\n",
    "    # https://timeisnullnull.s3.ap-northeast-2.amazonaws.com/codeapple.mp3\n",
    "    audio_uri = request.json.get('presigned_url')  # 요청에서 오디오 파일 가져오기\n",
    "    note_id = request.json.get('id')\n",
    "    \n",
    "    if audio_uri:  # audio_uri가 제공된 경우\n",
    "        try:\n",
    "            logger.info(f\"Downloading audio file from URI: {audio_uri}\")\n",
    "            response = requests.get(audio_uri)  # URI에서 파일 다운로드\n",
    "            response.raise_for_status()  # 오류 발생 시 예외 처리\n",
    "            \n",
    "            # 오디오 데이터를 저장할 임시 파일 생성\n",
    "            with tempfile.NamedTemporaryFile(delete=False) as temp_audio:\n",
    "                temp_audio.write(response.content)  # 다운로드한 파일 내용을 임시 파일에 저장\n",
    "                temp_audio_path = temp_audio.name  # 임시 파일 경로 저장\n",
    "            \n",
    "            logger.info(f\"Audio file downloaded and saved to {temp_audio_path}\")\n",
    "\n",
    "            task_queue.put((note_id, temp_audio_path))  # 큐에 작업 추가\n",
    "\n",
    "            logger.info(f\"Task {note_id} added to the queue\")\n",
    "    \n",
    "            return jsonify({\"note_id\": note_id, \"status\": \"processing\"}), 202  # 작업 ID와 상태 반환\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Failed to download audio file: {str(e)}\")\n",
    "            return jsonify({'error': 'Failed to download audio file'}), 400          \n",
    "\n",
    "@app.errorhandler(500)  # 500 내부 서버 오류 처리\n",
    "def internal_error(error):\n",
    "    logger.error(f\"Internal Server Error: {str(error)}\")  # 오류 로그\n",
    "    return jsonify({\"error\": \"Internal Server Error\"}), 500  # 오류 메시지 반환\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        app.run(host='localhost', port=5000)  # 포트 5000에서 Flask 앱 시작\n",
    "    finally:\n",
    "        # 애플리케이션 종료 시 작업자 스레드 중지\n",
    "        torch.cuda.empty_cache()  # 메모리 캐시 비우기\n",
    "        for _ in range(num_worker_threads):\n",
    "            task_queue.put((None, None))  # 스레드에 중지 신호 전송\n",
    "        for t in threads:\n",
    "            t.join()  # 모든 스레드가 종료될 때까지 대기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f34fd14-07da-4cf7-b2f1-471f76487991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc881748-0e54-450b-bc08-da4fd89186f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'ffmpeg' has no attribute 'probe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mffmpeg\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ffmpeg 실행 결과를 가져오기\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m version \u001b[38;5;241m=\u001b[39m \u001b[43mffmpeg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobe\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(version)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'ffmpeg' has no attribute 'probe'"
     ]
    }
   ],
   "source": [
    "import ffmpeg\n",
    "\n",
    "# ffmpeg 실행 결과를 가져오기\n",
    "version = ffmpeg.probe('version')\n",
    "\n",
    "print(version)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
