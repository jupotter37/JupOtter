{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m284.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: psutil in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: filelock in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (3.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (8.5.0.96)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: jinja2 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (4.6.3)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (11.7.101)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (11.10.3.66)\n",
      "Requirement already satisfied: networkx in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (3.1)\n",
      "Requirement already satisfied: sympy in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (1.12)\n",
      "Requirement already satisfied: wheel in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->accelerate) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->accelerate) (67.8.0)\n",
      "Requirement already satisfied: cmake in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.26.3)\n",
      "Requirement already satisfied: lit in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.5.post0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.20.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.29.2\n",
      "Uninstalling transformers-4.29.2:\n",
      "  Successfully uninstalled transformers-4.29.2\n",
      "Found existing installation: accelerate 0.20.3\n",
      "Uninstalling accelerate-0.20.3:\n",
      "  Successfully uninstalled accelerate-0.20.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall -y transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Using cached accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from transformers) (2023.6.3)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: filelock in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: psutil in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
      "Requirement already satisfied: fsspec in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\n",
      "Requirement already satisfied: sympy in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (11.7.4.91)\n",
      "Requirement already satisfied: jinja2 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (8.5.0.96)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (11.7.91)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from torch>=1.6.0->accelerate) (11.4.0.1)\n",
      "Requirement already satisfied: setuptools in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->accelerate) (67.8.0)\n",
      "Requirement already satisfied: wheel in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->accelerate) (0.38.4)\n",
      "Requirement already satisfied: cmake in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.26.3)\n",
      "Requirement already satisfied: lit in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.5.post0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
      "Installing collected packages: safetensors, transformers, accelerate\n",
      "Successfully installed accelerate-0.20.3 safetensors-0.3.1 transformers-4.30.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/surbhit/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 88.0/88.0 [00:00<00:00, 4.84kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.12k/1.12k [00:00<00:00, 72.4kB/s]\n",
      "Downloading (…)ve/main/spiece.model: 100%|██████████| 1.91M/1.91M [00:00<00:00, 2.85MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 65.0/65.0 [00:00<00:00, 30.6kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 2.28G/2.28G [06:37<00:00, 5.73MB/s]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 280/280 [00:00<00:00, 18.8kB/s]\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samsum_dataset = load_from_disk('Samsum-Data/samsum_dataset/')\n",
    "samsum_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split lengths: [14732, 819, 818]\n",
      "Features; ['id', 'dialogue', 'summary']\n"
     ]
    }
   ],
   "source": [
    "split_lengths = [len(samsum_dataset[split])for split in samsum_dataset]\n",
    "\n",
    "print('Split lengths: {}'.format(split_lengths))\n",
    "print('Features; {}'.format(samsum_dataset['train'].column_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue\n",
      "Amanda: I baked  cookies. Do you want some?\n",
      "Jerry: Sure!\n",
      "Amanda: I'll bring you tomorrow :-)\n",
      "\n",
      "Summary\n",
      "Amanda baked cookies and will bring Jerry some tomorrow.\n"
     ]
    }
   ],
   "source": [
    "#Sample data\n",
    "print(\"Dialogue\")\n",
    "print(samsum_dataset[\"train\"][0][\"dialogue\"])\n",
    "\n",
    "print(\"\\nSummary\")\n",
    "print(samsum_dataset[\"train\"][0][\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_features(example_batch):\n",
    "    input_encodings = tokenizer(example_batch['dialogue'], max_length=1024, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation=True)\n",
    "\n",
    "    return {\n",
    "        'input_ids' : input_encodings['input_ids'],\n",
    "        'attention_mask' : input_encodings['attention_mask'],\n",
    "        'labels' : target_encodings['input_ids']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]/home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3619: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "samsum_dataset_pt = samsum_dataset.map(convert_example_to_features, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 14732\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samsum_dataset_pt['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "trainer_args = TrainingArguments(\n",
    "    output_dir='pegasus-samsum', num_train_epochs=5, warmup_steps=500,\n",
    "    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01, logging_steps=10,\n",
    "    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n",
    "    gradient_accumulation_steps=16\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer = Trainer(model=model_pegasus, args=trainer_args,\n",
    "                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
    "                  train_dataset=samsum_dataset_pt[\"test\"], \n",
    "                  eval_dataset=samsum_dataset_pt[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surbhit/anaconda3/envs/TextSummarization/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/255 [00:00<?, ?it/s]You're using a PegasusTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 3.81 GiB total capacity; 2.79 GiB already allocated; 33.69 MiB free; 2.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/TextSummarization/lib/python3.8/site-packages/transformers/trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1642\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1643\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1644\u001b[0m )\n\u001b[0;32m-> 1645\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1646\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1647\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1648\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1649\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1650\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/TextSummarization/lib/python3.8/site-packages/transformers/trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1926\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1928\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1929\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1931\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1932\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1933\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1934\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1935\u001b[0m ):\n\u001b[1;32m   1936\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1937\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/TextSummarization/lib/python3.8/site-packages/transformers/trainer.py:2761\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2759\u001b[0m         scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m   2760\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2761\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[1;32m   2763\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/anaconda3/envs/TextSummarization/lib/python3.8/site-packages/accelerate/accelerator.py:1821\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1819\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1820\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1821\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/TextSummarization/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/TextSummarization/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 3.81 GiB total capacity; 2.79 GiB already allocated; 33.69 MiB free; 2.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from box import ConfigBox"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('TextSummarization')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d633edf13cec025cdabb99f5daa68647648c6401172487c0e26042f9a3907f1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
