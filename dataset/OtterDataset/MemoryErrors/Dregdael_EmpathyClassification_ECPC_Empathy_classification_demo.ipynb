{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d0a22c-4c57-42ec-9fd1-acccab9a2cc7",
   "metadata": {},
   "source": [
    "# DEMO: Empathy classification using a pattern classifier\n",
    "\n",
    "In this notebook, it is possible to use a previously trained contrast-pattern classification algorithm to obtain the empathy level of a conversation between two people. \n",
    "\n",
    "A conversation prompt is presented, pulled from the EmpatheticExchanges database subset for testing classification algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590e7fcf-9bce-4c89-9bd3-a0662e8ae9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c48750-894d-4cfa-b690-743951e0d5e6",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "This subsection focuses on setting up the environment, functions, utilities, and models required for the demo. Likewise, it is where the variables are manually declared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7d243c2-35f2-4a20-a573-6142fc14b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import random \n",
    "import re\n",
    "#import classifier\n",
    "from PBC4cip import PBC4cip\n",
    "from PBC4cip.core.Evaluation import obtainAUCMulticlass\n",
    "from PBC4cip.core.Helpers import get_col_dist, get_idx_val\n",
    "\n",
    "#utilities for database management\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import train_classifier as trainer\n",
    "import test_classifier as tester\n",
    "import database_processing_package as data_processer\n",
    "\n",
    "#relevant classifiers for annotating exchange feature\n",
    "from classifiers.empathetic_intent import intent_prediction as ip\n",
    "from classifiers.sentiment import sentiment_prediction as sp\n",
    "from classifiers.epitome_mechanisms import epitome_predictor as epitome\n",
    "from classifiers.nrc_vad_lexicon import lexicon_analysis as lexicon\n",
    "from classifiers.course_grained_emotion import pretrained_32emotions as em32\n",
    "from classifiers.course_grained_emotion import emotion_reductor as em_red\n",
    "import database_processing_package as data_processer\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "from time import sleep\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d9ab96-e4aa-47e5-b4f4-5face72f158e",
   "metadata": {},
   "source": [
    "### Selection of features\n",
    "\n",
    "In this cell, we define the model that will be used for this task. We declare its location directory, and its name \"trained_pbc4cip.sav\" \n",
    "\n",
    "Likewise, we declare a \"feature vector\" which contains binary flags for the features used by the model to predict empathy. \n",
    "\n",
    "Finally, we declare the database from which the prompts will be extracted from by declaring its directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74aab462-ebf7-495c-ba30-168b3f718f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relevant directories\n",
    "current_dir = os.getcwd() #get directory of the repository\n",
    "#Select an appropriate classification model in the Experiments folder\n",
    "model_directory = current_dir + '/Experiments/outputs/Experiment '+ str(70) + '/' + 'trained_pbc4cip.sav'\n",
    "\n",
    "\n",
    "feature2number = {'database_to_classify':0,'intent' : 1, 'sentiment' : 2, 'epitome':3, 'VAD_vectors':4, 'utterance_length':5,\n",
    "                  '32_emotion_labels':6,'20_emotion_labels':7, \n",
    "                  '8_emotion_labels':8, 'emotion_mimicry':9, 'Reduce_empathy_labels':10, \n",
    "                  'exchange_number' : 11}\n",
    "\n",
    "\n",
    "database_control_vector = [ 1,#database to classify 0 = empatheticconversations (old), 1 empatheticexchanges (new), selected automatically when reprocess_database flag is active (1)\n",
    "                            1,#intent\n",
    "                            1,#sentiment\n",
    "                            1,#epitome\n",
    "                            1,#vad lexicon\n",
    "                            1,#length\n",
    "                            0,#emotion 32\n",
    "                            0,#emotion 20\n",
    "                            0,#emotion 8\n",
    "                            1,#emotion mimicry\n",
    "                            1,#reduced empathy labels\n",
    "                            1, #exchange number\n",
    "                            1, #output processed database\n",
    "                            0 #7 emotion labels\n",
    "                            ]\n",
    "\n",
    "\n",
    "feature_vector = [1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,1,0]\n",
    "'''\n",
    "                 [1,#database to pull from 0 = empatheticconversations (old), 1 empatheticexchanges (new)\n",
    "                  1,#intent\n",
    "                  1,#sentiment\n",
    "                  0,#epitome\n",
    "                  1,#vad lexicon\n",
    "                  1,#length\n",
    "                  0,#emotion 32\n",
    "                  0,#emotion 20\n",
    "                  1,#emotion 8\n",
    "                  1,#emotion mimicry\n",
    "                  1, #reduce empathy labels\n",
    "                  1 #exchange number\n",
    "                  ]\n",
    "'''\n",
    "\n",
    "if feature_vector[feature2number['database_to_classify']] == 1: \n",
    "    database_dir = '/processed_databases/EmpatheticExchanges/EmpatheticExchanges_test.csv'\n",
    "else: \n",
    "    database_dir = '/processed_databases/EmpatheticConversationsExchangeFormat/EmpatheticConversations_ex.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8548e12f-e577-4f23-b177-d4273c9def5b",
   "metadata": {},
   "source": [
    "### Loading classification models\n",
    "\n",
    "In this cell, we prepare the classification models for obtaining empathy-related features. These models must be pretrained before they are loaded by this demo. \n",
    "\n",
    "### WARNING: DO NOT RUN THIS TWICE. It will cause memory errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e6768f2-4db7-4bb2-87b7-d0b7b391a930",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 29.38 MiB is free. Process 135729 has 6.64 GiB memory in use. Including non-PyTorch memory, this process has 3.58 GiB memory in use. Of the allocated memory 3.24 GiB is allocated by PyTorch, and 244.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#epitome model is loaded during inference due to the code of its classifier\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_vector[feature2number[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepitome\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 11\u001b[0m     epitome_empathy_classifier \u001b[38;5;241m=\u001b[39m epitome\u001b[38;5;241m.\u001b[39mload_epitome_classifier(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifiers/epitome_mechanisms/trained_models\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#load lexicon\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature_vector[feature2number[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVAD_vectors\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/EmpathyClassification_ECPC/classifiers/epitome_mechanisms/epitome_predictor.py:19\u001b[0m, in \u001b[0;36mload_epitome_classifier\u001b[0;34m(mdl_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo GPU available, using the CPU instead.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m empathy_classifier \u001b[38;5;241m=\u001b[39m EmpathyClassifier(device,\n\u001b[1;32m     20\u001b[0m                     ER_model_path \u001b[38;5;241m=\u001b[39m mdl_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/reddit_ER.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     21\u001b[0m                     IP_model_path \u001b[38;5;241m=\u001b[39m mdl_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/reddit_IP.pth\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     22\u001b[0m                     EX_model_path \u001b[38;5;241m=\u001b[39m mdl_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/reddit_EX.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m empathy_classifier\n",
      "File \u001b[0;32m~/EmpathyClassification_ECPC/classifiers/epitome_mechanisms/src/empathy_classifier.py:54\u001b[0m, in \u001b[0;36mEmpathyClassifier.__init__\u001b[0;34m(self, device, ER_model_path, IP_model_path, EX_model_path, batch_size)\u001b[0m\n\u001b[1;32m     51\u001b[0m EX_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(EX_model_path)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_EX\u001b[38;5;241m.\u001b[39mload_state_dict(EX_weights)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_ER\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_IP\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_EX\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/emp_detec/lib/python3.12/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m~/miniconda3/envs/emp_detec/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/emp_detec/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/emp_detec/lib/python3.12/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/emp_detec/lib/python3.12/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 29.38 MiB is free. Process 135729 has 6.64 GiB memory in use. Including non-PyTorch memory, this process has 3.58 GiB memory in use. Of the allocated memory 3.24 GiB is allocated by PyTorch, and 244.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "#load intent model\n",
    "if feature_vector[feature2number['intent']] == 1: \n",
    "    empIntSubDir = './classifiers/empathetic_intent/'\n",
    "    model_intent,tokenizer_intent,device = ip.loadModelTokenizerAndDevice(empIntSubDir) #get model and parameters\n",
    "#load sentiment model\n",
    "if feature_vector[feature2number['sentiment']] == 1: \n",
    "    empIntSubDir = './classifiers/empathetic_intent/'\n",
    "    sent_model, sent_tokenzr = sp.loadSentimentModel() #get model and tokenizer\n",
    "#epitome model is loaded during inference due to the code of its classifier\n",
    "if feature_vector[feature2number['epitome']] == 1:\n",
    "    epitome_empathy_classifier = epitome.load_epitome_classifier('classifiers/epitome_mechanisms/trained_models')\n",
    "#load lexicon\n",
    "if feature_vector[feature2number['VAD_vectors']] == 1:\n",
    "    lexicon_df, wnl, stp_wrds = lexicon.setup_lexicon('classifiers/nrc_vad_lexicon/BipolarScale/NRC-VAD-Lexicon.txt')\n",
    "#load emotion classifier with 32 labels for any of the emotion labels options\n",
    "if (feature_vector[feature2number['32_emotion_labels']] == 1) or (feature_vector[feature2number['20_emotion_labels']] == 1) or (feature_vector[feature2number['8_emotion_labels']] == 1):\n",
    "    emo32_model, emo32_tokenzr = em32.load32EmotionsModel() #get model and tokenizer\n",
    "#it is necessary to get the VAD vectors for obtaining emotion mimicry\n",
    "if feature_vector[feature2number['emotion_mimicry']] == 1:\n",
    "    lexicon_df, wnl, stp_wrds = lexicon.setup_lexicon('classifiers/nrc_vad_lexicon/BipolarScale/NRC-VAD-Lexicon.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08573e51-15fc-4461-b8a6-65f7ca8a13a7",
   "metadata": {},
   "source": [
    "### Definition of data processing function\n",
    "\n",
    "This is a function used to transform a text exchange into the format necessary for classification. It adds the following features: \n",
    "\n",
    "* Sentiment\n",
    "* EPITOME mechanisms (Sharma, 2019)\n",
    "* Valence, Arousal, and Dominance emotion vectors\n",
    "* Utterance lengths for both participants\n",
    "* Emotion labels\n",
    "* Empathetic Intent\n",
    "* Whether there is emotion mimicry\n",
    "\n",
    "This features are dependent on the feature vector defined at the start of this notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2ab89d-2d9d-433e-92d4-43c7f1de60c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_answer(sample_df,control_vector):\n",
    "    print('processing data....')\n",
    "    if control_vector[feature2number['sentiment']] == 1: \n",
    "        sample_df['speaker_sentiment'] = sample_df.apply(data_processer.get_sentiment_probabilities,axis = 1, args = (sent_model,sent_tokenzr,'speaker_utterance')) \n",
    "        sample_df[['s_negative','s_neutral', 's_positive']] = pd.DataFrame(sample_df.speaker_sentiment.tolist(),index = sample_df.index)\n",
    "        sample_df['listener_sentiment'] = sample_df.apply(data_processer.get_sentiment_probabilities,axis = 1, args = (sent_model,sent_tokenzr,'listener_utterance')) \n",
    "        sample_df[['l_negative','l_neutral', 'l_positive']] = pd.DataFrame(sample_df.listener_sentiment.tolist(),index = sample_df.index)\n",
    "        sample_df = sample_df.drop(columns=['speaker_sentiment','listener_sentiment'])\n",
    "    if control_vector[feature2number['epitome']] == 1:\n",
    "        sample_df = epitome.classify_epitome_values(epitome_empathy_classifier, sample_df)\n",
    "    if control_vector[feature2number['VAD_vectors']] == 1:\n",
    "        sample_df['vad_speaker'] = sample_df['speaker_utterance'].apply(lexicon.get_avg_vad, args = (lexicon_df,wnl,stp_wrds)) \n",
    "        sample_df['vad_listener'] = sample_df['listener_utterance'].apply(lexicon.get_avg_vad, args = (lexicon_df,wnl,stp_wrds)) \n",
    "        sample_df[['valence_speaker','arousal_speaker','dominance_speaker']] = pd.DataFrame(sample_df.vad_speaker.tolist(),index = sample_df.index)\n",
    "        sample_df[['valence_listener','arousal_listener','dominance_listener']] = pd.DataFrame(sample_df.vad_listener.tolist(),index = sample_df.index)\n",
    "        sample_df = sample_df.drop(columns = ['vad_speaker','vad_listener'])\n",
    "    if control_vector[feature2number['utterance_length']] == 1:\n",
    "        sample_df['s_word_len'] = sample_df['speaker_utterance'].apply(data_processer.get_word_len) \n",
    "        sample_df['l_word_len'] = sample_df['listener_utterance'].apply(data_processer.get_word_len) \n",
    "    if (control_vector[feature2number['32_emotion_labels']] == 1) or (control_vector[feature2number['20_emotion_labels']] == 1) or (control_vector[feature2number['8_emotion_labels']] == 1):\n",
    "        sample_df['speaker_emotion'] = sample_df.apply(data_processer.get_emotion_label,axis = 1, args = (emo32_model,emo32_tokenzr,'speaker_utterance')) \n",
    "        sample_df['listener_emotion'] = sample_df.apply(data_processer.get_emotion_label,axis = 1, args = (emo32_model,emo32_tokenzr,'listener_utterance')) \n",
    "        if (control_vector[feature2number['20_emotion_labels']] == 1): \n",
    "            sample_df = em_red.reduce_emotion_labels('speaker_emotion',sample_df)\n",
    "            sample_df = em_red.reduce_emotion_labels('listener_emotion',sample_df)\n",
    "        if (control_vector[feature2number['8_emotion_labels']] == 1): \n",
    "            sample_df = em_red.reduce_emotion_labels_to_8('speaker_emotion',sample_df)\n",
    "            sample_df = em_red.reduce_emotion_labels_to_8('listener_emotion',sample_df)\n",
    "    if control_vector[feature2number['intent']] == 1: \n",
    "        sample_df['utterance'] = str(answer)\n",
    "        sample_df['is_response'] = 1\n",
    "        sample_df['empathetic_intent'] = sample_df.apply(data_processer.get_emp_intent_probabilities, axis=1, args = (model_intent,tokenizer_intent,device,'listener_utterance'))\n",
    "        sample_df[data_processer.intent_labels] = pd.DataFrame(sample_df.empathetic_intent.tolist(),index = sample_df.index)\n",
    "        sample_df = sample_df.drop(columns=['empathetic_intent','utterance','is_response'])\n",
    "    if control_vector[feature2number['emotion_mimicry']] == 1:\n",
    "        if(control_vector[4] == 1):\n",
    "            #get the emotional similarity, if it is more than 0.7 set mimicry to 1\n",
    "            sample_df['emotional_similarity'] = sample_df.apply(data_processer.get_cosine_similarity,axis = 1) \n",
    "            sample_df['mimicry'] = sample_df.apply(lambda x: 1 if x['emotional_similarity'] > 0.7 else 0, axis = 1)\n",
    "            sample_df = sample_df.drop(columns = ['emotional_similarity'])\n",
    "        else: \n",
    "            sample_df['vad_speaker'] = sample_df['speaker_utterance'].apply(lexicon.get_avg_vad, args = (lexicon_df,wnl,stp_wrds)) \n",
    "            sample_df['vad_listener'] = sample_df['listener_utterance'].apply(lexicon.get_avg_vad, args = (lexicon_df,wnl,stp_wrds)) \n",
    "            sample_df[['valence_speaker','arousal_speaker','dominance_speaker']] = pd.DataFrame(sample_df.vad_speaker.tolist(),index = sample_df.index)\n",
    "            sample_df[['valence_listener','arousal_listener','dominance_listener']] = pd.DataFrame(sample_df.vad_listener.tolist(),index = sample_df.index)\n",
    "            sample_df = sample_df.drop(columns = ['vad_speaker','vad_listener'])                \n",
    "            sample_df['emotional_similarity'] = sample_df.apply(data_processer.get_cosine_similarity,axis = 1) \n",
    "            sample_df['mimicry'] = sample_df.apply(lambda x: 1 if x['emotional_similarity'] > 0.7 else 0, axis = 1)\n",
    "            sample_df = sample_df.drop(columns =  ['valence_speaker','arousal_speaker','dominance_speaker','valence_listener','arousal_listener','dominance_listener','emotional_similarity'])\n",
    "        sample_df['mimicry'] = sample_df['mimicry'].astype('category')\n",
    "        sample_df['mimicry'] = sample_df['mimicry'].astype('string')\n",
    "        #sample_df = sample_df.drop(columns =  ['predictions_EX'])\n",
    "    print('done')\n",
    "    return sample_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53078ccd-bfa0-4c1c-8a7d-b9c4572d2008",
   "metadata": {},
   "source": [
    "### database setup\n",
    "\n",
    "We load the database. Next, we filter it to have only samples that start a conversation. This is done by selecting those that have an \"exchange_number\" variable of 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5739e968-0026-46a3-80cf-8d9ec1ed07d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = pd.read_csv(current_dir + database_dir)\n",
    "\n",
    "starting_exchange_db = database[database['exchange_number'] == 1]\n",
    "starting_exchange_db = starting_exchange_db.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de78d7e6-b49d-4268-9b9f-41f665cb382e",
   "metadata": {},
   "source": [
    "### Load our classification model\n",
    "\n",
    "In this cell, we run the empathy classification model that we have previously trained. The model selection is done through specifying the directory in which the model was saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec3862b-594f-4d71-bbf2-c3fc7c672abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = current_dir + '/Experiments/outputs/Experiment '+ str(70) + '/' + 'trained_pbc4cip.sav'\n",
    "\n",
    "pbc = pickle.load(open(model_directory, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a4fd79-91cd-47cf-bfc8-8aac6c569646",
   "metadata": {},
   "source": [
    "## Application\n",
    "\n",
    "In this subsection, we present that working parts of the demo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93e8509-f8a2-49ba-99ba-5ae0b971e6a4",
   "metadata": {},
   "source": [
    "### Conversation starter\n",
    "\n",
    "We randomly sample the database for a conversation prompt. This is equivalent to an utterance of a first agent, to which we will provide a response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb94567e-25c0-4522-8f1e-a68816831299",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_of_db = len(starting_exchange_db)\n",
    "index_of_sample = random.randint(0, len_of_db)\n",
    "sample_text = starting_exchange_db.loc[index_of_sample,'speaker_utterance']\n",
    "sample_text = re.sub(\"_comma_\", ',', sample_text)\n",
    "print(f'Prompt: \"{sample_text}\"') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c2edf-6f8b-439c-983f-2c4dce6b04fa",
   "metadata": {},
   "source": [
    "### Response\n",
    "\n",
    "We provide a response to the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963666b4-3915-4a26-b780-b3d98d972397",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = True\n",
    "while(flag):\n",
    "    answer = input(\"Provide your response: \")\n",
    "    if answer.lower() == '':\n",
    "        print('No answer received, please provide a response')\n",
    "    else:\n",
    "        flag = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8de813-f386-40e2-8e56-83395a83517f",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "In this cell, the prompt-response pair is processed to have the format and features required for the classification algorithm. \n",
    "\n",
    "Subsequently, the data is passed to the classifier, and a prediction is made. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427849ee-3c1d-4775-80b9-3a59b8e70732",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = starting_exchange_db.iloc[[index_of_sample]]\n",
    "df = df.reset_index(drop=True)\n",
    "columns_list = starting_exchange_db.columns.to_list()\n",
    "df.loc[0, 'listener_utterance'] = str(answer)\n",
    "C = list(set(columns_list) - set(['speaker_utterance','listener_utterance','empathy','exchange_number']))\n",
    "df = df.drop(columns = C)\n",
    "df = process_answer(df,feature_vector)\n",
    "df = df.drop(columns = ['speaker_utterance', 'listener_utterance'])\n",
    "x_test = df.drop(columns=['empathy'])\n",
    "y_test = df.drop(columns=x_test.columns)\n",
    "y_pred = pbc.predict(x_test)\n",
    "print(f'Our classification algorithm predicts a level of {int(y_pred[0]) + 1} out of 3 for the perceived empathy of your response')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5623fa0-bded-4fc9-8da9-89155d0379be",
   "metadata": {},
   "source": [
    "### Multi-turn inference\n",
    "\n",
    "In this cell, we run multiple inferences through utterance exchanges. Together, these exchanges form a conversation centered in an emotional topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce614472-749f-4913-a783-b9d579c17d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = 0\n",
    "\n",
    "#1470 <---- index of a good conversation\n",
    "# hit:7872_conv:15745\n",
    "\n",
    "while(flag == 0):\n",
    "    len_of_db = len(starting_exchange_db)\n",
    "    index_of_sample = random.randint(0, len_of_db-1)\n",
    "    sample_conv_start = starting_exchange_db.loc[index_of_sample,'speaker_utterance']\n",
    "    conv_id = starting_exchange_db.loc[index_of_sample,'conv_id']\n",
    "    #print(conv_id)\n",
    "    #print(f'Prompt: \"{sample_text}\"') \n",
    "    \n",
    "    full_database = pd.read_csv(current_dir + '/processed_databases/EmpatheticExchanges/EmpatheticExchanges.csv')\n",
    "    conv_df = full_database[full_database['conv_id'] == conv_id]\n",
    "    conv_df = conv_df.reset_index(drop=True)\n",
    "    if len(conv_df) > 2 and conv_df.loc[0,'empathy'] > 2:\n",
    "        flag = 1\n",
    "    else:\n",
    "        flag = 0\n",
    "print('Example of an empathetic multi-turn conversation')\n",
    "for i in range(len(conv_df)):\n",
    "    print(f'Turn {conv_df.loc[i,'exchange_number']}')\n",
    "    print(f'Agent A: {conv_df.loc[i,'speaker_utterance']}')\n",
    "    print(f'Agent B: {conv_df.loc[i,'listener_utterance']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83267db2-c137-4f18-a596-655861fcf154",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m mt_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(conv_df)):\n\u001b[1;32m      3\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUtterance by Agent A: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m query) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m query):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conv_df' is not defined"
     ]
    }
   ],
   "source": [
    "mt_df = pd.DataFrame()\n",
    "for i in range(len(conv_df)):\n",
    "    query = input(\"Utterance by Agent A: \")\n",
    "    if ('end' in query) or ('quit' in query):\n",
    "        break\n",
    "    answer = input(\"Utterance by Agent B: \")\n",
    "    if ('end' in answer) or ('quit' in answer):\n",
    "        break\n",
    "    datarow = {'speaker_utterance': [str(query)], 'listener_utterance': [str(answer)],'exchange_number': [i+1], 'empathy': [3]}\n",
    "    mt_df = pd.concat([mt_df,pd.DataFrame.from_dict(datarow)])\n",
    "    ex_df = process_answer(mt_df.iloc[[i]].reset_index(drop=True), feature_vector)\n",
    "    ex_df = ex_df.drop(columns = ['speaker_utterance', 'listener_utterance'])\n",
    "    x_test = ex_df.drop(columns=['empathy'])\n",
    "    y_test = ex_df.drop(columns=x_test.columns)\n",
    "    y_pred = pbc.predict(x_test)\n",
    "    print(f'Predicted empathy of the exchange: {y_pred[0]+1}/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed32fb50-d922-4738-ad4a-4729756b0ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I was really nervous to move across country \n",
      "Knew no one where we were moving and also far away from my mother who is getting old. \n",
      "oh sorry we knew no one where we were moving to \n",
      "Amazingly hehe. But here I am. \n"
     ]
    }
   ],
   "source": [
    "persona = ['I was really nervous to move across country','Knew no one where we were moving and also  far away from my mother who is getting old.', 'oh sorry we knew no one where we were moving to', 'Amazingly hehe. But here I am.'] \n",
    "personb = ['why were you?','no one knew that you were moving?','oh that has to be scary', 'here you are killing it ']\n",
    "print('')\n",
    "\n",
    "for i in range(4):\n",
    "    print('', end = '')\n",
    "    dialogue = persona[i].split()\n",
    "    for j in range(len(persona[i].split())):\n",
    "        print(dialogue[j], end = ' ')\n",
    "        sleep(0.5)\n",
    "    print('', end = '\\r')\n",
    "    print()\n",
    "    sleep(1)\n",
    "    \n",
    "    #print('Person B: ', end = '')\n",
    "    #dialogue = personb[i].split()\n",
    "    #for j in range(len(personb[i].split())):\n",
    "    #    print(dialogue[j], end = ' ')\n",
    "    #    sleep(0.5)\n",
    "    #print()\n",
    "    #if i == 0:\n",
    "    #    emp_ex = 1\n",
    "    #    empathy_level = emp_ex\n",
    "    #if i == 1:\n",
    "    #    emp_ex = 2\n",
    "    #    empathy_level += emp_ex\n",
    "    #if i > 1:\n",
    "    #    emp_ex = 3\n",
    "    #    empathy_level += emp_ex\n",
    "    ##print('Processing data....')\n",
    "    #sleep(0.05)\n",
    "    #print('done')\n",
    "    #print(f'Predicted empathy of the exchange: {emp_ex}/3')\n",
    "    #loading = '['\n",
    "    #for i in range(empathy_level):\n",
    "    #    loading += '#'\n",
    "    #for i in range(12-empathy_level):\n",
    "    #    loading += '.'\n",
    "    #loading += ']'\n",
    "    #print('Empathy score')\n",
    "    #print(loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d826acf0-7efd-47f9-a436-3549ee24ba23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da7805c-a834-417e-979a-2eb17666274f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
