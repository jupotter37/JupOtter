{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuniorHZ19/HerramientasIA/blob/main/hresolutionprueba2.0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hsgw-_4AS6fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # *0) ** Instalando libreria(OBLIGATORIO)\n",
        "\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install google_images_download"
      ],
      "metadata": {
        "id": "R0Je-JZAxuYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # *1) ** Descargando DataSet  caras gatos(OBLIGATORIO)\n",
        "import zipfile\n",
        "\n",
        "nombre_zip=\"data_set_gatos.zip\"\n",
        "\n",
        "directorio_destino=\"/content/\"\n",
        "\n",
        "!gdown --id 1EK7H_QJqxDy0wBssMEi2_gWdo3zE2pmQ -O {nombre_zip}\n",
        "\n",
        "with zipfile.ZipFile(nombre_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(directorio_destino)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxzaMyUeTGCC",
        "outputId": "b9c82307-9f3d-4a1e-9e39-39c2460e7253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EK7H_QJqxDy0wBssMEi2_gWdo3zE2pmQ\n",
            "To: /content/data_set_gatos.zip\n",
            "100% 284M/284M [00:03<00:00, 91.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # *1) ** Clase Para manejo de directorios de datasets de imagenes\n",
        "\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "# Recorre el directorio  y elmiina los archvios que no tiene las extensioens permitidas\n",
        "\n",
        "class DataSetManage:\n",
        "\n",
        " def comprobar_ext_directorios(self,directorios):\n",
        "\n",
        "  for clase,[directorio,etiquetas] in(directorios.items()):\n",
        "   lista_directorio=os.listdir(directorio)\n",
        "   self.validarExt(directorio)\n",
        "\n",
        "\n",
        "\n",
        " def  validarExt(self,directorio):\n",
        "  print(directorio)\n",
        "   # Extensiones permitidas\n",
        "  extensiones_permitidas = {\".jpg\", \".jpeg\", \".png\"}\n",
        "  for root, dirs, files in os.walk(directorio):\n",
        "\n",
        "    for file in files:\n",
        "        # Obtiene la extensión del archivo\n",
        "        _, extension = os.path.splitext(file)\n",
        "\n",
        "        # Verifica si la extensión no está en la lista de extensiones permitidas y elimina el archivo\n",
        "        if extension.lower() not in extensiones_permitidas:\n",
        "            archivo_a_eliminar = os.path.join(root, file)\n",
        "            os.remove(archivo_a_eliminar)\n",
        "            print(f\"Se eliminó: {archivo_a_eliminar}\")\n",
        "\n",
        "\n",
        "# Cambia nombre de cada archivo dentro del directorio a un valor secuencial\n",
        "\n",
        " def cambiar_nombres_directorios(self,directorios):\n",
        "   for clase,[directorio,etiquetas] in(directorios.items()):\n",
        "     lista_directorio=os.listdir(directorio)\n",
        "     self.cambiarNombre(directorio,clase)\n",
        "\n",
        "\n",
        " def cambiarNombre(self,directorio,subfijo):\n",
        "  archivos_en_directorio = os.listdir(directorio)\n",
        "  for i, archivo in enumerate(archivos_en_directorio, start=1):\n",
        "    # Construir el nuevo nombre del archivo\n",
        "    nuevo_nombre = f\"{subfijo}{i}{os.path.splitext(archivo)[1]}\"\n",
        "\n",
        "    # Ruta completa del archivo antiguo y nuevo\n",
        "    ruta_antigua = os.path.join(directorio, archivo)\n",
        "    ruta_nueva = os.path.join(directorio, nuevo_nombre)\n",
        "\n",
        "    # Cambiar el nombre del archivo\n",
        "    os.rename(ruta_antigua, ruta_nueva)\n",
        "    print(f\"Se cambió el nombre de {ruta_antigua} a {ruta_nueva}\")\n",
        "\n",
        "\n",
        "#Obtiene la cantidad de elemntos que tiene la carpeta\n",
        "\n",
        " def len_directorio(self,directorio):\n",
        "    cantidad_elementos = sum(1 for elemento in os.listdir(directorio) if os.path.isfile(os.path.join(directorio, elemento)))\n",
        "    return cantidad_elementos\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------------------------\n",
        "\n",
        "#Valida si la imagen se puede leer usando  pill o cv2 si no se puede leer se elimina\n",
        "\n",
        " def validar_Img_Pill(self,directorios,):\n",
        "  for etiqueta,[directorio,clase] in(directorios.items()):\n",
        "\n",
        "    lista_paths=os.listdir(directorio)\n",
        "    self.validarLecturaImgPill(directorio,lista_paths)\n",
        "  print(f\"Se ah validado todas las imagenes\")\n",
        "\n",
        " def validar_Img_cv2(self,directorios):\n",
        "  for etiqueta,[directorio,clase] in(directorios.items()):\n",
        "\n",
        "    lista_paths=os.listdir(directorio)\n",
        "    self.validarLecturaImg(directorio,lista_paths)\n",
        "  print(f\"Se ah validado todas las imagenes\")\n",
        "\n",
        "\n",
        " def validarLecturaImg(self,directorio,lista):\n",
        "\n",
        "  for ruta in(lista):\n",
        "   imagen=cv2.imread(directorio+ruta)\n",
        "   if  imagen is None:\n",
        "     os.remove(directorio+ruta)\n",
        "     print(f\"No se pudo leer y se elimino archivo:{directorio+ruta}\")\n",
        "\n",
        " def validarLecturaImgPill(self,directorio,lista):\n",
        "    for ruta in(lista):\n",
        "     try:\n",
        "      imagen=Image.open(directorio+ruta)\n",
        "     except Exception as e:\n",
        "      os.remove(directorio+ruta)\n",
        "      print(f\"Archivo '{directorio+ruta}' eliminado.\")\n",
        "\n",
        "\n",
        "\n",
        "#vamos a recorrer el dicionario y validar ruta por ruta si se puede leer sino se elminara\n",
        "#vamos guaradno al mismo tiempo 3 listas, los directorios , listas de paths de los directiros y de las clases ,para usarlo luego usarlo al crear el csv\n",
        "\n",
        " def separar_datos_directorios(self,directorios):\n",
        "  listas_directorios=[]\n",
        "  listas_listas_directorios=[]\n",
        "  listas_clases=[]\n",
        "\n",
        "  for etiqueta,[directorio,clase] in(directorios.items()):\n",
        "\n",
        "       lista_paths=os.listdir(directorio)\n",
        "       listas_directorios.append(directorio)\n",
        "       listas_listas_directorios.append(lista_paths)\n",
        "       listas_clases.append(clase)\n",
        "\n",
        "  return listas_directorios,listas_listas_directorios,listas_clases\n",
        "\n",
        " def emparejar_listas_paths(self,lista_listas):\n",
        "\n",
        "   #Tomamos el minimo tamaño dentro de las lista de cada clase\n",
        "   tamaño_minimo = min(len(arr) for arr in lista_listas)\n",
        "\n",
        "   #Vamos a emparejar todas las listas con un tamaño igual que sea la del minimo tamaño de todas,esto para tener un set de datos parejo por cada clase\n",
        "   Reducido_lista_paths=[]\n",
        "\n",
        "   #Reduce cada lista de los paths a la cantidad minimo para que todos tenga iaugal cantidad\n",
        "   for listas in(lista_listas):\n",
        "    Reducido_lista_paths.append(listas[:tamaño_minimo])\n",
        "\n",
        "   return Reducido_lista_paths\n",
        "#---------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        " def crear_paths_csv(self,directorio_base,lista_paths,clases,nombre_archivo):\n",
        "\n",
        "   columnas=[\"path\",\"etiqueta\"]\n",
        "   datos_csv=[]\n",
        "\n",
        "   for dir_base,dir_path,clase in  zip(directorio_base,lista_paths,clases):\n",
        "\n",
        "    for path  in (dir_path):\n",
        "\n",
        "     datos_csv.append([dir_base+path ,clase])\n",
        "\n",
        "\n",
        "   df_lista=pd.DataFrame(datos_csv,columns=columnas)\n",
        "   df_lista.to_csv(nombre_archivo,index=False)\n",
        "   print(\"Csv Creado\")\n",
        "#---------------------------------------------------------------------------------\n",
        "\n",
        "#devuelve cuatnos elemtnos tiene cada clase\n",
        " def total_elementos(self,directorios,csv_path):\n",
        "    df=pd.read_csv(csv_path)\n",
        "    for clase,[directorio,etiqueta] in (directorios.items()):\n",
        "     tamaño_etiqueta=(df[\"etiqueta\"] == etiqueta).sum()\n",
        "     print(f\"la clase {clase} tiene :{tamaño_etiqueta} elementos\")\n",
        "\n"
      ],
      "metadata": {
        "id": "4Byd3D3TT7Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dmg=DataSetManage()\n",
        "\n",
        "directorios={\n",
        "     \"highResolution\":[\"/content/highresolution/\",1],\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "dmg.comprobar_ext_directorios(directorios)\n",
        "\n",
        "dmg.cambiar_nombres_directorios(directorios)\n",
        "\n",
        "dmg.validar_Img_Pill(directorios)\n",
        "\n",
        "listas_directorios,listas_paths_directorios,listas_clases=dmg.separar_datos_directorios(directorios)\n",
        "\n",
        "Reducido_lista_paths=dmg.emparejar_listas_paths(listas_paths_directorios)\n",
        "#validamos que todas las iamgnes en nuestro direcotrios puedan abrirse con openia cv2\n",
        "\n",
        "\n",
        "\n",
        "#La funcion que creamos para podre crear nuestro csv , con los parametros antes calucaldos que son 3 listas y el nombre del csv\n",
        "\n",
        "Dataset_csv=\"Data_set.csv\" #nombre que tendla nuestlo csv\n",
        "dmg.crear_paths_csv(listas_directorios,Reducido_lista_paths,listas_clases,Dataset_csv)\n",
        "\n",
        "#imprime total de elemtnos por clase\n",
        "dmg.total_elementos(directorios,\"/content/Data_set.csv\")"
      ],
      "metadata": {
        "id": "Kzybet7nU6gH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # *2) ** Creacion de clase DATASET(OBLIGATORIO)\n",
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class MiDataSet(Dataset):\n",
        "\n",
        "  def __init__(self,csv_file, transform=None):\n",
        "\n",
        "\n",
        "     self.data=pd.read_csv(csv_file)\n",
        "\n",
        "     self.x=self.data[\"path\"]\n",
        "     self.y=self.data[\"etiqueta\"]\n",
        "\n",
        "     self.transform=transform\n",
        "\n",
        "     self.samples=self.data[\"path\"].shape[0]\n",
        "\n",
        "  def __getitem__(self,id):\n",
        "\n",
        "    rut_imagen=self.x[id]\n",
        "    imagen=cv2.imread(rut_imagen)\n",
        "    etiqueta=self.y[id]\n",
        "\n",
        "\n",
        "    if imagen is None:\n",
        "      pillow_image = Image.open(rut_imagen)\n",
        "      numpy_image = np.array(pillow_image)\n",
        "      imagen=cv2.cvtColor(numpy_image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "\n",
        "    imagen_rgb = cv2.cvtColor(imagen, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "    if self.transform:\n",
        "      imagen_rgb = self.transform(imagen_rgb)\n",
        "\n",
        "    return imagen_rgb,etiqueta\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "   return self.samples\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5vcmR_ZKhkGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WYVki2qv_EJ"
      },
      "outputs": [],
      "source": [
        "#@markdown # *0) ** Creando clase Discriminadora y Geneaadora:\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Discriminator (nn.Module):\n",
        "  def __init__(self,input_size):\n",
        "   super(Discriminator,self).__init__()\n",
        "\n",
        "   self.conv1=nn.Conv2d(3,48,kernel_size=3,stride=2, bias=False)#=970x970x48\n",
        "\n",
        "\n",
        "   self.conv2=nn.Conv2d(48,56,kernel_size=3,stride=1, bias=False)#=484×484x56\n",
        "\n",
        "   self.conv3=nn.Conv2d(56,70,kernel_size=3,stride=4, bias=False)#=121×121x70\n",
        "\n",
        "   self.maxpol1=nn.MaxPool2d(kernel_size=4,stride=1)#=30×30x70\n",
        "\n",
        "   self.maxpol2=nn.MaxPool2d(kernel_size=2,stride=1)#=15×15×3.\n",
        "\n",
        "   self.flatt=nn.Flatten()\n",
        "\n",
        "   self.oculta1=nn.Linear(15*15*3,500)\n",
        "\n",
        "   self.salida= nn.Linear(500,1)\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    x=self.conv1(x)\n",
        "\n",
        "    x= nn.LeakyReLU(0.1)(x)\n",
        "\n",
        "    x=self.conv2(x)\n",
        "\n",
        "    x= nn.LeakyReLU(0.1)(x)\n",
        "\n",
        "    x=self.conv3(x)\n",
        "\n",
        "    x= nn.LeakyReLU(0.1)(x)\n",
        "\n",
        "    x=self.maxpol1(x)\n",
        "    x= nn.LeakyReLU(0.1)(x)\n",
        "\n",
        "    x=self.maxpol2(x)\n",
        "    x= nn.LeakyReLU(0.1)(x)\n",
        "\n",
        "\n",
        "\n",
        "    x=self.flatt(x)\n",
        "    x=self.oculta1(x)\n",
        "    x= F.relu(x)\n",
        "\n",
        "    x=self.salida(x)\n",
        "    out=torch.sigmoid(x)\n",
        "\n",
        "    return out\n",
        "\n",
        "#------------------------------------------------------------------------\n",
        "class Generador(nn.Module):\n",
        "\n",
        "  def __init__(self,img,output_size):\n",
        "   super(Generador,self).__init__()\n",
        "\n",
        "   self.tconv1=nn.ConvTranspose2d(3,12,kernel_size=3,stride=2, bias=False)\n",
        "\n",
        "\n",
        "   self.tconv2=nn.ConvTranspose2d(24,48,kernel_size=4,stride=2, bias=False)\n",
        "\n",
        "\n",
        "   self.tconv3=nn.ConvTranspose2d(48,24,kernel_size=7,stride=2, bias=False)\n",
        "\n",
        "   self.tconv4=nn.ConvTranspose2d(24,3,kernel_size=7,stride=2, bias=False)#1941*1941*3\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "\n",
        "    x= self.tconv1(x)\n",
        "\n",
        "    x= F.relu(x)\n",
        "\n",
        "    x= self.tconv2(x)\n",
        "\n",
        "    x= F.relu(x)\n",
        "\n",
        "    x= self.tconv3(x)\n",
        "    x= F.relu(x)\n",
        "\n",
        "    x= self.tconv4(x)\n",
        "    out= torch.tanh(x)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "TuQmnT8vZej0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision.utils as vutils\n",
        "import sklearn\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "# ...\n",
        "\n",
        "#d=Discriminator(image_dim).to(device)\n",
        "#g=Generador(dim_vector_ruido,image_dim).to(device)\n",
        "\n",
        "#d.load_state_dict(torch.load(\"/content/modelo_gatos_d.pt\"))\n",
        "#g.load_state_dict(torch.load(\"/content/modelo_gatos_g.pt\"))\n",
        "\n",
        "#d=d.to(device)\n",
        "#g=g.to(device)\n",
        "\n",
        "\n",
        "# Supongamos que ya has definido las instancias de los modelos Generador (g) y Discriminador (d),\n",
        "# y has configurado los optimizadores (g_optimizer y d_optimizer) y la función de pérdida (criterio).\n",
        "\n",
        "\n",
        "\n",
        "def GAN(discriminador, generador, z_dim, data_loader,data_loader_hq, num_epochs, batch_size, criterio, d_optimizador, g_optimizador, device):\n",
        "    generador.to(device)\n",
        "    discriminador.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for (x,_), (xhq,_) in zip(data_loader,data_loader_hq):\n",
        "\n",
        "            generador.eval()\n",
        "            discriminador.train()\n",
        "\n",
        "            real_images = x.to(device)\n",
        "            real_labels = torch.ones(x.size(0), 1).to(device)\n",
        "\n",
        "            # Entrenar el discriminador con imágenes reales\n",
        "            d_optimizador.zero_grad()\n",
        "            prediction_real = discriminador(real_images)\n",
        "            real_loss = criterio(prediction_real, real_labels)\n",
        "            real_loss.backward()\n",
        "\n",
        "            # Entrenar el discriminador con imágenes generadas\n",
        "\n",
        "            fake_labels = torch.zeros(x.size(0), 1).to(device)\n",
        "            xhq=xhq.to(device)\n",
        "            fake_images = generador(xhq).to(device)\n",
        "            prediction_fake = discriminador(fake_images)\n",
        "            fake_loss = criterio(prediction_fake, fake_labels)\n",
        "            fake_loss.backward()\n",
        "\n",
        "            discriminator_loss = real_loss + fake_loss\n",
        "            d_optimizador.step()\n",
        "\n",
        "            # Entrenar el generador\n",
        "            generador.train()\n",
        "            discriminador.eval()\n",
        "\n",
        "            g_optimizador.zero_grad()\n",
        "            generated_images = generador(xhq).to(device)\n",
        "            generator_loss = criterio(discriminador(generated_images), real_labels)\n",
        "            generator_loss.backward()\n",
        "            g_optimizador.step()\n",
        "            print(f'Época [{epoch}/{num_epochs}]], '\n",
        "                f'Pérdida del Discriminador: {discriminator_loss.item():.4f}, '\n",
        "                f'Pérdida del Generador: {generator_loss.item():.4f}')\n",
        "\n",
        "        # Imprimir estadísticas y visualizar imágenes generadas al final de cada época\n",
        "        with torch.no_grad():\n",
        "            generador.eval()\n",
        "            noise = torch.randn(1, z_dim).to(device)\n",
        "            generated_images = generador(noise).detach().cpu()\n",
        "\n",
        "            # Convertir las imágenes al rango [0, 1]\n",
        "            generated_images = (generated_images + 1) / 2.0\n",
        "\n",
        "            # Visualizar las imágenes generadas\n",
        "            plt.figure(figsize=(8, 8))\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(f\"Imágenes Generadas - Época {epoch + 1}\")\n",
        "            plt.imshow(vutils.make_grid(generated_images, nrow=4).permute(1, 2, 0).numpy())\n",
        "            plt.show()\n",
        "#--------------------------------------------------------------------------------------------\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"CUDA está disponible.\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"CUDA no está disponible. Se utilizará la CPU.\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "batch_size=5\n",
        "dim_vector_ruido =100\n",
        "image_dim=60*60*3 #dimensiones de la imagen que entrara al discrimiador\n",
        "num_epochs=100\n",
        "\n",
        "transformaciones = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "    transforms.Resize((60, 60),antialias=True)\n",
        "\n",
        "])\n",
        "\n",
        "transformacionesGenerador = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "    transforms.Resize((240, 240),antialias=True)\n",
        "\n",
        "])\n",
        "\n",
        "Dataset=MiDataSet(\"/content/Data_set.csv\",transform=transformaciones)\n",
        "data_loader=DataLoader(Dataset,batch_size=batch_size,shuffle=True)\n",
        "\n",
        "DatasetHq=MiDataSet(\"/content/Data_set.csv\",transform=transformacionesGenerador)\n",
        "data_loader_Hq=DataLoader(Dataset,batch_size=batch_size,shuffle=True)\n",
        "\n",
        "d=Discriminator(image_dim).to(device)\n",
        "g=Generador(dim_vector_ruido,image_dim).to(device)\n",
        "\n",
        "fixed_noise=torch.randn((batch_size,dim_vector_ruido)).to(device)\n",
        "\n",
        "lr_g=0.0001\n",
        "lr_d=lr_g*1\n",
        "d_optimizer=optim.Adam(d.parameters(),lr=lr_d)\n",
        "g_optimizer=optim.Adam(g.parameters(),lr=lr_g)\n",
        "\n",
        "criterio=nn.BCELoss()\n",
        "print(f\"learning rate discrimiador:{lr_d}\")\n",
        "print(f\"learning rate generador:{lr_g}\")\n",
        "\n",
        "GAN(d,g,dim_vector_ruido,data_loader,data_loader_Hq,num_epochs,batch_size,criterio,d_optimizer,g_optimizer,device)\n",
        "\n"
      ],
      "metadata": {
        "id": "xOaEJmpkzNvo",
        "outputId": "35d0be2a-1909-49d4-ac28-1e7ec685a3a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA está disponible.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-ad062042808a>\u001b[0m in \u001b[0;36m<cell line: 119>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0mdata_loader_Hq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGenerador\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim_vector_ruido\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m   1157\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m-> 1158\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 336.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 147.06 MiB is free. Process 6170 has 14.60 GiB memory in use. Of the allocated memory 14.07 GiB is allocated by PyTorch, and 394.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(g.state_dict(), 'modelo_gatos_g.pt')\n",
        "torch.save(d.state_dict(), 'modelo_gatos_d.pt')"
      ],
      "metadata": {
        "id": "nUbOcTLlt8m2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suponiendo que \"Generador\" es la clase de tu generador y \"ruta_modelo\" es la ruta del modelo guardado\n",
        "generador = Generador(100, 63*63*3)  # Ajusta input_size y output_size según tu implementación\n",
        "generador.load_state_dict(torch.load(\"modelo_gatos_g.pt\"))\n",
        "generador.to(device)\n",
        "generador.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    generador.eval()\n",
        "    noise = torch.randn(1, dim_vector_ruido).to(device)\n",
        "    generated_image = generador(noise).detach().cpu()\n",
        "\n",
        "# Convertir la imagen al rango [0, 1]\n",
        "generated_image = (generated_image + 1) / 2.0\n",
        "\n",
        "generated_image_np = generated_image.squeeze().permute(1, 2, 0).numpy()\n",
        "\n",
        "print(generated_image.shape)\n",
        "# Visualizar la imagen generada\n",
        "plt.imshow(generated_image_np)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k3ANH_wEuUBR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}