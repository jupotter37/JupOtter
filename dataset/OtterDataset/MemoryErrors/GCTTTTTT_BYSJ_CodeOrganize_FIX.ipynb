{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73e07e1a-6a84-4de7-a722-62ba5648d0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/default/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a90d3e7-cc83-4f7b-b04d-d034f922c299",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 设置随机种子\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# 加载数据\n",
    "file_path = '../../../datasets_FIX2/FIX2_deduplicated_mangoNews_Nums3000p_CategoryMerge_new_undersampled_Example.csv'\n",
    "# file_path = '../../../datasets_FIX2/FIX2_deduplicated_mangoNews_Nums3000p_CategoryMerge_new_undersampled.csv'\n",
    "\n",
    "data = pd.read_csv(file_path,low_memory=False,lineterminator=\"\\n\")\n",
    "\n",
    "texts = data['body'].tolist()\n",
    "labels = data['category1'].tolist()\n",
    "\n",
    "# 对标签进行编码\n",
    "unique_labels = list(set(labels))\n",
    "label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "labels = [label_to_id[label] for label in labels]\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "# # 加载BERT tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('../../bert-base-multilingual-cased')\n",
    "\n",
    "# 加载BERT tokenizer和模型\n",
    "model_name = '../../../bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09c9a647-9ef4-4a77-b4fe-33dcccab8996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将模型移动到GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert_model.to(device)\n",
    "\n",
    "# 定义数据集\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.texts[index])\n",
    "        label = self.labels[index]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "# 定义Self-Attention层\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        energy = self.projection(encoder_outputs)\n",
    "        weights = torch.softmax(energy.squeeze(-1), dim=1)\n",
    "        outputs = (encoder_outputs * weights.unsqueeze(-1)).sum(dim=1)\n",
    "        return outputs\n",
    "\n",
    "# 定义模型\n",
    "class NewsClassifier(nn.Module):\n",
    "    # hidden_size = 128\n",
    "    def __init__(self, num_classes, hidden_size, num_layers=2, bidirectional=True):\n",
    "        super(NewsClassifier, self).__init__()\n",
    "        # self.bert = BertModel.from_pretrained('../../bert-base-multilingual-cased')\n",
    "        self.bert = bert_model # FIX\n",
    "\n",
    "        # self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, \n",
    "        #                     bidirectional=bidirectional, batch_first=True)\n",
    "        self.lstm = nn.LSTM(input_size=bert_model.config.hidden_size, hidden_size=hidden_size, num_layers=num_layers, \n",
    "                            bidirectional=bidirectional, batch_first=True) # FIX\n",
    "        \n",
    "        self.attention = SelfAttention(hidden_size * (2 if bidirectional else 1))\n",
    "        self.fc = nn.Linear(hidden_size * (2 if bidirectional else 1), num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        lstm_outputs, _ = self.lstm(last_hidden_state)\n",
    "        attention_outputs = self.attention(lstm_outputs)\n",
    "        logits = self.fc(attention_outputs)\n",
    "        return logits\n",
    "\n",
    "# 定义训练函数\n",
    "def train_model(model, dataloader, optimizer, scheduler, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=f'Epoch {epoch + 1}', leave=False)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        # progress_bar.set_postfix({'Loss': avg_loss:.4f})\n",
    "        # progress_bar.set_postfix({'Loss': avg_loss:.4f})\n",
    "        # progress_bar.set_postfix({'Loss': \"{:.4f}\".format(avg_loss) })\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "        # progress_bar.set_postfix({'Loss': \"{:.4f}\".format(loss.item) }) # FIX\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "# 定义评估函数\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            batch_predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "            predictions.extend(batch_predictions.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return classification_report(true_labels, predictions, digits=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f308d97e-39b2-4214-83e3-9ef7088f0fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dba78c38-2e95-4e05-8cd6-1b9dea31db32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/default/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "                                                                   \r"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.69 GiB total capacity; 7.69 GiB already allocated; 36.94 MiB free; 7.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 49\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     val_report \u001b[38;5;241m=\u001b[39m evaluate_model(model, val_dataloader, device)\n\u001b[1;32m     52\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mfloat\u001b[39m(val_report\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# 提取验证集损失\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 94\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, optimizer, scheduler, device, epoch)\u001b[0m\n\u001b[1;32m     91\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask)\n\u001b[1;32m     92\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()(logits, labels)\n\u001b[0;32m---> 94\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     96\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/conda/envs/default/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/default/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 118.00 MiB (GPU 0; 23.69 GiB total capacity; 7.69 GiB already allocated; 36.94 MiB free; 7.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# 设置超参数\n",
    "# max_length = 256\n",
    "max_length = 512\n",
    "# origin:max_length = 512\n",
    "batch_size = 16\n",
    "epochs = 2\n",
    "learning_rate = 2e-5\n",
    "# hidden_size = 768\n",
    "hidden_size = 128\n",
    "# origin：hidden_size = 128\n",
    "\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "\n",
    "# 使用KFold进行交叉验证\n",
    "# kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=seed) # FIX\n",
    "\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "best_models = []\n",
    "best_reports = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kfold.split(texts, labels)):\n",
    "    print(f'Fold {fold + 1}')\n",
    "    print('-' * 30)\n",
    "\n",
    "    train_texts, val_texts = [texts[i] for i in train_index], [texts[i] for i in val_index]\n",
    "    train_labels, val_labels = [labels[i] for i in train_index], [labels[i] for i in val_index]\n",
    "\n",
    "    train_dataset = NewsDataset(train_texts, train_labels, tokenizer, max_length)\n",
    "    val_dataset = NewsDataset(val_texts, val_labels, tokenizer, max_length)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = NewsClassifier(num_classes, hidden_size, num_layers, bidirectional)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_model(model, train_dataloader, optimizer, scheduler, device, epoch)\n",
    "        val_report = evaluate_model(model, val_dataloader, device)\n",
    "\n",
    "        val_loss = 1 - float(val_report.split('\\n')[-3].split()[-2])  # 提取验证集损失\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}')\n",
    "        print(f'Train Loss: {train_loss:.4f}')\n",
    "        print('Validation Report:')\n",
    "        print(val_report)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # best_model = model.state_dict()\n",
    "            # best_report = val_report\n",
    "            torch.save(model.state_dict(), f'best_MultiBert_BiLSTM_SelfAttention_model_fold_{fold + 1}.pth')\n",
    "\n",
    "\n",
    "# 在每个fold结束后,评估最佳模型在验证集上的性能\n",
    "    best_model = NewsClassifier(num_classes, hidden_size, num_layers, bidirectional)\n",
    "\n",
    "    best_model.load_state_dict(torch.load(f'best_MultiBert_BiLSTM_SelfAttention_model_fold_{fold + 1}.pth'))\n",
    "    best_model.to(device)\n",
    "    val_report = evaluate_model(best_model, val_dataloader, device)\n",
    "    # all_reports.append(val_report)\n",
    "\n",
    "    print(f'Fold {fold + 1} Best Validation Report:')\n",
    "    print(val_report)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b7b094-5783-440d-a64a-916fbb8b6b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "==============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a18a1574-b7ea-4fc9-a877-a62539728639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/default/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/default/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "Train Loss: 2.0257\n",
      "Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5517    0.5517    0.5517        29\n",
      "           1     0.9412    0.7619    0.8421        21\n",
      "           2     1.0000    0.2500    0.4000        16\n",
      "           3     0.6667    0.5882    0.6250        17\n",
      "           4     0.8519    1.0000    0.9200        23\n",
      "           5     0.7333    0.6111    0.6667        18\n",
      "           6     0.4706    0.9600    0.6316        25\n",
      "           7     0.8421    0.6667    0.7442        24\n",
      "           8     0.6087    0.5185    0.5600        27\n",
      "\n",
      "    accuracy                         0.6700       200\n",
      "   macro avg     0.7407    0.6565    0.6601       200\n",
      "weighted avg     0.7215    0.6700    0.6632       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/default/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Best Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5517    0.5517    0.5517        29\n",
      "           1     0.9412    0.7619    0.8421        21\n",
      "           2     1.0000    0.2500    0.4000        16\n",
      "           3     0.6667    0.5882    0.6250        17\n",
      "           4     0.8519    1.0000    0.9200        23\n",
      "           5     0.7333    0.6111    0.6667        18\n",
      "           6     0.4706    0.9600    0.6316        25\n",
      "           7     0.8421    0.6667    0.7442        24\n",
      "           8     0.6087    0.5185    0.5600        27\n",
      "\n",
      "    accuracy                         0.6700       200\n",
      "   macro avg     0.7407    0.6565    0.6601       200\n",
      "weighted avg     0.7215    0.6700    0.6632       200\n",
      "\n",
      "\n",
      "Fold 2\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "Train Loss: 1.8558\n",
      "Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5333    0.4000    0.4571        20\n",
      "           1     0.7600    0.8636    0.8085        22\n",
      "           2     0.8000    0.3478    0.4848        23\n",
      "           3     0.6765    0.8214    0.7419        28\n",
      "           4     1.0000    0.9600    0.9796        25\n",
      "           5     0.9375    0.6522    0.7692        23\n",
      "           6     0.5938    0.8261    0.6909        23\n",
      "           7     0.6923    0.6429    0.6667        14\n",
      "           8     0.6129    0.8636    0.7170        22\n",
      "\n",
      "    accuracy                         0.7200       200\n",
      "   macro avg     0.7340    0.7086    0.7018       200\n",
      "weighted avg     0.7406    0.7200    0.7102       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/default/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Best Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5333    0.4000    0.4571        20\n",
      "           1     0.7600    0.8636    0.8085        22\n",
      "           2     0.8000    0.3478    0.4848        23\n",
      "           3     0.6765    0.8214    0.7419        28\n",
      "           4     1.0000    0.9600    0.9796        25\n",
      "           5     0.9375    0.6522    0.7692        23\n",
      "           6     0.5938    0.8261    0.6909        23\n",
      "           7     0.6923    0.6429    0.6667        14\n",
      "           8     0.6129    0.8636    0.7170        22\n",
      "\n",
      "    accuracy                         0.7200       200\n",
      "   macro avg     0.7340    0.7086    0.7018       200\n",
      "weighted avg     0.7406    0.7200    0.7102       200\n",
      "\n",
      "\n",
      "Fold 3\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "Train Loss: 1.8307\n",
      "Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7000    0.7000    0.7000        20\n",
      "           1     1.0000    0.9500    0.9744        20\n",
      "           2     0.9032    0.8750    0.8889        32\n",
      "           3     0.8421    0.7619    0.8000        21\n",
      "           4     1.0000    1.0000    1.0000        19\n",
      "           5     0.9375    0.6250    0.7500        24\n",
      "           6     0.8261    0.8636    0.8444        22\n",
      "           7     0.6071    0.8500    0.7083        20\n",
      "           8     0.8000    0.9091    0.8511        22\n",
      "\n",
      "    accuracy                         0.8350       200\n",
      "   macro avg     0.8462    0.8372    0.8352       200\n",
      "weighted avg     0.8500    0.8350    0.8360       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/default/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 Best Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7000    0.7000    0.7000        20\n",
      "           1     1.0000    0.9500    0.9744        20\n",
      "           2     0.9032    0.8750    0.8889        32\n",
      "           3     0.8421    0.7619    0.8000        21\n",
      "           4     1.0000    1.0000    1.0000        19\n",
      "           5     0.9375    0.6250    0.7500        24\n",
      "           6     0.8261    0.8636    0.8444        22\n",
      "           7     0.6071    0.8500    0.7083        20\n",
      "           8     0.8000    0.9091    0.8511        22\n",
      "\n",
      "    accuracy                         0.8350       200\n",
      "   macro avg     0.8462    0.8372    0.8352       200\n",
      "weighted avg     0.8500    0.8350    0.8360       200\n",
      "\n",
      "\n",
      "Fold 4\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "Train Loss: 1.7459\n",
      "Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8824    0.6000    0.7143        25\n",
      "           1     1.0000    1.0000    1.0000         7\n",
      "           2     0.7059    0.8000    0.7500        15\n",
      "           3     0.6250    0.9091    0.7407        22\n",
      "           4     0.9688    0.9688    0.9688        32\n",
      "           5     1.0000    0.4348    0.6061        23\n",
      "           6     0.8800    0.7857    0.8302        28\n",
      "           7     0.7083    0.8095    0.7556        21\n",
      "           8     0.7222    0.9630    0.8254        27\n",
      "\n",
      "    accuracy                         0.8000       200\n",
      "   macro avg     0.8325    0.8079    0.7990       200\n",
      "weighted avg     0.8321    0.8000    0.7937       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/default/lib/python3.8/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 Best Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8824    0.6000    0.7143        25\n",
      "           1     1.0000    1.0000    1.0000         7\n",
      "           2     0.7059    0.8000    0.7500        15\n",
      "           3     0.6250    0.9091    0.7407        22\n",
      "           4     0.9688    0.9688    0.9688        32\n",
      "           5     1.0000    0.4348    0.6061        23\n",
      "           6     0.8800    0.7857    0.8302        28\n",
      "           7     0.7083    0.8095    0.7556        21\n",
      "           8     0.7222    0.9630    0.8254        27\n",
      "\n",
      "    accuracy                         0.8000       200\n",
      "   macro avg     0.8325    0.8079    0.7990       200\n",
      "weighted avg     0.8321    0.8000    0.7937       200\n",
      "\n",
      "\n",
      "Fold 5\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "Train Loss: 1.7477\n",
      "Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7083    0.7083    0.7083        24\n",
      "           1     0.8929    1.0000    0.9434        25\n",
      "           2     0.8400    0.9545    0.8936        22\n",
      "           3     0.8621    0.9615    0.9091        26\n",
      "           4     1.0000    0.9545    0.9767        22\n",
      "           5     1.0000    0.7059    0.8276        17\n",
      "           6     0.9000    0.9000    0.9000        20\n",
      "           7     0.9333    0.6087    0.7368        23\n",
      "           8     0.7692    0.9524    0.8511        21\n",
      "\n",
      "    accuracy                         0.8650       200\n",
      "   macro avg     0.8784    0.8607    0.8607       200\n",
      "weighted avg     0.8742    0.8650    0.8613       200\n",
      "\n",
      "Fold 5 Best Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7083    0.7083    0.7083        24\n",
      "           1     0.8929    1.0000    0.9434        25\n",
      "           2     0.8400    0.9545    0.8936        22\n",
      "           3     0.8621    0.9615    0.9091        26\n",
      "           4     1.0000    0.9545    0.9767        22\n",
      "           5     1.0000    0.7059    0.8276        17\n",
      "           6     0.9000    0.9000    0.9000        20\n",
      "           7     0.9333    0.6087    0.7368        23\n",
      "           8     0.7692    0.9524    0.8511        21\n",
      "\n",
      "    accuracy                         0.8650       200\n",
      "   macro avg     0.8784    0.8607    0.8607       200\n",
      "weighted avg     0.8742    0.8650    0.8613       200\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# 设置随机种子\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# 加载数据\n",
    "file_path = '../../../datasets_FIX2/FIX2_deduplicated_mangoNews_Nums3000p_CategoryMerge_new_undersampled_Example.csv'\n",
    "# file_path = '../../../datasets_FIX2/FIX2_deduplicated_mangoNews_Nums3000p_CategoryMerge_new_undersampled.csv'\n",
    "\n",
    "data = pd.read_csv(file_path,low_memory=False,lineterminator=\"\\n\")\n",
    "\n",
    "texts = data['body'].tolist()\n",
    "labels = data['category1'].tolist()\n",
    "\n",
    "# 对标签进行编码\n",
    "unique_labels = list(set(labels))\n",
    "label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "labels = [label_to_id[label] for label in labels]\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "# # 加载BERT tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('../../bert-base-multilingual-cased')\n",
    "\n",
    "# 加载BERT tokenizer和模型\n",
    "model_name = '../../../bert-base-multilingual-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# 将模型移动到GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bert_model.to(device)\n",
    "\n",
    "# 定义数据集\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.texts[index])\n",
    "        label = self.labels[index]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "# 定义Self-Attention层\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        energy = self.projection(encoder_outputs)\n",
    "        weights = torch.softmax(energy.squeeze(-1), dim=1)\n",
    "        outputs = (encoder_outputs * weights.unsqueeze(-1)).sum(dim=1)\n",
    "        return outputs\n",
    "\n",
    "# 定义模型\n",
    "class NewsClassifier(nn.Module):\n",
    "    # hidden_size = 128\n",
    "    def __init__(self, num_classes, hidden_size, num_layers=2, bidirectional=True):\n",
    "        super(NewsClassifier, self).__init__()\n",
    "        # self.bert = BertModel.from_pretrained('../../bert-base-multilingual-cased')\n",
    "        self.bert = bert_model # FIX\n",
    "\n",
    "        # self.lstm = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, \n",
    "        #                     bidirectional=bidirectional, batch_first=True)\n",
    "        self.lstm = nn.LSTM(input_size=bert_model.config.hidden_size, hidden_size=hidden_size, num_layers=num_layers, \n",
    "                            bidirectional=bidirectional, batch_first=True) # FIX\n",
    "        \n",
    "        self.attention = SelfAttention(hidden_size * (2 if bidirectional else 1))\n",
    "        self.fc = nn.Linear(hidden_size * (2 if bidirectional else 1), num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        lstm_outputs, _ = self.lstm(last_hidden_state)\n",
    "        attention_outputs = self.attention(lstm_outputs)\n",
    "        logits = self.fc(attention_outputs)\n",
    "        return logits\n",
    "\n",
    "# 定义训练函数\n",
    "def train_model(model, dataloader, optimizer, scheduler, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=f'Epoch {epoch + 1}', leave=False)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        # progress_bar.set_postfix({'Loss': avg_loss:.4f})\n",
    "        # progress_bar.set_postfix({'Loss': avg_loss:.4f})\n",
    "        # progress_bar.set_postfix({'Loss': \"{:.4f}\".format(avg_loss) })\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "        # progress_bar.set_postfix({'Loss': \"{:.4f}\".format(loss.item) }) # FIX\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "# 定义评估函数\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            batch_predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "            predictions.extend(batch_predictions.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return classification_report(true_labels, predictions, digits=4)\n",
    "\n",
    "# 设置超参数\n",
    "# max_length = 256\n",
    "max_length = 512\n",
    "# origin:max_length = 512\n",
    "batch_size = 16\n",
    "epochs = 1\n",
    "learning_rate = 2e-5\n",
    "# hidden_size = 768\n",
    "hidden_size = 128\n",
    "# origin：hidden_size = 128\n",
    "\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "\n",
    "# 使用KFold进行交叉验证\n",
    "# kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=seed) # FIX\n",
    "\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "best_models = []\n",
    "best_reports = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kfold.split(texts, labels)):\n",
    "    print(f'Fold {fold + 1}')\n",
    "    print('-' * 30)\n",
    "\n",
    "    train_texts, val_texts = [texts[i] for i in train_index], [texts[i] for i in val_index]\n",
    "    train_labels, val_labels = [labels[i] for i in train_index], [labels[i] for i in val_index]\n",
    "\n",
    "    train_dataset = NewsDataset(train_texts, train_labels, tokenizer, max_length)\n",
    "    val_dataset = NewsDataset(val_texts, val_labels, tokenizer, max_length)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = NewsClassifier(num_classes, hidden_size, num_layers, bidirectional)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_model(model, train_dataloader, optimizer, scheduler, device, epoch)\n",
    "        val_report = evaluate_model(model, val_dataloader, device)\n",
    "\n",
    "        val_loss = 1 - float(val_report.split('\\n')[-3].split()[-2])  # 提取验证集损失\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}')\n",
    "        print(f'Train Loss: {train_loss:.4f}')\n",
    "        print('Validation Report:')\n",
    "        print(val_report)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # best_model = model.state_dict()\n",
    "            # best_report = val_report\n",
    "            torch.save(model.state_dict(), f'best_MultiBert_BiLSTM_SelfAttention_modelFIX_fold_{fold + 1}.pth')\n",
    "\n",
    "\n",
    "# 在每个fold结束后,评估最佳模型在验证集上的性能\n",
    "    best_model = NewsClassifier(num_classes, hidden_size, num_layers, bidirectional)\n",
    "\n",
    "    best_model.load_state_dict(torch.load(f'best_MultiBert_BiLSTM_SelfAttention_modelFIX_fold_{fold + 1}.pth'))\n",
    "    best_model.to(device)\n",
    "    val_report = evaluate_model(best_model, val_dataloader, device)\n",
    "    # all_reports.append(val_report)\n",
    "\n",
    "    print(f'Fold {fold + 1} Best Validation Report:')\n",
    "    print(val_report)\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eef889d5-c861-4844-ac92-b46e93a92887",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "649\n",
      "690\n",
      "712\n",
      "724\n",
      "729\n",
      "736\n",
      "741\n",
      "----\n",
      "0\n",
      "38777\n",
      "67002\n",
      "95214\n",
      "115576\n",
      "----\n",
      "126692\n",
      "146003\n",
      "162073\n"
     ]
    }
   ],
   "source": [
    "print(794-794)\n",
    "print(794-145)\n",
    "print(794-104)\n",
    "print(794-82)\n",
    "print(794-70)\n",
    "print(794-65)\n",
    "print(794-58)\n",
    "print(794-53)\n",
    "\n",
    "print(\"----\")\n",
    "print(1660838-1660838)\n",
    "print(1660838-1622061)\n",
    "print(1660838-1593836)\n",
    "print(1660838-1565624)\n",
    "print(1660838-1545262)\n",
    "print(\"----\")\n",
    "print(1660838-1534146)\n",
    "\n",
    "print(1660838-1514835)\n",
    "print(1660838-1498765)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70284f4c-a277-4051-8e5e-0f7d445a9a33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-default",
   "language": "python",
   "name": "conda-default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
