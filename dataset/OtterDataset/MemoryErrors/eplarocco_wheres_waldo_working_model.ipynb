{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Wheres Waldo?\n",
    "### Name: Eileanor LaRocco\n",
    "In this assignment, you will develop an object detection algorithm to locate Waldo in a set of images. You will develop a model to detect the bounding box around Waldo. Your final task is to submit your predictions on Kaggle for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import opendatasets as od\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 1\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = device = torch.device(\"cuda\") #mps/cuda\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_folder = \"2024-fall-ml-3-hw-4-wheres-waldo/train/train\" # Original Train Images\n",
    "test_folder = \"2024-fall-ml-3-hw-4-wheres-waldo/test/test\" # Original Test Images\n",
    "annotations_file = \"2024-fall-ml-3-hw-4-wheres-waldo/annotations.csv\" # Original Annotations File\n",
    "image_sz = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the dataset (Train and Test Loaders)\n",
    "class WaldoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transforms=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        #image = torchvision.transforms.ToTensor()(image)  # Convert to tensor\n",
    "        \n",
    "        box_data = self.img_labels.iloc[idx, 1:].values\n",
    "        boxes = [float(item) for item in box_data]\n",
    "        \n",
    "        boxes = torch.as_tensor([boxes], dtype=torch.float32)\n",
    "        labels = torch.ones((1,), dtype=torch.int64)\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((1,), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "# Set up the dataset and data loaders\n",
    "train_dataset = WaldoDataset(\n",
    "    annotations_file=\"2024-fall-ml-3-hw-4-wheres-waldo/train_annotations.csv\",\n",
    "    img_dir=\"2024-fall-ml-3-hw-4-wheres-waldo/train/chunks\",\n",
    "    transforms=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((image_sz, image_sz)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    ")\n",
    "\n",
    "val_dataset = WaldoDataset(\n",
    "    annotations_file=\"2024-fall-ml-3-hw-4-wheres-waldo/test_annotations.csv\",\n",
    "    img_dir=\"2024-fall-ml-3-hw-4-wheres-waldo/train/val\",\n",
    "    transforms=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((image_sz, image_sz)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: list(zip(*x))\n",
    ")\n",
    "\n",
    "val_data_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: list(zip(*x))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 626.00 MiB. GPU 0 has a total capacity of 44.34 GiB of which 213.75 MiB is free. Process 901941 has 12.85 GiB memory in use. Process 903143 has 14.31 GiB memory in use. Including non-PyTorch memory, this process has 16.84 GiB memory in use. Of the allocated memory 15.72 GiB is allocated by PyTorch, and 813.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 119\u001b[0m\n\u001b[1;32m    116\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Train and evaluate the model\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 92\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, device, optimizer, scheduler, checkpoint_path)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Validate the model\u001b[39;00m\n\u001b[1;32m     95\u001b[0m val_iou \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, device)\n",
      "Cell \u001b[0;32mIn[6], line 40\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, data_loader, optimizer, device)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected losses to be a dictionary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     43\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 626.00 MiB. GPU 0 has a total capacity of 44.34 GiB of which 213.75 MiB is free. Process 901941 has 12.85 GiB memory in use. Process 903143 has 14.31 GiB memory in use. Including non-PyTorch memory, this process has 16.84 GiB memory in use. Of the allocated memory 15.72 GiB is allocated by PyTorch, and 813.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "# Define IoU computation\n",
    "def calculate_iou(pred_boxes, target_boxes):\n",
    "    \"\"\"\n",
    "    Computes IoU between predicted and target boxes.\n",
    "    Args:\n",
    "        pred_boxes (Tensor): Predicted boxes, shape (N, 4).\n",
    "        target_boxes (Tensor): Target boxes, shape (M, 4).\n",
    "    Returns:\n",
    "        IoU scores: Tensor of shape (N, M).\n",
    "    \"\"\"\n",
    "    return ops.box_iou(pred_boxes, target_boxes)\n",
    "\n",
    "# Training function\n",
    "def train_one_epoch(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, targets in data_loader:\n",
    "        images = torch.stack([img.to(device) for img in images])\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        losses = model(images, targets)\n",
    "\n",
    "        # Ensure that losses is a dictionary and sum all loss components\n",
    "        if isinstance(losses, dict):\n",
    "            total_loss = sum(loss for loss in losses.values())\n",
    "        else:\n",
    "            raise ValueError(\"Expected losses to be a dictionary\")\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += total_loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(data_loader)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_iou = 0.0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(data_loader, desc=\"Validation\", leave=False):\n",
    "            # Move data to device\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Generate predictions\n",
    "            outputs = model(images)  # During evaluation, model outputs predictions\n",
    "            \n",
    "            # Compute IoU between predicted and target boxes\n",
    "            for output, target in zip(outputs, targets):\n",
    "                pred_boxes = output[\"boxes\"].detach()\n",
    "                target_boxes = target[\"boxes\"]\n",
    "\n",
    "                if len(pred_boxes) > 0 and len(target_boxes) > 0:\n",
    "                    iou = box_iou(pred_boxes, target_boxes).mean().item()\n",
    "                    total_iou += iou\n",
    "                else:\n",
    "                    total_iou += 0  # No predictions or no targets\n",
    "\n",
    "                num_samples += 1\n",
    "\n",
    "    # Calculate average IoU\n",
    "    avg_iou = total_iou / num_samples if num_samples > 0 else 0.0\n",
    "\n",
    "    return avg_iou\n",
    "\n",
    "\n",
    "# Main training loop\n",
    "def train_model(model, train_loader, val_loader, num_epochs, device, optimizer, scheduler, checkpoint_path):\n",
    "    best_iou = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Train the model\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "\n",
    "        # Validate the model\n",
    "        val_iou = evaluate(model, val_loader, device)\n",
    "\n",
    "        # Save the best model\n",
    "        if val_iou > best_iou:\n",
    "            best_iou = val_iou\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Best model saved with IoU: {best_iou:.4f}\")\n",
    "\n",
    "        # Update the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation IoU: {val_iou:.4f}\")\n",
    "\n",
    "    print(f\"Training complete. Best Validation IoU: {best_iou:.4f}\")\n",
    "\n",
    "\n",
    "# Set up optimizer, scheduler, and device\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, num_classes=2).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "checkpoint_path = \"best_model.pth\"\n",
    "num_epochs = 10\n",
    "\n",
    "# Train and evaluate the model\n",
    "train_model(\n",
    "    model,\n",
    "    train_data_loader,\n",
    "    val_data_loader,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    checkpoint_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "output_directory = \"2024-fall-ml-3-hw-4-wheres-waldo/outputs\"\n",
    "if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "# Function to predict bounding box on a new image\n",
    "# Function to predict bounding box on a new image\n",
    "# Function to predict bounding box on a new image\n",
    "def predict(model, image_path, transform=None):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")  # Open image and convert to RGB\n",
    "    orig_width, orig_height = image.size  # Get original image dimensions\n",
    "    if transform:\n",
    "        image = transform(image)  # Apply transformations\n",
    "    image = image.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Ensure image is a Tensor and move it to the correct device\n",
    "    image = image.to(device)\n",
    "\n",
    "    # Predict the bounding box\n",
    "    with torch.no_grad():\n",
    "        # Get the model's output (a list of dictionaries in this case)\n",
    "        outputs = model(image)\n",
    "        if isinstance(outputs, list):  # Check if the output is a list\n",
    "            outputs = outputs[0]  # Get the first dictionary in the list\n",
    "        \n",
    "        # Check if any bounding boxes were predicted\n",
    "        if len(outputs['boxes']) == 0:\n",
    "            print(f\"No objects detected in {image_path}\")\n",
    "            return None  # No detection, return None or handle as needed\n",
    "        \n",
    "        # Extract the predicted bounding box (assuming the first detected object)\n",
    "        predicted_bbox = outputs['boxes'][0].cpu().numpy()  # Assuming 'boxes' contains the bounding boxes\n",
    "\n",
    "    # Scale bbox back to original image dimensions\n",
    "    predicted_bbox[0] *= orig_width / image_sz  # x_min\n",
    "    predicted_bbox[1] *= orig_height / image_sz  # y_min\n",
    "    predicted_bbox[2] *= orig_width / image_sz  # x_max\n",
    "    predicted_bbox[3] *= orig_height / image_sz  # y_max\n",
    "\n",
    "    return predicted_bbox\n",
    "\n",
    "\n",
    "# Define image transformations (resize, normalization)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_sz, image_sz)),  # Resize the image to 512x512\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "#For each image in test folder, predict, draw bounding box, save image, and save into csv file for submission\n",
    "test_images = [img for img in os.listdir(test_folder) if img.endswith(\".jpg\")]\n",
    "predictions = []\n",
    "\n",
    "for name in test_images:\n",
    "    image_path = os.path.join(test_folder, name)  # Replace with the path to your test image\n",
    "    predicted_bbox = predict(model, image_path, transform)\n",
    "    \n",
    "    if predicted_bbox is None:\n",
    "        continue  # Skip the image if no bounding box is detected\n",
    "\n",
    "    # Print the predicted bounding box (x_min, y_min, x_max, y_max)\n",
    "    print(\"Predicted Bounding Box:\", predicted_bbox)\n",
    "\n",
    "    # Plot the image and the predicted bounding box\n",
    "    image = Image.open(image_path)\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    # Add the bounding box only if a prediction was made\n",
    "    plt.gca().add_patch(plt.Rectangle(\n",
    "        (predicted_bbox[0], predicted_bbox[1]),  # (x_min, y_min)\n",
    "        predicted_bbox[2] - predicted_bbox[0],  # Width (x_max - x_min)\n",
    "        predicted_bbox[3] - predicted_bbox[1],  # Height (y_max - y_min)\n",
    "        linewidth=2, edgecolor='r', facecolor='none'\n",
    "    ))\n",
    "    \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.savefig(os.path.join(output_directory, name), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Extract bounding box coordinates\n",
    "    x_min, y_min, x_max, y_max = predicted_bbox\n",
    "    predictions.append([name, x_min, y_min, x_max, y_max])\n",
    "\n",
    "# Save predictions to CSV\n",
    "df = pd.DataFrame(predictions, columns=[\"filename\", \"xmin\", \"ymin\", \"xmax\", \"ymax\"])\n",
    "df.to_csv(os.path.join(output_directory, 'predictions.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
