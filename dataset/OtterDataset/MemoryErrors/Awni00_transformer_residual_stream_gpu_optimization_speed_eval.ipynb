{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchinfo\n",
    "\n",
    "import sys; sys.path.append('..')\n",
    "from language_models import TransformerLM, configure_optimizers\n",
    "from dual_attention_transformer import DualAttnTransformerLM, configure_optimizers\n",
    "from dual_attention_transformer_old import DualAttnTransformerLM as DualAttnTransformerLM_old\n",
    "import time\n",
    "\n",
    "import os\n",
    "os.environ[\"TORCH_LOGS\"] = \"+dynamo\"\n",
    "os.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\"\n",
    "\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_model = False\n",
    "fused_optim = False\n",
    "use_bfloat16 = False\n",
    "use_tf32_matmul = False\n",
    "# use_flash_attention = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n",
      "Number of available GPUs: 1\n",
      "GPU Types:\n",
      "GPU 0: NVIDIA H100 80GB HBM3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "if cuda_available:\n",
    "    # Get the number of available GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "\n",
    "    # Get the type of each GPU\n",
    "    gpu_types = [torch.cuda.get_device_name(i) for i in range(num_gpus)]\n",
    "\n",
    "    print(\"CUDA is available\")\n",
    "    print(f\"Number of available GPUs: {num_gpus}\")\n",
    "    print(\"GPU Types:\")\n",
    "    for i, gpu_type in enumerate(gpu_types):\n",
    "        print(f\"GPU {i}: {gpu_type}\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 338024 tokens\n",
      "1 epoch = 165 batches\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "data_dir = '../data/shakespeare.txt'\n",
    "\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # at init load tokens from disk and store them in memory\n",
    "        with open(data_dir, 'r') as f:\n",
    "            text = f.read()\n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        print(f\"loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
    "\n",
    "        # state\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B * T\n",
    "        # if loading the next batch would be out of bounds, reset\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y\n",
    "\n",
    "# train_loader = DataLoaderLite(B=16, T=1024)\n",
    "train_loader = DataLoaderLite(B=8, T=256)\n",
    "\n",
    "x, y = train_loader.next_batch()\n",
    "x, y = x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Count Scaling & Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "TransformerLM                                 [8, 256, 50304]           --\n",
       "├─ModuleDict: 1-1                             --                        --\n",
       "│    └─Embedding: 2-1                         [8, 256, 768]             38,633,472\n",
       "│    └─Embedding: 2-2                         [256, 768]                786,432\n",
       "│    └─ModuleList: 2-3                        --                        --\n",
       "│    │    └─EncoderBlock: 3-1                 [8, 256, 768]             5,999,616\n",
       "│    │    └─EncoderBlock: 3-2                 [8, 256, 768]             5,999,616\n",
       "│    │    └─EncoderBlock: 3-3                 [8, 256, 768]             5,999,616\n",
       "│    │    └─EncoderBlock: 3-4                 [8, 256, 768]             5,999,616\n",
       "│    │    └─EncoderBlock: 3-5                 [8, 256, 768]             5,999,616\n",
       "│    │    └─EncoderBlock: 3-6                 [8, 256, 768]             5,999,616\n",
       "│    │    └─EncoderBlock: 3-7                 [8, 256, 768]             5,999,616\n",
       "│    │    └─EncoderBlock: 3-8                 [8, 256, 768]             5,999,616\n",
       "│    │    └─EncoderBlock: 3-9                 [8, 256, 768]             5,999,616\n",
       "│    │    └─EncoderBlock: 3-10                [8, 256, 768]             5,999,616\n",
       "│    │    └─EncoderBlock: 3-11                [8, 256, 768]             5,999,616\n",
       "│    │    └─EncoderBlock: 3-12                [8, 256, 768]             5,999,616\n",
       "│    └─LayerNorm: 2-4                         [8, 256, 768]             1,536\n",
       "│    └─Linear: 2-5                            [8, 256, 50304]           38,633,472\n",
       "===============================================================================================\n",
       "Total params: 150,050,304\n",
       "Trainable params: 150,050,304\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.40\n",
       "===============================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 2235.04\n",
       "Params size (MB): 600.20\n",
       "Estimated Total Size (MB): 2835.27\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 50304 # 50257\n",
    "def create_model():\n",
    "    model_config = dict(vocab_size=vocab_size, d_model=768, n_layers=12, n_heads=12, dff=None, activation='relu',\n",
    "        dropout_rate=0., norm_first=True, norm_type='layernorm', max_block_size=1024, bias=False, pos_enc_type='pos_emb',\n",
    "        block_kwargs=dict(attn_kwargs=dict(n_kv_heads=1))) # MQA\n",
    "    model = TransformerLM(**model_config)\n",
    "    return model\n",
    "model = create_model()\n",
    "model = model.to(device)\n",
    "torchinfo.summary(model, input_data=(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "TransformerLM                                 [8, 256, 50304]           --\n",
       "├─ModuleDict: 1-1                             --                        --\n",
       "│    └─Embedding: 2-1                         [8, 256, 768]             38,633,472\n",
       "│    └─Embedding: 2-2                         [256, 768]                786,432\n",
       "│    └─ModuleList: 2-3                        --                        --\n",
       "│    │    └─EncoderBlock: 3-1                 [8, 256, 768]             7,080,960\n",
       "│    │    └─EncoderBlock: 3-2                 [8, 256, 768]             7,080,960\n",
       "│    │    └─EncoderBlock: 3-3                 [8, 256, 768]             7,080,960\n",
       "│    │    └─EncoderBlock: 3-4                 [8, 256, 768]             7,080,960\n",
       "│    │    └─EncoderBlock: 3-5                 [8, 256, 768]             7,080,960\n",
       "│    │    └─EncoderBlock: 3-6                 [8, 256, 768]             7,080,960\n",
       "│    │    └─EncoderBlock: 3-7                 [8, 256, 768]             7,080,960\n",
       "│    │    └─EncoderBlock: 3-8                 [8, 256, 768]             7,080,960\n",
       "│    │    └─EncoderBlock: 3-9                 [8, 256, 768]             7,080,960\n",
       "│    │    └─EncoderBlock: 3-10                [8, 256, 768]             7,080,960\n",
       "│    │    └─EncoderBlock: 3-11                [8, 256, 768]             7,080,960\n",
       "│    │    └─EncoderBlock: 3-12                [8, 256, 768]             7,080,960\n",
       "│    └─LayerNorm: 2-4                         [8, 256, 768]             1,536\n",
       "│    └─Linear: 2-5                            [8, 256, 50304]           38,633,472\n",
       "===============================================================================================\n",
       "Total params: 163,026,432\n",
       "Trainable params: 163,026,432\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.50\n",
       "===============================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 2511.86\n",
       "Params size (MB): 652.11\n",
       "Estimated Total Size (MB): 3164.00\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 50304 # 50257\n",
    "def create_model():\n",
    "    model_config = dict(vocab_size=vocab_size, d_model=768, n_layers=12, n_heads=12, dff=None, activation='relu',\n",
    "        dropout_rate=0., norm_first=True, norm_type='layernorm', max_block_size=1024, bias=False, pos_enc_type='pos_emb')\n",
    "    model = TransformerLM(**model_config)\n",
    "    return model\n",
    "model = create_model()\n",
    "model = model.to(device)\n",
    "torchinfo.summary(model, input_data=(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "TransformerLM                                 [8, 256, 50304]           --\n",
       "├─ModuleDict: 1-1                             --                        --\n",
       "│    └─Embedding: 2-1                         [8, 256, 768]             38,633,472\n",
       "│    └─Embedding: 2-2                         [256, 768]                786,432\n",
       "│    └─ModuleList: 2-3                        --                        --\n",
       "│    │    └─EncoderBlock: 3-1                 [8, 256, 768]             7,080,960\n",
       "│    │    └─EncoderBlock: 3-2                 [8, 256, 768]             7,080,960\n",
       "│    │    └─EncoderBlock: 3-3                 [8, 256, 768]             7,080,960\n",
       "│    │    └─EncoderBlock: 3-4                 [8, 256, 768]             7,080,960\n",
       "│    │    └─EncoderBlock: 3-5                 [8, 256, 768]             7,080,960\n",
       "│    │    └─EncoderBlock: 3-6                 [8, 256, 768]             7,080,960\n",
       "│    │    └─EncoderBlock: 3-7                 [8, 256, 768]             7,080,960\n",
       "│    │    └─EncoderBlock: 3-8                 [8, 256, 768]             7,080,960\n",
       "│    │    └─EncoderBlock: 3-9                 [8, 256, 768]             7,080,960\n",
       "│    │    └─EncoderBlock: 3-10                [8, 256, 768]             7,080,960\n",
       "│    │    └─EncoderBlock: 3-11                [8, 256, 768]             7,080,960\n",
       "│    │    └─EncoderBlock: 3-12                [8, 256, 768]             7,080,960\n",
       "│    └─LayerNorm: 2-4                         [8, 256, 768]             1,536\n",
       "│    └─Linear: 2-5                            [8, 256, 50304]           38,633,472\n",
       "===============================================================================================\n",
       "Total params: 163,026,432\n",
       "Trainable params: 163,026,432\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.50\n",
       "===============================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 2713.19\n",
       "Params size (MB): 652.11\n",
       "Estimated Total Size (MB): 3365.33\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 50304 # 50257\n",
    "def create_transformer_model(use_flash_attention=False):\n",
    "    model_config = dict(vocab_size=vocab_size, d_model=768, n_layers=12, n_heads=12, dff=None, activation='swiglu', # swiglu activation\n",
    "        dropout_rate=0., norm_first=True, norm_type='layernorm', max_block_size=1024, bias=False, pos_enc_type='pos_emb')\n",
    "    model = TransformerLM(**model_config, use_flash_attention=use_flash_attention)\n",
    "    return model\n",
    "model = create_transformer_model()\n",
    "model = model.to(device)\n",
    "torchinfo.summary(model, input_data=(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "DualAttnTransformerLM                                   [8, 256, 50304]           --\n",
       "├─ModuleDict: 1-1                                       --                        --\n",
       "│    └─Embedding: 2-1                                   [8, 256, 768]             38,633,472\n",
       "│    └─Embedding: 2-2                                   [256, 768]                786,432\n",
       "│    └─SymbolicAttention: 2-3                           [8, 256, 768]             786,432\n",
       "│    │    └─Linear: 3-1                                 [8, 256, 768]             590,592\n",
       "│    └─ModuleList: 2-26                                 --                        (recursive)\n",
       "│    │    └─DualAttnEncoderBlock: 3-2                   [8, 256, 768]             6,891,520\n",
       "│    └─SymbolicAttention: 2-5                           [8, 256, 768]             (recursive)\n",
       "│    │    └─Linear: 3-3                                 [8, 256, 768]             (recursive)\n",
       "│    └─ModuleList: 2-26                                 --                        (recursive)\n",
       "│    │    └─DualAttnEncoderBlock: 3-4                   [8, 256, 768]             6,891,520\n",
       "│    └─SymbolicAttention: 2-7                           [8, 256, 768]             (recursive)\n",
       "│    │    └─Linear: 3-5                                 [8, 256, 768]             (recursive)\n",
       "│    └─ModuleList: 2-26                                 --                        (recursive)\n",
       "│    │    └─DualAttnEncoderBlock: 3-6                   [8, 256, 768]             6,891,520\n",
       "│    └─SymbolicAttention: 2-9                           [8, 256, 768]             (recursive)\n",
       "│    │    └─Linear: 3-7                                 [8, 256, 768]             (recursive)\n",
       "│    └─ModuleList: 2-26                                 --                        (recursive)\n",
       "│    │    └─DualAttnEncoderBlock: 3-8                   [8, 256, 768]             6,891,520\n",
       "│    └─SymbolicAttention: 2-11                          [8, 256, 768]             (recursive)\n",
       "│    │    └─Linear: 3-9                                 [8, 256, 768]             (recursive)\n",
       "│    └─ModuleList: 2-26                                 --                        (recursive)\n",
       "│    │    └─DualAttnEncoderBlock: 3-10                  [8, 256, 768]             6,891,520\n",
       "│    └─SymbolicAttention: 2-13                          [8, 256, 768]             (recursive)\n",
       "│    │    └─Linear: 3-11                                [8, 256, 768]             (recursive)\n",
       "│    └─ModuleList: 2-26                                 --                        (recursive)\n",
       "│    │    └─DualAttnEncoderBlock: 3-12                  [8, 256, 768]             6,891,520\n",
       "│    └─SymbolicAttention: 2-15                          [8, 256, 768]             (recursive)\n",
       "│    │    └─Linear: 3-13                                [8, 256, 768]             (recursive)\n",
       "│    └─ModuleList: 2-26                                 --                        (recursive)\n",
       "│    │    └─DualAttnEncoderBlock: 3-14                  [8, 256, 768]             6,891,520\n",
       "│    └─SymbolicAttention: 2-17                          [8, 256, 768]             (recursive)\n",
       "│    │    └─Linear: 3-15                                [8, 256, 768]             (recursive)\n",
       "│    └─ModuleList: 2-26                                 --                        (recursive)\n",
       "│    │    └─DualAttnEncoderBlock: 3-16                  [8, 256, 768]             6,891,520\n",
       "│    └─SymbolicAttention: 2-19                          [8, 256, 768]             (recursive)\n",
       "│    │    └─Linear: 3-17                                [8, 256, 768]             (recursive)\n",
       "│    └─ModuleList: 2-26                                 --                        (recursive)\n",
       "│    │    └─DualAttnEncoderBlock: 3-18                  [8, 256, 768]             6,891,520\n",
       "│    └─SymbolicAttention: 2-21                          [8, 256, 768]             (recursive)\n",
       "│    │    └─Linear: 3-19                                [8, 256, 768]             (recursive)\n",
       "│    └─ModuleList: 2-26                                 --                        (recursive)\n",
       "│    │    └─DualAttnEncoderBlock: 3-20                  [8, 256, 768]             6,891,520\n",
       "│    └─SymbolicAttention: 2-23                          [8, 256, 768]             (recursive)\n",
       "│    │    └─Linear: 3-21                                [8, 256, 768]             (recursive)\n",
       "│    └─ModuleList: 2-26                                 --                        (recursive)\n",
       "│    │    └─DualAttnEncoderBlock: 3-22                  [8, 256, 768]             6,891,520\n",
       "│    └─SymbolicAttention: 2-25                          [8, 256, 768]             (recursive)\n",
       "│    │    └─Linear: 3-23                                [8, 256, 768]             (recursive)\n",
       "│    └─ModuleList: 2-26                                 --                        (recursive)\n",
       "│    │    └─DualAttnEncoderBlock: 3-24                  [8, 256, 768]             6,891,520\n",
       "│    └─LayerNorm: 2-27                                  [8, 256, 768]             1,536\n",
       "│    └─Linear: 2-28                                     [8, 256, 50304]           38,633,472\n",
       "=========================================================================================================\n",
       "Total params: 162,130,176\n",
       "Trainable params: 162,130,176\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.54\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 2889.35\n",
       "Params size (MB): 645.26\n",
       "Estimated Total Size (MB): 3534.65\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 50304 # 50257\n",
    "def create_dat_model(use_flash_attention=False):\n",
    "    model_config = dict(vocab_size=vocab_size, d_model=768, n_layers=12, n_heads_sa=6, n_heads_ra=6, dff=None, activation='swiglu',\n",
    "        symbol_retrieval='symbolic_attention', symbol_retrieval_kwargs=dict(d_model=768, n_heads=4, n_symbols=512),\n",
    "        dropout_rate=0., norm_first=True, norm_type='layernorm', max_block_size=1024, bias=False, pos_enc_type='pos_emb',\n",
    "        sa_kwargs=dict(n_kv_heads=None), ra_kwargs=dict(n_kv_heads=1))\n",
    "    model = DualAttnTransformerLM(**model_config) # DAT\n",
    "    return model\n",
    "model = create_dat_model()\n",
    "model = model.to(device)\n",
    "torchinfo.summary(model, input_data=(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "DualAttnTransformerLM                                   [8, 256, 50304]           --\n",
       "├─ModuleDict: 1-1                                       --                        --\n",
       "│    └─Embedding: 2-1                                   [8, 256, 768]             38,633,472\n",
       "│    └─Embedding: 2-2                                   [256, 768]                786,432\n",
       "│    └─SymbolicAttention: 2-3                           [8, 256, 768]             786,432\n",
       "│    │    └─Linear: 3-1                                 [8, 256, 768]             590,592\n",
       "│    └─ModuleList: 2-26                                 --                        (recursive)\n",
       "│    │    └─DualAttnEncoderBlock: 3-2                   [8, 256, 768]             6,891,520\n",
       "│    └─SymbolicAttention: 2-5                           [8, 256, 768]             (recursive)\n",
       "│    │    └─Linear: 3-3                                 [8, 256, 768]             (recursive)\n",
       "│    └─ModuleList: 2-26                                 --                        (recursive)\n",
       "│    │    └─DualAttnEncoderBlock: 3-4                   [8, 256, 768]             6,891,520\n",
       "│    └─SymbolicAttention: 2-7                           [8, 256, 768]             (recursive)\n",
       "│    │    └─Linear: 3-5                                 [8, 256, 768]             (recursive)\n",
       "│    └─ModuleList: 2-26                                 --                        (recursive)\n",
       "│    │    └─DualAttnEncoderBlock: 3-6                   [8, 256, 768]             6,891,520\n",
       "│    └─SymbolicAttention: 2-9                           [8, 256, 768]             (recursive)\n",
       "│    │    └─Linear: 3-7                                 [8, 256, 768]             (recursive)\n",
       "│    └─ModuleList: 2-26                                 --                        (recursive)\n",
       "│    │    └─DualAttnEncoderBlock: 3-8                   [8, 256, 768]             6,891,520\n",
       "│    └─SymbolicAttention: 2-11                          [8, 256, 768]             (recursive)\n",
       "│    │    └─Linear: 3-9                                 [8, 256, 768]             (recursive)\n",
       "│    └─ModuleList: 2-26                                 --                        (recursive)\n",
       "│    │    └─DualAttnEncoderBlock: 3-10                  [8, 256, 768]             6,891,520\n",
       "│    └─SymbolicAttention: 2-13                          [8, 256, 768]             (recursive)\n",
       "│    │    └─Linear: 3-11                                [8, 256, 768]             (recursive)\n",
       "│    └─ModuleList: 2-26                                 --                        (recursive)\n",
       "│    │    └─DualAttnEncoderBlock: 3-12                  [8, 256, 768]             6,891,520\n",
       "│    └─SymbolicAttention: 2-15                          [8, 256, 768]             (recursive)\n",
       "│    │    └─Linear: 3-13                                [8, 256, 768]             (recursive)\n",
       "│    └─ModuleList: 2-26                                 --                        (recursive)\n",
       "│    │    └─DualAttnEncoderBlock: 3-14                  [8, 256, 768]             6,891,520\n",
       "│    └─SymbolicAttention: 2-17                          [8, 256, 768]             (recursive)\n",
       "│    │    └─Linear: 3-15                                [8, 256, 768]             (recursive)\n",
       "│    └─ModuleList: 2-26                                 --                        (recursive)\n",
       "│    │    └─DualAttnEncoderBlock: 3-16                  [8, 256, 768]             6,891,520\n",
       "│    └─SymbolicAttention: 2-19                          [8, 256, 768]             (recursive)\n",
       "│    │    └─Linear: 3-17                                [8, 256, 768]             (recursive)\n",
       "│    └─ModuleList: 2-26                                 --                        (recursive)\n",
       "│    │    └─DualAttnEncoderBlock: 3-18                  [8, 256, 768]             6,891,520\n",
       "│    └─SymbolicAttention: 2-21                          [8, 256, 768]             (recursive)\n",
       "│    │    └─Linear: 3-19                                [8, 256, 768]             (recursive)\n",
       "│    └─ModuleList: 2-26                                 --                        (recursive)\n",
       "│    │    └─DualAttnEncoderBlock: 3-20                  [8, 256, 768]             6,891,520\n",
       "│    └─SymbolicAttention: 2-23                          [8, 256, 768]             (recursive)\n",
       "│    │    └─Linear: 3-21                                [8, 256, 768]             (recursive)\n",
       "│    └─ModuleList: 2-26                                 --                        (recursive)\n",
       "│    │    └─DualAttnEncoderBlock: 3-22                  [8, 256, 768]             6,891,520\n",
       "│    └─SymbolicAttention: 2-25                          [8, 256, 768]             (recursive)\n",
       "│    │    └─Linear: 3-23                                [8, 256, 768]             (recursive)\n",
       "│    └─ModuleList: 2-26                                 --                        (recursive)\n",
       "│    │    └─DualAttnEncoderBlock: 3-24                  [8, 256, 768]             6,891,520\n",
       "│    └─LayerNorm: 2-27                                  [8, 256, 768]             1,536\n",
       "│    └─Linear: 2-28                                     [8, 256, 50304]           38,633,472\n",
       "=========================================================================================================\n",
       "Total params: 162,130,176\n",
       "Trainable params: 162,130,176\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.54\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.03\n",
       "Forward/backward pass size (MB): 22216.70\n",
       "Params size (MB): 645.37\n",
       "Estimated Total Size (MB): 22862.11\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 50304 # 50257\n",
    "def create_dat_old_model(use_flash_attention=False):\n",
    "    model_config = dict(vocab_size=vocab_size, d_model=768, n_layers=12, n_heads_sa=6, n_heads_ra=6, dff=None, activation='swiglu',\n",
    "        symbol_retrieval='symbolic_attention', symbol_retrieval_kwargs=dict(d_model=768, n_heads=4, n_symbols=512),\n",
    "        dropout_rate=0., norm_first=True, norm_type='layernorm', max_block_size=1024, bias=False, pos_enc_type='pos_emb',\n",
    "        sa_kwargs=dict(n_kv_heads=None), ra_kwargs=dict(n_kv_heads=1))\n",
    "    model = DualAttnTransformerLM_old(**model_config) # DAT\n",
    "    return model\n",
    "model = create_dat_old_model()\n",
    "model = model.to(device)\n",
    "torchinfo.summary(model, input_data=(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: before new implementation of relational attention, a 162M DAT has \"Forward/backward pass size (MB): 22216.70\". This is compared to a 163M Transformer with only Forward/backward pass size (MB): 2713.19\". This is almost a 10X difference. Here, $T = 256$ only. It would be much larger if $T$ was larger (quadratically).\n",
    "\n",
    "By comparison, with the new implementation, the 162M DAT has a \"Forward/backward pass size\" of only \"Forward/backward pass size (MB): 2889.35\". Only slightly larger than a Transformer!\n",
    "\n",
    "Total # of Mult-Adds (i.e., # of operations) is roughly the same in comparably sized Transformer (163M; Total mult-adds (Units.GIGABYTES): 1.50) vs DAT (162M; Total mult-adds (Units.GIGABYTES): 1.54). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Training Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_train_speed(model, n_steps=50, fused_optim=fused_optim, use_bfloat16=use_bfloat16, verbose=True):\n",
    "    optimizer = optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, fused=fused_optim)\n",
    "\n",
    "    dts = []\n",
    "    toks_per_sec = []\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        t0 = time.time()\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if use_bfloat16:\n",
    "            with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
    "                logits, loss = model(x, y)\n",
    "        else:\n",
    "            logits, loss = model(x, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.synchronize() # wait for the GPU to finish work\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0 # time difference in seconds\n",
    "        tokens_processed = train_loader.B * train_loader.T\n",
    "        tokens_per_sec = tokens_processed / dt\n",
    "        dts.append(dt)\n",
    "        toks_per_sec.append(tokens_per_sec)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"step {i:4d} | loss: {loss.item():.6f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n",
    "\n",
    "    return dts, toks_per_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_setup_speed(\n",
    "        create_model, use_flash_attention=False, use_tf32_matmul=False, compile_model=False, fused_optim=False, use_bfloat16=False, verbose=False, n_steps=26):\n",
    "    print('='*100)\n",
    "    print(f'use_flash_attention: {use_flash_attention}, use_tf32_matmul: {use_tf32_matmul}, compile_model: {compile_model}, fused_optim: {fused_optim}, use_bfloat16: {use_bfloat16}')\n",
    "    print('='*100)\n",
    "\n",
    "    model = create_model(use_flash_attention=use_flash_attention) # use_flash_attention is ignored for DAT\n",
    "    model = model.to(device)\n",
    "\n",
    "    if use_tf32_matmul:\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "\n",
    "    if compile_model:\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    dts, tokens_per_sec = eval_train_speed(model, n_steps=n_steps, fused_optim=fused_optim, use_bfloat16=use_bfloat16, verbose=verbose)\n",
    "    print('*'*100)\n",
    "    mean_dt = sum(dts[1:])/len(dts[1:])*1000 # exclude first step cuz of compilation overhead\n",
    "    mean_toks_per_sec = sum(tokens_per_sec[1:])/len(tokens_per_sec[1:])\n",
    "    print(f\"mean dt: {sum(dts[1:])/len(dts[1:])*1000:.2f}ms | mean tok/sec: {sum(tokens_per_sec)/len(tokens_per_sec):.0f}\")\n",
    "    print('*'*100)\n",
    "    print('='*100)\n",
    "    return mean_dt, mean_toks_per_sec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "setups = [\n",
    "    dict(use_flash_attention=False, use_tf32_matmul=False, compile_model=False, fused_optim=False, use_bfloat16=False),\n",
    "    dict(use_flash_attention=False, use_tf32_matmul=True, compile_model=False, fused_optim=False, use_bfloat16=False),\n",
    "    dict(use_flash_attention=False, use_tf32_matmul=True, compile_model=False, fused_optim=False, use_bfloat16=True),\n",
    "    dict(use_flash_attention=False, use_tf32_matmul=True, compile_model=True, fused_optim=False, use_bfloat16=True),\n",
    "    dict(use_flash_attention=False, use_tf32_matmul=True, compile_model=True, fused_optim=True, use_bfloat16=True),\n",
    "    dict(use_flash_attention=True, use_tf32_matmul=True, compile_model=True, fused_optim=True, use_bfloat16=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B = 4, T = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 338024 tokens\n",
      "1 epoch = 330 batches\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoaderLite(B=4, T=256)\n",
    "# train_loader = DataLoaderLite(B=8, T=256)\n",
    "# train_loader = DataLoaderLite(B=16, T=1024)\n",
    "# train_loader = DataLoaderLite(B=2, T=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "use_flash_attention: False, use_tf32_matmul: False, compile_model: False, fused_optim: False, use_bfloat16: False\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 30.58ms | mean tok/sec: 32381\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "use_flash_attention: False, use_tf32_matmul: True, compile_model: False, fused_optim: False, use_bfloat16: False\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 17.23ms | mean tok/sec: 57394\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "use_flash_attention: False, use_tf32_matmul: True, compile_model: False, fused_optim: False, use_bfloat16: True\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 17.28ms | mean tok/sec: 57217\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "use_flash_attention: False, use_tf32_matmul: True, compile_model: True, fused_optim: False, use_bfloat16: True\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 15.44ms | mean tok/sec: 63814\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "use_flash_attention: False, use_tf32_matmul: True, compile_model: True, fused_optim: True, use_bfloat16: True\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 12.39ms | mean tok/sec: 79574\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "use_flash_attention: True, use_tf32_matmul: True, compile_model: True, fused_optim: True, use_bfloat16: True\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 11.75ms | mean tok/sec: 83847\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# results for Transformer-LM\n",
    "for setup in setups:\n",
    "    mean_dt, mean_toks_per_sec = eval_setup_speed(create_model=create_transformer_model, **setup, verbose=False)\n",
    "    print()\n",
    "    results.append((setup, mean_dt, mean_toks_per_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "use_flash_attention: False, use_tf32_matmul: False, compile_model: False, fused_optim: False, use_bfloat16: False\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "mean dt: 52.56ms | mean tok/sec: 18957\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "use_flash_attention: False, use_tf32_matmul: True, compile_model: False, fused_optim: False, use_bfloat16: False\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 52.51ms | mean tok/sec: 19483\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "use_flash_attention: False, use_tf32_matmul: True, compile_model: False, fused_optim: False, use_bfloat16: True\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 41.88ms | mean tok/sec: 23708\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "use_flash_attention: False, use_tf32_matmul: True, compile_model: True, fused_optim: False, use_bfloat16: True\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 33.69ms | mean tok/sec: 29230\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "use_flash_attention: False, use_tf32_matmul: True, compile_model: True, fused_optim: True, use_bfloat16: True\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 28.58ms | mean tok/sec: 34460\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "use_flash_attention: True, use_tf32_matmul: True, compile_model: True, fused_optim: True, use_bfloat16: True\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 28.59ms | mean tok/sec: 34452\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# results for DAT-Old-LM\n",
    "for setup in setups:\n",
    "    mean_dt, mean_toks_per_sec = eval_setup_speed(create_dat_old_model, **setup, verbose=False)\n",
    "    print()\n",
    "    results.append((setup, mean_dt, mean_toks_per_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "use_flash_attention: False, use_tf32_matmul: False, compile_model: False, fused_optim: False, use_bfloat16: False\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 34.10ms | mean tok/sec: 29844\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "use_flash_attention: False, use_tf32_matmul: True, compile_model: False, fused_optim: False, use_bfloat16: False\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 34.14ms | mean tok/sec: 29833\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "use_flash_attention: False, use_tf32_matmul: True, compile_model: False, fused_optim: False, use_bfloat16: True\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 38.68ms | mean tok/sec: 26336\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "use_flash_attention: False, use_tf32_matmul: True, compile_model: True, fused_optim: False, use_bfloat16: True\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 28.23ms | mean tok/sec: 34892\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "use_flash_attention: False, use_tf32_matmul: True, compile_model: True, fused_optim: True, use_bfloat16: True\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 22.72ms | mean tok/sec: 43369\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "use_flash_attention: True, use_tf32_matmul: True, compile_model: True, fused_optim: True, use_bfloat16: True\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 22.73ms | mean tok/sec: 43351\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# results for DAT-LM\n",
    "for setup in setups:\n",
    "    mean_dt, mean_toks_per_sec = eval_setup_speed(create_dat_model, **setup, verbose=False)\n",
    "    print()\n",
    "    results.append((setup, mean_dt, mean_toks_per_sec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B = 16, T = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 338024 tokens\n",
      "1 epoch = 20 batches\n"
     ]
    }
   ],
   "source": [
    "# train_loader = DataLoaderLite(B=4, T=256)\n",
    "# train_loader = DataLoaderLite(B=8, T=256)\n",
    "train_loader = DataLoaderLite(B=16, T=1024)\n",
    "# train_loader = DataLoaderLite(B=2, T=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "use_flash_attention: False, use_tf32_matmul: False, compile_model: False, fused_optim: False, use_bfloat16: False\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "mean dt: 163.40ms | mean tok/sec: 100176\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "use_flash_attention: False, use_tf32_matmul: True, compile_model: False, fused_optim: False, use_bfloat16: False\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 163.39ms | mean tok/sec: 100258\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "use_flash_attention: False, use_tf32_matmul: True, compile_model: False, fused_optim: False, use_bfloat16: True\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 144.45ms | mean tok/sec: 113393\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "use_flash_attention: False, use_tf32_matmul: True, compile_model: True, fused_optim: False, use_bfloat16: True\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 79.29ms | mean tok/sec: 198697\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "use_flash_attention: False, use_tf32_matmul: True, compile_model: True, fused_optim: True, use_bfloat16: True\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 76.33ms | mean tok/sec: 206416\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "use_flash_attention: True, use_tf32_matmul: True, compile_model: True, fused_optim: True, use_bfloat16: True\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 47.51ms | mean tok/sec: 331611\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# results for Transformer-LM\n",
    "for setup in setups:\n",
    "    mean_dt, mean_toks_per_sec = eval_setup_speed(create_model=create_transformer_model, **setup, verbose=False, n_steps=26)\n",
    "    print()\n",
    "    results.append((setup, mean_dt, mean_toks_per_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "use_flash_attention: False, use_tf32_matmul: False, compile_model: False, fused_optim: False, use_bfloat16: False\n",
      "====================================================================================================\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 79.10 GiB of which 10.30 GiB is free. Including non-PyTorch memory, this process has 68.79 GiB memory in use. Of the allocated memory 66.59 GiB is allocated by PyTorch, and 1.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# results for DAT-Old-LM\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m setup \u001b[38;5;129;01min\u001b[39;00m setups:\n\u001b[0;32m----> 4\u001b[0m     mean_dt, mean_toks_per_sec \u001b[38;5;241m=\u001b[39m \u001b[43meval_setup_speed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_dat_old_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m26\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m      6\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend((setup, mean_dt, mean_toks_per_sec))\n",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m, in \u001b[0;36meval_setup_speed\u001b[0;34m(create_model, use_flash_attention, use_tf32_matmul, compile_model, fused_optim, use_bfloat16, verbose, n_steps)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compile_model:\n\u001b[1;32m     14\u001b[0m     model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcompile(model)\n\u001b[0;32m---> 16\u001b[0m dts, tokens_per_sec \u001b[38;5;241m=\u001b[39m \u001b[43meval_train_speed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfused_optim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfused_optim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_bfloat16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_bfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     18\u001b[0m mean_dt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(dts[\u001b[38;5;241m1\u001b[39m:])\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(dts[\u001b[38;5;241m1\u001b[39m:])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1000\u001b[39m \u001b[38;5;66;03m# exclude first step cuz of compilation overhead\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m, in \u001b[0;36meval_train_speed\u001b[0;34m(model, n_steps, fused_optim, use_bfloat16, verbose)\u001b[0m\n\u001b[1;32m     15\u001b[0m         logits, loss \u001b[38;5;241m=\u001b[39m model(x, y)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 17\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.conda/envs/abstract_transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/abstract_transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/radev/project/lafferty/ma2393/transformer_residual_stream/dev/../dual_attention_transformer_old.py:1011\u001b[0m, in \u001b[0;36mDualAttnTransformerLM.forward\u001b[0;34m(self, x, targets)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m   1010\u001b[0m     symbols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39msymbol_retriever(x)\n\u001b[0;32m-> 1011\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymbols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreqs_cos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_sin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreqs_sin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1013\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;66;03m# compute loss if given targets\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/abstract_transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/abstract_transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/radev/project/lafferty/ma2393/transformer_residual_stream/dev/../dual_attention_transformer_old.py:692\u001b[0m, in \u001b[0;36mDualAttnEncoderBlock.forward\u001b[0;34m(self, x, symbols, freqs_cos, freqs_sin)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, symbols, freqs_cos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, freqs_sin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_first:\n\u001b[0;32m--> 692\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_dual_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymbols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreqs_cos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_sin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreqs_sin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/gpfs/radev/project/lafferty/ma2393/transformer_residual_stream/dev/../dual_attention_transformer_old.py:702\u001b[0m, in \u001b[0;36mDualAttnEncoderBlock._compute_dual_attn\u001b[0;34m(self, x, symbols, freqs_cos, freqs_sin)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compute_dual_attn\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, symbols, freqs_cos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, freqs_sin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 702\u001b[0m     x, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdual_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymbols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcausal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfreqs_cos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreqs_cos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_sin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreqs_sin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x) \u001b[38;5;66;03m# dropout\u001b[39;00m\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.conda/envs/abstract_transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/abstract_transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/radev/project/lafferty/ma2393/transformer_residual_stream/dev/../dual_attention_transformer_old.py:120\u001b[0m, in \u001b[0;36mDualAttention.forward\u001b[0;34m(self, x, symbols, attn_mask, is_causal, freqs_cos, freqs_sin, need_weights)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# relational cross-attention\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_rel_attn:\n\u001b[0;32m--> 120\u001b[0m     rel_attn_out, \u001b[38;5;241m*\u001b[39mrel_attn_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelational_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymbols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfreqs_cos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreqs_cos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_sin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreqs_sin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# combine self-attention and relational cross-attention\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_rel_attn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_self_attn:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# concat self-attention output (E) and relational cross-attention output (A)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/abstract_transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/abstract_transformer/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/gpfs/radev/project/lafferty/ma2393/transformer_residual_stream/dev/../dual_attention_transformer_old.py:392\u001b[0m, in \u001b[0;36mRelationalAttention.forward\u001b[0;34m(self, x, symbols, freqs_cos, freqs_sin, attn_mask, is_causal)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_relative_positional_symbols:\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;66;03m# sv: (bs, seqlen, n_heads, head_dim)\u001b[39;00m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# attn_scores: (bs, n_heads, seqlen, seqlen)\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;66;03m# relations: (bs, seqlen, seqlen, n_heads, head_dim)\u001b[39;00m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;66;03m# Math: A_i^h = \\sum_j \\alpha_{ij}^h (r(x_i, x_j) W_r^h + s_j W_s^h)\u001b[39;00m\n\u001b[1;32m    391\u001b[0m     attended_symbols \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbhij,bjhd->bihd\u001b[39m\u001b[38;5;124m'\u001b[39m, attn_scores, sv) \u001b[38;5;66;03m# (bs, seqlen, n_heads, head_dim)\u001b[39;00m\n\u001b[0;32m--> 392\u001b[0m     attended_relations \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbhij,bijhd->bihd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproj_relations\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (bs, seqlen, n_heads, head_dim)\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     output \u001b[38;5;241m=\u001b[39m attended_symbols \u001b[38;5;241m+\u001b[39m attended_relations \u001b[38;5;66;03m# (bs, seqlen, n_heads, head_dim)\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;66;03m# sv: (seqlen, seqlen, n_heads, head_dim)\u001b[39;00m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;66;03m# attn_scores: (bs, n_heads, seqlen, seqlen)\u001b[39;00m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# relations: (bs, seqlen, seqlen, n_heads, head_dim)\u001b[39;00m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;66;03m# Math: A_i^h = \\sum_j \\alpha_{ij}^h (r(x_i, x_j) W_r^h + s_{j-i} W_s)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/abstract_transformer/lib/python3.11/site-packages/torch/functional.py:380\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    382\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 GiB. GPU 0 has a total capacity of 79.10 GiB of which 10.30 GiB is free. Including non-PyTorch memory, this process has 68.79 GiB memory in use. Of the allocated memory 66.59 GiB is allocated by PyTorch, and 1.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# runs out of memory because old implementation is too memory hungry\n",
    "torch.cuda.empty_cache()\n",
    "# results for DAT-Old-LM\n",
    "for setup in setups:\n",
    "    mean_dt, mean_toks_per_sec = eval_setup_speed(create_dat_old_model, **setup, verbose=False, n_steps=26)\n",
    "    print()\n",
    "    results.append((setup, mean_dt, mean_toks_per_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "use_flash_attention: False, use_tf32_matmul: False, compile_model: False, fused_optim: False, use_bfloat16: False\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 280.18ms | mean tok/sec: 58425\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "use_flash_attention: False, use_tf32_matmul: True, compile_model: False, fused_optim: False, use_bfloat16: False\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 280.32ms | mean tok/sec: 58441\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "use_flash_attention: False, use_tf32_matmul: True, compile_model: False, fused_optim: False, use_bfloat16: True\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 277.55ms | mean tok/sec: 59029\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "use_flash_attention: False, use_tf32_matmul: True, compile_model: True, fused_optim: False, use_bfloat16: True\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 203.19ms | mean tok/sec: 77545\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "use_flash_attention: False, use_tf32_matmul: True, compile_model: True, fused_optim: True, use_bfloat16: True\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 197.66ms | mean tok/sec: 79714\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "use_flash_attention: True, use_tf32_matmul: True, compile_model: True, fused_optim: True, use_bfloat16: True\n",
      "====================================================================================================\n",
      "****************************************************************************************************\n",
      "mean dt: 197.64ms | mean tok/sec: 79721\n",
      "****************************************************************************************************\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# results for DAT-LM\n",
    "for setup in setups:\n",
    "    mean_dt, mean_toks_per_sec = eval_setup_speed(create_dat_model, **setup, verbose=False, n_steps=26)\n",
    "    print()\n",
    "    results.append((setup, mean_dt, mean_toks_per_sec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems compile without flash attention is 145ms while flash attention without compile is 124ms. not too different. (with all other optimizations turned on)\n",
    "\n",
    "using tf32 or bfloat16 makes big difference. with both turned on and no flash attention or compilation, avg time per step is 257ms. This is down from 1033ms without tf32 or bfloat16.\n",
    "\n",
    "with compile + flash attention but no tf32 or bfloat16, the avg time per step is 246ms.\n",
    "\n",
    "With both, all optimizations, we are down to 105ms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all optimizations, RoPE is the same as positional embeddings (105 ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all optimizations, RMSNorm is the marginally faster than LayerNorm (103.47ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all optimizations, SwiGLU activation is 121ms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "norm_first = False or is marginally faster (for some reason) than pre-LN (101.80ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more to check speed\n",
    "# effect of rmsnorm vs layernorm\n",
    "# effect of bias vs no bias\n",
    "# effect of pos emb vs RoPE\n",
    "# effect of activation\n",
    "# effect of MQA vs MHA\n",
    "# clean custom loop vs pytorch lightning\n",
    "\n",
    "# how much slower is DAT for comparable configs that work for both (e.g., no compile but tf32 matmul and bfloat16, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
