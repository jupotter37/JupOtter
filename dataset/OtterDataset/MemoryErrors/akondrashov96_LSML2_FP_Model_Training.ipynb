{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "In this final project, we create a service for spelling correction for english language.\n",
    "\n",
    "### Setup and Models\n",
    "\n",
    "To perform this task, we chose [T5-lage-spell model](https://huggingface.co/ai-forever/T5-large-spell) (description found here : https://habr.com/ru/companies/sberdevices/articles/763932/) and a smaller model (T5-small) to make the main job.\n",
    "\n",
    "**UPDATE**: due to unimpressive results obtained after training and attempts to distill the model, we chose to proceed by simply quantising the weights of T5-large-spell model.\n",
    "\n",
    "### Procedure\n",
    "\n",
    "We begin by loading the models and creating datasets. We will be using two datasets: `bea60k`, containing about 60k sentences for training the model and `jfleg` for distillation. Afterwards we assess the trained models quality.\n",
    "\n",
    "**UPDATE**: model quality was not very good, so we just chose to perform quantisation of the large model.\n",
    "\n",
    "### Metric\n",
    "\n",
    "To evaluate the quality of the model, we suggest the following metric: we compare the original sentence ($x$), it's correction ($y$) and model predicted correction ($\\hat y$). Let's call it \"Error correction rate\". To calculate it, we use the following formula:\n",
    "\n",
    "$$Error\\ correction\\ rate = \\cfrac{\\sum \\mathbb{I}(\\hat y = y)}{\\#\\ of\\ errors}$$\n",
    "\n",
    "We first compare $x$ and $y$ to determine the indices of corrected words. The model does not deal with missing or extra whitespaces, the word count is always the same. Afterwards, we compare (at specified indices), whether the correction and predicted correction match. If yes, the error correction increases by one, if not, nothing happens. After comparing all words, we divide the matches by total number of errors. We also track the number of false positives (needless corrections) in a separate key.\n",
    "\n",
    "### Results\n",
    "\n",
    "**UPDATE**: it turned out, that the quality of t5-small model is too low to use to for production purposes (due to insufficient training, or maybe we messed up the procedure). So, we decided to perform quantization of the large model and use it in production.\n",
    "\n",
    "The large model shows quite good correction quality. Moreover, quantization helped to reduce model size from 2.8 GB to just 0.8 GB without losing quality. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer, AdamW, get_linear_schedule_with_warmup, PreTrainedTokenizerFast\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    '''\n",
    "    Flattens labels and calculates accuracy\n",
    "    '''\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def flat_accuracy_tocpu(preds, labels):\n",
    "    '''\n",
    "    Flattens labels and calculates accuracy\n",
    "    '''\n",
    "    pred_flat = np.argmax(preds.cpu().detach().numpy(), axis=1).flatten()\n",
    "    labels_flat = labels.cpu().detach().numpy().flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def calc_weights(model):    \n",
    "    '''\n",
    "    Calculates number of weights in the model\n",
    "    '''\n",
    "    result = 0\n",
    "    for layer in model.features.children():\n",
    "        if hasattr(layer, 'weight'):\n",
    "            result += len(layer.weight.reshape(-1))\n",
    "    for layer in model.classifier.children():\n",
    "        if hasattr(layer, 'weight'):\n",
    "            result += len(layer.weight.reshape(-1))\n",
    "    return result\n",
    "\n",
    "def calc_size(model):\n",
    "    '''\n",
    "    Calculates size of the model\n",
    "    '''\n",
    "    torch.save(model.state_dict(), \"./tmp/model.p\")\n",
    "    size=os.path.getsize(\"./tmp/model.p\")\n",
    "    os.remove('./tmp/model.p')\n",
    "    return \"{:.3f} KB\".format(size / 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_teacher_model = \"ai-forever/T5-large-spell\" # we will load it before distillation to save space on GPU\n",
    "path_to_student_model = \"t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # T5\n",
    "# model_T5_spell = T5ForConditionalGeneration.from_pretrained(path_to_teacher_model)\n",
    "# tokenizer_T5_spell = AutoTokenizer.from_pretrained(path_to_teacher_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T5 small\n",
    "tokenizer_student = AutoTokenizer.from_pretrained(path_to_student_model)\n",
    "t5_small = T5ForConditionalGeneration.from_pretrained(path_to_student_model)\n",
    "\n",
    "# load weights of a model (from saved results)\n",
    "# t5_small.load_state_dict(torch.load(\"t5-small2023-11-12-09-12-20.391733.bin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already mentioned, we will use two datasets: jfleg for distillation and bea60k for training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "jfleg = load_dataset(\"jfleg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>corrections</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>The government also should try to reduce the s...</td>\n",
       "      <td>The government should also try to reduce the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>Alot of memories with enogh time to remember w...</td>\n",
       "      <td>A lot of memories , with enough time to rememb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>Sceene of violence can affect on them .</td>\n",
       "      <td>A scene of violence can have an effect on them .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>While the communities in general have reckoned...</td>\n",
       "      <td>The communities in general have reckoned that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  \\\n",
       "750  The government also should try to reduce the s...   \n",
       "751  Alot of memories with enogh time to remember w...   \n",
       "752           Sceene of violence can affect on them .    \n",
       "753  While the communities in general have reckoned...   \n",
       "754                                                      \n",
       "\n",
       "                                           corrections  \n",
       "750  The government should also try to reduce the s...  \n",
       "751  A lot of memories , with enough time to rememb...  \n",
       "752  A scene of violence can have an effect on them .   \n",
       "753  The communities in general have reckoned that ...  \n",
       "754                                                     "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jfleg_validation = jfleg[\"validation\"][:]\n",
    "jfleg_test = jfleg[\"test\"][:]\n",
    "\n",
    "jfleg_validation = pd.DataFrame(jfleg_validation)\n",
    "jfleg_validation[\"corrections\"] = jfleg_validation[\"corrections\"].apply(lambda x: x[0])\n",
    "\n",
    "jfleg_validation.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('traintest/test.bea60k', 'rb') as f:\n",
    "    # Read the data into a NumPy array\n",
    "    corrections = np.fromfile(f, dtype=np.uint8)\n",
    "    corrections = \"\".join([chr(a) for a in corrections])\n",
    "    corrections = corrections.split(\"\\n\")\n",
    "\n",
    "with open('traintest/test.bea60k.noise', 'rb') as f:\n",
    "    # Read the data into a NumPy array\n",
    "    sentences = np.fromfile(f, dtype=np.uint8)\n",
    "    sentences = \"\".join([chr(a) for a in sentences])\n",
    "    sentences = sentences.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>corrections</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grammar: I WANT TO THAK YOU FOR PREPARING SUCH...</td>\n",
       "      <td>I WANT TO THANK YOU FOR PREPARING SUCH A GOOD ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>grammar: IN MY OPINION FAMOUS PEOPLE ARE BEING...</td>\n",
       "      <td>IN MY OPINION FAMOUS PEOPLE ARE BEING OBLIGED ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>grammar: Also, I want to say that the plays an...</td>\n",
       "      <td>Also, I want to say that the plays and films w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grammar: In our Acadamy we are not allowed to ...</td>\n",
       "      <td>In our Academy we are not allowed to smoke.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>grammar: I was trully dissapointed by it.</td>\n",
       "      <td>I was truly disappointed by it.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences  \\\n",
       "0  grammar: I WANT TO THAK YOU FOR PREPARING SUCH...   \n",
       "1  grammar: IN MY OPINION FAMOUS PEOPLE ARE BEING...   \n",
       "2  grammar: Also, I want to say that the plays an...   \n",
       "3  grammar: In our Acadamy we are not allowed to ...   \n",
       "4          grammar: I was trully dissapointed by it.   \n",
       "\n",
       "                                         corrections  \n",
       "0  I WANT TO THANK YOU FOR PREPARING SUCH A GOOD ...  \n",
       "1  IN MY OPINION FAMOUS PEOPLE ARE BEING OBLIGED ...  \n",
       "2  Also, I want to say that the plays and films w...  \n",
       "3        In our Academy we are not allowed to smoke.  \n",
       "4                    I was truly disappointed by it.  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bea60k = pd.DataFrame({\"sentences\" : sentences, \n",
    "                       \"corrections\" : corrections})\n",
    "\n",
    "bea60k['sentences'] = bea60k['sentences'].apply(lambda x: x.replace(\" .\", \".\").replace(\" ,\", \",\"))\n",
    "bea60k['sentences'] = bea60k['sentences'].apply(lambda x: \"grammar: \" + x)\n",
    "bea60k['corrections'] = bea60k['corrections'].apply(lambda x: x.replace(\" .\", \".\").replace(\" ,\", \",\"))\n",
    "\n",
    "bea60k.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63045, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bea60k.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the data tables into data loaders. Before that, we need to tokenise them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(sentence_list, corrections_list, tokenizer):\n",
    "\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids_sentence = []\n",
    "    attention_masks_sentence = []\n",
    "\n",
    "    input_ids_correction = []\n",
    "    attention_masks_correction = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sentence in sentence_list:\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict_sentence = tokenizer.encode_plus(\n",
    "                            sentence,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 128,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                    )\n",
    "\n",
    "        # Add the encoded sentence to the list.\n",
    "        input_ids_sentence.append(encoded_dict_sentence['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks_sentence.append(encoded_dict_sentence['attention_mask'])\n",
    "\n",
    "    for correction in corrections_list:\n",
    "\n",
    "        encoded_dict_correction = tokenizer.encode_plus(\n",
    "                            correction,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 128,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                    )\n",
    "\n",
    "        # Add the encoded sentence to the list.\n",
    "        input_ids_correction.append(encoded_dict_correction['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks_correction.append(encoded_dict_correction['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids_sentence = torch.cat(input_ids_sentence, dim=0)\n",
    "    attention_masks_sentence = torch.cat(attention_masks_sentence, dim=0)\n",
    "\n",
    "    input_ids_correction = torch.cat(input_ids_correction, dim=0)\n",
    "    attention_masks_correction = torch.cat(attention_masks_correction, dim=0)\n",
    "\n",
    "    # labels = torch.tensor(labels_list.to_numpy(), dtype=torch.long)\n",
    "\n",
    "    # Print sentence 0, now as a list of IDs.\n",
    "    print('Original: ', sentence_list[0])\n",
    "    print('Token IDs:', input_ids_sentence[0])\n",
    "\n",
    "    # Combine the training inputs into a TensorDataset.\n",
    "    dataset = TensorDataset(input_ids_sentence, attention_masks_sentence, input_ids_correction, attention_masks_correction)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\Artem Kondrashov\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  grammar: I WANT TO THAK YOU FOR PREPARING SUCH A GOOD PROGRAMME FOR US AND ESPECIALLY FOR TAKING US ON THE RIVER TRIP TO GREENWICH.\n",
      "Token IDs: tensor([19519,    10,    27,   549,  9156,  3001,     3,  4611, 12396,  6223,\n",
      "         5652,  6045, 13986, 21034,   180, 20314,    71, 28299,  6828,   517,\n",
      "        23203,  4369,  5652,   837,  3430,   262, 20452, 15397,  5121,  5652,\n",
      "            3,  3221, 20961,   837,  9191,  1853,     3,  5593, 16174,   332,\n",
      "        26017,  3001,     3,  8727, 23394,   518, 22293,     5,     1,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "dataset = tokenize_dataset(bea60k[\"sentences\"].values,\n",
    "                           bea60k[\"corrections\"].values, \n",
    "                           tokenizer_student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_valid = random_split(dataset, [0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            dataset_train,  \n",
    "            sampler = RandomSampler(dataset_train),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            dataset_valid, \n",
    "            sampler = SequentialSampler(dataset_valid), \n",
    "            batch_size = batch_size \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  So I think we can not live if old people could not find siences and tecnologies and they did not developped . \n",
      "Token IDs: tensor([ 264,   27,  317,   62,   54,   59,  619,    3,   99,  625,  151,  228,\n",
      "          59,  253,  108, 1433,    7,   11,    3, 5822,   29, 4137,    7,   11,\n",
      "          79,  410,   59, 1344, 3138,    3,    5,    1,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "dataset_distill = tokenize_dataset(jfleg_validation[\"sentence\"].values,\n",
    "                           jfleg_validation[\"corrections\"].values, \n",
    "                           tokenizer_student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "distll_dataloader = DataLoader(\n",
    "            dataset_distill,  \n",
    "            sampler = RandomSampler(dataset_distill),\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have described the metric above in the [Description](#description) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_correction_rate(sentence, correction, prediction):\n",
    "\n",
    "    sentence = np.array(sentence.split())\n",
    "    correction = np.array(correction.split())\n",
    "    prediction = np.array(prediction.split())\n",
    "\n",
    "    corr_indices = np.array([x != y for x, y in zip(sentence, correction)], dtype=np.uint8)\n",
    "\n",
    "    pred_indices = np.array([x != y for x, y in zip(sentence, prediction)], dtype=np.uint8)\n",
    "\n",
    "    # print(sentence)\n",
    "    # print(prediction)\n",
    "\n",
    "    # print(corr_indices)\n",
    "    # print(pred_indices)\n",
    "\n",
    "    acc = {\"corr\" : 0, \"FP\": 0}\n",
    "\n",
    "    for i, data in enumerate(zip(corr_indices, pred_indices)):\n",
    "        corr, pred = data\n",
    "        if corr == pred:\n",
    "            if corr == 1:\n",
    "                if correction[i] == prediction[i]:\n",
    "                    acc['corr'] += 1\n",
    "        elif pred == 1:\n",
    "            acc['FP'] += 1\n",
    "\n",
    "    if corr_indices.sum() != 0:\n",
    "        acc['corr'] /= corr_indices.sum()\n",
    "    else:\n",
    "        acc['corr'] = 1\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train T5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_small.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Artem Kondrashov\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "params = list(t5_small.named_parameters())\n",
    "optimizer = AdamW(t5_small.parameters(),\n",
    "                  lr = 2e-5,\n",
    "                  eps = 1e-8\n",
    "                )\n",
    "\n",
    "epochs = 3 # more epochs appear to give worse results on average\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0, \n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NLP(model, trainloader, validloader, optimizer, scheduler, seed_val):\n",
    "    '''\n",
    "    Train + Validation\n",
    "    '''\n",
    "\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    # store training data here\n",
    "    training_stats = []\n",
    "\n",
    "    # start timer\n",
    "    time_start = time.time() \n",
    "\n",
    "    for e in range(0, epochs):\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(e + 1, epochs))\n",
    "\n",
    "        # training segment\n",
    "        print('Training...')\n",
    "\n",
    "        t0 = time.time()\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        # process data\n",
    "        for step, batch in tqdm(enumerate(trainloader), total=len(trainloader)):\n",
    "            \n",
    "            if step % 1000 == 0 and not step == 0:\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(trainloader), elapsed))\n",
    "\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_attention_mask = batch[1].to(device)\n",
    "\n",
    "            b_labels = batch[2].to(device)\n",
    "            d_attention_mask = batch[3].to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            res = model(input_ids=b_input_ids, \n",
    "                        #attention_mask = b_attention_mask,\n",
    "                        #decoder_input_ids=b_labels, \n",
    "                        #decoder_attention_mask = d_attention_mask,\n",
    "                        labels=b_labels)\n",
    "            \n",
    "            loss = res['loss']\n",
    "            logits = res['logits']\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(trainloader)\n",
    "\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "        # validation segment\n",
    "        print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "        model.eval()\n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        for step, batch in tqdm(enumerate(validloader), total=len(validloader)):\n",
    "\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_attention_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            d_attention_mask = batch[3].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                res = model(input_ids=b_input_ids, \n",
    "                        #decoder_input_ids=b_labels, \n",
    "                        #attention_mask = b_attention_mask,\n",
    "                        #decoder_attention_mask = d_attention_mask,\n",
    "                        labels=b_labels)\n",
    "\n",
    "            loss = res['loss']\n",
    "            logits = res['logits']\n",
    "\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "        avg_val_accuracy = total_eval_accuracy / len(validloader)\n",
    "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "        avg_val_loss = total_eval_loss / len(validloader)\n",
    "\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': e + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Accur.': avg_val_accuracy,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "\n",
    "        torch.save(model.state_dict(), path_to_student_model + str(datetime.datetime.now()).replace(\" \", \"-\").replace(\":\", \"-\") + \".bin\")\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-time_start)))\n",
    "\n",
    "    return training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 1000/3547 [05:16<13:09,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,000  of  3,547.    Elapsed: 0:05:17.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 2000/3547 [10:32<08:00,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 2,000  of  3,547.    Elapsed: 0:10:33.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 3000/3547 [15:44<03:06,  2.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 3,000  of  3,547.    Elapsed: 0:15:44.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3547/3547 [18:37<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:18:37\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/394 [00:00<?, ?it/s]C:\\Users\\Artem Kondrashov\\AppData\\Local\\Temp\\ipykernel_16876\\615669151.py:14: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
      "100%|██████████| 394/394 [02:01<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.00\n",
      "  Validation Loss: 0.03\n",
      "  Validation took: 0:02:01\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 1000/3547 [05:11<13:12,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,000  of  3,547.    Elapsed: 0:05:12.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 2000/3547 [10:23<08:00,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 2,000  of  3,547.    Elapsed: 0:10:23.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 3000/3547 [15:36<02:50,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 3,000  of  3,547.    Elapsed: 0:15:37.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3547/3547 [18:26<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:18:27\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 394/394 [02:01<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.00\n",
      "  Validation Loss: 0.03\n",
      "  Validation took: 0:02:01\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 1000/3547 [05:09<13:11,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,000  of  3,547.    Elapsed: 0:05:09.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 2000/3547 [10:19<08:00,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 2,000  of  3,547.    Elapsed: 0:10:20.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 3000/3547 [15:30<02:49,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 3,000  of  3,547.    Elapsed: 0:15:30.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3547/3547 [18:20<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:18:20\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 394/394 [02:00<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.00\n",
      "  Validation Loss: 0.03\n",
      "  Validation took: 0:02:01\n",
      "\n",
      "Training complete!\n",
      "Total training took 1:01:29 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "seed_val = 42\n",
    "result = train_NLP(t5_small, \n",
    "                   train_dataloader, \n",
    "                   validation_dataloader, \n",
    "                   optimizer, \n",
    "                   scheduler, \n",
    "                   seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5_small.load_state_dict(torch.load('t5_small.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: grammar: IN MY OPINION FAMOUS PEOPLE ARE BEING OBLIGED TO PAY A PRICE FOR BEING FAMOUS THAT, IN SOME CASS, COSTS MORE THAN THEY DESERVE TO PAY.\n",
      "Output: IN MY OPINION FAMOUS PEOPLE ARE BEING OBLIGED TO PAY A PRICE FOR BEING FAMOUS THAT, IN SOME CASS, COSTS MORE THAN THEY\n"
     ]
    }
   ],
   "source": [
    "sentence = bea60k['sentences'][1]\n",
    "\n",
    "print(\"Input:\", sentence)\n",
    "\n",
    "encodings = tokenizer_student(sentence, add_special_tokens=False, return_tensors=\"pt\").to(device).input_ids\n",
    "generated_tokens = t5_small.generate(encodings, bos_token_id=101, eos_token_id=102, max_length = 50)\n",
    "answer = tokenizer_student.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Output:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: grammar: I yu bкoght something goregous, you well be vry happy\n",
      "Output: I yu bкoght something goregous, you well be vry happy happy be vry happy happy be vry happy happy be vry happy happy be vry happy happy\n"
     ]
    }
   ],
   "source": [
    "prefix = \"grammar: \"\n",
    "\n",
    "sentence = \"I yu bкoght something goregous, you well be vry happy\"\n",
    "sentence = prefix + sentence\n",
    "\n",
    "print(\"Input:\", sentence)\n",
    "\n",
    "encodings = tokenizer_student(sentence, add_special_tokens=False, return_tensors=\"pt\").to(device).input_ids\n",
    "generated_tokens = t5_small.generate(encodings, max_length = 50)\n",
    "answer = tokenizer_student.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "print(\"Output:\", answer[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model appears to be working too bad. Either we have messed up with the learning procedure, or learning was insufficient (which is strange, as we used quite a big dataset on multiple epochs - more than actually listed in this notebook, as we also loaded saved states). \n",
    "\n",
    "We suggest that we don't use this model for error correction, but instead try to make the big model lighter using quantisation. See block [Quantization](#quantization) for this procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict - full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_teacher_model = \"ai-forever/T5-large-spell\"\n",
    "\n",
    "# T5\n",
    "model_T5_spell = T5ForConditionalGeneration.from_pretrained(path_to_teacher_model)\n",
    "tokenizer_T5_spell = AutoTokenizer.from_pretrained(path_to_teacher_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: grammar: IN MY OPINION FAMOUS PEOPLE ARE BEING OBLIGED TO PAY A PRICE FOR BEING FAMOUS THAT, IN SOME CASS, COSTS MORE THAN THEY DESERVE TO PAY.\n",
      "Output: IN MY OPINION FAMOUS PEOPLE ARE BEING OBLIGED TO PAY A PRICE FOR BEING FAMOUS THAT, IN SOME CASS, COSTS MORE THAN THEY DESERVE TO PAY.\n"
     ]
    }
   ],
   "source": [
    "sentence = bea60k['sentences'][1]\n",
    "\n",
    "print(\"Input:\", sentence)\n",
    "\n",
    "# create encodings for the sentence and then generate a correction for it\n",
    "encodings = tokenizer_T5_spell(sentence, return_tensors=\"pt\")\n",
    "generated_tokens = model_T5_spell.generate(**encodings, max_new_tokens = 100)\n",
    "answer = tokenizer_T5_spell.batch_decode(generated_tokens, skip_special_tokens=True, )\n",
    "\n",
    "print(\"Output:\", answer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: grammar: I yu bкoght something goregous, you well be vry happy. Sorry for unconenvinence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Artem Kondrashov\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: If you brought something gorgeous, you will be very happy. Sorry for inconvenience\n"
     ]
    }
   ],
   "source": [
    "prefix = \"grammar: \"\n",
    "\n",
    "sentence = \"I yu bкoght something goregous, you well be vry happy. Sorry for unconenvinence\"\n",
    "sentence = prefix + sentence\n",
    "\n",
    "print(\"Input:\", sentence)\n",
    "\n",
    "encodings = tokenizer_T5_spell(sentence, return_tensors=\"pt\")\n",
    "generated_tokens = model_T5_spell.generate(**encodings)\n",
    "answer = tokenizer_T5_spell.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "print(\"Output:\", answer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 755/755 [17:16<00:00,  1.37s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19124576775596863"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ECR = []\n",
    "\n",
    "for i, val in tqdm(jfleg_validation.iterrows(), total=len(jfleg_validation)):\n",
    "    \n",
    "    encodings = tokenizer_T5_spell(val['sentence'], return_tensors=\"pt\")\n",
    "    generated_tokens = model_T5_spell.generate(**encodings)\n",
    "    answer = tokenizer_T5_spell.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "    ECR.append(error_correction_rate(val['sentence'].replace(\"grammar: \", \"\"),\n",
    "                                     val['corrections'],\n",
    "                                     answer)['corr'])\n",
    "    \n",
    "np.mean(np.array(ECR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For full model, the corrections work quite well. However, our metric shows quite low result: only 20%. Note though, that this implies 20% match with some \"benchmark\" correction, which may not only contain spelling correction, but also restructure phrase a bit. Overall, I believe spelling correction is OK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_small.to(device)\n",
    "model_T5_spell.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill(teacher_model, student_model, train_loader, epoch_number=2, alpha=0.5, temperature=2):\n",
    "    \n",
    "    def error_and_output(var_X_batch, var_y_batch): # create loss function\n",
    "        # Kullback-Leibler Divergence is used to calc cross-entropy between answers of models\n",
    "        kldloss = nn.KLDivLoss()  \n",
    "        # Regular cross-entropy\n",
    "        celoss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # teached model outputs\n",
    "        teacher_logits = teacher_model(var_X_batch, labels=var_y_batch)['logits']\n",
    "        # student model outputs\n",
    "        student_preds = student_model(var_X_batch, labels=var_y_batch)\n",
    "        student_logits = student_preds['logits']\n",
    "        \n",
    "        # softmax with temperature T for student network\n",
    "        soft_predictions = F.log_softmax( student_logits / temperature, dim=1 )\n",
    "        # and for teacher network\n",
    "        soft_labels = F.softmax( teacher_logits / temperature, dim=1 )\n",
    "        # distillation loss\n",
    "        distillation_loss = kldloss(soft_predictions, soft_labels)\n",
    "        \n",
    "        # regular loss\n",
    "        student_loss = student_preds['loss']\n",
    "        # student_loss = celoss(student_logits, var_y_batch)\n",
    "        \n",
    "        # sum-up\n",
    "        return distillation_loss * alpha + student_loss * (1 - alpha), student_logits\n",
    "    \n",
    "    optimizer = torch.optim.Adam(student_model.parameters())\n",
    "    student_model.train()\n",
    "    \n",
    "    # train goes as usual\n",
    "    for epoch in range(epoch_number):\n",
    "        correct = 0\n",
    "        \n",
    "        for batch_idx, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_attention_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            d_attention_mask = batch[3].to(device)\n",
    "            \n",
    "            var_X_batch = b_input_ids.long()\n",
    "            var_y_batch = b_labels.long()\n",
    "            optimizer.zero_grad()\n",
    "            loss, output = error_and_output(var_X_batch, var_y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # predicted = torch.max(output.data, 1)[1] \n",
    "            # correct += (predicted == var_y_batch).sum()\n",
    "            correct += flat_accuracy_tocpu(output, b_labels)\n",
    "            if batch_idx % 200 == 0:\n",
    "                print('Epoch : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n",
    "                    epoch, \\\n",
    "                    batch_idx*len(b_input_ids), \\\n",
    "                    len(train_loader.dataset), \\\n",
    "                    100.*batch_idx / len(train_loader), \\\n",
    "                    loss.data, \\\n",
    "                    float(correct*100) / float(batch_size*(batch_idx+1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/48 [00:00<?, ?it/s]C:\\Users\\Artem Kondrashov\\AppData\\Local\\Temp\\ipykernel_10320\\2584833987.py:25: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
      "  2%|▏         | 1/48 [02:35<2:01:43, 155.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 [0/755 (0%)]\tLoss: 0.928890\t Accuracy:0.000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/48 [06:41<2:34:04, 200.97s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 252.00 MiB (GPU 0; 6.00 GiB total capacity; 19.28 GiB already allocated; 0 bytes free; 19.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Artem Kondrashov\\Documents\\Обучение\\HSE MDS\\22. LSML 2\\FP\\Model_Training.ipynb Cell 52\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Artem%20Kondrashov/Documents/%D0%9E%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5/HSE%20MDS/22.%20LSML%202/FP/Model_Training.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(\u001b[39m2023\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Artem%20Kondrashov/Documents/%D0%9E%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5/HSE%20MDS/22.%20LSML%202/FP/Model_Training.ipynb#X63sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m distill(model_T5_spell, t5_small, distll_dataloader, temperature\u001b[39m=\u001b[39;49m\u001b[39m10.0\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Artem Kondrashov\\Documents\\Обучение\\HSE MDS\\22. LSML 2\\FP\\Model_Training.ipynb Cell 52\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Artem%20Kondrashov/Documents/%D0%9E%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5/HSE%20MDS/22.%20LSML%202/FP/Model_Training.ipynb#X63sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Artem%20Kondrashov/Documents/%D0%9E%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5/HSE%20MDS/22.%20LSML%202/FP/Model_Training.ipynb#X63sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m loss, output \u001b[39m=\u001b[39m error_and_output(var_X_batch, var_y_batch)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Artem%20Kondrashov/Documents/%D0%9E%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5/HSE%20MDS/22.%20LSML%202/FP/Model_Training.ipynb#X63sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Artem%20Kondrashov/Documents/%D0%9E%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5/HSE%20MDS/22.%20LSML%202/FP/Model_Training.ipynb#X63sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Artem%20Kondrashov/Documents/%D0%9E%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5/HSE%20MDS/22.%20LSML%202/FP/Model_Training.ipynb#X63sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# predicted = torch.max(output.data, 1)[1] \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Artem%20Kondrashov/Documents/%D0%9E%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5/HSE%20MDS/22.%20LSML%202/FP/Model_Training.ipynb#X63sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# correct += (predicted == var_y_batch).sum()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Artem Kondrashov\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Artem Kondrashov\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 252.00 MiB (GPU 0; 6.00 GiB total capacity; 19.28 GiB already allocated; 0 bytes free; 19.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2023)\n",
    "distill(model_T5_spell, t5_small, distll_dataloader, temperature=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that using two models simultaneusly is a bit too much for my PC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As distillation results prove to be disappointing, we have decided to quantize the full model to make it smaller.  \n",
    "Took some references from [here](https://huggingface.co/docs/transformers/main/main_classes/quantization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_T5_spell.state_dict(), \"model_T5_spell.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2881706.759 KB'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_size(model_T5_spell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current full model weight is about 2.8 GB. Let's try to reduce it by quantising linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = torch.quantization.quantize_dynamic(model_T5_spell, {torch.nn.Linear}, dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at first, errors in quantization indicated that we needed to set ceratin quantisation configuration to Embedding layers\n",
    "#  but after a few tries, it became obsolete\n",
    "\n",
    "# for _, mod in model_T5_spell.named_modules():\n",
    "#     if isinstance(mod, torch.nn.Embedding):\n",
    "#         mod.qconfig = torch.ao.quantization.float_qparams_weight_only_qconfig\n",
    "\n",
    "# quantized_model = torch.quantization.quantize_dynamic(model_T5_spell, {torch.nn.Linear}, dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./tokenizer\\\\tokenizer_config.json',\n",
       " './tokenizer\\\\special_tokens_map.json',\n",
       " './tokenizer\\\\spiece.model',\n",
       " './tokenizer\\\\added_tokens.json',\n",
       " './tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.save(quantized_model, \"quantized_model.pt\") # commented out saving\n",
    "tokenizer_T5_spell.save_pretrained('./tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'849763.708 KB'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_size(quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, the model size has reduced a lot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that loading a model and tokenizer work OK (as we are going to use them on a server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"./server/src/tokenizer/tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Artem Kondrashov\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_utils.py:314: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (k): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (v): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (o): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (wo): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (k): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (v): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (o): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (wo): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (k): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (v): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (o): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (k): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (v): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (o): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (wo): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (k): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (v): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (o): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (k): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (v): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (o): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): DynamicQuantizedLinear(in_features=1024, out_features=4096, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (wo): DynamicQuantizedLinear(in_features=4096, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): DynamicQuantizedLinear(in_features=1024, out_features=32128, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = torch.load(\"./server/src/models/quantized_T5-large.pt\")\n",
    "new_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = bea60k['sentences'][0]\n",
    "\n",
    "encodings = new_tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
    "generated_tokens = new_model.generate(encodings, max_new_tokens = 100)\n",
    "answer = new_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True, )\n",
    "print(sentence)\n",
    "print(answer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Artem Kondrashov\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you bought something gorgeous, you will be very happy. Sorry for inconvenience\n"
     ]
    }
   ],
   "source": [
    "prefix = \"grammar: \"\n",
    "\n",
    "sentence = \"I yu bкoght something goregous, you well be vry happy. Sorry for unconenvinence\"\n",
    "sentence = prefix + sentence\n",
    "\n",
    "encodings = new_tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
    "generated_tokens = new_model.generate(encodings)\n",
    "answer = new_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "print(answer[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
