{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jjaniak/Desktop/studia/nlp/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Using the latest cached version of the dataset since lewtun/github-issues couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/jjaniak/.cache/huggingface/datasets/lewtun___github-issues/default/0.0.0/3bb24dcad2b45b45e20fc0accc93058dcbe8087d (last modified on Wed Nov 27 18:12:25 2024).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 3019\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "issues_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset = issues_dataset.filter(\n",
    "    lambda x: (x[\"is_pull_request\"] == False and len(x[\"comments\"]) > 0)\n",
    ")\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 808\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = issues_dataset.column_names\n",
    "columns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"]\n",
    "columns_to_remove = set(columns_to_keep).symmetric_difference(columns)\n",
    "issues_dataset = issues_dataset.remove_columns(columns_to_remove)\n",
    "issues_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>[Cool, I think we can do both :), @lhoestq now...</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>[Hi ! I guess the caching mechanism should hav...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>OSCAR unshuffled_original_ko: NonMatchingSplit...</td>\n",
       "      <td>[I tried `unshuffled_original_da` and it is al...</td>\n",
       "      <td>## Describe the bug\\r\\n\\r\\nCannot download OSC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>load_dataset using default cache on Windows ca...</td>\n",
       "      <td>[Hi @daqieq, thanks for reporting.\\r\\n\\r\\nUnfo...</td>\n",
       "      <td>## Describe the bug\\r\\nStandard process to dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>to_tf_dataset keeps a reference to the open da...</td>\n",
       "      <td>[I did some investigation and, as it seems, th...</td>\n",
       "      <td>To reproduce:\\r\\n```python\\r\\nimport datasets ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues/6</td>\n",
       "      <td>Error when citation is not given in the Datase...</td>\n",
       "      <td>[Yes looks good to me.\\r\\nNote that we may ref...</td>\n",
       "      <td>The following error is raised when the `citati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues/5</td>\n",
       "      <td>ValueError when a split is empty</td>\n",
       "      <td>[To fix this I propose to modify only the file...</td>\n",
       "      <td>When a split is empty either TEST, VALIDATION ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues/4</td>\n",
       "      <td>[Feature] Keep the list of labels of a dataset...</td>\n",
       "      <td>[Yes! I see mostly two options for this:\\r\\n- ...</td>\n",
       "      <td>It would be useful to keep the list of the lab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues/3</td>\n",
       "      <td>[Feature] More dataset outputs</td>\n",
       "      <td>[Yes!\\r\\n- pandas will be a one-liner in `arro...</td>\n",
       "      <td>Add the following dataset outputs:\\r\\n\\r\\n- Sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues/2</td>\n",
       "      <td>Issue to read a local dataset</td>\n",
       "      <td>[My first bug report ❤️\\r\\nLooking into this r...</td>\n",
       "      <td>Hello,\\r\\n\\r\\nAs proposed by @thomwolf, I open...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>808 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              html_url  \\\n",
       "0    https://github.com/huggingface/datasets/issues...   \n",
       "1    https://github.com/huggingface/datasets/issues...   \n",
       "2    https://github.com/huggingface/datasets/issues...   \n",
       "3    https://github.com/huggingface/datasets/issues...   \n",
       "4    https://github.com/huggingface/datasets/issues...   \n",
       "..                                                 ...   \n",
       "803   https://github.com/huggingface/datasets/issues/6   \n",
       "804   https://github.com/huggingface/datasets/issues/5   \n",
       "805   https://github.com/huggingface/datasets/issues/4   \n",
       "806   https://github.com/huggingface/datasets/issues/3   \n",
       "807   https://github.com/huggingface/datasets/issues/2   \n",
       "\n",
       "                                                 title  \\\n",
       "0                                Protect master branch   \n",
       "1    Backwards compatibility broken for cached data...   \n",
       "2    OSCAR unshuffled_original_ko: NonMatchingSplit...   \n",
       "3    load_dataset using default cache on Windows ca...   \n",
       "4    to_tf_dataset keeps a reference to the open da...   \n",
       "..                                                 ...   \n",
       "803  Error when citation is not given in the Datase...   \n",
       "804                   ValueError when a split is empty   \n",
       "805  [Feature] Keep the list of labels of a dataset...   \n",
       "806                     [Feature] More dataset outputs   \n",
       "807                      Issue to read a local dataset   \n",
       "\n",
       "                                              comments  \\\n",
       "0    [Cool, I think we can do both :), @lhoestq now...   \n",
       "1    [Hi ! I guess the caching mechanism should hav...   \n",
       "2    [I tried `unshuffled_original_da` and it is al...   \n",
       "3    [Hi @daqieq, thanks for reporting.\\r\\n\\r\\nUnfo...   \n",
       "4    [I did some investigation and, as it seems, th...   \n",
       "..                                                 ...   \n",
       "803  [Yes looks good to me.\\r\\nNote that we may ref...   \n",
       "804  [To fix this I propose to modify only the file...   \n",
       "805  [Yes! I see mostly two options for this:\\r\\n- ...   \n",
       "806  [Yes!\\r\\n- pandas will be a one-liner in `arro...   \n",
       "807  [My first bug report ❤️\\r\\nLooking into this r...   \n",
       "\n",
       "                                                  body  \n",
       "0    After accidental merge commit (91c55355b634d0d...  \n",
       "1    ## Describe the bug\\r\\nAfter upgrading to data...  \n",
       "2    ## Describe the bug\\r\\n\\r\\nCannot download OSC...  \n",
       "3    ## Describe the bug\\r\\nStandard process to dow...  \n",
       "4    To reproduce:\\r\\n```python\\r\\nimport datasets ...  \n",
       "..                                                 ...  \n",
       "803  The following error is raised when the `citati...  \n",
       "804  When a split is empty either TEST, VALIDATION ...  \n",
       "805  It would be useful to keep the list of the lab...  \n",
       "806  Add the following dataset outputs:\\r\\n\\r\\n- Sp...  \n",
       "807  Hello,\\r\\n\\r\\nAs proposed by @thomwolf, I open...  \n",
       "\n",
       "[808 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_dataset.set_format(\"pandas\")\n",
    "df = issues_dataset[:]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cool, I think we can do both :)',\n",
       " '@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"comments\"][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>html_url</th>\n",
       "      <th>title</th>\n",
       "      <th>comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>Cool, I think we can do both :)</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Protect master branch</td>\n",
       "      <td>@lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...</td>\n",
       "      <td>After accidental merge commit (91c55355b634d0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>Hi ! I guess the caching mechanism should have...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://github.com/huggingface/datasets/issues...</td>\n",
       "      <td>Backwards compatibility broken for cached data...</td>\n",
       "      <td>If it's easy enough to implement, then yes ple...</td>\n",
       "      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            html_url  \\\n",
       "0  https://github.com/huggingface/datasets/issues...   \n",
       "1  https://github.com/huggingface/datasets/issues...   \n",
       "2  https://github.com/huggingface/datasets/issues...   \n",
       "3  https://github.com/huggingface/datasets/issues...   \n",
       "\n",
       "                                               title  \\\n",
       "0                              Protect master branch   \n",
       "1                              Protect master branch   \n",
       "2  Backwards compatibility broken for cached data...   \n",
       "3  Backwards compatibility broken for cached data...   \n",
       "\n",
       "                                            comments  \\\n",
       "0                    Cool, I think we can do both :)   \n",
       "1  @lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...   \n",
       "2  Hi ! I guess the caching mechanism should have...   \n",
       "3  If it's easy enough to implement, then yes ple...   \n",
       "\n",
       "                                                body  \n",
       "0  After accidental merge commit (91c55355b634d0d...  \n",
       "1  After accidental merge commit (91c55355b634d0d...  \n",
       "2  ## Describe the bug\\r\\nAfter upgrading to data...  \n",
       "3  ## Describe the bug\\r\\nAfter upgrading to data...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df = df.explode(\"comments\", ignore_index=True)\n",
    "comments_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Protect master branch',\n",
       "       'Backwards compatibility broken for cached datasets that use `.filter()`',\n",
       "       'OSCAR unshuffled_original_ko: NonMatchingSplitsSizesError',\n",
       "       'load_dataset using default cache on Windows causes PermissionError: [WinError 5] Access is denied',\n",
       "       'to_tf_dataset keeps a reference to the open data somewhere, causing issues on windows',\n",
       "       'Conda build fails', 'Mutable columns argument breaks set_format',\n",
       "       'Datasets 1.12 dataset.filter TypeError: get_indices_from_mask_function() got an unexpected keyword argument',\n",
       "       '\"File name too long\" error for file locks',\n",
       "       'Unwanted progress bars when accessing examples',\n",
       "       '`Can not decode content-encoding: gzip` when loading `scitldr` dataset with streaming',\n",
       "       'windows download abnormal',\n",
       "       'Having a dependency defining fsspec entrypoint raises an AttributeError when importing datasets',\n",
       "       'timit_asr dataset only includes one text phrase',\n",
       "       'FORCE_REDOWNLOAD does not work', 'Add WIT Dataset',\n",
       "       'Incompatibility with pytest',\n",
       "       'Error when encoding a dataset with None objects with a Sequence feature',\n",
       "       'v1.11.1 release date',\n",
       "       'Adding an Elastic Search index to a Dataset',\n",
       "       \"`load_dataset('docred')` results in a `NonMatchingChecksumError` \",\n",
       "       'In v1.4.1, all TIMIT train transcripts are \"Would such an act of refusal be useful?\"',\n",
       "       \"datasets.config.PYARROW_VERSION has no attribute 'major'\",\n",
       "       \"TypeError: 'NoneType' object is not callable\",\n",
       "       '\"counter\" dataset raises an error in normal mode, but not in streaming mode',\n",
       "       'Cannot download TOTTO dataset',\n",
       "       'Loading allenai/c4 in streaming mode does too many HEAD requests',\n",
       "       'Negative timezone',\n",
       "       'always requiring the username in the dataset name when there is one',\n",
       "       'OpenWebText: NonMatchingSplitsSizesError',\n",
       "       'prepare_module issue when loading from read-only fs',\n",
       "       'ArrowInvalid when mapping dataset with missing values',\n",
       "       'Add a Text Classification dataset: KanHope',\n",
       "       'The datasets.map function does not load cached dataset after moving python script',\n",
       "       'HF_DATASETS_CACHE variable in Windows',\n",
       "       'Cannot load linnaeus dataset',\n",
       "       'Downloading “reddit” dataset keeps timing out.',\n",
       "       'cannot load data from my loacal path',\n",
       "       'Add Mostly Basic Python Problems Dataset',\n",
       "       'Remove compression from xopen',\n",
       "       'Loading JSON throws ArrowNotImplementedError',\n",
       "       'How to sample every file in a list of files making up a split in a dataset when loading?',\n",
       "       \"ConnectionError: Couldn't reach https://raw.githubusercontent.com\",\n",
       "       \"`generate_random_fingerprint()` deterministic with 🤗Transformers' `set_seed()`\",\n",
       "       \"`ArrowInvalid: Added column's length must match table's length.` after using `select`\",\n",
       "       'equal operation to perform unbatch for huggingface datasets ',\n",
       "       'BERTScore Error', 'English wikipedia datasets is not clean',\n",
       "       'Add RVL-CDIP dataset', 'Error loading C4 realnewslike dataset',\n",
       "       'the meteor metric seems not consist with the official version',\n",
       "       'Unexpected type after `concatenate_datasets`',\n",
       "       'Second concatenation of datasets produces errors',\n",
       "       'Raise a proper exception when trying to stream a dataset that requires to manually download files',\n",
       "       'Cannot load `few-nerd` dataset', 'Dataset JSON is incorrect',\n",
       "       'Improve detection of streamable file types', 'SacreBLEU update',\n",
       "       'Add Microsoft Building Footprints dataset',\n",
       "       'Update CommonVoice with new release',\n",
       "       'Concurrent use of same dataset (already downloaded)',\n",
       "       'Error in loading the Arabic Billion Words Corpus',\n",
       "       '404 Error when loading remote data files from private repo',\n",
       "       'Missing cache file',\n",
       "       'Calling shuffle on IterableDataset will disable batching in case any functions were mapped',\n",
       "       'add more precise information for size',\n",
       "       'Missing documentation for wnut_17 (ner_tags)',\n",
       "       'QASC: incomplete training set ',\n",
       "       '404 Not Found Error when loading LAMA dataset',\n",
       "       '404 not found error on loading WIKIANN dataset',\n",
       "       'from datasets import Dataset is failing ',\n",
       "       'cannot combine splits merging and streaming?',\n",
       "       'Cannot import load_dataset on Colab',\n",
       "       'xtreme / pan-x cannot be downloaded',\n",
       "       'cannot save the dataset to disk after rename_column',\n",
       "       'hebrew language codes he and iw should be treated as aliases',\n",
       "       '5 duplicate datasets',\n",
       "       'Cannot load the blog_authorship_corpus due to codec errors',\n",
       "       'Import Error in Kaggle notebook', 'Error when downloading C4',\n",
       "       'Metric kwargs are not passed to underlying external metric f1_score',\n",
       "       '[`to_json`] add multi-proc sharding support',\n",
       "       'Allow the selection of multiple columns at once',\n",
       "       'Give a user feedback if the dataset he loads is streamable or not',\n",
       "       'Add SD task for SUPERB',\n",
       "       'Setting log level higher than warning does not suppress progress bar',\n",
       "       'Add web_split dataset for Paraphase and Rephrase benchmark',\n",
       "       'downloading of yahoo_answers_topics dataset failed',\n",
       "       'load_dataset processing failed with OS error after downloading a dataset',\n",
       "       'Batched `map` not allowed to return 0 items',\n",
       "       'Enum used in map functions will raise a RecursionError with dill.',\n",
       "       'Support multi-worker with streaming dataset (IterableDataset).',\n",
       "       'load_dataset(\"financial_phrasebank\") NonMatchingChecksumError',\n",
       "       'Progress bars are not properly rendered in Jupyter notebook',\n",
       "       'Load datasets from the Hub without requiring a dataset script',\n",
       "       \"can't set verbosity for `metric.py`\", 'Integration with AugLy',\n",
       "       '`filelock.py` Error', 'Jsonlines export error',\n",
       "       'Streaming local gzip compressed JSON line files is not working',\n",
       "       '[Metrics] addition of wiki_split metrics',\n",
       "       'Add option to delete temporary files (e.g. extracted files) when loading dataset',\n",
       "       'Unable to download omp dataset', 'Transformer Class on dataset',\n",
       "       \"ModuleNotFoundError: No module named 'datasets.tasks' while importing common voice datasets\",\n",
       "       'Cached dataset overflowing disk space',\n",
       "       'sqaud_v2 dataset contains misalignment between the answer text and the context value at the answer index',\n",
       "       'Error iteration over IterableDataset using Torch DataLoader',\n",
       "       'Finding right block-size with JSON loading difficult for user',\n",
       "       'Weights of model checkpoint not initialized for RobertaModel for Bertscore',\n",
       "       'Existing cache for local dataset builder file updates is ignored with `ignore_verifications=True`',\n",
       "       'Memory usage consistently increases when processing a dataset with `.map`',\n",
       "       'Multilabel metrics not supported',\n",
       "       'load_dataset(\"web_nlg\") NonMatchingChecksumError',\n",
       "       'Keys should be unique error on code_search_net',\n",
       "       'Handling unlabeled datasets', 'Field order issue in loading json',\n",
       "       'Dataset load_from_disk is too slow',\n",
       "       \"switching some low-level log.info's to log.debug?\",\n",
       "       '`datasets.keyhash.DuplicatedKeysError` for `drop` and `adversarial_qa/adversarialQA`',\n",
       "       'Loading partial dataset when debugging',\n",
       "       'Use `Audio` features for `AutomaticSpeechRecognition` task template',\n",
       "       \"Tokenizer's normalization preprocessor cause misalignment in return_offsets_mapping for tokenizer classification task\",\n",
       "       'Logging cannot be set to NOTSET similar to transformers',\n",
       "       'Documentation Mistakes in Dataset: emotion',\n",
       "       'datasets.map pickle issue resulting in invalid mapping function',\n",
       "       'Can datasets remove duplicated rows?',\n",
       "       'Corelation should be Correlation',\n",
       "       \"seqeval metric does not work with a recent version of sklearn: classification_report() got an unexpected keyword argument 'output_dict'\",\n",
       "       'Add C4', 'Load Image Classification Dataset from Local ',\n",
       "       'SubjQA wrong boolean values in entries',\n",
       "       ' Python Programming Puzzles',\n",
       "       'Improve torch formatting performance',\n",
       "       'Implement loading a dataset builder',\n",
       "       'Delete extracted files to save disk space',\n",
       "       'Set download/extracted paths configurable',\n",
       "       'Issue in timit_asr database',\n",
       "       'cache_dir parameter for load_from_disk ?',\n",
       "       'Fix automatic generation of Zenodo DOI',\n",
       "       'Crash when `num_proc` > dataset length for `map()` on a `datasets.Dataset`.',\n",
       "       '`Proto_qa` hosting seems to be broken',\n",
       "       'Revert default in-memory for small datasets',\n",
       "       'MRPC test set differences between torch and tensorflow datasets',\n",
       "       'BLUE file not found',\n",
       "       'dataset adversarial_qa has no answers in the \"test\" set',\n",
       "       '`yelp_polarity` is broken',\n",
       "       'Sentence Boundaries missing in Dataset: xtreme / udpos',\n",
       "       'Some tests hang on Windows',\n",
       "       'DuplicatedKeysError on personal dataset',\n",
       "       'Remove `extended` field from dataset tagger',\n",
       "       'Extend QuestionAnsweringExtractive template to handle nested columns',\n",
       "       'DuplicatedKeysError when trying to load adversarial_qa',\n",
       "       'Saving Graph/Structured Data in Datasets',\n",
       "       'load_from_disk and save_to_disk are not compatible with each other',\n",
       "       'Cached dataset not loaded',\n",
       "       \"AttributeError: 'DatasetInfo' object has no attribute 'task_templates'\",\n",
       "       'Docstring mistake: dataset vs. metric',\n",
       "       \".map() function got an unexpected keyword argument 'cache_file_name'\",\n",
       "       'load_dataset(\\'natural_questions\\') fails with \"ValueError: External features info don\\'t match the dataset\"',\n",
       "       'Concatenate several datasets with removed columns is not working.',\n",
       "       'strange datasets from OSCAR corpus',\n",
       "       'Missing original answers in kilt-TriviaQA',\n",
       "       'datasets 1.6 ignores cache',\n",
       "       'Accessing Arrow dataset cache_files',\n",
       "       'ArrowDataset.save_to_disk produces files that cannot be read using pyarrow.feather',\n",
       "       'Loading dataset from local path',\n",
       "       'Trying to use metric.compute but get OSError',\n",
       "       'How to Add New Metrics Guide',\n",
       "       '`FaissIndex.save` throws error on GPU',\n",
       "       'Add an API to access the language and pretty name of a dataset',\n",
       "       '[Question] How to move and reuse preprocessed dataset? ',\n",
       "       'Is there a way to join multiple datasets in one?',\n",
       "       'NonMatchingChecksumError for web_of_science dataset',\n",
       "       'Allow passing `desc` to `tqdm` in `Dataset.map()`',\n",
       "       'A syntax error in example',\n",
       "       'load_dataset(\"timit_asr\") gives back duplicates of just one sample text',\n",
       "       'Calls to map are not cached.',\n",
       "       'UnicodeDecodeError for OSCAR (Afrikaans)',\n",
       "       '[api request] API to obtain \"dataset_module\" dynamic path?',\n",
       "       'Incorrect version specification for pyarrow',\n",
       "       'Add COCO evaluation metrics',\n",
       "       'Unable to setup dev env on Windows', 'Add VoxPopuli',\n",
       "       'Slow #0 when using map to tokenize.',\n",
       "       'Load_dataset for local CSV files',\n",
       "       'Help understanding how to build a dataset for language modeling as with the old TextDataset',\n",
       "       'Compatibility with Ubuntu 18 and GLIBC 2.27?',\n",
       "       'Loss result inGptNeoForCasual',\n",
       "       'concatenate_datasets loads all the data into memory',\n",
       "       'SNLI dataset has labels of -1 ',\n",
       "       'Bug in Dataset.class_encode_column',\n",
       "       'Synchronize table metadata with features',\n",
       "       'DatasetDict save load Failing test in 1.6 not in 1.5',\n",
       "       'NewsPH NLI dataset script fails to access test data.',\n",
       "       'Running `datase.map` with `num_proc > 1` uses a lot of memory',\n",
       "       'Slow dataloading with big datasets issue persists',\n",
       "       'some issue in loading local txt file as Dataset for run_mlm.py',\n",
       "       'Map is slow and processes batches one after another',\n",
       "       'Link to datasets viwer on Quick Tour page returns \"502 Bad Gateway\"',\n",
       "       'Error loading wikihow dataset',\n",
       "       'Update Dataset.dataset_size after transformed with map',\n",
       "       'Keys yielded while generating dataset are not being checked',\n",
       "       '`xnli` dataset creating a tuple key while yielding instead of `str` or `int`',\n",
       "       'Batched map fails when removing all columns',\n",
       "       'Duplicates in the LAMA dataset',\n",
       "       \"load_metric error: module 'datasets.utils.file_utils' has no attribute 'add_start_docstrings'\",\n",
       "       'Can\\'t reach \"https://storage.googleapis.com/illuin/fquad/train.json.zip\" when trying to load fquad dataset',\n",
       "       'Getting checksum error when trying to load lc_quad dataset',\n",
       "       'dataloading slow when using HUGE dataset',\n",
       "       'making labels consistent across the datasets',\n",
       "       'Got pyarrow error when loading a dataset while adding special tokens into the tokenizer',\n",
       "       '_prepare_split will overwrite DatasetBuilder.info.features',\n",
       "       '`load_dataset` caches two arrow files?',\n",
       "       \"KeyError: '_indices_files' in `arrow_dataset.py`\",\n",
       "       \"py3.7: TypeError: can't pickle _LazyModule objects\",\n",
       "       'Filtering/mapping on one column is very slow',\n",
       "       'News_commentary Dataset Translation Pairs are of Incorrect Language Specified Pairs',\n",
       "       \"save_to_disk doesn't work when we use concatenate_datasets function before creating the final dataset_object.\",\n",
       "       'Duplicate data in Timit dataset',\n",
       "       'Question (potential issue?) related to datasets caching',\n",
       "       '.map() and distributed training',\n",
       "       'Error when loading a HUGE json file (pyarrow.lib.ArrowInvalid: straddling object straddles two block boundaries)',\n",
       "       'Converting a Value to a ClassLabel',\n",
       "       'dataset.search_batch() function outputs all -1 indices sometime.',\n",
       "       'Wikipedia historic dumps are deleted but hf/datasets hardcodes dump date',\n",
       "       'Regarding Test Sets for the GEM datasets',\n",
       "       'How to convert datasets.arrow_dataset.Dataset to torch.utils.data.Dataset',\n",
       "       'visualization for cc100 is broken ',\n",
       "       'any possibility to download part of large datasets only?',\n",
       "       'data_args.preprocessing_num_workers almost freezes ',\n",
       "       'adding ccnet dataset', 'viewer \"fake_news_english\" error',\n",
       "       'load_dataset ignoring features',\n",
       "       'Telugu subset missing for xtreme tatoeba dataset',\n",
       "       'Add configurable options to `seqeval` metric',\n",
       "       'Dataset file size on disk is very large with 3D Array',\n",
       "       'Loading wikipedia 20200501.en throws pyarrow related error',\n",
       "       'TypeError when using save_to_disk in a dataset loaded with ReadInstruction split',\n",
       "       'en language data from MLQA dataset is missing',\n",
       "       'Saving large in-memory datasets with save_to_disk crashes because of pickling',\n",
       "       'bug in mlqa dataset ',\n",
       "       'TydiQA dataset is mixed and is not split per language ',\n",
       "       \"When training with Multi-Node Multi-GPU the worker 2 has TypeError: 'NoneType' object\",\n",
       "       'wikiann dataset is missing columns ',\n",
       "       'How to train BERT model with next sentence prediction?',\n",
       "       'Dialogue action slot name and value are reversed in MultiWoZ 2.2',\n",
       "       'Is dataset timit_asr broken?', 'Adding ScaNN library to do MIPS?',\n",
       "       'Problem downloading GEM wiki_auto_asset_turk dataset',\n",
       "       'dataset viewer does not work anymore ',\n",
       "       'load_metric from local \"glue.py\" meet error \\'NoneType\\' object is not callable',\n",
       "       'Creating custom dataset results in error while calling the map() function',\n",
       "       'WMT19 Dataset for Kazakh-English is not formatted correctly',\n",
       "       'Request to remove S2ORC dataset', 'Trouble loading wiki_movies',\n",
       "       'citation, homepage, and license fields of `dataset_info.json` are duplicated many times',\n",
       "       'load_from_disk takes a long time to load local dataset',\n",
       "       'SQuAD version ',\n",
       "       'How to disable making arrow tables in load_dataset ?',\n",
       "       'Add documentaton for dataset README.md files',\n",
       "       'CUAD - Contract Understanding Atticus Dataset',\n",
       "       '`concatenate_datasets` throws error when changing the order of datasets to concatenate',\n",
       "       'Multidimensional arrays in a Dataset',\n",
       "       'MemoryError when computing WER metric',\n",
       "       'Issue: Dataset download error',\n",
       "       \"ConnectionError: Couldn't reach common_voice.py\",\n",
       "       'Multiprocessing is slower than single process',\n",
       "       'ArrowInvalid issue for squad v2 dataset',\n",
       "       'PyTorch not available error on SageMaker GPU docker though it is installed ',\n",
       "       'Multiprocessing windows error',\n",
       "       'Only user permission of saved cache files, not group',\n",
       "       'Cannot load udpos subsets from xtreme dataset using load_dataset()',\n",
       "       'Error while following docs to load the `ted_talks_iwslt` dataset',\n",
       "       'issue with opus100/en-fr dataset ',\n",
       "       'is there a way to override a dataset object saved with save_to_disk?',\n",
       "       'Could not find file for ZEST dataset',\n",
       "       'Timit_asr dataset repeats examples',\n",
       "       'Build custom dataset to fine-tune Wav2Vec2',\n",
       "       'add_faisis_index  gets very slow when doing it interatively  ',\n",
       "       \"ValueError: datasets' indices [1] come from memory and datasets' indices [0] come from disk\",\n",
       "       'outdated dataset_infos.json might fail verifications',\n",
       "       'Cannot load wikitext',\n",
       "       'wiki40b/wikipedia for almost all languages cannot be downloaded',\n",
       "       \"wikipedia.py generator that extracts XML doesn't release memory\",\n",
       "       'Loading a faiss index KeyError',\n",
       "       'KeyError on using map after renaming a column',\n",
       "       'ValueError when rename_column on splitted dataset',\n",
       "       'Interactively doing  save_to_disk and load_from_disk corrupts the datasets object?',\n",
       "       'No upstream branch', 'Local testing fails',\n",
       "       'Ambiguous documentation',\n",
       "       'How to not load huggingface datasets into memory ',\n",
       "       'Setting to torch format not working with torchvision and MNIST',\n",
       "       'Messages are being printed to the `stdout`',\n",
       "       'Windows Permission Error (most recent version of datasets)',\n",
       "       'Error when exploring `arabic_speech_corpus`',\n",
       "       'not being able to get wikipedia es language',\n",
       "       'How to load a dataset with load_from disk and save it again after doing transformations without changing the original? ',\n",
       "       '`datasets.map` multi processing much slower than single processing ',\n",
       "       'OSError: Memory mapping file failed: Cannot allocate memory',\n",
       "       'Question/problem with dataset labels',\n",
       "       'Readme.md is misleading about kinds of datasets?',\n",
       "       'wmt datasets fail to load',\n",
       "       'The size of CoNLL-2003 is not consistant with the official release.',\n",
       "       \"ModuleNotFoundError: No module named 'apache_beam' for wikipedia datasets \",\n",
       "       'Question: what gets stored in the datasets cache and why is it so huge?',\n",
       "       \"'Dataset' object has no attribute 'rename_column'\",\n",
       "       'Can we parallelized the add_faiss_index process over dataset shards ?',\n",
       "       'Datasets.py function load_dataset does not match squad dataset',\n",
       "       'bug in SNLI dataset ',\n",
       "       'Bug in skip_rows argument of load_dataset function ?',\n",
       "       'XSum dataset download link broken',\n",
       "       '[distributed env] potentially unsafe parallel execution',\n",
       "       'add a new column ', 'Enable Fast Filtering using Arrow Dataset',\n",
       "       'dataset loading logger level',\n",
       "       \"AttributeError: 'DatasetDict' object has no attribute 'concatenate_datasets'\",\n",
       "       '[experiment] missing default_experiment-1-0.arrow',\n",
       "       \"Loading of FAISS index fails for index_name = 'exact'\",\n",
       "       'Side effect when filtering data due to `does_function_return_dict` call in `Dataset.map()`',\n",
       "       '[firewalled env] OFFLINE mode',\n",
       "       'CommonGen dataset page shows an error OSError: [Errno 28] No space left on device',\n",
       "       'Add Stanford Sentiment Treebank (SST)',\n",
       "       'Anonymous Dataset Addition (i.e Anonymous PR?)',\n",
       "       'How to update the \"wino_bias\" dataset',\n",
       "       'Failure to save with save_to_disk',\n",
       "       'UnicodeDecodeError: windows 10 machine',\n",
       "       'Unable to download `wiki_dpr`',\n",
       "       'Saving processed dataset running infinitely',\n",
       "       'DBPedia14 Dataset Checksum bug?',\n",
       "       'Feature Request: Support for Pandas `Categorical`',\n",
       "       'ALT dataset has repeating instances in all splits',\n",
       "       'Bug Report: timestamp[ns] not recognized',\n",
       "       'benchmarking against MMapIndexedDataset', 'wmt19 is broken',\n",
       "       'request to mirror wmt datasets, as they are really slow to download',\n",
       "       'Allow concatenation of both in-memory and on-disk datasets',\n",
       "       ' load_dataset(\"multi_woz_v22\") NonMatchingChecksumError',\n",
       "       'Adding a new column to the dataset after set_format was called',\n",
       "       'ERROR WHEN USING SET_TRANSFORM() ', 'Add Winogender Schemas',\n",
       "       'Add WikiCREM',\n",
       "       'Error \"in void don\\'t know how to serialize this type of index\" when saving index to disk when device=0 (GPU)',\n",
       "       'Unable to upload \"community provided\" dataset - 400 Client Error',\n",
       "       'load_dataset(\"amazon_polarity\") NonMatchingChecksumError',\n",
       "       'Feature Request: Dataset.add_item', 'Add TIMIT',\n",
       "       'Update Open Subtitles corpus with original sentence IDs',\n",
       "       'MustC Speech Translation', 'Add common voice', 'Add tedlium',\n",
       "       'test.json has been removed from the limit dataset repo (breaks dataset)',\n",
       "       'Some question about raw dataset download info in the project .',\n",
       "       'using map on loaded Tokenizer 10x - 100x slower than default Tokenizer?',\n",
       "       'Regarding On-the-fly Data Loading',\n",
       "       'Datasets library not suitable for huge text datasets.',\n",
       "       'Provide better exception message when one of many files results in an exception',\n",
       "       'Loading local dataset raise requests.exceptions.ConnectTimeout',\n",
       "       'pyarrow.lib.ArrowInvalid: Column 1 named input_ids expected length 599 but got length 1500',\n",
       "       'Unable to add Multi-label Datasets', 'Add Hateful Memes Dataset',\n",
       "       'writing Datasets in a human readable format',\n",
       "       \"can't pickle SwigPyObject objects when calling dataset.get_nearest_examples from FAISS index\",\n",
       "       'Querying examples from big datasets is slower than small datasets',\n",
       "       'Connection error', 'Filter on dataset too much slowww',\n",
       "       \"ModuleNotFoundError: No module named 'apache_beam', when specific languages.\",\n",
       "       'How to use split dataset ',\n",
       "       'Not enough disk space (Needed: Unknown size) when caching on a cluster',\n",
       "       'JSONDecodeError on JSON with multiple lines',\n",
       "       'Dataset Examples Explorer',\n",
       "       \"AttributeError: module 'pyarrow' has no attribute 'PyExtensionType' during import \",\n",
       "       '[Question & Bug Report] Can we preprocess a dataset on the fly?',\n",
       "       'Efficient ways to iterate the dataset',\n",
       "       'is it possible to make slice to be more compatible like python list and numpy?',\n",
       "       'bug in loading datasets ',\n",
       "       \"Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/1.2.1/datasets/csv/csv.py\",\n",
       "       'how can I combine 2 dataset with different/same features?',\n",
       "       \"_pickle.PicklingError: Can't pickle typing.Union[str, NoneType]: it's not the same object as typing.Union when calling datasets.map with num_proc=2\",\n",
       "       'Issues when run two programs compute the same metrics',\n",
       "       'Error iterating over Dataset with DataLoader',\n",
       "       'Connection Issues', 'Unable to format dataset to CUDA Tensors',\n",
       "       'wikipedia dataset incomplete',\n",
       "       'dataset.search() (elastic) cannot reliably retrieve search results',\n",
       "       'FewRel',\n",
       "       'Using select/reordering datasets slows operations down immensely',\n",
       "       'datasets slicing with seed ',\n",
       "       'difference between wsc and wsc.fixed for superglue',\n",
       "       'Issue while Creating Custom Metric',\n",
       "       'error when run fine_tuning on text_classification',\n",
       "       'connection issue with glue, what is the data url for glue? ',\n",
       "       \"Couldn't reach swda.py\",\n",
       "       'Is there support for Deep learning datasets?',\n",
       "       'Add an entry to an arrow dataset',\n",
       "       'BLEURT score calculation raises UnrecognizedFlagError',\n",
       "       'load the local dataset',\n",
       "       'could not run models on a offline server successfully',\n",
       "       'Possible cache miss in datasets',\n",
       "       'SciFact dataset - minor changes', 'Installation using conda',\n",
       "       'IsADirectoryError when trying to download C4',\n",
       "       'Error when downloading a large dataset on slow connection.',\n",
       "       'Some datasets miss dataset_infos.json or dummy_data.zip',\n",
       "       'Unable to install datasets',\n",
       "       \"Question: Shouldn't .info be a part of DatasetDict?\",\n",
       "       'Dataset Error: DaNE contains empty samples at the end',\n",
       "       '`ArrowInvalid` occurs while running `Dataset.map()` function for DPRContext',\n",
       "       'Dataset \"dane\" missing', \"Can't import cc100 dataset\",\n",
       "       'Add the 800GB Pile dataset?', \"dutch_social can't be loaded\",\n",
       "       'Unable to Download Hindi Wikipedia Dataset',\n",
       "       'load_dataset hang on file_lock', 'connection issue ',\n",
       "       'wiki_dpr pre-processing performance',\n",
       "       'wiki_dpr dataset pre-processesing performance',\n",
       "       'Arrow file is too large when saving vector data',\n",
       "       'NarrativeQA fails to load with `load_dataset`',\n",
       "       'HoVeR dataset fails to load', 'Dataset social_bias_frames 404',\n",
       "       'muchocine dataset cannot be dowloaded', 'bug with sst2 in glue ',\n",
       "       'winogrande cannot be dowloaded ',\n",
       "       'Inspecting datasets per category',\n",
       "       'social_i_qa wrong format of labels',\n",
       "       'Adding UKP Argument Aspect Similarity Corpus',\n",
       "       '`Dataset.map` disable progress bar',\n",
       "       'Cannot download ade_corpus_v2',\n",
       "       \"Can't call shape on the output of select()\",\n",
       "       \"Can't filter language:EN on https://huggingface.co/datasets\",\n",
       "       \"Bug: Can't download TriviaQA with `load_dataset` - custom `cache_dir`\",\n",
       "       'shuffle with torch generator ', 'shuffle does not accept seed ',\n",
       "       \"Not able to use 'jigsaw_toxicity_pred' dataset\",\n",
       "       \"AttributeError: 'DatasetDict' object has no attribute 'train_test_split'\",\n",
       "       'connection error ', 'Access to key in DatasetDict map',\n",
       "       'Using datasets.Metric with Trainer()', 'IWSLT-17 Link Broken',\n",
       "       'Add helper to resolve namespace collision',\n",
       "       'FileNotFoundError for `amazon_polarity`',\n",
       "       \"Installing datasets and transformers in a tensorflow docker image throws Permission Error on 'import transformers'\",\n",
       "       'connection issue while downloading data',\n",
       "       'how to get all the options of a property in datasets ',\n",
       "       'Inconsistent argument names.',\n",
       "       'SNLI dataset contains labels with value -1',\n",
       "       \"FileNotFound remotly, can't load a dataset\",\n",
       "       \"Can't map dataset (loaded from csv)\",\n",
       "       '❓ Sharing ElasticSearch indexed dataset ',\n",
       "       'can\\'t load \"german_legal_entity_recognition\" dataset',\n",
       "       'imdb dataset cannot be downloaded',\n",
       "       \"'iwslt2017-ro-nl', cannot be downloaded \",\n",
       "       \"[libprotobuf FATAL /sentencepiece/src/../third_party/protobuf-lite/google/protobuf/repeated_field.h:1505] CHECK failed: (index) >= (0):  terminate called after throwing an instance of 'google::protobuf::FatalException'   what():  CHECK failed: (index) >= (0):  Aborted\",\n",
       "       'boolq does not work ',\n",
       "       '❓ On-the-fly tokenization with datasets, tokenizers, and torch Datasets and Dataloaders',\n",
       "       'Incorrect URL for MRQA SQuAD train subset',\n",
       "       'Using a feature named \"_type\" fails with certain operations',\n",
       "       'Add support to download kaggle datasets',\n",
       "       'Not support links with 302 redirect ',\n",
       "       'Dataset.map() turns tensors into lists?',\n",
       "       'how large datasets are handled under the hood ',\n",
       "       'NotADirectoryError while loading the CNN/Dailymail dataset',\n",
       "       'Problem downloading amazon_reviews_multi',\n",
       "       'making sure datasets are not loaded in memory and distributed training of them',\n",
       "       'sample multiple datasets ',\n",
       "       'Local machine/cluster Beam Datasets example/tutorial',\n",
       "       'wrong length with datasets ',\n",
       "       'Shall we change the hashing to encoding to reduce potential replicated cache files?',\n",
       "       'datasets module not found',\n",
       "       'datasets.load_dataset() custom chaching directory bug',\n",
       "       'Dataset viewer issues', 'Nested lists are zipped unexpectedly',\n",
       "       'pyarrow.lib.ArrowNotImplementedError: MakeBuilder: cannot construct builder for type extension<arrow.py_extension_type>',\n",
       "       'Very slow cold-start',\n",
       "       \"Downloading/caching only a part of a datasets' dataset.\",\n",
       "       'Add SQA', 'boolq does not load ',\n",
       "       'Loading Data From S3 Path in Sagemaker',\n",
       "       'DataLoader(datasets) become more and more slowly within iterations',\n",
       "       'imdb dataset cannot be loaded ', 'bug in boolq dataset loading',\n",
       "       'trec dataset unavailable ',\n",
       "       \"load_dataset('cnn_dalymail', '3.0.0') gives a 'Not a directory' error\",\n",
       "       \"terminate called after throwing an instance of 'google::protobuf::FatalException'\",\n",
       "       '[Feature Request] Add optional parameter in text loading script to preserve linebreaks',\n",
       "       'OSCAR from Inria group', 'Have Trouble importing `datasets`',\n",
       "       'Unable to download cnn_dailymail dataset',\n",
       "       'Possible Bug: Small training/dataset file creates gigantic output',\n",
       "       'wmt16 does not download ',\n",
       "       'concatenate_datasets support axis=0 or 1 ？',\n",
       "       'Add support for other languages for rouge', 'Load amazon dataset',\n",
       "       'Error when concatenate_datasets',\n",
       "       'multiprocessing in dataset map \"can only test a child process\"',\n",
       "       'Add HoVer multi-hop fact verification dataset',\n",
       "       'use_custom_baseline still produces errors for bertscore',\n",
       "       'How to enable `.map()` pre-processing pipelines to support multi-node parallelism?',\n",
       "       'Can not reuse datasets already downloaded',\n",
       "       \"load_dataset with 'csv' is not working. while the same file is loading with 'text' mode or with pandas\",\n",
       "       'Wikipedia postprocessing',\n",
       "       '[GEM] add WikiLingua cross-lingual abstractive summarization dataset',\n",
       "       '[GEM] add ToTTo Table-to-text dataset',\n",
       "       '[GEM] MultiWOZ dialogue dataset',\n",
       "       'Discussion using datasets in offline mode',\n",
       "       'how processing in batch works in datasets ', 'datasets freezes ',\n",
       "       'Add MRQA dataset',\n",
       "       '[Caching] Dill globalvars() output order is not deterministic and can cause cache issues.',\n",
       "       'Is dataset iterative or not?', 'Joining multiple datasets ',\n",
       "       'How to implement DistributedSampler with datasets ',\n",
       "       'Too much logging ', 'nlp viewer error',\n",
       "       'Add Google Taskmaster dataset',\n",
       "       'load_dataset for LOCAL CSV files report CONNECTION ERROR',\n",
       "       'Quail dataset urls are out of date',\n",
       "       'On loading a metric from datasets, I get the following error',\n",
       "       \"Empty output/answer in TriviaQA test set (both in 'kilt_tasks' and 'trivia_qa')\",\n",
       "       'How to join two datasets?',\n",
       "       'Cannot load TREC dataset: ConnectionError',\n",
       "       \"Token classification labels are strings and we don't have the list of labels\",\n",
       "       'Seq2Seq Metrics QOL: Bleu, Rouge',\n",
       "       'Descriptions of raw and processed versions of wikitext are inverted',\n",
       "       'self.options cannot be converted to a Python object for pickling',\n",
       "       'KILT dataset: empty string in triviaqa input field',\n",
       "       'Error running pip install -e \".[dev]\" on MacOS 10.13.6: faiss/python does not exist',\n",
       "       'feat(dataset): multiprocessing _generate_examples',\n",
       "       'Issue with downloading Wikipedia data for low resource language',\n",
       "       'Unexpected behavior when loading cached csv file?',\n",
       "       'Adding CC-100: Monolingual Datasets from Web Crawl Data',\n",
       "       'Using `Dataset.map` with `n_proc>1` print multiple progress bars',\n",
       "       'How to choose proper download_mode in function load_dataset?',\n",
       "       'Add a `lazy_map` method to `Dataset` and `DatasetDict`',\n",
       "       'Add option for named splits when using ds.train_test_split',\n",
       "       '[GEM] add DART data-to-text generation dataset',\n",
       "       'Downloaded datasets are not usable offline',\n",
       "       '(Load dataset failure) ConnectionError: Couldn’t reach https://raw.githubusercontent.com/huggingface/datasets/1.1.2/datasets/cnn_dailymail/cnn_dailymail.py',\n",
       "       'Process 0 very slow when using num_procs with map to tokenizer',\n",
       "       'CUDA out of memory',\n",
       "       'Clicking on a metric in the search page points to datasets page giving \"Missing dataset\" warning',\n",
       "       'Error loading ms_marco v2.1 using load_dataset()',\n",
       "       '[XGLUE] Adding new dataset',\n",
       "       \"Dataset Explorer Doesn't Work for squad_es and squad_it\",\n",
       "       'load_dataset for CSV files not working',\n",
       "       'Creating dataset consumes too much memory',\n",
       "       'Trec Dataset Connection Error',\n",
       "       'Throw error when an unexpected key is used in data_files',\n",
       "       'Possible caching bug',\n",
       "       '\"Checksums didn\\'t match for dataset source files\" error while loading openwebtext dataset',\n",
       "       'need to redirect /nlp to /datasets and remove outdated info',\n",
       "       'Adding pseudo-labels to datasets',\n",
       "       'feat(dl_manager): add support for ftp downloads',\n",
       "       'OSError: Cannot find data file when not using the dummy dataset in RAG',\n",
       "       'Error in the notebooks/Overview.ipynb notebook',\n",
       "       'How to use similarity settings  other then \"BM25\" in Elasticsearch index ?',\n",
       "       'Datasets performance slow? - 6.4x slower than in memory dataset',\n",
       "       'Requirements should specify pyarrow<1',\n",
       "       \"TypeError: '<' not supported between instances of 'NamedSplit' and 'NamedSplit'\",\n",
       "       'XNLI dataset is not loading ',\n",
       "       'XNLI dataset: NonMatchingChecksumError',\n",
       "       '`ArrowInvalid` occurs while running `Dataset.map()` function',\n",
       "       'Dataset browser url is still https://huggingface.co/nlp/viewer/',\n",
       "       'The download instructions for c4 datasets are not contained in the error message',\n",
       "       'train_test_split returns empty dataset item',\n",
       "       'Add custom dataset to NLP?',\n",
       "       \"load_dataset() won't download in Windows\",\n",
       "       'blog_authorship_corpus crashed', 'Questions about XSUM ',\n",
       "       'How to skip a example when running dataset.map',\n",
       "       'Loss not decrease with Datasets and Transformers',\n",
       "       \"Does both 'bookcorpus' and 'wikipedia' belong to the same datasets which Google used for pretraining BERT?\",\n",
       "       \"runing dataset.map, it raises TypeError: can't pickle Tokenizer objects\",\n",
       "       \"load_dataset from local squad.py, raise error: TypeError: 'NoneType' object is not callable \",\n",
       "       'Squad Metric Description & Feature Mismatch',\n",
       "       'Problem with JSON dataset format',\n",
       "       \"dummy data testing can't test datasets using `dl_manager.extract` in `_split_generators`\",\n",
       "       'Inconsistent behavior in map',\n",
       "       'offset overflow when multiprocessing batched map on large datasets.',\n",
       "       'Cannot download dataset_info.json',\n",
       "       'Caching processed dataset at wrong folder',\n",
       "       'GLUE/QQP dataset: NonMatchingChecksumError',\n",
       "       'Load large text file for LM pre-training resulting in OOM',\n",
       "       'Text dataset not working with large files',\n",
       "       'straddling object straddles two block boundaries',\n",
       "       'dtype of tensors should be preserved',\n",
       "       'Custom feature types in `load_dataset` from CSV',\n",
       "       'load_dataset for text files not working',\n",
       "       'map/filter multiprocessing raises errors and corrupts datasets',\n",
       "       'Mistakes in MLQA features names',\n",
       "       'Compare different Rouge implementations  ',\n",
       "       'UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors',\n",
       "       'Offset overflow when slicing a big dataset with an array of indices in Pyarrow >= 1.0.0',\n",
       "       'ArrowCapacityError: List array cannot contain more than 2147483646 child elements, have 2147483648',\n",
       "       'Load text file for RoBERTa pre-training. ',\n",
       "       \"Don't use the old NYU GLUE dataset URLs\",\n",
       "       'Pickling error when loading dataset',\n",
       "       'The current version of the package on github has an error when loading dataset',\n",
       "       'Indices incorrect with multiprocessing',\n",
       "       \"`Dataset`/`DatasetDict` has no attribute 'save_to_disk'\",\n",
       "       'The process cannot access the file because it is being used by another process (windows)',\n",
       "       'nlp re-creates already-there caches when using a script, but not within a shell',\n",
       "       'Some languages in wikipedia dataset are not loading',\n",
       "       \"Couldn't reach certain URLs and for the ones that can be reached, code just blocks after downloading.\",\n",
       "       '`metric.compute` throws `ArrowInvalid` error',\n",
       "       \"No module named 'nlp.logging'\",\n",
       "       'Using custom DownloadConfig results in an error',\n",
       "       'nlp downloads to its module path',\n",
       "       'Very slow data loading on large dataset',\n",
       "       'New release coming up for this library',\n",
       "       'nlp.load_dataset is not safe for multi processes when loading from local files',\n",
       "       '[Dataset] `NonMatchingChecksumError` due to an update in the LinCE benchmark data',\n",
       "       '[Dataset] RACE dataset Checksums error',\n",
       "       '`list_datasets()` is broken.',\n",
       "       'File exists error when used with TPU',\n",
       "       'wmt download speed example',\n",
       "       'Some docs are missing parameter names',\n",
       "       'dictionnary typo in docs',\n",
       "       '[BUG] Metrics throwing new error on master since 0.4.0',\n",
       "       'add MLDoc dataset',\n",
       "       'dataset.shuffle(keep_in_memory=True) is never allowed',\n",
       "       'dataset.shuffle() and select() resets format. Intended?',\n",
       "       'Version of numpy to use the library',\n",
       "       'Converting TensorFlow dataset example',\n",
       "       'TypeError: Receiver() takes no arguments', 'Errors when I use ',\n",
       "       \"Caching doesn't work for map (non-deterministic)\",\n",
       "       'nlp.Features does not distinguish between nullable and non-nullable types in PyArrow schema',\n",
       "       'No 0.4.0 release on GitHub', 'ug',\n",
       "       'issues with downloading datasets for wmt16 and wmt19',\n",
       "       'Bookcorpus data contains pretokenized text',\n",
       "       'rotten tomatoes movie review dataset taken down',\n",
       "       'Bugs : dataset.map() is frozen on ELI5',\n",
       "       'Export TFRecord to GCP bucket',\n",
       "       'Overview.ipynb throws exceptions with nlp 0.4.0',\n",
       "       'test_load_real_dataset when config has BUILDER_CONFIGS that matter',\n",
       "       \"invalid data type 'str' at _convert_outputs in arrow_dataset.py\",\n",
       "       'UnicodeDecodeError while loading PAN-X task of XTREME dataset',\n",
       "       'DEFAULT_TOKENIZER import error in sacrebleu',\n",
       "       'Keep loading old file even I specify a new file in load_dataset',\n",
       "       'Cannot unpickle saved .pt dataset with torch.save()/load()',\n",
       "       'Issues: Adding a FAISS or Elastic Search index to a Dataset',\n",
       "       'New Datasets: IWSLT15+, ITTB',\n",
       "       'Google Colab - load_dataset - PyArrow exception',\n",
       "       'ImportWarning for pyarrow 1.0.0',\n",
       "       'How to reuse functionality of a (generic) dataset?',\n",
       "       '[FEATURE REQUEST] Multiprocessing with for dataset.map, dataset.filter',\n",
       "       'Correct data structure for PAN-X task in XTREME dataset?',\n",
       "       'Addition of google drive links to dl_manager',\n",
       "       'from_dict delete?', 'Is there a way to download only NQ dev?',\n",
       "       'Unable to load XTREME dataset from disk',\n",
       "       \"train_test_split error: 'dict' object has no attribute 'deepcopy'\",\n",
       "       'MissingBeamOptions for Wikipedia 20200501.en',\n",
       "       'Faster Shuffling?',\n",
       "       '🐛 [Dataset] Cannot download wmt14, wmt15 and wmt17',\n",
       "       'Conversion through to_pandas output numpy arrays for lists instead of python objects',\n",
       "       '[dataset] Structure of MLQA seems unecessary nested',\n",
       "       \"to_pandas conversion doesn't always work\",\n",
       "       'TypeError when computing bertscore',\n",
       "       'Segmentation fault when loading local JSON dataset as of #372',\n",
       "       \"can't load local dataset: pyarrow.lib.ArrowInvalid: straddling object straddles two block boundaries\",\n",
       "       \"load_metric can't acquire lock anymore\", 'How to augment data ?',\n",
       "       '[dateset subset missing]  xtreme paws-x',\n",
       "       '🐛 [Metrics] ROUGE is non-deterministic',\n",
       "       '[Feature request] Add dataset.ragged_map() function for many-to-many transformations',\n",
       "       'ArrowBasedBuilder _prepare_split parse_schema breaks on nested structures',\n",
       "       \"can't load SNLI dataset\",\n",
       "       '[Dataset requests] New datasets for Text Classification',\n",
       "       \"'cp950' codec error from load_dataset('xtreme', 'tydiqa')\",\n",
       "       'Supporting documents in ELI5',\n",
       "       'Features should be updated when `map()` changes schema',\n",
       "       'Loading CNN/Daily Mail dataset produces `nlp.utils.info_utils.NonMatchingSplitsSizesError`',\n",
       "       '[Bug] FileLock dependency incompatible with filesystem',\n",
       "       'Fork dataset', 'Large dataset in Squad2-format',\n",
       "       'Error when calculating glue score', 'ERROR:root:mwparserfromhell',\n",
       "       'Blog Authorship Corpus, Non Matching Splits Sizes Error, nlp viewer',\n",
       "       'Nested sequences with dicts',\n",
       "       'Adding a dataset with multiple subtasks',\n",
       "       '[Question] Best way to batch a large dataset?',\n",
       "       '[Feature request] Add `shard()` method to dataset',\n",
       "       'Question - Sign Language Datasets',\n",
       "       'Setting cache_dir gives error on wikipedia download',\n",
       "       'Error in Demo for Specific Datasets', 'snli -1 labels',\n",
       "       'Cannot load arxiv dataset on MacOS?',\n",
       "       'ConnectionError - Eli5 dataset download',\n",
       "       \"Error at the first example in README: AttributeError: module 'dill' has no attribute '_dill'\",\n",
       "       'Private/sensitive data',\n",
       "       'Dataset Preprocessing Cache with .map() function not working as expected',\n",
       "       'MemoryError when loading German Wikipedia',\n",
       "       'Empty samples in glue/qqp',\n",
       "       'NonMatchingChecksumError when loading pubmed dataset', 'PG-19',\n",
       "       'c4 dataset is not viewable in nlpviewer demo',\n",
       "       'How can I load/find WMT en-romanian?',\n",
       "       '[Feature request] Support for external modality for language datasets',\n",
       "       'Downloading dataset error with pyarrow.lib.RecordBatch',\n",
       "       'documentation missing how to split a dataset',\n",
       "       'Why is dataset after tokenization far more larger than the orginal one ?',\n",
       "       'Tokenizer pickling issue fix not landed in `nlp` yet?',\n",
       "       '[Feature request] Add a feature to dataset',\n",
       "       '[Feature request] Be able to remove a specific sample of the dataset',\n",
       "       'NonMatchingSplitsSizesError error when reading the IMDB dataset',\n",
       "       '[Dataset created] some critical small issues when I was creating a dataset',\n",
       "       'What is the best way to cache a dataset? ',\n",
       "       'SST-2 test labels are all -1',\n",
       "       'UnicodeDecodeError when downloading GLUE-MNLI',\n",
       "       'Deterministic dataset loading',\n",
       "       '[Creating new dataset] Not found dataset_info.json',\n",
       "       '[Metric] Bertscore : Warning : Empty candidate sentence; Setting recall to be 0.',\n",
       "       \"Can't download MultiNLI\",\n",
       "       'Huggingface NLP, Uploading custom dataset',\n",
       "       'Fail to download c4 english corpus',\n",
       "       'Not able to access the XNLI dataset',\n",
       "       'Should we still have to force to install apache_beam to download wikipedia ?',\n",
       "       '[ROUGE] Different scores with `files2rouge`',\n",
       "       '[Feature Request/Help] BLEURT model -> PyTorch',\n",
       "       '[Feature request] Add FLUE dataset ',\n",
       "       'Colab Notebook breaks when downloading the squad dataset',\n",
       "       'Multi-task dataset mixing',\n",
       "       '❓ How to get ROUGE-2 with the ROUGE metric ?',\n",
       "       'NonMatchingSplitsSizesError when loading blog_authorship_corpus',\n",
       "       '[Arrow writer, Trivia_qa] Could not convert TagMe with type str: converting to null type',\n",
       "       'Remove test set from NLP viewer',\n",
       "       '[Question] Combine 2 datasets which have the same columns',\n",
       "       'Mistaken `_KWARGS_DESCRIPTION` for XNLI metric',\n",
       "       'Index outside of table length',\n",
       "       'Scientific Papers only downloading Pubmed',\n",
       "       '[Tensorflow] Use something else than `from_tensor_slices()`',\n",
       "       '[Question] Create Apache Arrow dataset from raw text file',\n",
       "       '[Question] BERT-style multiple choice formatting',\n",
       "       'When will the remaining math_dataset modules be added as dataset objects',\n",
       "       '[Question] How to load wikipedia ? Beam runner ?',\n",
       "       'Weird-ish: Not creating unique caches for different phases',\n",
       "       '[Bug] labels of glue/ax are all -1 ',\n",
       "       'Cannot upload my own dataset',\n",
       "       '[Feature request] separate split name and split instructions',\n",
       "       'Clone not working on Windows environment',\n",
       "       \"Loading 'wikitext' dataset fails\",\n",
       "       'Add a method to shuffle a dataset',\n",
       "       'Add Spanish POR and NER Datasets',\n",
       "       '[Feature request] Add cos-e v1.0',\n",
       "       'Discussion on version identifier & MockDataLoaderManager for test data',\n",
       "       'caching in map causes same result to be returned for train, validation and test',\n",
       "       'How can we add more datasets to nlp library?',\n",
       "       'nlp.load_dataset() gives \"TypeError: list_() takes exactly one argument (2 given)\"',\n",
       "       'SyntaxError with WMT datasets',\n",
       "       'Meta-datasets (GLUE/XTREME/...) – Special care to attributions and citations',\n",
       "       '[Feature request] Add Ubuntu Dialogue Corpus dataset',\n",
       "       \"_download_and_prepare() got an unexpected keyword argument 'verify_infos'\",\n",
       "       'Error with sklearn train_test_split',\n",
       "       'ArrowTypeError in squad metrics', 'Consider renaming to nld',\n",
       "       'Tokenized BLEU considered harmful - Discussion on community-based process',\n",
       "       '[Question] Using/adding a local dataset',\n",
       "       '[Feature Request] Add the OpenWebText dataset',\n",
       "       '[Feature request] Add Toronto BookCorpus dataset',\n",
       "       'Loading GLUE dataset loads CoLA by default',\n",
       "       '[Feature request] Add Google Natural Question dataset',\n",
       "       'Some error inside nlp.load_dataset()', '🐛 `map` not working',\n",
       "       \"🐛 Colab : type object 'pyarrow.lib.RecordBatch' has no attribute 'from_struct_array'\",\n",
       "       '❓ How to apply a map to all subsets ?',\n",
       "       '❓ How to remove specific rows of a dataset ?',\n",
       "       '🐛 Trying to use ROUGE metric : pyarrow.lib.ArrowInvalid: Column 1 named references expected length 534 but got length 323',\n",
       "       \"AttributeError: 'dict' object has no attribute 'info'\",\n",
       "       \"Couldn't reach CNN/DM dataset\",\n",
       "       '[Checksums] Error for some datasets',\n",
       "       'Error when citation is not given in the DatasetInfo',\n",
       "       'ValueError when a split is empty',\n",
       "       '[Feature] Keep the list of labels of a dataset as metadata',\n",
       "       '[Feature] More dataset outputs', 'Issue to read a local dataset'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df[\"title\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body'],\n",
       "    num_rows: 2964\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "comments_dataset = Dataset.from_pandas(comments_df)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2964/2964 [00:00<00:00, 51441.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.map(\n",
    "    lambda x: {\"comment_length\": len(x[\"comments\"].split())}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 2964/2964 [00:00<00:00, 256799.43 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n",
       "    num_rows: 2175\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_dataset = comments_dataset.filter(lambda x: x[\"comment_length\"] > 15)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2175/2175 [00:00<00:00, 26416.78 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text'],\n",
       "    num_rows: 2175\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def concatenate_text(examples):\n",
    "    return {\n",
    "        \"text\": examples[\"title\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"body\"]\n",
    "        + \" \\n \"\n",
    "        + examples[\"comments\"]\n",
    "    }\n",
    "\n",
    "\n",
    "comments_dataset = comments_dataset.map(concatenate_text)\n",
    "comments_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = get_embeddings(comments_dataset[\"text\"][0])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2175/2175 [05:32<00:00,  6.54 examples/s]  \n"
     ]
    }
   ],
   "source": [
    "embeddings_dataset = comments_dataset.map(\n",
    "    lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 398.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text', 'embeddings'],\n",
       "    num_rows: 2175\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dataset.add_faiss_index(column=\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How can I load a dataset offline?\"\n",
    "question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
    "question_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, samples = embeddings_dataset.get_nearest_examples(\n",
    "    \"embeddings\", question_embedding, k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "samples_df = pd.DataFrame.from_dict(samples)\n",
    "samples_df[\"scores\"] = scores\n",
    "samples_df.sort_values(\"scores\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.\n",
      "\n",
      "@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
      "SCORE: 25.50499725341797\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)\n",
      "You can now use them offline\n",
      "```python\n",
      "datasets = load_dataset('text', data_files=data_files)\n",
      "```\n",
      "\n",
      "We'll do a new release soon\n",
      "SCORE: 24.55550765991211\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.\n",
      "\n",
      "Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :) \n",
      "\n",
      "I already note the \"freeze\" modules option, to prevent local modules updates. It would be a cool feature.\n",
      "\n",
      "----------\n",
      "\n",
      "> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n",
      "\n",
      "Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.\n",
      "For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do\n",
      "```python\n",
      "load_dataset(\"./my_dataset\")\n",
      "```\n",
      "and the dataset script will generate your dataset once and for all.\n",
      "\n",
      "----------\n",
      "\n",
      "About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.\n",
      "cf #1724 \n",
      "SCORE: 24.14898109436035\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine\n",
      "> \n",
      "> 1. (online machine)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> import datasets\n",
      "> \n",
      "> data = datasets.load_dataset(...)\n",
      "> \n",
      "> data.save_to_disk(/YOUR/DATASET/DIR)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> 2. copy the dir from online to the offline machine\n",
      "> \n",
      "> 3. (offline machine)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> import datasets\n",
      "> \n",
      "> data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
      "> \n",
      "> ```\n",
      "> \n",
      "> \n",
      "> \n",
      "> HTH.\n",
      "\n",
      "\n",
      "SCORE: 22.89398765563965\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n",
      "COMMENT: here is my way to load a dataset offline, but it **requires** an online machine\n",
      "1. (online machine)\n",
      "```\n",
      "import datasets\n",
      "data = datasets.load_dataset(...)\n",
      "data.save_to_disk(/YOUR/DATASET/DIR)\n",
      "```\n",
      "2. copy the dir from online to the offline machine\n",
      "3. (offline machine)\n",
      "```\n",
      "import datasets\n",
      "data = datasets.load_from_disk(/SAVED/DATA/DIR)\n",
      "```\n",
      "\n",
      "HTH.\n",
      "SCORE: 22.406635284423828\n",
      "TITLE: Discussion using datasets in offline mode\n",
      "URL: https://github.com/huggingface/datasets/issues/824\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _, row in samples_df.iterrows():\n",
    "    print(f\"COMMENT: {row.comments}\")\n",
    "    print(f\"SCORE: {row.scores}\")\n",
    "    print(f\"TITLE: {row.title}\")\n",
    "    print(f\"URL: {row.html_url}\")\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION What is Elastic Search?\n",
      "COMMENT: Thanks for the clarification.\n",
      "\n",
      "I am not familiar with ElasticSearch, but if I understand well you're trying to migrate your data along with the ES index.\n",
      "My advice would be to check out ES documentation, for instance, this might help you: https://www.elastic.co/guide/en/cloud/current/ec-migrate-data.html\n",
      "\n",
      "Let me know if it helps\n",
      "SCORE: 47.348907470703125\n",
      "TITLE: ❓ Sharing ElasticSearch indexed dataset \n",
      "URL: https://github.com/huggingface/datasets/issues/1324\n",
      "\n",
      "COMMENT: Hi, is this bug deterministic in your poetry env ? I mean, does it always stop at 90% or is it random ?\n",
      "\n",
      "Also, can you try using another version of Elasticsearch ? Maybe there's an issue with the one of you poetry env\n",
      "SCORE: 46.14360046386719\n",
      "TITLE: Adding an Elastic Search index to a Dataset\n",
      "URL: https://github.com/huggingface/datasets/issues/2885\n",
      "\n",
      "COMMENT: Hi, is this bug deterministic in your poetry env ? I mean, does it always stop at 90% or is it random ?\n",
      "\n",
      "Also, can you try using another version of Elasticsearch ? Maybe there's an issue with the one of you poetry env\n",
      "SCORE: 46.14360046386719\n",
      "TITLE: Adding an Elastic Search index to a Dataset\n",
      "URL: https://github.com/huggingface/datasets/issues/2885\n",
      "\n",
      "COMMENT: Hi !\n",
      "I tried your code on my side and I was able to workaround this issue by waiting a few seconds before querying the index.\n",
      "Maybe this is because the index is not updated yet on the ElasticSearch side ?\n",
      "SCORE: 44.494300842285156\n",
      "TITLE: dataset.search() (elastic) cannot reliably retrieve search results\n",
      "URL: https://github.com/huggingface/datasets/issues/1758\n",
      "\n",
      "COMMENT: Thanks for the feedback! I added a 30 second \"sleep\" and that seemed to work well!\n",
      "SCORE: 44.494300842285156\n",
      "TITLE: dataset.search() (elastic) cannot reliably retrieve search results\n",
      "URL: https://github.com/huggingface/datasets/issues/1758\n",
      "\n",
      "==================================================\n",
      "QUESTION What to do when 'NoneType' object is not callable?\n",
      "COMMENT: One naive question: do you have internet access from the machine where you execute the code?\n",
      "SCORE: 28.16557502746582\n",
      "TITLE: TypeError: 'NoneType' object is not callable\n",
      "URL: https://github.com/huggingface/datasets/issues/2869\n",
      "\n",
      "COMMENT: For that environment, I am sorry but I can't reproduce the bug: I can load the dataset without any problem.\n",
      "SCORE: 28.029094696044922\n",
      "TITLE: TypeError: 'NoneType' object is not callable\n",
      "URL: https://github.com/huggingface/datasets/issues/2869\n",
      "\n",
      "COMMENT: For that environment, I am sorry but I can't reproduce the bug: I can load the dataset without any problem.\n",
      "SCORE: 28.029094696044922\n",
      "TITLE: TypeError: 'NoneType' object is not callable\n",
      "URL: https://github.com/huggingface/datasets/issues/2869\n",
      "\n",
      "COMMENT: - For the platform, we need to know the operating system of your machine. Could you please run the command `datasets-cli env` and copy-and-paste its output below?\n",
      "- In relation with the error, you just gave us the error type and message (`TypeError: 'NoneType' object is not callable`). Could you please copy-paste the complete stack trace, so that we know exactly which part of the code threw the error?\n",
      "SCORE: 26.181304931640625\n",
      "TITLE: TypeError: 'NoneType' object is not callable\n",
      "URL: https://github.com/huggingface/datasets/issues/2869\n",
      "\n",
      "COMMENT: - For the platform, we need to know the operating system of your machine. Could you please run the command `datasets-cli env` and copy-and-paste its output below?\n",
      "- In relation with the error, you just gave us the error type and message (`TypeError: 'NoneType' object is not callable`). Could you please copy-paste the complete stack trace, so that we know exactly which part of the code threw the error?\n",
      "SCORE: 26.181304931640625\n",
      "TITLE: TypeError: 'NoneType' object is not callable\n",
      "URL: https://github.com/huggingface/datasets/issues/2869\n",
      "\n",
      "==================================================\n",
      "QUESTION What does get_indices_from_mask_function() return?\n",
      "COMMENT: I fixed a bug that could cause this issue earlier today. Could you pull the latest version and try again ?\n",
      "SCORE: 50.528594970703125\n",
      "TITLE: Indices incorrect with multiprocessing\n",
      "URL: https://github.com/huggingface/datasets/issues/597\n",
      "\n",
      "COMMENT: Thanks for the report !\n",
      "\n",
      "Currently we don't have a way to let the user easily disable this behavior.\n",
      "However I agree that we should support stateful processing functions, ideally by removing `does_function_return_dict`.\n",
      "\n",
      "We needed this function in order to know whether the `map` functions needs to write data or not. if `does_function_return_dict` returns False then we don't write anything.\n",
      "\n",
      "Instead of checking the output of the processing function outside of the for loop that iterates through the dataset to process it, we can check the output of the first processed example and at that point decide if we need to write data or not.\n",
      "\n",
      "Therefore it's definitely possible to fix this unwanted behavior, any contribution going into this direction is welcome :)\n",
      "SCORE: 49.362308502197266\n",
      "TITLE: Side effect when filtering data due to `does_function_return_dict` call in `Dataset.map()`\n",
      "URL: https://github.com/huggingface/datasets/issues/1940\n",
      "\n",
      "COMMENT: This has been fixed in this commit: https://github.com/huggingface/datasets/pull/2254/commits/88676c930216cd4cc31741b99827b477d2b46cb6\n",
      "\n",
      "It was introduced in #2246 : using map with `input_columns` doesn't return the other columns anymore\n",
      "SCORE: 48.02884292602539\n",
      "TITLE: Bug in Dataset.class_encode_column\n",
      "URL: https://github.com/huggingface/datasets/issues/2272\n",
      "\n",
      "COMMENT: Still the case on master.\n",
      "I guess we should have an offset in the multi-procs indeed (hopefully it's enough).\n",
      "\n",
      "Also, side note is that we should add some logging before the \"test\" to say we are testing the function otherwise its confusing for the user to see two outputs I think. Proposal (see the \"Testing the mapped function outputs:\" lines):\n",
      "```\n",
      ">>> d.select(range(10)).map(fn, with_indices=True, batched=True, num_proc=2)\n",
      "Done writing 10 indices in 80 bytes .\n",
      "Done writing 5 indices in 41 bytes .\n",
      "Done writing 5 indices in 41 bytes .\n",
      "Spawning 2 processes\n",
      "Testing the mapped function outputs:\n",
      "inds: [0, 1]\n",
      "inds: [0, 1]\n",
      "Testing finished, running the mapped function on the dataset:\n",
      "#0:   0%|                                                                                                                                                                    | 0/1 [00:00<?, ?ba/s]\n",
      "inds: [0, 1, 2, 3, 4]                                                                                                                                                                             inds: [0, 1, 2, 3, 4]                                                                                                                                                         | 0/1 [00:00<?, ?ba/s]\n",
      "#0: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1321.04ba/s]\n",
      "#1: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1841.22ba/s]\n",
      "Concatenating 2 shards from multiprocessing\n",
      "Dataset(features: {'text': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['neg', 'pos'], names_file=None, id=None)}, num_rows: 10)\n",
      "```\n",
      "SCORE: 46.59754943847656\n",
      "TITLE: Indices incorrect with multiprocessing\n",
      "URL: https://github.com/huggingface/datasets/issues/597\n",
      "\n",
      "COMMENT: To convert from arrow type we have three options: to_numpy, to_pandas and to_pydict/to_pylist.\n",
      "\n",
      "- to_numpy and to_pandas return numpy arrays instead of lists but are very fast.\n",
      "- to_pydict/to_pylist can be 100x slower and become the bottleneck for reading data, but at least they return lists.\n",
      "\n",
      "Maybe we can have to_pydict/to_pylist as the default and use to_numpy or to_pandas when the format (set by `set_format`) is 'numpy' or 'pandas'\n",
      "SCORE: 43.310699462890625\n",
      "TITLE: Conversion through to_pandas output numpy arrays for lists instead of python objects\n",
      "URL: https://github.com/huggingface/datasets/issues/387\n",
      "\n",
      "==================================================\n",
      "QUESTION What is the max length of file name?\n",
      "COMMENT: @albertvillanova Thanks for additional info on this issue.\n",
      "\n",
      "Yes, I think the best option is to throw an error instead of suppressing it in a loop. I've considered 2 more options, but I don't really like them:\n",
      "1. create a temporary file with a filename longer than 255 characters on import; if this fails, long paths are not enabled and raise a warning. I'm not sure about this approach because I don't like the idea of creating a temporary file on import for this purpose.\n",
      "2. check if long paths are enabled with [this code](https://stackoverflow.com/a/46546731/14095927). As mentioned in the comment, this code relies on an undocumented function and Win10-specific.\n",
      "SCORE: 43.782562255859375\n",
      "TITLE: Some tests hang on Windows\n",
      "URL: https://github.com/huggingface/datasets/issues/2443\n",
      "\n",
      "COMMENT: Hi, the filename here is less than 255\n",
      "```python\n",
      ">>> len(\"_home_garrett_.cache_huggingface_datasets_csv_test-7c856aea083a7043_0.0.0_9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff.incomplete.lock\")\n",
      "154\n",
      "```\n",
      "so not sure why it's considered too long for your filesystem.\n",
      "(also note that the lock files we use always have smaller filenames than 255)\n",
      "\n",
      "https://github.com/huggingface/datasets/blob/5d1a9f1e3c6c495dc0610b459e39d2eb8893f152/src/datasets/utils/filelock.py#L135-L135\n",
      "SCORE: 42.684776306152344\n",
      "TITLE: \"File name too long\" error for file locks\n",
      "URL: https://github.com/huggingface/datasets/issues/2924\n",
      "\n",
      "COMMENT: Yes, you're right! I need to get you more info here. Either there's something going with the name itself that the file system doesn't like (an encoding that blows up the name length??) or perhaps there's something with the path that's causing the entire string to  be used as a name. I haven't seen this on any system before and the Internet's not forthcoming with any info.\n",
      "SCORE: 42.684776306152344\n",
      "TITLE: \"File name too long\" error for file locks\n",
      "URL: https://github.com/huggingface/datasets/issues/2924\n",
      "\n",
      "COMMENT: Hi, the filename here is less than 255\n",
      "```python\n",
      ">>> len(\"_home_garrett_.cache_huggingface_datasets_csv_test-7c856aea083a7043_0.0.0_9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff.incomplete.lock\")\n",
      "154\n",
      "```\n",
      "so not sure why it's considered too long for your filesystem.\n",
      "(also note that the lock files we use always have smaller filenames than 255)\n",
      "\n",
      "https://github.com/huggingface/datasets/blob/5d1a9f1e3c6c495dc0610b459e39d2eb8893f152/src/datasets/utils/filelock.py#L135-L135\n",
      "SCORE: 42.684776306152344\n",
      "TITLE: \"File name too long\" error for file locks\n",
      "URL: https://github.com/huggingface/datasets/issues/2924\n",
      "\n",
      "COMMENT: Yes, you're right! I need to get you more info here. Either there's something going with the name itself that the file system doesn't like (an encoding that blows up the name length??) or perhaps there's something with the path that's causing the entire string to  be used as a name. I haven't seen this on any system before and the Internet's not forthcoming with any info.\n",
      "SCORE: 42.684776306152344\n",
      "TITLE: \"File name too long\" error for file locks\n",
      "URL: https://github.com/huggingface/datasets/issues/2924\n",
      "\n",
      "==================================================\n",
      "QUESTION How to import datasets?\n",
      "COMMENT: You should rephrase your question or give more examples and details on what you want to do.\n",
      "\n",
      "it’s not possible to understand it and help you with only this information.\n",
      "SCORE: 30.4146728515625\n",
      "TITLE: load the local dataset\n",
      "URL: https://github.com/huggingface/datasets/issues/1725\n",
      "\n",
      "COMMENT: Looks like the data has been moved from its original location to google drive\n",
      "\n",
      "New url: https://drive.google.com/u/0/uc?id=12ycYSzLIG253AFN35Y6qoyf9wtkOjakp&export=download\n",
      "SCORE: 29.80609703063965\n",
      "TITLE: 'iwslt2017-ro-nl', cannot be downloaded \n",
      "URL: https://github.com/huggingface/datasets/issues/1287\n",
      "\n",
      "COMMENT: cc100 was added recently, that's why it wasn't available yet.\n",
      "\n",
      "To load it you can just update `datasets`\n",
      "```\n",
      "pip install --upgrade datasets\n",
      "```\n",
      "\n",
      "and then you can load `cc100` with\n",
      "\n",
      "```python\n",
      "from datasets import load_dataset\n",
      "\n",
      "lang = \"en\"\n",
      "dataset = load_dataset(\"cc100\", lang=lang, split=\"train\")\n",
      "```\n",
      "SCORE: 27.868860244750977\n",
      "TITLE: Can't import cc100 dataset\n",
      "URL: https://github.com/huggingface/datasets/issues/1679\n",
      "\n",
      "COMMENT: Hi @kswamy15, thanks for reporting.\n",
      "\n",
      "We are fixing this critical issue and making an urgent patch release of the `datasets` library today.\n",
      "\n",
      "In the meantime, you can circumvent this issue by updating the `tqdm` library: `!pip install -U tqdm`\n",
      "SCORE: 27.762876510620117\n",
      "TITLE: from datasets import Dataset is failing \n",
      "URL: https://github.com/huggingface/datasets/issues/2700\n",
      "\n",
      "COMMENT: One the PR is merged the fix will be available in the next release of `datasets`.\n",
      "\n",
      "If you don't want to wait the next release you can still load the script from the master branch with\n",
      "\n",
      "```python\n",
      "load_dataset(\"dane\", script_version=\"master\")\n",
      "```\n",
      "SCORE: 26.91754913330078\n",
      "TITLE: Dataset Error: DaNE contains empty samples at the end\n",
      "URL: https://github.com/huggingface/datasets/issues/1686\n",
      "\n",
      "==================================================\n",
      "QUESTION How to make pancakes?\n",
      "COMMENT: One the PR is merged the fix will be available in the next release of `datasets`.\n",
      "\n",
      "If you don't want to wait the next release you can still load the script from the master branch with\n",
      "\n",
      "```python\n",
      "load_dataset(\"dane\", script_version=\"master\")\n",
      "```\n",
      "SCORE: 46.65039825439453\n",
      "TITLE: Dataset Error: DaNE contains empty samples at the end\n",
      "URL: https://github.com/huggingface/datasets/issues/1686\n",
      "\n",
      "COMMENT: Using batched map is the way to go then.\n",
      "We'll make it clearer in the docs that map could be used for augmentation.\n",
      "\n",
      "Let me know if you think there should be another way to do it. Or feel free to close the issue otherwise.\n",
      "SCORE: 46.62315368652344\n",
      "TITLE: How to augment data ?\n",
      "URL: https://github.com/huggingface/datasets/issues/365\n",
      "\n",
      "COMMENT: To convert from arrow type we have three options: to_numpy, to_pandas and to_pydict/to_pylist.\n",
      "\n",
      "- to_numpy and to_pandas return numpy arrays instead of lists but are very fast.\n",
      "- to_pydict/to_pylist can be 100x slower and become the bottleneck for reading data, but at least they return lists.\n",
      "\n",
      "Maybe we can have to_pydict/to_pylist as the default and use to_numpy or to_pandas when the format (set by `set_format`) is 'numpy' or 'pandas'\n",
      "SCORE: 46.42205047607422\n",
      "TITLE: Conversion through to_pandas output numpy arrays for lists instead of python objects\n",
      "URL: https://github.com/huggingface/datasets/issues/387\n",
      "\n",
      "COMMENT: Thanks @VictorSanh!\n",
      "There's also a [notebook](https://aka.ms/python_puzzles) and [demo](https://aka.ms/python_puzzles_study) available now to try out some of the puzzles\n",
      "SCORE: 45.37348175048828\n",
      "TITLE:  Python Programming Puzzles\n",
      "URL: https://github.com/huggingface/datasets/issues/2499\n",
      "\n",
      "COMMENT: cc @beurkinger but I think this has been fixed internally and will soon be updated right ?\n",
      "SCORE: 43.67822265625\n",
      "TITLE: A syntax error in example\n",
      "URL: https://github.com/huggingface/datasets/issues/2327\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is Elastic Search?\",\n",
    "    \"What to do when 'NoneType' object is not callable?\",\n",
    "    \"What does get_indices_from_mask_function() return?\",\n",
    "    \"What is the max length of file name?\",\n",
    "    \"How to import datasets?\",\n",
    "    \"How to make pancakes?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "\n",
    "    print(\"QUESTION\", question)\n",
    "    question_embedding = get_embeddings([question]).cpu().detach().numpy()\n",
    "    question_embedding.shape\n",
    "    scores, samples = embeddings_dataset.get_nearest_examples(\n",
    "    \"embeddings\", question_embedding, k=5\n",
    "    )\n",
    "\n",
    "    samples_df = pd.DataFrame.from_dict(samples)\n",
    "    samples_df[\"scores\"] = scores\n",
    "    samples_df.sort_values(\"scores\", ascending=False, inplace=True)\n",
    "\n",
    "    for _, row in samples_df.iterrows():\n",
    "        print(f\"COMMENT: {row.comments}\")\n",
    "        print(f\"SCORE: {row.scores}\")\n",
    "        print(f\"TITLE: {row.title}\")\n",
    "        print(f\"URL: {row.html_url}\")\n",
    "        print()\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tutorial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a simple application for sentence embeddings: semantic search\n",
    "\n",
    "We have a corpus with various sentences. Then, for a given query sentence,\n",
    "we want to find the most similar sentence in this corpus.\n",
    "\n",
    "This script outputs for various queries the top 5 most similar sentences in the corpus.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Corpus with example sentences\n",
    "corpus = [\n",
    "    \"A man is eating food.\",\n",
    "    \"A man is eating a piece of bread.\",\n",
    "    \"The girl is carrying a baby.\",\n",
    "    \"A man is riding a horse.\",\n",
    "    \"A woman is playing violin.\",\n",
    "    \"Two men pushed carts through the woods.\",\n",
    "    \"A man is riding a white horse on an enclosed ground.\",\n",
    "    \"A monkey is playing drums.\",\n",
    "    \"A cheetah is running behind its prey.\",\n",
    "]\n",
    "# Use \"convert_to_tensor=True\" to keep the tensors on GPU (if available)\n",
    "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "# Query sentences:\n",
    "queries = [\n",
    "    \"A man is eating pasta.\",\n",
    "    \"Someone in a gorilla costume is playing a set of drums.\",\n",
    "    \"A cheetah chases prey on across a field.\",\n",
    "]\n",
    "\n",
    "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "top_k = min(5, len(corpus))\n",
    "for query in queries:\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
    "    similarity_scores = embedder.similarity(query_embedding, corpus_embeddings)[0]\n",
    "    scores, indices = torch.topk(similarity_scores, k=top_k)\n",
    "\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"Top 5 most similar sentences in corpus:\")\n",
    "\n",
    "    for score, idx in zip(scores, indices):\n",
    "        print(corpus[idx])\n",
    "        print(f\"(Score: {score:.4f})\")\n",
    "\n",
    "    \"\"\"\n",
    "    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk\n",
    "    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)\n",
    "    hits = hits[0]      #Get the hits for the first query\n",
    "    for hit in hits:\n",
    "        print(corpus[hit['corpus_id']], \"(Score: {:.4f})\".format(hit['score']))\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
