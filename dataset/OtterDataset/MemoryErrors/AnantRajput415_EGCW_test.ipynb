{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>537</td>\n",
       "      <td>350</td>\n",
       "      <td>551</td>\n",
       "      <td>343</td>\n",
       "      <td>568</td>\n",
       "      <td>344</td>\n",
       "      <td>582</td>\n",
       "      <td>353</td>\n",
       "      <td>567</td>\n",
       "      <td>357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028289</td>\n",
       "      <td>0.556645</td>\n",
       "      <td>-0.910367</td>\n",
       "      <td>497.835052</td>\n",
       "      <td>-32.616417</td>\n",
       "      <td>-1.906307</td>\n",
       "      <td>498.891266</td>\n",
       "      <td>33.729710</td>\n",
       "      <td>0.085573</td>\n",
       "      <td>496.778839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>529</td>\n",
       "      <td>342</td>\n",
       "      <td>543</td>\n",
       "      <td>336</td>\n",
       "      <td>559</td>\n",
       "      <td>337</td>\n",
       "      <td>573</td>\n",
       "      <td>345</td>\n",
       "      <td>558</td>\n",
       "      <td>349</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025278</td>\n",
       "      <td>-3.828883</td>\n",
       "      <td>-4.842280</td>\n",
       "      <td>508.932648</td>\n",
       "      <td>-36.961273</td>\n",
       "      <td>-5.369772</td>\n",
       "      <td>511.060089</td>\n",
       "      <td>29.303507</td>\n",
       "      <td>-4.314788</td>\n",
       "      <td>506.805176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>564</td>\n",
       "      <td>342</td>\n",
       "      <td>579</td>\n",
       "      <td>335</td>\n",
       "      <td>597</td>\n",
       "      <td>337</td>\n",
       "      <td>612</td>\n",
       "      <td>346</td>\n",
       "      <td>596</td>\n",
       "      <td>349</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039401</td>\n",
       "      <td>14.374902</td>\n",
       "      <td>-4.112084</td>\n",
       "      <td>503.034241</td>\n",
       "      <td>-18.804157</td>\n",
       "      <td>-5.365746</td>\n",
       "      <td>503.405701</td>\n",
       "      <td>47.553963</td>\n",
       "      <td>-2.858423</td>\n",
       "      <td>502.662750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>544</td>\n",
       "      <td>334</td>\n",
       "      <td>558</td>\n",
       "      <td>327</td>\n",
       "      <td>575</td>\n",
       "      <td>328</td>\n",
       "      <td>590</td>\n",
       "      <td>337</td>\n",
       "      <td>574</td>\n",
       "      <td>341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036817</td>\n",
       "      <td>4.041533</td>\n",
       "      <td>-8.495733</td>\n",
       "      <td>506.759644</td>\n",
       "      <td>-29.090580</td>\n",
       "      <td>-9.368177</td>\n",
       "      <td>508.774963</td>\n",
       "      <td>37.173645</td>\n",
       "      <td>-7.623289</td>\n",
       "      <td>504.744324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>531</td>\n",
       "      <td>336</td>\n",
       "      <td>545</td>\n",
       "      <td>330</td>\n",
       "      <td>561</td>\n",
       "      <td>330</td>\n",
       "      <td>575</td>\n",
       "      <td>339</td>\n",
       "      <td>560</td>\n",
       "      <td>343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027466</td>\n",
       "      <td>-3.035088</td>\n",
       "      <td>-7.765797</td>\n",
       "      <td>510.304565</td>\n",
       "      <td>-36.161705</td>\n",
       "      <td>-8.315956</td>\n",
       "      <td>512.514587</td>\n",
       "      <td>30.091526</td>\n",
       "      <td>-7.215638</td>\n",
       "      <td>508.094513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>625</td>\n",
       "      <td>402</td>\n",
       "      <td>638</td>\n",
       "      <td>397</td>\n",
       "      <td>653</td>\n",
       "      <td>399</td>\n",
       "      <td>665</td>\n",
       "      <td>408</td>\n",
       "      <td>652</td>\n",
       "      <td>411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071705</td>\n",
       "      <td>42.613365</td>\n",
       "      <td>30.039637</td>\n",
       "      <td>572.214172</td>\n",
       "      <td>9.628985</td>\n",
       "      <td>27.512640</td>\n",
       "      <td>569.349792</td>\n",
       "      <td>75.597740</td>\n",
       "      <td>32.566635</td>\n",
       "      <td>575.078552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>616</td>\n",
       "      <td>415</td>\n",
       "      <td>629</td>\n",
       "      <td>410</td>\n",
       "      <td>644</td>\n",
       "      <td>412</td>\n",
       "      <td>656</td>\n",
       "      <td>421</td>\n",
       "      <td>643</td>\n",
       "      <td>423</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060802</td>\n",
       "      <td>37.677147</td>\n",
       "      <td>36.256992</td>\n",
       "      <td>564.379089</td>\n",
       "      <td>4.536333</td>\n",
       "      <td>34.250275</td>\n",
       "      <td>564.847107</td>\n",
       "      <td>70.817963</td>\n",
       "      <td>38.263710</td>\n",
       "      <td>563.911072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>556</td>\n",
       "      <td>399</td>\n",
       "      <td>570</td>\n",
       "      <td>392</td>\n",
       "      <td>586</td>\n",
       "      <td>393</td>\n",
       "      <td>599</td>\n",
       "      <td>401</td>\n",
       "      <td>584</td>\n",
       "      <td>405</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049988</td>\n",
       "      <td>7.426083</td>\n",
       "      <td>23.939354</td>\n",
       "      <td>532.911804</td>\n",
       "      <td>-25.725267</td>\n",
       "      <td>22.326677</td>\n",
       "      <td>531.938782</td>\n",
       "      <td>40.577435</td>\n",
       "      <td>25.552031</td>\n",
       "      <td>533.884827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>555</td>\n",
       "      <td>410</td>\n",
       "      <td>569</td>\n",
       "      <td>404</td>\n",
       "      <td>585</td>\n",
       "      <td>404</td>\n",
       "      <td>599</td>\n",
       "      <td>413</td>\n",
       "      <td>584</td>\n",
       "      <td>416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038618</td>\n",
       "      <td>7.657980</td>\n",
       "      <td>29.278347</td>\n",
       "      <td>525.193726</td>\n",
       "      <td>-25.518261</td>\n",
       "      <td>28.025335</td>\n",
       "      <td>524.622192</td>\n",
       "      <td>40.834221</td>\n",
       "      <td>30.531359</td>\n",
       "      <td>525.765259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>557</td>\n",
       "      <td>378</td>\n",
       "      <td>573</td>\n",
       "      <td>370</td>\n",
       "      <td>591</td>\n",
       "      <td>371</td>\n",
       "      <td>606</td>\n",
       "      <td>380</td>\n",
       "      <td>590</td>\n",
       "      <td>384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031848</td>\n",
       "      <td>12.097072</td>\n",
       "      <td>11.796187</td>\n",
       "      <td>478.188110</td>\n",
       "      <td>-21.090744</td>\n",
       "      <td>10.735056</td>\n",
       "      <td>478.238647</td>\n",
       "      <td>45.284885</td>\n",
       "      <td>12.857319</td>\n",
       "      <td>478.137543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>510 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9   ...        31  \\\n",
       "0    537  350  551  343  568  344  582  353  567  357  ...  0.028289   \n",
       "1    529  342  543  336  559  337  573  345  558  349  ...  0.025278   \n",
       "2    564  342  579  335  597  337  612  346  596  349  ...  0.039401   \n",
       "3    544  334  558  327  575  328  590  337  574  341  ...  0.036817   \n",
       "4    531  336  545  330  561  330  575  339  560  343  ...  0.027466   \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...       ...   \n",
       "505  625  402  638  397  653  399  665  408  652  411  ...  0.071705   \n",
       "506  616  415  629  410  644  412  656  421  643  423  ...  0.060802   \n",
       "507  556  399  570  392  586  393  599  401  584  405  ...  0.049988   \n",
       "508  555  410  569  404  585  404  599  413  584  416  ...  0.038618   \n",
       "509  557  378  573  370  591  371  606  380  590  384  ...  0.031848   \n",
       "\n",
       "            32         33          34         35         36          37  \\\n",
       "0     0.556645  -0.910367  497.835052 -32.616417  -1.906307  498.891266   \n",
       "1    -3.828883  -4.842280  508.932648 -36.961273  -5.369772  511.060089   \n",
       "2    14.374902  -4.112084  503.034241 -18.804157  -5.365746  503.405701   \n",
       "3     4.041533  -8.495733  506.759644 -29.090580  -9.368177  508.774963   \n",
       "4    -3.035088  -7.765797  510.304565 -36.161705  -8.315956  512.514587   \n",
       "..         ...        ...         ...        ...        ...         ...   \n",
       "505  42.613365  30.039637  572.214172   9.628985  27.512640  569.349792   \n",
       "506  37.677147  36.256992  564.379089   4.536333  34.250275  564.847107   \n",
       "507   7.426083  23.939354  532.911804 -25.725267  22.326677  531.938782   \n",
       "508   7.657980  29.278347  525.193726 -25.518261  28.025335  524.622192   \n",
       "509  12.097072  11.796187  478.188110 -21.090744  10.735056  478.238647   \n",
       "\n",
       "            38         39          40  \n",
       "0    33.729710   0.085573  496.778839  \n",
       "1    29.303507  -4.314788  506.805176  \n",
       "2    47.553963  -2.858423  502.662750  \n",
       "3    37.173645  -7.623289  504.744324  \n",
       "4    30.091526  -7.215638  508.094513  \n",
       "..         ...        ...         ...  \n",
       "505  75.597740  32.566635  575.078552  \n",
       "506  70.817963  38.263710  563.911072  \n",
       "507  40.577435  25.552031  533.884827  \n",
       "508  40.834221  30.531359  525.765259  \n",
       "509  45.284885  12.857319  478.137543  \n",
       "\n",
       "[510 rows x 41 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Assuming the annotations are in a text file\n",
    "annotations_file = 'annotation.txt'\n",
    "image_folder = './'\n",
    "\n",
    "# Read the annotations\n",
    "annotations = pd.read_csv(annotations_file, sep=' ', header=None)\n",
    "\n",
    "annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from PIL import Image\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import torchvision.transforms as transforms\n",
    "# import torch\n",
    "\n",
    "# class GazeDataset(Dataset):\n",
    "#     def __init__(self, annotations, image_folder, transform=None):\n",
    "#         self.annotations = annotations\n",
    "#         self.image_folder = image_folder\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.annotations)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # Format the image filename as '0001.jpg', '0002.jpg', ...\n",
    "#         image_filename = f'{idx + 1:04d}.jpg'\n",
    "#         image_path = os.path.join(self.image_folder, image_filename)\n",
    "        \n",
    "#         # Open the image\n",
    "#         image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "#         # Apply transformations (if any)\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "        \n",
    "#         # Extract gaze vector (last three values)\n",
    "#         gaze_vector = torch.tensor(self.annotations.iloc[idx, -3:].values, dtype=torch.float32)\n",
    "        \n",
    "#         return image, gaze_vector\n",
    "\n",
    "# # Define any image transformations (e.g., normalization)\n",
    "# data_transforms = transforms.Compose([\n",
    "#     transforms.ToTensor(),  # Converts PIL image to torch.Tensor with values in [0, 1]\n",
    "#     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalizes the data\n",
    "# ])\n",
    "\n",
    "# # Create dataset and DataLoader\n",
    "# gaze_dataset = GazeDataset(annotations, image_folder, transform=data_transforms)\n",
    "# train_loader = DataLoader(gaze_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# print(f'Dataset created with {len(gaze_dataset)} samples.')\n",
    "\n",
    "# # Sample usage of the DataLoader\n",
    "# for images, gaze_vectors in train_loader:\n",
    "#     print(f'Batch size: {images.size(0)}')\n",
    "#     break  # Remove this in training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the transformations: ToTensor converts image to float32, and Normalize scales the values\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts PIL image to torch.FloatTensor and scales pixel values to [0, 1]\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Optional normalization\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "gaze_vectors = []\n",
    "\n",
    "# Load images and gaze vectors\n",
    "for idx, row in annotations.iterrows():\n",
    "    # Format the image filename as '0001.jpg', '0002.jpg', ...\n",
    "    image_filename = f'{idx + 1:04d}.jpg'  # +1 to start from 0001 instead of 0000\n",
    "    image_path = os.path.join(image_folder, image_filename)\n",
    "    \n",
    "    # Open the image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Optionally apply transformations\n",
    "    image = data_transforms(image)  # Uncomment if using transformations\n",
    "    \n",
    "    images.append(image)\n",
    "    gaze_vectors.append(row[-3:].values)  # Gaze vector (last three values)\n",
    "\n",
    "# # Convert lists to numpy arrays or tensors as needed\n",
    "# images = np.array(images)  # Note: PIL images cannot be directly converted; consider using a tensor or a list of arrays\n",
    "# gaze_vectors = np.array(gaze_vectors)\n",
    "\n",
    "# print(f'Loaded {len(images)} images and corresponding gaze vectors.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 720, 1280])\n",
      "[3.37297100e+01 8.55730000e-02 4.96778839e+02]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "print(images[0].shape)\n",
    "print(gaze_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GazeDataset(Dataset):\n",
    "    def __init__(self, images, gaze_vectors):\n",
    "        self.images = images\n",
    "        self.gaze_vectors = gaze_vectors\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], torch.tensor(self.gaze_vectors[idx], dtype=torch.float32)\n",
    "\n",
    "gaze_dataset = GazeDataset(images, gaze_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         ...,\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "        [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         ...,\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
      "\n",
      "        [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         ...,\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "         [-1., -1., -1.,  ..., -1., -1., -1.]]])\n"
     ]
    }
   ],
   "source": [
    "print(gaze_dataset.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "train_size = int(0.8 * len(gaze_dataset))\n",
    "val_size = len(gaze_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(gaze_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with 510 samples.\n",
      "Batch size: 8\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset created with {len(gaze_dataset)} samples.')\n",
    "\n",
    "# Sample usage of the DataLoader\n",
    "for images, gaze_vectors in train_loader:\n",
    "    print(f'Batch size: {images.size(0)}')\n",
    "    break  # Remove this in training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class GazeCNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(GazeCNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "#         self.fc1 = nn.Linear(64 * 64 * 64, 128)  # Adjust based on input size\n",
    "#         self.fc2 = nn.Linear(128, 3)  # Output: gaze vector\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "#         x = x.view(x.size(0), -1)  # Flatten\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# model = GazeCNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after conv layers: torch.Size([8, 128, 56, 56])\n",
      "Shape after flattening: torch.Size([8, 401408])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 56 * 56, 128)  # Adjust based on the output size of conv layers\n",
    "        self.fc2 = nn.Linear(128, 3)  # Output layer (e.g., for 3 gaze vector components)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward through conv layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)  # Max pooling\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)  # Max pooling\n",
    "        \n",
    "        # Check the output shape of the conv layers\n",
    "        print(f\"Shape after conv layers: {x.shape}\")\n",
    "        \n",
    "        # Flatten the output from conv layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        print(f\"Shape after flattening: {x.shape}\")\n",
    "        \n",
    "        # Forward through fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = CNNModel()\n",
    "\n",
    "# Create a dummy input with batch size 8 and image size 224x224\n",
    "sample_input = torch.randn(8, 3, 224, 224)\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(sample_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available:  True\n",
      "CUDA version:  12.1\n",
      "PyTorch version:  2.4.1+cu121\n",
      "Number of GPUs:  1\n",
      "CUDA device:  NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Is CUDA available: \", torch.cuda.is_available())\n",
    "print(\"CUDA version: \", torch.version.cuda)\n",
    "print(\"PyTorch version: \", torch.__version__)\n",
    "print(\"Number of GPUs: \", torch.cuda.device_count())\n",
    "print(\"CUDA device: \", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=401408, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.76 GiB. GPU 0 has a total capacity of 3.80 GiB of which 1.59 GiB is free. Including non-PyTorch memory, this process has 2.20 GiB memory in use. Of the allocated memory 2.11 GiB is allocated by PyTorch, and 4.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Forward pass: compute the output\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, gaze_vectors)\n",
      "File \u001b[0;32m~/miniconda3/envs/datas/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/datas/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m, in \u001b[0;36mCNNModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Forward through conv layers\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(x, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Max pooling\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n",
      "File \u001b[0;32m~/miniconda3/envs/datas/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/datas/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/datas/lib/python3.9/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/datas/lib/python3.9/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.76 GiB. GPU 0 has a total capacity of 3.80 GiB of which 1.59 GiB is free. Including non-PyTorch memory, this process has 2.20 GiB memory in use. Of the allocated memory 2.11 GiB is allocated by PyTorch, and 4.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Training loop\n",
    "    for images, gaze_vectors in train_loader:\n",
    "        # Move data to GPU if using CUDA\n",
    "        images = images.to(device)\n",
    "        gaze_vectors = gaze_vectors.to(device)\n",
    "        \n",
    "        # Zero gradients from the previous step\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: compute the output\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, gaze_vectors)\n",
    "        \n",
    "        # Backward pass: compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate the running loss\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Calculate average loss over the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    # Optional: Validation loop (use a validation DataLoader)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():  # No need to compute gradients for validation\n",
    "        for images, gaze_vectors in val_loader:\n",
    "            images = images.to(device)\n",
    "            gaze_vectors = gaze_vectors.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, gaze_vectors)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    print(f'Validation Loss: {val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, gaze_vectors \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      6\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 7\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, gaze_vectors)\n\u001b[1;32m      9\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/datas/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/datas/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m, in \u001b[0;36mGazeCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(x, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n",
      "File \u001b[0;32m~/miniconda3/envs/datas/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/datas/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/datas/lib/python3.9/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/datas/lib/python3.9/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, gaze_vectors in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, gaze_vectors)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0)]\n",
      "Image size: (1280, 720)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Open an image\n",
    "image_path = '0011.jpg'\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Convert image to RGB if not already\n",
    "image = image.convert('RGB')\n",
    "\n",
    "# Get pixel values\n",
    "pixels = list(image.getdata())  # Get all pixel values\n",
    "\n",
    "# Print the first 10 pixel values\n",
    "print(pixels[:10])\n",
    "\n",
    "# To see the size of the image\n",
    "print(f'Image size: {image.size}')  # Outputs (width, height)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
