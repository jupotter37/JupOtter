{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and imports"
      ],
      "metadata": {
        "id": "zWMuTvRAxcNe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEDeFLW-ABWB"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets\n",
        "!pip install tokenizers\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "GKxGVQcRANsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "O4dZCZfb-zwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyUk2iY--2AB",
        "outputId": "af566e00-74ef-4a4a-871e-462665153bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/drive/MyDrive/Models/pytorch-transformer/weights\n",
        "!mkdir -p /content/drive/MyDrive/Models/pytorch-transformer/vocab"
      ],
      "metadata": {
        "id": "nuTvchjO-_10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_config():\n",
        "    return {\n",
        "        \"batch_size\": 16,\n",
        "        \"num_epochs\": 100,\n",
        "        \"lr\": 10**-4,\n",
        "        \"seq_len\": 400,\n",
        "        \"d_model\": 512,\n",
        "        \"datasource\": 'NikitiusIvanov/protein_seq_to_go_bio_process',\n",
        "        \"lang_src\": \"sequence\",\n",
        "        \"lang_tgt\": \"go_process\",\n",
        "        \"model_folder\": \"pytorch-transformer/weights\",\n",
        "        \"model_basename\": \"tmodel_\",\n",
        "        \"preload\": None,\n",
        "        \"tokenizer_file\": \"pytorch-transformer/vocab/tokenizer_{0}.json\",\n",
        "        \"experiment_name\": \"runs/tmodel\"\n",
        "    }"
      ],
      "metadata": {
        "id": "O3U3xo5Q_Is9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = get_config()\n",
        "cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z01ptGn_4Oi",
        "outputId": "cca1a43a-fa62-487f-a851-4c3a53817068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 16,\n",
              " 'num_epochs': 100,\n",
              " 'lr': 0.0001,\n",
              " 'seq_len': 400,\n",
              " 'd_model': 512,\n",
              " 'datasource': 'NikitiusIvanov/protein_seq_to_go_bio_process',\n",
              " 'lang_src': 'sequence',\n",
              " 'lang_tgt': 'go_process',\n",
              " 'model_folder': 'pytorch-transformer/weights',\n",
              " 'model_basename': 'tmodel_',\n",
              " 'preload': None,\n",
              " 'tokenizer_file': 'pytorch-transformer/vocab/tokenizer_{0}.json',\n",
              " 'experiment_name': 'runs/tmodel'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### model imports ####\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "#### dataset imports ####\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "#### train imports ####\n",
        "# import torchtext.datasets as datasets\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Huggingface datasets and tokenizers\n",
        "from datasets import load_dataset\n",
        "from tokenizers import models\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.trainers import WordPieceTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "import torchmetrics\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "ZjwsUkJQAQ0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model code"
      ],
      "metadata": {
        "id": "kIDkdDTdAfpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n",
        "        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, hidden_size)\n",
        "         # Keep the dimension for broadcasting\n",
        "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
        "        # Keep the dimension for broadcasting\n",
        "        std = x.std(dim = -1, keepdim = True) # (batch, seq_len, 1)\n",
        "        # eps is to prevent dividing by zero or when std is very small\n",
        "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
        "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
        "\n",
        "class InputEmbeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
        "        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Create a matrix of shape (seq_len, d_model)\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "        # Create a vector of shape (seq_len)\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)\n",
        "        # Create a vector of shape (d_model)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
        "        # Apply sine to even indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
        "        # Apply cosine to odd indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
        "        # Add a batch dimension to the positional encoding\n",
        "        pe = pe.unsqueeze(0) # (1, seq_len, d_model)\n",
        "        # Register the positional encoding as a buffer\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "\n",
        "        def __init__(self, features: int, dropout: float) -> None:\n",
        "            super().__init__()\n",
        "            self.dropout = nn.Dropout(dropout)\n",
        "            self.norm = LayerNormalization(features)\n",
        "\n",
        "        def forward(self, x, sublayer):\n",
        "            return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model # Embedding vector size\n",
        "        self.h = h # Number of heads\n",
        "        # Make sure d_model is divisible by h\n",
        "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
        "\n",
        "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
        "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "        d_k = query.shape[-1]\n",
        "        # Just apply the formula from the paper\n",
        "        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
        "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
        "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
        "        # return attention scores which can be used for visualization\n",
        "        return (attention_scores @ value), attention_scores\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
        "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Calculate attention\n",
        "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "        # Combine all the heads together\n",
        "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
        "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "\n",
        "        # Multiply by Wo\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        return self.w_o(x)\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class ProjectionLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, vocab_size) -> None:\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x) -> None:\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
        "        return self.proj(x)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self, encoder: Encoder, decoder: Decoder,\n",
        "        src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding,\n",
        "        tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.src_pos = src_pos\n",
        "        self.tgt_pos = tgt_pos\n",
        "        self.projection_layer = projection_layer\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        # (batch, seq_len, d_model)\n",
        "        src = self.src_embed(src)\n",
        "        src = self.src_pos(src)\n",
        "        return self.encoder(src, src_mask)\n",
        "\n",
        "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
        "        # (batch, seq_len, d_model)\n",
        "        tgt = self.tgt_embed(tgt)\n",
        "        tgt = self.tgt_pos(tgt)\n",
        "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "    def project(self, x):\n",
        "        # (batch, seq_len, vocab_size)\n",
        "        return self.projection_layer(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "N8Hk8f6wAj5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### build_transformer"
      ],
      "metadata": {
        "id": "LCWK8_1x20U9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
        "    # Create the embedding layers\n",
        "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
        "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the positional encoding layers\n",
        "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
        "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
        "\n",
        "    # Create the encoder blocks\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    # Create the decoder blocks\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
        "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "    # Create the encoder and decoder\n",
        "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    # Create the projection layer\n",
        "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the transformer\n",
        "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
        "\n",
        "    # Initialize the parameters\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer\n"
      ],
      "metadata": {
        "id": "EZpzINBY2ynM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset code"
      ],
      "metadata": {
        "id": "olK9m4qJAvMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BilingualDataset(Dataset):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        ds,\n",
        "        tokenizer_src,\n",
        "        tokenizer_tgt,\n",
        "        src_lang,\n",
        "        tgt_lang,\n",
        "        seq_len\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.ds = ds\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_tgt = tokenizer_tgt\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "\n",
        "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
        "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
        "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_target_pair = self.ds[idx]\n",
        "        src_text = src_target_pair['translation'][self.src_lang]\n",
        "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
        "\n",
        "        # Transform the text into tokens\n",
        "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
        "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
        "\n",
        "        # Add sos, eos and padding to each sentence\n",
        "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # We will add <s> and </s>\n",
        "        # We will only add <s>, and </s> only on the label\n",
        "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1\n",
        "\n",
        "        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long\n",
        "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
        "            raise ValueError(\"Sentence is too long\")\n",
        "\n",
        "        # Add <s> and </s> token\n",
        "        encoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Add only <s> token\n",
        "        decoder_input = torch.cat(\n",
        "            [\n",
        "                self.sos_token,\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Add only </s> token\n",
        "        label = torch.cat(\n",
        "            [\n",
        "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
        "                self.eos_token,\n",
        "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "\n",
        "        # Double check the size of the tensors to make sure they are all seq_len long\n",
        "        assert encoder_input.size(0) == self.seq_len\n",
        "        assert decoder_input.size(0) == self.seq_len\n",
        "        assert label.size(0) == self.seq_len\n",
        "\n",
        "        return {\n",
        "            \"encoder_input\": encoder_input,  # (seq_len)\n",
        "            \"decoder_input\": decoder_input,  # (seq_len)\n",
        "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)\n",
        "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),\n",
        "            \"label\": label,  # (seq_len)\n",
        "            \"src_text\": src_text,\n",
        "            \"tgt_text\": tgt_text,\n",
        "        }\n",
        "\n",
        "def causal_mask(size):\n",
        "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
        "    return mask == 0\n"
      ],
      "metadata": {
        "id": "K25eYDD7A1GW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model code"
      ],
      "metadata": {
        "id": "P10zuuEUA41U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### greedy_decode"
      ],
      "metadata": {
        "id": "p9qi-Zvs12Nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
        "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
        "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
        "\n",
        "    # Precompute the encoder output and reuse it for every step\n",
        "    encoder_output = model.encode(source, source_mask)\n",
        "    # Initialize the decoder input with the sos token\n",
        "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
        "    while True:\n",
        "        if decoder_input.size(1) == max_len:\n",
        "            break\n",
        "\n",
        "        # build mask for target\n",
        "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
        "\n",
        "        # calculate output\n",
        "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
        "\n",
        "        # get next token\n",
        "        prob = model.project(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        decoder_input = torch.cat(\n",
        "            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
        "        )\n",
        "\n",
        "        if next_word == eos_idx:\n",
        "            break\n",
        "\n",
        "    return decoder_input.squeeze(0)\n"
      ],
      "metadata": {
        "id": "Zu04R5XS18DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### run_validation"
      ],
      "metadata": {
        "id": "6Xqf9N3V19pa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, writer, num_examples=2):\n",
        "    model.eval()\n",
        "    count = 0\n",
        "\n",
        "    source_texts = []\n",
        "    expected = []\n",
        "    predicted = []\n",
        "\n",
        "    try:\n",
        "        # get the console window width\n",
        "        with os.popen('stty size', 'r') as console:\n",
        "            _, console_width = console.read().split()\n",
        "            console_width = int(console_width)\n",
        "    except:\n",
        "        # If we can't get the console width, use 80 as default\n",
        "        console_width = 80\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_ds:\n",
        "            count += 1\n",
        "            encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
        "            encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
        "\n",
        "            # check that the batch size is 1\n",
        "            assert encoder_input.size(\n",
        "                0) == 1, \"Batch size must be 1 for validation\"\n",
        "\n",
        "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
        "\n",
        "            source_text = batch[\"src_text\"][0]\n",
        "            target_text = batch[\"tgt_text\"][0]\n",
        "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
        "\n",
        "            source_texts.append(source_text)\n",
        "            expected.append(target_text)\n",
        "            predicted.append(model_out_text)\n",
        "\n",
        "            # Print the source, target and model output\n",
        "            print_msg('-'*console_width)\n",
        "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
        "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
        "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
        "\n",
        "            if count == num_examples:\n",
        "                print_msg('-'*console_width)\n",
        "                break\n",
        "\n",
        "    if writer:\n",
        "        # Evaluate the character error rate\n",
        "        # Compute the char error rate\n",
        "        metric = torchmetrics.text.CharErrorRate()\n",
        "        cer = metric(predicted, expected)\n",
        "        writer.add_scalar('validation cer', cer, global_step)\n",
        "        writer.flush()\n",
        "\n",
        "        # Compute the word error rate\n",
        "        metric = torchmetrics.text.WordErrorRate()\n",
        "        wer = metric(predicted, expected)\n",
        "        writer.add_scalar('validation wer', wer, global_step)\n",
        "        writer.flush()\n",
        "\n",
        "        # Compute the BLEU metric\n",
        "        metric = torchmetrics.text.BLEUScore()\n",
        "        bleu = metric(predicted, expected)\n",
        "        writer.add_scalar('validation BLEU', bleu, global_step)\n",
        "        writer.flush()\n",
        "\n"
      ],
      "metadata": {
        "id": "7xlpS7qH2Cb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_all_sentences & get_or_build_tokenizer"
      ],
      "metadata": {
        "id": "5RxiKl5p2DjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_sentences(ds, lang):\n",
        "    for item in ds:\n",
        "        yield item['translation'][lang]\n",
        "\n",
        "\n",
        "def get_or_build_tokenizer(config, ds, lang):\n",
        "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
        "    if not Path.exists(tokenizer_path):\n",
        "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
        "        tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "        trainer = BpeTrainer(\n",
        "            special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],\n",
        "            min_frequency=10,\n",
        "            show_progress=True,\n",
        "        )\n",
        "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
        "        print(str(tokenizer_path))\n",
        "        tokenizer.save(str(tokenizer_path))\n",
        "    else:\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "    return tokenizer\n",
        "\n"
      ],
      "metadata": {
        "id": "NAnxCiDr2Nco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_ds"
      ],
      "metadata": {
        "id": "7koyfRGz2TwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_ds(config):\n",
        "    # It only has the train split, so we divide it overselves\n",
        "    ds_raw = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n",
        "\n",
        "    # Build tokenizers\n",
        "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n",
        "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
        "\n",
        "    # Keep 90% for training, 10% for validation\n",
        "    train_ds_size = int(0.9 * len(ds_raw))\n",
        "    val_ds_size = len(ds_raw) - train_ds_size\n",
        "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
        "\n",
        "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "\n",
        "    # Find the maximum length of each sentence in the source and target sentence\n",
        "    max_len_src = 0\n",
        "    max_len_tgt = 0\n",
        "\n",
        "    for item in ds_raw:\n",
        "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids\n",
        "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids\n",
        "        max_len_src = max(max_len_src, len(src_ids))\n",
        "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
        "\n",
        "    print(f'Max length of source sentence: {max_len_src}')\n",
        "    print(f'Max length of target sentence: {max_len_tgt}')\n",
        "\n",
        "\n",
        "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
        "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
        "\n",
        "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n",
        "\n"
      ],
      "metadata": {
        "id": "88rDSYx9BCll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_model"
      ],
      "metadata": {
        "id": "eaD_AS-z2c70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
        "    model = build_transformer(\n",
        "        vocab_src_len,\n",
        "        vocab_tgt_len,\n",
        "        config[\"seq_len\"],\n",
        "        config['seq_len'],\n",
        "        d_model=config['d_model']\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "iQq7vkil2fHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get weights"
      ],
      "metadata": {
        "id": "WC6lL4hgoY5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weights_file_path(config, epoch: str):\n",
        "    model_folder = f\"{config['model_folder']}\"\n",
        "    model_filename = f\"{config['model_basename']}{epoch}.pt\"\n",
        "    return str(Path('.') / model_folder / model_filename)\n",
        "\n",
        "# Find the latest weights file in the weights folder\n",
        "def latest_weights_file_path(config):\n",
        "    model_folder = f\"{config['model_folder']}\"\n",
        "    model_filename = f\"{config['model_basename']}*\"\n",
        "    weights_files = list(Path(model_folder).glob(model_filename))\n",
        "    if len(weights_files) == 0:\n",
        "        return None\n",
        "    weights_files.sort()\n",
        "    return str(weights_files[-1])"
      ],
      "metadata": {
        "id": "YJ1ZGmkUoX8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom loss"
      ],
      "metadata": {
        "id": "P_b1NTH15wNm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DWxM8okoaE4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(config):\n",
        "    # Define the device\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
        "    print(\"Using device:\", device)\n",
        "    if (device == 'cuda'):\n",
        "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
        "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
        "    elif (device == 'mps'):\n",
        "        print(f\"Device name: <mps>\")\n",
        "    else:\n",
        "        print(\"NOTE: If you have a GPU, consider using it for training.\")\n",
        "        print(\"      On a Windows machine with NVidia GPU, check this video: https://www.youtube.com/watch?v=GMSjDTU8Zlc\")\n",
        "        print(\"      On a Mac machine, run: pip3 install --pre torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/nightly/cpu\")\n",
        "    device = torch.device(device)\n",
        "\n",
        "    # Make sure the weights folder exists\n",
        "    Path(f\"{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n",
        "    print('run get_ds')\n",
        "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
        "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
        "    # Tensorboard\n",
        "    writer = SummaryWriter(config['experiment_name'])\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
        "\n",
        "    # If the user specified a model to preload before training, load it\n",
        "    initial_epoch = 0\n",
        "    global_step = 0\n",
        "    preload = config['preload']\n",
        "    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
        "    if model_filename:\n",
        "        print(f'Preloading model {model_filename}')\n",
        "        state = torch.load(model_filename)\n",
        "        model.load_state_dict(state['model_state_dict'])\n",
        "        initial_epoch = state['epoch'] + 1\n",
        "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "        global_step = state['global_step']\n",
        "    else:\n",
        "        print('No model to preload, starting from scratch')\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
        "\n",
        "    for epoch in range(initial_epoch, config['num_epochs']):\n",
        "        torch.cuda.empty_cache()\n",
        "        model.train()\n",
        "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
        "        for batch in batch_iterator:\n",
        "\n",
        "            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n",
        "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
        "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
        "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
        "\n",
        "            # Run the tensors through the encoder, decoder and the projection layer\n",
        "            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
        "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
        "            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n",
        "\n",
        "            # Compare the output with the label\n",
        "            label = batch['label'].to(device) # (B, seq_len)\n",
        "\n",
        "            # Compute the loss using a simple cross entropy\n",
        "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "\n",
        "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
        "\n",
        "            # Log the loss\n",
        "            writer.add_scalar('train loss', loss.item(), global_step)\n",
        "            writer.flush()\n",
        "\n",
        "            # Backpropagate the loss\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the weights\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            global_step += 1\n",
        "        print('run validation')\n",
        "        # Run validation at the end of every epoch\n",
        "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
        "\n",
        "        # Save the model at the end of every epoch\n",
        "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'global_step': global_step\n",
        "        }, model_filename)\n"
      ],
      "metadata": {
        "id": "0ZNRJBF7Bejc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "9IZfX3d_H76A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = get_config()\n",
        "# cfg['preload'] = 'latest'\n",
        "cfg['seq_len'] = 370"
      ],
      "metadata": {
        "id": "wAOx5pnXCSI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtZZJpJr8MBm",
        "outputId": "e2b16ef8-77bd-4f71-8a2b-f3a47995353f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 16,\n",
              " 'num_epochs': 100,\n",
              " 'lr': 0.0001,\n",
              " 'seq_len': 370,\n",
              " 'd_model': 512,\n",
              " 'datasource': 'NikitiusIvanov/protein_seq_to_go_bio_process',\n",
              " 'lang_src': 'sequence',\n",
              " 'lang_tgt': 'go_process',\n",
              " 'model_folder': 'pytorch-transformer/weights',\n",
              " 'model_basename': 'tmodel_',\n",
              " 'preload': None,\n",
              " 'tokenizer_file': 'pytorch-transformer/vocab/tokenizer_{0}.json',\n",
              " 'experiment_name': 'runs/tmodel'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "9IdkTSK1_EBj",
        "outputId": "87194f44-cd5d-4b8b-8bb6-80fd709c8a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Device name: Tesla T4\n",
            "Device memory: 14.74786376953125 GB\n",
            "run get_ds\n",
            "Max length of source sentence: 361\n",
            "Max length of target sentence: 181\n",
            "No model to preload, starting from scratch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 00: 100%|██████████| 733/733 [10:01<00:00,  1.22it/s, loss=8.157]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run validation\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: MAFRRRTKSYPLFSQEFVIHNHADIGFCLVLCVLIGLMFEVTAKTAFLFILPQYNISVPTADSETVHYHYGPKDLVTILFYIFITIILHAVVQEYILDKISKRLHLSKVKHSKFNESGQLVVFHFTSVIWCFYVVVTEGYLTNPRSLWEDYPHVHLPFQVKFFYLCQLAYWLHALPELYFQKVRKEEIPRQLQYICLYLVHIAGAYLLNLSRLGLILLLLQYSTEFLFHTARLFYFADENNEKLFSAWAAVFGVTRLFILTLAVLAIGFGLARMENQAFDPEKGNFNTLFCRLCVLLLVCAAQAWLMWRFIHSQLRHWREYWNEQSAKRRVPATPRLPARLIKRESGYHENGVVKAENGTSPRTKKLKSP\n",
            "    TARGET: collagen biosynthetic process; protein insertion into ER membrane; SRP-dependent cotranslational protein targeting to membrane, translocation\n",
            " PREDICTED: germinal ST germinal ST germinal ST germinal ST germinal ST germinal ST germinal ST germinal ST germinal germinal ST germinal ST germinal ST germinal ST germinal ST germinal ST germinal ST germinal ST germinal ST germinal ST germinal germinal germinal germinal ST germinal ST germinal germinal germinal germinal germinal germinal ST germinal ST germinal ST riboside er germinal ST germinal IRES germinal ST germinal ST germinal ST germinal ST germinal riboside riboside riboside riboside riboside er er er er germinal IRES er germinal IRES er er germinal IRES er hel hel hel hel hel hel hel hel hel hel hel hel er er er er er hel hel hel hel hel er er er germinal ST germinal er er germinal er germinal ST germinal ST germinal ST germinal ST germinal ST germinal germinal germinal germinal IRES er germinal ST germinal riboside er germinal riboside unwinding IRES target unwinding target unwinding ST germinal ST germinal germinal germinal riboside unwinding germinal germinal riboside unwinding germinal riboside unwinding germinal IRES germinal germinal germinal germinal germinal germinal IRES germinal IRES germinal riboside unwinding germinal riboside unwinding IRES germinal ST ST ST riboside unwinding IRES germinal IRES germinal unwinding unwinding unwinding unwinding germinal ST germinal ST germinal ST germinal riboside unwinding germinal stran unwinding germinal ST germinal ST ST ST germinal ST germinal ST germinal ST germinal ST germinal germinal germinal germinal germinal germinal IRES germinal ST germinal ST germinal ST germinal ST germinal ST ST ST ST ST ST germinal ST er er er er er er er er er germinal er germinal er er er er germinal riboside unwinding germinal IRES germinal IRES germinal germinal germinal germinal germinal germinal germinal er hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel er er er hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel germinal germinal germinal riboside er er er er germinal germinal germinal germinal germinal er er hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: MAVPPGHGPFSGFPGPQEHTQVLPDVRLLPRRLPLAFRDATSAPLRKLSVDLIKTYKHINEVYYAKKKRRAQQAPPQDSSNKKEKKVLNHGYDDDNHDYIVRSGERWLERYEIDSLIGKGSFGQVVKAYDHQTQELVAIKIIKNKKAFLNQAQIELRLLELMNQHDTEMKYYIVHLKRHFMFRNHLCLVFELLSYNLYDLLRNTHFRGVSLNLTRKLAQQLCTALLFLATPELSIIHCDLKPENILLCNPKRSAIKIVDFGSSCQLGQRIYQYIQSRFYRSPEVLLGTPYDLAIDMWSLGCILVEMHTGEPLFSGSNEVDQMNRIVEVLGIPPAAMLDQAPKARKYFERLPGGGWTLRRTKELRKDLVLRMLEYEPAARISPLGALQHGFFRRTADEATNTGPAGSSASTSPAPLDTCPSSSTASSISSSGGSSGSSSDNRTYRYSNRYCGGPGPPITDCEMNSPQVPPSQPLRPWAGGDVPHKTHQAPASASSLPGTGAQLPPQPRYLGRPPSPTSPPPPELMDVSLVGGPADCSPPHPAPAPQHPAASALRTRMTGGRPPLPPPDDPATLGPHLGLRGVPQSTAASS\n",
            "    TARGET: protein autophosphorylation\n",
            " PREDICTED: ing depolymerization depolymerization depolymerization depolymerization germinal depolymerization germinal depolymerization germinal depolymerization germinal depolymerization germinal depolymerization germinal depolymerization germinal depolymerization germinal depolymerization germinal depolymerization germinal depolymerization germinal depolymerization germinal depolymerization germinal depolymerization germinal depolymerization germinal depolymerization germinal depolymerization germinal depolymerization germinal depolymerization germinal depolymerization germinal germinal germinal germinal germinal germinal germinal depolymerization germinal depolymerization riboside er germinal riboside er germinal riboside er germinal riboside er hel yn riboside er yn riboside er riboside riboside riboside riboside er er er er yn riboside er yn riboside er yn riboside er target riboside er yn riboside er target target target target target target target target target target target target target target target target target target target target target target target target target target target target target target riboside er germinal target target target target unwinding germinal pi germinal riboside er germinal riboside target target target target target target target target target target riboside er germinal depolymerization target riboside er germinal riboside germinal riboside germinal riboside riboside er germinal riboside germinal depolymerization germinal depolymerization germinal depolymerization germinal riboside germinal riboside germinal riboside germinal depolymerization target riboside er yn riboside er yn riboside riboside yn riboside yn riboside target unwinding riboside depolymerization target riboside yn riboside er yn riboside target riboside target target target target target target target target riboside er germinal riboside er hel target target target riboside germinal germinal target target target target target target target target target target target target target target tron germinal depolymerization riboside er hel yn riboside er er er er er er er er er er er er er er target riboside target riboside target target target target target target target target target target target target target target riboside er hel hel hel hel hel hel hel hel hel hel riboside er er er germinal er healing riboside er er er hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel germinal riboside er hel hel hel hel hel germinal riboside germinal germinal depolymerization germinal er hel hel hel hel hel hel hel hel hel hel hel hel hel hel hel\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Epoch 01:   0%|          | 1/733 [00:01<16:04,  1.32s/it, loss=8.157]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-f386b18ca283>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-6b84de2b5d1e>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;31m# Run the tensors through the encoder, decoder and the projection layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mencoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mproj_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, seq_len, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-61d40e9a1acc>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, encoder_output, src_mask, tgt, tgt_mask)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgt_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-61d40e9a1acc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, encoder_output, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-61d40e9a1acc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, encoder_output, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual_connections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attention_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual_connections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attention_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual_connections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_block\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-61d40e9a1acc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, sublayer)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msublayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMultiHeadAttentionBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-61d40e9a1acc>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual_connections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attention_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual_connections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attention_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual_connections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_block\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-61d40e9a1acc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# Calculate attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadAttentionBlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Combine all the heads together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-61d40e9a1acc>\u001b[0m in \u001b[0;36mattention\u001b[0;34m(query, key, value, mask, dropout)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# Just apply the formula from the paper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;31m# Write a very low value (indicating -inf) to the positions where mask == 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 68.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 44.81 MiB is free. Process 43687 has 14.70 GiB memory in use. Of the allocated memory 12.58 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop debug"
      ],
      "metadata": {
        "id": "o5bwUmQ1-CIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = get_config()"
      ],
      "metadata": {
        "id": "_BR1MeRRqvxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config['preload'] = 'latest'"
      ],
      "metadata": {
        "id": "Bda330NkcXhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_model(config):\n",
        "# Define the device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "if (device == 'cuda'):\n",
        "    print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
        "    print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
        "elif (device == 'mps'):\n",
        "    print(f\"Device name: <mps>\")\n",
        "else:\n",
        "    print(\"NOTE: If you have a GPU, consider using it for training.\")\n",
        "    print(\"      On a Windows machine with NVidia GPU, check this video: https://www.youtube.com/watch?v=GMSjDTU8Zlc\")\n",
        "    print(\"      On a Mac machine, run: pip3 install --pre torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/nightly/cpu\")\n",
        "device = torch.device(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxoI3NYH-YnF",
        "outputId": "f27b0ed1-0ae2-4e01-daa1-321c2a817df5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Device name: Tesla T4\n",
            "Device memory: 14.74786376953125 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure the weights folder exists\n",
        "Path(f\"{config['model_folder']}\").mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "yAUqVVn--vZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n"
      ],
      "metadata": {
        "id": "NbBUblRf-15d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "452459d8-cdcd-4125-842c-3a6e80b7c8d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length of source sentence: 361\n",
            "Max length of target sentence: 181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)"
      ],
      "metadata": {
        "id": "qK3thGjL59kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorboard\n",
        "writer = SummaryWriter(config['experiment_name'])\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n"
      ],
      "metadata": {
        "id": "L18QCLmK-9Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If the user specified a model to preload before training, load it\n",
        "initial_epoch = 0\n",
        "global_step = 0\n"
      ],
      "metadata": {
        "id": "_6K7hPTV_Hqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preload = config['preload']"
      ],
      "metadata": {
        "id": "rLF79KnsctMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preload = None"
      ],
      "metadata": {
        "id": "hhTD97Rc6RaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n"
      ],
      "metadata": {
        "id": "p7MrPYyd-sJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_filename"
      ],
      "metadata": {
        "id": "1MB_eIRD_baJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if model_filename:\n",
        "    print(f'Preloading model {model_filename}')\n",
        "    state = torch.load(model_filename)\n",
        "    model.load_state_dict(state['model_state_dict'])\n",
        "    initial_epoch = state['epoch'] + 1\n",
        "    optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "    global_step = state['global_step']\n",
        "else:\n",
        "    print('No model to preload, starting from scratch')\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FVH7hNy-Oac",
        "outputId": "71b716f1-aecd-48e8-bbe7-ab99843e98ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No model to preload, starting from scratch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# for epoch in range(initial_epoch, config['num_epochs']):\n",
        "#     torch.cuda.empty_cache()\n",
        "#     model.train()\n",
        "#     batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
        "#     for batch in batch_iterator:\n"
      ],
      "metadata": {
        "id": "rbnVjcLgrMXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 0\n",
        "torch.cuda.empty_cache()\n",
        "model.train()\n",
        "batch_iterator = train_dataloader.__iter__()"
      ],
      "metadata": {
        "id": "hQwLJlRBrNjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(batch_iterator)"
      ],
      "metadata": {
        "id": "KCv9Nax6rmRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n",
        "decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
        "encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
        "decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n"
      ],
      "metadata": {
        "id": "JApcEqsgrqW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the tensors through the encoder, decoder and the projection layer\n",
        "encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
        "decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
        "proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n"
      ],
      "metadata": {
        "id": "dluWMErFrswL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the output with the label\n",
        "label = batch['label'].to(device) # (B, seq_len)"
      ],
      "metadata": {
        "id": "voFhTdJIGtuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom loss debug"
      ],
      "metadata": {
        "id": "lg_sJ10WGwp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the loss using a simple cross entropy\n",
        "loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "# batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})"
      ],
      "metadata": {
        "id": "G_3rW62IGofn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############ split the target seq ############\n",
        "SEQ_LEN = config['seq_len']\n",
        "\n",
        "EOS_ID = tokenizer_tgt.token_to_id('[EOS]')\n",
        "\n",
        "SEP_ID = tokenizer_tgt.token_to_id(';')\n",
        "\n",
        "PAD_ID = tokenizer_tgt.token_to_id('[PAD]')\n",
        "\n",
        "UNK_ID = tokenizer_tgt.token_to_id('[UNK]')\n",
        "\n",
        "VOCAB_SIZE = tokenizer_tgt.get_vocab_size()\n",
        "\n",
        "label_splitted = []"
      ],
      "metadata": {
        "id": "0NLinstMG3Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(label)):\n",
        "    item_splitted = []\n",
        "    item = label[i]\n",
        "    sub_seqs_ends = torch.where(\n",
        "        (item == SEP_ID)\n",
        "        |\n",
        "        (item == EOS_ID)\n",
        "    )[0]\n",
        "    start_idx = 0\n",
        "    for end_idx in sub_seqs_ends:\n",
        "        subitem = item[start_idx: end_idx]\n",
        "        if (subitem == PAD_ID).all() == False:\n",
        "            end_seq = torch.ones(config['seq_len'] - len(subitem), dtype=torch.int64).to(device)\n",
        "            end_seq[0] = EOS_ID\n",
        "            subitem = torch.cat(\n",
        "                [\n",
        "                    subitem,\n",
        "                    end_seq\n",
        "                ]\n",
        "            ).to(device)\n",
        "\n",
        "            item_splitted.append(subitem)\n",
        "\n",
        "            start_idx = end_idx + 1\n",
        "\n",
        "    label_splitted.append(item_splitted)\n"
      ],
      "metadata": {
        "id": "nGjV_I2wG8QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item.shape, SEQ_LEN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFU_lsYdIg6v",
        "outputId": "bb1e1b49-eae4-40d6-f3dd-e401b51ab966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([400, 3486]), 400)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eos_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K79o3FjiJxX3",
        "outputId": "eb8ba7bf-ed49-46a0-b7e4-9d611956ab20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(65, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############ split the projection layer output ############\n",
        "proj_output_splitted = []\n",
        "\n",
        "for i in range(len(proj_output)):\n",
        "\n",
        "    item = proj_output[i]\n",
        "\n",
        "    eos_ids = (\n",
        "        (\n",
        "            torch.max(\n",
        "                item,\n",
        "                dim=1\n",
        "            )[1] == EOS_ID\n",
        "        )\n",
        "        .nonzero()\n",
        "        .squeeze(1)\n",
        "    )\n",
        "    if len(eos_ids) > 0:\n",
        "\n",
        "      eos_row_id = int(eos_ids[0])\n",
        "    else:\n",
        "      eos_row_id = SEQ_LEN\n",
        "\n",
        "    sub_seqs_ends = (\n",
        "        (\n",
        "            (\n",
        "                torch.max(\n",
        "                    item[:eos_row_id + 1], dim=1\n",
        "                )[1] == SEP_ID\n",
        "            )\n",
        "            |\n",
        "            (\n",
        "                torch.max(\n",
        "                    item[:eos_row_id + 1], dim=1\n",
        "                )[1] == EOS_ID\n",
        "            )\n",
        "        )\n",
        "        .nonzero()\n",
        "        .squeeze(1)\n",
        "    ).to(device)\n",
        "\n",
        "    item_splitted = []\n",
        "\n",
        "    start_idx = 0\n",
        "    for end_idx in sub_seqs_ends:\n",
        "\n",
        "        subitem = item[start_idx: end_idx]\n",
        "\n",
        "        # create appendix with ones\n",
        "        end_seq = torch.ones(\n",
        "            size=(\n",
        "                SEQ_LEN - subitem.shape[0],\n",
        "                VOCAB_SIZE\n",
        "            )\n",
        "        ).to(device)\n",
        "\n",
        "        # create a row with maximum element\n",
        "        # which index corresponded eos id\n",
        "        end_seq[0, EOS_ID] = 2\n",
        "\n",
        "        # all following rows fill with pads\n",
        "        end_seq[1:, PAD_ID] = 2\n",
        "\n",
        "        subitem = torch.cat(\n",
        "            [\n",
        "                subitem,\n",
        "                end_seq\n",
        "            ]\n",
        "        ).to(device)\n",
        "\n",
        "        item_splitted.append(subitem)\n",
        "\n",
        "        start_idx = end_idx + 1\n",
        "\n",
        "    diff_lenght = len(item_splitted) - len(label_splitted[i])\n",
        "\n",
        "    # if lenght splitted predict less than splitted fact\n",
        "    # fill difference in predict with pads\n",
        "    if diff_lenght < 0:\n",
        "        for n in range(-diff_lenght ):\n",
        "            # create appendix with ones\n",
        "            end_seq = torch.ones(\n",
        "                size=(\n",
        "                    SEQ_LEN,\n",
        "                    VOCAB_SIZE\n",
        "                )\n",
        "            ).to(device)\n",
        "\n",
        "            # all following rows fill with pads\n",
        "            end_seq[:, UNK_ID] = 2\n",
        "\n",
        "            item_splitted.append(end_seq)\n",
        "\n",
        "    # if lenght splitted predict more than splitted fact\n",
        "    # fill difference in fact with pads\n",
        "    if diff_lenght > 0:\n",
        "        for n in range(diff_lenght ):\n",
        "            # create appendix with ones\n",
        "            end_seq = torch.tensor([UNK_ID] * SEQ_LEN).to(device)\n",
        "\n",
        "            label_splitted[i].append(end_seq)\n",
        "\n",
        "    proj_output_splitted.append(item_splitted)\n"
      ],
      "metadata": {
        "id": "neY5py8-HKrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "############ calculate losses for all pairs sublabels ############\n",
        "losses = []\n",
        "min_losses = {}\n",
        "\n",
        "for label_idx in range(len(label_splitted)):\n",
        "    label_losses = {}\n",
        "    min_losses[label_idx] = {}\n",
        "\n",
        "    for sub_label_idx in range(len(label_splitted[label_idx])):\n",
        "        min_loss = torch.inf\n",
        "        label_losses[sub_label_idx] = {}\n",
        "\n",
        "        for sub_pred_label_idx in range(len(proj_output_splitted[label_idx])):\n",
        "\n",
        "            loss = loss_fn(\n",
        "                proj_output_splitted[label_idx][sub_pred_label_idx],\n",
        "                label_splitted[label_idx][sub_label_idx]\n",
        "            )\n",
        "\n",
        "            if loss < min_loss:\n",
        "                min_loss = loss\n",
        "                min_loss_idx = sub_pred_label_idx\n",
        "\n",
        "            label_losses[sub_label_idx][sub_pred_label_idx] = loss\n",
        "\n",
        "        min_losses[label_idx][sub_label_idx] = min_loss_idx\n",
        "\n",
        "    losses.append(label_losses)\n",
        "\n",
        "############ find collisions  ############\n",
        "optimal_losses = []\n",
        "checked_items = []\n",
        "collision_items = []\n",
        "for item_idx in range(len(label_splitted)):\n",
        "\n",
        "    min_stats = {\n",
        "        x: (list(min_losses[item_idx].values())).count(x)\n",
        "        for x in list(set(list(min_losses[item_idx].values())))\n",
        "    }\n",
        "\n",
        "    for subitem_idx in range(len(label_splitted[item_idx])):\n",
        "        # if predicted sublabel return minimal loss for only one sublabel\n",
        "        if min_stats[min_losses[item_idx][subitem_idx]] == 1:\n",
        "            optimal_losses.append(\n",
        "                losses[item_idx][subitem_idx][min_losses[item_idx][subitem_idx]]\n",
        "            )\n",
        "\n",
        "            checked_items.append((item_idx, subitem_idx))\n",
        "\n",
        "        # if predicted sublabel return minimal loss for more than one sublabel\n",
        "        else:\n",
        "            collision_items.append((item_idx, subitem_idx))\n",
        "\n",
        "############ resolve collisions  ############\n",
        "while len(collision_items) > 0:\n",
        "\n",
        "    label_idx, sub_label_idx = collision_items[0]\n",
        "\n",
        "    # print(f'collision_item: {label_idx, sub_label_idx}')\n",
        "\n",
        "    order = losses[label_idx][sub_label_idx].copy()\n",
        "\n",
        "    cur_min = min_losses[label_idx][sub_label_idx]\n",
        "\n",
        "    # print(f'cur_min_idx: {cur_min}')\n",
        "\n",
        "    try: del order[cur_min]\n",
        "    except: pass\n",
        "\n",
        "    new_min = (\n",
        "        list(\n",
        "            order.keys()\n",
        "        )\n",
        "        [\n",
        "            list(order.values())\n",
        "            .index(\n",
        "                min(list(order.values()))\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    min_losses[label_idx][sub_label_idx] = new_min\n",
        "\n",
        "    # print(f'new_min_idx: {new_min}')\n",
        "\n",
        "    # Check loss statistics for choosen subseq\n",
        "    min_stats = {\n",
        "        x: (list(min_losses[label_idx].values())).count(x)\n",
        "        for x in list(set(list(min_losses[label_idx].values())))\n",
        "    }\n",
        "\n",
        "    # if we choose new predicted sub_seq with that return minimum loss\n",
        "    # for more than one true sub_seq try search next one\n",
        "    # untill we find only exclusive pair\n",
        "    while min_stats[new_min] > 1:\n",
        "\n",
        "        cur_min = new_min\n",
        "\n",
        "        try: del order[cur_min]\n",
        "        except: pass\n",
        "\n",
        "        new_min = (\n",
        "            list(\n",
        "                order.keys()\n",
        "            )\n",
        "            [\n",
        "                list(order.values())\n",
        "                .index(\n",
        "                    min(list(order.values()))\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        min_losses[label_idx][sub_label_idx] = new_min\n",
        "\n",
        "        # Check statistics\n",
        "        min_stats = {\n",
        "            x: (list(min_losses[label_idx].values())).count(x)\n",
        "            for x in list(set(list(min_losses[label_idx].values())))\n",
        "        }\n",
        "\n",
        "    # print(f'new_min: {new_min}')\n",
        "\n",
        "    # if there no more collisions in sublabels\n",
        "    # del all their items from list of collisions\n",
        "    if max(list(min_stats.values())) == 1:\n",
        "        uncollision_items = [\n",
        "            x for x in collision_items\n",
        "            if x[0] == label_idx\n",
        "        ]\n",
        "        # print('uncolided items:')\n",
        "        for item in uncollision_items:\n",
        "\n",
        "            # print(f'{item}')\n",
        "\n",
        "            del collision_items[collision_items.index(item)]\n",
        "\n",
        "            optimal_losses.append(\n",
        "                losses[item[0]][item[1]][min_losses[item[0]][item[1]]]\n",
        "            )\n",
        "        #continue\n",
        "\n",
        "    # if in selected label there is another collision just del current\n",
        "    # collision and go next\n",
        "    else:\n",
        "        # print(f'uncolided item: {label_idx, sub_label_idx}')\n",
        "        del collision_items[collision_items.index((label_idx, sub_label_idx))]\n",
        "\n",
        "        del order[new_min]\n",
        "\n",
        "        optimal_losses.append(\n",
        "            losses[label_idx][sub_label_idx][min_losses[label_idx][sub_label_idx]]\n",
        "        )\n",
        "\n",
        "custom_loss = torch.mean(torch.tensor(optimal_losses, requires_grad=True)).to(device)\n",
        "custom_loss.retain_grad()\n"
      ],
      "metadata": {
        "id": "ScrySB8IGhu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gjlpr2jKBxv",
        "outputId": "a67c09d7-e322-4fc5-d7da-5cff271efa50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(8.1428, device='cuda:0', grad_fn=<ToCopyBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Log the loss\n",
        "writer.add_scalar('train loss', loss.item(), global_step)\n",
        "writer.flush()\n",
        "\n",
        "# Backpropagate the loss\n",
        "loss.backward()\n",
        "\n",
        "# Update the weights\n",
        "optimizer.step()\n",
        "optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "global_step += 1"
      ],
      "metadata": {
        "id": "86i5DLgorkaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0g7vR9ZcRY4",
        "outputId": "2f3a0915-5291-488a-80f0-9f21634147e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x EncoderBlock(\n",
              "        (self_attention_block): MultiHeadAttentionBlock(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward_block): FeedForwardBlock(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (residual_connections): ModuleList(\n",
              "          (0-1): 2 x ResidualConnection(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (norm): LayerNormalization()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNormalization()\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x DecoderBlock(\n",
              "        (self_attention_block): MultiHeadAttentionBlock(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (cross_attention_block): MultiHeadAttentionBlock(\n",
              "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward_block): FeedForwardBlock(\n",
              "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (residual_connections): ModuleList(\n",
              "          (0-2): 3 x ResidualConnection(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (norm): LayerNormalization()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNormalization()\n",
              "  )\n",
              "  (src_embed): InputEmbeddings(\n",
              "    (embedding): Embedding(25203, 512)\n",
              "  )\n",
              "  (tgt_embed): InputEmbeddings(\n",
              "    (embedding): Embedding(3486, 512)\n",
              "  )\n",
              "  (src_pos): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (tgt_pos): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (projection_layer): ProjectionLayer(\n",
              "    (proj): Linear(in_features=512, out_features=3486, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run validation at the end of every epoch\n",
        "run_validation(\n",
        "    model,\n",
        "    val_dataloader,\n",
        "    tokenizer_src,\n",
        "    tokenizer_tgt,\n",
        "    config['seq_len'],\n",
        "    device,\n",
        "    print_msg=None,\n",
        "    global_step=None, writer=None)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "CHLFn8nksNkV",
        "outputId": "f051dbbf-a278-43e5-a04a-20210cd2dd7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-bd9ca9a3a95c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run validation at the end of every epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m run_validation(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtokenizer_src\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-aa7de1942560>\u001b[0m in \u001b[0;36mrun_validation\u001b[0;34m(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, writer, num_examples)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Print the source, target and model output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mprint_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconsole_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mprint_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{f'SOURCE: ':>12}{source_text}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mprint_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{f'TARGET: ':>12}{target_text}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save the model at the end of every epoch\n",
        "model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
        "torch.save({\n",
        "    'epoch': epoch,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'global_step': global_step\n",
        "}, model_filename)\n"
      ],
      "metadata": {
        "id": "K2BwzwXm9kje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run validation debug"
      ],
      "metadata": {
        "id": "L82Mjxxf_70O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, writer, num_examples=2):\n",
        "model.eval()\n",
        "count = 0\n"
      ],
      "metadata": {
        "id": "z82S_UOBATWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_examples=2\n",
        "max_len = config['seq_len']"
      ],
      "metadata": {
        "id": "FFd6WA7n9LOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_ds = val_dataloader"
      ],
      "metadata": {
        "id": "IR8upYWm8i9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_texts = []\n",
        "expected = []\n",
        "predicted = []\n",
        "\n",
        "try:\n",
        "    # get the console window width\n",
        "    with os.popen('stty size', 'r') as console:\n",
        "        _, console_width = console.read().split()\n",
        "        console_width = int(console_width)\n",
        "except:\n",
        "    # If we can't get the console width, use 80 as default\n",
        "    console_width = 80\n"
      ],
      "metadata": {
        "id": "UFyACasjAiBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    for batch in validation_ds:\n",
        "        count += 1\n",
        "        encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
        "        encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)\n",
        "\n",
        "        # check that the batch size is 1\n",
        "        assert encoder_input.size(\n",
        "            0) == 1, \"Batch size must be 1 for validation\"\n",
        "\n",
        "        model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
        "\n",
        "        source_text = batch[\"src_text\"][0]\n",
        "        target_text = batch[\"tgt_text\"][0]\n",
        "        model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
        "\n",
        "        source_texts.append(source_text)\n",
        "        expected.append(target_text)\n",
        "        predicted.append(model_out_text)\n",
        "\n",
        "        # Print the source, target and model output\n",
        "        print('-'*console_width)\n",
        "        print(f\"{f'SOURCE: ':>12}{source_text}\")\n",
        "        print(f\"{f'TARGET: ':>12}{target_text}\")\n",
        "        print(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
        "\n",
        "        if count == num_examples:\n",
        "            print('-'*console_width)\n",
        "            break\n",
        "\n"
      ],
      "metadata": {
        "id": "0uQqfQiV_7ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e00ecbfa-a01f-4ee8-f2c3-0b2db1cff932"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: MSEEIITPVYCTGVSAQVQKQRARELGLGRHENAIKYLGQDYEQLRVRCLQSGTLFRDEAFPPVPQSLGYKDLGPNSSKTYGIKWKRPTELLSNPQFIVDGATRTDICQGALGDCWLLAAIASLTLNDTLLHRVVPHGQSFQNGYAGIFHFQLWQFGEWVDVVVDDLLPIKDGKLVFVHSAEGNEFWSALLEKAYAKVNGSYEALSGGSTSEGFEDFTGGVTEWYELRKAPSDLYQIILKALERGSLLGCSIDISSVLDMEAITFKKLVKGHAYSVTGAKQVNYRGQVVSLIRMRNPWGEVEWTGAWSDSSSEWNNVDPYERDQLRVKMEDGEFWMSFRDFMREFTRLEICNLTPDALKSRTIRKWNTTLYEGTWRRGSTAGGCRNYPATFWVNPQFKIRLDETDDPDDYGDRESGCSFVLALMQKHRRRERRFGRDMETIGFAVYEVPPELVGQPAVHLKRDFFLANASRARSEQFINLREVSTRFRLPPGEYVVVPSTFEPNKEGDFVLRFFSEKSAGTVELDDQIQANLPDEQVLSEEEIDENFKALFRQLAGEDMEISVKELRTILNRIISKHKDLRTKGFSLESCRSMVNLMDRDGNGKLGLVEFNILWNRIRNYLSIFRKFDLDKSGSMSAYEMRMAIESAGFKLNKKLYELIITRYSEPDLAVDFDNFVCCLVRLETMFRFFKTLDTDLDGVVTFDLFKWLQLTMFA\n",
            "    TARGET: mammary gland involution; positive regulation of cell population proliferation; proteolysis; receptor catabolic process; regulation of catalytic activity; regulation of macroautophagy; regulation of NMDA receptor activity; self proteolysis\n",
            " PREDICTED: cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle\n",
            "--------------------------------------------------------------------------------\n",
            "    SOURCE: MAEPASVAAESLAGSRARAARTVLGQVVLPGEELLLPEQEDAEGPGGAVERPLSLNARACSRVRVVCGPGLRRCGDRLLVTKCGRLRHKEPGSGSGGGVYWVDSQQKRYVPVKGDHVIGIVTAKSGDIFKVDVGGSEPASLSYLSFEGATKRNRPNVQVGDLIYGQFVVANKDMEPEMVCIDSCGRANGMGVIGQDGLLFKVTLGLIRKLLAPDCEIIQEVGKLYPLEIVFGMNGRIWVKAKTIQQTLILANILEACEHMTSDQRKQIFSRLAES\n",
            "    TARGET: CUT catabolic process; DNA deamination; exonucleolytic catabolism of deadenylated mRNA; exonucleolytic trimming to generate mature 3'-end of 5.8S rRNA from tricistronic rRNA transcript (SSU-rRNA, 5.8S rRNA, LSU-rRNA); isotype switching; nuclear polyadenylation-dependent rRNA catabolic process; nuclear polyadenylation-dependent tRNA catabolic process; nuclear-transcribed mRNA catabolic process, exonucleolytic, 3'-5'; polyadenylation-dependent snoRNA 3'-end processing; positive regulation of isotype switching; RNA catabolic process; RNA processing; rRNA processing; U4 snRNA 3'-end processing\n",
            " PREDICTED: cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzOwH92cdrn1",
        "outputId": "8d583ef7-78af-4c5d-b7f1-61b600f0bce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle',\n",
              " 'cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle ; regulation of cell cycle']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "expected"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv21OzJJduuY",
        "outputId": "4016fb62-4720-4237-bcef-c4478e0a9860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mammary gland involution; positive regulation of cell population proliferation; proteolysis; receptor catabolic process; regulation of catalytic activity; regulation of macroautophagy; regulation of NMDA receptor activity; self proteolysis',\n",
              " \"CUT catabolic process; DNA deamination; exonucleolytic catabolism of deadenylated mRNA; exonucleolytic trimming to generate mature 3'-end of 5.8S rRNA from tricistronic rRNA transcript (SSU-rRNA, 5.8S rRNA, LSU-rRNA); isotype switching; nuclear polyadenylation-dependent rRNA catabolic process; nuclear polyadenylation-dependent tRNA catabolic process; nuclear-transcribed mRNA catabolic process, exonucleolytic, 3'-5'; polyadenylation-dependent snoRNA 3'-end processing; positive regulation of isotype switching; RNA catabolic process; RNA processing; rRNA processing; U4 snRNA 3'-end processing\"]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if writer:\n",
        "# Evaluate the character error rate\n",
        "# Compute the char error rate\n",
        "metric = torchmetrics.text.CharErrorRate()\n",
        "cer = metric(predicted, expected)\n",
        "writer.add_scalar('validation cer', cer, global_step)\n",
        "writer.flush()\n",
        "\n",
        "# Compute the word error rate\n",
        "metric = torchmetrics.text.WordErrorRate()\n",
        "wer = metric(predicted, expected)\n",
        "writer.add_scalar('validation wer', wer, global_step)\n",
        "writer.flush()\n",
        "\n",
        "# Compute the BLEU metric\n",
        "metric = torchmetrics.text.BLEUScore()\n",
        "bleu = metric(predicted, expected)\n",
        "writer.add_scalar('validation BLEU', bleu, global_step)\n",
        "writer.flush()"
      ],
      "metadata": {
        "id": "2JMlNRtj9BGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cer, wer, bleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Q-FOT8bd79o",
        "outputId": "19f5bc04-1fff-4ca9-9cb6-1c7bbd650f44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(1.2311), tensor(2.7609), tensor(0.))"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Debug greedy decode"
      ],
      "metadata": {
        "id": "GROw23nNBGwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)"
      ],
      "metadata": {
        "id": "9wXO-x_kBa8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = val_ds[0]"
      ],
      "metadata": {
        "id": "tjCZE5sGBv4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = batch[\"encoder_input\"].to(device) # (b, seq_len)\n",
        "encoder_mask = batch[\"encoder_mask\"].to(device) # (b, 1, 1, seq_len)"
      ],
      "metadata": {
        "id": "Adx2nW0bBeeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
        "sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
        "eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n"
      ],
      "metadata": {
        "id": "4TQm5E7pBQzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source = encoder_input\n",
        "source_mask = encoder_mask"
      ],
      "metadata": {
        "id": "zBxZElD2Cxey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Precompute the encoder output and reuse it for every step\n",
        "encoder_output = model.encode(source, source_mask)"
      ],
      "metadata": {
        "id": "Lu6zliy2C9p2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-RYFLyTDCEd",
        "outputId": "478c6419-d4ba-4aa7-de26-ab7157cb188f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1318,  0.2228, -0.1117,  ...,  0.2391, -0.1218, -0.4987],\n",
              "         [ 0.1752,  0.1941, -0.0727,  ...,  0.2105, -0.1081, -0.4962],\n",
              "         [ 0.1814,  0.1903, -0.0675,  ...,  0.2199, -0.1120, -0.5122],\n",
              "         ...,\n",
              "         [ 0.1550,  0.2167, -0.1046,  ...,  0.2522, -0.0928, -0.5195],\n",
              "         [ 0.1506,  0.1858, -0.0776,  ...,  0.2522, -0.0940, -0.5201],\n",
              "         [ 0.1235,  0.1759, -0.0769,  ...,  0.2546, -0.0882, -0.5206]]],\n",
              "       device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the decoder input with the sos token\n",
        "decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)"
      ],
      "metadata": {
        "id": "rhr18anzDPPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R657eB2VDQxT",
        "outputId": "dfb5d0b4-08a3-44c3-f251-50ff1e6a259c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 400"
      ],
      "metadata": {
        "id": "TRtcfp9QDaH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "while True:\n",
        "    if decoder_input.size(1) == max_len:\n",
        "        break\n",
        "\n",
        "    # build mask for target\n",
        "    decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
        "\n",
        "    # calculate output\n",
        "    out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
        "\n",
        "    # get next token\n",
        "    prob = model.project(out[:, -1])\n",
        "\n",
        "    _, next_word = torch.max(prob, dim=1)\n",
        "    print(next_word)\n",
        "    decoder_input = torch.cat(\n",
        "        [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
        "    )\n",
        "\n",
        "    if next_word == eos_idx:\n",
        "        break\n",
        "\n",
        "# return decoder_input.squeeze(0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGlGX-sQBJua",
        "outputId": "98d15687-4590-4b2c-f5ac-8d56e35098ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0], device='cuda:0')\n",
            "tensor([3], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, ind = torch.sort(model.project(out[:, -1]))\n"
      ],
      "metadata": {
        "id": "fGzM3-3SEgp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ind"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6QyR-Yb95d-",
        "outputId": "f8448902-a091-4b83-af7c-4550178e646c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 760, 6245, 2906,  ..., 1023,    0,    3]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_word = ind[0, -3]\n",
        "next_word\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1gaELvFEAl0",
        "outputId": "793d4a37-2390-4588-e371-e07a692ca536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1023, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)"
      ],
      "metadata": {
        "id": "xQHbS9MUH03W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input = torch.cat(\n",
        "        [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n",
        "    )"
      ],
      "metadata": {
        "id": "7mY-hIW1H5fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_out = decoder_input\n",
        "model_out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wioxwDcsIZM8",
        "outputId": "08fcfd98-dcf3-46ec-d6df-1963c5f243ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2, 34, 43, 32,  3]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_out.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "JUzwPO7EI1NH",
        "outputId": "9d81f588-2e3c-4441-ac04-744c79f4d260",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2, 34, 43, 32,  3]])"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_tgt.decode()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "U_uZAVaFHgJO",
        "outputId": "4c3d3ffd-0ecf-4a1b-d128-72fd9aec05b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-112-5ac1efd219dc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer_tgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: argument 'ids': only integer scalar arrays can be converted to a scalar index"
          ]
        }
      ]
    }
  ]
}