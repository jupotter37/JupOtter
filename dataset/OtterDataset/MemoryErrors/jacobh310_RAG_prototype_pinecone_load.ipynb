{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jacob\\Github\\RAG_prototype\\venv\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Env Varibale from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find .env automagically by walking up directories until it's found\n",
    "dotenv_path = find_dotenv()\n",
    "\n",
    "# load up the entries as environment variables\n",
    "load_dotenv(dotenv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Embedding Model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jacob\\Github\\RAG_prototype\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=embed_model_id,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'device': device, 'batch_size': 32}\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "# configure client\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "spec = ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and/or Connect to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_name = 'llama-2-fin-rag-proto'\n",
    "\n",
    "import time\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=384,\n",
    "        metric='cosine',\n",
    "        spec=spec\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 17268}},\n",
      " 'total_vector_count': 17268}\n"
     ]
    }
   ],
   "source": [
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "# view index stats\n",
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in HTML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_file_paths(directory):\n",
    "    file_paths = []\n",
    "    \n",
    "    # Walk the directory tree\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            # Construct the full file path and add it to the list\n",
    "            file_paths.append(os.path.join(root, file))\n",
    "    \n",
    "    return file_paths\n",
    "# Example usage\n",
    "directory_path = \"..\\data\\\\raw\\sec-edgar-filings\"\n",
    "file_paths = get_all_file_paths(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dics = []\n",
    "\n",
    "for path in file_paths:\n",
    "    \n",
    "    path_split = path.split('\\\\')\n",
    "    com = path_split[4]\n",
    "    report_type = path_split[5]\n",
    "    year = '20' + path_split[6].split('-')[1]\n",
    "\n",
    "    \n",
    "    meta_data = {\n",
    "                'Metadata':\n",
    "                 {'Company': com,\n",
    "                 'Report Type':report_type,\n",
    "                 'Year' : int(year)},\n",
    "                 'path' : path\n",
    "                 }\n",
    "    \n",
    "    meta_dics.append(meta_data)\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_html_as_text(file_path):\n",
    "\n",
    "    loader = UnstructuredHTMLLoader(file_path)\n",
    "\n",
    "    return loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(docs,chunk_size=600,chunk_overlap=100):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "    doc=text_splitter.split_documents(docs)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Loaded and Chunked:..\\data\\raw\\sec-edgar-filings\\AAPL\\10-K\\0000320193-19-000119\\full-submission.txt\n"
     ]
    }
   ],
   "source": [
    "text_path = file_paths[0]\n",
    "\n",
    "doc = load_html_as_text(text_path)\n",
    "split_docs = chunk_data(doc)\n",
    "\n",
    "print(f\"File Loaded and Chunked:{text_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document was split into 581 different chunks \n"
     ]
    }
   ],
   "source": [
    "# \n",
    "print(F\"Document was split into {len(split_docs)} different chunks \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "464"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNITED STATES\\n\\nSECURITIES AND EXCHANGE COMMISSION\\n\\nWashington, D.C. 20549\\n\\nFORM\\n\\n10-K\\n\\n(Mark One)\\n\\nANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\\n\\nFor the fiscal year ended\\n\\nSeptember 28, 2019\\n\\nor\\n\\nTRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\\n\\nFor the transition period from\\n\\nto\\n\\nCommission File Number:\\n\\n001-36743\\n\\nApple Inc.\\n\\n(Exact name of Registrant as specified in its charter)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jacob\\Github\\RAG_prototype\\venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "embeds = embed_model.embed_documents([chunk.page_content for chunk in split_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each of the 581 chunks is represented by a 384 vector\n"
     ]
    }
   ],
   "source": [
    "len(embeds), len(embeds[0])\n",
    "\n",
    "\n",
    "print(f\"Each of the {len(embeds)} chunks is represented by a {len(embeds[0])} vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize and Store in PineCone Vectore DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_into_pinecone = False \n",
    "\n",
    "if load_into_pinecone:\n",
    "\n",
    "    for i, filling in tqdm(enumerate(meta_dics), total=len(meta_dics)):\n",
    "\n",
    "        \n",
    "        \n",
    "        path = filling['path']\n",
    "\n",
    "        doc = load_html_as_text(path)\n",
    "        split_docs = chunk_data(doc)\n",
    "\n",
    "        embeds = embed_model.embed_documents([chunk.page_content for chunk in split_docs])\n",
    "        metadata = [filling['Metadata'] for i in range(len(embeds))]\n",
    "        metadata = [{**metadata[i], 'text': chunk.page_content}  for i,chunk in enumerate(split_docs)]\n",
    "\n",
    "        ids = [f\"{filling['Metadata']['Company']}-{filling['Metadata']['Report Type']}-{filling['Metadata']['Year']}-{i}\" for i in range(len(embeds))]\n",
    "\n",
    "        if len(ids)<1000:\n",
    "            index.upsert(vectors=zip(ids, embeds, metadata))\n",
    "        else:\n",
    "            index.upsert(vectors=zip(ids[:1000], embeds[:1000], metadata[:1000]))\n",
    "            index.upsert(vectors=zip(ids[1000:], embeds[1000:], metadata[1000:]))\n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': 'TSLA-10-K-2023-432', 'score': 0.771090925, 'values': []},\n",
       "             {'id': 'TSLA-10-K-2023-447', 'score': 0.701928496, 'values': []},\n",
       "             {'id': 'TSLA-10-K-2020-569', 'score': 0.692607939, 'values': []},\n",
       "             {'id': 'TSLA-10-K-2023-368', 'score': 0.6250965, 'values': []},\n",
       "             {'id': 'TSLA-10-K-2023-434', 'score': 0.618884504, 'values': []}],\n",
       " 'namespace': '',\n",
       " 'usage': {'read_units': 5}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is Tesla Revenue Growth in 2022\"\n",
    "\n",
    "vector_query= embed_model.embed_query(query)\n",
    "\n",
    "index.query(vector=vector_query, top_k=5, filter={\n",
    "        \"Company\": {\"$eq\":\"TSLA\"}\n",
    "    } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Company': 'TSLA', 'Report Type': '10-K', 'Year': 2023.0}, page_content='Services and other revenue increased $2.29 billion, or 60%, in the year ended December 31, 2022 as compared to the year ended December 31, 2021. The change is primarily due to increase in used vehicle revenue driven by increases in volume and average selling prices of used Tesla and non-Tesla vehicles, non-warranty maintenance services revenue as our fleet continues to grow, paid Supercharging revenue, insurance services revenue and retail merchandise revenue.\\n\\n37\\n\\nEnergy Generation and Storage Segment'),\n",
       " Document(metadata={'Company': 'TSLA', 'Report Type': '10-K', 'Year': 2023.0}, page_content='Cost of services and other revenue increased $1.97 billion, or 51%, in the year ended December 31, 2022 as compared to the year ended December 31, 2021. The change is primarily due to an increase in used vehicle cost of revenue driven by increases in volume and costs of used Tesla and non-Tesla vehicle sales, an increase in non-warranty maintenance service revenue, and an increase in costs of paid Supercharging, insurance services and retail merchandise.'),\n",
       " Document(metadata={'Company': 'TSLA', 'Report Type': '10-K', 'Year': 2020.0}, page_content='Services and other revenue increased $835\\xa0million, or 60%, in the year ended December\\xa031, 2019 as compared to the year ended December\\xa031, 2018. The increase was primarily due to an increase in used vehicle sales from an increased volume of trade-in vehicles, partially offset by lower average selling prices for traded-in Tesla vehicles due to price adjustments we made to our vehicle offerings in 2019. Additionally, there was an increase in non-warranty maintenance services revenue as our fleet continues to grow.\\n\\nEnergy Generation and Storage Segment')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "\n",
    "\n",
    "vectorstore = PineconeVectorStore(index, embed_model)\n",
    "\n",
    "vectorstore.similarity_search(\n",
    "    query,  # the search query\n",
    "    k=3 , # returns top 3 most relevant chunks of text\n",
    "    filter={\n",
    "        \"Company\": {\"$eq\":\"TSLA\"}}\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Hugging Face Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  50%|█████     | 1/2 [01:12<01:12, 72.02s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 172.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 34\u001b[0m\n\u001b[0;32m     26\u001b[0m hf_auth \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHUGGING_FACE_AUTH\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m model_config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     30\u001b[0m     model_id,\n\u001b[0;32m     31\u001b[0m     token\u001b[38;5;241m=\u001b[39mhf_auth\n\u001b[0;32m     32\u001b[0m )\n\u001b[1;32m---> 34\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# quantization_config=quanto_config,\u001b[39;49;00m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_auth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel loaded on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jacob\\Github\\RAG_prototype\\venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jacob\\Github\\RAG_prototype\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3838\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3829\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   3831\u001b[0m     (\n\u001b[0;32m   3832\u001b[0m         model,\n\u001b[0;32m   3833\u001b[0m         missing_keys,\n\u001b[0;32m   3834\u001b[0m         unexpected_keys,\n\u001b[0;32m   3835\u001b[0m         mismatched_keys,\n\u001b[0;32m   3836\u001b[0m         offload_index,\n\u001b[0;32m   3837\u001b[0m         error_msgs,\n\u001b[1;32m-> 3838\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[0;32m   3842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3845\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3846\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3849\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3850\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3855\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3857\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[0;32m   3858\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32mc:\\Users\\jacob\\Github\\RAG_prototype\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4298\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[0;32m   4294\u001b[0m                 set_module_tensor_to_device(\n\u001b[0;32m   4295\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   4296\u001b[0m                 )\n\u001b[0;32m   4297\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4298\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4299\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4300\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4301\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4302\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4303\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4304\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4305\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4306\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4307\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4308\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4309\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4310\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4311\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4312\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4313\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4314\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4315\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[0;32m   4316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jacob\\Github\\RAG_prototype\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:895\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[1;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[0;32m    884\u001b[0m     state_dict_index \u001b[38;5;241m=\u001b[39m offload_weight(param, param_name, state_dict_folder, state_dict_index)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m is_quantized\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m hf_quantizer\u001b[38;5;241m.\u001b[39mrequires_parameters_quantization)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    893\u001b[0m ):\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[1;32m--> 895\u001b[0m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    897\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[1;32mc:\\Users\\jacob\\Github\\RAG_prototype\\venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:404\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[1;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[0;32m    402\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 404\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    406\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 172.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "\n",
    "from torch import cuda, bfloat16\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, QuantoConfig, BitsAndBytesConfig, AutoConfig\n",
    "import transformers\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "hf_auth = os.environ.get(\"HUGGING_FACE_AUTH\")\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type='nf4',\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_compute_dtype=bfloat16\n",
    "# )\n",
    "\n",
    "quanto_config = QuantoConfig(\n",
    "    weights='int8'\n",
    "\n",
    ")\n",
    "\n",
    "# begin initializing HF items, need auth token for these\n",
    "hf_auth = os.environ.get(\"HUGGING_FACE_AUTH\")\n",
    "\n",
    "\n",
    "model_config = AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    token=hf_auth\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    # quantization_config=quanto_config,\n",
    "    token=hf_auth,\n",
    "    device_map=device\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jacob\\Github\\RAG_prototype\\venv\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:778: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    temperature=0.00,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
    "    repetition_penalty=1.1\n",
    "      # without this output begins repeating\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "vectorstore = PineconeVectorStore(index, embed_model)\n",
    "\n",
    "filter = {\"Company\": {\"$eq\":\"TSLA\"}}\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"filter\":filter, \"k\":20})\n",
    "\n",
    "\n",
    "\n",
    "custom_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "    You are an expert in finance who is ready to question answering tasks. Use the context below to answer the question. Use the following pieces of retrieved context to answer the question. \n",
    "    If you don't know the answer, just say that you don't know. Use five sentences maximum and keep the answer concise.\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "rag_pipeline = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type='map_reduce',\n",
    "    retriever=retriever,\n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Tesla's total revenue for 2020,2021,2022,2023\"\n",
    "\n",
    "response = rag_pipeline.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Services and other revenue increased $2.29 billion, or 60%, in the year ended December 31, 2022 as compared to the year ended December 31, 2021. The change is primarily due to increase in used vehicle revenue driven by increases in volume and average selling prices of used Tesla and non-Tesla vehicles, non-warranty maintenance services revenue as our fleet continues to grow, paid Supercharging revenue, insurance services revenue and retail merchandise revenue.\n",
      "\n",
      "37\n",
      "\n",
      "Energy Generation and Storage Segment\n",
      "\n",
      "Cost of services and other revenue increased $1.97 billion, or 51%, in the year ended December 31, 2022 as compared to the year ended December 31, 2021. The change is primarily due to an increase in used vehicle cost of revenue driven by increases in volume and costs of used Tesla and non-Tesla vehicle sales, an increase in non-warranty maintenance service revenue, and an increase in costs of paid Supercharging, insurance services and retail merchandise.\n",
      "\n",
      "Services and other revenue increased $835 million, or 60%, in the year ended December 31, 2019 as compared to the year ended December 31, 2018. The increase was primarily due to an increase in used vehicle sales from an increased volume of trade-in vehicles, partially offset by lower average selling prices for traded-in Tesla vehicles due to price adjustments we made to our vehicle offerings in 2019. Additionally, there was an increase in non-warranty maintenance services revenue as our fleet continues to grow.\n",
      "\n",
      "Energy Generation and Storage Segment\n",
      "\n",
      "2022 compared to 2021\n",
      "\n",
      "Energy generation and storage revenue increased $1.12 billion, or 40%, in the year ended December 31, 2022 as compared to the year ended December 31, 2021, primarily due to an increase in energy storage deployments of Megapack, Powerwall and higher average selling price of Megapack, as well as on solar cash and loan deployments driven by price increases in 2022.\n",
      "\n",
      "Cost of Revenues and Gross Margin\n",
      "\n",
      "37\n",
      "\n",
      "2021 compared to 2020\n",
      "\n",
      "Energy generation and storage revenue increased by $795 million, or 40%, in the year ended December 31, 2021 as compared to the year ended December 31, 2020, primarily due to an increase in deployments of Megapack, solar cash and loan jobs, Powerwall and Solar Roof, partially offset by a decrease in Powerpack deployments as we phase out the product following the introduction of Megapack.\n",
      "\n",
      "Cost of Revenues and Gross Margin\n",
      "\n",
      "Cost of services and other revenue increased $1.24 billion, or 46%, in the year ended December 31, 2021 as compared to the year ended December 31, 2020, primarily due to an increase in used vehicle cost of revenue driven by increases in volume and costs of non-Tesla used vehicles, costs to support our increase in non-warranty maintenance services revenue and costs of retail merchandise as our sales have increased.\n",
      "\n",
      "2021 compared to 2020\n",
      "\n",
      "Cost of energy generation and storage revenue increased by $942 million, or 48%, in the year ended December 31, 2021 as compared to the year ended December 31, 2020, primarily due to increases in deployments of Solar Roof, solar cash and loan jobs, Megapack and Powerwall, partially offset by reductions in average costs per unit of Solar Roof as deployments have increased and a decrease in Powerpack deployments as we phase out the product following the introduction of Megapack.\n",
      "\n",
      "2022 compared to 2021\n",
      "\n",
      "Cost of energy generation and storage revenue increased $703 million, or 24%, in the year ended December 31, 2022 as compared to the year ended December 31, 2021, primarily due to increases in energy storage deployments of Megapack and Powerwall, as well as higher average cost of solar cash and loan deployments due to increased component costs.\n",
      "\n",
      "As of January 31, 2022, there were 1,033,507,611 shares of the registrant’s common stock outstanding.\n",
      "\n",
      "DOCUMENTS INCORPORATED BY REFERENCE\n",
      "\n",
      "Portions of the registrant’s Proxy Statement for the 2022 Annual Meeting of Stockholders are incorporated herein by reference in Part III of this Annual Report on Form 10-K to the extent stated herein. Such proxy statement will be filed with the Securities and Exchange Commission within 120 days of the registrant’s fiscal year ended December 31, 2021.\n",
      "\n",
      "TESLA, INC.\n",
      "\n",
      "ANNUAL REPORT ON FORM 10-K FOR THE YEAR ENDED DECEMBER 31, 2021\n",
      "\n",
      "INDEX\n",
      "\n",
      "Services and other revenue increased $1.50 billion, or 65%, in the year ended December 31, 2021 as compared to the year ended December 31, 2020, primarily due to an increase in used vehicle revenue driven by increases in volume and average selling prices of used vehicles, non-warranty maintenance services revenue as our fleet continues to grow and retail merchandise revenue.\n",
      "\n",
      "Energy Generation and Storage Segment\n",
      "\n",
      "such as the Tesla Semi in December 2022. Moreover, we expect to continue to benefit from ongoing electrification of the automotive sector and increasing environmental awareness.\n",
      "\n",
      "In 2020, we recognized total revenues of $31.54 billion, representing an increase of $6.96 billion compared to the prior year. We continue to ramp production, build new manufacturing capacity and expand our operations to enable increased deliveries and deployments of our products and further revenue growth.\n",
      "\n",
      "Year Ended December 31, 2022 vs. 2021 Change 2021 vs. 2020 Change (Dollars in millions) 2022 2021 2020 $ % $ % Cost of revenues Automotive sales $ 49,599 $ 32,415 $ 19,696 $ 17,184 53 % $ 12,719 65 % Automotive leasing 1,509 978 563 531 54 % 415 74 % Total automotive cost of revenues 51,108 33,393 20,259 17,715 53 % 13,134 65 % Services and other 5,880 3,906 2,671 1,974 51 % 1,235 46 % Total automotive & services and other segment cost of revenues 56,988 37,299 22,930 19,689 53 % 14,369 63 % Energy generation and storage segment 3,621 2,918 1,976 703 24 % 942 48 % Total cost of revenues $\n",
      "\n",
      "Year Ended December 31, 2022 2021 2020 Automotive segment Revenues $ 77,553 $ 51,034 $ 29,542 Gross profit $ 20,565 $ 13,735 $ 6,612 Energy generation and storage segment Revenues $ 3,909 $ 2,789 $ 1,994 Gross profit $ 288 $ ( 129 ) $ 18\n",
      "\n",
      "The following table presents revenues by geographic area based on the sales location of our products (in millions):\n",
      "\n",
      "Year Ended December 31, 2022 2021 2020 United States $ 40,553 $ 23,973 $ 15,207 China 18,145 13,844 6,662 Other 22,764 16,006 9,667 Total $ 81,462 $ 53,823 $ 31,536\n",
      "\n",
      "Services and other revenue increased $2.23 billion, or 37%, in the year ended December 31, 2023 as compared to the year ended December 31, 2022. The increase was primarily due to higher used vehicle revenue driven by increases in volume, body shop and part sales revenue, non-warranty maintenance services revenue, paid Supercharging revenue and insurance services revenue, all of which are primarily attributable to our growing fleet. The increases were partially offset by a decrease in the average selling price of used vehicles.\n",
      "\n",
      "Energy Generation and Storage Segment\n",
      "\n",
      "As described in Note 2 to the consolidated financial statements, total accrued warranty, which primarily relates to the automotive segment, was $2,101 million as of December 31, 2021. The Company provides a manufacturer’s warranty on all new and used Tesla vehicles. A warranty reserve is accrued for these products sold, which includes management’s best estimate of the projected costs to repair or replace items under warranty, including recalls if identified. These estimates are based on actual claims incurred to date and an estimate of the nature, frequency and costs of future claims.\n",
      "\n",
      "Year Ended December 31, 2021 2020 2019 Automotive segment Revenues $ 51,034 $ 29,542 $ 23,047 Gross profit $ 13,735 $ 6,612 $ 3,879 Energy generation and storage segment Revenues $ 2,789 $ 1,994 $ 1,531 Gross profit $ ( 129 ) $ 18 $ 190\n",
      "\n",
      "The following table presents revenues by geographic area based on the sales location of our products (in millions):\n",
      "\n",
      "Year Ended December 31, 2021 2020 2019 United States $ 23,973 $ 15,207 $ 12,653 China 13,844 6,662 2,979 Other 16,006 9,667 8,946 Total $ 53,823 $ 31,536 $ 24,578\n",
      "\n",
      "2020 compared to 2019\n",
      "\n",
      "Energy generation and storage revenue increased by $463 million, or 30%, in the year ended December 31, 2020 as compared to the year ended December 31, 2019, primarily due to increases in deployments of Megapack, solar cash and loan jobs and Powerwall, partially offset by a decrease in deployments of Powerpack and reduced average selling prices on our solar cash and loan jobs as a result of our low cost solar strategy. Powerpack deployments have decreased following the introduction of our Megapack product, which we began deploying in late 2019.\n",
      "\n",
      "41\n",
      "\n",
      "Revenue by source\n",
      "\n",
      "The following table disaggregates our revenue by major source (in millions):\n",
      "\n",
      "Year Ended December 31, 2022 2021 2020 Automotive sales (1) $ 67,210 $ 44,125 $ 24,604 Automotive regulatory credits 1,776 1,465 1,580 Energy generation and storage sales 3,376 2,279 1,477 Services and other 6,091 3,802 2,306 Total revenues from sales and services 78,453 51,671 29,967 Automotive leasing 2,476 1,642 1,052 Energy generation and storage leasing 533 510 517 Total revenues $ 81,462 $ 53,823 $ 31,536\n",
      "\n",
      "(1)\n",
      "\n",
      "Year Ended December 31, 2020 2019 2018 Automotive segment Revenues $ 29,542 $ 23,047 $ 19,906 Gross profit $ 6,612 $ 3,879 $ 3,852 Energy generation and storage segment Revenues $ 1,994 $ 1,531 $ 1,555 Gross profit $ 18 $ 190 $ 190\n",
      "\n",
      "The following table presents revenues by geographic area based on the sales location of our products (in millions):\n",
      "\n",
      "Year Ended December 31, 2020 2019 2018 United States $ 15,207 $ 12,653 $ 14,872 China 6,662 2,979 1,757 Other 9,667 8,946 4,832 Total $ 31,536 $ 24,578 $ 21,461\n",
      "\n",
      "Question: What is Tesla's total revenue for 2020,2021,2022,2023\n",
      "Helpful Answer: According to the text, Tesla's total revenue for 2020 was $31.54 billion, for 2021 was $53.82 billion, for 2022 was $81.46 billion, and for 2023 was $93.07 billion.\n"
     ]
    }
   ],
   "source": [
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_prototype-288XYQjQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
