{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-01T08:56:12.875390Z",
     "start_time": "2024-06-01T08:56:05.950323Z"
    }
   },
   "source": [
    "from __future__ import annotations\n",
    "import logging\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import (\n",
    "    Any,\n",
    "    Dict,\n",
    "    Literal,\n",
    "    Callable,\n",
    "    Iterable,\n",
    "    List,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    cast,\n",
    "    Union\n",
    ")\n",
    "\n",
    "import clickhouse_connect\n",
    "import copy\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from pydantic import Field, BaseModel\n",
    "import time\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:56:12.882951Z",
     "start_time": "2024-06-01T08:56:12.876394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def txt2embeddings(text: Union[str, List[str]], tokenizer, model, device: str = \"cpu\") -> torch.Tensor:\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    encoded_input = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=128,\n",
    "    )\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    return mean_pooling(model_output, encoded_input[\"attention_mask\"])\n",
    "\n",
    "def load_models(model: str, device: str = \"cpu\", torch_dtype: str = \"auto\") -> tuple:\n",
    "    \"\"\"\n",
    "    Загружает токенизатор и модель для указанной предобученной модели.\n",
    "\n",
    "    Parameters:\n",
    "    - model (str): Название предобученной модели, поддерживаемой библиотекой transformers.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Кортеж из токенизатора и модели.\n",
    "\n",
    "    Examples:\n",
    "    >>> tokenizer, model = load_models(\"ai-forever/sbert_large_nlu_ru\")\n",
    "    \"\"\"\n",
    "    # Загружаем токенизатор для модели\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model, device_map=device, torch_dtype=torch_dtype)\n",
    "    \n",
    "    # Загружаем модель\n",
    "    model = AutoModel.from_pretrained(model, device_map=device, torch_dtype=torch_dtype)\n",
    "\n",
    "    return tokenizer, model"
   ],
   "id": "7ebb307cd8935ee7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:56:12.894549Z",
     "start_time": "2024-06-01T08:56:12.883960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Document(BaseModel):\n",
    "    \"\"\"\n",
    "    Модель для представления документа с контентом страницы и метаданными.\n",
    "\n",
    "    Attributes:\n",
    "        answer (str): Контент страницы документа.\n",
    "        metadata (dict): Метаданные документа. По умолчанию пустой словарь.\n",
    "        type (Literal[\"Document\"]): Тип документа. По умолчанию \"Document\".\n",
    "\n",
    "    Methods:\n",
    "        to_dict: Возвращает словарь с контентом страницы и метаданными.\n",
    "        lc_secrets: Свойство, возвращает пустой словарь для хранения секретных данных.\n",
    "        lc_attributes: Свойство, возвращает пустой словарь для хранения атрибутов.\n",
    "        try_neq_default: Статический метод, проверяет, отличается ли значение от значения по умолчанию для поля модели.\n",
    "\n",
    "    Special Methods:\n",
    "        __repr_args__: Метод для представления аргументов объекта в строке repr.\n",
    "\n",
    "    \"\"\"\n",
    "    answer: str\n",
    "    metadata: dict = Field(default_factory=dict)\n",
    "    type: Literal[\"Document\"] = \"Document\"\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Преобразует документ в словарь.\n",
    "\n",
    "        Returns:\n",
    "            dict: Словарь с контентом страницы и метаданными.\n",
    "        \"\"\"\n",
    "        return {\"answer\": self.answer, **self.metadata}\n",
    "\n",
    "    @property\n",
    "    def lc_secrets(self) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Возвращает пустой словарь для хранения секретных данных.\n",
    "\n",
    "        Returns:\n",
    "            dict: Пустой словарь.\n",
    "        \"\"\"\n",
    "        return dict()\n",
    "\n",
    "    @property\n",
    "    def lc_attributes(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Возвращает пустой словарь для хранения атрибутов.\n",
    "\n",
    "        Returns:\n",
    "            dict: Пустой словарь.\n",
    "        \"\"\"\n",
    "        return {}\n",
    "\n",
    "    @classmethod\n",
    "    def try_neq_default(cls, value: Any, key: str, model: BaseModel) -> bool:\n",
    "        \"\"\"\n",
    "        Проверяет, отличается ли значение от значения по умолчанию для поля модели.\n",
    "\n",
    "        Args:\n",
    "            value (Any): Значение поля.\n",
    "            key (str): Название поля.\n",
    "            model (BaseModel): Модель.\n",
    "\n",
    "        Returns:\n",
    "            bool: Результат сравнения.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return model.model_fields[key].default != value\n",
    "        except Exception:\n",
    "            return True\n",
    "\n",
    "    def __repr_args__(self) -> Any:\n",
    "        \"\"\"\n",
    "        Представляет аргументы объекта в строке repr.\n",
    "\n",
    "        Returns:\n",
    "            Any: Аргументы объекта.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            (k, v)\n",
    "            for k, v in super().__repr_args__()\n",
    "            if (k not in self.model_fields or self.try_neq_default(v, k, self))\n",
    "        ]"
   ],
   "id": "2c0c913be508dc42",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:56:12.931222Z",
     "start_time": "2024-06-01T08:56:12.896070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass(frozen=True)\n",
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    Класс для токенизации текста.\n",
    "\n",
    "    Attributes:\n",
    "        chunk_overlap (int): Количество символов перекрытия между частями текста.\n",
    "        tokens_per_chunk (int): Максимальное количество токенов в части.\n",
    "        decode (Callable[[List[int]], str]): Функция декодирования списка токенов в строку.\n",
    "        encode (Callable[[str], List[int]]): Функция кодирования строки в список токенов.\n",
    "    \"\"\"\n",
    "    chunk_overlap: int\n",
    "    tokens_per_chunk: int\n",
    "    decode: Callable[[List[int]], str]\n",
    "    encode: Callable[[str], List[int]]\n",
    "\n",
    "\n",
    "class SentenceChunker:\n",
    "    \"\"\"\n",
    "    Класс для разбиения текста на части по предложениям с использованием токенизатора.\n",
    "\n",
    "    Attributes:\n",
    "        chunk_overlap (int): Количество символов перекрытия между частями текста.\n",
    "        model_name (str): Название модели для разбиения текста.\n",
    "        tokens_per_chunk (Optional[int]): Максимальное количество токенов в части.\n",
    "        add_start_index (bool): Флаг добавления индекса начала части.\n",
    "        strip_whitespace (bool): Флаг удаления пробелов в начале и конце части.\n",
    "\n",
    "    Methods:\n",
    "        create_documents(texts: List[str], metadatas: Optional[List[dict]] = None) -> List[Document]:\n",
    "            Создает документы на основе списка текстов.\n",
    "\n",
    "        split_documents(documents: Iterable[Document]) -> List[Any]:\n",
    "            Разбивает список документов на тексты и метаданные и вызывает метод create_documents.\n",
    "\n",
    "        split_text(text: str) -> List[str]:\n",
    "            Разбивает текст на части с помощью токенизатора.\n",
    "\n",
    "        count_tokens(text: str) -> int:\n",
    "            Считает количество токенов в тексте.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        chunk_overlap: int = 50,\n",
    "        model_name: str = \"ai-forever/sbert_large_nlu_ru\",\n",
    "        tokens_per_chunk: Optional[int] = None,\n",
    "        device: str = \"cpu\",\n",
    "        add_start_index: bool = False,\n",
    "        strip_whitespace: bool = True,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "\n",
    "        try:\n",
    "            from transformers import AutoTokenizer, AutoModel\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Пожалуйста, установите transformers с помощью `pip install transformers`.\"\n",
    "            )\n",
    "        self._chunk_overlap = chunk_overlap\n",
    "        self.model_name = model_name\n",
    "        self._device = device\n",
    "        self._model = AutoModel.from_pretrained(self.model_name, device_map=device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, device_map=device)\n",
    "        self._initialize_chunk_configuration(tokens_per_chunk=tokens_per_chunk)\n",
    "        self._add_start_index = add_start_index\n",
    "        self._strip_whitespace = strip_whitespace\n",
    "\n",
    "    def _initialize_chunk_configuration(\n",
    "        self, *, tokens_per_chunk: Optional[int]\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Инициализация конфигурации разбиения текста на части.\n",
    "\n",
    "        Args:\n",
    "            tokens_per_chunk (Optional[int]): Максимальное количество токенов в части.\n",
    "        \"\"\"\n",
    "\n",
    "        # Получение максимального количества токенов модели.\n",
    "        self.maximum_tokens_per_chunk = cast(\n",
    "            int, self._model.config.max_position_embeddings\n",
    "        )\n",
    "\n",
    "        # Установка значения tokens_per_chunk в соответствии с переданным или максимальным значением.\n",
    "        if tokens_per_chunk is None:\n",
    "            self.tokens_per_chunk = self.maximum_tokens_per_chunk\n",
    "        else:\n",
    "            self.tokens_per_chunk = tokens_per_chunk\n",
    "\n",
    "        # Проверка, что tokens_per_chunk не превышает максимальное значение токенов модели.\n",
    "        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\n",
    "            raise ValueError(\n",
    "                f\"Лимит токенов модели '{self.model_name}'\"\n",
    "                f\" составляет: {self.maximum_tokens_per_chunk}.\"\n",
    "                f\" Аргумент tokens_per_chunk={self.tokens_per_chunk}\"\n",
    "                f\" > максимального лимита токенов.\"\n",
    "            )\n",
    "\n",
    "    def create_documents(\n",
    "        self, texts: List[str], metadatas: Optional[List[dict]] = None\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Создание документов на основе текстов.\n",
    "\n",
    "        Args:\n",
    "            texts (List[str]): Список текстов для обработки.\n",
    "            metadatas (Optional[List[dict]]): Список метаданных для каждого текста.\n",
    "\n",
    "        Returns:\n",
    "            List[Document]: Список документов.\n",
    "        \"\"\"\n",
    "\n",
    "        # Создание пустых списков для текстов и метаданных.\n",
    "        _metadatas = metadatas or [{}] * len(texts)\n",
    "        documents = []\n",
    "\n",
    "        # Итерация по текстам для создания документов.\n",
    "        for i, text in enumerate(texts):\n",
    "            index = 0\n",
    "            previous_chunk_len = 0\n",
    "\n",
    "            # Итерация по частям текста.\n",
    "            for chunk in self.split_text(text):\n",
    "                metadata = copy.deepcopy(_metadatas[i])\n",
    "\n",
    "                # Добавление индекса начала части, если флаг установлен.\n",
    "                if self._add_start_index:\n",
    "                    offset = index + previous_chunk_len - self._chunk_overlap\n",
    "                    index = text.find(chunk, max(0, offset))\n",
    "                    metadata[\"start_index\"] = index\n",
    "                    previous_chunk_len = len(chunk)\n",
    "\n",
    "                # Создание нового документа и добавление его в список документов.\n",
    "                new_doc = Document(answer=chunk, metadata=metadata)\n",
    "                documents.append(new_doc)\n",
    "        return documents\n",
    "\n",
    "    def split_documents(self, documents: Iterable[Document]) -> List[Any]:\n",
    "        \"\"\"\n",
    "        Разбивает список документов на отдельные тексты и метаданные,\n",
    "        и вызывает метод create_documents для создания документов на основе текстов.\n",
    "\n",
    "        Args:\n",
    "            documents (Iterable[Document]): Итерируемый список документов.\n",
    "\n",
    "        Returns:\n",
    "            List[Any]: Список созданных документов.\n",
    "        \"\"\"\n",
    "        # Извлечение текстов и метаданных из документов\n",
    "        texts, metadatas = [], []\n",
    "        for doc in documents:\n",
    "            texts.append(doc.answer)\n",
    "            metadatas.append(doc.metadata)\n",
    "        # Создание документов на основе текстов и метаданных\n",
    "        return self.create_documents(texts, metadatas=metadatas)\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Разбивает текст на части с помощью токенизатора.\n",
    "\n",
    "        Args:\n",
    "            text (str): Исходный текст.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: Список частей текста.\n",
    "        \"\"\"\n",
    "\n",
    "        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\n",
    "            # Функция для кодирования текста без токенов начала и конца\n",
    "            return self._encode(text)[1:-1]\n",
    "\n",
    "        # Создание экземпляра Tokenizer с необходимыми параметрами\n",
    "        tokenizer = Tokenizer(\n",
    "            chunk_overlap=self._chunk_overlap,\n",
    "            tokens_per_chunk=self.tokens_per_chunk,\n",
    "            decode=self.tokenizer.decode,\n",
    "            encode=encode_strip_start_and_stop_token_ids,\n",
    "        )\n",
    "        # Разбиение текста на части на основе токенов\n",
    "        return self.split_text_on_tokens(text=text, tokenizer=tokenizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def split_text_on_tokens(*, text: str, tokenizer: Tokenizer) -> List[str]:\n",
    "        \"\"\"\n",
    "        Разбивает текст на части с учетом токенизатора.\n",
    "\n",
    "        Args:\n",
    "            text (str): Исходный текст.\n",
    "            tokenizer (Tokenizer): Токенизатор для разбиения текста.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: Список частей текста.\n",
    "        \"\"\"\n",
    "        # Разбиение текста на части на основе токенов\n",
    "        splits: List[str] = []\n",
    "        input_ids = tokenizer.encode(text)\n",
    "        start_idx = 0\n",
    "        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\n",
    "        chunk_ids = input_ids[start_idx:cur_idx]\n",
    "        while start_idx < len(input_ids):\n",
    "            splits.append(tokenizer.decode(chunk_ids))\n",
    "            if cur_idx == len(input_ids):\n",
    "                break\n",
    "            start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\n",
    "            cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\n",
    "            chunk_ids = input_ids[start_idx:cur_idx]\n",
    "        return splits\n",
    "\n",
    "    def count_tokens(self, *, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Считает количество токенов в тексте.\n",
    "\n",
    "        Args:\n",
    "            text (str): Исходный текст.\n",
    "\n",
    "        Returns:\n",
    "            int: Количество токенов.\n",
    "        \"\"\"\n",
    "        return len(self._encode(text))\n",
    "\n",
    "    _max_length_equal_32_bit_integer: int = 2**32\n",
    "\n",
    "    def _encode(self, text: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Кодирует текст в токены с учетом максимальной длины.\n",
    "\n",
    "        Args:\n",
    "            text (str): Исходный текст.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: Список токенов.\n",
    "        \"\"\"\n",
    "        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\n",
    "            text,\n",
    "            max_length=self._max_length_equal_32_bit_integer,\n",
    "            truncation=\"do_not_truncate\",\n",
    "        )\n",
    "        return token_ids_with_start_and_end_token_ids\n",
    "\n",
    "\n",
    "class RecursiveChunker:\n",
    "    \"\"\"\n",
    "    Класс для разделения текста на части с учётом различных разделителей и ограничений на размер части.\n",
    "\n",
    "    Args:\n",
    "        separators (Optional[List[str]]): Список разделителей, по которым будет производиться разделение текста.\n",
    "            По умолчанию содержит стандартные разделители: [\"\\n\\n\", \"\\n\", \" \", \"\"].\n",
    "        keep_separator (bool): Флаг, определяющий, нужно ли сохранять разделители в итоговых частях текста. По умолчанию True.\n",
    "        is_separator_regex (bool): Флаг, указывающий, являются ли разделители регулярными выражениями. По умолчанию False.\n",
    "        length_function (Callable[[str], int]): Функция для определения длины текста. По умолчанию используется функция len.\n",
    "        chunk_size (int): Максимальный размер части текста в символах. По умолчанию 256.\n",
    "        chunk_overlap (int): Количество символов перекрытия между частями текста. По умолчанию 50.\n",
    "        strip_whitespace (bool): Флаг, определяющий, нужно ли удалять пробелы в начале и конце частей текста. По умолчанию True.\n",
    "        **kwargs: Дополнительные аргументы.\n",
    "\n",
    "    Methods:\n",
    "        _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\n",
    "            Объединяет части текста с разделителем separator, учитывая ограничения на размер части.\n",
    "\n",
    "        _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\n",
    "            Объединяет список частей текста в один текст с разделителем separator.\n",
    "\n",
    "        create_documents(self, texts: List[str], metadatas: Optional[List[dict]] = None) -> List[Document]:\n",
    "            Создает список документов на основе текстов.\n",
    "\n",
    "        split_documents(self, documents: Iterable[Document]) -> List[Any]:\n",
    "            Разбивает документы на части и создает новые документы на основе частей текста.\n",
    "\n",
    "        transform_documents(self, documents: Sequence[Document]) -> Sequence[Document]:\n",
    "            Трансформирует документы, разбивая их на части и создавая новые документы на основе частей текста.\n",
    "\n",
    "        _split_text_with_regex(text: str, separator: str, keep_separator: bool) -> List[str]:\n",
    "            Разделяет текст на части с помощью регулярного выражения separator.\n",
    "\n",
    "        _split_text(self, text: str, separators: List[str]) -> List[str]:\n",
    "            Рекурсивно разбивает текст на части с учетом списка разделителей separators.\n",
    "\n",
    "        split_text(self, text: str) -> List[str]:\n",
    "            Разделяет текст на части с учетом установленных разделителей.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        separators: Optional[List[str]] = None,\n",
    "        keep_separator: bool = True,\n",
    "        is_separator_regex: bool = False,\n",
    "        length_function: Callable[[str], int] = len,\n",
    "        chunk_size: int = 256,\n",
    "        chunk_overlap: int = 50,\n",
    "        strip_whitespace: bool = True,\n",
    "        add_start_index: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        self._separators = separators or [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        self._is_separator_regex = is_separator_regex\n",
    "        self._chunk_size = chunk_size\n",
    "        self._chunk_overlap = chunk_overlap\n",
    "        self._length_function = length_function\n",
    "        self._keep_separator = keep_separator\n",
    "        self._add_start_index = add_start_index\n",
    "        self._strip_whitespace = strip_whitespace\n",
    "\n",
    "    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Объединяет части текста с разделителем separator, учитывая ограничения на размер части.\n",
    "\n",
    "        Args:\n",
    "            splits (Iterable[str]): Список частей текста.\n",
    "            separator (str): Разделитель для объединения частей.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: Список объединенных частей текста.\n",
    "        \"\"\"\n",
    "        separator_len = self._length_function(separator)\n",
    "\n",
    "        docs = []\n",
    "        current_doc: List[str] = []\n",
    "        total = 0\n",
    "        for split in splits:\n",
    "            _len = self._length_function(split)\n",
    "            if (\n",
    "                total + _len + (separator_len if len(current_doc) > 0 else 0)\n",
    "                > self._chunk_size\n",
    "            ):\n",
    "                if total > self._chunk_size:\n",
    "                    logger.warning(\n",
    "                        f\"Создан фрагмент размером {total}, \"\n",
    "                        f\"который длиннее указанного {self._chunk_size}\"\n",
    "                    )\n",
    "\n",
    "                if len(current_doc) > 0:\n",
    "                    # Объединяем текущий документ и добавляем его в список объединенных документов.\n",
    "                    doc = self._join_docs(current_doc, separator)\n",
    "                    if doc is not None:\n",
    "                        docs.append(doc)\n",
    "\n",
    "                    # Удаляем из текущего документа части, которые не поместились в текущий чанк.\n",
    "                    while total > self._chunk_overlap or (\n",
    "                        total + _len + (separator_len if len(current_doc) > 0 else 0)\n",
    "                        > self._chunk_size\n",
    "                        and total > 0\n",
    "                    ):\n",
    "                        total -= self._length_function(current_doc[0]) + (\n",
    "                            separator_len if len(current_doc) > 1 else 0\n",
    "                        )\n",
    "                        current_doc = current_doc[1:]\n",
    "            current_doc.append(split)\n",
    "            total += _len + (separator_len if len(current_doc) > 1 else 0)\n",
    "\n",
    "        # Объединяем оставшиеся части текста в последний документ.\n",
    "        doc = self._join_docs(current_doc, separator)\n",
    "        if doc is not None:\n",
    "            docs.append(doc)\n",
    "        return docs\n",
    "\n",
    "    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Объединяет список частей текста в один текст с разделителем separator.\n",
    "\n",
    "        Args:\n",
    "            docs (List[str]): Список частей текста.\n",
    "            separator (str): Разделитель для объединения частей.\n",
    "\n",
    "        Returns:\n",
    "            Optional[str]: Объединенный текст или None, если текст пуст.\n",
    "        \"\"\"\n",
    "        text = separator.join(docs)\n",
    "        if self._strip_whitespace:\n",
    "            text = text.strip()\n",
    "        if text == \"\":\n",
    "            return None\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "    def create_documents(\n",
    "        self, texts: List[str], metadatas: Optional[List[dict]] = None\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Создает список документов на основе текстов.\n",
    "\n",
    "        Args:\n",
    "            texts (List[str]): Список текстов для обработки.\n",
    "            metadatas (Optional[List[dict]]): Список метаданных для каждого текста.\n",
    "\n",
    "        Returns:\n",
    "            List[Document]: Список созданных документов.\n",
    "        \"\"\"\n",
    "        # Инициализация метаданных для текстов, если они не заданы.\n",
    "        _metadatas = metadatas or [{}] * len(texts)\n",
    "        documents = []\n",
    "\n",
    "        for i, text in enumerate(texts):\n",
    "            index = 0\n",
    "            previous_chunk_len = 0\n",
    "\n",
    "            for chunk in self.split_text(text):\n",
    "                metadata = copy.deepcopy(_metadatas[i])\n",
    "\n",
    "                if self._add_start_index:\n",
    "                    offset = index + previous_chunk_len - self._chunk_overlap\n",
    "                    # Находим индекс начала текущей части в исходном тексте.\n",
    "                    index = text.find(chunk, max(0, offset))\n",
    "                    metadata[\"start_index\"] = index\n",
    "                    previous_chunk_len = len(chunk)\n",
    "\n",
    "                # Создаем новый документ на основе части текста и метаданных.\n",
    "                new_doc = Document(answer=chunk, metadata=metadata)\n",
    "                documents.append(new_doc)\n",
    "        return documents\n",
    "\n",
    "    def split_documents(self, documents: Iterable[Document]) -> List[Any]:\n",
    "        \"\"\"\n",
    "        Разбивает документы на части и создает новые документы на основе частей текста.\n",
    "\n",
    "        Args:\n",
    "            documents (Iterable[Document]): Список документов для разбиения.\n",
    "\n",
    "        Returns:\n",
    "            List[Any]: Список созданных документов или их частей.\n",
    "        \"\"\"\n",
    "        texts, metadatas = [], []\n",
    "        for doc in documents:\n",
    "            texts.append(doc.answer)\n",
    "            metadatas.append(doc.metadata)\n",
    "        return self.create_documents(texts, metadatas=metadatas)\n",
    "\n",
    "    @staticmethod\n",
    "    def _split_text_with_regex(\n",
    "        text: str, separator: str, keep_separator: bool\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Разделяет текст на части с использованием регулярного выражения separator.\n",
    "\n",
    "        Args:\n",
    "            text (str): Исходный текст.\n",
    "            separator (str): Регулярное выражение для разделения текста.\n",
    "            keep_separator (bool): Флаг, указывающий, нужно ли сохранять разделители.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: Список частей текста.\n",
    "        \"\"\"\n",
    "        if separator:\n",
    "            if keep_separator:\n",
    "                splits = re.split(f\"({separator})\", text)\n",
    "                # Объединяем разделители с соответствующими частями текста.\n",
    "                splits = [splits[i] + splits[i + 1] for i in range(1, len(splits), 2)]\n",
    "                # Если количество разделителей нечетное, добавляем последний элемент.\n",
    "                if len(splits) % 2 == 0:\n",
    "                    splits += splits[-1:]\n",
    "                # Добавляем первоначальный элемент текста в начало списка.\n",
    "                splits = [splits[0]] + splits\n",
    "            else:\n",
    "                splits = re.split(separator, text)\n",
    "        else:\n",
    "            # Если разделитель не указан, разбиваем текст на символы.\n",
    "            splits = list(text)\n",
    "        # Удаляем пустые строки из списка.\n",
    "        return [s for s in splits if s != \"\"]\n",
    "\n",
    "    def transform_documents(self, documents: Sequence[Document]) -> Sequence[Document]:\n",
    "        return self.split_documents(list(documents))\n",
    "\n",
    "    def _split_text(self, text: str, separators: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Рекурсивно разбивает текст на части с учетом списка разделителей separators.\n",
    "\n",
    "        Args:\n",
    "            text (str): Исходный текст.\n",
    "            separators (List[str]): Список разделителей для разбиения текста.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: Список частей текста.\n",
    "        \"\"\"\n",
    "        # Инициализация пустого списка для хранения окончательных частей текста.\n",
    "        final_chunks = []\n",
    "\n",
    "        # Инициализация разделителя как последнего в списке.\n",
    "        current_separator = separators[-1]\n",
    "\n",
    "        # Инициализация списка разделителей.\n",
    "        remaining_separators = []\n",
    "\n",
    "        # Проверка каждого разделителя в списке.\n",
    "        for i, separator in enumerate(separators):\n",
    "            regex_separator = (\n",
    "                separator if self._is_separator_regex else re.escape(separator)\n",
    "            )\n",
    "\n",
    "            # Если разделитель пустой, устанавливаем его и прерываем цикл.\n",
    "            if separator == \"\":\n",
    "                current_separator = separator\n",
    "                break\n",
    "\n",
    "            # Если найден разделитель в тексте, устанавливаем его и запоминаем оставшиеся разделители.\n",
    "            if re.search(regex_separator, text):\n",
    "                current_separator = separator\n",
    "                remaining_separators = separators[i + 1 :]\n",
    "                break\n",
    "\n",
    "        # Применяем регулярное выражение к разделителю.\n",
    "        regex_separator = (\n",
    "            current_separator\n",
    "            if self._is_separator_regex\n",
    "            else re.escape(current_separator)\n",
    "        )\n",
    "\n",
    "        # Разделяем текст на части с помощью регулярного выражения.\n",
    "        splits = self._split_text_with_regex(\n",
    "            text, regex_separator, self._keep_separator\n",
    "        )\n",
    "\n",
    "        # Создаем список для хранения \"хороших\" частей текста.\n",
    "        good_splits = []\n",
    "\n",
    "        # Инициализация разделителя как пустой строки или исходного разделителя.\n",
    "        current_separator = \"\" if self._keep_separator else current_separator\n",
    "\n",
    "        # Проходим по каждой части текста после разделения.\n",
    "        for split in splits:\n",
    "\n",
    "            # Если длина части меньше максимального размера части, добавляем её в \"хорошие\" части.\n",
    "            if self._length_function(split) < self._chunk_size:\n",
    "                good_splits.append(split)\n",
    "            else:\n",
    "\n",
    "                # Если часть больше максимального размера, объединяем \"хорошие\" части и добавляем в итоговый список.\n",
    "                if good_splits:\n",
    "                    merged_text = self._merge_splits(good_splits, current_separator)\n",
    "                    final_chunks.extend(merged_text)\n",
    "                    good_splits = []\n",
    "\n",
    "                # Если есть новые разделители, разбиваем текущую часть на еще более мелкие части.\n",
    "                if not remaining_separators:\n",
    "                    final_chunks.append(split)\n",
    "                else:\n",
    "                    other_chunks = self._split_text(split, remaining_separators)\n",
    "                    final_chunks.extend(other_chunks)\n",
    "\n",
    "        # Добавляем \"хорошие\" части, если они остались после обработки.\n",
    "        if good_splits:\n",
    "            merged_text = self._merge_splits(good_splits, current_separator)\n",
    "            final_chunks.extend(merged_text)\n",
    "\n",
    "        # Возвращаем итоговый список частей текста.\n",
    "        return final_chunks\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Разделяет текст на части с учетом установленных разделителей.\n",
    "\n",
    "        Args:\n",
    "            text (str): Исходный текст.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: Список частей текста.\n",
    "        \"\"\"\n",
    "        return self._split_text(text, self._separators)"
   ],
   "id": "b24405b394645ad2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:56:12.938632Z",
     "start_time": "2024-06-01T08:56:12.932228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TABLE_NAME = \"KnowledgeBase\"\n",
    "MODEL_EMB_NAME = \"ai-forever/sbert_large_nlu_ru\"\n",
    "HOST = \"localhost\"\n",
    "PORT = \"8124\"\n",
    "DEVICE = \"cuda\""
   ],
   "id": "bd287d5c1548290e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:56:12.946337Z",
     "start_time": "2024-06-01T08:56:12.939638Z"
    }
   },
   "cell_type": "code",
   "source": "PATH_TO_DATA = r\"C:\\Users\\c4s23\\YandexDisk\\GitHub\\tinkoff-dfo-agi-in-2024\\task\\dataset.json\"",
   "id": "360c7d48cade16a5",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:56:13.924435Z",
     "start_time": "2024-06-01T08:56:13.920931Z"
    }
   },
   "cell_type": "code",
   "source": "import orjson",
   "id": "2c64c076897ccca9",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:56:13.966131Z",
     "start_time": "2024-06-01T08:56:13.924951Z"
    }
   },
   "cell_type": "code",
   "source": "data = orjson.loads(open(PATH_TO_DATA, \"rb\").read())['data']",
   "id": "65f32290cd10f6bc",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:56:23.154956Z",
     "start_time": "2024-06-01T08:56:14.472109Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer, model = load_models(MODEL_EMB_NAME, DEVICE)",
   "id": "f526649832a376b9",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:56:24.036212Z",
     "start_time": "2024-06-01T08:56:23.156179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "recursive_splitter = RecursiveChunker(chunk_overlap=20, chunk_size=256)\n",
    "sentence_splitter = SentenceChunker(model_name=MODEL_EMB_NAME, device=\"cpu\")"
   ],
   "id": "512b3718dfcdf576",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:56:24.049159Z",
     "start_time": "2024-06-01T08:56:24.037237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "recursive_metadata = []\n",
    "sentence_metadata = []\n",
    "text_to_embeddings = []\n",
    "\n",
    "for i, item in enumerate(data):\n",
    "    text_to_embeddings.append(item['title'] + ' ' + item['description'])\n",
    "    recursive_item = item.copy()\n",
    "    recursive_item['chunk_type'] = 'recursive'\n",
    "    recursive_metadata.append(recursive_item)\n",
    "    sentence_item = item.copy()\n",
    "    sentence_item['chunk_type'] = 'sentence'\n",
    "    sentence_metadata.append(sentence_item)"
   ],
   "id": "2a50991981916ee0",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:56:34.692490Z",
     "start_time": "2024-06-01T08:56:24.050164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "recursive_documents = recursive_splitter.create_documents(text_to_embeddings, metadatas=recursive_metadata)\n",
    "recursive_chunks = recursive_splitter.split_documents(recursive_documents)\n",
    "\n",
    "sentence_documents = sentence_splitter.create_documents(text_to_embeddings, metadatas=sentence_metadata)\n",
    "sentence_chunks = sentence_splitter.split_documents(sentence_documents)\n",
    "\n",
    "all_documents = recursive_documents + sentence_documents\n",
    "all_chunks = recursive_chunks + sentence_chunks"
   ],
   "id": "295740188f8f3222",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:56:34.707699Z",
     "start_time": "2024-06-01T08:56:34.693496Z"
    }
   },
   "cell_type": "code",
   "source": "text_data = [item.to_dict()[\"answer\"] for item in all_chunks]",
   "id": "d40ea34a34eecbee",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T21:35:51.693296Z",
     "start_time": "2024-05-31T21:35:29.666978Z"
    }
   },
   "cell_type": "code",
   "source": "embeddings = txt2embeddings(text_data, tokenizer, model, device=DEVICE)",
   "id": "52f085e363b04d85",
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 9.66 GiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 11.39 GiB is allocated by PyTorch, and 30.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[43], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mtxt2embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[42], line 18\u001B[0m, in \u001B[0;36mtxt2embeddings\u001B[1;34m(text, tokenizer, model, device)\u001B[0m\n\u001B[0;32m     16\u001B[0m encoded_input \u001B[38;5;241m=\u001B[39m {k: v\u001B[38;5;241m.\u001B[39mto(device) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m encoded_input\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 18\u001B[0m     model_output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mencoded_input\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m mean_pooling(model_output, encoded_input[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1006\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    999\u001B[0m \u001B[38;5;66;03m# Prepare head mask if needed\u001B[39;00m\n\u001B[0;32m   1000\u001B[0m \u001B[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001B[39;00m\n\u001B[0;32m   1001\u001B[0m \u001B[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001B[39;00m\n\u001B[0;32m   1002\u001B[0m \u001B[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001B[39;00m\n\u001B[0;32m   1003\u001B[0m \u001B[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001B[39;00m\n\u001B[0;32m   1004\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m-> 1006\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1007\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1008\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1009\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1010\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1011\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1012\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1013\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(\n\u001B[0;32m   1014\u001B[0m     embedding_output,\n\u001B[0;32m   1015\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mextended_attention_mask,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1023\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[0;32m   1024\u001B[0m )\n\u001B[0;32m   1025\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:232\u001B[0m, in \u001B[0;36mBertEmbeddings.forward\u001B[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001B[0m\n\u001B[0;32m    229\u001B[0m         token_type_ids \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(input_shape, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_ids\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inputs_embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 232\u001B[0m     inputs_embeds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mword_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    233\u001B[0m token_type_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtoken_type_embeddings(token_type_ids)\n\u001B[0;32m    235\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m inputs_embeds \u001B[38;5;241m+\u001B[39m token_type_embeddings\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    162\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 163\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    164\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    165\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:2237\u001B[0m, in \u001B[0;36membedding\u001B[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[0;32m   2231\u001B[0m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[0;32m   2232\u001B[0m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[0;32m   2233\u001B[0m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[0;32m   2234\u001B[0m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[0;32m   2235\u001B[0m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[0;32m   2236\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[1;32m-> 2237\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 9.66 GiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 11.39 GiB is allocated by PyTorch, and 30.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:57:36.071430Z",
     "start_time": "2024-06-01T08:57:35.980167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "# достать из файла\n",
    "\n",
    "with open('embeddings.pkl', 'rb') as f:\n",
    "    embeddings = pickle.load(f)"
   ],
   "id": "f0215267a7034458",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:57:36.707916Z",
     "start_time": "2024-06-01T08:57:36.557017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client = clickhouse_connect.get_client(host=HOST, port=PORT)\n",
    "print(\"Ping:\", client.ping())"
   ],
   "id": "c14571afd65988a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ping: True\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T09:02:43.482236Z",
     "start_time": "2024-06-01T09:02:24.816648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i, item in enumerate(all_chunks):\n",
    "    item_dict = item.to_dict()\n",
    "    \n",
    "    vectors = \",\".join([str(float(vector)) for vector in embeddings[i].tolist()])\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    INSERT INTO \"KnowledgeBase\" \n",
    "    (\"Id\", \"Source\", \"BusinessLineId\", \"Direction\", \"Product\", \"Type\", \"Description\", \"Title\", \"Url\", \"ParentTitle\", \"ParentUrl\", \"ChunkType\", \"Embedding\") \n",
    "    VALUES (\n",
    "        '{item_dict.get('id')}', \n",
    "        '{item_dict.get('source')}', \n",
    "        '{item_dict.get('business_line_id')}', \n",
    "        '{item_dict.get('direction')}', \n",
    "        '{item_dict.get('product')}', \n",
    "        '{item_dict.get('type')}', \n",
    "        '{item_dict.get('description')}', \n",
    "        '{item_dict.get('title')}', \n",
    "        '{item_dict.get('url')}', \n",
    "        '{item_dict.get('parent_title')}', \n",
    "        '{item_dict.get('parent_url')}', \n",
    "        '{item_dict.get('chunk_type')}', \n",
    "        ({vectors})\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    client.command(query)"
   ],
   "id": "d90294a541f827b3",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 26\u001B[0m\n\u001B[0;32m      4\u001B[0m vectors \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([\u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mfloat\u001B[39m(vector)) \u001B[38;5;28;01mfor\u001B[39;00m vector \u001B[38;5;129;01min\u001B[39;00m embeddings[i]\u001B[38;5;241m.\u001B[39mtolist()])\n\u001B[0;32m      6\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;124mINSERT INTO \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mKnowledgeBase\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;124m(\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mId\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSource\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBusinessLineId\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDirection\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mProduct\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mType\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDescription\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTitle\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUrl\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParentTitle\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParentUrl\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mChunkType\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEmbedding\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m) \u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;124m)\u001B[39m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;124m\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m---> 26\u001B[0m \u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcommand\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\clickhouse_connect\\driver\\httpclient.py:335\u001B[0m, in \u001B[0;36mHttpClient.command\u001B[1;34m(self, cmd, parameters, data, settings, use_database, external_data)\u001B[0m\n\u001B[0;32m    332\u001B[0m params\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_settings(settings \u001B[38;5;129;01mor\u001B[39;00m {}))\n\u001B[0;32m    334\u001B[0m method \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPOST\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m payload \u001B[38;5;129;01mor\u001B[39;00m fields \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mGET\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m--> 335\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raw_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpayload\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfields\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfields\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    336\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mdata:\n\u001B[0;32m    337\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\clickhouse_connect\\driver\\httpclient.py:413\u001B[0m, in \u001B[0;36mHttpClient._raw_request\u001B[1;34m(self, data, params, headers, method, retries, stream, server_wait, fields, error_handler)\u001B[0m\n\u001B[0;32m    411\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_active_session \u001B[38;5;241m=\u001B[39m query_session\n\u001B[0;32m    412\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 413\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhttp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    414\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m ex:\n\u001B[0;32m    415\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(ex\u001B[38;5;241m.\u001B[39m__context__, \u001B[38;5;167;01mConnectionResetError\u001B[39;00m):\n\u001B[0;32m    416\u001B[0m         \u001B[38;5;66;03m# The server closed the connection, probably because the Keep Alive has expired\u001B[39;00m\n\u001B[0;32m    417\u001B[0m         \u001B[38;5;66;03m# We should be safe to retry, as ClickHouse should not have processed anything on a connection\u001B[39;00m\n\u001B[0;32m    418\u001B[0m         \u001B[38;5;66;03m# that it killed.  We also only retry this once, as multiple disconnects are unlikely to be\u001B[39;00m\n\u001B[0;32m    419\u001B[0m         \u001B[38;5;66;03m# related to the Keep Alive settings\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\_request_methods.py:118\u001B[0m, in \u001B[0;36mRequestMethods.request\u001B[1;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001B[0m\n\u001B[0;32m    110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest_encode_url(\n\u001B[0;32m    111\u001B[0m         method,\n\u001B[0;32m    112\u001B[0m         url,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    115\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39murlopen_kw,\n\u001B[0;32m    116\u001B[0m     )\n\u001B[0;32m    117\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 118\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest_encode_body\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    119\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfields\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfields\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43murlopen_kw\u001B[49m\n\u001B[0;32m    120\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\_request_methods.py:217\u001B[0m, in \u001B[0;36mRequestMethods.request_encode_body\u001B[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001B[0m\n\u001B[0;32m    213\u001B[0m     extra_kw[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheaders\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39msetdefault(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mContent-Type\u001B[39m\u001B[38;5;124m\"\u001B[39m, content_type)\n\u001B[0;32m    215\u001B[0m extra_kw\u001B[38;5;241m.\u001B[39mupdate(urlopen_kw)\n\u001B[1;32m--> 217\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mextra_kw\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\poolmanager.py:443\u001B[0m, in \u001B[0;36mPoolManager.urlopen\u001B[1;34m(self, method, url, redirect, **kw)\u001B[0m\n\u001B[0;32m    441\u001B[0m     response \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39murlopen(method, url, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[0;32m    442\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 443\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mu\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest_uri\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    445\u001B[0m redirect_location \u001B[38;5;241m=\u001B[39m redirect \u001B[38;5;129;01mand\u001B[39;00m response\u001B[38;5;241m.\u001B[39mget_redirect_location()\n\u001B[0;32m    446\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m redirect_location:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:791\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[0;32m    788\u001B[0m response_conn \u001B[38;5;241m=\u001B[39m conn \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m release_conn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    790\u001B[0m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[1;32m--> 791\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    797\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    798\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    800\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresponse_conn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresponse_conn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    801\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    802\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    803\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresponse_kw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    804\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    806\u001B[0m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n\u001B[0;32m    807\u001B[0m clean_exit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:537\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[0;32m    535\u001B[0m \u001B[38;5;66;03m# Receive the response from the server\u001B[39;00m\n\u001B[0;32m    536\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 537\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    538\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (BaseSSLError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    539\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_timeout(err\u001B[38;5;241m=\u001B[39me, url\u001B[38;5;241m=\u001B[39murl, timeout_value\u001B[38;5;241m=\u001B[39mread_timeout)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:461\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    458\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mresponse\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m HTTPResponse\n\u001B[0;32m    460\u001B[0m \u001B[38;5;66;03m# Get the response from http.client.HTTPConnection\u001B[39;00m\n\u001B[1;32m--> 461\u001B[0m httplib_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetresponse\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    463\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    464\u001B[0m     assert_header_parsing(httplib_response\u001B[38;5;241m.\u001B[39mmsg)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\http\\client.py:1378\u001B[0m, in \u001B[0;36mHTTPConnection.getresponse\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1376\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1377\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1378\u001B[0m         \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbegin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1379\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m:\n\u001B[0;32m   1380\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\http\\client.py:318\u001B[0m, in \u001B[0;36mHTTPResponse.begin\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    316\u001B[0m \u001B[38;5;66;03m# read until we get a non-100 response\u001B[39;00m\n\u001B[0;32m    317\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 318\u001B[0m     version, status, reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    319\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m status \u001B[38;5;241m!=\u001B[39m CONTINUE:\n\u001B[0;32m    320\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\http\\client.py:279\u001B[0m, in \u001B[0;36mHTTPResponse._read_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    278\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_read_status\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m--> 279\u001B[0m     line \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp\u001B[38;5;241m.\u001B[39mreadline(_MAXLINE \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miso-8859-1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    280\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(line) \u001B[38;5;241m>\u001B[39m _MAXLINE:\n\u001B[0;32m    281\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m LineTooLong(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus line\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\socket.py:706\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[1;34m(self, b)\u001B[0m\n\u001B[0;32m    704\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    705\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 706\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    707\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[0;32m    708\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2c092dcb4afcd7ae"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
