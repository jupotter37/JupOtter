{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-02-29T14:38:38.756752600Z",
     "start_time": "2024-02-29T14:38:32.861504800Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4d54f357df52bb0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T14:38:59.921590400Z",
     "start_time": "2024-02-29T14:38:41.245403600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:18<00:00,  1.17s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": "16"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = r'C:\\Users\\jaini\\PycharmProjects\\pythonProject3\\documents\\pdf'\n",
    "\n",
    "def load_docs(directory):\n",
    "  loader = DirectoryLoader(directory,show_progress=True) #unstructuredloader by default has used this auto identify file type and load it, mode=\"single\", strategy='fast'(other option is strategy='hi_res' that use yolo varient if mode is elements)\n",
    "  documents = loader.load()\n",
    "  return documents\n",
    "\n",
    "documents = load_docs(directory) \n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afe7346c168c84ce",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T14:38:59.966886400Z",
     "start_time": "2024-02-29T14:38:59.933578800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356\n"
     ]
    }
   ],
   "source": [
    "def split_docs(documents, chunk_size=1000, chunk_overlap=100):\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "  docs = text_splitter.split_documents(documents)\n",
    "  return docs\n",
    "\n",
    "docs = split_docs(documents)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b629c79217f5a891",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T14:38:59.976884400Z",
     "start_time": "2024-02-29T14:38:59.938579Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Document(page_content='Data Engineer Charles Obuseh, MSc, PE Self-driven individual, analytical, problem solver with over 3 years leveraging data engineering skills. I am proficient in technical software, languages, and tools with the ability to learn new tools, databases and systems to maintain/enhance strategic vision of the organization and able to provide cutting-edge data engineering skills. Seeking a reputable organization to contribute to with opportunities for personal growth.\\n\\ncharleside2001@yahoo.com Cypress, TX 77429 • (972)904-3380\\n\\n. TECHNICAL SKILLS\\n\\nLanguages • Python, SQL Others • Git, Bash\\n\\nData Manipulation & Visualization • Pandas, Tableau BI, ETL,\\n\\nApache Spark, Hadoop HDFS, Snowflake Data Factory, SnowSQL and Snowpipe, Amazon Kinesis Firehose, Databricks, AWS EMR, Athena, Lambda, Amazon MW Apache Airflow and AWS EC2 QuickSight and SNS, Cloudera infrastructure – Ambari, Hortonworks Sandbox and Zeppelin Notebook', metadata={'source': 'C:\\\\Users\\\\jaini\\\\PycharmProjects\\\\pythonProject3\\\\documents\\\\pdf\\\\Charles Obuseh.pdf'})"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7354b96e0e62f53a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T14:39:04.917656400Z",
     "start_time": "2024-02-29T14:38:59.955578400Z"
    }
   },
   "outputs": [],
   "source": [
    "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "model_kwargs = {'device':'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    " model_name=modelPath, \n",
    " model_kwargs=model_kwargs, \n",
    " encode_kwargs=encode_kwargs \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1dbcce378015aff",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T14:39:22.624877800Z",
     "start_time": "2024-02-29T14:39:04.918655600Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "# Embed and store the texts\n",
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "persist_directory = 'db'\n",
    "\n",
    "\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=docs, \n",
    "                                 embedding=embeddings,\n",
    "                                 persist_directory=persist_directory)\n",
    "# persiste the db to disk\n",
    "vectordb.persist()\n",
    "vectordb = None\n",
    "\n",
    "# Now we can load the persisted database from disk, and use it as normal. \n",
    "vectordb = Chroma(persist_directory=persist_directory, \n",
    "                  embedding_function=embeddings)\n",
    "\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 2}) # by default search_type=\"similarity_score_threshold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42c4ae2254b5f660",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T14:39:22.685877900Z",
     "start_time": "2024-02-29T14:39:22.626878100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximillian Wiesner maxanth112@gmail.com (949) 636-9002 San Diego, CA\n",
      "\n",
      "Versatile data engineer who likes breaking things, fixing things, and occasionally asking too many questions. No particular order.\n",
      "\n",
      "PROFESSIONAL EXPERIENCE\n",
      "\n",
      "SIGNPOST Data Engineer (Remote)\n",
      "\n",
      "San Diego, CA Aug 21’-Current\n",
      "\n",
      "\n",
      "\n",
      "Sole data engineer for a Series D startup with a distributed engineer team out of New York and Denver. Have implemented new ETL pipelines, optimized existing ones, and maintain the existing data infrastructure. Role has been a mix of data engineering (70%), devops (20%), and machine learning (10%). I wear many hats in this position, working closely with our infrastructure engineers, devops engineers, business operations team, BI analysts, and PM’s.\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.invoke(\"which candidate is good fit for Machine learning engineer roles.\")\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a83a772c8c32c5b6",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T14:39:22.697878200Z",
     "start_time": "2024-02-29T14:39:22.658878900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working as a Data Analyst in 3 jobs and 5 research projects, I am highly experienced in cooperation as well as leadership, having helped groups develop statistical analysis and gain more insights. I am a team-player, risk taker, and positive- minded individual who looks forward to bringing change in my community. I am seeking a data analyst role in technology field, which will enable me to contribute my skills and enrich my knowledge.\n",
      "\n",
      "EDUCATION New York University, Center for Data Science, NY Master of Science in Data Science\n",
      "\n",
      "09/2022-05/2024\n",
      "\n",
      "\n",
      "\n",
      "Courses to be taken: Intro to Data Science, Probability and Statistics for Data Science, Linear Algebra and Optimization, Machine Learning (expected), Big Data (expected), Database Management (expected), Natural Language Processing (expected)\n"
     ]
    }
   ],
   "source": [
    "# docs = retriever.invoke(\"which candidate is good fit for Data Analyst roles.\")\n",
    "docs = retriever.invoke(\"Give name of candidate who is good fit for Data Analyst roles.\")\n",
    "\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe1679ca1980f9e4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T14:39:22.930894600Z",
     "start_time": "2024-02-29T14:39:22.687877900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\jaini\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "huggingface_hub.login(token =  \"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8aafa4952fffc0b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T14:40:30.487972800Z",
     "start_time": "2024-02-29T14:39:22.926901600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e860026355ff4b1588b37c505fd07296"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "import torch\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\", padding=True, truncation=True, max_length=512)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T14:40:30.794921300Z",
     "start_time": "2024-02-29T14:40:30.410514600Z"
    }
   },
   "id": "a6465ae331fab070"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c644f6900c5c3b3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T14:42:00.028139500Z",
     "start_time": "2024-02-29T14:41:02.257884900Z"
    }
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 12.40 GiB is allocated by PyTorch, and 1.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m pipe \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtext-generation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m \u001B[49m\u001B[43mreturn_tensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m512\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m512\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m \u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtorch_dtype\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbfloat16\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[0;32m     10\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m llm \u001B[38;5;241m=\u001B[39m HuggingFacePipeline(\n\u001B[0;32m     13\u001B[0m  pipeline\u001B[38;5;241m=\u001B[39mpipe,\n\u001B[0;32m     14\u001B[0m  model_kwargs\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemperature\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m0.7\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_length\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m512\u001B[39m},\n\u001B[0;32m     15\u001B[0m )\n",
      "File \u001B[1;32m~\\PycharmProjects\\mined_resume\\venv\\lib\\site-packages\\transformers\\pipelines\\__init__.py:1107\u001B[0m, in \u001B[0;36mpipeline\u001B[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001B[0m\n\u001B[0;32m   1104\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m device \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1105\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdevice\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m device\n\u001B[1;32m-> 1107\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pipeline_class(model\u001B[38;5;241m=\u001B[39mmodel, framework\u001B[38;5;241m=\u001B[39mframework, task\u001B[38;5;241m=\u001B[39mtask, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\mined_resume\\venv\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:84\u001B[0m, in \u001B[0;36mTextGenerationPipeline.__init__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     83\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m---> 84\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     85\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_model_type(\n\u001B[0;32m     86\u001B[0m         TF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n\u001B[0;32m     87\u001B[0m     )\n\u001B[0;32m     88\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprefix\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_preprocess_params:\n\u001B[0;32m     89\u001B[0m         \u001B[38;5;66;03m# This is very specific. The logic is quite complex and needs to be done\u001B[39;00m\n\u001B[0;32m     90\u001B[0m         \u001B[38;5;66;03m# as a \"default\".\u001B[39;00m\n\u001B[0;32m     91\u001B[0m         \u001B[38;5;66;03m# It also defines both some preprocess_kwargs and generate_kwargs\u001B[39;00m\n\u001B[0;32m     92\u001B[0m         \u001B[38;5;66;03m# which is why we cannot put them in their respective methods.\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\mined_resume\\venv\\lib\\site-packages\\transformers\\pipelines\\base.py:874\u001B[0m, in \u001B[0;36mPipeline.__init__\u001B[1;34m(self, model, tokenizer, feature_extractor, image_processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001B[0m\n\u001B[0;32m    867\u001B[0m \u001B[38;5;66;03m# We shouldn't call `model.to()` for models loaded with accelerate\u001B[39;00m\n\u001B[0;32m    868\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    869\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    870\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    871\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m    872\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m hf_device_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    873\u001B[0m ):\n\u001B[1;32m--> 874\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    876\u001B[0m \u001B[38;5;66;03m# Update config and generation_config with task specific parameters\u001B[39;00m\n\u001B[0;32m    877\u001B[0m task_specific_params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mtask_specific_params\n",
      "File \u001B[1;32m~\\PycharmProjects\\mined_resume\\venv\\lib\\site-packages\\transformers\\modeling_utils.py:2556\u001B[0m, in \u001B[0;36mPreTrainedModel.to\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2551\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype_present_in_args:\n\u001B[0;32m   2552\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   2553\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2554\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2555\u001B[0m         )\n\u001B[1;32m-> 2556\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\mined_resume\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1152\u001B[0m, in \u001B[0;36mModule.to\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1148\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1149\u001B[0m                     non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[0;32m   1150\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, non_blocking)\n\u001B[1;32m-> 1152\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\mined_resume\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    800\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    801\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 802\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    804\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    805\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    806\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    807\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    812\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    813\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\mined_resume\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    800\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    801\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 802\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    804\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    805\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    806\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    807\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    812\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    813\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "    \u001B[1;31m[... skipping similar frames: Module._apply at line 802 (2 times)]\u001B[0m\n",
      "File \u001B[1;32m~\\PycharmProjects\\mined_resume\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    800\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    801\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 802\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    804\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    805\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    806\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    807\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    812\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    813\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\mined_resume\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:825\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    821\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[0;32m    822\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[0;32m    823\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[0;32m    824\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 825\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    826\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[0;32m    827\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[1;32m~\\PycharmProjects\\mined_resume\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1150\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m   1147\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[0;32m   1148\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1149\u001B[0m                 non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[1;32m-> 1150\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 12.40 GiB is allocated by PyTorch, and 1.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    " \"text-generation\", \n",
    " model=model, \n",
    " tokenizer=tokenizer,\n",
    " return_tensors='pt',\n",
    " max_length=512,\n",
    " max_new_tokens=512,\n",
    " model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    " device=\"cuda\"\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    " pipeline=pipe,\n",
    " model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e54954aa3b41518",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-29T13:35:24.123223Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m qa_chain \u001B[38;5;241m=\u001B[39m RetrievalQA\u001B[38;5;241m.\u001B[39mfrom_chain_type(llm\u001B[38;5;241m=\u001B[39m\u001B[43mllm\u001B[49m, \n\u001B[0;32m      2\u001B[0m                                   chain_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstuff\u001B[39m\u001B[38;5;124m\"\u001B[39m, \n\u001B[0;32m      3\u001B[0m                                   retriever\u001B[38;5;241m=\u001B[39mretriever, \n\u001B[0;32m      4\u001B[0m                                   return_source_documents\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprocess_llm_response\u001B[39m(llm_response):\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28mprint\u001B[39m(llm_response[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresult\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "\u001B[1;31mNameError\u001B[0m: name 'llm' is not defined"
     ]
    }
   ],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                  chain_type=\"stuff\", \n",
    "                                  retriever=retriever, \n",
    "                                  return_source_documents=True)\n",
    "\n",
    "\n",
    "def process_llm_response(llm_response):\n",
    "    print(llm_response['result'])\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba2ac8c03db3ddf",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6ec2d7a4c7cff2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# full example\n",
    "warning = \"If you don't know the answer, just say that you don't know, don't try to make up an answer\"\n",
    "job_description = \"MS or PhD in computer science or a related technical field,5+ years of industry work experience. Good sense of product with a focus on shipping user-facing data-driven features, Expertise in Python and Python based ML/DL and Data Science frameworks. \\\n",
    "Excellent coding, analysis, and problem-solving skills. Proven knowledge of data structure and algorithms. \\\n",
    "Familiarity in relevant machine learning frameworks and packages such as Tensorflow, PyTorch and HuggingFace\\\n",
    "Experience working with Product Management and decomposing feature requirements into technical work items to ship products\\\n",
    "Experience with generative AI, knowledge of ML Ops and ML services is a plus. This includes Pinecone, LangChain, Weights and Biases etc. \\\n",
    "Familiarity with deployment technologies such as Docker, Kubernetes and Triton are a plus\\\n",
    "Strong communication and collaboration skills\"\n",
    "question = warning+job_description + \" Based on the given job description\"\n",
    "query = question + \"short list resumes which is good fit based on skills,education and work experience mwntioned in it? also provide the candidate name which will be mentioned in first line of pdf without subheading\"\n",
    "# query = \"short list resumes which is good fit for Data analysis roles based on skills,education and work experience mwntioned in it?\"\n",
    "\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a885c3e788744ab",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "warning = \"If you don't know the answer, just say that you don't know, don't try to make up an answer\"\n",
    "job_description = \"MS or PhD in computer science or a related technical field,5+ years of industry work experience. Good sense of product with a focus on shipping user-facing data-driven features, Expertise in Python and Python based ML/DL and Data Science frameworks. \\\n",
    "Excellent coding, analysis, and problem-solving skills. Proven knowledge of data structure and algorithms. \\\n",
    "Familiarity in relevant machine learning frameworks and packages such as Tensorflow, PyTorch and HuggingFace\\\n",
    "Experience working with Product Management and decomposing feature requirements into technical work items to ship products\\\n",
    "Experience with generative AI, knowledge of ML Ops and ML services is a plus. This includes Pinecone, LangChain, Weights and Biases etc. \\\n",
    "Familiarity with deployment technologies such as Docker, Kubernetes and Triton are a plus\\\n",
    "Strong communication and collaboration skills\"\n",
    "question = warning+job_description + \" Based on the given job description\"\n",
    "query = question + \"retrive the full document information of a resume which is good fit based on skills,education and work experience mwntioned in it? \"\n",
    "\n",
    "resume_doc = retriever.invoke(query)\n",
    "\n",
    "print(resume_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1780159cfd91af99",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# tempelate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f81666a9bb4c13",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resume_doc = resume_doc[0].page_content\n",
    "print(resume_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f38e5d9d77ce03",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "Skills: what are the technical and non technical skills? \\\n",
    "Answer output them as a comma separated Python list.\n",
    "\n",
    "Education: What is the highest education of the candidate and what is the GPA as mentioned in the text?\\\n",
    "Answer Output should be the university/college name and GPA if given in text, output them as a comma separated Python list.\n",
    "\n",
    "Projects: Extract all project titles mentioned in a text\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Publications: Extract all publication titles mentioned in a text\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Work experience: Extract all organisation name where he/she has worked along with number of years or months worked there and also extract designation\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "Skills\n",
    "Education\n",
    "Projects\n",
    "Publications\n",
    "Work experience\n",
    "\n",
    "text: {text}\n",
    "\"\"\"\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127c54f652cf4389",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "llm = HuggingFacePipeline(\n",
    " pipeline=pipe,\n",
    " model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
    ")\n",
    "\n",
    "memory_llm_conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "messages = prompt_template.format_messages(text=resume_doc)\n",
    "# chat = ChatOpenAI(temperature=0.0, model=turbo_llm_memory)\n",
    "response = memory_llm_conversation(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ea44fee183d21",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(response)\n",
    "\n",
    "print(response.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b506f4c3a810553b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3637aa44069b28c9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
