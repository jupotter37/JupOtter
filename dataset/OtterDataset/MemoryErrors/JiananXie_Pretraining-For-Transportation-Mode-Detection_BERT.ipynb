{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "# path = 'D:/SHL_dataset' #解压文件的文件夹目录\n",
    "# target_folder = 'D:/SHL_dataset'#要提取到的文件夹目录\n",
    "# folder_list = os.listdir(path=path + '/SHLDataset_User1Hips_v2/release/User1')\n",
    "# for folder in folder_list:\n",
    "#     file_name_motion = 'Hips_Motion.txt'\n",
    "#     file_name_label = 'Label.txt'\n",
    "#     source_path_motion = os.path.join(path + '/SHLDataset_User1Hips_v2/release/User1/'+folder, file_name_motion)\n",
    "#     source_path_label = os.path.join(path + '/SHLDataset_User1Hips_v2/release/User1/'+folder, file_name_label)\n",
    "#     target_path_motion = os.path.join(target_folder , folder)\n",
    "#     target_path_label = os.path.join(target_folder, folder)\n",
    "    \n",
    "#     if os.path.isfile(source_path_motion) and os.path.isfile(source_path_label):\n",
    "#         os.makedirs(os.path.join(target_folder, folder))\n",
    "#         shutil.move(source_path_motion, target_path_motion)\n",
    "#         shutil.move(source_path_label, target_path_label)\n",
    "#         print(f\"Moved file {source_path_motion} and {source_path_label} to {target_path_motion},{target_path_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['170301',\n",
       "  '170302',\n",
       "  '170303',\n",
       "  '170306',\n",
       "  '170307',\n",
       "  '170308',\n",
       "  '170309',\n",
       "  '170310',\n",
       "  '170313',\n",
       "  '170314',\n",
       "  '170315',\n",
       "  '170316',\n",
       "  '170317',\n",
       "  '170321',\n",
       "  '170322',\n",
       "  '170323',\n",
       "  '170324',\n",
       "  '170325',\n",
       "  '170327',\n",
       "  '170328',\n",
       "  '170329',\n",
       "  '170330',\n",
       "  '170419',\n",
       "  '170420',\n",
       "  '170424',\n",
       "  '170425',\n",
       "  '170426',\n",
       "  '170427',\n",
       "  '170428',\n",
       "  '170502',\n",
       "  '170503',\n",
       "  '170504',\n",
       "  '170505',\n",
       "  '170508',\n",
       "  '170509',\n",
       "  '170510',\n",
       "  '170511',\n",
       "  '170512',\n",
       "  '170515',\n",
       "  '170517',\n",
       "  '170519',\n",
       "  '170520',\n",
       "  '170522',\n",
       "  '170523',\n",
       "  '170524',\n",
       "  '170525',\n",
       "  '170526',\n",
       "  '170529',\n",
       "  '170530',\n",
       "  '170531',\n",
       "  '170601',\n",
       "  '170602',\n",
       "  '170603',\n",
       "  '170605',\n",
       "  '170606',\n",
       "  '170607',\n",
       "  '170608',\n",
       "  '170609',\n",
       "  '170612',\n",
       "  '170613',\n",
       "  '170614',\n",
       "  '170615',\n",
       "  '170620',\n",
       "  '170622',\n",
       "  '170623',\n",
       "  '170626',\n",
       "  '170627',\n",
       "  '170628',\n",
       "  '170629',\n",
       "  '170630',\n",
       "  '170703',\n",
       "  '170704',\n",
       "  '170705'],\n",
       " 73,\n",
       " ['m170301',\n",
       "  'm170303',\n",
       "  'm170317',\n",
       "  'm170324',\n",
       "  'm170329',\n",
       "  'm170424',\n",
       "  'm170428',\n",
       "  'm170508',\n",
       "  'm170515',\n",
       "  'm170522',\n",
       "  'm170530',\n",
       "  'm170605',\n",
       "  'm170609',\n",
       "  'm170629',\n",
       "  'm170706'],\n",
       " 15)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "folder_list = os.listdir('D:/SHL/SHL_dataset')\n",
    "day_list = []\n",
    "miday_list = []\n",
    "for l in folder_list:\n",
    "    if 'm' not in l:\n",
    "        day_list.append(l)\n",
    "    else:\n",
    "        miday_list.append(l)\n",
    "\n",
    "day_list = [ x[4:6] + x[2:4] + x[0:2] for x in day_list]\n",
    "day_list.sort()\n",
    "miday_list= [x[0] + x[5:7] + x[3:5] + x[1:3] for x in miday_list]\n",
    "miday_list.sort()\n",
    "day_list, len(day_list), miday_list, len(miday_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on 010317...\n",
      "working on 010617...\n",
      "working on 020317...\n",
      "working on 020517...\n",
      "working on 020617...\n",
      "working on 030317...\n",
      "working on 030517...\n",
      "working on 030617...\n",
      "working on 030717...\n",
      "working on 040517...\n",
      "working on 040717...\n",
      "working on 050517...\n",
      "working on 050617...\n",
      "working on 050717...\n",
      "working on 060317...\n",
      "working on 060617...\n",
      "working on 070317...\n",
      "working on 070617...\n",
      "working on 080317...\n",
      "working on 080517...\n",
      "working on 080617...\n",
      "working on 090317...\n",
      "working on 090517...\n",
      "working on 090617...\n",
      "working on 100317...\n",
      "working on 100517...\n",
      "working on 110517...\n",
      "working on 120517...\n",
      "working on 120617...\n",
      "working on 130317...\n",
      "working on 130617...\n",
      "working on 140317...\n",
      "working on 140617...\n",
      "working on 150317...\n",
      "working on 150517...\n",
      "working on 150617...\n",
      "working on 160317...\n",
      "working on 170317...\n",
      "working on 170517...\n",
      "working on 180417...\n",
      "working on 190417...\n",
      "working on 190517...\n",
      "working on 200317...\n",
      "working on 200417...\n",
      "working on 200517...\n",
      "working on 200617...\n",
      "working on 210317...\n",
      "working on 220317...\n",
      "working on 220517...\n",
      "working on 220617...\n",
      "working on 230317...\n",
      "working on 230517...\n",
      "working on 230617...\n",
      "working on 240317...\n",
      "working on 240417...\n",
      "working on 240517...\n",
      "working on 250317...\n",
      "working on 250417...\n",
      "working on 250517...\n",
      "working on 260417...\n",
      "working on 260517...\n",
      "working on 260617...\n",
      "working on 270317...\n",
      "working on 270417...\n",
      "working on 270617...\n",
      "working on 280317...\n",
      "working on 280417...\n",
      "working on 280617...\n",
      "working on 290317...\n",
      "working on 290517...\n",
      "working on 290617...\n",
      "working on 300317...\n",
      "working on 300517...\n",
      "working on 300617...\n",
      "working on 310517...\n",
      "working on m010317...\n",
      "working on m030317...\n",
      "working on m050617...\n",
      "working on m060717...\n",
      "working on m080517...\n",
      "working on m090617...\n",
      "working on m150517...\n",
      "working on m170317...\n",
      "working on m220517...\n",
      "working on m240317...\n",
      "working on m240417...\n",
      "working on m280417...\n",
      "working on m290317...\n",
      "working on m290617...\n",
      "working on m300517...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "140729469"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# #统计data无nan值同时label非null的数据量\n",
    "# path = 'D:/SHL/SHL_dataset/'\n",
    "# data_file = '/Hips_Motion.txt'\n",
    "# label_file = '/Label.txt'\n",
    "# num = 0\n",
    "# valid_timestamp = []\n",
    "# for folder in folder_list:\n",
    "#     print(f'working on {folder}...')\n",
    "#     df = pd.read_csv(path + folder + data_file, header=None, delimiter=' ')\n",
    "#     df_label = pd.read_csv(path + folder + label_file, header=None, delimiter=' ')\n",
    "#     no_nan_df = df[df.notna().all(axis=1)]\n",
    "#     no_null_df_label = df_label[df_label[1] != 0]\n",
    "#     data_1 = set(no_nan_df[0].astype('int64'))\n",
    "#     data_2 = set(no_null_df_label[0])\n",
    "#     data_3 = data_1.intersection(data_2)\n",
    "#     num += len(data_3)\n",
    "#     valid_timestamp.append(list(data_3))\n",
    "# num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #删除\n",
    "# for folder in folder_list:\n",
    "#     os.remove(path + folder + '/Hips_Motion.csv')\n",
    "#     os.remove(path + folder + '/Label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on 010317\n",
      "working on 010617\n",
      "working on 020317\n",
      "working on 020517\n",
      "working on 020617\n",
      "working on 030317\n",
      "working on 030517\n",
      "working on 030617\n",
      "working on 030717\n",
      "working on 040517\n",
      "working on 040717\n",
      "working on 050517\n",
      "working on 050617\n",
      "working on 050717\n",
      "working on 060317\n",
      "working on 060617\n",
      "working on 070317\n",
      "working on 070617\n",
      "working on 080317\n",
      "working on 080517\n",
      "working on 080617\n",
      "working on 090317\n",
      "working on 090517\n",
      "working on 090617\n",
      "working on 100317\n",
      "working on 100517\n",
      "working on 110517\n",
      "working on 120517\n",
      "working on 120617\n",
      "working on 130317\n",
      "working on 130617\n",
      "working on 140317\n",
      "working on 140617\n",
      "working on 150317\n",
      "working on 150517\n",
      "working on 150617\n",
      "working on 160317\n",
      "working on 170317\n",
      "working on 170517\n",
      "working on 180417\n",
      "working on 190417\n",
      "working on 190517\n",
      "working on 200317\n",
      "working on 200417\n",
      "working on 200517\n",
      "working on 200617\n",
      "working on 210317\n",
      "working on 220317\n",
      "working on 220517\n",
      "working on 220617\n",
      "working on 230317\n",
      "working on 230517\n",
      "working on 230617\n",
      "working on 240317\n",
      "working on 240417\n",
      "working on 240517\n",
      "working on 250317\n",
      "working on 250417\n",
      "working on 250517\n",
      "working on 260417\n",
      "working on 260517\n",
      "working on 260617\n",
      "working on 270317\n",
      "working on 270417\n",
      "working on 270617\n",
      "working on 280317\n",
      "working on 280417\n",
      "working on 280617\n",
      "working on 290317\n",
      "working on 290517\n",
      "working on 290617\n",
      "working on 300317\n",
      "working on 300517\n",
      "working on 300617\n",
      "working on 310517\n",
      "working on m010317\n",
      "working on m030317\n",
      "working on m050617\n",
      "working on m060717\n",
      "working on m080517\n",
      "working on m090617\n",
      "working on m150517\n",
      "working on m170317\n",
      "working on m220517\n",
      "working on m240317\n",
      "working on m240417\n",
      "working on m280417\n",
      "working on m290317\n",
      "working on m290617\n",
      "working on m300517\n"
     ]
    }
   ],
   "source": [
    "# #整理出新的数据集\n",
    "# for id, folder in enumerate(folder_list):\n",
    "#     print(f'working on {folder}')\n",
    "#     df = pd.read_csv(path + folder + data_file, header=None, delimiter=' ')\n",
    "#     df_label = pd.read_csv(path + folder + label_file, header=None, delimiter=' ')\n",
    "#     data = df[df[0].astype('int64').isin(valid_timestamp[id])]\n",
    "#     label = df_label[df_label[0].isin(valid_timestamp[id])]\n",
    "#     os.remove(path + folder + '/Hips_Motion.txt')\n",
    "#     os.remove(path + folder + '/Label.txt')\n",
    "#     data.to_csv(path + folder + '/Hips_Motion.csv', index=False)\n",
    "#     label.to_csv(path + folder + '/Label.csv',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2017, 3, 18, 2, 38, 48, 740000)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime(2017,3,1,13,57,2).timestamp()\n",
    "timestamp*1000\n",
    "time = datetime.datetime.fromtimestamp(1489775928740/1000)\n",
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "labels = torch.tensor([1]).unsqueeze(1)# Batch size 1\n",
    "loss, logits = model(**inputs, labels=labels)[:2]\n",
    "# loss, logits = outputs[:2]\n",
    "loss, logits, logits.argmax(dim=1)\n",
    "# paraphrase_results = round(torch.softmax(logits, dim=1).tolist()[0][1] * 100,2)\n",
    "# paraphrase_results\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "prob = F.softmax(logits,1)\n",
    "prob.argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_list = os.listdir('D:/SHL/SHL_dataset')\n",
    "len(folder_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20596912., 18629527.,  6365389., 17385286., 24565677., 18395128.,\n",
       "       20249140., 14542410.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#统计下所有类别下的数据量\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "path = 'D:/SHL/SHL_dataset/'\n",
    "data_file = '/Hips_Motion.csv'\n",
    "label_file = '/Label.csv'\n",
    "label = np.zeros(8)\n",
    "for folder in folder_list:\n",
    "    df_label = pd.read_csv(path + folder + label_file)\n",
    "    count = df_label['1'].value_counts()\n",
    "    for id in count.index:\n",
    "        label[id-1] += count[id]\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['2', '[]', '1', '4'], dtype='object', name='1')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label = pd.read_csv(path + '010317' + label_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对label进行1 min重采样，以众数作为标签\n",
    "for folder in folder_list:\n",
    "    df_label = pd.read_csv(path + folder + label_file)\n",
    "    try:\n",
    "        df_label['0'] = pd.to_datetime(df_label['0'], unit='ms')\n",
    "        resample = df_label.resample('T',on='0').apply(lambda x:x.mode()).reset_index()\n",
    "        os.remove(path + folder + label_file)\n",
    "        resample.to_csv(path + folder + label_file,index= False)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    except TypeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对Data进行1 min重采样，取均值\n",
    "for folder in folder_list:\n",
    "    df_data = pd.read_csv(path + folder + data_file)\n",
    "    try:\n",
    "        df_data['0'] = pd.to_datetime(df_data['0'], unit='ms')\n",
    "        resample = df_data.resample('T',on='0').mean().reset_index()\n",
    "        os.remove(path + folder + data_file)\n",
    "        resample.to_csv(path + folder + data_file,index= False)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    except TypeError:\n",
    "        pass\n",
    "    except AttributeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24609"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#计数现在的数据集大小\n",
    "count = 0\n",
    "for folder in folder_list:\n",
    "    df_data = pd.read_csv(path + folder + data_file)\n",
    "    df_data.dropna(how='any', inplace=True)\n",
    "    count += len(df_data)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建最终input数据集和对应label数据集，并按时间升序排列\n",
    "data_list = []\n",
    "label_list = []\n",
    "for folder in folder_list:\n",
    "    df_data = pd.read_csv(path + folder + data_file)\n",
    "    df_label = pd.read_csv(path + folder + label_file)\n",
    "    df_label = df_label[df_label['1'] != '[]']\n",
    "    df_data.dropna(how='any', inplace=True)\n",
    "    data_list.append(df_data)\n",
    "    label_list.append(df_label)\n",
    "\n",
    "final_data = pd.concat(data_list, axis=0, join='outer', ignore_index=True)\n",
    "final_label = pd.concat(label_list, axis=0, join='outer', ignore_index=True)\n",
    "sorted_by_timestamp_data = final_data.sort_values('0')\n",
    "sorted_by_timestamp_label = final_label.sort_values('0')\n",
    "#只保留label列，并为了后续实验调整类别编号\n",
    "sorted_by_timestamp_label = sorted_by_timestamp_label[['0','1']]\n",
    "sorted_by_timestamp_label['1'] = sorted_by_timestamp_label['1'].astype(int) - 1 \n",
    "\n",
    "sorted_by_timestamp_data.to_csv('final_data.csv', index=False)\n",
    "sorted_by_timestamp_label.to_csv('final_label.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data = pd.read_csv('final_data.csv')\n",
    "label = pd.read_csv('final_label.csv')\n",
    "#去除时间戳列\n",
    "data.drop('0',axis=1,inplace=True)\n",
    "label.drop('0',axis=1,inplace=True)\n",
    "\n",
    "# epsilon = 0.00000001\n",
    "scaler = MinMaxScaler()\n",
    "data[data.columns] = scaler.fit_transform(data[data.columns])\n",
    "# data = data + epsilon\n",
    "data = data.round(2)\n",
    "\n",
    "data_sentence = data.apply(lambda x: ' '.join(x.astype(str)), axis=1)\n",
    "\n",
    "input = pd.DataFrame()\n",
    "input['text'] = data_sentence\n",
    "input['label'] = label\n",
    "\n",
    "Classification_train = []\n",
    "Classification_valid = []\n",
    "Classification_test = []\n",
    "for l in input['label'].unique():\n",
    "    temp = input[input['label'] == l]\n",
    "    train, valid_test = np.split(temp.sample(frac=1,random_state=42), [int(0.8*len(temp))], axis=0)\n",
    "    valid, test = np.split(valid_test.sample(frac=1,random_state=42), [int(0.5*len(valid_test))], axis=0)\n",
    "    Classification_train.append(train)\n",
    "    Classification_valid.append(valid)\n",
    "    Classification_test.append(test)\n",
    "data_train = pd.concat(Classification_train, axis=0, join='outer', ignore_index=True)\n",
    "data_valid = pd.concat(Classification_valid, axis=0, join='outer', ignore_index=True)\n",
    "data_test = pd.concat(Classification_test, axis=0, join='outer', ignore_index=True)\n",
    "\n",
    "data_train.to_csv('data_train.csv', index=False)\n",
    "data_valid.to_csv('data_valid.csv', index=False)\n",
    "data_test.to_csv('data_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19684, 2460, 2465)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train), len(data_valid), len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24609, 24609)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('final_data.csv')\n",
    "label = pd.read_csv('final_label.csv')\n",
    "#去除时间戳列\n",
    "data.drop('0',axis=1,inplace=True)\n",
    "label.drop('0',axis=1,inplace=True)\n",
    "len(data), len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.22</td>\n",
       "      <td>...</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24604</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24605</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24606</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24607</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.36</td>\n",
       "      <td>...</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24608</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.41</td>\n",
       "      <td>...</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24609 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1     2     3     4     5     6     7     8     9    10  ...    13  \\\n",
       "0      0.64  0.96  0.56  0.69  0.45  0.67  0.84  0.50  0.13  0.22  ...  0.83   \n",
       "1      0.59  0.98  0.53  0.69  0.46  0.68  0.84  0.50  0.13  0.28  ...  0.45   \n",
       "2      0.61  0.97  0.55  0.68  0.45  0.69  0.84  0.50  0.13  0.21  ...  0.61   \n",
       "3      0.67  0.97  0.54  0.69  0.46  0.67  0.84  0.50  0.13  0.16  ...  0.83   \n",
       "4      0.66  0.97  0.54  0.69  0.46  0.68  0.84  0.50  0.13  0.15  ...  0.72   \n",
       "...     ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "24604  0.56  0.12  0.46  0.68  0.45  0.68  0.84  0.54  0.13  0.23  ...  0.59   \n",
       "24605  0.56  0.12  0.47  0.67  0.45  0.67  0.84  0.54  0.13  0.11  ...  0.66   \n",
       "24606  0.56  0.12  0.46  0.67  0.46  0.67  0.84  0.54  0.13  0.11  ...  0.56   \n",
       "24607  0.57  0.13  0.45  0.67  0.46  0.68  0.85  0.54  0.13  0.36  ...  0.73   \n",
       "24608  0.56  0.12  0.47  0.67  0.45  0.69  0.85  0.54  0.13  0.41  ...  0.78   \n",
       "\n",
       "         14    15    16    17    18    19    20    21    22  \n",
       "0      0.61  0.96  0.56  0.32  0.64  0.50  0.35  0.81  0.69  \n",
       "1      0.55  0.98  0.53  0.32  0.64  0.51  0.35  0.81  0.69  \n",
       "2      0.57  0.98  0.55  0.32  0.64  0.51  0.35  0.81  0.68  \n",
       "3      0.64  0.97  0.53  0.33  0.64  0.52  0.35  0.81  0.67  \n",
       "4      0.63  0.97  0.54  0.32  0.64  0.51  0.35  0.81  0.67  \n",
       "...     ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "24604  0.56  0.02  0.50  0.26  0.55  0.45  0.58  0.45  0.00  \n",
       "24605  0.55  0.01  0.52  0.27  0.55  0.42  0.58  0.45  0.00  \n",
       "24606  0.56  0.02  0.49  0.25  0.54  0.46  0.58  0.45  0.00  \n",
       "24607  0.58  0.03  0.47  0.24  0.54  0.48  0.57  0.45  0.00  \n",
       "24608  0.56  0.02  0.50  0.25  0.54  0.46  0.57  0.45  0.00  \n",
       "\n",
       "[24609 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#标准化数值列\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# epsilon = 0.00000001\n",
    "scaler = MinMaxScaler()\n",
    "data[data.columns] = scaler.fit_transform(data[data.columns])\n",
    "# data = data + epsilon\n",
    "data = data.round(2)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.64 0.96 0.56 0.69 0.45 0.67 0.84 0.5 0.13 0....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.59 0.98 0.53 0.69 0.46 0.68 0.84 0.5 0.13 0....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.61 0.97 0.55 0.68 0.45 0.69 0.84 0.5 0.13 0....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.67 0.97 0.54 0.69 0.46 0.67 0.84 0.5 0.13 0....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.66 0.97 0.54 0.69 0.46 0.68 0.84 0.5 0.13 0....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24604</th>\n",
       "      <td>0.56 0.12 0.46 0.68 0.45 0.68 0.84 0.54 0.13 0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24605</th>\n",
       "      <td>0.56 0.12 0.47 0.67 0.45 0.67 0.84 0.54 0.13 0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24606</th>\n",
       "      <td>0.56 0.12 0.46 0.67 0.46 0.67 0.84 0.54 0.13 0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24607</th>\n",
       "      <td>0.57 0.13 0.45 0.67 0.46 0.68 0.85 0.54 0.13 0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24608</th>\n",
       "      <td>0.56 0.12 0.47 0.67 0.45 0.69 0.85 0.54 0.13 0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24609 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      0.64 0.96 0.56 0.69 0.45 0.67 0.84 0.5 0.13 0....      1\n",
       "1      0.59 0.98 0.53 0.69 0.46 0.68 0.84 0.5 0.13 0....      1\n",
       "2      0.61 0.97 0.55 0.68 0.45 0.69 0.84 0.5 0.13 0....      1\n",
       "3      0.67 0.97 0.54 0.69 0.46 0.67 0.84 0.5 0.13 0....      1\n",
       "4      0.66 0.97 0.54 0.69 0.46 0.68 0.84 0.5 0.13 0....      1\n",
       "...                                                  ...    ...\n",
       "24604  0.56 0.12 0.46 0.68 0.45 0.68 0.84 0.54 0.13 0...      1\n",
       "24605  0.56 0.12 0.47 0.67 0.45 0.67 0.84 0.54 0.13 0...      1\n",
       "24606  0.56 0.12 0.46 0.67 0.46 0.67 0.84 0.54 0.13 0...      1\n",
       "24607  0.57 0.13 0.45 0.67 0.46 0.68 0.85 0.54 0.13 0...      1\n",
       "24608  0.56 0.12 0.47 0.67 0.45 0.69 0.85 0.54 0.13 0...      1\n",
       "\n",
       "[24609 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#以每个属性值作为word，整个作为sentence输入\n",
    "data_sentence = data.apply(lambda x: ' '.join(x.astype(str)), axis=1)\n",
    "data_sentence\n",
    "input = pd.DataFrame()\n",
    "input['text'] = data_sentence\n",
    "input['label'] = label\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                    text  label\n",
       " 0      0.86 0.84 0.44 0.68 0.47 0.69 0.84 0.5 0.13 0....      1\n",
       " 1      0.51 0.13 0.53 0.69 0.46 0.61 0.85 0.54 0.13 0...      1\n",
       " 2      0.58 0.15 0.47 0.68 0.43 0.65 0.84 0.54 0.13 0...      1\n",
       " 3      0.98 0.53 0.56 0.69 0.45 0.67 0.83 0.53 0.13 0...      1\n",
       " 4      0.66 0.13 0.47 0.68 0.46 0.68 0.84 0.54 0.13 0...      1\n",
       " ...                                                  ...    ...\n",
       " 20909  0.17 0.61 0.33 0.68 0.46 0.64 0.87 0.51 0.13 0...      4\n",
       " 20910  0.64 0.68 0.03 0.68 0.46 0.64 0.85 0.52 0.14 0...      4\n",
       " 20911  0.66 0.67 0.04 0.68 0.46 0.64 0.84 0.52 0.13 0...      4\n",
       " 20912  0.64 0.7 0.04 0.68 0.46 0.64 0.85 0.52 0.14 0....      4\n",
       " 20913  0.31 0.58 0.09 0.68 0.46 0.64 0.85 0.53 0.14 0...      4\n",
       " \n",
       " [20914 rows x 2 columns],\n",
       "                                                    text  label\n",
       " 0     0.52 0.13 0.54 0.69 0.47 0.59 0.84 0.55 0.13 0...      1\n",
       " 1     0.55 0.12 0.49 0.67 0.47 0.58 0.85 0.54 0.14 0...      1\n",
       " 2     0.57 0.11 0.46 0.68 0.46 0.68 0.84 0.55 0.13 0...      1\n",
       " 3     0.25 0.25 0.44 0.69 0.46 0.67 0.85 0.54 0.13 0...      1\n",
       " 4     0.59 0.15 0.43 0.67 0.45 0.67 0.84 0.54 0.13 0...      1\n",
       " ...                                                 ...    ...\n",
       " 3690  0.79 0.71 0.87 0.68 0.46 0.64 0.84 0.51 0.13 0...      4\n",
       " 3691  0.56 0.72 0.04 0.68 0.46 0.64 0.85 0.51 0.14 0...      4\n",
       " 3692  0.31 0.55 0.1 0.68 0.46 0.64 0.86 0.51 0.14 0....      4\n",
       " 3693  0.54 0.97 0.58 0.68 0.46 0.64 0.84 0.5 0.12 0....      4\n",
       " 3694  0.57 0.73 0.04 0.68 0.46 0.64 0.84 0.52 0.14 0...      4\n",
       " \n",
       " [3695 rows x 2 columns])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#从每个类里取85%的数据作为训练集，15%作为验证集\n",
    "import numpy as np\n",
    "Classification_train = []\n",
    "Classification_valid = []\n",
    "for l in input['label'].unique():\n",
    "    temp = input[input['label'] == l]\n",
    "    train, valid= np.split(temp.sample(frac=1,random_state=42) ,[int(0.85*len(temp))],axis=0)\n",
    "    Classification_train.append(train)\n",
    "    Classification_valid.append(valid)\n",
    "data_train = pd.concat(Classification_train, axis=0, join='outer', ignore_index=True)\n",
    "data_valid = pd.concat(Classification_valid, axis=0, join='outer', ignore_index=True)\n",
    "data_train, data_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = df['label'].values\n",
    "        self.texts = [tokenizer(text, \n",
    "                                padding='max_length', \n",
    "                                max_length = 256, \n",
    "                                truncation=True,\n",
    "                                \n",
    "                                return_tensors=\"pt\") \n",
    "                            for text in df['text']]\n",
    "        pass\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from Bert import Dataset, BertClassifier, BertClassifier_npre\n",
    "\n",
    "def train(model, train_data, val_data,learning_rate, epochs):\n",
    "    #获取训练和验证集\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "    #训练采用RandomSampler,验证采用SequentialSampler\n",
    "    train_dataloader = DataLoader(train, batch_size=2,sampler=RandomSampler(train))\n",
    "    val_dataloader = DataLoader(val,batch_size=2, sampler=SequentialSampler(val))\n",
    "    #判断是否使用GPU\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "    #定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "    total_step = len(train_dataloader)*epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0.1*total_step, num_training_steps=total_step)#参数待调\n",
    "\n",
    "    if use_cuda:\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.empty_cache()\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    \n",
    "    #循环训练\n",
    "    for epoch in range(epochs):\n",
    "        total_acc_train = 0 #记录训练集的准确率\n",
    "        total_loss_train = 0 #记录训练集的损失\n",
    "        #进度条函数\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            train_label = train_label.type(torch.LongTensor).to(device)\n",
    "            mask = train_input['attention_mask'].to(device)\n",
    "            input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "        #输出\n",
    "            output = model(input_id, mask)\n",
    "        #计算损失\n",
    "            batch_loss = criterion(output, train_label)\n",
    "            total_loss_train += batch_loss.item()\n",
    "        #计算精度\n",
    "            acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "        #模型更新\n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "    #验证\n",
    "        total_acc_val = 0#记录验证集的准确率\n",
    "        total_loss_val = 0#记录验证集的损失\n",
    "        with torch.no_grad():\n",
    "            for val_input, val_label in val_dataloader:\n",
    "                val_label = val_label.type(torch.LongTensor).to(device)\n",
    "                mask = val_input['attention_mask'].to(device)\n",
    "                input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "\n",
    "                batch_loss = criterion(output, val_label)\n",
    "                total_loss_val += batch_loss.item()\n",
    "\n",
    "                acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "        print(\n",
    "            f'''Epochs: {epoch + 1} \n",
    "            | Train Loss: {total_loss_train / len(train_data): .3f} \n",
    "            | Train Accuracy: {total_acc_train / len(train_data): .3f} \n",
    "            | Val Loss: {total_loss_val / len(val_data): .3f} \n",
    "            | Val Accuracy: {total_acc_val / len(val_data): .3f}''')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10457/10457 [34:58<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 \n",
      "            | Train Loss:  0.716 \n",
      "            | Train Accuracy:  0.449 \n",
      "            | Val Loss:  0.481 \n",
      "            | Val Accuracy:  0.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10457/10457 [33:16<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 \n",
      "            | Train Loss:  0.366 \n",
      "            | Train Accuracy:  0.734 \n",
      "            | Val Loss:  0.304 \n",
      "            | Val Accuracy:  0.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10457/10457 [36:52<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 \n",
      "            | Train Loss:  0.233 \n",
      "            | Train Accuracy:  0.836 \n",
      "            | Val Loss:  0.215 \n",
      "            | Val Accuracy:  0.848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10457/10457 [36:24<00:00,  4.79it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 \n",
      "            | Train Loss:  0.144 \n",
      "            | Train Accuracy:  0.903 \n",
      "            | Val Loss:  0.192 \n",
      "            | Val Accuracy:  0.870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10457/10457 [30:52<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 \n",
      "            | Train Loss:  0.085 \n",
      "            | Train Accuracy:  0.944 \n",
      "            | Val Loss:  0.164 \n",
      "            | Val Accuracy:  0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10457/10457 [37:06<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 \n",
      "            | Train Loss:  0.046 \n",
      "            | Train Accuracy:  0.971 \n",
      "            | Val Loss:  0.164 \n",
      "            | Val Accuracy:  0.902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10457/10457 [41:00<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 \n",
      "            | Train Loss:  0.022 \n",
      "            | Train Accuracy:  0.987 \n",
      "            | Val Loss:  0.166 \n",
      "            | Val Accuracy:  0.912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10457/10457 [34:34<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 \n",
      "            | Train Loss:  0.011 \n",
      "            | Train Accuracy:  0.994 \n",
      "            | Val Loss:  0.178 \n",
      "            | Val Accuracy:  0.916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10457/10457 [30:30<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 \n",
      "            | Train Loss:  0.004 \n",
      "            | Train Accuracy:  0.999 \n",
      "            | Val Loss:  0.176 \n",
      "            | Val Accuracy:  0.921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10457/10457 [30:26<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 \n",
      "            | Train Loss:  0.002 \n",
      "            | Train Accuracy:  0.999 \n",
      "            | Val Loss:  0.177 \n",
      "            | Val Accuracy:  0.921\n"
     ]
    }
   ],
   "source": [
    "model = BertClassifier()\n",
    "lr = 0.00001 \n",
    "epochs = 10\n",
    "train(model, data_train, data_valid, lr, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "class BertClassifier_npre(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super(BertClassifier_npre, self).__init__()\n",
    "        self.bert = BertModel(BertConfig().from_pretrained('bert-base-uncased'))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 8)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10457 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 3.20 GiB already allocated; 0 bytes free; 3.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mf:\\创新实践\\TMD-BERT\\BERT.ipynb 单元格 24\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/%E5%88%9B%E6%96%B0%E5%AE%9E%E8%B7%B5/TMD-BERT/BERT.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m lr \u001b[39m=\u001b[39m \u001b[39m0.00001\u001b[39m \n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/%E5%88%9B%E6%96%B0%E5%AE%9E%E8%B7%B5/TMD-BERT/BERT.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/%E5%88%9B%E6%96%B0%E5%AE%9E%E8%B7%B5/TMD-BERT/BERT.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train(model, data_train, data_valid, lr, epochs)\n",
      "\u001b[1;32mf:\\创新实践\\TMD-BERT\\BERT.ipynb 单元格 24\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/%E5%88%9B%E6%96%B0%E5%AE%9E%E8%B7%B5/TMD-BERT/BERT.ipynb#X32sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m         model\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/%E5%88%9B%E6%96%B0%E5%AE%9E%E8%B7%B5/TMD-BERT/BERT.ipynb#X32sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m         batch_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/%E5%88%9B%E6%96%B0%E5%AE%9E%E8%B7%B5/TMD-BERT/BERT.ipynb#X32sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m         optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/%E5%88%9B%E6%96%B0%E5%AE%9E%E8%B7%B5/TMD-BERT/BERT.ipynb#X32sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m         scheduler\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/%E5%88%9B%E6%96%B0%E5%AE%9E%E8%B7%B5/TMD-BERT/BERT.ipynb#X32sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m#验证\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     68\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\optim\\adam.py:132\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    129\u001b[0m     state_steps \u001b[39m=\u001b[39m []\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m--> 132\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m    135\u001b[0m         grads,\n\u001b[0;32m    136\u001b[0m         exp_avgs,\n\u001b[0;32m    137\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[0;32m    141\u001b[0m     adam(\n\u001b[0;32m    142\u001b[0m         params_with_grad,\n\u001b[0;32m    143\u001b[0m         grads,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39m\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfound_inf\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\optim\\adam.py:92\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[1;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[0;32m     86\u001b[0m state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m (\n\u001b[0;32m     87\u001b[0m     torch\u001b[39m.\u001b[39mzeros((\u001b[39m1\u001b[39m,), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat, device\u001b[39m=\u001b[39mp\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     88\u001b[0m     \u001b[39mif\u001b[39;00m group[\u001b[39m'\u001b[39m\u001b[39mcapturable\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mor\u001b[39;00m group[\u001b[39m'\u001b[39m\u001b[39mfused\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     89\u001b[0m     \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0.\u001b[39m)\n\u001b[0;32m     90\u001b[0m )\n\u001b[0;32m     91\u001b[0m \u001b[39m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m state[\u001b[39m'\u001b[39m\u001b[39mexp_avg\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros_like(p, memory_format\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mpreserve_format)\n\u001b[0;32m     93\u001b[0m \u001b[39m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[0;32m     94\u001b[0m state[\u001b[39m'\u001b[39m\u001b[39mexp_avg_sq\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(p, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 3.20 GiB already allocated; 0 bytes free; 3.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model = BertClassifier_npre()\n",
    "lr = 0.00001 \n",
    "epochs = 10\n",
    "train(model, data_train, data_valid, lr, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup, BertForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "from Bert import Dataset\n",
    "\n",
    "def training(model, train_data, val_data,learning_rate, epochs):\n",
    "    #获取训练和验证集\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "    #训练采用RandomSampler,验证采用SequentialSampler\n",
    "    train_dataloader = DataLoader(train, batch_size=2,sampler=RandomSampler(train))\n",
    "    val_dataloader = DataLoader(val,batch_size=2, sampler=SequentialSampler(val))\n",
    "    #判断是否使用GPU\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "    #定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "    total_step = len(train_dataloader)*epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0.1*total_step, num_training_steps=total_step)#参数待调\n",
    "\n",
    "    if use_cuda:\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.empty_cache()\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    \n",
    "    #循环训练\n",
    "    for epoch in range(epochs):\n",
    "        total_acc_train = 0 #记录训练集的准确率\n",
    "        total_loss_train = 0 #记录训练集的损失\n",
    "        #进度条函数\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            mask = train_input['attention_mask'].to(device)\n",
    "            input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "            train_label = train_label.type(torch.LongTensor).to(device)\n",
    "        #输出\n",
    "            outputs = model(**train_input.to(device), labels= train_label)\n",
    "\n",
    "            y_pred_prob = outputs[1]\n",
    "            y_pred_label = y_pred_prob.argmax(dim=1)\n",
    "        #计算损失\n",
    "            batch_loss = criterion(y_pred_prob, train_label)\n",
    "            total_loss_train += batch_loss.item()\n",
    "        #计算精度\n",
    "            acc = (y_pred_label == train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "        #模型更新\n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "    #验证\n",
    "        total_acc_val = 0#记录验证集的准确率\n",
    "        total_loss_val = 0#记录验证集的损失\n",
    "        with torch.no_grad():\n",
    "            for val_input, val_label in val_dataloader:\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "                val_label = val_label.type(torch.LongTensor).to(device)\n",
    "\n",
    "\n",
    "                outputs = model(input_id,mask,labels= val_label)\n",
    "\n",
    "                y_pred_prob = outputs[1]\n",
    "                y_pred_label = y_pred_prob.argmax(dim=1)\n",
    "\n",
    "                batch_loss = criterion(y_pred_prob, val_label)\n",
    "                total_loss_val += batch_loss.item()\n",
    "\n",
    "                acc = (y_pred_label == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "        print(\n",
    "            f'''Epochs: {epoch + 1} \n",
    "            | Train Loss: {total_loss_train / len(train_data): .3f} \n",
    "            | Train Accuracy: {total_acc_train / len(train_data): .3f} \n",
    "            | Val Loss: {total_loss_val / len(val_data): .3f} \n",
    "            | Val Accuracy: {total_acc_val / len(val_data): .3f}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/10457 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mf:\\创新实践\\TMD-BERT\\BERT.ipynb 单元格 26\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/%E5%88%9B%E6%96%B0%E5%AE%9E%E8%B7%B5/TMD-BERT/BERT.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m lr \u001b[39m=\u001b[39m \u001b[39m0.00001\u001b[39m \n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/%E5%88%9B%E6%96%B0%E5%AE%9E%E8%B7%B5/TMD-BERT/BERT.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/%E5%88%9B%E6%96%B0%E5%AE%9E%E8%B7%B5/TMD-BERT/BERT.ipynb#X34sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m training(model, data_train, data_valid, lr, epochs)\n",
      "\u001b[1;32mf:\\创新实践\\TMD-BERT\\BERT.ipynb 单元格 26\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/%E5%88%9B%E6%96%B0%E5%AE%9E%E8%B7%B5/TMD-BERT/BERT.ipynb#X34sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     train_label \u001b[39m=\u001b[39m train_label\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mLongTensor)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/%E5%88%9B%E6%96%B0%E5%AE%9E%E8%B7%B5/TMD-BERT/BERT.ipynb#X34sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m#输出\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/%E5%88%9B%E6%96%B0%E5%AE%9E%E8%B7%B5/TMD-BERT/BERT.ipynb#X34sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrain_input\u001b[39m.\u001b[39mto(device), labels\u001b[39m=\u001b[39m train_label)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/%E5%88%9B%E6%96%B0%E5%AE%9E%E8%B7%B5/TMD-BERT/BERT.ipynb#X34sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     y_pred_prob \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/%E5%88%9B%E6%96%B0%E5%AE%9E%E8%B7%B5/TMD-BERT/BERT.ipynb#X34sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     y_pred_label \u001b[39m=\u001b[39m y_pred_prob\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1562\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1554\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1555\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1556\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1562\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[0;32m   1563\u001b[0m     input_ids,\n\u001b[0;32m   1564\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1565\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1566\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1567\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1568\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1569\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1570\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1571\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1572\u001b[0m )\n\u001b[0;32m   1574\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Python\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:976\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 976\u001b[0m batch_size, seq_length \u001b[39m=\u001b[39m input_shape\n\u001b[0;32m    977\u001b[0m device \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mdevice \u001b[39mif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m inputs_embeds\u001b[39m.\u001b[39mdevice\n\u001b[0;32m    979\u001b[0m \u001b[39m# past_key_values_length\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "lr = 0.00001 \n",
    "epochs = 10\n",
    "training(model, data_train, data_valid, lr, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
