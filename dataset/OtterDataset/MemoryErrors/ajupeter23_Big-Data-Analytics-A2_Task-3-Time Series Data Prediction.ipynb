{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4deece4c-764f-43c8-88da-89b498a5109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0723702f-30b9-4bac-948a-f3f3f3ef972f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- TotalAmount: double (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[204.336s][warning][os,thread] Failed to start thread - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.\n",
      "[204.337s][warning][os,thread] Failed to start thread - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.\n",
      "[204.402s][warning][os,thread] Failed to start thread - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"block-manager-storage-async-thread-pool-76\" java.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached\n",
      "\tat java.base/java.lang.Thread.start0(Native Method)\n",
      "\tat java.base/java.lang.Thread.start(Thread.java:798)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:937)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:1005)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "java.lang.IllegalStateException: problem in scala.concurrent internal callback\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.reportFailure(Future.scala:877)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.IllegalArgumentException: requirement failed\n",
      "\tat scala.Predef$.require(Predef.scala:268)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:55)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:76)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\t... 12 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[204.945s][warning][os,thread] Failed to start thread - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.\n",
      "[204.947s][warning][os,thread] Failed to start thread - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.\n",
      "[204.948s][warning][os,thread] Failed to start thread - pthread_create failed (EAGAIN) for attributes: stacksize: 1024k, guardsize: 0k, detached.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java.lang.IllegalStateException: problem in scala.concurrent internal callback\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.reportFailure(Future.scala:877)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.tryFailure(Promise.scala:112)\n",
      "\tat scala.concurrent.Promise.tryFailure$(Promise.scala:112)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:214)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:223)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.sendFailure(NettyRpcCallContext.scala:36)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:108)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.IllegalArgumentException: requirement failed\n",
      "\tat scala.Predef$.require(Predef.scala:268)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:55)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:76)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\t... 34 more\n",
      "24/08/01 05:22:20 ERROR Inbox: An error happened while processing message in the inbox for BlockManagerEndpoint2\n",
      "java.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached\n",
      "\tat java.base/java.lang.Thread.start0(Native Method)\n",
      "\tat java.base/java.lang.Thread.start(Thread.java:798)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:937)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1343)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$KeptPromise$Kept.onComplete(Promise.scala:372)\n",
      "\tat scala.concurrent.impl.Promise$KeptPromise$Kept.onComplete$(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$KeptPromise$Successful.onComplete(Promise.scala:379)\n",
      "\tat scala.concurrent.impl.Promise.transform(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.transform$(Promise.scala:31)\n",
      "\tat scala.concurrent.impl.Promise$KeptPromise$Successful.transform(Promise.scala:379)\n",
      "\tat scala.concurrent.Future.map(Future.scala:292)\n",
      "\tat scala.concurrent.Future.map$(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise$KeptPromise$Successful.map(Promise.scala:379)\n",
      "\tat scala.concurrent.Future$.apply(Future.scala:659)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.org$apache$spark$storage$BlockManagerStorageEndpoint$$doAsync(BlockManagerStorageEndpoint.scala:99)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerStorageEndpoint.scala:69)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Exception in thread \"dispatcher-BlockManagerEndpoint2\" java.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached\n",
      "\tat java.base/java.lang.Thread.start0(Native Method)\n",
      "\tat java.base/java.lang.Thread.start(Thread.java:798)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:937)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:1005)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+------+\n",
      "|CountryIndex|prediction| count|\n",
      "+------------+----------+------+\n",
      "|         2.0|       0.0|  2529|\n",
      "|        15.0|       0.0|   126|\n",
      "|        22.0|       0.0|    78|\n",
      "|        34.0|       0.0|    13|\n",
      "|        23.0|       0.0|    54|\n",
      "|        31.0|       0.0|    13|\n",
      "|         0.0|       0.0|108414|\n",
      "|        28.0|       0.0|    14|\n",
      "|        17.0|       0.0|   124|\n",
      "|        36.0|       0.0|     3|\n",
      "|        18.0|       0.0|   111|\n",
      "|        16.0|       0.0|   125|\n",
      "|        14.0|       0.0|   169|\n",
      "|        12.0|       0.0|   218|\n",
      "|         4.0|       0.0|   761|\n",
      "|        19.0|       0.0|    99|\n",
      "|        26.0|       0.0|    42|\n",
      "|        25.0|       0.0|    54|\n",
      "|        32.0|       0.0|    11|\n",
      "|        33.0|       0.0|    11|\n",
      "+------------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test Accuracy: 0.888712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/01 05:24:19 ERROR BlockManagerMasterEndpoint: Fail to know the executor driver is alive or not.\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$handleBlockRemovalFailure$1.applyOrElse(BlockManagerMasterEndpoint.scala:317)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$handleBlockRemovalFailure$1.applyOrElse(BlockManagerMasterEndpoint.scala:306)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat scala.util.Failure.recover(Try.scala:234)\n",
      "\tat scala.concurrent.Future.$anonfun$recover$1(Future.scala:395)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@ip-10-0-0-112.ec2.internal:43553\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 16 more\n",
      "24/08/01 05:24:19 WARN BlockManagerMasterEndpoint: Error trying to remove shuffle 5. The executor driver may have been lost.\n",
      "org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from ip-10-0-0-112.ec2.internal:43553 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat scala.util.Failure.recover(Try.scala:234)\n",
      "\tat scala.concurrent.Future.$anonfun$recover$1(Future.scala:395)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.tryFailure(Promise.scala:112)\n",
      "\tat scala.concurrent.Promise.tryFailure$(Promise.scala:112)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:214)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:264)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from ip-10-0-0-112.ec2.internal:43553 in 120 seconds\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:265)\n",
      "\t... 6 more\n",
      "24/08/01 05:24:19 ERROR BlockManagerMasterEndpoint: Fail to know the executor driver is alive or not.\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$handleBlockRemovalFailure$1.applyOrElse(BlockManagerMasterEndpoint.scala:317)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$handleBlockRemovalFailure$1.applyOrElse(BlockManagerMasterEndpoint.scala:306)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat scala.util.Failure.recover(Try.scala:234)\n",
      "\tat scala.concurrent.Future.$anonfun$recover$1(Future.scala:395)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@ip-10-0-0-112.ec2.internal:43553\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
      "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
      "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
      "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
      "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
      "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
      "\t... 16 more\n",
      "24/08/01 05:24:19 WARN BlockManagerMasterEndpoint: Error trying to remove broadcast 18. The executor driver may have been lost.\n",
      "org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from ip-10-0-0-112.ec2.internal:43553 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat scala.util.Failure.recover(Try.scala:234)\n",
      "\tat scala.concurrent.Future.$anonfun$recover$1(Future.scala:395)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
      "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
      "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
      "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
      "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.tryFailure(Promise.scala:112)\n",
      "\tat scala.concurrent.Promise.tryFailure$(Promise.scala:112)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:187)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:214)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:264)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from ip-10-0-0-112.ec2.internal:43553 in 120 seconds\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:265)\n",
      "\t... 6 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, year, month, dayofmonth\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Initialize Spark session with adjusted configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RetailDataAnalysis\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.task.cpus\", \"1\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"50\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"retail_data.csv\"\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Convert InvoiceDate to datetime and create year, month, day columns\n",
    "df = df.withColumn(\"InvoiceDate\", to_timestamp(col(\"InvoiceDate\"), \"dd/MM/yyyy HH:mm\"))\n",
    "df = df.withColumn(\"Year\", year(col(\"InvoiceDate\")))\n",
    "df = df.withColumn(\"Month\", month(col(\"InvoiceDate\")))\n",
    "df = df.withColumn(\"Day\", dayofmonth(col(\"InvoiceDate\")))\n",
    "\n",
    "# Drop rows with missing CustomerID and convert CustomerID to string\n",
    "df = df.dropna(subset=['CustomerID'])\n",
    "df = df.withColumn(\"CustomerID\", df[\"CustomerID\"].cast(\"string\"))\n",
    "\n",
    "# Verify data types\n",
    "df.printSchema()\n",
    "\n",
    "# Encode the target variable (Country)\n",
    "indexer = StringIndexer(inputCol=\"Country\", outputCol=\"CountryIndex\")\n",
    "df = indexer.fit(df).transform(df)\n",
    "\n",
    "# Ensure feature columns are numeric\n",
    "df = df.withColumn(\"Quantity\", col(\"Quantity\").cast(\"double\"))\n",
    "df = df.withColumn(\"UnitPrice\", col(\"UnitPrice\").cast(\"double\"))\n",
    "df = df.withColumn(\"TotalAmount\", col(\"TotalAmount\").cast(\"double\"))\n",
    "\n",
    "# Select features and target variable\n",
    "feature_cols = [\"Quantity\", \"UnitPrice\", \"TotalAmount\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Split data into training and test sets\n",
    "train, test = df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Feature engineering steps\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Define the classifier\n",
    "rf = RandomForestClassifier(featuresCol=\"scaledFeatures\", labelCol=\"CountryIndex\", numTrees=10)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline(stages=[scaler, rf])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"CountryIndex\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "predictions.groupBy(\"CountryIndex\", \"prediction\").count().show()\n",
    "print(f\"Test Accuracy: {accuracy:.6f}\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c133fd60-d523-4ab3-93d6-22e4f6d68c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48680bfe-913c-4c8d-8e6e-2136c151dcac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36034d46-5822-460a-bc97-934fb163095a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbab00e-8237-477c-b35c-6faf451ca6d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "248b940e-fec1-4fc0-8f1c-eb098d598d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: svm\n",
      "Train Accuracy (CV): 0.888401\n",
      "Test Accuracy: 0.888243\n",
      "Confusion Matrix:\n",
      "[[    0     0     0 ...     0    32     0]\n",
      " [    0     0     0 ...     0    12     0]\n",
      " [    0     0     0 ...     0    51     0]\n",
      " ...\n",
      " [    0     0     0 ...     0     1     0]\n",
      " [    0     0     0 ...     0 10841     0]\n",
      " [    0     0     0 ...     0     8     0]]\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "           Australia       0.00      0.00      0.00        32\n",
      "             Austria       0.00      0.00      0.00        12\n",
      "             Bahrain       0.00      0.00      0.00         0\n",
      "             Belgium       0.00      0.00      0.00        51\n",
      "              Brazil       0.00      0.00      0.00         1\n",
      "              Canada       0.00      0.00      0.00         2\n",
      "     Channel Islands       0.00      0.00      0.00        19\n",
      "              Cyprus       0.00      0.00      0.00         9\n",
      "      Czech Republic       0.00      0.00      0.00         2\n",
      "             Denmark       0.00      0.00      0.00        13\n",
      "                EIRE       0.00      0.00      0.00       241\n",
      "  European Community       0.00      0.00      0.00         2\n",
      "             Finland       0.00      0.00      0.00        21\n",
      "              France       0.00      0.00      0.00       260\n",
      "             Germany       0.00      0.00      0.00       322\n",
      "              Greece       0.00      0.00      0.00         5\n",
      "             Iceland       0.00      0.00      0.00         7\n",
      "              Israel       0.00      0.00      0.00         8\n",
      "               Italy       0.00      0.00      0.00        22\n",
      "               Japan       0.00      0.00      0.00         9\n",
      "             Lebanon       0.00      0.00      0.00         1\n",
      "           Lithuania       0.00      0.00      0.00         1\n",
      "               Malta       0.00      0.00      0.00         3\n",
      "         Netherlands       0.00      0.00      0.00        60\n",
      "              Norway       0.00      0.00      0.00        25\n",
      "              Poland       0.00      0.00      0.00        11\n",
      "            Portugal       0.00      0.00      0.00        32\n",
      "                 RSA       0.00      0.00      0.00         1\n",
      "        Saudi Arabia       0.00      0.00      0.00         0\n",
      "           Singapore       0.00      0.00      0.00         5\n",
      "               Spain       0.00      0.00      0.00        84\n",
      "              Sweden       0.00      0.00      0.00        21\n",
      "         Switzerland       0.00      0.00      0.00        68\n",
      "                 USA       0.00      0.00      0.00         5\n",
      "United Arab Emirates       0.00      0.00      0.00         1\n",
      "      United Kingdom       0.89      1.00      0.94     10841\n",
      "         Unspecified       0.00      0.00      0.00         8\n",
      "\n",
      "           micro avg       0.89      0.89      0.89     12205\n",
      "           macro avg       0.02      0.03      0.03     12205\n",
      "        weighted avg       0.79      0.89      0.84     12205\n",
      "\n",
      "==================================================\n",
      "Classifier: random_forest\n",
      "Train Accuracy (CV): 0.884995\n",
      "Test Accuracy: 0.885457\n",
      "Confusion Matrix:\n",
      "[[    0     1     0 ...     0    27     0]\n",
      " [    0     0     0 ...     0    11     0]\n",
      " [    0     0     0 ...     0    50     0]\n",
      " ...\n",
      " [    0     0     0 ...     0     1     0]\n",
      " [    2     1     0 ...     0 10786     0]\n",
      " [    0     0     0 ...     0     8     0]]\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "           Australia       0.00      0.00      0.00        32\n",
      "             Austria       0.00      0.00      0.00        12\n",
      "             Bahrain       0.00      0.00      0.00         0\n",
      "             Belgium       0.00      0.00      0.00        51\n",
      "              Brazil       0.00      0.00      0.00         1\n",
      "              Canada       0.00      0.00      0.00         2\n",
      "     Channel Islands       0.00      0.00      0.00        19\n",
      "              Cyprus       0.00      0.00      0.00         9\n",
      "      Czech Republic       0.00      0.00      0.00         2\n",
      "             Denmark       0.00      0.00      0.00        13\n",
      "                EIRE       0.28      0.02      0.04       241\n",
      "  European Community       0.00      0.00      0.00         2\n",
      "             Finland       0.00      0.00      0.00        21\n",
      "              France       0.28      0.03      0.05       260\n",
      "             Germany       0.13      0.01      0.02       322\n",
      "              Greece       0.00      0.00      0.00         5\n",
      "             Iceland       0.00      0.00      0.00         7\n",
      "              Israel       0.00      0.00      0.00         8\n",
      "               Italy       1.00      0.05      0.09        22\n",
      "               Japan       0.00      0.00      0.00         9\n",
      "             Lebanon       0.00      0.00      0.00         1\n",
      "           Lithuania       0.00      0.00      0.00         1\n",
      "               Malta       0.00      0.00      0.00         3\n",
      "         Netherlands       0.19      0.07      0.10        60\n",
      "              Norway       0.00      0.00      0.00        25\n",
      "              Poland       0.00      0.00      0.00        11\n",
      "            Portugal       0.00      0.00      0.00        32\n",
      "                 RSA       0.00      0.00      0.00         1\n",
      "        Saudi Arabia       0.00      0.00      0.00         0\n",
      "           Singapore       0.00      0.00      0.00         5\n",
      "               Spain       0.25      0.01      0.02        84\n",
      "              Sweden       0.00      0.00      0.00        21\n",
      "         Switzerland       0.00      0.00      0.00        68\n",
      "                 USA       0.00      0.00      0.00         5\n",
      "United Arab Emirates       0.00      0.00      0.00         1\n",
      "      United Kingdom       0.89      0.99      0.94     10841\n",
      "         Unspecified       0.00      0.00      0.00         8\n",
      "\n",
      "           micro avg       0.89      0.89      0.89     12205\n",
      "           macro avg       0.08      0.03      0.03     12205\n",
      "        weighted avg       0.81      0.89      0.84     12205\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import warnings\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"retail_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Preprocess the data\n",
    "# Convert InvoiceDate to datetime\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], format='%d/%m/%Y %H:%M')\n",
    "\n",
    "# Drop rows with missing CustomerID\n",
    "df = df.dropna(subset=['CustomerID'])\n",
    "\n",
    "# Convert CustomerID to string\n",
    "df['CustomerID'] = df['CustomerID'].astype(str)\n",
    "\n",
    "# Encode the target variable (Country)\n",
    "le = LabelEncoder()\n",
    "df['Country'] = le.fit_transform(df['Country'])\n",
    "\n",
    "# Select features and target variable\n",
    "X = df[['Quantity', 'UnitPrice', 'TotalAmount']]\n",
    "y = df['Country']\n",
    "\n",
    "# Sample the data for initial testing (optional, remove for full dataset)\n",
    "X, _, y, _ = train_test_split(X, y, test_size=0.9, random_state=42)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define feature engineering steps including preprocessing\n",
    "feature_engineering_steps = [\n",
    "    ('scaling', StandardScaler()),                   # Step 1: Standard scaling\n",
    "    ('feature_selection', SelectKBest(f_classif))    # Step 2: Feature selection (SelectKBest)\n",
    "    # Omitting PolynomialFeatures and PCA for now\n",
    "]\n",
    "\n",
    "# Classifiers to evaluate\n",
    "classifiers = [\n",
    "    ('svm', SVC()),                                 # SVM classifier\n",
    "    ('random_forest', RandomForestClassifier())     # Random Forest classifier\n",
    "]\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over classifiers\n",
    "for clf_name, clf_method in classifiers:\n",
    "    # Define pipeline with warning suppression\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\")  # Ignore warnings\n",
    "        pipeline = Pipeline([\n",
    "            ('feature_engineering', Pipeline(feature_engineering_steps)),  # Feature engineering steps\n",
    "            ('classification', clf_method)  # Classifier\n",
    "        ])\n",
    "\n",
    "        # Perform grid search with cross-validation\n",
    "        grid_search = GridSearchCV(pipeline, param_grid={}, cv=5, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Best parameters and best score\n",
    "        best_params = grid_search.best_params_\n",
    "        best_score = grid_search.best_score_\n",
    "\n",
    "        # Predict on test set with best model\n",
    "        y_pred = grid_search.predict(X_test)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        # Classification report for more detailed evaluation\n",
    "        clf_report = classification_report(y_test, y_pred, labels=le.transform(le.classes_), target_names=le.classes_)\n",
    "\n",
    "        # Print confusion matrix and classification report\n",
    "        print(f\"Classifier: {clf_name}\")\n",
    "        print(f\"Train Accuracy (CV): {best_score:.6f}\")\n",
    "        print(f\"Test Accuracy: {test_accuracy:.6f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "        print(\"Classification Report:\")\n",
    "        print(clf_report)\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Store results if needed\n",
    "        results.append({\n",
    "            'Classifier': clf_name,\n",
    "            'Train Accuracy (CV)': best_score,\n",
    "            'Test Accuracy': test_accuracy,\n",
    "            'Confusion Matrix': cm,\n",
    "            'Classification Report': clf_report\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e8149dd-33c6-4907-92be-5c9c56e772a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8833\n",
      "Confusion Matrix:\n",
      " [[    1     0     0 ...     0    26     0]\n",
      " [    0     0     0 ...     0    11     0]\n",
      " [    0     0     0 ...     0    50     0]\n",
      " ...\n",
      " [    0     0     0 ...     0     1     0]\n",
      " [   10     0     0 ...     0 10754     0]\n",
      " [    0     0     0 ...     0     8     0]]\n",
      "Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "           Australia       0.05      0.03      0.04        32\n",
      "             Austria       0.00      0.00      0.00        12\n",
      "             Belgium       0.00      0.00      0.00        51\n",
      "              Brazil       0.00      0.00      0.00         1\n",
      "              Canada       0.00      0.00      0.00         2\n",
      "     Channel Islands       0.00      0.00      0.00        19\n",
      "              Cyprus       0.00      0.00      0.00         9\n",
      "      Czech Republic       0.00      0.00      0.00         2\n",
      "             Denmark       0.00      0.00      0.00        13\n",
      "                EIRE       0.23      0.02      0.04       241\n",
      "  European Community       0.00      0.00      0.00         2\n",
      "             Finland       0.00      0.00      0.00        21\n",
      "              France       0.24      0.03      0.05       260\n",
      "             Germany       0.11      0.02      0.03       322\n",
      "              Greece       0.00      0.00      0.00         5\n",
      "             Iceland       0.00      0.00      0.00         7\n",
      "              Israel       0.00      0.00      0.00         8\n",
      "               Italy       0.50      0.05      0.08        22\n",
      "               Japan       0.00      0.00      0.00         9\n",
      "             Lebanon       0.00      0.00      0.00         1\n",
      "           Lithuania       0.00      0.00      0.00         1\n",
      "               Malta       0.00      0.00      0.00         3\n",
      "         Netherlands       0.21      0.08      0.12        60\n",
      "              Norway       0.00      0.00      0.00        25\n",
      "              Poland       0.00      0.00      0.00        11\n",
      "            Portugal       0.00      0.00      0.00        32\n",
      "                 RSA       0.00      0.00      0.00         1\n",
      "           Singapore       0.00      0.00      0.00         5\n",
      "               Spain       0.33      0.01      0.02        84\n",
      "              Sweden       1.00      0.05      0.09        21\n",
      "         Switzerland       0.00      0.00      0.00        68\n",
      "                 USA       0.00      0.00      0.00         5\n",
      "United Arab Emirates       0.00      0.00      0.00         1\n",
      "      United Kingdom       0.89      0.99      0.94     10841\n",
      "         Unspecified       0.00      0.00      0.00         8\n",
      "\n",
      "            accuracy                           0.88     12205\n",
      "           macro avg       0.10      0.04      0.04     12205\n",
      "        weighted avg       0.81      0.88      0.84     12205\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajupe\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ajupe\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ajupe\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "import warnings\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"retail_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Preprocess the data\n",
    "# Convert InvoiceDate to datetime\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], format='%d/%m/%Y %H:%M')\n",
    "\n",
    "# Drop rows with missing CustomerID\n",
    "df = df.dropna(subset=['CustomerID'])\n",
    "\n",
    "# Convert CustomerID to string\n",
    "df['CustomerID'] = df['CustomerID'].astype(str)\n",
    "\n",
    "# Encode the target variable (Country)\n",
    "le = LabelEncoder()\n",
    "df['Country'] = le.fit_transform(df['Country'])\n",
    "\n",
    "# Select features and target variable\n",
    "X = df[['Quantity', 'UnitPrice', 'TotalAmount']]\n",
    "y = df['Country']\n",
    "\n",
    "# Sample the data for initial testing (optional, remove for full dataset)\n",
    "X, _, y, _ = train_test_split(X, y, test_size=0.9, random_state=42)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Custom transformer to select a classifier\n",
    "class ClassifierTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, classifier):\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.classifier.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.classifier.predict_proba(X)\n",
    "\n",
    "# Define classifiers to use for feature engineering\n",
    "feature_classifiers = {\n",
    "    'logistic': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression())\n",
    "    ]),\n",
    "    'random_forest': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', RandomForestClassifier())\n",
    "    ]),\n",
    "    'gradient_boosting': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', GradientBoostingClassifier())\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Create feature engineering pipeline with FeatureUnion\n",
    "feature_union = FeatureUnion([\n",
    "    (name, ClassifierTransformer(clf)) for name, clf in feature_classifiers.items()\n",
    "])\n",
    "\n",
    "# Final classifier\n",
    "final_classifier = LogisticRegression()\n",
    "\n",
    "# Final pipeline combining feature union and final classifier\n",
    "pipeline = Pipeline([\n",
    "    ('features', feature_union),    # Feature union\n",
    "    ('classifier', final_classifier)   # Classifier\n",
    "])\n",
    "\n",
    "# Fit pipeline with warning suppression\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")  # Ignore warnings\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Adjust the target names to match the predicted labels\n",
    "unique_labels = le.inverse_transform(sorted(set(y_test).union(set(y_pred))))\n",
    "clf_report = classification_report(y_test, y_pred, labels=sorted(set(y_test).union(set(y_pred))), target_names=unique_labels)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Classification Report:\\n\", clf_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0ce8963-2c5d-4687-85c4-407651771d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8868\n",
      "Confusion Matrix:\n",
      " [[    0     0     0 ...     0    31     0]\n",
      " [    0     0     0 ...     0    12     0]\n",
      " [    0     0     0 ...     0    50     0]\n",
      " ...\n",
      " [    0     0     0 ...     0     1     0]\n",
      " [    2     0     0 ...     0 10805     0]\n",
      " [    0     0     0 ...     0     8     0]]\n",
      "Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "           Australia       0.00      0.00      0.00        32\n",
      "             Austria       0.00      0.00      0.00        12\n",
      "             Belgium       0.00      0.00      0.00        51\n",
      "              Brazil       0.00      0.00      0.00         1\n",
      "              Canada       0.00      0.00      0.00         2\n",
      "     Channel Islands       0.00      0.00      0.00        19\n",
      "              Cyprus       0.00      0.00      0.00         9\n",
      "      Czech Republic       0.00      0.00      0.00         2\n",
      "             Denmark       0.00      0.00      0.00        13\n",
      "                EIRE       0.26      0.02      0.04       241\n",
      "  European Community       0.00      0.00      0.00         2\n",
      "             Finland       0.00      0.00      0.00        21\n",
      "              France       0.33      0.03      0.05       260\n",
      "             Germany       0.08      0.00      0.01       322\n",
      "              Greece       0.00      0.00      0.00         5\n",
      "             Iceland       0.00      0.00      0.00         7\n",
      "              Israel       0.00      0.00      0.00         8\n",
      "               Italy       1.00      0.05      0.09        22\n",
      "               Japan       0.00      0.00      0.00         9\n",
      "             Lebanon       0.00      0.00      0.00         1\n",
      "           Lithuania       0.00      0.00      0.00         1\n",
      "               Malta       0.00      0.00      0.00         3\n",
      "         Netherlands       0.33      0.07      0.11        60\n",
      "              Norway       0.00      0.00      0.00        25\n",
      "              Poland       0.00      0.00      0.00        11\n",
      "            Portugal       0.00      0.00      0.00        32\n",
      "                 RSA       0.00      0.00      0.00         1\n",
      "           Singapore       0.00      0.00      0.00         5\n",
      "               Spain       0.50      0.01      0.02        84\n",
      "              Sweden       0.00      0.00      0.00        21\n",
      "         Switzerland       0.00      0.00      0.00        68\n",
      "                 USA       0.00      0.00      0.00         5\n",
      "United Arab Emirates       0.00      0.00      0.00         1\n",
      "      United Kingdom       0.89      1.00      0.94     10841\n",
      "         Unspecified       0.00      0.00      0.00         8\n",
      "\n",
      "            accuracy                           0.89     12205\n",
      "           macro avg       0.10      0.03      0.04     12205\n",
      "        weighted avg       0.81      0.89      0.84     12205\n",
      "\n",
      "Best Classifier: random_forest\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"retail_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Preprocess the data\n",
    "# Convert InvoiceDate to datetime\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], format='%d/%m/%Y %H:%M')\n",
    "\n",
    "# Drop rows with missing CustomerID\n",
    "df = df.dropna(subset=['CustomerID'])\n",
    "\n",
    "# Convert CustomerID to string\n",
    "df['CustomerID'] = df['CustomerID'].astype(str)\n",
    "\n",
    "# Encode the target variable (Country)\n",
    "le = LabelEncoder()\n",
    "df['Country'] = le.fit_transform(df['Country'])\n",
    "\n",
    "# Select features and target variable\n",
    "X = df[['Quantity', 'UnitPrice', 'TotalAmount']]\n",
    "y = df['Country']\n",
    "\n",
    "# Sample the data for initial testing (optional, remove for full dataset)\n",
    "X, _, y, _ = train_test_split(X, y, test_size=0.9, random_state=42)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Custom transformer to select a classifier\n",
    "class ClassifierTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, classifier):\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.classifier.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.classifier.predict_proba(X)\n",
    "\n",
    "# Define classifiers to use for feature engineering\n",
    "feature_classifiers = {\n",
    "    'logistic': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression())\n",
    "    ]),\n",
    "    'random_forest': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', RandomForestClassifier())\n",
    "    ]),\n",
    "    'gradient_boosting': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', GradientBoostingClassifier())\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Create feature engineering pipeline with FeatureUnion\n",
    "feature_union = FeatureUnion([\n",
    "    (name, Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', ClassifierTransformer(clf))\n",
    "    ])) for name, clf in feature_classifiers.items()\n",
    "])\n",
    "\n",
    "# Final classifier (SVM)\n",
    "final_classifier = SVC(probability=True)\n",
    "\n",
    "# Combine original features with the outputs from classifiers\n",
    "class FeatureConcatenator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_union):\n",
    "        self.feature_union = feature_union\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.feature_union.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Get the outputs from the classifiers\n",
    "        classifier_features = self.feature_union.transform(X)\n",
    "        # Concatenate original features with classifier outputs\n",
    "        combined_features = np.hstack((X, classifier_features))\n",
    "        return combined_features\n",
    "\n",
    "# Create pipeline including original features and classifier outputs\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureConcatenator(feature_union)),  # Combine features\n",
    "    ('scaler', StandardScaler()),  # Standardize features\n",
    "    ('classifier', final_classifier)   # Classifier\n",
    "])\n",
    "\n",
    "# Fit pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "unique_labels = le.inverse_transform(sorted(set(y_test).union(set(y_pred))))\n",
    "clf_report = classification_report(y_test, y_pred, labels=sorted(set(y_test).union(set(y_pred))), target_names=unique_labels)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Classification Report:\\n\", clf_report)\n",
    "\n",
    "# Print best classifier name based on the highest accuracy score\n",
    "best_classifier_name = max(feature_classifiers, key=lambda k: accuracy_score(y_test, feature_classifiers[k].predict(X_test)))\n",
    "print(f\"Best Classifier: {best_classifier_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81c9a5f1-6a40-42b6-a663-d710914193a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of combined features: 114\n",
      "Test Accuracy: 0.8864\n",
      "Confusion Matrix:\n",
      " [[    0     0     0 ...     0    31     0]\n",
      " [    0     0     0 ...     0    12     0]\n",
      " [    0     0     0 ...     0    50     0]\n",
      " ...\n",
      " [    0     0     0 ...     0     1     0]\n",
      " [    2     0     0 ...     0 10799     0]\n",
      " [    0     0     0 ...     0     8     0]]\n",
      "Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "           Australia       0.00      0.00      0.00        32\n",
      "             Austria       0.00      0.00      0.00        12\n",
      "             Belgium       0.00      0.00      0.00        51\n",
      "              Brazil       0.00      0.00      0.00         1\n",
      "              Canada       0.00      0.00      0.00         2\n",
      "     Channel Islands       0.00      0.00      0.00        19\n",
      "              Cyprus       0.00      0.00      0.00         9\n",
      "      Czech Republic       0.00      0.00      0.00         2\n",
      "             Denmark       0.00      0.00      0.00        13\n",
      "                EIRE       0.26      0.02      0.04       241\n",
      "  European Community       0.00      0.00      0.00         2\n",
      "             Finland       0.00      0.00      0.00        21\n",
      "              France       0.33      0.03      0.05       260\n",
      "             Germany       0.05      0.00      0.01       322\n",
      "              Greece       0.00      0.00      0.00         5\n",
      "             Iceland       0.00      0.00      0.00         7\n",
      "              Israel       0.00      0.00      0.00         8\n",
      "               Italy       1.00      0.05      0.09        22\n",
      "               Japan       0.00      0.00      0.00         9\n",
      "             Lebanon       0.00      0.00      0.00         1\n",
      "           Lithuania       0.00      0.00      0.00         1\n",
      "               Malta       0.00      0.00      0.00         3\n",
      "         Netherlands       0.31      0.07      0.11        60\n",
      "              Norway       0.00      0.00      0.00        25\n",
      "              Poland       0.00      0.00      0.00        11\n",
      "            Portugal       0.00      0.00      0.00        32\n",
      "                 RSA       0.00      0.00      0.00         1\n",
      "           Singapore       0.00      0.00      0.00         5\n",
      "               Spain       0.50      0.01      0.02        84\n",
      "              Sweden       0.00      0.00      0.00        21\n",
      "         Switzerland       0.00      0.00      0.00        68\n",
      "                 USA       0.00      0.00      0.00         5\n",
      "United Arab Emirates       0.00      0.00      0.00         1\n",
      "      United Kingdom       0.89      1.00      0.94     10841\n",
      "         Unspecified       0.00      0.00      0.00         8\n",
      "\n",
      "            accuracy                           0.89     12205\n",
      "           macro avg       0.10      0.03      0.04     12205\n",
      "        weighted avg       0.81      0.89      0.84     12205\n",
      "\n",
      "Best Classifier: random_forest\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"retail_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Preprocess the data\n",
    "# Convert InvoiceDate to datetime\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], format='%d/%m/%Y %H:%M')\n",
    "\n",
    "# Drop rows with missing CustomerID\n",
    "df = df.dropna(subset=['CustomerID'])\n",
    "\n",
    "# Convert CustomerID to string\n",
    "df['CustomerID'] = df['CustomerID'].astype(str)\n",
    "\n",
    "# Encode the target variable (Country)\n",
    "le = LabelEncoder()\n",
    "df['Country'] = le.fit_transform(df['Country'])\n",
    "\n",
    "# Select features and target variable\n",
    "X = df[['Quantity', 'UnitPrice', 'TotalAmount']]\n",
    "y = df['Country']\n",
    "\n",
    "# Sample the data for initial testing (optional, remove for full dataset)\n",
    "X, _, y, _ = train_test_split(X, y, test_size=0.9, random_state=42)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Custom transformer to select a classifier\n",
    "class ClassifierTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, classifier):\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.classifier.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.classifier.predict_proba(X)\n",
    "\n",
    "# Define classifiers to use for feature engineering\n",
    "feature_classifiers = {\n",
    "    'logistic': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression())\n",
    "    ]),\n",
    "    'random_forest': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', RandomForestClassifier())\n",
    "    ]),\n",
    "    'gradient_boosting': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', GradientBoostingClassifier())\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Create feature engineering pipeline with FeatureUnion\n",
    "feature_union = FeatureUnion([\n",
    "    (name, Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', ClassifierTransformer(clf))\n",
    "    ])) for name, clf in feature_classifiers.items()\n",
    "])\n",
    "\n",
    "# Final classifier (SVM)\n",
    "final_classifier = SVC(probability=True)\n",
    "\n",
    "# Combine original features with the outputs from classifiers\n",
    "class FeatureConcatenator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_union):\n",
    "        self.feature_union = feature_union\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.feature_union.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Get the outputs from the classifiers\n",
    "        classifier_features = self.feature_union.transform(X)\n",
    "        # Concatenate original features with classifier outputs\n",
    "        combined_features = np.hstack((X, classifier_features))\n",
    "        return combined_features\n",
    "\n",
    "# Create pipeline including original features and classifier outputs\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureConcatenator(feature_union)),  # Combine features\n",
    "    ('scaler', StandardScaler()),  # Standardize features\n",
    "    ('classifier', final_classifier)   # Classifier\n",
    "])\n",
    "\n",
    "# Fit pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Transform X_test to get the combined features\n",
    "X_test_combined = pipeline.named_steps['features'].transform(X_test)\n",
    "\n",
    "# Number of features after combining\n",
    "num_combined_features = X_test_combined.shape[1]\n",
    "print(f\"Number of combined features: {num_combined_features}\")\n",
    "\n",
    "# Predict and evaluate on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "unique_labels = le.inverse_transform(sorted(set(y_test).union(set(y_pred))))\n",
    "clf_report = classification_report(y_test, y_pred, labels=sorted(set(y_test).union(set(y_pred))), target_names=unique_labels)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Classification Report:\\n\", clf_report)\n",
    "\n",
    "# Print best classifier name based on the highest accuracy score\n",
    "best_classifier_name = max(feature_classifiers, key=lambda k: accuracy_score(y_test, feature_classifiers[k].predict(X_test)))\n",
    "print(f\"Best Classifier: {best_classifier_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92e92931-882b-443a-bb5b-d798b56da03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples (instances): 406829\n",
      "Number of features: 9\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"retail_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Determine the size of the dataset\n",
    "num_samples = df.shape[0]\n",
    "num_features = df.shape[1]\n",
    "\n",
    "print(f\"Number of samples (instances): {num_samples}\")\n",
    "print(f\"Number of features: {num_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1dbc301-19a6-4f8a-8eea-624f6c334d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Features shape: (12205, 3)\n",
      "Additional Features shape: (12205, 111)\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already fitted the pipeline and predicted on test data\n",
    "\n",
    "# Transform X_test to get the combined features\n",
    "X_test_combined = pipeline.named_steps['features'].transform(X_test)\n",
    "\n",
    "# Original features are the first n columns (where n is the number of original features)\n",
    "num_original_features = X_test.shape[1]\n",
    "original_features = X_test_combined[:, :num_original_features]\n",
    "\n",
    "# Additional features are the remaining columns\n",
    "additional_features = X_test_combined[:, num_original_features:]\n",
    "\n",
    "print(f\"Original Features shape: {original_features.shape}\")\n",
    "print(f\"Additional Features shape: {additional_features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f482a8-5bb6-4477-974e-7e808c6d6ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44281bb0-2a18-45c2-9310-2ef2305da1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400c56a9-7b44-411f-b33b-fe648f59ff57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eee753-3d67-4306-8c59-f88e5385359f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3df52b-66df-4e12-a8f2-9abdc1c4a31e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
