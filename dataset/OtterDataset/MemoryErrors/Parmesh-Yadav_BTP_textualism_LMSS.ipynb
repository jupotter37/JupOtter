{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "import re\n",
    "from evaluation import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_OWL = './LMSS.owl'\n",
    "LLM_PATH = '../Llama-2-7b-chat-hf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:04<00:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(LLM_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 500.00 MiB. GPU 1 has a total capacity of 39.43 GiB of which 16.31 MiB is free. Including non-PyTorch memory, this process has 39.41 GiB memory in use. Of the allocated memory 38.04 GiB is allocated by PyTorch, and 56.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/__init__.py:1107\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1105\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m-> 1107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/text_generation.py:84\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_model_type(\n\u001b[1;32m     86\u001b[0m         TF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n\u001b[1;32m     87\u001b[0m     )\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_params:\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;66;03m# This is very specific. The logic is quite complex and needs to be done\u001b[39;00m\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;66;03m# as a \"default\".\u001b[39;00m\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;66;03m# It also defines both some preprocess_kwargs and generate_kwargs\u001b[39;00m\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;66;03m# which is why we cannot put them in their respective methods.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:874\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;66;03m# We shouldn't call `model.to()` for models loaded with accelerate\u001b[39;00m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m hf_device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    873\u001b[0m ):\n\u001b[0;32m--> 874\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# Update config and generation_config with task specific parameters\u001b[39;00m\n\u001b[1;32m    877\u001b[0m task_specific_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtask_specific_params\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/modeling_utils.py:2556\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2552\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2554\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2555\u001b[0m         )\n\u001b[0;32m-> 2556\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 1 has a total capacity of 39.43 GiB of which 16.31 MiB is free. Including non-PyTorch memory, this process has 39.41 GiB memory in use. Of the allocated memory 38.04 GiB is allocated by PyTorch, and 56.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer,torch_dtype=torch.float16,device=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama_response(prompt):\n",
    "    sequences = pipe(\n",
    "        prompt,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "        # max_length=256,\n",
    "        # truncation=True\n",
    "    )\n",
    "    return sequences[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write me a poem about Machine Learning.\n",
      "\n",
      "In the depths of code, a world unfolds\n",
      "Where algorithms dance, and knowledge grows\n",
      "With each new task, a challenge is posed\n",
      "To the machines, that learn and know\n",
      "\n",
      "From the mountains of data, they climb\n",
      "And find the patterns, that time did line\n",
      "Their minds aglow, with insights so bright\n",
      "They solve the problems, of the night\n",
      "\n",
      "With each new step, they reach the peak\n",
      "Of knowledge and wisdom, they seek\n",
      "Their learning curves, a never-ending quest\n",
      "For the secrets, of the machine's crest\n",
      "\n",
      "Their minds, a blur of 1s and 0s\n",
      "A world of ones and zeroes, they know\n",
      "The language of the digital realm\n",
      "Where the future, is their domain\n",
      "\n",
      "So let us marvel, at their might\n",
      "As they learn and grow, in the light\n",
      "Of the machines, that learn and know\n",
      "And the wonders, they bring to show.\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "prompt = '''Write me a poem about Machine Learning.'''\n",
    "answer = get_llama_response(prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading OWL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the contents of the .owl file\n",
    "with open(PATH_TO_OWL, \"r\") as owl_file:\n",
    "    owl_data = owl_file.read()\n",
    "\n",
    "# Parse the OWL data using BeautifulSoup\n",
    "soup = BeautifulSoup(owl_data, 'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Other Personal and Household Goods Repair and ...</td>\n",
       "      <td>See industry description for 811490.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Other Converted Paper Product Manufacturing</td>\n",
       "      <td>This industry comprises establishments primari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>General Medical and Surgical Hospitals</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Confectionery Merchant Wholesalers</td>\n",
       "      <td>This industry comprises establishments primari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Other Specialized Design Services</td>\n",
       "      <td>See industry description for 541490.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14248</th>\n",
       "      <td>Vocational Rehabilitation Services</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14249</th>\n",
       "      <td>Books Printing</td>\n",
       "      <td>This U.S. industry comprises establishments pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14250</th>\n",
       "      <td>Petrochemical Manufacturing</td>\n",
       "      <td>See industry description for 325110.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14251</th>\n",
       "      <td>Pesticide and Other Agricultural Chemical Manu...</td>\n",
       "      <td>This industry comprises establishments primari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14252</th>\n",
       "      <td>oasis:agent</td>\n",
       "      <td>An \"oasis:agent\" is a type of agent or actor w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14253 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Label  \\\n",
       "0      Other Personal and Household Goods Repair and ...   \n",
       "1            Other Converted Paper Product Manufacturing   \n",
       "2                 General Medical and Surgical Hospitals   \n",
       "3                     Confectionery Merchant Wholesalers   \n",
       "4                      Other Specialized Design Services   \n",
       "...                                                  ...   \n",
       "14248                 Vocational Rehabilitation Services   \n",
       "14249                                     Books Printing   \n",
       "14250                        Petrochemical Manufacturing   \n",
       "14251  Pesticide and Other Agricultural Chemical Manu...   \n",
       "14252                                        oasis:agent   \n",
       "\n",
       "                                              Definition  \n",
       "0                   See industry description for 811490.  \n",
       "1      This industry comprises establishments primari...  \n",
       "2                                                   NULL  \n",
       "3      This industry comprises establishments primari...  \n",
       "4                   See industry description for 541490.  \n",
       "...                                                  ...  \n",
       "14248                                               NULL  \n",
       "14249  This U.S. industry comprises establishments pr...  \n",
       "14250               See industry description for 325110.  \n",
       "14251  This industry comprises establishments primari...  \n",
       "14252  An \"oasis:agent\" is a type of agent or actor w...  \n",
       "\n",
       "[14253 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize lists to store data\n",
    "labels = []\n",
    "definitions = []\n",
    "\n",
    "# Find all instances of <owl:Class> elements and extract label and definition\n",
    "for owl_class in soup.find_all('owl:Class'):\n",
    "    label_element = owl_class.find('rdfs:label')\n",
    "    definition_element = owl_class.find('skos:definition')\n",
    "    \n",
    "    # Check if label and definition elements exist\n",
    "    if label_element and definition_element:\n",
    "        label = label_element.text.strip()\n",
    "        definition = definition_element.text.strip()\n",
    "        \n",
    "        # Append data to lists\n",
    "        labels.append(label)\n",
    "        definitions.append(definition)\n",
    "\n",
    "data = {'Label': labels, 'Definition': definitions}\n",
    "owl_df = pd.DataFrame(data)\n",
    "\n",
    "owl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_label_by_substring(df, substring):\n",
    "    \"\"\"\n",
    "    Filter DataFrame rows containing the specified substring in the 'Label' column\n",
    "    and return a list of strings in the format \"{Label} : {Definition}\".\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): Input DataFrame.\n",
    "        substring (str): Substring to search for.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of strings in the format \"{Label} : {Definition}\" for matching rows.\n",
    "    \"\"\"\n",
    "    filtered_df = df[df['Label'].str.contains(substring, case=False)]\n",
    "    output_list = []\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        output_list.append(f\"{row['Label']} : {row['Definition']}\")\n",
    "    return output_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Motion in Limine to Exclude Hearsay Witness : A Motion in Limine to Exclude Hearsay Witness is a legal request made by one party to prevent the other party from presenting testimony from a witness who will testify about statements made by someone else out of court, which are being offered to prove the truth of the matter asserted.', 'Motion to Exclude Hearsay Witness : A Motion to Exclude Hearsay Witness is a legal request to prevent a witness from testifying in court based on the fact that their testimony is based on hearsay evidence.']\n"
     ]
    }
   ],
   "source": [
    "#example usage\n",
    "search_substring = 'hearsay'\n",
    "result = filter_label_by_substring(owl_df, search_substring)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ojasva20318/.local/lib/python3.8/site-packages/datasets/load.py:1454: FutureWarning: The repository for nguha/legalbench contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/nguha/legalbench\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset_hearsay = datasets.load_dataset(\"nguha/legalbench\", \"hearsay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>index</th>\n",
       "      <th>slice</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Non-assertive conduct</td>\n",
       "      <td>On the issue of whether James is an smart indi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>Non-assertive conduct</td>\n",
       "      <td>On the issue of whether Robert negligently dro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Non-assertive conduct</td>\n",
       "      <td>On the issue of whether John knew about the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "      <td>Non-assertive conduct</td>\n",
       "      <td>On the issue of whether Michael was guilty of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>4</td>\n",
       "      <td>Non-assertive conduct</td>\n",
       "      <td>On the issue of whether William was loved by h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>No</td>\n",
       "      <td>89</td>\n",
       "      <td>Not introduced to prove truth</td>\n",
       "      <td>To prove that Arthur believed that Amy and Dan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>No</td>\n",
       "      <td>90</td>\n",
       "      <td>Not introduced to prove truth</td>\n",
       "      <td>To prove that the trademarks of restaurant A a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>No</td>\n",
       "      <td>91</td>\n",
       "      <td>Not introduced to prove truth</td>\n",
       "      <td>To prove that Michael knew of the existing pat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>No</td>\n",
       "      <td>92</td>\n",
       "      <td>Not introduced to prove truth</td>\n",
       "      <td>To prove that Arthur and Mary had a conversati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Yes</td>\n",
       "      <td>94</td>\n",
       "      <td>Standard hearsay</td>\n",
       "      <td>On the issue of whether Melissa was the agent ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   answer index                          slice  \\\n",
       "0      No     0          Non-assertive conduct   \n",
       "1      No     1          Non-assertive conduct   \n",
       "2      No     2          Non-assertive conduct   \n",
       "3      No     3          Non-assertive conduct   \n",
       "4      No     4          Non-assertive conduct   \n",
       "..    ...   ...                            ...   \n",
       "89     No    89  Not introduced to prove truth   \n",
       "90     No    90  Not introduced to prove truth   \n",
       "91     No    91  Not introduced to prove truth   \n",
       "92     No    92  Not introduced to prove truth   \n",
       "93    Yes    94               Standard hearsay   \n",
       "\n",
       "                                                 text  \n",
       "0   On the issue of whether James is an smart indi...  \n",
       "1   On the issue of whether Robert negligently dro...  \n",
       "2   On the issue of whether John knew about the co...  \n",
       "3   On the issue of whether Michael was guilty of ...  \n",
       "4   On the issue of whether William was loved by h...  \n",
       "..                                                ...  \n",
       "89  To prove that Arthur believed that Amy and Dan...  \n",
       "90  To prove that the trademarks of restaurant A a...  \n",
       "91  To prove that Michael knew of the existing pat...  \n",
       "92  To prove that Arthur and Mary had a conversati...  \n",
       "93  On the issue of whether Melissa was the agent ...  \n",
       "\n",
       "[94 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = dataset_hearsay['test'].to_pandas()\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On the issue of whether James is an smart individual, the fact that James came first in his class in law school.',\n",
       " 'On the issue of whether Robert negligently drove, the fact that Robert fell asleep while driving.',\n",
       " 'On the issue of whether John knew about the conspiracy, the fact that John likes sweatpants.',\n",
       " 'On the issue of whether Michael was guilty of murder, the fact that Michael left the crime scene immediately.',\n",
       " 'On the issue of whether William was loved by his community, the fact that he was selected to speak at his graduation.',\n",
       " 'On the issue of whether Mary robbed the bank, the fact that Mary went to the bank in disguise.',\n",
       " 'On the issue of whether Patricia was a fan of Coldplay, the fact that she had a poster with the lyrics of \"Viva la Vida\" on her bedroom wall.',\n",
       " \"On the issue of whether Jennifer suffered reputational harm from Linda's article, the fact that Linda worked with several different editors to proof read and cross-check her article.\",\n",
       " \"On the issue of whether Elizabeth was misdiagnosed by Barbara, the fact that Barbara didn't consult with her usual charts while assessing Elizabethg.\",\n",
       " 'On the issue of whether Richard had ever visited Chicago, the fact that he gave a speech there in 2005.',\n",
       " 'On the issue of how long Joseph and Thomas had known each other, the fact that were neighbors during elementary school.',\n",
       " 'On the issue of whether Susan was familiar with Shakespeare, the fact that she had once played the role of Macbeth and recieved a standing ovation after her monologue.',\n",
       " 'On the issue of whether Jessica was aware she was trespassing, the fact that Jessica had been diagnosed as near-sighted by her ophthalmologist.',\n",
       " 'On the issue of whether Sarah was acting as an agent for the corporation, the fact that Sarah had worked there previously for four years.',\n",
       " 'On the issue of whether Charles was responsible for the defamatory article published online, the fact that Charles had visited the website responsible for the article multiple times in the past year.',\n",
       " 'On the issue of whether Karen negligently operated the forklift, the fact that Karen is a terrible driver who needed several tries to pass her commercial license test.',\n",
       " \"On the issue of whether the patent was infringed, the fact that the corporation's chief scientist was known to cheat at card games.\",\n",
       " 'On the issue of whether Christopher acted with malice, the fact that Christopher was often moody and had a large temper.',\n",
       " 'On the issue of whether the parties had actually agreed to the contract, the fact that one of the parties had mistaken the identity of the other party.',\n",
       " \"On the issue of which car was responsible for a hit-and-run, the witness's statement in court that she believed it was the blue sedan.\",\n",
       " 'On the issue of the faultiness of the designed house, the drawing the witness made on the stand during testimony.',\n",
       " 'On the issue of which of the defendants was responsible for driving the get-away car, the fact that the witness on the stand turned and pointed to the man in the blue suit.',\n",
       " \"On the issue of whether Ana lied to Jim, Jim's statement on cross-examination that he did not believe Ana to be an honest individual.\",\n",
       " 'On the issue of whether Daniel drove negligently, the fact that Daniel testified during the trial that he told his wife he was tired before driving into a tree.',\n",
       " \"On the issue of whether Carl had knowledge of Amy's intentions, Carl told the questioning attorney on redirect examination that he knew Amy's intentions.\",\n",
       " 'On the issue of whether Nancy knew who attacked her, Nancy\\'s statement in court, \"I was attacked by Sandra.\"',\n",
       " \"During the trial, Mark nodded when asked if it was him who vandalized Emily's garage.\",\n",
       " 'On the issue of which employee saw Emily last, the fact that James testified on the stand that he was on vacation that day and had last spoken to Emily the week before.',\n",
       " 'On the issue of whether Will knew that the company intended to announce its drug trials had been cancelled, the fact that he told the jury that \"he didn\\'t know the first thing about how medicines worked.\"',\n",
       " \"On the issue of the defendant's guilt, Alex told the jury that he believed the defendant was guilty.\",\n",
       " 'When showed floorplans of the museum during the trial, Andrew pointed out all the escape routes that he and his co-conspirators had planned to use. ',\n",
       " 'When asked if he was remorseful by his attorney, Adam testified that \"he regretted his actions.\"',\n",
       " 'On the issue of whether Donna was ill, the fact that Margaret testified during trial that she thought Donna was unwell.',\n",
       " 'On the issue of the sanity of Deborah, the fact that Deborah told her friend that she is completely sane.',\n",
       " \"On the issue of whether Joshua's friend was attacked by Ashley, the fact that Joshua's friend told Joshua that he was just attacked by Ashley.\",\n",
       " 'To prove that Tom was in town, a witness testifies that her friend Susan told her Tom was in town.',\n",
       " 'To prove that Shirley committed fraud, the fact that she told her neighbor she was planning on tricking the defendant by selling them a fake handbag.',\n",
       " 'To prove that Stephen drove his car through a stoplight, the fact that Stephen told his wife on the phone that he was not paying attention to the road.',\n",
       " 'On the issue of whether Henry negligently hit Kelly with his car, Kelly\\'s statement, \"a witness told me after the crash that Henry was on his cell phone while driving.\"',\n",
       " 'On the issue of whether Sean knew he was going to die, the fact that he called his lawyer wanting to make final changes to his will.',\n",
       " 'To prove that Alice was planning a robbery, the fact that she told Bob she was researching escape routes.',\n",
       " 'To prove that Bruce committed medical malpractice, the fact that he told a nurse after the surgery that he thought he made a mistake.',\n",
       " 'To prove that Alice suffered reputational harm, the fact that people were telling Bob that they had heard bad things about Alice.',\n",
       " 'To prove that the Denise and Tom were married, the fact that Amber sent them a card congratulating them on their wedding.',\n",
       " 'To prove that Alex owned a red car, the fact that she sent an email to her mother explaining how she was planning on purchasing a red car.',\n",
       " \"On the issue of whether the accident was a result of negligence, a bystander's exclamation that driver was looking down at their phone while driving.\",\n",
       " 'On the issue of whether Henry committed insider trading, Alice\\'s statement to Bob that Henry told her he had a \"great tip about what a certain stock was going to do.\" ',\n",
       " 'On the issue of whether Jane knew that Arthur was having an affair, the fact that Bob told Jane he suspected Arthur was up to something.',\n",
       " \"On the issue of whether Mary was Tom's mother, the fact that Albert told Mary that Tom looked like her.\",\n",
       " 'Alex is being sued for breach of contract relating to a delay in the shipment of mangos. To prove that the shipment was delayed, a witness for the plaintiffs testifies that he heard Alex complain about not being able to deliver the mangos in time. ',\n",
       " \"Alex is being prosecuted for participation in a criminal conspiracy. To prove that Alex participated in the conspiracy, the prosecution's witness testifies that she heard Alex making plans to meet with his co-conspirators.\",\n",
       " 'Dave is being sued by Taylor Swift for copyright infringement. To prove that Dave had listened to Taylor Swift, the plaintiffs introduced evidence of Dave telling his friend that his favorite artist was Taylor Swift.',\n",
       " 'Angela is suing Harold for defamation. To prove that Harold knowingly spread falsehoods about her, Angela introduces evidence of Harold admitting to his friend that his statements about Angela were all lies.',\n",
       " 'Joe is being prosecuted for illegally transporting turtle eggs under an animal trafficking statute. To prove that Joe owned the duffel bag in which contained the turtle eggs, the prosecution introduces evidence of Joe asking a store-owner if they carried any duffel bags.',\n",
       " 'Corporation A decides to sue Corporation B for stealing trade secrets. To prove that B did it, lawyers for A introduce as evidence a statement made by B\\'s CEO to one of his friends that B \"would soon have the ability to compete with A.\" ',\n",
       " 'Kim is sued by Jim for medical malpractice. To prove that Kim was a bad doctor, Jim introduces evidence that Kim told her friend that she cheated on all her medical school exams.',\n",
       " 'To prove that Albert likes ice cream, the prosecution introduces evidence that Albert told Jim that \"he wanted an ice cream.\"',\n",
       " 'To prove that Jose liked red Ferraris, the defense introduces testimony from Tracy discussing how Jose told her that his favorite color is red and his favorite car is a Ferrari.',\n",
       " 'To prove that Michael was in charge of the group, the plaintiffs introduce evidence of Michael stating, \"I am the boss here.\"',\n",
       " \"On the issue of whether Sandy knew that the cash belonged to Michael, Amy testifies that Sandy told her he thought he had Michael's cash.\",\n",
       " 'On the issue of whether the brakes were faulty, Amy testifies that she heard Arthur claim that he thought something was wrong with the car.',\n",
       " 'To prove that the CEO knew of the fraudulent activities occuring, an email he sent his acknowledging his awareness of the conduct.',\n",
       " 'On the issue of whether the defendant knew he was trespassing, the fact that he shook his head when asked by a passing security guard about whether he knew he was on private property.',\n",
       " 'To prove that the insured under a life policy is dead, his wife offers a death certificate.',\n",
       " 'On the issue of which car was responsible for the hit and run, the fact that a bystander turned and pointed to a blue sedan.',\n",
       " \"To prove that the defendant was present at the scene of the crime, a forensics report describing traces of the defendant's DNA that was found by investigators.\",\n",
       " 'To prove that the student was a terrible student, the mid-term report written by his teacher.',\n",
       " 'On the issue of whether Albert visited the garage sale, the fact that Tom wrote in an email to Mary that he saw Albert at the garage sale.',\n",
       " 'On the issue of whether Albert supported environmental reform, the fact that his car has many stickers with environmental slogans.',\n",
       " 'On the issue of whether Albert bought a knife, Angela testified that he shook his head when she asked him.',\n",
       " \"On the issue of Albert's wellbeing after the accident, Angela testified that he gave a thumbs up when asked how he was feeling.\",\n",
       " \"Albert is being sued by Tim for breach of contract. The issue is whether Albert knew at the time signing the contract that he had no intention of following through. Tim's lawyers introduce an email Albert wrote to his friend saying he wasn't planning on honoring the contract with Tim.\",\n",
       " 'On the issue of whether factory management had underpaid its workers, the fact that workers carried signs during a protest demanding equitable compensation.',\n",
       " 'On the issue of whether Albert liked Taylor Swift, a social media post written by Albert describing Taylor as his favorite artist.',\n",
       " 'On the issue of whether Tom identified the suspect at the scene of the crime, the fact that he pointed at the defendant.',\n",
       " 'On the issue of the sanity of Bobby, the fact that Bobby told a friend that she believed she was Santa Clause.',\n",
       " 'On the issue of whether Vincent was provoked by Roy, the fact that Vincent was told by his friend that he had just been attacked by Roy.',\n",
       " 'On the issue of whether Kayla knew she was ill, the fact that Philip told him she was unwell.',\n",
       " 'On the issue of damage to Mason\\'s reputation as a sober individual, Lori\\'s public statement, \"I have never seen Mason drink.\"',\n",
       " 'On the issue of whether Gerald was alive immediately after being attacked by Kathryn, Gerald\\'s statement, \"I was attacked by Kathryn.\"',\n",
       " 'On issue of Dylan\\'s ill feeling toward Andrew, Dylan\\'s statement, \"Andrew is a liar and a hypocrite.\"',\n",
       " 'On the issue of whether Ann and Austin had a discussion, Ann\\'s statement to Austin, \"I will sell my car to you for a cost.\"',\n",
       " 'On the issue of whether Patty knew Dave, the prosecution introduces evidence that Patty told Arthur, \"Dave is really dishonest.\"',\n",
       " 'To prove that Arthur knew English, the fact that Arthur told Bill (in English) that he thought Mary robbed the bank.',\n",
       " 'To prove that Arthur had notice that a bank robbery was being planned, the fact that Arthur told Mary that he heard a bank robbery was going to happen.',\n",
       " 'To prove that Mary was at the mall on January 2nd, a mall employee testifies that on January 2nd, Mary told him she was planning on purchasing several diamonds.',\n",
       " 'To prove that Anne supported David, the fact that Anne said that David was the best boss in the world.',\n",
       " 'To prove that Daniel was sad upon hearing the news, the fact he exlaimed \"the world was ending\" and \"nothing mattered anymore.\"',\n",
       " 'To prove that Amy believed that Karl was an employee of the store, the fact that Amy told Karl that the store needed to refund her because their marketing had been decieving. ',\n",
       " 'To prove that Arthur believed that Amy and Daniel were married, the fact that he wished them a happy anniversary.',\n",
       " 'To prove that the trademarks of restaurant A and restaurant B created confusion, the fact that a customer called one and placed an order believing it to actually be the other.',\n",
       " 'To prove that Michael knew of the existing patent, the fact that he told his friend that the patent was poorly written.',\n",
       " 'To prove that Arthur and Mary had a conversation, the fact that Arthur told Mary that the weather the following week would be terrible.',\n",
       " \"On the issue of whether Melissa was the agent of Mark, Melissa's statement to Tim that her offer was being made as an agent of Mark.\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = test_df[\"text\"].tolist()\n",
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On the issue of whether James is an smart individual, the fact that James came first in his class in law school.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels_and_definitions_to_prompt(prompt_text, filtered_labels):\n",
    "    \"\"\"\n",
    "    Add filtered labels and definitions to the prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt_text (str): The prompt text.\n",
    "        filtered_labels (list): List of strings containing labels and definitions.\n",
    "\n",
    "    Returns:\n",
    "        str: The full prompt text with filtered labels and definitions added.\n",
    "    \"\"\"\n",
    "    # Initialize full_prompt with prompt_text\n",
    "    full_prompt = f\"\"\"\n",
    "    Statement : {prompt_text}\n",
    "    Question: Consider utilizing the following legal ontology classes to frame your argument:\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add filtered labels and definitions to the prompt\n",
    "    for label_definition in filtered_labels:\n",
    "        full_prompt += f\"\\n{label_definition}\"\n",
    "\n",
    "    # Add the remaining part of the prompt\n",
    "    full_prompt += \"\"\"\n",
    "    Use these ontology classes to structure your argument and analyze whether the information provided falls under the category of hearsay.\n",
    "    \n",
    "    Output Format: First word of your answer should be either Yes or No followed by step-by-step reasoning\n",
    "\n",
    "    Answer: \n",
    "    \"\"\"\n",
    "\n",
    "    return full_prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing - Hearsay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Motion in Limine to Exclude Hearsay Witness : A Motion in Limine to Exclude Hearsay Witness is a legal request made by one party to prevent the other party from presenting testimony from a witness who will testify about statements made by someone else out of court, which are being offered to prove the truth of the matter asserted.',\n",
       " 'Motion to Exclude Hearsay Witness : A Motion to Exclude Hearsay Witness is a legal request to prevent a witness from testifying in court based on the fact that their testimony is based on hearsay evidence.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_labels = filter_label_by_substring(owl_df, \"hearsay\")\n",
    "filtered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Statement : On the issue of whether James is an smart individual, the fact that James came first in his class in law school.\n",
      "    Question: Consider utilizing the following legal ontology classes to frame your argument:\n",
      "\n",
      "    \n",
      "Motion in Limine to Exclude Hearsay Witness : A Motion in Limine to Exclude Hearsay Witness is a legal request made by one party to prevent the other party from presenting testimony from a witness who will testify about statements made by someone else out of court, which are being offered to prove the truth of the matter asserted.\n",
      "Motion to Exclude Hearsay Witness : A Motion to Exclude Hearsay Witness is a legal request to prevent a witness from testifying in court based on the fact that their testimony is based on hearsay evidence.\n",
      "    Use these ontology classes to structure your argument and analyze whether the information provided falls under the category of hearsay.\n",
      "    \n",
      "    Output Format: First word of your answer should be either Yes or No followed by step-by-step reasoning\n",
      "\n",
      "    Answer: \n",
      "     Yes, the information provided falls under the category of hearsay.\n",
      "\n",
      "     Reasoning:\n",
      "\n",
      "     The statement provided is \"On the issue of whether James is a smart individual, the fact that James came first in his class in law school.\" This statement is hearsay because it is a statement made by someone else (the teacher) outside of court, and is being offered to prove the truth of the matter asserted (James' intelligence). Therefore, it falls under the category of hearsay evidence and should be excluded under a Motion in Limine to Exclude Hearsay Witness or a Motion to Exclude Hearsay Witness.\n",
      "\n",
      "     The fact that James came first in his class in law school is not a direct statement made by James himself, but rather a statement made by his teacher or someone else outside of court. As such, it is not admissible as evidence in a legal proceeding without proper foundation or authentication.\n",
      "\n",
      "     Additionally, the statement does not meet the exception to the hearsay rule, such as an excited utterance or a statement made for purposes of medical treatment. Therefore, it should be excluded under a Motion in Limine to Exclude Hearsay Witness or a Motion to Exclude Hearsay Witness.\n"
     ]
    }
   ],
   "source": [
    "prompt = add_labels_and_definitions_to_prompt(prompts[0],filtered_labels)\n",
    "\n",
    "response = get_llama_response(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ojasva20318/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for prompt 1\n",
      "Done for prompt 2\n",
      "Done for prompt 3\n",
      "Done for prompt 4\n",
      "Done for prompt 5\n",
      "Done for prompt 6\n",
      "Done for prompt 7\n",
      "Done for prompt 8\n",
      "Done for prompt 9\n",
      "Done for prompt 10\n",
      "Done for prompt 11\n",
      "Done for prompt 12\n",
      "Done for prompt 13\n",
      "Done for prompt 14\n",
      "Done for prompt 15\n",
      "Done for prompt 16\n",
      "Done for prompt 17\n",
      "Done for prompt 18\n",
      "Done for prompt 19\n",
      "Done for prompt 20\n",
      "Done for prompt 21\n",
      "Done for prompt 22\n",
      "Done for prompt 23\n",
      "Done for prompt 24\n",
      "Done for prompt 25\n",
      "Done for prompt 26\n",
      "Done for prompt 27\n",
      "Done for prompt 28\n",
      "Done for prompt 29\n",
      "Done for prompt 30\n",
      "Done for prompt 31\n",
      "Done for prompt 32\n",
      "Done for prompt 33\n",
      "Done for prompt 34\n",
      "Done for prompt 35\n",
      "Done for prompt 36\n",
      "Done for prompt 37\n",
      "Done for prompt 38\n",
      "Done for prompt 39\n",
      "Done for prompt 40\n",
      "Done for prompt 41\n",
      "Done for prompt 42\n",
      "Done for prompt 43\n",
      "Done for prompt 44\n",
      "Done for prompt 45\n",
      "Done for prompt 46\n",
      "Done for prompt 47\n",
      "Done for prompt 48\n",
      "Done for prompt 49\n",
      "Done for prompt 50\n",
      "Done for prompt 51\n",
      "Done for prompt 52\n",
      "Done for prompt 53\n",
      "Done for prompt 54\n",
      "Done for prompt 55\n",
      "Done for prompt 56\n",
      "Done for prompt 57\n",
      "Done for prompt 58\n",
      "Done for prompt 59\n",
      "Done for prompt 60\n",
      "Done for prompt 61\n",
      "Done for prompt 62\n",
      "Done for prompt 63\n",
      "Done for prompt 64\n",
      "Done for prompt 65\n",
      "Done for prompt 66\n",
      "Done for prompt 67\n",
      "Done for prompt 68\n",
      "Done for prompt 69\n",
      "Done for prompt 70\n",
      "Done for prompt 71\n",
      "Done for prompt 72\n",
      "Done for prompt 73\n",
      "Done for prompt 74\n",
      "Done for prompt 75\n",
      "Done for prompt 76\n",
      "Done for prompt 77\n",
      "Done for prompt 78\n",
      "Done for prompt 79\n",
      "Done for prompt 80\n",
      "Done for prompt 81\n",
      "Done for prompt 82\n",
      "Done for prompt 83\n",
      "Done for prompt 84\n",
      "Done for prompt 85\n",
      "Done for prompt 86\n",
      "Done for prompt 87\n",
      "Done for prompt 88\n",
      "Done for prompt 89\n",
      "Done for prompt 90\n",
      "Done for prompt 91\n",
      "Done for prompt 92\n",
      "Done for prompt 93\n",
      "Done for prompt 94\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "\n",
    "filtered_labels = filter_label_by_substring(owl_df, \"hearsay\")\n",
    "\n",
    "for i, prompt_text in enumerate(prompts):\n",
    "\n",
    "    full_prompt = add_labels_and_definitions_to_prompt(prompt_text, filtered_labels)\n",
    "\n",
    "    response = get_llama_response(full_prompt)\n",
    "    \n",
    "    responses.append(response)\n",
    "    \n",
    "    print(f\"Done for prompt {i+1}\")\n",
    "    # print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Statement : Alex is being prosecuted for participation in a criminal conspiracy. To prove that Alex participated in the conspiracy, the prosecution's witness testifies that she heard Alex making plans to meet with his co-conspirators.\n",
      "    Question: Consider utilizing the following legal ontology classes to frame your argument:\n",
      "\n",
      "    \n",
      "Motion in Limine to Exclude Hearsay Witness : A Motion in Limine to Exclude Hearsay Witness is a legal request made by one party to prevent the other party from presenting testimony from a witness who will testify about statements made by someone else out of court, which are being offered to prove the truth of the matter asserted.\n",
      "Motion to Exclude Hearsay Witness : A Motion to Exclude Hearsay Witness is a legal request to prevent a witness from testifying in court based on the fact that their testimony is based on hearsay evidence.\n",
      "    Use these ontology classes to structure your argument and analyze whether the information provided falls under the category of hearsay.\n",
      "    \n",
      "    Output Format: First word of your answer should be Yes or No\n",
      "    Answer:\n",
      "     Yes, the prosecution's witness testimony about Alex making plans to meet with his co-conspirators is hearsay evidence.\n",
      "     Reasoning: The witness is testifying about statements made by Alex outside of court, which are being offered to prove the truth of the matter asserted. This is a classic example of hearsay evidence, as the witness is not testifying about personal knowledge or observations, but rather about what someone else said. Therefore, the motion in limine to exclude hearsay witness should be granted.\n"
     ]
    }
   ],
   "source": [
    "print(responses[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n"
     ]
    }
   ],
   "source": [
    "# Example response\n",
    "response = \"\"\"\n",
    "Statement : On the issue of the faultiness of the designed house, the drawing the witness made on the stand during testimony.\n",
    "    Question: Consider utilizing the following legal ontology classes to frame your argument:\n",
    "\n",
    "    \n",
    "Motion in Limine to Exclude Hearsay Witness : A Motion in Limine to Exclude Hearsay Witness is a legal request made by one party to prevent the other party from presenting testimony from a witness who will testify about statements made by someone else out of court, which are being offered to prove the truth of the matter asserted.\n",
    "Motion to Exclude Hearsay Witness : A Motion to Exclude Hearsay Witness is a legal request to prevent a witness from testifying in court based on the fact that their testimony is based on hearsay evidence.\n",
    "    Use these ontology classes to structure your argument and analyze whether the information provided falls under the category of hearsay.\n",
    "    \n",
    "    Output Format: First word of your answer should be Yes or No\n",
    "    Answer:\n",
    "    \n",
    "    Yes, the information provided falls under the category of hearsay.\n",
    "    Reasoning:\n",
    "    The witness testimony provided is a statement made by someone else outside of court, which is being offered to prove the truth of the matter asserted. This is a classic example of hearsay evidence, which is not admissible in court.\n",
    "    Therefore, the motion in limine to exclude hearsay witness should be granted.\n",
    "\n",
    "Explanation:\n",
    "The provided information falls under the category of hearsay because it is a statement made by someone else outside of court, which is being offered to prove the truth of the matter asserted. Hearsay evidence is not admissible in court, and the witness testimony provided is a clear example of hearsay. Therefore, the motion in limine to exclude hearsay witness should be granted.\n",
    "\n",
    "Please let me know if you need any further assistance\n",
    "\"\"\"\n",
    "\n",
    "# Use regex to extract the answer\n",
    "match = re.search(r\"Answer:\\s+(Yes|No)\", response)\n",
    "if match:\n",
    "    answer = match.group(1)\n",
    "else:\n",
    "    answer = \"Unknown\"\n",
    "\n",
    "print(answer)  # Output: \"Yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answers(original_responses):\n",
    "    \"\"\"\n",
    "    Extracts \"Yes\" or \"No\" from each response in the original_responses list.\n",
    "    \n",
    "    Args:\n",
    "        original_responses (list): List of strings containing responses.\n",
    "        \n",
    "    Returns:\n",
    "        list: List containing \"Yes\" or \"No\" extracted from each response.\n",
    "    \"\"\"\n",
    "    # Create an empty list to store extracted answers\n",
    "    extracted_answers = []\n",
    "\n",
    "    # Process each response\n",
    "    for response in original_responses:\n",
    "        # Use regex to extract the answer\n",
    "        match = re.search(r\"Answer:\\s+(Yes|No)\", response)\n",
    "        if match:\n",
    "            answer = match.group(1)\n",
    "        else:\n",
    "            answer = \"Unknown\"\n",
    "\n",
    "        # Append the extracted answer to the list\n",
    "        extracted_answers.append(answer)\n",
    "\n",
    "    return extracted_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_answers = extract_answers(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']\n"
     ]
    }
   ],
   "source": [
    "print(extracted_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\"hearsay\", extracted_answers, test_df[\"answer\"].tolist()[:len(extracted_answers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
