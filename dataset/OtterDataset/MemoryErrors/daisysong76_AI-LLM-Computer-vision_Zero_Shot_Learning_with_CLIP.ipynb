{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYrDN86ZdUTGIYTJfZnsdQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daisysong76/AI-LLM-Computer-vision/blob/main/Zero_Shot_Learning_with_CLIP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimize Batch Size Dynamically\n",
        "Why Dynamic Batch Size?\n",
        "Maximizes GPU memory utilization without running out of memory.\n",
        "Allows larger datasets to process efficiently."
      ],
      "metadata": {
        "id": "DzBC3gpF9SAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch\n",
        "\n",
        "def get_dataloader(X, y, batch_size):\n",
        "    dataset = TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long))\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    return dataloader\n",
        "\n",
        "def dynamic_batch_size(X, y, min_size=32, max_size=512, step=32):\n",
        "    for batch_size in range(min_size, max_size + 1, step):\n",
        "        try:\n",
        "            dataloader = get_dataloader(X, y, batch_size)\n",
        "            for _ in dataloader:\n",
        "                pass  # Test if data fits in memory\n",
        "            return batch_size\n",
        "        except RuntimeError:\n",
        "            continue\n",
        "    raise MemoryError(\"No batch size fits available memory.\")\n"
      ],
      "metadata": {
        "id": "2izhbhQe9UCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = dynamic_batch_size(X_train, y_train)\n",
        "train_loader = get_dataloader(X_train, y_train, batch_size)\n",
        "val_loader = get_dataloader(X_val, y_val, batch_size)"
      ],
      "metadata": {
        "id": "_2DXMZPg9X8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional Enhancements:\n",
        "Gradient Clipping:\n",
        "Prevents exploding gradients in deep networks."
      ],
      "metadata": {
        "id": "pChZkSK_9hZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n"
      ],
      "metadata": {
        "id": "nA_xQsE39irh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mixed Precision Training:\n",
        "Speeds up computation using float16 without losing much accuracy."
      ],
      "metadata": {
        "id": "7gAUKbZE9vGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "scaler = GradScaler()\n",
        "\n",
        "for data, target in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    with autocast():\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n"
      ],
      "metadata": {
        "id": "tDXBXcZX9wZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning Rate Scheduling:\n",
        "Adjusts the learning rate dynamically for faster convergence."
      ],
      "metadata": {
        "id": "XNA1nbnT9pJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # Reduce LR by 10x every 10 epochs\n"
      ],
      "metadata": {
        "id": "66gzWIp59qZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monitoring with TensorBoard:\n",
        "Track performance metrics like loss and accuracy visually."
      ],
      "metadata": {
        "id": "DKrpzlKG91DC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorboard\n"
      ],
      "metadata": {
        "id": "g26XdMxx92iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter('runs/genre_classifier')\n",
        "\n",
        "# Log loss and accuracy\n",
        "writer.add_scalar('Loss/train', loss.item(), epoch)\n",
        "writer.add_scalar('Accuracy/train', accuracy, epoch)\n"
      ],
      "metadata": {
        "id": "CzSUU5D394G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout Layers: Prevent overfitting and improve generalization.\n",
        "\n",
        "Deeper Layers: Capture nonlinear relationships better.\n",
        "\n",
        "Transformer Models (Optional): Enhance embeddings for complex relationships.\n",
        "\n",
        "Dynamic Batch Sizing: Optimizes memory utilization.\n",
        "\n",
        "Advanced Techniques: Gradient clipping, LR scheduling, and mixed precision ensure efficient training.\n",
        "\n",
        "Monitoring Tools: TensorBoard tracks performance and debugging.\n"
      ],
      "metadata": {
        "id": "irWODEaZ99ko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zero-Shot Learning with CLIP\n",
        "\n",
        "Zero-shot learning allows you to classify data into categories that were not explicitly seen during training. In this case, we want to predict genres that might not be present in your original dataset.\n",
        "\n",
        "Steps to Implement Zero-Shot Learning\n",
        "\n",
        "Define Genre Prompts: Create textual prompts that represent each genre you want to classify. These prompts should capture the essence of the genre. For example:\n",
        "\n",
        "\"a jazz music artist\"\n",
        "\"an opera singer\"\n",
        "\"a country music band\"\n",
        "\"an electronic music producer\"\n",
        "... (and so on for other genres)\n",
        "Generate Embeddings for Prompts: Use the CLIP model to generate embeddings for these genre prompts:\n",
        "\n",
        "\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "\n",
        "   device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "   model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "   processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "   genre_prompts = [\n",
        "       \"a jazz music artist\",\n",
        "       \"an opera singer\",\n",
        "       # ... other genre prompts\n",
        "   ]\n",
        "\n",
        "   genre_prompt_embeddings = []\n",
        "   for prompt in genre_prompts:\n",
        "       inputs = processor(text=[prompt], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "       with torch.no_grad():\n",
        "           embedding = model.get_text_features(**inputs).cpu().numpy()\n",
        "       genre_prompt_embeddings.append(embedding[0])\n",
        "Use code with caution\n",
        "Generate Embedding for New Artist: Get the CLIP embedding for the new artist name you want to classify, as you did before:\n",
        "\n",
        "def get_embedding(artist_name):\n",
        "       inputs = processor(text=[artist_name], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "       with torch.no_grad():\n",
        "           embedding = model.get_text_features(**inputs).cpu().numpy()\n",
        "       return embedding[0]\n",
        "\n",
        "   new_artist_name = \"Billie Eilish\" # Example\n",
        "   new_artist_embedding = get_embedding(new_artist_name)\n",
        "Use code with caution\n",
        "Calculate Similarity: Calculate the cosine similarity between the new artist's embedding and the genre prompt embeddings:\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "   similarities = []\n",
        "   for genre_embedding in genre_prompt_embeddings:\n",
        "       similarity = np.dot(new_artist_embedding, genre_embedding) / (np.linalg.norm(new_artist_embedding) * np.linalg.norm(genre_embedding))\n",
        "       similarities.append(similarity)\n",
        "Use code with caution\n",
        "Predict Genre: The genre with the highest similarity score is the predicted genre for the new artist:\n",
        "\n",
        "predicted_genre_index = np.argmax(similarities)\n",
        "   predicted_genre = genre_prompts[predicted_genre_index]\n",
        "\n",
        "   print(f\"Predicted genre for {new_artist_name}: {predicted_genre}\")\n",
        "Use code with caution\n",
        "How it Works\n",
        "\n",
        "CLIP's pre-training allows it to understand the semantic meaning of the genre prompts.\n",
        "By comparing the embedding of the new artist to the genre prompt embeddings, we're essentially asking CLIP: \"Which of these genre concepts is most similar to this artist?\"\n",
        "The genre with the highest similarity is the most likely genre for the artist.\n",
        "Advantages\n",
        "\n",
        "No Training Data for New Genres: You can predict genres that were not in your original training data.\n",
        "Leverages CLIP's General Knowledge: CLIP's pre-training on a massive dataset provides a broad understanding of concepts, including genres.\n",
        "Limitations\n",
        "\n",
        "Prompt Engineering: The quality of the genre prompts is crucial for accurate predictions.\n",
        "CLIP's Biases: CLIP's pre-training data might contain biases that could affect the predictions.\n",
        "I hope this helps you understand how to implement zero-shot learning with CLIP for genre classification. Feel free to ask any further questions. Let me know if you need help crafting effective genre prompts!"
      ],
      "metadata": {
        "id": "REHdGut05qvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embeddings Capture Semantic Information: The key is the use of CLIP embeddings. CLIP is trained on a massive dataset of images and text, learning to associate related concepts in a high-dimensional space. Even if artist names don't directly reveal their genre, CLIP embeddings can capture semantic relationships. For example, the embeddings for \"Miles Davis\" and \"John Coltrane\" might be closer together in the embedding space because they are both associated with jazz music in the vast dataset CLIP was trained on."
      ],
      "metadata": {
        "id": "QQtYKtPy5y1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classifier Learns Patterns in the Embedding Space: The logistic regression classifier is trained on these embeddings and their corresponding genres. It essentially learns a mapping between the embedding space and the genre labels. It identifies patterns in the embeddings that distinguish different genres.\n",
        "\n",
        "Generalization: The classifier is expected to generalize to new artist names, even if they haven't been seen before. When you provide a new artist name, CLIP generates an embedding for it, and the classifier uses the learned patterns in the embedding space to predict the genre."
      ],
      "metadata": {
        "id": "402zRXn_52Vw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example\n",
        "\n",
        "Let's say you input the artist name \"Taylor Swift\". Even if this name isn't in the training data, CLIP might place its embedding in a region of the embedding space that's close to other artists associated with pop or country music. The classifier, having learned these patterns during training, would then predict the genre based on the embedding's location in the embedding space.\n",
        "\n",
        "In essence, the classifier doesn't rely on direct similarities between artist names but rather on the semantic relationships captured by the CLIP embeddings.\n",
        "\n",
        "Why it Works without Explicit Relationships\n",
        "\n",
        "CLIP's large-scale training allows it to learn complex relationships between text and concepts, including genre associations. The embedding space represents these relationships, even if they are not obvious from the surface level of the artist names. The classifier learns to exploit these relationships to make predictions. Important Note: While this approach can work well, it's not perfect. There will always be cases where the classifier makes incorrect predictions due to limitations in the embeddings or the classifier itself. The performance of the model depends heavily on the quality and relevance of the data used to train CLIP and the classifier."
      ],
      "metadata": {
        "id": "lZ-W-uwn56u4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patterns Behind the Classifier's Predictions\n",
        "\n",
        "Semantic Clusters in Embedding Space: CLIP embeddings tend to create semantic clusters in the embedding space. Artists belonging to the same genre often have embeddings that are closer together, while artists from different genres are further apart. This clustering is due to the underlying semantic relationships learned by CLIP during its pre-training on a massive dataset.\n",
        "\n",
        "Decision Boundaries: The logistic regression classifier essentially learns decision boundaries in the embedding space. These boundaries separate regions associated with different genres. When a new artist name is embedded, the classifier determines which side of the decision boundaries the embedding falls on to make the genre prediction.\n",
        "\n",
        "Feature Importance: While CLIP embeddings are high-dimensional and complex, the classifier might learn to focus on specific features or dimensions within the embedding space that are most indicative of certain genres. These features might represent characteristics like musical style, instrumentation, or lyrical themes, which CLIP has implicitly learned to associate with genres.\n",
        "\n",
        "Contextual Information: CLIP's training on a vast dataset of images and text allows it to capture contextual information about artists. For example, an artist name might be associated with images of certain musical instruments or with text descriptions of their musical style. This contextual information influences the embedding and helps the classifier learn more nuanced genre associations.\n",
        "\n",
        "Illustrative Example\n",
        "\n",
        "Imagine a simplified 2D embedding space. Artists belonging to \"rock\" might cluster in one region, while \"pop\" artists cluster in another. The classifier would learn a decision boundary separating these regions. When you embed a new artist name, its location relative to this boundary determines the predicted genre.\n",
        "\n",
        "In summary, the patterns behind the classifier's predictions involve:\n",
        "\n",
        "Semantic clustering of embeddings based on genre. Decision boundaries learned by the classifier to separate genres. Feature importance within the embedding space for genre discrimination. Contextual information captured by CLIP influencing embedding and genre associations. Important Considerations\n",
        "\n",
        "The complexity of these patterns depends on the CLIP model used and the diversity of the training data. The classifier's ability to generalize to new artists relies on the quality and relevance of these learned patterns. While CLIP embeddings provide a powerful basis for genre prediction, they are not perfect, and misclassifications can occur."
      ],
      "metadata": {
        "id": "KwNaOAMV5-mw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scalability and Deployment Solution\n",
        "To address scalability and deployment challenges, follow these steps"
      ],
      "metadata": {
        "id": "OK8UFRaN-Imc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For fast nearest-neighbor search on embeddings, use Faiss (Facebook AI Similarity Search) or Annoy (Approximate Nearest Neighbors)."
      ],
      "metadata": {
        "id": "XlYB7GTz-N_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why?\n",
        "Faiss is optimized for high-dimensional vectors and GPU acceleration.\n",
        "Annoy is memory-efficient and ideal for read-only, low-latency lookup tasks"
      ],
      "metadata": {
        "id": "3LrYPtQD-SHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu"
      ],
      "metadata": {
        "id": "JSEgmDTd-hsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "# Create the Faiss index\n",
        "dimension = embeddings_train.shape[1]  # Dimensionality of embeddings\n",
        "index = faiss.IndexFlatL2(dimension)  # L2 distance metric\n",
        "\n",
        "# Add training embeddings to the index\n",
        "index.add(np.array(embeddings_train))\n",
        "\n",
        "# Query the index (example: find 5 nearest neighbors for the first sample)\n",
        "distances, indices = index.search(np.array([embeddings_val[0]]), k=5)\n",
        "print(\"Nearest neighbors:\", indices)\n"
      ],
      "metadata": {
        "id": "bgHEPTWP-XFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install annoy\n"
      ],
      "metadata": {
        "id": "DuEI11LU-d17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from annoy import AnnoyIndex\n",
        "\n",
        "# Create Annoy index\n",
        "dimension = embeddings_train.shape[1]\n",
        "index = AnnoyIndex(dimension, 'angular')  # Use 'angular' distance metric\n",
        "\n",
        "# Add embeddings to Annoy index\n",
        "for i, embedding in enumerate(embeddings_train):\n",
        "    index.add_item(i, embedding)\n",
        "\n",
        "# Build the index\n",
        "index.build(10)  # Number of trees\n",
        "index.save('genre_classifier_annoy_index.ann')\n",
        "\n",
        "# Query the index\n",
        "indices, distances = index.get_nns_by_vector(embeddings_val[0], n=5, include_distances=True)\n",
        "print(\"Nearest neighbors:\", indices, distances)\n"
      ],
      "metadata": {
        "id": "dfl-o9tG-ksD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploy Model with FastAPI for Real-Time Inference"
      ],
      "metadata": {
        "id": "egVl2Iic-oQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fastapi uvicorn\n"
      ],
      "metadata": {
        "id": "a0BK87ZY-rCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "import torch\n",
        "from transformers import CLIPModel, CLIPTokenizer\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Load Model and Tokenizer\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Prediction function\n",
        "def get_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        return model.get_text_features(**inputs).cpu().numpy()\n",
        "\n",
        "@app.post(\"/predict/\")\n",
        "def predict(artist_name: str):\n",
        "    embedding = get_embedding(artist_name)\n",
        "    distances, indices = index.search(np.array([embedding]), k=5)\n",
        "    return {\"nearest_genres\": [label_encoder.inverse_transform([y_train[i]])[0] for i in indices[0]]}\n"
      ],
      "metadata": {
        "id": "CX7-5R8H-wmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Run the Server"
      ],
      "metadata": {
        "id": "uxedWeT0-57J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uvicorn main:app --reload --port 8000\n"
      ],
      "metadata": {
        "id": "1XHQQ7M9-2nR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Test the API\n",
        "\n",
        "Send a POST request with an artist name:"
      ],
      "metadata": {
        "id": "oC1dNI8J--ZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "curl -X 'POST' \\\n",
        "  'http://127.0.0.1:8000/predict/' \\\n",
        "  -H 'Content-Type: application/json' \\\n",
        "  -d '{\"artist_name\": \"Miles Davis\"}'\n"
      ],
      "metadata": {
        "id": "pI41yDvM-9lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Containerize with Docker\n",
        "Why Docker?\n",
        "Makes deployment consistent across environments.\n",
        "Enables scalability with tools like Kubernetes."
      ],
      "metadata": {
        "id": "pY4yKEhy_Isn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Create Dockerfile"
      ],
      "metadata": {
        "id": "OJKVyeZ7_Nnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base Image\n",
        "FROM python:3.9-slim\n",
        "\n",
        "# Set the working directory\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy files\n",
        "COPY requirements.txt .\n",
        "COPY main.py .\n",
        "\n",
        "# Install dependencies\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Expose port\n",
        "EXPOSE 8000\n",
        "\n",
        "# Start FastAPI server\n",
        "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n"
      ],
      "metadata": {
        "id": "zRKC4i4-_LJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Create requirements.txt"
      ],
      "metadata": {
        "id": "YDHDUwPk_Wqg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Build and Run the Docker Container"
      ],
      "metadata": {
        "id": "VQUgDTXJ_ZfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Monitoring and Scaling\n",
        "Logging Requests and Responses:\n",
        "Add logging in FastAPI:"
      ],
      "metadata": {
        "id": "vGE2pK5H_cNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "@app.post(\"/predict/\")\n",
        "def predict(artist_name: str):\n",
        "    logging.info(f\"Received request for: {artist_name}\")\n",
        "    embedding = get_embedding(artist_name)\n",
        "    distances, indices = index.search(np.array([embedding]), k=5)\n",
        "    result = [label_encoder.inverse_transform([y_train[i]])[0] for i in indices[0]]\n",
        "    logging.info(f\"Response: {result}\")\n",
        "    return {\"nearest_genres\": result}\n"
      ],
      "metadata": {
        "id": "k77pWSc3_f18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling with Kubernetes (Optional):\n",
        "Deploy the containerized model to Kubernetes for horizontal scaling:"
      ],
      "metadata": {
        "id": "7xD9Vx3N_iyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kubectl create deployment genre-classifier --image=genre-classifier\n",
        "kubectl expose deployment genre-classifier --type=LoadBalancer --port=8000\n"
      ],
      "metadata": {
        "id": "GI2jWoC1_lQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Testing with Real-World Data\n",
        "Stress Testing with Locust:\n",
        "Simulate multiple users querying the API:"
      ],
      "metadata": {
        "id": "vO5c8A0z_rE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install locust\n"
      ],
      "metadata": {
        "id": "ukelkvfs_ur-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from locust import HttpUser, task\n",
        "\n",
        "class GenreClassifierTest(HttpUser):\n",
        "    @task\n",
        "    def test_model(self):\n",
        "        self.client.post(\"/predict/\", json={\"artist_name\": \"Miles Davis\"})\n"
      ],
      "metadata": {
        "id": "tEaVVYIB_wv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "locust -f locustfile.py --host=http://127.0.0.1:8000\n"
      ],
      "metadata": {
        "id": "iTDQROfw_5Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Takeaways:\n",
        "Faiss/Annoy:\n",
        "\n",
        "Handles fast nearest-neighbor lookup for embeddings.\n",
        "Faiss is GPU-accelerated; Annoy is lightweight and works well for smaller data.\n",
        "FastAPI:\n",
        "\n",
        "Provides a low-latency API for real-time inference.\n",
        "Can scale easily with Kubernetes or Docker Swarm.\n",
        "Dockerization:\n",
        "\n",
        "Enables portability and scalability for deployment.\n",
        "Supports CI/CD pipelines for updates.\n",
        "Scalability Testing:\n",
        "\n",
        "Use tools like Locust to simulate production workloads.\n",
        "This pipeline ensures your application can handle larger datasets efficiently and scale seamlessly for real-time inference. Let me know if you need more code examples or explanations!"
      ],
      "metadata": {
        "id": "V4cXFHJj_89v"
      }
    }
  ]
}