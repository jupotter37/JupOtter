{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b1d513-992c-4d14-910f-28c486e99841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In diesem Notebook wurde versucht mistral ohne unsloth zu trainieren, was jedoch GPU technisch schwer umsetzbar war"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0389fdf0-4bc8-4217-995a-588aa2978448",
   "metadata": {},
   "source": [
    "## Imports & Cuda-Verfügbarkeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e025d95b-4e63-42d3-9741-af4262611295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], device='cuda:0')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "torch.zeros(1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0e76f34-7fce-452c-bd7e-368fafc03ce6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "bitsandbytes\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "transformers\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "peft\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "accelerate\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "datasets\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "scipy\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "ipywidgets\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Needed installs\n",
    "# Um Model besser downloaden zu können\n",
    "%pip install -q -U bitsandbytes\n",
    "print(\"bitsandbytes\")\n",
    "%pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "print(\"transformers\")\n",
    "# Für Lora Finetuning benötigt\n",
    "%pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "print(\"peft\")\n",
    "%pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "print(\"accelerate\")\n",
    "# Sonstige Visualisierungs &\n",
    "%pip install -q -U datasets\n",
    "print(\"datasets\")\n",
    "%pip install -q -U scipy\n",
    "print(\"scipy\")\n",
    "%pip install -q -U ipywidgets\n",
    "print(\"ipywidgets\")\n",
    "%pip install -q -U matplotlib\n",
    "print(\"matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd45f2c-d9c5-4b3e-9107-8e60442a1948",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Artificial Datapreperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44e99199-f1c1-4d04-ba59-ee5506a96483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial xml preprocessing zum Testen des Trainings\n",
    "from lxml import etree\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "def extract_text_from_element(element):\n",
    "    texts = []\n",
    "    for elem in element.iter():\n",
    "        if elem.text:\n",
    "            texts.append(elem.text.strip())\n",
    "        if elem.tail:\n",
    "            texts.append(elem.tail.strip())\n",
    "    return ' '.join(texts)\n",
    "\n",
    "def process_xml_file(file_path):\n",
    "    tree = etree.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Extract Titelzeile\n",
    "    titelzeile_elem = root.xpath('.//titelzeile//p')\n",
    "    titelzeile = titelzeile_elem[0].text.strip() if titelzeile_elem else \"\"\n",
    "\n",
    "    rest_text = extract_text_from_element(root)\n",
    "\n",
    "    rest_text = re.sub(r'\\s+', ' ', rest_text).strip()\n",
    "\n",
    "    return {\"input\": titelzeile, \"output\": rest_text}\n",
    "\n",
    "def process_xml_files_in_directory(directory_paths):\n",
    "    for directory_single_path in directory_paths:\n",
    "        data = []\n",
    "        for filename in os.listdir(directory_single_path):\n",
    "            if filename.endswith('.xml'):\n",
    "                file_path = os.path.join(directory_single_path, filename)\n",
    "                result = process_xml_file(file_path)\n",
    "                data.append(result)\n",
    "        with open(directory_single_path + \".json\", 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(data, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f177d3e-9079-4a93-9fbe-808cf6ad7f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = [\"data/bund\"]\n",
    "\n",
    "process_xml_files_in_directory(directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "131fbbf6-fa52-4278-8454-99a94946625b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "459acd22-b909-48bc-8526-ba46bc7cb36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evtl später JsonL files benutzen um Daten zu speichern & zu laden\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "json_paths = ['data/bund.json']\n",
    "\n",
    "dataset = load_dataset('json', data_files=json_paths, split='train')\n",
    "\n",
    "# 90 - 10 Split\n",
    "train_test_split = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49b25747-5942-4eb5-a6c8-5b88dbfeeced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatierungsfunktion unserer Daten\n",
    "\n",
    "def formatting_func_1(example):\n",
    "    text = f\"### Question: {example['input']}\\n ### Answer: {example['output']}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6454187a-3dc7-48cf-b6fc-fe60d2400cd9",
   "metadata": {},
   "source": [
    "## Huggingface Login & Modeldownload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b04f9db-528c-4b8b-8280-6de5cb28b6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ps2024/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login, logout\n",
    "\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e4013fc-92e6-4edb-b456-dc62a8a83010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6dad5dee75f4dea932f4a2227228958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Es wird eine quantisierte Version von mistral benutzt -> kleinere Größe -> deswegen Genauigkeitsverlust\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3f9722-b4b7-4db3-b1c4-15f00218f2f5",
   "metadata": {},
   "source": [
    "## Tokenerstellung + Anpassung (Training & Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1ca33ff-741e-431f-9150-0c9e09c69139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Tokenerstellung, Padding führt zu geringerem Memory loss beim Training, eos = end of sentence\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_and_tokenize_prompt(prompt):\n",
    "    return tokenizer(formatting_func_1(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5546a122-bbbc-48ae-a79f-4d170e08a5bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenized_train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241m.\u001b[39mmap(generate_and_tokenize_prompt)\n\u001b[1;32m      2\u001b[0m tokenized_val_dataset \u001b[38;5;241m=\u001b[39m test_dataset\u001b[38;5;241m.\u001b[39mmap(generate_and_tokenize_prompt)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = test_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0fe15df-18ae-480d-8b3c-acddc70af1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2616\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hier geht es darum eine maximale länge der Tokenized inputs zu bestimmen\n",
    "# Evtl. zu kurze verlängern und zu lange kürzen\n",
    "# Vielleicht auch viel zu lange rausschmeißen\n",
    "# Am besten nicht in mitte von Sätzen kürzen\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
    "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
    "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
    "    print(len(lengths))\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
    "    plt.xlabel('Length of input_ids')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Lengths of input_ids')\n",
    "    plt.show()\n",
    "\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68614082-7155-47cf-b9ea-1f34b68ad303",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 8000 # kann verändert werden\n",
    "\n",
    "def generate_and_tokenize_prompt2(prompt):\n",
    "    result = tokenizer(\n",
    "        formatting_func_1(prompt),\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1996c3cd-2cbb-4a18-830d-098987678d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ec96e140aa4fcea1bbd39cce02ceb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2354 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f810059a6d9a462386e545e92dfd1430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/262 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
    "tokenized_val_dataset = test_dataset.map(generate_and_tokenize_prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1db596c1-7070-4a86-8714-8933c15e3b3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 774, 22478, 28747, 2852, 28721, 5300, 28764, 4539, 7352, 1946, 969, 5984, 597, 1105, 1046, 343, 18820, 7246, 22627, 786, 14354, 28719, 950, 28714, 1899, 301, 3021, 1457, 365, 962, 13, 774, 26307, 28747, 524, 6394, 28784, 28734, 28734, 28734, 28770, 28740, 28774, 28783, 28734, 365, 2377, 28705, 28781, 28723, 5355, 270, 28705, 28750, 28734, 28740, 28734, 28734, 28783, 28750, 28782, 28705, 28781, 330, 28828, 28754, 28705, 28740, 28781, 28748, 28734, 28774, 11149, 14084, 25585, 28705, 28784, 28740, 28740, 12709, 28705, 28740, 365, 6903, 28725, 25585, 28705, 28740, 28770, 28770, 365, 6903, 28725, 25585, 28705, 28740, 28782, 28787, 365, 6903, 28725, 25585, 28705, 28750, 28734, 12709, 28705, 28740, 5046, 28733, 28758, 28725, 25585, 28705, 28750, 28740, 12709, 28705, 28740, 5046, 28961, 28733, 28758, 28725, 25585, 28705, 28750, 12709, 28705, 28740, 5046, 28961, 28733, 28758, 28725, 5046, 28733, 28999, 28712, 14798, 28725, 5046, 28733, 28999, 28712, 14798, 28748, 28790, 28796, 28741, 363, 3816, 12669, 1010, 28726, 28777, 393, 28837, 23698, 28725, 28705, 28740, 28740, 28723, 3999, 28705, 28750, 28734, 28734, 28783, 28725, 7393, 28747, 6204, 28723, 28757, 28723, 28705, 28781, 11013, 28705, 28740, 287, 28748, 28734, 28783, 28725, 11149, 14084, 363, 3816, 12669, 5062, 22221, 1105, 1046, 1097, 2450, 1609, 867, 14759, 28733, 28584, 6559, 28725, 28705, 28740, 28750, 28723, 4349, 28705, 28750, 28734, 28734, 28783, 28725, 7393, 28747, 28705, 28784, 7506, 28705, 28750, 28750, 28783, 28748, 28734, 28783, 28725, 11149, 14084, 4562, 28779, 15678, 267, 7891, 4634, 19030, 2852, 28721, 5300, 28764, 4539, 7352, 1946, 969, 5984, 597, 1105, 1046, 343, 18820, 7246, 22627, 786, 14354, 28719, 950, 28714, 1899, 301, 3021, 1457, 365, 962, 28705, 28740, 28723, 2941, 4375, 1522, 634, 7211, 28830, 7850, 12039, 2846, 11149, 14084, 634, 5062, 22221, 1105, 1046, 1097, 2450, 28713, 1609, 867, 14759, 28733, 28584, 6559, 9322, 28705, 28740, 28750, 28723, 4349, 28705, 28750, 28734, 28734, 28783, 387, 28705, 28784, 7506, 28705, 28750, 28750, 28783, 28748, 28734, 28783, 387, 7534, 16460, 16539, 497, 269, 28723, 28705, 28750, 28723, 4823, 7211, 26252, 4613, 1202, 524, 17074, 891, 4375, 1522, 3155, 467, 5786, 28723, 28705, 28740, 2941, 24162, 1375, 2811, 6329, 5431, 7897, 1094, 886, 551, 338, 634, 7211, 28830, 7850, 3021, 1054, 7010, 969, 5984, 1054, 12841, 416, 969, 3446, 2846, 6621, 28705, 28750, 28734, 28734, 28784, 640, 297, 21756, 1054, 14533, 15140, 5431, 1202, 1094, 23466, 969, 634, 14740, 335, 343, 434, 1291, 5431, 3834, 1054, 12841, 416, 969, 3446, 3663, 374, 479, 424, 9322, 28705, 28740, 28750, 28723, 17721, 28705, 28740, 28774, 28787, 28770, 1910, 28765, 634, 18912, 2520, 969, 7890, 335, 343, 434, 1291, 9322, 28705, 28770, 28740, 28723, 17649, 28705, 28750, 28734, 28734, 28770, 325, 28711, 595, 17090, 416, 5046, 1054, 12841, 416, 969, 28731, 842, 28705, 28750, 4823, 7211, 26252, 3103, 13959, 1493, 28705, 28740, 28723, 3999, 28705, 28740, 28774, 28787, 28787, 297, 891, 401, 595, 28729, 2294, 849, 3446, 13020, 20307, 3552, 28725, 3147, 28718, 22068, 412, 640, 1298, 23005, 5174, 297, 3147, 469, 25916, 613, 28723, 382, 28723, 15049, 21084, 8335, 28723, 2941, 401, 595, 28729, 2294, 849, 3350, 5617, 2927, 18820, 819, 338, 25500, 11039, 2835, 2533, 3368, 3347, 274, 7787, 978, 269, 11209, 634, 22924, 1609, 867, 14759, 28733, 28584, 6559, 924, 25077, 28723, 2236, 24355, 1046, 343, 18820, 634, 7211, 28830, 7850, 9322, 28705, 28770, 28734, 28723, 17099, 28705, 28740, 28774, 28787, 28784, 400, 21395, 28707, 1037, 297, 25585, 28705, 28750, 28747, 3018, 28757, 293, 24355, 1046, 343, 28716, 17966, 11765, 1489, 321, 2630, 3866, 4856, 1493, 15678, 28733, 10201, 374, 17633, 308, 283, 335, 343, 18820, 325, 28760, 962, 28731, 9322, 28705, 28750, 28770, 28723, 21341, 28705, 28740, 28774, 28784, 28740, 387, 325, 7118, 1074, 1231, 1061, 1609, 867, 14759, 28733, 28584, 6559, 318, 28723, 28705, 28740, 28782, 28782, 28731, 387, 640, 1457, 5148, 269, 1234, 28721, 5300, 28764, 8331, 7765, 14239, 2520, 292, 269, 14740, 335, 343, 434, 28830, 2383, 24033, 28705, 28770, 6684, 1493, 22924, 2711, 13659, 5431, 1202, 5830, 2450, 969, 6204, 496, 308, 3744, 28733, 26429, 18241, 5523, 4416, 9959, 6283, 401, 595, 28729, 2294, 21417, 325, 28765, 595, 28729, 2294, 849, 2711, 13659, 387, 401, 28796, 28714, 28777, 28731, 9322, 28705, 28783, 28723, 19314, 28705, 28740, 28774, 28774, 28782, 325, 28777, 28790, 4919, 28723, 318, 28723, 28705, 28781, 28782, 28750, 28731, 1166, 2450, 3811, 2846, 5062, 1609, 867, 14759, 28733, 28584, 6559, 332, 28708, 28723, 1202, 401, 595, 28729, 2294, 849, 3147, 469, 25916, 28748, 28584, 6559, 2835, 1094, 303, 2304, 634, 6204, 496, 308, 7246, 1298, 18201, 325, 28741, 28834, 28754, 28731, 1200, 3021, 1202, 4856, 25585, 28705, 28740, 28740, 12709, 28723, 28705, 28740, 401, 28796, 28714, 28777, 6103, 28705, 28740, 28723, 17649, 28705, 28740, 28774, 28774, 28784, 1202, 5617, 1493, 22924, 6175, 978, 269, 11209, 19495, 12669, 269, 24355, 1046, 343, 28716, 17966, 28711, 6813, 5431, 3080, 269, 28723, 816, 1685, 15176, 400, 21395, 28707, 1037, 297, 25585, 28705, 28740, 28740, 12709, 28723, 28705, 28750, 401, 28796, 28714, 28777, 28747, 3018, 28732, 28750, 28731, 18034, 1202, 1808, 12709, 6124, 28705, 28740, 1234, 6024, 28880, 1210, 26231, 21084, 27598, 12694, 1210, 1202, 4505, 6103, 10740, 28720, 13518, 891, 5830, 2450, 969, 891, 401, 595, 28729, 2294, 849, 4128, 28880, 5141, 7246, 597, 1105, 1046, 343, 18820, 7246, 20017, 1822, 4540, 640, 14740, 335, 343, 434, 28830, 490, 297, 891, 2218, 769, 4544, 319, 1807, 8331, 401, 489, 969, 15298, 28723, 6177, 12694, 1210, 7724, 1079, 1202, 16936, 14740, 335, 343, 434, 28830, 490, 446, 6749, 632, 326, 14239, 2520, 292, 269, 640, 1234, 28721, 5300, 28764, 8331, 14740, 335, 343, 434, 28830, 490, 28723, 7029, 1298, 5862, 891, 401, 595, 28729, 2294, 21417, 28725, 3446, 17080, 26231, 21084, 27598, 14740, 335, 343, 434, 28830, 490, 534, 28764, 26628, 5715, 9721, 28725, 8012, 15046, 9654, 28728, 266, 521, 537, 7736, 28707, 28723, 3850, 28835, 28705, 28781, 6684, 15298, 366, 22924, 2711, 13659, 6156, 3147, 28718, 556, 15065, 891, 401, 595, 28729, 2294, 21417, 325, 28765, 28796, 28714, 17167, 28731, 634, 22924, 1609, 867, 14759, 28733, 28584, 6559, 9322, 28705, 28750, 28782, 28723, 4349, 28705, 28750, 28734, 28734, 28750, 325, 28777, 28790, 4919, 28723, 318, 28723, 28705, 28750, 28770, 28787, 28731, 3350, 2846, 19191, 28834, 2383, 891, 401, 595, 28729, 2294, 849, 3147, 469, 25916, 613, 28723, 382, 28723, 387, 330, 28834, 28754, 387, 28725, 317, 1126, 338, 25197, 891, 19187, 19495, 12669, 269, 24355, 1046, 343, 28716, 17966, 28711, 6813, 3021, 1202, 401, 595, 28729, 2294, 849, 650, 309, 3347, 28716, 2015, 269, 387, 330, 28834, 28754, 387, 5431, 434, 5786, 28725, 1202, 17043, 1127, 2176, 1457, 24333, 3018, 782, 7702, 270, 16988, 19348, 3571, 1767, 28835, 467, 786, 28723, 25585, 28705, 28740, 28734, 401, 28796, 28714, 17167, 543, 329, 299, 28747, 3018, 28732, 28740, 28731, 18034, 1202, 26231, 21084, 27598, 891, 401, 595, 28729, 2294, 849, 1609, 867, 14759, 640, 891, 7140, 270, 16988, 19348, 3571, 1767, 12694, 1210, 1202, 4505, 663, 3347, 597, 1105, 1046, 343, 18820, 7246, 20017, 1822, 4540, 640, 14740, 335, 343, 434, 28830, 490, 297, 891, 2218, 769, 4544, 319, 1807, 8331, 401, 489, 969, 15298, 28723, 7075, 6822, 27362, 16768, 5617, 891, 413, 4138, 479, 969, 26231, 21084, 326, 360, 396, 28764, 12841, 8331, 28723, 7029, 1298, 5862, 891, 401, 595, 28729, 2294, 849, 1609, 867, 14759, 640, 891, 7140, 270, 16988, 19348, 3571, 1767, 28725, 14740, 335, 343, 434, 28830, 490, 534, 28764, 26628, 5715, 9721, 28725, 8012, 15046, 521, 537, 7736, 28707, 28723, 3850, 28835, 28705, 28782, 2941, 7140, 270, 16988, 19348, 3571, 1767, 3350, 6331, 3951, 28723, 28705, 28740, 634, 11461, 299, 12189, 6156, 12514, 27358, 13005, 5523, 4416, 9959, 6283, 6004, 16598, 4540, 640, 6662, 9590, 28716, 6571, 276, 10891, 1210, 325, 28753, 3467, 338, 28749, 28733, 28779, 28719, 28727, 28777, 28731, 9322, 28705, 28750, 28781, 28723, 4074, 28705, 28750, 28734, 28734, 28781, 325, 28777, 28790, 4919, 28723, 318, 28723, 28705, 28770, 28782, 28734, 28731, 640, 891, 9654, 13581, 1234, 12532, 12224, 22924, 343, 556, 15065, 5431, 1457, 4849, 769, 22509, 640, 1202, 2927, 28830, 23016, 263, 969, 891, 7140, 270, 16988, 19348, 3571, 1767, 9322, 28705, 28740, 28770, 28723, 17721, 28705, 28750, 28734, 28734, 28781, 325, 28777, 28790, 4919, 28723, 318, 28723, 28705, 28781, 28734, 28787, 28731, 297, 3834, 420, 1757, 28769, 3205, 16539, 391, 1807, 28725, 1202, 837, 28705, 28781, 28723, 17649, 28705, 28750, 28734, 28734, 28782, 297, 2846, 6322, 1190, 5724, 13262, 12182, 5786, 3350, 28723, 7029, 5062, 1609, 867, 14759, 28733, 28584, 6559, 295, 14022, 26477, 8477, 11461, 20250, 28713, 2237, 546, 28723, 7496, 3951, 28723, 28705, 28770, 12709, 28723, 28705, 28770, 17221, 28749, 28733, 28779, 28719, 28727, 28777, 17726, 25585, 28705, 28740, 28734, 401, 28796, 28714, 17167, 837, 12996, 4856, 891, 2927, 28729, 9744, 969, 634, 11461, 299, 12189, 837, 28705, 28770, 28734, 28723, 4074, 28705, 28750, 28734, 28734, 28781, 2505, 28028, 524, 2869, 28723, 560, 5984, 13217, 1493, 5062, 1609, 867, 14759, 28733, 28584, 6559, 640, 1457, 11461, 314, 8024, 1084, 282, 5308, 891, 7140, 270, 16988, 19348, 3571, 1767, 534, 12386, 9195, 12224, 318, 16151, 6571, 10888, 262, 1822, 969, 9322, 17721, 28705, 28750, 28734, 28734, 28781, 3103, 297, 25585, 28705, 28750, 12709, 28723, 28705, 28740, 10952, 4737, 28723, 28705, 28740, 332, 28708, 28723, 1489, 321, 2630, 28725, 3018, 28715, 489, 3446, 1202, 15037, 28723, 25585, 28705, 28740, 10665, 539, 28645, 351, 5098, 1105, 1685, 17190, 1202, 12039, 28727, 2017, 28707, 326, 3446, 4748, 5617, 891, 330, 28834, 28754, 1094, 23466, 969, 1300, 8331, 14740, 335, 343, 434, 28830, 490, 5617, 1457, 28319, 11461, 20250, 269, 2835, 10207, 6283, 8393, 4334, 1793, 1429, 28706, 262, 28726, 444, 6446, 14771, 2941, 11461, 20250, 28713, 2237, 546, 891, 420, 1757, 28769, 9031, 396, 2452, 13920, 416, 9322, 5062, 15298, 343, 28830, 23016, 930, 640, 1202, 420, 1757, 28769, 297, 1202, 461, 13659, 3580, 365, 950, 9403, 424, 3205, 28722, 2780, 6257, 28723, 394, 2865, 1202, 401, 595, 28729, 2294, 849, 3147, 469, 25916, 613, 28723, 382, 28723, 387, 330, 28834, 28754, 387, 11039, 1202, 7140, 270, 16988, 19348, 3571, 1767, 640, 11039, 1202, 347, 4784, 357, 424, 420, 1757, 28769, 10744, 640, 6822, 11838, 335, 5141, 16436, 28723, 28705, 28784, 21761, 15152, 28711, 634, 4096, 411, 28705, 28740, 28774, 28774, 28784, 275, 416, 14478, 1202, 1010, 10943, 490, 537, 262, 640, 17080, 1298, 18201, 28728, 1909, 22436, 1202, 2218, 769, 309, 3347, 2678, 301, 4540, 634, 365, 962, 640, 891, 19467, 1234, 28721, 5300, 28764, 8331, 14740, 335, 343, 434, 28830, 490, 3021, 2846, 24355, 1046, 343, 28716, 17966, 11765, 634, 7211, 28830, 7850, 396, 28723, 6662, 886, 267, 631, 292, 1493, 6103, 28705, 28740, 28723, 4349, 28705, 28750, 28734, 28734, 28784, 297, 524, 2869, 625, 267, 1210, 269, 14740, 335, 343, 18820, 3446, 1457, 6204, 496, 308, 7246, 384, 19239, 891, 393, 28830, 2520, 325, 28728, 300, 28705, 28740, 28750, 28723, 17721, 28705, 28750, 28734, 28734, 28784, 387, 5046, 28733, 28758, 28731, 4856, 25585, 28705, 28750, 28734, 12709, 28723, 28705, 28740, 5046, 28733, 28758, 613, 28790, 28719, 28723, 25585, 28705, 28750, 28740, 12709, 28723, 28705, 28740, 634, 14740, 335, 343, 434, 1291, 6156, 19265, 291, 24422, 891, 26231, 21084, 27598, 891, 393, 28830, 2520, 297, 1457, 5046, 28733, 28758, 640, 6156, 2678, 301, 969, 634, 13156, 4146, 23766, 267, 18201, 325, 28728, 300, 28705, 28740, 28750, 28723, 17721, 28705, 28750, 28734, 28734, 28784, 387, 5046, 28961, 28733, 28758, 28731, 686, 7010, 424, 1202, 365, 950, 9403, 424, 396, 1457, 7211, 26252, 3834, 4096, 638, 9816, 27483, 969, 297, 18137, 265, 1808, 28705, 28770, 28782, 363, 28769, 891, 3217, 1449, 343, 28721, 11270, 969, 3446, 1457, 3217, 270, 4074, 28705, 28750, 28734, 28734, 28784, 28723, 6684, 1609, 20628, 269, 9322, 28705, 28740, 28787, 28723, 3999, 28705, 28750, 28734, 28734, 28787, 640, 9322, 28705, 28740, 28787, 28723, 17570, 28705, 28750, 28734, 28734, 28787, 290, 22986, 891, 7211, 26252, 12039, 20173, 891, 365, 950, 9403, 1210, 3018, 28722, 2355, 1202, 393, 7397, 11649, 634, 365, 962, 503, 6621, 28705, 28750, 28734, 28734, 28784, 325, 21650, 11042, 387, 17721, 28731, 1202, 396, 14084, 2762, 25829, 3580, 7496, 27483, 969, 634, 28705, 28740, 28770, 28723, 3217, 1449, 490, 6019, 1074, 28835, 1234, 5880, 1727, 385, 319, 1807, 416, 28723, 28705, 28787, 6684, 11366, 7211, 465, 4613, 891, 7211, 26252, 6578, 1739, 490, 22126, 15298, 343, 5880, 3606, 28723, 2236, 2927, 27997, 634, 1298, 18201, 12776, 1046, 1429, 28706, 262, 28726, 12125, 1202, 24162, 1375, 396, 15135, 819, 3744, 1808, 14740, 335, 343, 434, 1787, 343, 2625, 28714, 4540, 891, 365, 950, 9403, 1210, 2367, 1493, 1471, 7945, 1097, 12583, 28725, 1202, 6227, 6103, 12709, 338, 25500, 12531, 14740, 335, 343, 434, 1291, 19549, 1210, 28725, 2367, 18912, 2520, 6571, 343, 18820, 9322, 28705, 28784, 28723, 21341, 28705, 28750, 28734, 28734, 28783, 1202, 1094, 23466, 969, 634, 13217, 1493, 1471, 7945, 1097, 12583, 640, 891, 14740, 335, 18232, 1126, 6655, 16765, 393, 28830, 2520, 325, 28738, 28715, 28758, 28731, 15781, 9195, 12224, 14740, 335, 343, 434, 1291, 3446, 18912, 28712, 3903, 17190, 640, 18912, 28712, 14798, 396, 2396, 25038, 1074, 28729, 2294, 21417, 9322, 28705, 28770, 28734, 28723, 17721, 28705, 28750, 28734, 28734, 28784, 325, 10639, 28733, 28999, 28712, 14798, 28748, 28738, 28715, 28758, 28731, 3021, 2846, 13217, 5837, 4588, 19495, 12669, 28706, 24355, 1046, 343, 28716, 17966, 11765, 534, 1493, 28705, 28740, 28723, 17649, 28705, 28750, 28734, 28734, 28787, 28723, 4823, 7211, 26252, 528, 501, 28725, 6103, 507, 374, 4505, 6103, 28705, 28770, 28740, 28723, 19314, 28705, 28750, 28734, 28734, 28784, 427, 1375, 891, 365, 962, 640, 891, 5046, 1054, 12841, 416, 969, 3446, 6578, 24355, 1046, 343, 28716, 17966, 11765, 4128, 28880, 5141, 416, 9514, 23668, 28723, 21144, 330, 1231, 12548, 969, 6331, 1457, 5046, 28733, 28758, 3019, 28706, 6227, 25183, 490, 28722, 16436, 640, 3446, 6578, 24355, 1046, 343, 28716, 17966, 11765, 12694, 424, 4856, 23092, 25585, 28705, 28750, 6227, 28713, 304, 12343, 28723, 2941, 1429, 18820, 8518, 330, 2152, 450, 435, 28716, 1127, 7475, 1202, 1234, 28721, 5300, 28764, 8331, 640, 14239, 2520, 292, 269, 14740, 335, 343, 434, 28830, 490, 297, 22627, 786, 28725, 6227, 11554, 1202, 15920, 13659, 8331, 28723, 4823, 5046, 28733, 28758, 18578, 945, 262, 1234, 28721, 5300, 28764, 2341, 7765, 14239, 2520, 2520, 14740, 335, 343, 18820, 6103, 365, 962, 28723, 17457, 891, 22627, 786, 14354, 28719, 950, 28714, 1899, 301, 1021, 1643, 1037, 3866, 3205, 3834, 26796, 539, 11647, 6571, 375, 267, 450, 613, 28735, 28715, 28723, 27197, 663, 269, 1298, 5862, 886, 267, 338, 969, 634, 15678, 283, 1105, 1046, 1097, 2450, 28713, 28723, 7496, 394, 641, 9197, 891, 10665, 13659, 7246, 1094, 556, 15065, 891, 420, 1807, 969, 634, 365, 962, 3021, 2846, 24355, 1046, 343, 28716, 17966, 11765, 891, 24162, 1375, 3021, 19878, 634, 401, 28796, 28714, 17167, 3019, 28706, 891, 365, 962, 317, 1126, 338, 25197, 891, 5148, 269, 1234, 28721, 5300, 28764, 8331, 14740, 335, 343, 434, 28830, 2383, 13125, 446, 2869, 765, 6283, 597, 1105, 1046, 343, 18820, 18241, 22627, 786, 14354, 1127, 3021, 2846, 24355, 1046, 343, 28716, 17966, 11765, 1094, 23466, 969, 17241, 16436, 28723, 1091, 262, 1094, 886, 551, 338, 1234, 490, 1105, 3866, 5370, 3021, 19878, 891, 7496, 28727, 7316, 969, 891, 11838, 335, 7246, 11098, 269, 640, 891, 15781, 9195, 12224, 318, 16151, 6571, 10888, 262, 1822, 969, 28723, 28705, 28783, 4823, 7211, 26252, 4613, 3155, 895, 3903, 24853, 18820, 28707, 28725, 1202, 365, 950, 9403, 424, 3155, 1429, 324, 14084, 269, 28725, 396, 19467, 28705, 28770, 28723, 28770, 28781, 28774, 28725, 28740, 28781, 14869, 16147, 532, 435, 14370, 1054, 1126, 269, 297, 18137, 265, 1808, 27373, 1133, 15200, 28720, 2060, 1210, 5431, 1493, 6827, 278, 26404, 819, 6124, 13959, 1493, 28705, 28750, 28723, 19314, 28705, 28750, 28734, 28734, 28784, 3155, 686, 912, 2004, 28723, 28705, 28774, 2941, 365, 950, 9403, 424, 4613, 24853, 18820, 28707, 28725, 1202, 7211, 465, 534, 13581, 769, 10555, 28723, 21144, 420, 1807, 969, 7765, 1094, 23466, 1822, 11231, 634, 365, 962, 5431, 1457, 28705, 28770, 28740, 28723, 17721, 28705, 28750, 28734, 28734, 28784, 295, 1380, 381, 18578, 6227, 2970, 19166, 28723, 25585, 28705, 28750, 634, 24355, 1046, 343, 434, 1291, 347, 262, 6019, 424, 22712, 765, 3751, 2927, 769, 278, 969, 3021, 1457, 365, 962, 28725, 1966, 22502, 3834, 10207, 3751, 28723, 2941, 7211, 1899, 301, 1234, 28722, 4550, 2835, 14239, 2520, 292, 269, 14740, 335, 343, 18820, 5370, 1457, 6956, 3347, 28725, 891, 1457, 365, 962, 28353, 21591, 326, 28705, 5901, 397, 28723, 28705, 28740, 28734, 2941, 11975, 4138, 276, 3039, 17738, 1202, 7211, 465, 534, 16539, 497, 269, 28723, 6684, 891, 9322, 5062, 22221, 1105, 1046, 1097, 2450, 686, 786, 301, 489, 12224, 4375, 1522, 1429, 5880, 3606, 891, 7211, 26252, 6578, 1739, 490, 22126, 15298, 28723, 2941, 365, 950, 9403, 424, 24853, 18820, 28707, 28725, 1202, 4375, 1522, 16460, 13581, 769, 10555, 28723, 28705, 28740, 28740, 2941, 686, 353, 28830, 819, 3580, 4375, 1522, 3103, 521, 28726, 25906, 299, 28723, 28705, 28740, 28750, 315, 28723, 4823, 4375, 5662, 276, 18820, 634, 7211, 28830, 7850, 2855, 283, 28722, 891, 7352, 1946, 969, 28723, 318, 7443, 279, 891, 7211, 26252, 6578, 1739, 490, 22126, 297, 1457, 11975, 4138, 276, 3039, 435, 3173, 891, 597, 1105, 1046, 343, 18820, 7246, 22627, 786, 14354, 28719, 950, 28714, 1899, 301, 5370, 3021, 1202, 7496, 28727, 7316, 969, 11838, 335, 18241, 11098, 269, 11298, 3021, 1202, 318, 16151, 6571, 10888, 262, 1822, 969, 640, 23929, 3021, 12229, 24908, 22347, 5901, 3580, 4589, 279, 490, 2383, 303, 18919, 7165, 11270, 3903, 4613, 325, 28728, 1727, 28723, 341, 28723, 399, 22694, 2063, 365, 2377, 28705, 28740, 28740, 28723, 14313, 28705, 28750, 28734, 28734, 28782, 387, 28705, 28781, 330, 28828, 28754, 28705, 28770, 28740, 28782, 28748, 28734, 28781, 387, 3155, 315, 28705, 28781, 891, 1778, 9744, 28706, 28725, 365, 6130, 28705, 28740, 28740, 28781, 28725, 28705, 28770, 28770, 28750, 28745, 15298, 15176, 28705, 28740, 28782, 28723, 18035, 28705, 28750, 28734, 28734, 28784, 387, 28705, 28781, 330, 28828, 28754, 28705, 28787, 28770, 28748, 28734, 28782, 387, 399, 28711, 28723, 28705, 28740, 28783, 28725, 10502, 1054, 3402, 25585, 28705, 28782, 28782, 28740, 25108, 28723, 28705, 28784, 28770, 327, 23963, 28741, 1054, 3402, 28705, 28750, 28734, 28734, 28750, 25585, 28705, 28782, 28782, 28740, 25108, 28723, 28705, 28750, 28745, 28705, 28774, 28723, 3999, 28705, 28750, 28734, 28734, 28783, 387, 28705, 28781, 330, 28828, 28754, 28705, 28740, 28734, 28781, 28748, 28734, 28787, 387, 399, 28711, 28723, 28705, 28784, 28781, 28725, 10502, 5046, 28777, 25585, 28705, 28740, 25108, 28723, 28705, 28781, 28770, 327, 23963, 28741, 1054, 3402, 28705, 28750, 28734, 28734, 28750, 25585, 28705, 28750, 28782, 28774, 25108, 28723, 28705, 28740, 28731, 1200, 4613, 1234, 11733, 4375, 1522, 297, 686, 353, 28830, 819, 4886, 816, 864, 3021, 7897, 1094, 886, 551, 338, 3642, 891, 1429, 18820, 7246, 22627, 786, 14354, 28719, 950, 28714, 1899, 301, 297, 25585, 28705, 28750, 634, 24355, 1046, 343, 434, 1291, 15049, 28712, 5300, 2254, 28723, 7029, 1234, 28721, 15046, 3866, 21037, 3642, 11366, 4375, 5662, 28726, 25906, 969, 28725, 1202, 3021, 1202, 21930, 19532, 4589, 279, 490, 2383, 303, 18919, 6227, 11152, 13262, 28706, 407, 28723, 4823, 7211, 26252, 4613, 16936, 26231, 28712, 5300, 28729, 969, 11366, 4375, 1522, 297, 891, 290, 9744, 7246, 2927, 2625, 13005, 6876, 1493, 5355, 270, 3642, 2922, 7362, 3744, 1489, 5291, 8335, 28723, 28705, 28740, 28770, 3717, 28723, 6004, 1094, 886, 551, 338, 634, 7211, 28830, 7850, 3021, 1054, 7010, 969, 5984, 1054, 12841, 416, 969, 4856, 1493, 5046, 1054, 12841, 416, 969, 3446, 2846, 12117, 2341, 28768, 8775, 28705, 28750, 28734, 28734, 28784, 1234, 28721, 15046, 3866, 6227, 3642, 891, 597, 1105, 1046, 343, 18820, 7246, 22627, 786, 14354, 28719, 950, 28714, 1899, 301, 28725, 1202, 8477, 262, 11039, 420, 5772, 1793, 891, 4375, 1522, 3103, 28723, 28705, 28740, 28781, 4823, 5046, 1054, 12841, 416, 969, 9322, 28705, 28740, 28750, 28723, 17721, 28705, 28740, 28774, 28787, 28770, 1910, 28765, 634, 18912, 2520, 969, 7890, 335, 343, 434, 1291, 9322, 28705, 28770, 28740, 28723, 17649, 28705, 28750, 28734, 28734, 28770, 28725, 3021, 1457, 891, 7211, 26252, 17507, 1054, 7010, 6571, 509, 28720, 551, 338, 341, 11270, 3903, 28725, 1300, 299, 3021, 6578, 24355, 1046, 343, 28716, 17966, 11765, 22712, 1094, 23466, 969, 28723, 384, 20205, 15557, 1037, 281, 912, 262, 2453, 540, 28725, 818, 1202, 1429, 18820, 8518, 20017, 1822, 969, 2835, 26796, 539, 11647, 6571, 375, 267, 450, 613, 28735, 28715, 28723, 27197, 663, 269, 1298, 5862, 886, 267, 338, 969, 634, 5355, 1449, 3642, 28764, 1390, 2383, 7765, 818, 1202, 7211, 1899, 301, 3021, 19878, 891, 534, 1493, 12552, 28705, 28740, 28774, 28774, 28784, 6331, 28721, 7605, 326, 25066, 1210, 21915, 15491, 969, 634, 24355, 1046, 343, 28716, 17966, 28711, 815, 274, 7014, 1094, 23466, 969, 634, 365, 962, 640, 11366, 1054, 381, 270, 3903, 283, 335, 343, 434, 28830, 490, 297, 19320, 2218, 769, 309, 3347, 401, 489, 969, 4319, 328, 490, 5765, 4784, 554, 7262, 2927, 434, 1787, 28830, 2520, 969, 2835, 10207, 3751, 22627, 786, 14354, 1127, 3155, 1429, 2453, 540, 3103, 28723, 28705, 28740, 28782, 28705, 28740, 28723, 4823, 1094, 886, 551, 338, 634, 7211, 28830, 7850, 3103, 521, 28726, 25906, 299, 28725, 24718, 1037, 3866, 387, 7220, 1234, 22347, 297, 1457, 11975, 4138, 276, 3039, 19582, 15491, 28707, 4613, 387, 5617, 891, 22627, 786, 14354, 28719, 950, 28714, 1899, 301, 297, 25585, 28705, 28750, 634, 24355, 1046, 343, 434, 1291, 4856, 7220, 6876, 3205, 3834, 26796, 539, 11647, 6571, 375, 267, 450, 613, 28735, 28715, 28723, 891, 27197, 663, 269, 1298, 5862, 886, 267, 338, 969, 1021, 1807, 28723, 21988, 1234, 17284, 2453, 18093, 6227, 1457, 5046, 1054, 12841, 416, 969, 28725, 3021, 1457, 8477, 262, 891, 7211, 26252, 6578, 1739, 490, 22126, 341, 11270, 3903, 28723, 2941, 14740, 335, 5893, 969, 634, 1010, 10943, 490, 1891, 936, 28722, 755, 21037, 837, 28705, 28740, 28723, 17649, 28705, 28740, 28774, 28774, 28784, 640, 23929, 6876, 560, 28729, 2869, 4110, 1210, 634, 5046, 1054, 12841, 416, 969, 1910, 28765, 634, 18912, 2520, 969, 7890, 335, 343, 434, 1291, 9322, 28705, 28770, 28740, 28723, 17649, 28705, 28750, 28734, 28734, 28770, 28723, 28705, 28740, 28784, 264, 28731, 17457, 891, 22627, 786, 14354, 28719, 950, 28714, 1899, 301, 1021, 1807, 1037, 3866, 6294, 269, 16768, 6103, 10740, 28720, 13518, 634, 24355, 1046, 343, 18820, 819, 338, 25500, 274, 2367, 7026, 11838, 335, 5141, 915, 12224, 1010, 10943, 490, 537, 503, 12552, 28705, 28740, 28774, 28787, 28784, 3205, 3834, 26796, 539, 11647, 6571, 375, 267, 450, 613, 28735, 28715, 28723, 27197, 663, 269, 1298, 5862, 886, 267, 338, 969, 634, 5355, 1449, 28723, 28705, 28740, 28787, 264, 28708, 28731, 7496, 13815, 1298, 5862, 886, 267, 338, 969, 10744, 5617, 20248, 1779, 631, 2520, 14740, 335, 5141, 16436, 12307, 634, 1010, 10943, 490, 1891, 6103, 10740, 28720, 13518, 634, 2927, 18820, 819, 338, 25500, 274, 2927, 769, 278, 969, 1252, 28714, 1899, 12228, 7220, 1202, 6876, 5715, 28721, 4539, 2835, 579, 2652, 25542, 26796, 539, 11647, 6571, 375, 893, 269, 3642, 28764, 1390, 2383, 28723, 7029, 19549, 28707, 5617, 7026, 394, 641, 9197, 891, 14740, 335, 5141, 16436, 12307, 634, 1010, 10943, 490, 1891, 281, 22960, 28725, 8595, 1202, 297, 22627, 786, 2652, 18902, 269, 14740, 335, 343, 434, 28830, 490, 534, 21756, 10740, 28720, 13518, 7475, 11039, 765, 3143, 396, 28764, 12841, 8331, 6822, 28723, 21988, 7352, 1946, 6571, 1376, 301, 275, 27522, 891, 5355, 270, 3642, 1778, 9744, 269, 634, 2927, 434, 581, 596, 2452, 329, 12189, 15298, 15176, 3021, 22627, 786, 14354, 28719, 950, 28714, 1899, 12228, 396, 28725, 1202, 6876, 1493, 560, 28729, 2869, 4110, 1210, 891, 21494, 28715, 267, 18201, 267, 674, 6103, 28705, 28740, 28723, 17649, 28705, 28750, 28734, 28734, 28750, 1429, 28706, 262, 28726, 444, 13348, 6822, 325, 820, 9778, 28723, 365, 2377, 28705, 28740, 28781, 28723, 19314, 28705, 28750, 28734, 28734, 28782, 387, 28705, 28781, 330, 28828, 28754, 28705, 28782, 28770, 28784, 28748, 28734, 28781, 387, 399, 28711, 28723, 28705, 28750, 28781, 285, 28722, 2063, 365, 6130, 28705, 28740, 28740, 28784, 28725, 28705, 28770, 28750, 28784, 28745, 28705, 28740, 28783, 28723, 3999, 28705, 28750, 28734, 28734, 28787, 387, 28705, 28781, 330, 28828, 28754, 28705, 28784, 28782, 28750, 28748, 28734, 28782, 387, 399, 28711, 28723, 28705, 28750, 28774, 285, 28722, 2063, 365, 6130, 28705, 28740, 28750, 28750, 28725, 28705, 28787, 28781, 28745, 268, 28723, 15298, 15176, 28705, 28750, 28750, 28723, 3999, 28705, 28750, 28734, 28734, 28774, 387, 28705, 28781, 330, 11145, 28705, 28740, 28781, 28748, 28734, 28783, 387, 399, 28711, 28723, 28705, 28784, 28781, 290, 28727, 28759, 28725, 10502, 6382, 28712, 28790, 28777, 28705, 28740, 28774, 28787, 28750, 25585, 28705, 28774, 28774, 413, 288, 551, 587, 11911, 25108, 28723, 28705, 28770, 28783, 327, 23963, 28741, 5046, 28777, 25585, 28705, 28770, 22627, 786, 14354, 1127, 3021, 14740, 335, 343, 18820, 25108, 28723, 28705, 28781, 28740, 2753, 28705, 28750, 28784, 28723, 3628, 28705, 28750, 28734, 28734, 28774, 387, 28705, 28781, 330, 28828, 28754, 28705, 28750, 28783, 28782, 28748, 28734, 28783, 387, 399, 28711, 28723, 28705, 28781, 28783, 285, 28723, 290, 28727, 28759, 28725, 10502, 5046, 28777, 25585, 28705, 28770, 25108, 28723, 28705, 28781, 28782, 327, 23963, 28741, 5046, 28777, 25585, 28705, 28770, 25108, 28723, 28705, 28770, 28750, 28745, 28705, 28740, 28783, 28723, 4349, 28705, 28750, 28734, 28734, 28774, 387, 28705, 28781, 330, 28828, 28754, 28705, 28782, 28740, 28781, 28748, 28734, 28783, 387, 399, 28711, 28723, 28705, 28740, 28787, 285, 28722, 2063, 10502, 5046, 28777, 25585, 28705, 28740, 22627, 786, 14354, 1127, 3021, 14740, 335, 343, 18820, 25108, 28723, 28705, 28787, 28734, 327, 23963, 28741, 5046, 28777, 25585, 28705, 28770, 22627, 786, 14354, 1127, 3021, 14740, 335, 343, 18820, 25108, 28723, 28705, 28781, 28770, 1143, 842, 28705, 28740, 28783, 287, 28726, 28731, 4823, 837, 28705, 28770, 28734, 28723, 17099, 28705, 28740, 28774, 28787, 28784, 2367, 1493, 5062, 1609, 867, 14759, 28733, 28584, 6559, 15781, 9195, 1860, 24355, 1046, 343, 18820, 3103, 1493, 28711, 595, 396, 2625, 891, 27197, 663, 269, 5355, 1449, 26429, 886, 267, 338, 969, 3155, 347, 324, 14084, 269, 28723, 2941, 2927, 769, 278, 969, 1252, 28714, 1899, 301, 297, 25585, 28705, 28750, 634, 24355, 1046, 343, 434, 1291, 3103, 6294, 269, 16768, 28705, 1726, 783, 22572, 3744, 3834, 26796, 539, 11647, 6571, 375, 267, 450, 28725, 478, 309, 4748, 3021, 1202, 285, 595, 3744, 317, 1126, 338, 15135, 28721, 3347, 14740, 335, 343, 434, 28830, 490, 1429, 769, 392, 640, 891, 1010, 10943, 490, 537, 6103, 10740, 28720, 13518, 634, 2927, 18820, 819, 338, 25500, 274, 6331, 11733, 20977, 7097, 297, 891, 320, 28715, 28758, 11838, 335, 5141, 16436, 1496, 28723, 17457, 25585, 28705, 28750, 634, 24355, 1046, 343, 434, 1291, 1021, 1807, 1037, 3866, 3205, 3834, 2514, 3751, 22627, 786, 14354, 28719, 950, 28714, 1899, 301, 28723, 4823, 5355, 270, 4613, 20248, 1779, 631, 18773, 7211, 1899, 12228, 13125, 5236, 28707, 2835, 26796, 539, 11647, 6571, 375, 893, 269, 26227, 28661, 325, 28728, 1727, 28723, 686, 28760, 28705, 28750, 28784, 28723, 4074, 28705, 28750, 28734, 28734, 28740, 387, 28705, 28781, 330, 28828, 28754, 28705, 28782, 28781, 28781, 28748, 28734, 28734, 387, 365, 6130, 28705, 28774, 28774, 28725, 28705, 28740, 28750, 28734, 28745, 28705, 28740, 28774, 28723, 18035, 28705, 28750, 28734, 28734, 28770, 387, 28705, 28781, 330, 28828, 28754, 28705, 28770, 28770, 28740, 28748, 28734, 28750, 387, 365, 6130, 28705, 28740, 28734, 28782, 28725, 28705, 28750, 28783, 28781, 28731, 842, 6662, 490, 2383, 891, 330, 1292, 489, 969, 634, 7211, 28830, 7850, 1021, 1807, 1037, 3866, 5617, 891, 7211, 1899, 301, 521, 375, 28716, 7605, 326, 1808, 23092, 16588, 4886, 20977, 7097, 503, 1471, 7945, 1097, 12583, 290, 602, 1190, 319, 641, 3639, 309, 4886, 1094, 18274, 886, 2060, 424, 4856, 891, 341, 5901, 3347, 1298, 5862, 886, 267, 338, 969, 5370, 13959, 19320, 20017, 1822, 969, 503, 12552, 28705, 28740, 28774, 28787, 28784, 3205, 3834, 5765, 303, 3288, 495, 2927, 434, 1787, 1376, 301, 969, 325, 28713, 28723, 7475, 365, 2377, 28705, 28750, 28750, 28723, 3999, 28705, 28750, 28734, 28734, 28774, 387, 28705, 28781, 330, 28828, 28754, 28705, 28740, 28734, 28734, 28748, 28734, 28783, 387, 399, 28711, 28723, 28705, 28770, 28787, 285, 28722, 28723, 290, 28727, 28759, 28725, 10502, 365, 6903, 25585, 28705, 28784, 28740, 28770, 28708, 25108, 28723, 28705, 28770, 28787, 28740, 327, 23963, 28741, 365, 6903, 28705, 28750, 28734, 28734, 28750, 25585, 28705, 28784, 28740, 28770, 28708, 25108, 28723, 28705, 28740, 28740, 28734, 28731, 842, 28705, 28740, 28774, 287, 28731, 12543, 21379, 6822, 4856, 1493, 6382, 14945, 28713, 28837, 4146, 602, 3021, 7897, 11838, 335, 370, 5141, 915, 12224, 1010, 10943, 490, 537, 1202, 297, 22627, 786, 2652, 18902, 269, 14740, 335, 1376, 301, 4540, 7475, 11039, 765, 3143, 2367, 1493, 7613, 6103, 9888, 383, 634, 13156, 4146, 23766, 634, 24355, 1046, 343, 28716, 17966, 28711, 815, 274, 396, 28764, 12841, 8331, 325, 28728, 1727, 28723, 7475, 365, 2377, 28705, 28740, 28734, 28723, 19314, 28705, 28750, 28734, 28734, 28783, 387, 28705, 28781, 330, 28828, 28754, 28705, 28783, 28783, 28740, 28748, 28734, 28787, 387, 399, 28711, 28723, 28705, 28750, 28734, 290, 28727, 28759, 28725, 10502, 5046, 28777, 25585, 28705, 28740, 22627, 786, 14354, 1127, 3021, 14740, 335, 343, 18820, 25108, 28723, 28705, 28784, 28783, 28731, 842, 4823, 7211, 26252, 15557, 3866, 1866, 663, 6227, 3021, 3834, 4856, 1493, 28705, 28770, 28740, 28723, 19314, 28705, 28740, 28774, 28774, 28782, 15781, 9195, 1860, 14740, 335, 1376, 301, 969, 10831, 1194, 269, 28725, 5661, 631, 1457, 1054, 7010, 6571, 276, 18820, 347, 820, 9744, 269, 14967, 15111, 28723, 384, 2180, 4856, 1457, 521, 909, 820, 2728, 12224, 22236, 11647, 4540, 634, 5062, 22221, 1105, 1046, 1097, 2450, 28713, 1496, 21037, 1202, 1094, 303, 2304, 634, 6204, 496, 308, 7246, 1298, 18201, 28725, 3021, 1202, 1202, 401, 595, 28729, 2294, 849, 317, 1126, 338, 25197, 891, 19187, 19495, 12669, 269, 24355, 1046, 343, 28716, 17966, 28711, 6813, 837, 28705, 28740, 28723, 17649, 28705, 28740, 28774, 28774, 28784, 446, 2869, 11461, 299, 12189, 5431, 3080, 28725, 6227, 20977, 891, 320, 28715, 28758, 640, 1866, 663, 6227, 11838, 335, 5141, 16436, 28723, 2941, 14740, 335, 5893, 969, 634, 1010, 10943, 490, 1891, 634, 7211, 28830, 7850, 936, 28722, 755, 1866, 663, 21037, 3155, 21756, 10740, 28720, 13518, 640, 6227, 15825, 28725, 7220, 891, 7211, 26252, 297, 11366, 3405, 1194, 969, 4513, 25906, 969, 528, 501, 28725, 503, 12552, 28705, 28750, 28734, 28734, 28781, 396, 15135, 819, 3744, 634, 13156, 4146, 23766, 3021, 1202, 7140, 270, 16988, 19348, 3571, 1767, 28723, 28705, 28750, 28734, 277, 28731, 6004, 1094, 886, 551, 338, 3021, 17583, 20537, 634, 5046, 1054, 12841, 416, 969, 1910, 28765, 634, 18912, 2520, 969, 7890, 335, 343, 434, 1291, 9322, 28705, 28770, 28740, 28723, 14313, 28705, 28740, 28774, 28774, 28782, 325, 28710, 28769, 28728, 28723, 28705, 28774, 28782, 363, 28769, 634, 3207, 282, 3347, 4128, 28880, 5141, 8331, 3217, 1449, 308, 28721, 1807, 274, 28731, 1200, 891, 3866, 3021, 17583, 20537, 891, 26796, 539, 11647, 6571, 375, 267, 450, 1234, 19166, 14967, 15111, 28725, 3103, 6227, 420, 5772, 1793, 634, 6876, 5715, 28721, 8331, 2927, 28722, 18657, 28713, 28723, 28705, 28750, 28740, 28705, 28750, 28723, 2941, 7211, 465, 3103, 11554, 5370, 18093, 521, 28726, 25906, 299, 28725, 24718, 891, 7211, 26252, 640, 1202, 365, 950, 9403, 424, 7765, 25283, 1298, 18201, 28728, 1909, 22436, 6331, 1202, 15646, 28706, 2927, 434, 1787, 28715, 1826, 15491, 969, 1202, 22627, 786, 14354, 28719, 950, 28714, 1899, 301, 5765, 4784, 554, 308, 281, 912, 22503, 12669, 534, 490, 5901, 930, 17738, 15746, 1210, 28725, 8595, 4748, 2835, 521, 3101, 288, 424, 6797, 279, 28715, 3792, 3751, 2927, 769, 278, 969, 3021, 1202, 503, 24355, 1046, 343, 18820, 2652, 13104, 269, 14740, 335, 343, 434, 28830, 490, 3155, 1429, 2453, 540, 3103, 28725, 7220, 1037, 891, 7211, 26252, 6294, 269, 16768, 297, 11366, 4375, 5662, 28726, 25906, 969, 319, 1807, 416, 290, 4755, 28723, 28705, 28750, 28750, 6177, 15557, 1866, 663, 281, 912, 262, 2453, 540, 28725, 818, 1796, 295, 262, 267, 5680, 2520, 550, 419, 14806, 634, 7211, 28830, 7850, 297, 1457, 320, 1449, 23517, 4138, 276, 3039, 6876, 5715, 3606, 7765, 1234, 3866, 7897, 281, 912, 22503, 12669, 269, 550, 419, 14806, 891, 365, 950, 9403, 1210, 5431, 28716, 12099, 295, 309, 2547, 14647, 3155, 11060, 15037, 4755, 4613, 28725, 891, 1037, 7165, 11110, 14967, 15111, 28725, 1808, 5984, 2484, 28830, 2520, 969, 891, 28705, 1726, 783, 22572, 3744, 2835, 26796, 539, 11647, 6571, 375, 267, 450, 1429, 28706, 262, 28726, 12125, 22627, 786, 14354, 28719, 950, 28714, 1899, 301, 6331, 1094, 23466, 969, 891, 2218, 769, 309, 3347, 2678, 301, 4540, 634, 365, 962, 317, 1126, 338, 25197, 891, 5148, 269, 1234, 28721, 5300, 28764, 8331, 640, 14239, 2520, 292, 269, 14740, 335, 343, 434, 28830, 490, 297, 19320, 2218, 769, 4544, 319, 1807, 8331, 401, 489, 969, 3642, 28764, 3683, 540, 28723, 23337, 290, 1349, 891, 5355, 270, 6227, 20248, 631, 6667, 28725, 818, 3866, 1202, 365, 950, 9403, 424, 7765, 17080, 1298, 18201, 28728, 1909, 22436, 6227, 21037, 3021, 19878, 891, 10665, 13659, 7246, 2678, 301, 4540, 297, 25585, 28705, 28740, 28740, 12709, 28723, 28705, 28750, 401, 28796, 28714, 28777, 640, 25585, 28705, 28740, 28734, 12709, 28723, 28705, 28740, 401, 28796, 28714, 17167, 7765, 297, 1094, 10200, 28712, 4755, 891, 534, 12386, 9195, 12224, 318, 16151, 6571, 10888, 262, 1822, 969, 1429, 10018, 15761, 299, 319, 3368, 540, 17738, 28725, 1202, 2218, 769, 309, 3347, 14740, 335, 1376, 301, 4540, 3021, 2846, 2367, 1493, 7211, 26252, 19495, 12669, 28706, 24355, 1046, 343, 28716, 17966, 11765, 396, 28764, 12841, 8331, 28725, 403, 891, 7303, 912, 1127, 5984, 5765, 4784, 554, 308, 269, 2927, 434, 1787, 28830, 2520, 969, 936, 490, 2383, 2453, 540, 14967, 15111, 28723, 11361, 14370, 24718, 3834, 2128, 631, 6876, 5715, 2383, 275, 2355, 450, 28725, 275, 26963, 6294, 269, 16768, 891, 9654, 2811, 279, 490, 2383, 21591, 8518, 5046, 1054, 12841, 416, 969, 4319, 328, 490, 11366, 330, 1231, 12548, 969, 6331, 1202, 396, 11733, 662, 3222, 625, 267, 1210, 269, 14740, 335, 4197, 405, 6227, 11152, 1808, 891, 22627, 786, 14354, 1127, 1234, 17284, 303, 28723, 28705, 28750, 28770, 264, 28731, 7496, 25585, 28705, 28750, 634, 24355, 1046, 343, 434, 1291, 1489, 321, 2630, 3866, 2846, 24355, 1046, 343, 28716, 17966, 11765, 4856, 1493, 3018, 28760, 915, 274, 28733, 10201, 374, 17633, 308, 283, 335, 343, 18820, 325, 28760, 962, 28731, 9322, 28705, 28750, 28770, 28723, 21341, 28705, 28740, 28774, 28784, 28740, 387, 325, 7118, 1074, 1231, 1061, 1609, 867, 14759, 28733, 28584, 6559, 318, 28723, 28705, 28740, 28782, 28782, 28731, 387, 640, 1457, 5148, 269, 1234, 28721, 5300, 28764, 8331, 7765, 14239, 2520, 292, 269, 14740, 335, 343, 434, 28830, 2383, 14771, 21144, 521, 3101, 288, 424, 10207, 3751, 560, 21610, 786, 14354, 1127, 275, 2355, 450, 281, 22960, 285, 11472, 951, 28725, 8595, 668, 28830, 1613, 596, 534, 1493, 28705, 28740, 28723, 4349, 28705, 28750, 28734, 28734, 28784, 1202, 7496, 5880, 527, 283, 335, 343, 434, 28830, 490, 6103, 365, 962, 1094, 23466, 969, 1300, 269, 28723, 384, 20205, 15557, 1037, 281, 912, 262, 2453, 540, 28725, 818, 5148, 21037, 1202, 7352, 1946, 969, 891, 22627, 786, 14354, 28719, 950, 28714, 1899, 301, 1234, 28721, 15046, 325, 13581, 1457, 6068, 28880, 303, 28830, 3173, 268, 28723, 7475, 365, 2377, 28705, 28740, 28774, 28723, 14313, 28705, 28750, 28734, 28740, 28734, 387, 28705, 28781, 330, 28828, 28754, 28705, 28787, 28774, 28784, 28748, 28734, 28783, 387, 399, 28711, 28723, 28705, 28740, 28787, 290, 28727, 28759, 28725, 1054, 2050, 28705, 28750, 28734, 28740, 28734, 28725, 28705, 28781, 28783, 28740, 28731, 1200, 1202, 6227, 7475, 1457, 3018, 2099, 769, 309, 3347, 365, 962, 28835, 2798, 269, 448, 325, 667, 297, 1457, 6662, 8203, 313, 4540, 634, 5355, 1449, 28705, 28740, 28774, 28723, 14313, 28705, 28750, 28734, 28740, 28734, 387, 28705, 28781, 330, 28828, 28754, 28705, 28787, 28774, 28784, 28748, 28734, 28783, 387, 1054, 2050, 28705, 28750, 28734, 28740, 28734, 28725, 28705, 28781, 28783, 28740, 28745, 28705, 28740, 28784, 28723, 17099, 28705, 28750, 28734, 28740, 28734, 387, 28705, 28781, 330, 28828, 28754, 28705, 28774, 28750, 28781, 28748, 28734, 28783, 387, 399, 28711, 28723, 28705, 28750, 28781, 28745, 19049, 14452, 28705, 28740, 28734, 28723, 17099, 28705, 28750, 28734, 28734, 28774, 387, 28705, 28781, 330, 28828, 28754, 28705, 28740, 28774, 28781, 28748, 28734, 28783, 387, 399, 28711, 28723, 28705, 28770, 28783, 28725, 10502, 365, 6903, 25585, 28705, 28740, 28782, 28787, 25108, 28723, 28705, 28770, 28783, 28731, 1200, 1966, 22502, 5370, 1202, 3018, 28830, 2520, 292, 269, 28835, 14740, 335, 343, 434, 28830, 490, 28725, 7220, 1037, 2846, 5062, 22221, 1105, 1046, 1097, 2450, 396, 2383, 18902, 4613, 28723, 11361, 14370, 24718, 676, 2367, 1493, 7211, 26252, 891, 330, 1292, 489, 969, 3103, 28725, 8595, 23929, 1202, 18347, 3039, 891, 7352, 1946, 969, 4675, 1891, 18853, 3149, 6578, 15746, 1210, 28725, 5074, 3606, 2846, 363, 1909, 269, 25542, 2852, 5141, 11765, 6294, 269, 16768, 3642, 1234, 28721, 5300, 28764, 2341, 7352, 1946, 969, 5984, 10207, 3143, 1429, 1793, 12224, 22627, 786, 14354, 28719, 397, 9281, 969, 297, 25585, 28705, 28750, 634, 24355, 1046, 343, 434, 1291, 28723, 28705, 28750, 28781, 264, 28708, 28731, 11975, 1899, 673, 25397, 5984, 1234, 28721, 5300, 28764, 8331, 2927, 434, 1787, 1899, 1946, 969, 3103, 28725, 8595, 3834, 20017, 1822, 969, 3834, 2678, 301, 969, 2181, 28837, 18727, 503, 11794, 485, 5984, 2623, 8727, 28712, 3347, 935, 28728, 793, 21591, 17134, 3021, 769, 392, 28723, 21144, 2678, 301, 969, 2181, 28837, 18727, 18610, 28395, 7475, 6876, 28725, 24718, 1202, 24162, 1375, 7897, 367, 13518, 5431, 331, 540, 7765, 686, 6234, 6227, 5431, 331, 540, 28725, 11554, 511, 338, 20039, 381, 303, 634, 19273, 805, 980, 301, 16044, 17738, 28725, 478, 309, 4748, 19467, 503, 10740, 28720, 13518, 634, 2927, 18820, 819, 338, 25500, 274, 3446, 6227, 983, 301, 6571, 3101, 2355, 632, 326, 2970, 17397, 17738, 28725, 640, 16936, 7303, 912, 1127, 3866, 307, 4755, 16718, 21379, 2835, 521, 28764, 329, 267, 496, 416, 559, 1899, 11647, 28707, 842, 15025, 5984, 9116, 8727, 15293, 11231, 15557, 7475, 1202, 3690, 28706, 6578, 28725, 24718, 891, 2927, 18820, 3834, 5470, 8645, 969, 1429, 7924, 269, 305, 11227, 303, 28725, 1202, 1234, 28722, 2274, 3744, 3103, 28725, 3205, 1457, 17934, 686, 786, 9265, 28706, 4852, 28721, 8331, 2678, 301, 6571, 11009, 3155, 14125, 7316, 7246, 28725, 24718, 835, 24986, 550, 784, 793, 21591, 19089, 634, 2927, 434, 1291, 3834, 3720, 366, 409, 1860, 28725, 791, 409, 980, 397, 24176, 393, 12548, 969, 6227, 3155, 1234, 28764, 19127, 3103, 325, 28728, 1727, 28723, 365, 2377, 28705, 28740, 28774, 28723, 14313, 28705, 28750, 28734, 28740, 28734, 387, 28705, 28781, 330, 28828, 28754, 28705, 28787, 28774, 28784, 28748, 28734, 28783, 387, 399, 28711, 28723, 28705, 28750, 28770, 290, 28727, 28759, 28725, 1054, 2050, 28705, 28750, 28734, 28740, 28734, 28725, 28705, 28781, 28783, 28740, 28745, 28705, 28740, 28784, 28723, 17099, 28705, 28750, 28734, 28740, 28734, 387, 28705, 28781, 330, 28828, 28754, 28705, 28774, 28750, 28781, 28748, 28734, 28783, 387, 399, 28711, 28723, 28705, 28750, 28770, 28731, 842, 28705, 28750, 28782, 287, 28726, 28731, 4294, 595, 3103, 1202, 5470, 8645, 969, 297, 25585, 28705, 28750, 634, 24355, 1046, 343, 434, 1291, 28725, 24718, 676, 4748, 2835, 10207, 3751, 22627, 786, 14354, 1127, 1429, 2453, 407, 28725, 305, 7362, 269, 2799, 632, 28723, 7352, 891, 10207, 3096, 7352, 5111, 2304, 969, 891, 22627, 786, 14354, 1127, 3021, 2846, 2218, 769, 4544, 319, 1807, 4539, 11838, 335, 8518, 2678, 301, 969, 1616, 6751, 1234, 28721, 15046, 3866, 891, 394, 2457, 891, 24162, 1375, 28725, 1202, 24355, 1046, 3101, 288, 4540, 634, 24355, 1046, 343, 28716, 17966, 28711, 815, 274, 6227, 297, 5984, 1489, 8645, 1210, 816, 864, 12649, 28764, 26628, 20628, 269, 28725, 1966, 22502, 4748, 396, 891, 14740, 335, 308, 25739, 503, 6204, 496, 308, 7246, 384, 19239, 3642, 28764, 324, 20850, 28723, 7029, 24355, 1046, 343, 28716, 17966, 11765, 7534, 297, 11366, 6662, 25739, 396, 1202, 6956, 3347, 24355, 1046, 3101, 288, 4540, 10560, 16436, 28725, 1202, 3446, 1202, 1010, 10943, 12667, 794, 12694, 1210, 28725, 1202, 1808, 1493, 297, 22627, 786, 2652, 18902, 269, 14740, 335, 343, 18820, 1234, 17284, 303, 6446, 28723, 2941, 24355, 1046, 343, 18820, 886, 16313, 1375, 17738, 544, 5517, 742, 5617, 891, 22627, 786, 14354, 28719, 950, 28714, 1899, 301, 4856, 1493, 12692, 6035, 729, 1839, 269, 550, 419, 14806, 891, 365, 950, 9403, 1210, 1202, 261, 1449, 12319, 3744, 13262, 299, 267, 1210, 28706, 28119, 352, 6227, 2855, 4755, 28725, 8595, 22992, 1653, 539, 2846, 10207, 3143, 297, 22627, 786, 25567, 28719, 1860, 2678, 301, 16817, 634, 365, 962, 6227, 11152, 5922, 490, 15491, 28707, 6446, 14967, 15111, 28723, 18034, 5148, 269, 12168, 701, 28716, 2244, 634, 19273, 3834, 2678, 301, 969, 297, 25585, 28705, 28750, 634, 24355, 1046, 343, 434, 1291, 28723, 21915, 1202, 17642, 374, 490, 12669, 28706, 2852, 673, 25397, 634, 365, 962, 3446, 1457, 19911, 539, 634, 15678, 640, 891, 9227, 9487, 269, 6103, 28705, 28740, 28723, 17721, 28705, 28750, 28734, 28734, 28782, 6331, 1457, 14740, 335, 343, 18820, 3446, 1457, 6204, 496, 308, 7246, 384, 19239, 325, 10639, 28834, 28757, 28731, 9322, 28705, 28740, 28770, 28723, 4074, 28705, 28750, 28734, 28734, 28782, 325, 29038, 28705, 28750, 14740, 335, 343, 18820, 6156, 19265, 291, 24422, 891, 26231, 21084, 27598, 634, 15678, 297, 1457, 5046, 28834, 28757, 640, 6156, 2678, 301, 969, 634, 13156, 4146, 23766, 267, 18201, 733, 10639, 28961, 28733, 28760, 915, 1668, 25585, 28705, 28750, 14740, 335, 343, 18820, 6156, 19265, 291, 24422, 891, 26231, 21084, 27598, 891, 18815, 16777, 1010, 10943, 490, 537, 297, 1457, 5046, 28834, 28757, 640, 6156, 2678, 301, 969, 634, 13156, 4146, 23766, 267, 18201, 733, 10639, 28961, 28733, 28790, 28796, 28741, 1181, 13564, 28723, 9322, 28705, 28740, 28770, 28723, 4074, 28705, 28750, 28734, 28734, 28782, 28731, 11298, 1457, 14740, 335, 343, 18820, 3446, 18912, 28712, 3903, 17190, 640, 18912, 28712, 14798, 396, 18815, 16777, 7787, 978, 25449, 28830, 381, 1173, 503, 19911, 539, 891, 20017, 19089, 891, 18815, 16777, 1010, 10943, 490, 537, 12715, 18919, 325, 10639, 28733, 28999, 28712, 14798, 28748, 28790, 28796, 28741, 28731, 9322, 28705, 28740, 28787, 28723, 3628, 28705, 28750, 28734, 28734, 28784, 325, 29038, 28705, 28750, 12709, 28723, 28705, 28740, 14740, 335, 343, 18820, 6156, 19265, 291, 24422, 891, 18912, 28712, 3903, 17190, 640, 18912, 28712, 14798, 396, 18815, 16777, 7787, 978, 25449, 28830, 381, 1173, 297, 1457, 5046, 28733, 28999, 28712, 14798, 28748, 28790, 28796, 28741, 640, 6156, 2678, 301, 969, 634, 13156, 4146, 23766, 267, 18201, 733, 10639, 28961, 28733, 28999, 28712, 14798, 28748, 28790, 28796, 28741, 28793, 9322, 28705, 28740, 28787, 28723, 3628, 28705, 28750, 28734, 28734, 28784, 28731, 640, 3446, 1457, 19911, 539, 891, 393, 28830, 2520, 6103, 28705, 28740, 28723, 4349, 28705, 28750, 28734, 28734, 28784, 6331, 1457, 5046, 28733, 28758, 9322, 28705, 28740, 28750, 28723, 17721, 28705, 28750, 28734, 28734, 28784, 4856, 25585, 28705, 28750, 5046, 28961, 28733, 28758, 11298, 6331, 1457, 14740, 335, 343, 18820, 3446, 18912, 28712, 3903, 17190, 640, 18912, 28712, 14798, 396, 2396, 25038, 1074, 28729, 2294, 21417, 9322, 28705, 28770, 28734, 28723, 17721, 28705, 28750, 28734, 28734, 28784, 325, 10639, 28733, 28999, 28712, 14798, 28748, 28738, 28715, 28758, 28731, 4856, 25585, 28705, 28750, 12709, 28723, 28705, 28740, 318, 6124, 28705, 28740, 613, 28790, 28719, 28723, 891, 1094, 20537, 28705, 28740, 13422, 330, 634, 14740, 335, 343, 434, 1291, 6156, 19265, 291, 24422, 891, 18912, 28712, 3903, 17190, 640, 18912, 28712, 14798, 396, 2396, 25038, 1074, 28729, 2294, 21417, 9322, 28705, 28770, 28734, 28723, 17721, 28705, 28750, 28734, 28734, 28784, 325, 10639, 28961, 28733, 28999, 28712, 14798, 28748, 28738, 28715, 28758, 28731, 3103, 891, 2927, 18820, 668, 28830, 1613, 596, 13959, 1493, 28705, 28740, 28723, 4349, 28705, 28750, 28734, 28734, 28784, 305, 7362, 269, 2799, 632, 9514, 19220, 28723, 28705, 28750, 28784, 21144, 307, 4755, 16718, 28721, 8518, 2678, 301, 969, 2181, 28837, 18727, 15557, 6227, 634, 19273, 1429, 485, 501, 6446, 28725, 478, 309, 891, 365, 962, 11039, 5922, 28726, 7939, 407, 640, 2367, 17342, 387, 765, 3096, 387, 560, 18274, 2846, 24355, 1046, 343, 28716, 17966, 11765, 891, 24162, 1375, 11039, 983, 12228, 14967, 15111, 28723, 6004, 2128, 1927, 2927, 21591, 11765, 3103, 275, 2865, 2367, 1493, 394, 419, 28714, 1228, 891, 7211, 1899, 301, 11039, 2367, 1493, 16667, 606, 5984, 6797, 279, 28715, 3792, 3096, 22627, 786, 14354, 1127, 1429, 28706, 262, 1822, 28723, 6177, 467, 28830, 424, 3834, 765, 3751, 6963, 28721, 1807, 969, 891, 21037, 21602, 5431, 5236, 1210, 11838, 335, 7246, 1298, 18201, 20537, 634, 4096, 411, 28705, 28750, 28734, 28734, 28770, 1796, 28723, 7029, 319, 2605, 521, 375, 28716, 7605, 326, 21696, 266, 28725, 8595, 3866, 1202, 24162, 1375, 503, 3217, 270, 21341, 28705, 28750, 28734, 28734, 28783, 3021, 3834, 387, 503, 9654, 1833, 391, 301, 1210, 1054, 14533, 15140, 28747, 24908, 387, 2927, 434, 1787, 28830, 2520, 969, 319, 2443, 262, 8335, 17738, 28723, 4823, 15920, 2450, 8518, 2678, 301, 969, 1616, 2457, 5984, 2128, 1839, 10207, 3096, 22627, 786, 14354, 28719, 950, 28714, 1899, 301, 924, 10786, 632, 1202, 6004, 1105, 9590, 25934, 891, 11838, 335, 7246, 2678, 301, 4540, 503, 6204, 496, 308, 7246, 384, 19239, 3446, 1202, 3663, 374, 479, 1210, 297, 19320, 2218, 769, 309, 3347, 6662, 25739, 28723, 18034, 1202, 1808, 1493, 297, 22627, 786, 2652, 18902, 269, 365, 962, 22926, 14538, 1822, 1234, 17284, 6062, 24355, 1046, 343, 28716, 17966, 28711, 6813, 4613, 3866, 1202, 14740, 335, 308, 25739, 5922, 2711, 17982, 28723, 6177, 6822, 1202, 7496, 5880, 527, 283, 335, 343, 434, 28830, 490, 6103, 365, 962, 396, 23092, 662, 3222, 625, 267, 1210, 325, 28728, 1727, 28723, 365, 2377, 28705, 28740, 28774, 28723, 14313, 28705, 28750, 28734, 28740, 28734, 387, 28705, 28781, 330, 28828, 28754, 28705, 28787, 28774, 28784, 28748, 28734, 28783, 387, 399, 28711, 28723, 28705, 28750, 28787, 290, 28727, 28759, 28725, 1054, 2050, 28705, 28750, 28734, 28740, 28734, 28725, 28705, 28781, 28783, 28740, 28745, 28705, 28740, 28784, 28723, 17099, 28705, 28750, 28734, 28740, 28734, 387, 28705, 28781, 330, 28828, 28754, 28705, 28774, 28750, 28781, 28748, 28734, 28783, 387, 399, 28711, 28723, 28705, 28781, 28770, 28731, 842, 28705, 28750, 28787, 287, 28731, 2941, 2367, 891, 2852, 673, 25397, 634, 365, 962, 640, 891, 19467, 1234, 28721, 5300, 28764, 8331, 640, 14239, 2520, 292, 269, 14740, 335, 343, 434, 28830, 490, 668, 28830, 1613, 596, 837, 28705, 28740, 28723, 4349, 28705, 28750, 28734, 28734, 28784, 26537, 1860, 307, 4755, 16718, 28721, 8518, 2678, 301, 969, 2181, 28837, 18727, 3103, 503, 816, 490, 891, 1234, 28721, 5300, 28764, 8331, 2927, 434, 1787, 1899, 1946, 969, 3155, 1543, 5715, 9721, 325, 13581, 1457, 6068, 28880, 303, 28830, 3173, 891, 1234, 28721, 5300, 28764, 8331, 2927, 434, 1787, 1899, 1946, 969, 365, 2377, 28705, 28740, 28774, 28723, 14313, 28705, 28750, 28734, 28740, 28734, 387, 28705, 28781, 330, 28828, 28754, 28705, 28787, 28774, 28784, 28748, 28734, 28783, 387, 399, 28711, 28723, 28705, 28770, 28740, 290, 28727, 28759, 28725, 1054, 2050, 28705, 28750, 28734, 28740, 28734, 28725, 28705, 28781, 28783, 28740, 28731, 842, 21988, 1234, 28721, 15046, 28725, 8595, 1202, 24162, 1375, 3446, 1457, 12168, 891, 9654, 6876, 5715, 28721, 8331, 14740, 335, 2913, 28729, 28764, 1893, 634, 503, 24355, 1046, 343, 18820, 2798, 13104, 269, 11838, 335, 7246, 2678, 301, 969, 1616, 6751, 274, 2846, 4856, 17090, 4539, 11838, 335, 8518, 2678, 301, 969, 1616, 6751, 634, 6204, 496, 308, 7246, 384, 19239, 274, 1429, 28706, 262, 28726, 444, 295, 13013, 269, 28725, 478, 309, 3834, 765, 3751, 2678, 301, 969, 891, 24355, 1046, 3101, 288, 4540, 3021, 1457, 10740, 28720, 13518, 891, 9654, 6876, 5715, 28721, 8331, 14740, 335, 2913, 28729, 28764, 1893, 6227, 24461, 4287, 21951, 20248, 783, 595, 28723, 11403, 275, 28830, 951, 6876, 5715, 28721, 416, 6103, 507, 374, 1202, 14740, 335, 1376, 301, 4540, 3446, 1457, 6204, 496, 308, 7246, 384, 19239, 891, 393, 28830, 2520, 9514, 23668, 28723, 4294, 595, 4803, 313, 299, 3446, 2846, 6621, 28705, 28750, 28734, 28734, 28784, 3834, 1429, 18820, 8518, 6004, 1105, 9590, 25934, 634, 5046, 1054, 12841, 416, 969, 28725, 3021, 1457, 891, 7211, 26252, 17507, 1094, 886, 551, 338, 341, 11270, 3903, 28725, 3642, 28723, 28705, 28750, 28783, 264, 28708, 28731, 2941, 24162, 1375, 17738, 1202, 307, 6533, 397, 7352, 5111, 2304, 969, 634, 13217, 5837, 4588, 19495, 12669, 269, 24355, 1046, 343, 28716, 17966, 28711, 815, 274, 5617, 5984, 10207, 3096, 7352, 5111, 2304, 969, 891, 22627, 786, 14354, 1127, 3021, 2846, 11838, 335, 8518, 2678, 301, 969, 1616, 6751, 634, 365, 962, 3446, 1202, 1054, 2950, 370, 632, 891, 2678, 301, 969, 2762, 4755, 891, 14740, 335, 343, 18820, 2]\n"
     ]
    }
   ],
   "source": [
    "# Right Format sollte sein [2, 2, 2 padding..., 1, [tokens], 2]\n",
    "print(tokenized_train_dataset[1]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2175b866-bfc6-4731-b61f-29b48210c325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2616\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Alle gleiche Länge jetzt!!\n",
    "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cacf0f9-6080-45e5-ab2b-eec0b4288199",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Untrainiertes Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb5c5312-d43e-4749-adcc-13f93a787faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was ist die Rechtsprechung für irreführende Werbung? 1.\n",
      "\n",
      "## What is the legal definition of misleading advertising?\n",
      "\n",
      "Misleading advertising is a form of false advertising that involves making claims about products or services that are not true, or that could be interpreted as being untrue by consumers. Misleading advertising can take many forms, including: False statements about product features or benefits.\n",
      "\n",
      "What is an example of misleading advertising?\n",
      "\n",
      "An example of misleading advertising would be if a company claimed their product was “all natural” when it actually contained synthetic ingredients. Another example would be if a company advertised their product as being able to cure cancer when there is no scientific evidence to support this claim.\n",
      "\n",
      "### How do you prove misleading advertising?\n",
      "\n",
      "There are a few ways to go about proving that an advertisement is misleading. One way is to look at the ad itself and see if any of the claims made in the ad are false. If so, then the ad may be considered misleading. Another way to prove that an ad is misleading is to look at how the ad is presented.\n",
      "\n",
      "How does the FTC define deceptive advertising?\n",
      "\n",
      "The Federal Trade Commission (FTC) defines deceptive advertising as “any advertisement\n"
     ]
    }
   ],
   "source": [
    "# Testen des Basemodels für einen Input-Prompt\n",
    "input_prompt = \" Was ist die Rechtsprechung für irreführende Werbung? \"\n",
    "\n",
    "# Init an input tokenizer that doesn't add padding or eos token\n",
    "input_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "\n",
    "model_input = input_tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(input_tokenizer.decode(model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4437128-1a7b-4589-80da-a97b77d5cd3e",
   "metadata": {},
   "source": [
    "## Preprocessing Model (Finetuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb7b8196-ea57-4781-a21e-9206a11bafcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to model -> Vorbereitung auf Training -> Nutzung von PEFT = Parameter Efficient Fine-Tuning\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fecd1149-3390-46a1-830d-eb21c6fc709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18ce22c5-3730-48ff-8e86-13883a0d64f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cca6197b-cfcf-4c5a-9a6b-e1e9db911d3b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 85041152 || all params: 3837112320 || trainable%: 2.2162799758751914\n"
     ]
    }
   ],
   "source": [
    "# Lora Konfiguration\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2777386-3183-417c-ae7b-6d7d120c2850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MistralForCausalLM(\n",
      "      (model): MistralModel(\n",
      "        (embed_tokens): Embedding(32000, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x MistralDecoderLayer(\n",
      "            (self_attn): MistralSdpaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): MistralRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): MistralMLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "      (lm_head): lora.Linear(\n",
      "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.05, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=4096, out_features=32, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=32, out_features=32000, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "        (lora_magnitude_vector): ModuleDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad909e7-cf58-4706-b1d2-518379e5f9c6",
   "metadata": {},
   "source": [
    "## Modeltraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e387e58-c4b5-4e94-aea7-bc3c45831fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    print(\"More than 1 GPU\")\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3992d89b-942e-4fa4-8edc-17e576ed9e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free memory: 24017.44 MB (23.45 GB)\n",
      "Total memory: 24211.31 MB (23.64 GB)\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "# Freien Speicher anzeigen\n",
    "import torch\n",
    "free_mem, total_mem = torch.cuda.mem_get_info()\n",
    "\n",
    "free_mem_MB = free_mem / (1024 ** 2)\n",
    "total_mem_MB = total_mem / (1024 ** 2)\n",
    "\n",
    "free_mem_GB = free_mem / (1024 ** 3)\n",
    "total_mem_GB = total_mem / (1024 ** 3)\n",
    "\n",
    "print(f\"Free memory: {free_mem_MB:.2f} MB ({free_mem_GB:.2f} GB)\")\n",
    "print(f\"Total memory: {total_mem_MB:.2f} MB ({total_mem_GB:.2f} GB)\")\n",
    "print(torch.cuda.memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6b14d9b-c082-4b9e-ba0b-4d343a5e9c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speicher frei machen\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7dbbb14-d789-4953-a33e-908fe7a614c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingDataSize 2354\n",
      "TestgDataSize 262\n"
     ]
    }
   ],
   "source": [
    "print(\"TrainingDataSize\", len(tokenized_train_dataset))\n",
    "print(\"TestgDataSize\", len(tokenized_val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a4793f8-f169-45c7-9918-331b90f4c66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ps2024/miniconda3/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/home/ps2024/miniconda3/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 26/400 12:04 < 3:08:17, 0.03 it/s, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.42 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 42\u001b[0m\n\u001b[1;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     16\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_train_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mtransformers\u001b[38;5;241m.\u001b[39mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     41\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# silence the warnings. Please re-enable for inference!\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:1948\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1946\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1948\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1949\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1953\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:2366\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2364\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2366\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2368\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:2814\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2812\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2814\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m   2817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_checkpoint(model, trial, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:2771\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   2770\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 2771\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2772\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2774\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:3676\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3673\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3675\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3676\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3677\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3679\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3680\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3684\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3686\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:3867\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3864\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   3866\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 3867\u001b[0m losses, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3868\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3869\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:4085\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   4083\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_labels \u001b[38;5;129;01mor\u001b[39;00m loss_without_labels:\n\u001b[1;32m   4084\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 4085\u001b[0m         loss, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   4086\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m   4088\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:3373\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3371\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3372\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3373\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3374\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3375\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/accelerate/utils/operations.py:819\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/accelerate/utils/operations.py:807\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/peft/peft_model.py:1586\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1585\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1586\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1590\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1591\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1592\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1593\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1594\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1597\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:188\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:1033\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1030\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1033\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1046\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1047\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:808\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    797\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    798\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    799\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    805\u001b[0m         cache_position,\n\u001b[1;32m    806\u001b[0m     )\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 808\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:564\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    563\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 564\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    565\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    567\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/models/mistral/modeling_mistral.py:156\u001b[0m, in \u001b[0;36mMistralMLP.forward\u001b[0;34m(self, hidden_state)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_state):\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(hidden_state))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:502\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m requires_conversion:\n\u001b[1;32m    500\u001b[0m             output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto(expected_dtype)\n\u001b[0;32m--> 502\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.42 GiB. GPU "
     ]
    }
   ],
   "source": [
    "# Training Schritt\n",
    "# WandB auskommentiert erstmal -> wenn nicht wird da n cooles Dashboard fürs Training erstellt\n",
    "# Hier muss viel Finetuned werden!!\n",
    "# Overfitting wäre prinzipiell gut ?\n",
    "\n",
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"journal-finetune\"\n",
    "base_model_name = \"mistral\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=2,\n",
    "        per_device_train_batch_size=1, # Besser 2\n",
    "        gradient_accumulation_steps=2, # Besser 1\n",
    "        gradient_checkpointing=True,\n",
    "        max_steps=25, # Besser > 400\n",
    "        learning_rate=2.5e-5, # Want a small lr for finetuning\n",
    "        fp16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_steps=25,              # When to start reporting loss\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=25,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=25,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        #report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fc5f0f-83eb-43b6-a59d-dc4bd8274f25",
   "metadata": {},
   "source": [
    "## Modeltesting (am besten Kernel hierfür neu starten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18b3681e-6b79-4ea9-b744-af77f4b390a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fdb00b65a014932955f384842f8655e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing the trained model\n",
    "# Hier kann ein Kernel neustart gut sein, damit das Model nicht doppelt geladen wird\n",
    "# Dann Model nochmal laden und gleiches mistral_id/quantization Konfig wie davor\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "621e0350-6838-47c8-8b1a-e31450b4dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auswahl des besten checkpoints und anwenden des QLora Adapters auf das Modell\n",
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"mistral-journal-finetune/checkpoint-20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbdb3ae7-75d3-4cd0-b87a-f0449dd79abd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ausführen des Models mit Input Prompt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m input_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Kurze und einfache Erklärung für die Rechtsprechung und Strafen bei Betrug mit Vorsatz ? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model_input \u001b[38;5;241m=\u001b[39m \u001b[43meval_tokenizer\u001b[49m(input_prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m ft_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Ausführen des Models mit Input Prompt\n",
    "input_prompt = \" Kurze und einfache Erklärung für die Rechtsprechung und Strafen bei Betrug mit Vorsatz ? \"\n",
    "model_input = eval_tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=200, repetition_penalty=1.15)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ea06f5-5c60-4466-bbf9-daf247385040",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
