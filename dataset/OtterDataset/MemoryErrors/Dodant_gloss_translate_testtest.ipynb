{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-05T05:34:50.298586774Z",
     "start_time": "2023-10-05T05:34:50.290165458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "model_ = 'skt/kogpt2-base-v2'\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_, bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>')\n",
    "model = GPT2LMHeadModel.from_pretrained(model_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T05:41:17.525096740Z",
     "start_time": "2023-10-05T05:41:10.766017658Z"
    }
   },
   "id": "c858811c44839e37"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "도움 주다? >> 도와주시겠어요?\n",
      "과학 실험 준비 끝? >> 과학 실험 준비 다 했어?\n",
      "체육 시간 축구 하다. >> 체육 시간에 축구를 했어\n",
      "교실 소음 크다. >> 운동 시간이 너무 길어서. -_-; 학교 가기 싫은데... 수업 끝나고 바로 교실로 가자고 해서.\n",
      "운동장 가서 쉬고 싶다고 했는데.. 그게 안되서 못갔다\n",
      "#대구대공원은행에서 운영하는 곳인데요~!\n",
      "이곳도 저희들이 좋아하는 곳이죠!</d> 오늘 아침, 제가 어제 먹은 빵을 먹었습니다.\n",
      "빵에 들어있는 설탕과 당분이랑 우유 그리고 치즈까지 모두 맛있었는데요.\n",
      "그냥 그냥 먹는거라 생각했는데 정말 꿀맛이었어요.\n",
      "그리고 이번에 나온 신메뉴는 역시나 달달한 크림치즈가 있었구요,\n",
      "크림치는 진짜 고소하고 담백해서 좋았답니다.\n",
      "저는 개인적으로 요렇게 먹으면 더 맛있는 것 같아요\n",
      "제가 선택한 메인은 ``바닐라 아이스크림이 나왔는데요.^^ \n",
      "아이스 아메리카노 한잔으로 딱인것 같네용ᄒᄏ<unk>♥♡ -\n",
      "오늘의 포스팅은, 바닐라빈스 케이크를 먹고 난 후로 꼭 한번 먹어봐야 할\n"
     ]
    }
   ],
   "source": [
    "text = '도움 주다 ? >> 도와주시겠어요?\\n과학 실험 준비 끝 ? >> 과학 실험 준비 다 했어?\\n체육 시간 축구 하다 . >> 체육 시간에 축구를 했어\\n교실 소음 크다 . >>'\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "gen_ids = model.generate(input_ids, max_length=256, repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)\n",
    "generated = tokenizer.decode(gen_ids[0])\n",
    "print(generated)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T05:45:09.138018432Z",
     "start_time": "2023-10-05T05:45:03.514770154Z"
    }
   },
   "id": "a4e7232d7349c1d2"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 75, but ``max_length`` is set to 64. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(출발어) 도움 주다? >> (도착어) 도와주시겠어요?', '(출발어) 과학 실험 준비 끝? >> (도착어) 과학 실험 준비 다 했어?', '(출발어) 체육 시간 축구 하다. >> (도착어) 체육 시간에 축구를 했어.', '(출발어) 커피? >>']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "model_ = 'skt/ko-gpt-trinity-1.2B-v0.5'\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_, bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>')\n",
    "model = GPT2LMHeadModel.from_pretrained(model_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T04:22:18.511170750Z",
     "start_time": "2023-10-10T04:22:04.310672296Z"
    }
   },
   "id": "6a9885ff1d1a6620"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "도움 주다? > 도와주시겠어요?\n",
      "물 너무 차갑다. > 물이 너무 차가워요.\n",
      "사다리 필요하다! > 사다리가 필요해요.\n",
      "세일 언제 까지? > 세일 행사 언제까지 하나요?\n",
      "휴식 시간 잡지 보다. > 휴식 시간에는 잡지를 본다.\n",
      "책 우리 지식주다. > 책은 우리에게 여러 지식을 준다.\n",
      "연구 위해 논문 여러 읽다. > 연구를 위해 여러 논문을 읽는다.\n",
      "자연 재해 우리 경각심 주다. > 자연 재해가 발생하면 우리는 어떻게 해야 할까?\n",
      "지진 나면 어떡하지?> 지진 났을 때 대처 방법\n",
      " 지진이 일어나면 어떤 일이 벌어질까?<1단계 :\n"
     ]
    }
   ],
   "source": [
    "text = ('도움 주다 ? > 도와주시겠어요?\\n'\n",
    "        '물 너무 차갑다 . > 물이 너무 차가워요.\\n'\n",
    "        '사다리 필요하다 ! > 사다리가 필요해요.\\n'\n",
    "        '세일 언제 까지 ? > 세일 행사 언제까지 하나요?\\n'\n",
    "        '휴식 시간 잡지 보다 . > 휴식 시간에는 잡지를 본다.\\n'\n",
    "        '책 우리 지식주다 . > 책은 우리에게 여러 지식을 준다.\\n'\n",
    "        '연구 위해 논문 여러 읽다 . > 연구를 위해 여러 논문을 읽는다.\\n'\n",
    "        '자연 재해 우리 경각심 주다 . >')\n",
    "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "gen_ids = model.generate(input_ids, max_length=128, repetition_penalty=2.0,\n",
    "                         pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id, bos_token_id=tokenizer.bos_token_id, use_cache=True)\n",
    "generated = tokenizer.decode(gen_ids[0])\n",
    "print(generated)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T07:24:32.274677910Z",
     "start_time": "2023-10-10T07:24:24.954940707Z"
    }
   },
   "id": "98a5328b91f413ce"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.69 GiB total capacity; 22.44 GiB already allocated; 45.56 MiB free; 22.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 8\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoTokenizer, AutoModelForCausalLM \n\u001B[1;32m      4\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m      5\u001B[0m   \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkakaobrain/kogpt\u001B[39m\u001B[38;5;124m'\u001B[39m, revision\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mKoGPT6B-ryan1.5b-float16\u001B[39m\u001B[38;5;124m'\u001B[39m,  \u001B[38;5;66;03m# or float32 version: revision=KoGPT6B-ryan1.5b\u001B[39;00m\n\u001B[1;32m      6\u001B[0m   bos_token\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[BOS]\u001B[39m\u001B[38;5;124m'\u001B[39m, eos_token\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[EOS]\u001B[39m\u001B[38;5;124m'\u001B[39m, unk_token\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[UNK]\u001B[39m\u001B[38;5;124m'\u001B[39m, pad_token\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[PAD]\u001B[39m\u001B[38;5;124m'\u001B[39m, mask_token\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[MASK]\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      7\u001B[0m )\n\u001B[0;32m----> 8\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m  \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mkakaobrain/kogpt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mKoGPT6B-ryan1.5b-float16\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# or float32 version: revision=KoGPT6B-ryan1.5b\u001B[39;49;00m\n\u001B[1;32m     10\u001B[0m \u001B[43m  \u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mauto\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m _ \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39meval()\n",
      "File \u001B[0;32m~/anaconda3/envs/gloss_translate/lib/python3.8/site-packages/torch/nn/modules/module.py:989\u001B[0m, in \u001B[0;36mModule.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    985\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    986\u001B[0m                     non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[1;32m    987\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, non_blocking)\n\u001B[0;32m--> 989\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/gloss_translate/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    639\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    640\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 641\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    643\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    644\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    645\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    646\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    651\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    652\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/gloss_translate/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    639\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    640\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 641\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    643\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    644\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    645\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    646\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    651\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    652\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "    \u001B[0;31m[... skipping similar frames: Module._apply at line 641 (1 times)]\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/gloss_translate/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    639\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply\u001B[39m(\u001B[38;5;28mself\u001B[39m, fn):\n\u001B[1;32m    640\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 641\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    643\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    644\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    645\u001B[0m             \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    646\u001B[0m             \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    651\u001B[0m             \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    652\u001B[0m             \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/gloss_translate/lib/python3.8/site-packages/torch/nn/modules/module.py:688\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn)\u001B[0m\n\u001B[1;32m    686\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, buf \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffers\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    687\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buf \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 688\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffers[key] \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    690\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/gloss_translate/lib/python3.8/site-packages/torch/nn/modules/module.py:987\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m    984\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[1;32m    985\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    986\u001B[0m                 non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[0;32m--> 987\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.69 GiB total capacity; 22.44 GiB already allocated; 45.56 MiB free; 22.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
    "  bos_token='[BOS]', eos_token='[EOS]', unk_token='[UNK]', pad_token='[PAD]', mask_token='[MASK]'\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
    "  pad_token_id=tokenizer.eos_token_id, torch_dtype='auto', low_cpu_mem_usage=True).to(device='cuda', non_blocking=True)\n",
    "_ = model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T07:52:45.844236733Z",
     "start_time": "2023-10-10T07:51:14.970430923Z"
    }
   },
   "id": "1190ea90cec173fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = ('도움 주다 ? > 도와주시겠어요?\\n'\n",
    "        '물 너무 차갑다 . > 물이 너무 차가워요.\\n'\n",
    "        '사다리 필요하다 ! > 사다리가 필요해요.\\n'\n",
    "        '세일 언제 까지 ? > 세일 행사 언제까지 하나요?\\n'\n",
    "        '휴식 시간 잡지 보다 . > 휴식 시간에는 잡지를 본다.\\n'\n",
    "        '책 우리 지식주다 . > 책은 우리에게 여러 지식을 준다.\\n'\n",
    "        '연구 위해 논문 여러 읽다 . > 연구를 위해 여러 논문을 읽는다.\\n'\n",
    "        '자연 재해 우리 경각심 주다 . >')\n",
    "with torch.no_grad():\n",
    "  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n",
    "  gen_tokens = model.generate(tokens, do_sample=True, temperature=0.8, max_length=64)\n",
    "  generated = tokenizer.batch_decode(gen_tokens)[0]\n",
    "  \n",
    "print(generated)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-10T07:52:45.842634914Z"
    }
   },
   "id": "d88a094fb0ee3e46"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 아이의 이런 저런 생각 듣고 있어요.\n",
      "내 세상도 넓어지겠다 ♩ 그렇니까 나중에 크면 내 마음대로 할래~�\n"
     ]
    }
   ],
   "source": [
    "# coding=utf8\n",
    "# REST API 호출에 필요한 라이브러리\n",
    "import requests\n",
    "import json\n",
    "\n",
    "REST_API_KEY = '9bf07821abd1d1ef32b75a30f44cbf52'\n",
    "\n",
    "def kogpt_api(prompt, max_tokens = 1, temperature = 1.0, top_p = 1.0, n = 1):\n",
    "    r = requests.post(\n",
    "        'https://api.kakaobrain.com/v1/inference/kogpt/generation',\n",
    "        json = {\n",
    "            'prompt': prompt,\n",
    "            'max_tokens': max_tokens,\n",
    "            'temperature': temperature,\n",
    "            'top_p': top_p,\n",
    "            'n': n\n",
    "        },\n",
    "        headers = {\n",
    "            'Authorization': 'KakaoAK ' + REST_API_KEY,\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "    )\n",
    "    response = json.loads(r.content)\n",
    "    return response\n",
    "\n",
    "prompt = '''주어진 단어를 문장으로 자연스럽게 바꿔주세요.\n",
    "\n",
    "도움 주다 ? > 도와주시겠어요?\n",
    "물 너무 차갑다 . > 물이 너무 차가워요.\n",
    "사다리 필요하다 ! > 사다리가 필요해요.\n",
    "세일 언제 까지 ? > 세일 행사 언제까지 하나요?\n",
    "휴식 시간 잡지 보다 . > 휴식 시간에는 잡지를 본다.\n",
    "책 우리 지식주다 . > 책은 우리에게 여러 지식을 준다.\n",
    "\n",
    "지금 무슨 생각 ? > '''\n",
    "\n",
    "response = kogpt_api(prompt = prompt, max_tokens = 32)\n",
    "print(response['generations'][0]['text'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T08:09:19.004230228Z",
     "start_time": "2023-10-10T08:09:18.150481379Z"
    }
   },
   "id": "96d7e9c12296bf1c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
