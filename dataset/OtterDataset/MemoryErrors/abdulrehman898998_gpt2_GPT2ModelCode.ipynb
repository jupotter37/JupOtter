{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMb6sjo2TwVn/pC3jY6HAMx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdulrehman898998/gpt2/blob/main/Untitled66.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"vocab_size\": 50257,               # Size of the vocabulary (e.g., BERT tokenizer vocab size)\n",
        "    \"token_embedding_dim\": 768,       # Dimension of token embeddings\n",
        "    \"context_len\": 500,               # Maximum length of input sequence (context length)\n",
        "    \"dropout\": 0.1,                   # Dropout rate\n",
        "    \"num_of_transformer\": 8,         # Number of transformer blocks/layers\n",
        "    \"num_heads\": 8,                  # Number of attention heads\n",
        "    \"qkv_bias\": True,                 # Whether to use bias in QKV projections\n",
        "}\n"
      ],
      "metadata": {
        "id": "vidPYK0ffpyK"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Layer Normalization class: Normalizes the input tensor\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5  # Small epsilon value added to the denominator for numerical stability\n",
        "        self.scale = nn.Parameter(torch.ones(embed_dim))  # Learnable scaling parameter\n",
        "        self.shift = nn.Parameter(torch.zeros(embed_dim))  # Learnable shifting parameter\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (b, n_tokens, embed_dim)\n",
        "        # b: batch size, n_tokens: number of tokens in the sequence, embed_dim: embedding dimension\n",
        "\n",
        "        mean = x.mean(dim=-1, keepdim=True)  # Mean along the embedding dimension (embed_dim)\n",
        "        std = x.std(dim=-1, keepdim=True)    # Standard deviation along the embedding dimension (embed_dim)\n",
        "\n",
        "        # Normalize: (x - mean) / (std + eps)\n",
        "        normalized_x = (x - mean) / (std + self.eps)  # Shape: (b, n_tokens, embed_dim)\n",
        "\n",
        "        # Apply the scale and shift parameters\n",
        "        # (b, n_tokens, embed_dim) * scale + shift\n",
        "        output = self.scale * normalized_x + self.shift  # Shape: (b, n_tokens, embed_dim)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# GELU activation function class: Applies GELU activation to the input tensor\n",
        "class Gelu(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # GELU activation function: 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))\n",
        "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "\n",
        "\n",
        "# Feed-Forward neural network class: Implements a simple FFNN with a hidden layer\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Sequential container for the layers of the FeedForward network\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 4 * embed_dim),  # Linear transformation (embed_dim -> 4 * embed_dim)\n",
        "            Gelu(),  # GELU activation function\n",
        "            nn.Linear(4 * embed_dim, embed_dim)   # Linear transformation (4 * embed_dim -> embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (b, n_tokens, embed_dim)\n",
        "        # Apply the FeedForward network\n",
        "        return self.net(x)  # Output shape: (b, n_tokens, embed_dim)\n"
      ],
      "metadata": {
        "id": "rv85R23zFBlG"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, n_heads, qkv_bias=False, context_len=None):\n",
        "        super().__init__()\n",
        "        self.d_in = d_in  # The dimensionality of the input\n",
        "        self.d_out = d_out  # The output dimensionality of the attention\n",
        "        self.n_heads = n_heads  # The number of attention heads\n",
        "\n",
        "        # Check if the output dimensionality is divisible by the number of heads\n",
        "        if d_out % n_heads != 0:\n",
        "            raise ValueError(\"d_out must be divisible by n_heads\")\n",
        "\n",
        "        self.head_dim = d_out // n_heads  # Dimensionality of each attention head\n",
        "\n",
        "        # Define linear transformations to get Q, K, and V\n",
        "        self.w_q = nn.Linear(d_in, d_out, bias=qkv_bias)  # Linear layer for query\n",
        "        self.w_k = nn.Linear(d_in, d_out, bias=qkv_bias)  # Linear layer for key\n",
        "        self.w_v = nn.Linear(d_in, d_out, bias=qkv_bias)  # Linear layer for value\n",
        "\n",
        "        # Mask for causal attention (used during autoregressive generation)\n",
        "        if context_len is not None:\n",
        "            self.register_buffer(\n",
        "                \"mask\",\n",
        "                torch.triu(torch.ones(context_len, context_len), diagonal=1).bool()  # Upper triangular mask\n",
        "            )\n",
        "        else:\n",
        "            self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, n_tokens, _ = x.shape  # (batch_size, seq_len, d_in)\n",
        "\n",
        "        # Apply the linear transformations to get Q, K, and V\n",
        "        q = self.w_q(x)  # (b, n_tokens, d_out)\n",
        "        k = self.w_k(x)  # (b, n_tokens, d_out)\n",
        "        v = self.w_v(x)  # (b, n_tokens, d_out)\n",
        "\n",
        "        # Reshape Q, K, V for multi-head attention\n",
        "        # q: (b, n_tokens, n_heads, head_dim) -> (b, n_heads, n_tokens, head_dim)\n",
        "        q = q.view(b, n_tokens, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # k: (b, n_tokens, n_heads, head_dim) -> (b, n_heads, n_tokens, head_dim)\n",
        "        k = k.view(b, n_tokens, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        # v: (b, n_tokens, n_heads, head_dim) -> (b, n_heads, n_tokens, head_dim)\n",
        "        v = v.view(b, n_tokens, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # q, k, v are now shaped as (b, n_heads, n_tokens, head_dim)\n",
        "\n",
        "        # Compute attention scores: Q * K^T / sqrt(head_dim)\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1))  # (b, n_heads, n_tokens, n_tokens)\n",
        "        scores = scores / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))  # Scaling by sqrt(head_dim)\n",
        "\n",
        "        # Apply mask if available (for causal attention during autoregressive generation)\n",
        "        if self.mask is not None:\n",
        "            # Mask is of shape (n_tokens, n_tokens), we slice it to match the batch size\n",
        "            mask_bool = self.mask[:n_tokens, :n_tokens]  # (n_tokens, n_tokens)\n",
        "            scores = scores.masked_fill(mask_bool, float('-inf'))  # Set masked positions to -inf\n",
        "\n",
        "        # Compute attention weights using softmax\n",
        "        attention_weights = torch.softmax(scores, dim=-1)  # (b, n_heads, n_tokens, n_tokens)\n",
        "\n",
        "        # Output is the weighted sum of values\n",
        "        out = torch.matmul(attention_weights, v)  # (b, n_heads, n_tokens, head_dim)\n",
        "\n",
        "        # Reshape the output back to (b, n_tokens, d_out)\n",
        "        out = out.transpose(1, 2).contiguous().view(b, n_tokens, self.n_heads * self.head_dim)\n",
        "        # out is now of shape (b, n_tokens, d_out)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "IPbpewGhFBqG"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Assuming you have defined the following modules:\n",
        "# MultiHeadAttention, FeedForward, and LayerNorm\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        embed_dim = config['token_embedding_dim']\n",
        "\n",
        "        # Multi-head attention layer\n",
        "        self.attention = MultiHeadAttention(\n",
        "            d_in=embed_dim,\n",
        "            d_out=embed_dim,\n",
        "            n_heads=config['num_heads'],\n",
        "            qkv_bias=config.get('qkv_bias', False),\n",
        "            context_len=config['context_len']\n",
        "        )\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.feed_forward = FeedForward(embed_dim)\n",
        "\n",
        "        # Layer normalization for attention and feed-forward outputs\n",
        "        self.norm1 = LayerNorm(embed_dim)\n",
        "        self.norm2 = LayerNorm(embed_dim)\n",
        "\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(config['dropout'])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply attention with residual connection and layer normalization\n",
        "        attn_output = self.attention(self.norm1(x))  # (batch_size, seq_len, embed_dim)\n",
        "        x = x + self.dropout(attn_output)  # Add residual connection + dropout\n",
        "\n",
        "        # Apply feed-forward network with residual connection and layer normalization\n",
        "        ffn_output = self.feed_forward(self.norm2(x))  # (batch_size, seq_len, embed_dim)\n",
        "        x = x + self.dropout(ffn_output)  # Add residual connection + dropout\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "DX_BVX-qFBs8"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "2qyfIMb24olE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Assuming TransformerBlock and any other necessary classes (like LayerNorm) are defined elsewhere\n",
        "\n",
        "class MyGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # Token embeddings: map each token index to a vector representation\n",
        "        self.token_embed_lookup = nn.Embedding(config['vocab_size'], config['token_embedding_dim'])\n",
        "\n",
        "        # Positional embeddings: each position gets a corresponding embedding\n",
        "        self.positional_embed = nn.Embedding(config['context_len'], config['token_embedding_dim'])\n",
        "\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(config['dropout'])\n",
        "\n",
        "        # Sequence of transformer blocks\n",
        "        self.transformers = nn.Sequential(\n",
        "            *[TransformerBlock(config) for _ in range(config['num_of_transformer'])]\n",
        "        )\n",
        "\n",
        "        # Final LayerNorm to normalize the output of the transformer blocks\n",
        "        self.final_norm = nn.LayerNorm(config['token_embedding_dim'])\n",
        "\n",
        "        # Linear layer to project the output to the vocab size (logits)\n",
        "        self.out_head = nn.Linear(config['token_embedding_dim'], config['vocab_size'])\n",
        "\n",
        "    def forward(self, idx):\n",
        "        # Get the batch size and sequence length from the input\n",
        "        batch_size, seq_len = idx.shape\n",
        "\n",
        "        # Get token embeddings (batch_size, seq_len, token_embedding_dim)\n",
        "        token_embed = self.token_embed_lookup(idx)\n",
        "\n",
        "        # Create positional indices for the sequence (1, seq_len)\n",
        "        positional_indices = torch.arange(seq_len, device=idx.device).unsqueeze(0)  # Shape: (1, seq_len)\n",
        "\n",
        "        # Get positional embeddings (1, seq_len, token_embedding_dim)\n",
        "        positional_embed = self.positional_embed(positional_indices)\n",
        "\n",
        "        # Combine token embeddings and positional embeddings (batch_size, seq_len, token_embedding_dim)\n",
        "        x = token_embed + positional_embed\n",
        "\n",
        "        # Apply dropout to the combined embeddings\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass the embeddings through the transformer blocks (batch_size, seq_len, token_embedding_dim)\n",
        "        x = self.transformers(x)\n",
        "\n",
        "        # Apply final layer normalization (batch_size, seq_len, token_embedding_dim)\n",
        "        x = self.final_norm(x)\n",
        "\n",
        "        # Project the output of the transformer to logits (batch_size, seq_len, vocab_size)\n",
        "        logits = self.out_head(x)\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "OPjJKwUCFBn7"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=MyGPT(config)\n",
        "model.to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "SoYzwX25FBvv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e58d889-e765-4d78-ae87-723487aa38a2"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyGPT(\n",
              "  (token_embed_lookup): Embedding(50257, 768)\n",
              "  (positional_embed): Embedding(500, 768)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (transformers): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): Gelu()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): Gelu()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): Gelu()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): Gelu()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): Gelu()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): Gelu()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): Gelu()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (attention): MultiHeadAttention(\n",
              "        (w_q): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (w_k): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (w_v): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): Gelu()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model, max_len, idx, context_len):\n",
        "    model.eval()  # Set the model to evaluation mode (no gradient computation)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        # Crop the current context to use the last `context_len` tokens\n",
        "        idx_cond = idx[:, -context_len:]  # (batch_size, context_len)\n",
        "\n",
        "        with torch.no_grad():  # No gradient computation\n",
        "            logits = model(idx_cond)  # (batch_size, context_len, vocab_size)\n",
        "\n",
        "            # Focus on the last time step (batch_size, vocab_size)\n",
        "            logits = logits[:, -1, :]  # (batch_size, vocab_size)\n",
        "\n",
        "            # Apply softmax to get probabilities (batch_size, vocab_size)\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, vocab_size)\n",
        "\n",
        "            # Get the index of the highest probability (batch_size, 1)\n",
        "            idx_next = torch.argmax(probs, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "            # Append the new token index to the sequence (batch_size, n_tokens+1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, n_tokens+1)\n",
        "\n",
        "    return idx\n"
      ],
      "metadata": {
        "id": "lgX2BT4kFB1C"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "4JX2K3FJFB3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3c1877a-814f-4a39-e4c6-11940f4e4d1b"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "erqBAMgt62kB",
        "outputId": "2262ec7c-4447-4279-c802-7f96273d666b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you:\\ ScrollThat hengetgow unarmed flame Dresdenodied\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "\n",
        "# Initialize the GPT-2 tokenizer\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def text_to_token_ids(start_seq, tokenizer, device):\n",
        "    \"\"\"\n",
        "    Converts a text sequence into token IDs using the tokenizer.\n",
        "\n",
        "    Parameters:\n",
        "    - start_seq: The input text string to tokenize.\n",
        "    - tokenizer: The GPT-2 tokenizer instance.\n",
        "    - device: The device (CPU/GPU) to which the tensor will be moved.\n",
        "\n",
        "    Returns:\n",
        "    - encoded_tensor: A tensor containing token IDs for the input sequence,\n",
        "                      with a batch dimension added.\n",
        "    \"\"\"\n",
        "    # Encode the input text into token IDs.\n",
        "    # Special tokens like '<|endoftext|>' are allowed in the encoded output.\n",
        "    encoded = tokenizer.encode(start_seq, allowed_special={'<|endoftext|>'})\n",
        "    # Example: For `start_seq = \"Every effort moves you\"`, `encoded` might be [2130, 1015, 3187, 345, 345].\n",
        "\n",
        "    # Convert the list of token IDs into a PyTorch tensor and add a batch dimension.\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0).to(device)  # Shape: (1, len(encoded))\n",
        "    # Example Output: tensor([[2130, 1015, 3187, 345, 345]]) (on the specified device)\n",
        "\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    \"\"\"\n",
        "    Converts a tensor of token IDs back into text using the tokenizer.\n",
        "\n",
        "    Parameters:\n",
        "    - token_ids: A tensor of token IDs to decode.\n",
        "    - tokenizer: The GPT-2 tokenizer instance.\n",
        "\n",
        "    Returns:\n",
        "    - Decoded text string.\n",
        "    \"\"\"\n",
        "    # Remove the batch dimension by squeezing the tensor.\n",
        "    flat = token_ids.squeeze(0)  # Shape: (sequence length,)\n",
        "    # Example: If `token_ids = tensor([[2130, 1015, 3187, 345, 345]])`, `flat` is tensor([2130, 1015, 3187, 345, 345]).\n",
        "\n",
        "    # Decode the token IDs back into text.\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "    # Example Output: \"Every effort moves you\"\n",
        "\n",
        "# Example input text to generate from\n",
        "start_context = \"Every effort moves you\"\n",
        "\n",
        "# Convert the input text into token IDs and prepare for generation.\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_seq=start_context, tokenizer=tokenizer, device=device),  # Tokenized input\n",
        "    max_len=10,  # Maximum length of the generated sequence\n",
        "    context_len=config[\"context_len\"]  # Context length for the model\n",
        ")\n",
        "\n",
        "# Convert the generated token IDs back into text for readability.\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "lMOGrt5Q62kG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "file_path = \"the-verdict.txt\"\n",
        "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "        text_data = response.read().decode('utf-8')\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(text_data)\n",
        "else:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text_data = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader,  Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, tokenizer,max_len,stride):\n",
        "        self.text = text\n",
        "        self.tokenizer = tokenizer\n",
        "        idx=tokenizer.encode(text)\n",
        "        self.input_ids=[]\n",
        "        self.target_ids=[]\n",
        "        for i in range(0,len(idx)-max_len,stride):\n",
        "          self.input_ids.append(idx[i:i+max_len])\n",
        "          self.target_ids.append(idx[i+1:i+max_len+1])\n",
        "    def __getitem__(self, index):\n",
        "        return torch.tensor(self.input_ids[index]),torch.tensor(self.target_ids[index])\n",
        "    def __len__(self):\n",
        "      return len(self.input_ids)\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset =   TextDataset(text=txt, tokenizer=tokenizer, max_len=max_length, stride=stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader\n",
        "\n"
      ],
      "metadata": {
        "id": "FjJimhXzFCAO"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "oeh8phH962kI"
      },
      "outputs": [],
      "source": [
        "# Train/validation ratio\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=1,\n",
        "    max_length=config[\"context_len\"],\n",
        "    stride=config[\"context_len\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=1,\n",
        "    max_length=config[\"context_len\"],\n",
        "    stride=config[\"context_len\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss_batch(model, inputs, targets, device):\n",
        "    \"\"\"\n",
        "    Calculates the cross-entropy loss for a batch of inputs and targets.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The neural network model that takes inputs and outputs logits.\n",
        "    - inputs: The input data for the batch (e.g., shape: [B, T, ...]).\n",
        "    - targets: The ground truth labels for the batch (e.g., shape: [B, T]).\n",
        "    - device: The device (CPU or GPU) to which tensors should be moved.\n",
        "\n",
        "    Returns:\n",
        "    - loss: The computed cross-entropy loss for the batch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Move inputs and targets to the specified device (CPU or GPU).\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    # Pass inputs through the model to get logits.\n",
        "    # logits shape: (B, T, C), where B is batch size, T is sequence length, C is the number of classes.\n",
        "    logits = model(inputs)\n",
        "\n",
        "    # Compute the cross-entropy loss.\n",
        "    # - logits.flatten(0, 1): Flattens the batch and sequence dimensions to shape (B*T, C).\n",
        "    # - targets.flatten(0, 1): Flattens the batch and sequence dimensions to shape (B*T,).\n",
        "    # This ensures compatibility with CrossEntropyLoss, which expects inputs of shape (N, C) and (N,).\n",
        "    loss = nn.CrossEntropyLoss()(logits.flatten(0, 1), targets.flatten(0, 1))\n",
        "\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "bWnfRMnEyXuf"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_print_sample(model,tokenizer,start_seq,max_len=50,device=device):\n",
        "  model.eval()\n",
        "  encoded=text_to_token_ids(start_seq,tokenizer,device)\n",
        "  context_len=config[\"context_len\"]\n",
        "  with torch.no_grad():\n",
        "    token_ids = generate_text_simple(model, max_len, idx=encoded, context_len=context_len)\n",
        "  decoded=token_ids_to_text(token_ids,tokenizer)\n",
        "  print(decoded.replace(\"\\n\", \" \"))  # Compact print format\n",
        "  model.train()\n",
        ""
      ],
      "metadata": {
        "id": "yU2Mnnkm75oh"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss_loader(model,data_loader,device,batch_size):\n",
        "  total_loss = 0.\n",
        "  if batch_size is None:\n",
        "    batch_size=len(data_loader)\n",
        "  else:\n",
        "    batch_size=min(batch_size,len(data_loader))\n",
        "  for i,(inputs,targets) in enumerate(data_loader):\n",
        "    if i==batch_size:\n",
        "      break\n",
        "    total_loss+=calculate_loss_batch(model,inputs,targets,device)\n",
        "  return total_loss/batch_size"
      ],
      "metadata": {
        "id": "LGOc2FLr3vRT"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(model,train_loader,val_loader,device,eval_iter):\n",
        "  model.eval()\n",
        "  train_loss=calculate_loss_loader(model,train_loader,device,batch_size=eval_iter)\n",
        "  val_loss=calculate_loss_loader(model,val_loader,device,batch_size=eval_iter)\n",
        "  return train_loss,val_loss\n"
      ],
      "metadata": {
        "id": "UWBHOO1WQpXq"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training(model,epoch,train_loader,start_seq,optimizer,val_loader,device,eval_freq,eval_iter,tokenizer=tokenizer):\n",
        "  token_seen,global_step=0,0\n",
        "  train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "  for i in range(epoch):\n",
        "    model.train()\n",
        "    for inputs,targets in  train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      loss=calculate_loss_batch(model,inputs,targets,device)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      token_seen+=inputs.numel()\n",
        "      global_step += 1\n",
        "      if global_step % eval_freq:\n",
        "        train_loss,val_loss=evaluation(model,train_loader,val_loader,device,eval_iter)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        track_tokens_seen.append(token_seen)\n",
        "        print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "    generate_and_print_sample(\n",
        "        model, tokenizer, start_context\n",
        "        )\n",
        "\n",
        "  return train_losses, val_losses, track_tokens_seen\n",
        "\n"
      ],
      "metadata": {
        "id": "Rt644ANzQpVc"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "torch.manual_seed(123)\n",
        "\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 5\n",
        "train_losses, val_losses, tokens_seen = training(\n",
        "    model=model, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device,\n",
        "    epoch=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_seq=\"Every effort moves you\"\n",
        ")\n",
        "\n",
        "# Note:\n",
        "# Uncomment the following code to show the execution time\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "h_njHKPyQpbP",
        "outputId": "b1487bae-db64-4d21-b03d-abd0f721ca1e"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 3.06 MiB is free. Process 22276 has 14.74 GiB memory in use. Of the allocated memory 14.55 GiB is allocated by PyTorch, and 63.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-112-e37d4106f835>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m train_losses, val_losses, tokens_seen = training(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-109-a5f4331ccad7>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(model, epoch, train_loader, start_seq, optimizer, val_loader, device, eval_freq, eval_iter, tokenizer)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m  \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcalculate_loss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-105-039b1fffded4>\u001b[0m in \u001b[0;36mcalculate_loss_batch\u001b[0;34m(model, inputs, targets, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Pass inputs through the model to get logits.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# logits shape: (B, T, C), where B is batch size, T is sequence length, C is the number of classes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Compute the cross-entropy loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-96-41606c6432bb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Pass the embeddings through the transformer blocks (batch_size, seq_len, token_embedding_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Apply final layer normalization (batch_size, seq_len, token_embedding_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-95-0bc5fd951ce4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Apply feed-forward network with residual connection and layer normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mffn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, seq_len, embed_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mffn_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add residual connection + dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-93-4e955516fa07>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# x shape: (b, n_tokens, embed_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Apply the FeedForward network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Output shape: (b, n_tokens, embed_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-93-4e955516fa07>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# GELU activation function: 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.044715\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 3.06 MiB is free. Process 22276 has 14.74 GiB memory in use. Of the allocated memory 14.55 GiB is allocated by PyTorch, and 63.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q9fVku0VT89e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "1i7yJS2r62kP"
      },
      "outputs": [],
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    }
  ]
}
