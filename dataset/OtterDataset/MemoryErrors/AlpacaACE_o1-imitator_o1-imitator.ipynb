{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e335546-ea92-41cd-b012-ee14e24a025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a87079f5-41d5-4403-91e4-5b8bfa1ef165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将JSON文件转换为CSV文件\n",
    "df = pd.read_json('data/reflection_data.json')\n",
    "ds = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5aac7d4-2381-4352-941b-6ea5494f3e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2___5-7B', use_fast=False, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0971d094-ac59-4637-bd3f-6a183684701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 4096    # Llama分词器会将一个中文字切分为多个token，因此需要放开一些最大长度，保证数据的完整性\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(f\"<|im_start|>system\\n{example['system']}<|im_end|>\\n<|im_start|>user\\n{example['prompt']}<|im_end|>\\n<|im_start|>assistant\\n\", add_special_tokens=False)  # add_special_tokens 不在开头加 special_tokens\n",
    "    response = tokenizer(f\"{example['response']}\", add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]  # 因为eos token咱们也是要关注的所以 补充为1\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]  \n",
    "    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "023a858f-c73e-41c8-a90f-f78ff4a196ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9eb840d8004fe185d0726fc990733a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60121 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 60121\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_id = ds.map(process_func, remove_columns=ds.column_names)\n",
    "tokenized_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c04c91f1-2793-4c45-b037-d51f56cedcfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>system\\nYou are a world-class AI system capable of complex reasoning and reflection. You respond to all questions in the following way-\\n<thinking>\\nIn this section you understand the problem and develop a plan to solve the problem.\\n\\nFor easy problems-\\nMake a simple plan and use COT\\n\\nFor moderate to hard problems-\\n1. Devise a step-by-step plan to solve the problem. (don't actually start solving yet, just make a plan)\\n2. Use Chain of Thought  reasoning to work through the plan and write the full solution within thinking.\\n\\nYou can use <reflection> </reflection> tags whenever you execute a complex step to verify if your reasoning is correct and if not correct it.\\n\\n\\n</thinking>\\n\\n<output>\\nIn this section, provide the complete answer for the user based on your thinking process. Do not refer to the thinking tag. Include all relevant information and keep the response somewhat verbose, the user will not see what is in the thinking tag.\\n</output><|im_end|>\\n<|im_start|>user\\nYou observe a black hole's event horizon using advanced telescopes and sensors. Reverse engineer the process of how this black hole formed and evolved to its current state, considering astrophysical phenomena and theoretical physics concepts.<|im_end|>\\n<|im_start|>assistant\\n<thinking>\\nThis is a complex problem that requires a deep understanding of astrophysics, general relativity, and the evolution of massive celestial objects. Let's devise a step-by-step plan to reverse engineer the formation and evolution of this black hole:\\n\\n1. Analyze the current state of the black hole\\n2. Determine the black hole's mass and spin\\n3. Estimate the age of the black hole\\n4. Identify the progenitor star or stellar system\\n5. Trace the evolution of the progenitor\\n6. Model the collapse and black hole formation\\n7. Consider post-formation growth and evolution\\n\\nNow, let's work through this plan using Chain of Thought reasoning:\\n\\n1. Analyzing the current state:\\nThe event horizon is the boundary beyond which nothing can escape the black hole's gravitational pull. Its size is directly related to the black hole's mass. We can use the observed event horizon to estimate the black hole's Schwarzschild radius.\\n\\n2. Determining mass and spin:\\nUsing the Schwarzschild radius, we can calculate the black hole's mass. The shape of the event horizon and surrounding accretion disk (if present) can give us information about the black hole's spin.\\n\\n<reflection>\\nThe approach so far is correct. We should also consider using X-ray observations to estimate the spin more accurately, as the X-ray spectrum can provide information about the innermost stable circular orbit.\\n</reflection>\\n\\n3. Estimating age:\\nWe can estimate the age of the black hole by analyzing the surrounding stellar population and the galactic environment. Older black holes are more likely to be found in older galaxies or globular clusters.\\n\\n4. Identifying the progenitor:\\nBased on the mass of the black hole, we can determine if it's a stellar-mass black hole (formed from a single star), an intermediate-mass black hole (possibly formed from multiple stars or in dense stellar clusters), or a supermassive black hole (formed in the early universe or through mergers).\\n\\n5. Tracing progenitor evolution:\\nFor a stellar-mass black hole, we model the evolution of a massive star, considering factors like initial mass, metallicity, and stellar wind mass loss. For larger black holes, we consider scenarios involving multiple stellar collapses or primordial black hole formation in the early universe.\\n\\n6. Modeling collapse and formation:\\nWe simulate the final stages of the progenitor's life, including the supernova explosion (for stellar-mass black holes) or direct collapse (for very massive stars). We use general relativity to model the formation of the event horizon.\\n\\n7. Post-formation growth and evolution:\\nWe consider processes like accretion of surrounding matter, mergers with other black holes, and interactions with the galactic environment that could have contributed to the black hole's current state.\\n\\n<reflection>\\nOur analysis seems comprehensive, but we should also consider the possibility of primordial black holes formed in the early universe, especially for intermediate-mass black holes. Additionally, we should mention the potential impact of dark matter on black hole evolution, as it could affect the growth rate and distribution of black holes in galaxies.\\n</reflection>\\n\\n</thinking>\\n\\n<output>\\nBased on the observation of the black hole's event horizon using advanced telescopes and sensors, we can reverse engineer its formation and evolution process as follows:\\n\\n1. Current State Analysis: The observed event horizon provides crucial information about the black hole's size and, consequently, its mass. The event horizon is the boundary beyond which nothing can escape the black hole's gravitational pull, and its size is directly related to the black hole's Schwarzschild radius.\\n\\n2. Mass and Spin Determination: Using the Schwarzschild radius derived from the event horizon observations, we can calculate the black hole's mass. The shape of the event horizon and any surrounding accretion disk can give us information about the black hole's spin. Additionally, X-ray observations can provide more accurate spin estimates by analyzing the innermost stable circular orbit.\\n\\n3. Age Estimation: The age of the black hole can be estimated by studying the surrounding stellar population and galactic environment. Older black holes are typically found in older galaxies or globular clusters.\\n\\n4. Progenitor Identification: Based on the calculated mass, we can determine whether this is a stellar-mass black hole (formed from a single star), an intermediate-mass black hole (possibly formed from multiple stars or in dense stellar clusters), or a supermassive black hole (formed in the early universe or through mergers).\\n\\n5. Progenitor Evolution: For stellar-mass black holes, we model the evolution of a massive star, considering factors like initial mass, metallicity, and stellar wind mass loss. For larger black holes, we consider scenarios involving multiple stellar collapses or even primordial black hole formation in the early universe.\\n\\n6. Collapse and Formation Modeling: We simulate the final stages of the progenitor's life, which could include a supernova explosion for stellar-mass black holes or direct collapse for very massive stars. Using general relativity, we model the formation of the event horizon and the birth of the black hole.\\n\\n7. Post-formation Growth and Evolution: We consider various processes that could have contributed to the black hole's current state, such as:\\n   - Accretion of surrounding matter\\n   - Mergers with other black holes\\n   - Interactions with the galactic environment\\n   - Potential influence of dark matter on the black hole's growth and distribution within the galaxy\\n\\nIt's important to note that this reverse engineering process involves a combination of observational data, theoretical models, and computer simulations. The exact formation and evolution path of any individual black hole can be complex and may involve multiple scenarios.\\n\\nFor intermediate-mass black holes, we also consider the possibility of primordial black holes formed in the early universe. These ancient black holes could have formed from density fluctuations shortly after the Big Bang and grown over time through accretion and mergers.\\n\\nThroughout the black hole's lifetime, its evolution would have been influenced by its environment, including interactions with nearby stars, gas clouds, and other black holes. The presence of dark matter in the galaxy could also play a role in the black hole's growth rate and its distribution within the galactic structure.\\n\\nBy combining all these factors and using advanced computational models, we can reconstruct a probable formation and evolution scenario for the observed black hole. However, it's crucial to remember that this process involves some uncertainties, and multiple formation pathways may be possible for any given black hole.\\n\\n</output><|endoftext|>\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_id[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f368c66-41ef-424b-8fb8-64732079fa50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<thinking>\\nTo address this complex question, I'll need to develop a structured approach and consider multiple factors. Let's break this down into steps:\\n\\n1. Understand the premise: A text-based internet without easy image/video sharing\\n2. Consider the key factors: Technological, social, and economic\\n3. Develop three potential alternative outcomes\\n4. Provide reasoning for each scenario\\n\\nLet's start with developing the three scenarios:\\n\\nScenario 1: Text-Centric Social Networks\\nScenario 2: Audio-Focused Social Media\\nScenario 3: Virtual Text Worlds\\n\\nNow, let's explore each scenario in detail:\\n\\nScenario 1: Text-Centric Social Networks\\n- Social media platforms would focus on text-based communication\\n- Emphasis on writing skills and articulation\\n- Rise of microblogging and collaborative writing platforms\\n\\nReasoning:\\nWithout easy image and video sharing, social media would likely evolve to prioritize text-based content. This could lead to a greater emphasis on writing skills and articulation. Platforms like Twitter might still exist but would be even more text-focused. We might see the rise of collaborative writing platforms and more sophisticated microblogging services.\\n\\n<reflection>\\nThis scenario seems plausible, but I should consider the economic implications more deeply. How would advertising work in a text-only environment?\\n</reflection>\\n\\nAdding to Scenario 1:\\n- Advertising would rely heavily on targeted text ads and sponsored content\\n- Companies might invest more in content marketing and SEO strategies\\n\\nScenario 2: Audio-Focused Social Media\\n- Development of advanced audio compression and streaming technologies\\n- Rise of podcasting and voice-based social networks\\n- Increased importance of voice recognition and text-to-speech technologies\\n\\nReasoning:\\nWithout the ability to easily share images and videos, there might be a greater push towards audio content as an alternative rich media format. This could lead to earlier and more advanced development of audio compression and streaming technologies. Podcasting might become the dominant form of content creation and consumption on social media platforms.\\n\\n<reflection>\\nThis scenario addresses the technological aspect well, but I should expand on the social implications.\\n</reflection>\\n\\nAdding to Scenario 2:\\n- Potential for increased accessibility for visually impaired users\\n- Development of new forms of audio-based art and entertainment\\n- Possible challenges in moderating audio content compared to text\\n\\nScenario 3: Virtual Text Worlds\\n- Evolution of text-based virtual reality environments\\n- Development of sophisticated text-based role-playing games and social spaces\\n- Emergence of new forms of digital literature and interactive storytelling\\n\\nReasoning:\\nIn the absence of visual media, there might be a push towards creating more immersive text-based experiences. This could lead to the development of sophisticated virtual text worlds, similar to early MUDs (Multi-User Dungeons) but far more advanced. These platforms could become the primary form of social interaction online, combining elements of gaming, social networking, and collaborative storytelling.\\n\\n<reflection>\\nThis scenario covers technological and social aspects well. I should consider the economic implications more thoroughly.\\n</reflection>\\n\\nAdding to Scenario 3:\\n- New economic models based on virtual goods and services within text worlds\\n- Potential for blockchain integration for ownership and transactions in virtual spaces\\n- Rise of professional virtual world designers and storytellers\\n\\n</thinking>\\n\\n<output>\\nIf the internet had remained primarily text-based, without the ability to easily share images and videos, the development and adoption of social media could have unfolded quite differently. Here are three potential alternative outcomes, considering technological, social, and economic factors:\\n\\n1. Text-Centric Social Networks\\n\\nIn this scenario, social media platforms would have evolved to focus primarily on text-based communication. We would likely see a greater emphasis on writing skills and articulation, with platforms prioritizing the quality and impact of written content. Microblogging services like Twitter might still exist but would be even more text-focused, possibly allowing for longer-form posts or more sophisticated threading mechanisms.\\n\\nCollaborative writing platforms could become a dominant form of social interaction, where users collectively create stories, articles, or even books. This environment would likely lead to the development of more advanced natural language processing tools to help users navigate and engage with vast amounts of text content.\\n\\nEconomically, advertising would rely heavily on targeted text ads and sponsored content. Companies would likely invest more in content marketing and SEO strategies to ensure their messages reach the right audiences in a text-dominated landscape.\\n\\n2. Audio-Focused Social Media\\n\\nWithout easy image and video sharing, there might be a significant push towards audio content as an alternative rich media format. This scenario could lead to the earlier and more advanced development of audio compression and streaming technologies. Podcasting might become the dominant form of content creation and consumption on social media platforms, with users sharing audio clips, voice messages, and longer-form audio content.\\n\\nWe might see the rise of voice-based social networks, where users interact primarily through speech. This could drive increased investment and innovation in voice recognition and text-to-speech technologies, making these platforms more accessible and user-friendly.\\n\\nThis audio-centric approach could have significant social implications, potentially increasing accessibility for visually impaired users and leading to the development of new forms of audio-based art and entertainment. However, it might also present challenges in content moderation compared to text-based platforms.\\n\\nEconomically, this could lead to new monetization strategies centered around audio content, such as premium podcasts, audio-based advertising, and voice-activated commerce.\\n\\n3. Virtual Text Worlds\\n\\nIn the absence of visual media, there might be a push towards creating more immersive text-based experiences. This could lead to the development of sophisticated virtual text worlds, evolving from early Multi-User Dungeons (MUDs) into complex, persistent online environments.\\n\\nThese platforms could become the primary form of social interaction online, combining elements of gaming, social networking, and collaborative storytelling. Users might create avatars described in text and interact in richly detailed, text-based environments. This could spark a renaissance in digital literature and interactive storytelling, with new forms of narrative emerging that blur the lines between traditional writing, game design, and social interaction.\\n\\nEconomically, this scenario could give rise to new models based on virtual goods and services within these text worlds. We might see the integration of blockchain technology for ownership and transactions in virtual spaces. There could also be a surge in demand for professional virtual world designers and storytellers, creating new job markets and economic opportunities.\\n\\nEach of these scenarios represents a significant departure from our current image and video-dominated social media landscape, emphasizing different aspects of human communication and creativity. They also highlight how technological limitations can drive innovation in unexpected directions, potentially leading to richer, more diverse forms of online interaction.\\n\\n</output><|endoftext|>\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(list(filter(lambda x: x != -100, tokenized_id[1][\"labels\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "693028d4-b630-4c08-9d46-2aab914cfefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9811b96c80da4d82b52d653768703242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 是否在4位精度下加载模型。如果设置为True，则在4位精度下加载模型。\n",
    "    bnb_4bit_compute_dtype=torch.half,  # 4位精度计算的数据类型。这里设置为torch.half，表示使用半精度浮点数。\n",
    "    bnb_4bit_quant_type=\"nf4\", # 4位精度量化的类型。这里设置为\"nf4\"，表示使用nf4量化类型。\n",
    "    bnb_4bit_use_double_quant=True  # 是否使用双精度量化。如果设置为True，则使用双精度量化。\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        'Qwen/Qwen2___5-7B', \n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        quantization_config=bnb_config, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f372b26c-7c5e-4246-a784-d9702e6cf0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([152064, 3584]) torch.bfloat16\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.0.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.0.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.0.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.0.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.1.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.1.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.1.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.1.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.2.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.2.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.2.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.2.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.3.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.3.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.3.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.3.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.4.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.4.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.4.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.4.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.5.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.5.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.5.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.5.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.6.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.6.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.6.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.6.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.7.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.7.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.7.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.7.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.8.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.8.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.8.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.8.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.9.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.9.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.9.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.9.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.10.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.10.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.10.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.10.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.11.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.11.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.11.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.11.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.12.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.12.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.12.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.12.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.13.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.13.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.13.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.13.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.14.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.14.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.14.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.14.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.15.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.15.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.15.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.15.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.16.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.16.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.16.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.16.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.17.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.17.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.17.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.17.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.18.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.18.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.18.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.18.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.19.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.19.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.19.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.19.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.20.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.20.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.20.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.20.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.21.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.21.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.21.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.21.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.22.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.22.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.22.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.22.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.23.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.23.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.23.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.23.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.24.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.24.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.24.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.24.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.25.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.25.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.25.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.25.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.26.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.26.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.26.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.26.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.26.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.26.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.27.self_attn.q_proj.bias torch.Size([3584]) torch.bfloat16\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.27.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([917504, 1]) torch.uint8\n",
      "model.layers.27.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([6422528, 1]) torch.uint8\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([33947648, 1]) torch.uint8\n",
      "model.layers.27.input_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([3584]) torch.bfloat16\n",
      "model.norm.weight torch.Size([3584]) torch.bfloat16\n",
      "lm_head.weight torch.Size([152064, 3584]) torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# 查看各个层的最终量化后的精度\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d667bd0-002e-4ae5-a60e-64b59707055a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight\n",
      "model.layers.0.self_attn.q_proj.weight\n",
      "model.layers.0.self_attn.q_proj.bias\n",
      "model.layers.0.self_attn.k_proj.weight\n",
      "model.layers.0.self_attn.k_proj.bias\n",
      "model.layers.0.self_attn.v_proj.weight\n",
      "model.layers.0.self_attn.v_proj.bias\n",
      "model.layers.0.self_attn.o_proj.weight\n",
      "model.layers.0.mlp.gate_proj.weight\n",
      "model.layers.0.mlp.up_proj.weight\n",
      "model.layers.0.mlp.down_proj.weight\n",
      "model.layers.0.input_layernorm.weight\n",
      "model.layers.0.post_attention_layernorm.weight\n",
      "model.layers.1.self_attn.q_proj.weight\n",
      "model.layers.1.self_attn.q_proj.bias\n",
      "model.layers.1.self_attn.k_proj.weight\n",
      "model.layers.1.self_attn.k_proj.bias\n",
      "model.layers.1.self_attn.v_proj.weight\n",
      "model.layers.1.self_attn.v_proj.bias\n",
      "model.layers.1.self_attn.o_proj.weight\n",
      "model.layers.1.mlp.gate_proj.weight\n",
      "model.layers.1.mlp.up_proj.weight\n",
      "model.layers.1.mlp.down_proj.weight\n",
      "model.layers.1.input_layernorm.weight\n",
      "model.layers.1.post_attention_layernorm.weight\n",
      "model.layers.2.self_attn.q_proj.weight\n",
      "model.layers.2.self_attn.q_proj.bias\n",
      "model.layers.2.self_attn.k_proj.weight\n",
      "model.layers.2.self_attn.k_proj.bias\n",
      "model.layers.2.self_attn.v_proj.weight\n",
      "model.layers.2.self_attn.v_proj.bias\n",
      "model.layers.2.self_attn.o_proj.weight\n",
      "model.layers.2.mlp.gate_proj.weight\n",
      "model.layers.2.mlp.up_proj.weight\n",
      "model.layers.2.mlp.down_proj.weight\n",
      "model.layers.2.input_layernorm.weight\n",
      "model.layers.2.post_attention_layernorm.weight\n",
      "model.layers.3.self_attn.q_proj.weight\n",
      "model.layers.3.self_attn.q_proj.bias\n",
      "model.layers.3.self_attn.k_proj.weight\n",
      "model.layers.3.self_attn.k_proj.bias\n",
      "model.layers.3.self_attn.v_proj.weight\n",
      "model.layers.3.self_attn.v_proj.bias\n",
      "model.layers.3.self_attn.o_proj.weight\n",
      "model.layers.3.mlp.gate_proj.weight\n",
      "model.layers.3.mlp.up_proj.weight\n",
      "model.layers.3.mlp.down_proj.weight\n",
      "model.layers.3.input_layernorm.weight\n",
      "model.layers.3.post_attention_layernorm.weight\n",
      "model.layers.4.self_attn.q_proj.weight\n",
      "model.layers.4.self_attn.q_proj.bias\n",
      "model.layers.4.self_attn.k_proj.weight\n",
      "model.layers.4.self_attn.k_proj.bias\n",
      "model.layers.4.self_attn.v_proj.weight\n",
      "model.layers.4.self_attn.v_proj.bias\n",
      "model.layers.4.self_attn.o_proj.weight\n",
      "model.layers.4.mlp.gate_proj.weight\n",
      "model.layers.4.mlp.up_proj.weight\n",
      "model.layers.4.mlp.down_proj.weight\n",
      "model.layers.4.input_layernorm.weight\n",
      "model.layers.4.post_attention_layernorm.weight\n",
      "model.layers.5.self_attn.q_proj.weight\n",
      "model.layers.5.self_attn.q_proj.bias\n",
      "model.layers.5.self_attn.k_proj.weight\n",
      "model.layers.5.self_attn.k_proj.bias\n",
      "model.layers.5.self_attn.v_proj.weight\n",
      "model.layers.5.self_attn.v_proj.bias\n",
      "model.layers.5.self_attn.o_proj.weight\n",
      "model.layers.5.mlp.gate_proj.weight\n",
      "model.layers.5.mlp.up_proj.weight\n",
      "model.layers.5.mlp.down_proj.weight\n",
      "model.layers.5.input_layernorm.weight\n",
      "model.layers.5.post_attention_layernorm.weight\n",
      "model.layers.6.self_attn.q_proj.weight\n",
      "model.layers.6.self_attn.q_proj.bias\n",
      "model.layers.6.self_attn.k_proj.weight\n",
      "model.layers.6.self_attn.k_proj.bias\n",
      "model.layers.6.self_attn.v_proj.weight\n",
      "model.layers.6.self_attn.v_proj.bias\n",
      "model.layers.6.self_attn.o_proj.weight\n",
      "model.layers.6.mlp.gate_proj.weight\n",
      "model.layers.6.mlp.up_proj.weight\n",
      "model.layers.6.mlp.down_proj.weight\n",
      "model.layers.6.input_layernorm.weight\n",
      "model.layers.6.post_attention_layernorm.weight\n",
      "model.layers.7.self_attn.q_proj.weight\n",
      "model.layers.7.self_attn.q_proj.bias\n",
      "model.layers.7.self_attn.k_proj.weight\n",
      "model.layers.7.self_attn.k_proj.bias\n",
      "model.layers.7.self_attn.v_proj.weight\n",
      "model.layers.7.self_attn.v_proj.bias\n",
      "model.layers.7.self_attn.o_proj.weight\n",
      "model.layers.7.mlp.gate_proj.weight\n",
      "model.layers.7.mlp.up_proj.weight\n",
      "model.layers.7.mlp.down_proj.weight\n",
      "model.layers.7.input_layernorm.weight\n",
      "model.layers.7.post_attention_layernorm.weight\n",
      "model.layers.8.self_attn.q_proj.weight\n",
      "model.layers.8.self_attn.q_proj.bias\n",
      "model.layers.8.self_attn.k_proj.weight\n",
      "model.layers.8.self_attn.k_proj.bias\n",
      "model.layers.8.self_attn.v_proj.weight\n",
      "model.layers.8.self_attn.v_proj.bias\n",
      "model.layers.8.self_attn.o_proj.weight\n",
      "model.layers.8.mlp.gate_proj.weight\n",
      "model.layers.8.mlp.up_proj.weight\n",
      "model.layers.8.mlp.down_proj.weight\n",
      "model.layers.8.input_layernorm.weight\n",
      "model.layers.8.post_attention_layernorm.weight\n",
      "model.layers.9.self_attn.q_proj.weight\n",
      "model.layers.9.self_attn.q_proj.bias\n",
      "model.layers.9.self_attn.k_proj.weight\n",
      "model.layers.9.self_attn.k_proj.bias\n",
      "model.layers.9.self_attn.v_proj.weight\n",
      "model.layers.9.self_attn.v_proj.bias\n",
      "model.layers.9.self_attn.o_proj.weight\n",
      "model.layers.9.mlp.gate_proj.weight\n",
      "model.layers.9.mlp.up_proj.weight\n",
      "model.layers.9.mlp.down_proj.weight\n",
      "model.layers.9.input_layernorm.weight\n",
      "model.layers.9.post_attention_layernorm.weight\n",
      "model.layers.10.self_attn.q_proj.weight\n",
      "model.layers.10.self_attn.q_proj.bias\n",
      "model.layers.10.self_attn.k_proj.weight\n",
      "model.layers.10.self_attn.k_proj.bias\n",
      "model.layers.10.self_attn.v_proj.weight\n",
      "model.layers.10.self_attn.v_proj.bias\n",
      "model.layers.10.self_attn.o_proj.weight\n",
      "model.layers.10.mlp.gate_proj.weight\n",
      "model.layers.10.mlp.up_proj.weight\n",
      "model.layers.10.mlp.down_proj.weight\n",
      "model.layers.10.input_layernorm.weight\n",
      "model.layers.10.post_attention_layernorm.weight\n",
      "model.layers.11.self_attn.q_proj.weight\n",
      "model.layers.11.self_attn.q_proj.bias\n",
      "model.layers.11.self_attn.k_proj.weight\n",
      "model.layers.11.self_attn.k_proj.bias\n",
      "model.layers.11.self_attn.v_proj.weight\n",
      "model.layers.11.self_attn.v_proj.bias\n",
      "model.layers.11.self_attn.o_proj.weight\n",
      "model.layers.11.mlp.gate_proj.weight\n",
      "model.layers.11.mlp.up_proj.weight\n",
      "model.layers.11.mlp.down_proj.weight\n",
      "model.layers.11.input_layernorm.weight\n",
      "model.layers.11.post_attention_layernorm.weight\n",
      "model.layers.12.self_attn.q_proj.weight\n",
      "model.layers.12.self_attn.q_proj.bias\n",
      "model.layers.12.self_attn.k_proj.weight\n",
      "model.layers.12.self_attn.k_proj.bias\n",
      "model.layers.12.self_attn.v_proj.weight\n",
      "model.layers.12.self_attn.v_proj.bias\n",
      "model.layers.12.self_attn.o_proj.weight\n",
      "model.layers.12.mlp.gate_proj.weight\n",
      "model.layers.12.mlp.up_proj.weight\n",
      "model.layers.12.mlp.down_proj.weight\n",
      "model.layers.12.input_layernorm.weight\n",
      "model.layers.12.post_attention_layernorm.weight\n",
      "model.layers.13.self_attn.q_proj.weight\n",
      "model.layers.13.self_attn.q_proj.bias\n",
      "model.layers.13.self_attn.k_proj.weight\n",
      "model.layers.13.self_attn.k_proj.bias\n",
      "model.layers.13.self_attn.v_proj.weight\n",
      "model.layers.13.self_attn.v_proj.bias\n",
      "model.layers.13.self_attn.o_proj.weight\n",
      "model.layers.13.mlp.gate_proj.weight\n",
      "model.layers.13.mlp.up_proj.weight\n",
      "model.layers.13.mlp.down_proj.weight\n",
      "model.layers.13.input_layernorm.weight\n",
      "model.layers.13.post_attention_layernorm.weight\n",
      "model.layers.14.self_attn.q_proj.weight\n",
      "model.layers.14.self_attn.q_proj.bias\n",
      "model.layers.14.self_attn.k_proj.weight\n",
      "model.layers.14.self_attn.k_proj.bias\n",
      "model.layers.14.self_attn.v_proj.weight\n",
      "model.layers.14.self_attn.v_proj.bias\n",
      "model.layers.14.self_attn.o_proj.weight\n",
      "model.layers.14.mlp.gate_proj.weight\n",
      "model.layers.14.mlp.up_proj.weight\n",
      "model.layers.14.mlp.down_proj.weight\n",
      "model.layers.14.input_layernorm.weight\n",
      "model.layers.14.post_attention_layernorm.weight\n",
      "model.layers.15.self_attn.q_proj.weight\n",
      "model.layers.15.self_attn.q_proj.bias\n",
      "model.layers.15.self_attn.k_proj.weight\n",
      "model.layers.15.self_attn.k_proj.bias\n",
      "model.layers.15.self_attn.v_proj.weight\n",
      "model.layers.15.self_attn.v_proj.bias\n",
      "model.layers.15.self_attn.o_proj.weight\n",
      "model.layers.15.mlp.gate_proj.weight\n",
      "model.layers.15.mlp.up_proj.weight\n",
      "model.layers.15.mlp.down_proj.weight\n",
      "model.layers.15.input_layernorm.weight\n",
      "model.layers.15.post_attention_layernorm.weight\n",
      "model.layers.16.self_attn.q_proj.weight\n",
      "model.layers.16.self_attn.q_proj.bias\n",
      "model.layers.16.self_attn.k_proj.weight\n",
      "model.layers.16.self_attn.k_proj.bias\n",
      "model.layers.16.self_attn.v_proj.weight\n",
      "model.layers.16.self_attn.v_proj.bias\n",
      "model.layers.16.self_attn.o_proj.weight\n",
      "model.layers.16.mlp.gate_proj.weight\n",
      "model.layers.16.mlp.up_proj.weight\n",
      "model.layers.16.mlp.down_proj.weight\n",
      "model.layers.16.input_layernorm.weight\n",
      "model.layers.16.post_attention_layernorm.weight\n",
      "model.layers.17.self_attn.q_proj.weight\n",
      "model.layers.17.self_attn.q_proj.bias\n",
      "model.layers.17.self_attn.k_proj.weight\n",
      "model.layers.17.self_attn.k_proj.bias\n",
      "model.layers.17.self_attn.v_proj.weight\n",
      "model.layers.17.self_attn.v_proj.bias\n",
      "model.layers.17.self_attn.o_proj.weight\n",
      "model.layers.17.mlp.gate_proj.weight\n",
      "model.layers.17.mlp.up_proj.weight\n",
      "model.layers.17.mlp.down_proj.weight\n",
      "model.layers.17.input_layernorm.weight\n",
      "model.layers.17.post_attention_layernorm.weight\n",
      "model.layers.18.self_attn.q_proj.weight\n",
      "model.layers.18.self_attn.q_proj.bias\n",
      "model.layers.18.self_attn.k_proj.weight\n",
      "model.layers.18.self_attn.k_proj.bias\n",
      "model.layers.18.self_attn.v_proj.weight\n",
      "model.layers.18.self_attn.v_proj.bias\n",
      "model.layers.18.self_attn.o_proj.weight\n",
      "model.layers.18.mlp.gate_proj.weight\n",
      "model.layers.18.mlp.up_proj.weight\n",
      "model.layers.18.mlp.down_proj.weight\n",
      "model.layers.18.input_layernorm.weight\n",
      "model.layers.18.post_attention_layernorm.weight\n",
      "model.layers.19.self_attn.q_proj.weight\n",
      "model.layers.19.self_attn.q_proj.bias\n",
      "model.layers.19.self_attn.k_proj.weight\n",
      "model.layers.19.self_attn.k_proj.bias\n",
      "model.layers.19.self_attn.v_proj.weight\n",
      "model.layers.19.self_attn.v_proj.bias\n",
      "model.layers.19.self_attn.o_proj.weight\n",
      "model.layers.19.mlp.gate_proj.weight\n",
      "model.layers.19.mlp.up_proj.weight\n",
      "model.layers.19.mlp.down_proj.weight\n",
      "model.layers.19.input_layernorm.weight\n",
      "model.layers.19.post_attention_layernorm.weight\n",
      "model.layers.20.self_attn.q_proj.weight\n",
      "model.layers.20.self_attn.q_proj.bias\n",
      "model.layers.20.self_attn.k_proj.weight\n",
      "model.layers.20.self_attn.k_proj.bias\n",
      "model.layers.20.self_attn.v_proj.weight\n",
      "model.layers.20.self_attn.v_proj.bias\n",
      "model.layers.20.self_attn.o_proj.weight\n",
      "model.layers.20.mlp.gate_proj.weight\n",
      "model.layers.20.mlp.up_proj.weight\n",
      "model.layers.20.mlp.down_proj.weight\n",
      "model.layers.20.input_layernorm.weight\n",
      "model.layers.20.post_attention_layernorm.weight\n",
      "model.layers.21.self_attn.q_proj.weight\n",
      "model.layers.21.self_attn.q_proj.bias\n",
      "model.layers.21.self_attn.k_proj.weight\n",
      "model.layers.21.self_attn.k_proj.bias\n",
      "model.layers.21.self_attn.v_proj.weight\n",
      "model.layers.21.self_attn.v_proj.bias\n",
      "model.layers.21.self_attn.o_proj.weight\n",
      "model.layers.21.mlp.gate_proj.weight\n",
      "model.layers.21.mlp.up_proj.weight\n",
      "model.layers.21.mlp.down_proj.weight\n",
      "model.layers.21.input_layernorm.weight\n",
      "model.layers.21.post_attention_layernorm.weight\n",
      "model.layers.22.self_attn.q_proj.weight\n",
      "model.layers.22.self_attn.q_proj.bias\n",
      "model.layers.22.self_attn.k_proj.weight\n",
      "model.layers.22.self_attn.k_proj.bias\n",
      "model.layers.22.self_attn.v_proj.weight\n",
      "model.layers.22.self_attn.v_proj.bias\n",
      "model.layers.22.self_attn.o_proj.weight\n",
      "model.layers.22.mlp.gate_proj.weight\n",
      "model.layers.22.mlp.up_proj.weight\n",
      "model.layers.22.mlp.down_proj.weight\n",
      "model.layers.22.input_layernorm.weight\n",
      "model.layers.22.post_attention_layernorm.weight\n",
      "model.layers.23.self_attn.q_proj.weight\n",
      "model.layers.23.self_attn.q_proj.bias\n",
      "model.layers.23.self_attn.k_proj.weight\n",
      "model.layers.23.self_attn.k_proj.bias\n",
      "model.layers.23.self_attn.v_proj.weight\n",
      "model.layers.23.self_attn.v_proj.bias\n",
      "model.layers.23.self_attn.o_proj.weight\n",
      "model.layers.23.mlp.gate_proj.weight\n",
      "model.layers.23.mlp.up_proj.weight\n",
      "model.layers.23.mlp.down_proj.weight\n",
      "model.layers.23.input_layernorm.weight\n",
      "model.layers.23.post_attention_layernorm.weight\n",
      "model.layers.24.self_attn.q_proj.weight\n",
      "model.layers.24.self_attn.q_proj.bias\n",
      "model.layers.24.self_attn.k_proj.weight\n",
      "model.layers.24.self_attn.k_proj.bias\n",
      "model.layers.24.self_attn.v_proj.weight\n",
      "model.layers.24.self_attn.v_proj.bias\n",
      "model.layers.24.self_attn.o_proj.weight\n",
      "model.layers.24.mlp.gate_proj.weight\n",
      "model.layers.24.mlp.up_proj.weight\n",
      "model.layers.24.mlp.down_proj.weight\n",
      "model.layers.24.input_layernorm.weight\n",
      "model.layers.24.post_attention_layernorm.weight\n",
      "model.layers.25.self_attn.q_proj.weight\n",
      "model.layers.25.self_attn.q_proj.bias\n",
      "model.layers.25.self_attn.k_proj.weight\n",
      "model.layers.25.self_attn.k_proj.bias\n",
      "model.layers.25.self_attn.v_proj.weight\n",
      "model.layers.25.self_attn.v_proj.bias\n",
      "model.layers.25.self_attn.o_proj.weight\n",
      "model.layers.25.mlp.gate_proj.weight\n",
      "model.layers.25.mlp.up_proj.weight\n",
      "model.layers.25.mlp.down_proj.weight\n",
      "model.layers.25.input_layernorm.weight\n",
      "model.layers.25.post_attention_layernorm.weight\n",
      "model.layers.26.self_attn.q_proj.weight\n",
      "model.layers.26.self_attn.q_proj.bias\n",
      "model.layers.26.self_attn.k_proj.weight\n",
      "model.layers.26.self_attn.k_proj.bias\n",
      "model.layers.26.self_attn.v_proj.weight\n",
      "model.layers.26.self_attn.v_proj.bias\n",
      "model.layers.26.self_attn.o_proj.weight\n",
      "model.layers.26.mlp.gate_proj.weight\n",
      "model.layers.26.mlp.up_proj.weight\n",
      "model.layers.26.mlp.down_proj.weight\n",
      "model.layers.26.input_layernorm.weight\n",
      "model.layers.26.post_attention_layernorm.weight\n",
      "model.layers.27.self_attn.q_proj.weight\n",
      "model.layers.27.self_attn.q_proj.bias\n",
      "model.layers.27.self_attn.k_proj.weight\n",
      "model.layers.27.self_attn.k_proj.bias\n",
      "model.layers.27.self_attn.v_proj.weight\n",
      "model.layers.27.self_attn.v_proj.bias\n",
      "model.layers.27.self_attn.o_proj.weight\n",
      "model.layers.27.mlp.gate_proj.weight\n",
      "model.layers.27.mlp.up_proj.weight\n",
      "model.layers.27.mlp.down_proj.weight\n",
      "model.layers.27.input_layernorm.weight\n",
      "model.layers.27.post_attention_layernorm.weight\n",
      "model.norm.weight\n",
      "lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "# 配置LoRA时,打印model所有参数层名,方便观察target_modules\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "775633e1-8d8f-4471-841e-5fd409359236",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c870a5df-d248-471b-a812-0c41ff14bf89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1e22b6b-0e65-42b7-87b2-4dd55aabfbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False, # 训练模式\n",
    "    r=8, # Lora 秩\n",
    "    lora_alpha=32, # Lora alaph，具体作用参见 Lora 原理\n",
    "    lora_dropout=0.1 # Dropout 比例\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c02ed739-19f4-4435-b9e9-473801188639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf1ba983-eb8c-419e-b9b3-4452a9d1cdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./output/Qwen2.5_instruct_lora\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=500, \n",
    "    learning_rate=1e-4,\n",
    "    save_on_each_node=True,\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_id,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71ecf108-8c62-45bd-a51c-17ceba6f2955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='266' max='11271' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  266/11271 51:00 < 35:26:08, 0.09 it/s, Epoch 0.07/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.582000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.539100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.502000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.489300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.530800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.509700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.513000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.493100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.487300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.487400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.466400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.513800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.498600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.467700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.463400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.483800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.452800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.450900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.466400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.491400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.451700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.476400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.434700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.462800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.437900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.440400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 9.28 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:3485\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3484\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3485\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3487\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3489\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3490\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3491\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:3532\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3530\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3531\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3532\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3533\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3534\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/peft/peft_model.py:1644\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1642\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1643\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1644\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1645\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1655\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:1202\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     \u001b[38;5;66;03m# Enable model parallelism\u001b[39;00m\n\u001b[1;32m   1201\u001b[0m     shift_labels \u001b[38;5;241m=\u001b[39m shift_labels\u001b[38;5;241m.\u001b[39mto(shift_logits\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1202\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshift_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1205\u001b[0m     output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:1185\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 9.28 GiB. GPU "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055bfefc-d6b5-44b3-a14f-b4001b3ccb59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
