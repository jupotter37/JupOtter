{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5bdaa3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pydantic in ./micromamba/envs/vllm/lib/python3.11/site-packages (2.9.2)\n",
      "Requirement already satisfied: openai in ./micromamba/envs/vllm/lib/python3.11/site-packages (1.58.1)\n",
      "Requirement already satisfied: datasets in ./micromamba/envs/vllm/lib/python3.11/site-packages (3.2.0)\n",
      "Requirement already satisfied: trl in ./micromamba/envs/vllm/lib/python3.11/site-packages (0.11.4)\n",
      "Requirement already satisfied: transformers in ./micromamba/envs/vllm/lib/python3.11/site-packages (4.47.1)\n",
      "Requirement already satisfied: peft in ./micromamba/envs/vllm/lib/python3.11/site-packages (0.14.0)\n",
      "Requirement already satisfied: torch in ./micromamba/envs/vllm/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: pandas in ./micromamba/envs/vllm/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: wandb in ./micromamba/envs/vllm/lib/python3.11/site-packages (0.18.5)\n",
      "Requirement already satisfied: vllm in ./micromamba/envs/vllm/lib/python3.11/site-packages (0.6.5)\n",
      "Requirement already satisfied: tqdm in ./micromamba/envs/vllm/lib/python3.11/site-packages (4.66.5)\n",
      "Requirement already satisfied: fundus in ./micromamba/envs/vllm/lib/python3.11/site-packages (0.4.4)\n",
      "Requirement already satisfied: arxiv in ./micromamba/envs/vllm/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: clize in ./micromamba/envs/vllm/lib/python3.11/site-packages (5.0.2)\n",
      "Requirement already satisfied: xformers in ./micromamba/envs/vllm/lib/python3.11/site-packages (0.0.28.post3)\n",
      "Requirement already satisfied: flash-attn in ./micromamba/envs/vllm/lib/python3.11/site-packages (2.7.2.post1)\n",
      "Requirement already satisfied: llama-index in ./micromamba/envs/vllm/lib/python3.11/site-packages (0.12.8)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from pydantic) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from pydantic) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: sniffio in ./micromamba/envs/vllm/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: filelock in ./micromamba/envs/vllm/lib/python3.11/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in ./micromamba/envs/vllm/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in ./micromamba/envs/vllm/lib/python3.11/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from datasets) (0.25.2)\n",
      "Requirement already satisfied: packaging in ./micromamba/envs/vllm/lib/python3.11/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: accelerate in ./micromamba/envs/vllm/lib/python3.11/site-packages (from trl) (1.2.1)\n",
      "Requirement already satisfied: tyro>=0.5.11 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from trl) (0.8.13)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: psutil in ./micromamba/envs/vllm/lib/python3.11/site-packages (from peft) (6.0.0)\n",
      "Requirement already satisfied: networkx in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in ./micromamba/envs/vllm/lib/python3.11/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from wandb) (5.28.2)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from wandb) (2.17.0)\n",
      "Requirement already satisfied: setproctitle in ./micromamba/envs/vllm/lib/python3.11/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in ./micromamba/envs/vllm/lib/python3.11/site-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: sentencepiece in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.2.0)\n",
      "Requirement already satisfied: blake3 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (1.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (9.0.0)\n",
      "Requirement already satisfied: fastapi!=0.113.*,!=0.114.0,>=0.107.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.115.2)\n",
      "Requirement already satisfied: uvicorn[standard] in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.32.0)\n",
      "Requirement already satisfied: pillow in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (10.4.0)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.21.0)\n",
      "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (7.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.7.0)\n",
      "Requirement already satisfied: lm-format-enforcer<0.11,>=0.10.9 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.10.9)\n",
      "Requirement already satisfied: outlines==0.1.11 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.1.11)\n",
      "Requirement already satisfied: xgrammar>=0.1.6 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.1.7)\n",
      "Requirement already satisfied: partial-json-parser in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.2.1.1.post4)\n",
      "Requirement already satisfied: pyzmq in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (26.2.0)\n",
      "Requirement already satisfied: msgspec in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.18.6)\n",
      "Requirement already satisfied: gguf==0.10.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.10.0)\n",
      "Requirement already satisfied: importlib_metadata in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (8.5.0)\n",
      "Requirement already satisfied: mistral_common>=1.5.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from mistral_common[opencv]>=1.5.0->vllm) (1.5.0)\n",
      "Requirement already satisfied: einops in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.8.0)\n",
      "Requirement already satisfied: compressed-tensors==0.8.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.8.1)\n",
      "Requirement already satisfied: depyf==0.18.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.18.0)\n",
      "Requirement already satisfied: ray>=2.9 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (2.37.0)\n",
      "Requirement already satisfied: nvidia-ml-py>=12.560.30 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (12.560.30)\n",
      "Requirement already satisfied: torchvision==0.20.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from vllm) (0.20.1)\n",
      "Requirement already satisfied: astor in ./micromamba/envs/vllm/lib/python3.11/site-packages (from depyf==0.18.0->vllm) (0.8.1)\n",
      "Requirement already satisfied: interegular in ./micromamba/envs/vllm/lib/python3.11/site-packages (from outlines==0.1.11->vllm) (0.3.3)\n",
      "Requirement already satisfied: lark in ./micromamba/envs/vllm/lib/python3.11/site-packages (from outlines==0.1.11->vllm) (1.2.2)\n",
      "Requirement already satisfied: nest_asyncio in ./micromamba/envs/vllm/lib/python3.11/site-packages (from outlines==0.1.11->vllm) (1.6.0)\n",
      "Requirement already satisfied: cloudpickle in ./micromamba/envs/vllm/lib/python3.11/site-packages (from outlines==0.1.11->vllm) (3.1.0)\n",
      "Requirement already satisfied: diskcache in ./micromamba/envs/vllm/lib/python3.11/site-packages (from outlines==0.1.11->vllm) (5.6.3)\n",
      "Requirement already satisfied: referencing in ./micromamba/envs/vllm/lib/python3.11/site-packages (from outlines==0.1.11->vllm) (0.35.1)\n",
      "Requirement already satisfied: jsonschema in ./micromamba/envs/vllm/lib/python3.11/site-packages (from outlines==0.1.11->vllm) (4.23.0)\n",
      "Requirement already satisfied: pycountry in ./micromamba/envs/vllm/lib/python3.11/site-packages (from outlines==0.1.11->vllm) (24.6.1)\n",
      "Requirement already satisfied: airportsdata in ./micromamba/envs/vllm/lib/python3.11/site-packages (from outlines==0.1.11->vllm) (20241001)\n",
      "Requirement already satisfied: outlines_core==0.1.26 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from outlines==0.1.11->vllm) (0.1.26)\n",
      "Requirement already satisfied: lxml<6,>=4.9 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fundus) (5.3.0)\n",
      "Requirement already satisfied: more-itertools<10,>=9.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fundus) (9.1.0)\n",
      "Requirement already satisfied: cssselect<2,>=1.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fundus) (1.2.0)\n",
      "Requirement already satisfied: feedparser<7,>=6.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fundus) (6.0.11)\n",
      "Requirement already satisfied: colorama<1,>=0.4 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fundus) (0.4.6)\n",
      "Requirement already satisfied: langdetect<2,>=1.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fundus) (1.0.9)\n",
      "Requirement already satisfied: validators<1,>=0.24 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fundus) (0.34.0)\n",
      "Requirement already satisfied: fastwarc<1,>=0.14 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fundus) (0.14.9)\n",
      "Requirement already satisfied: chardet<6,>=5.2 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fundus) (5.2.0)\n",
      "Requirement already satisfied: dict2xml<2,>=1.7.6 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fundus) (1.7.6)\n",
      "Requirement already satisfied: xmltodict<1,>=0.13.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fundus) (0.14.1)\n",
      "Requirement already satisfied: sigtools>=4.0.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from clize) (4.0.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from clize) (24.2.0)\n",
      "Requirement already satisfied: od in ./micromamba/envs/vllm/lib/python3.11/site-packages (from clize) (2.0.2)\n",
      "Requirement already satisfied: docutils>=0.17.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from clize) (0.21.2)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index) (0.4.1)\n",
      "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.8 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index) (0.12.8)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index) (0.6.3)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index) (0.3.12)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index) (0.4.1)\n",
      "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index) (0.4.1)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: six>=1.4.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: starlette<0.41.0,>=0.37.2 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fastapi!=0.113.*,!=0.114.0,>=0.107.0->vllm) (0.40.0)\n",
      "Requirement already satisfied: brotli in ./micromamba/envs/vllm/lib/python3.11/site-packages (from fastwarc<1,>=0.14->fundus) (1.1.0)\n",
      "Requirement already satisfied: sgmllib3k in ./micromamba/envs/vllm/lib/python3.11/site-packages (from feedparser<7,>=6.0->fundus) (1.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from aiohttp->datasets) (1.15.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: certifi in ./micromamba/envs/vllm/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./micromamba/envs/vllm/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.8->llama-index) (2.0.36)\n",
      "Requirement already satisfied: dataclasses-json in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (1.2.15)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (1.2.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (9.0.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index) (1.17.0)\n",
      "Requirement already satisfied: llama-cloud>=0.1.5 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.6)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.12.3)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.1.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.5.18)\n",
      "Requirement already satisfied: opencv-python-headless<5.0.0,>=4.0.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from mistral_common[opencv]>=1.5.0->vllm) (4.10.0.84)\n",
      "Requirement already satisfied: joblib in ./micromamba/envs/vllm/lib/python3.11/site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from ray>=2.9->vllm) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from tyro>=0.5.11->trl) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from tyro>=0.5.11->trl) (13.9.2)\n",
      "Requirement already satisfied: shtab>=1.5.6 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from tyro>=0.5.11->trl) (1.7.1)\n",
      "Requirement already satisfied: pybind11 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from xgrammar>=0.1.6->vllm) (2.13.6)\n",
      "Requirement already satisfied: pytest in ./micromamba/envs/vllm/lib/python3.11/site-packages (from xgrammar>=0.1.6->vllm) (8.3.4)\n",
      "Requirement already satisfied: zipp>=3.20 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from importlib_metadata->vllm) (3.20.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from jinja2->torch) (3.0.1)\n",
      "Requirement already satisfied: httptools>=0.5.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from uvicorn[standard]->vllm) (0.6.2)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from uvicorn[standard]->vllm) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from uvicorn[standard]->vllm) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from uvicorn[standard]->vllm) (0.24.0)\n",
      "Requirement already satisfied: websockets>=10.4 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from uvicorn[standard]->vllm) (13.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from jsonschema->outlines==0.1.11->vllm) (2024.10.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from jsonschema->outlines==0.1.11->vllm) (0.20.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.18.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.8->llama-index) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.8->llama-index) (1.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.8->llama-index) (3.23.2)\n",
      "Requirement already satisfied: iniconfig in ./micromamba/envs/vllm/lib/python3.11/site-packages (from pytest->xgrammar>=0.1.6->vllm) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from pytest->xgrammar>=0.1.6->vllm) (1.5.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./micromamba/envs/vllm/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pydantic openai datasets trl transformers peft torch pandas wandb vllm tqdm fundus arxiv datasets clize xformers flash-attn llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ad5a675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-25 05:13:37 config.py:2167] Downcasting torch.float32 to torch.float16.\n",
      "INFO 12-25 05:13:37 config.py:478] This model supports multiple tasks: {'score', 'generate', 'reward', 'classify', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 12-25 05:13:37 config.py:1216] Defaulting to use mp for distributed inference\n",
      "WARNING 12-25 05:13:37 arg_utils.py:1086] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 12-25 05:13:37 config.py:1364] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 12-25 05:13:37 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='/home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411', speculative_config=None, tokenizer='/home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='lm-format-enforcer'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m INFO 12-25 05:13:42 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m INFO 12-25 05:13:42 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m INFO 12-25 05:13:43 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m INFO 12-25 05:13:43 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m INFO 12-25 05:13:43 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m INFO 12-25 05:13:43 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "INFO 12-25 05:13:44 utils.py:922] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m INFO 12-25 05:13:44 utils.py:922] Found nccl from library libnccl.so.2\n",
      "INFO 12-25 05:13:44 utils.py:922] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m INFO 12-25 05:13:44 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-25 05:13:44 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m INFO 12-25 05:13:44 utils.py:922] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m INFO 12-25 05:13:44 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-25 05:13:44 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "WARNING 12-25 05:13:44 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m WARNING 12-25 05:13:44 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 12-25 05:13:44 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m WARNING 12-25 05:13:44 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 12-25 05:13:44 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_445f6e56'), local_subscribe_port=59381, remote_subscribe_port=None)\n",
      "INFO 12-25 05:13:44 model_runner.py:1092] Starting to load model /home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m INFO 12-25 05:13:44 model_runner.py:1092] Starting to load model /home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m INFO 12-25 05:13:44 model_runner.py:1092] Starting to load model /home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m INFO 12-25 05:13:44 model_runner.py:1092] Starting to load model /home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f988f5cfd97d48119b5ffd39836dcd74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/51 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236] Exception in worker VllmWorkerProcess while processing method load_model.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py\", line 230, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/worker/worker.py\", line 155, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     self.model_runner.load_model()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 1094, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     self.model = get_model(vllm_config=self.vllm_config)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py\", line 12, in get_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     return loader.load_model(vllm_config=vllm_config)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 361, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     model = _initialize_model(vllm_config=vllm_config)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 114, in _initialize_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     return model_class(vllm_config=vllm_config, prefix=prefix)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 517, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     self.model = self._init_model(vllm_config=vllm_config,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 554, in _init_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     return LlamaModel(vllm_config=vllm_config, prefix=prefix)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 147, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 318, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     self.start_layer, self.end_layer, self.layers = make_layers(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                                                     ^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 550, in make_layers\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     [PPMissingLayer() for _ in range(start_layer)] + [\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                                                      ^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 551, in <listcomp>\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 320, in <lambda>\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     lambda prefix: layer_type(config=config,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 247, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     self.mlp = LlamaMLP(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                ^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 72, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     self.gate_up_proj = MergedColumnParallelLinear(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 426, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     super().__init__(input_size=input_size,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 306, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     self.quant_method.create_weights(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 124, in create_weights\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     weight = Parameter(torch.empty(sum(output_partition_sizes),\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/torch/utils/_device.py\", line 106, in __torch_function__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42485)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 336.00 MiB. GPU 1 has a total capacity of 79.14 GiB of which 287.19 MiB is free. Process 39437 has 72.63 GiB memory in use. Including non-PyTorch memory, this process has 6.18 GiB memory in use. Of the allocated memory 5.53 GiB is allocated by PyTorch, and 1.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236] Exception in worker VllmWorkerProcess while processing method load_model.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py\", line 230, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/worker/worker.py\", line 155, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     self.model_runner.load_model()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 1094, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     self.model = get_model(vllm_config=self.vllm_config)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py\", line 12, in get_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     return loader.load_model(vllm_config=vllm_config)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 361, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     model = _initialize_model(vllm_config=vllm_config)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 114, in _initialize_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     return model_class(vllm_config=vllm_config, prefix=prefix)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 517, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     self.model = self._init_model(vllm_config=vllm_config,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 554, in _init_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     return LlamaModel(vllm_config=vllm_config, prefix=prefix)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 147, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 318, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     self.start_layer, self.end_layer, self.layers = make_layers(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                                                     ^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 550, in make_layers\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     [PPMissingLayer() for _ in range(start_layer)] + [\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                                                      ^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 551, in <listcomp>\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 320, in <lambda>\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     lambda prefix: layer_type(config=config,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 247, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     self.mlp = LlamaMLP(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                ^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 72, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     self.gate_up_proj = MergedColumnParallelLinear(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 426, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     super().__init__(input_size=input_size,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 306, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     self.quant_method.create_weights(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 124, in create_weights\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     weight = Parameter(torch.empty(sum(output_partition_sizes),\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/torch/utils/_device.py\", line 106, in __torch_function__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42487)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 336.00 MiB. GPU 3 has a total capacity of 79.14 GiB of which 323.19 MiB is free. Process 39439 has 72.62 GiB memory in use. Including non-PyTorch memory, this process has 6.18 GiB memory in use. Of the allocated memory 5.53 GiB is allocated by PyTorch, and 1.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236] Exception in worker VllmWorkerProcess while processing method load_model.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236] Traceback (most recent call last):\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/executor/multiproc_worker_utils.py\", line 230, in _run_worker_process\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     output = executor(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/worker/worker.py\", line 155, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     self.model_runner.load_model()\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 1094, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     self.model = get_model(vllm_config=self.vllm_config)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py\", line 12, in get_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     return loader.load_model(vllm_config=vllm_config)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 361, in load_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     model = _initialize_model(vllm_config=vllm_config)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 114, in _initialize_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     return model_class(vllm_config=vllm_config, prefix=prefix)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 517, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     self.model = self._init_model(vllm_config=vllm_config,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 554, in _init_model\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     return LlamaModel(vllm_config=vllm_config, prefix=prefix)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/compilation/decorators.py\", line 147, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 318, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     self.start_layer, self.end_layer, self.layers = make_layers(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                                                     ^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 550, in make_layers\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     [PPMissingLayer() for _ in range(start_layer)] + [\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                                                      ^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/utils.py\", line 551, in <listcomp>\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 320, in <lambda>\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     lambda prefix: layer_type(config=config,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 247, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     self.mlp = LlamaMLP(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                ^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py\", line 72, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     self.gate_up_proj = MergedColumnParallelLinear(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 426, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     super().__init__(input_size=input_size,\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 306, in __init__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     self.quant_method.create_weights(\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py\", line 124, in create_weights\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     weight = Parameter(torch.empty(sum(output_partition_sizes),\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]   File \"/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/torch/utils/_device.py\", line 106, in __torch_function__\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(VllmWorkerProcess pid=42486)\u001b[0;0m ERROR 12-25 05:13:45 multiproc_worker_utils.py:236] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 336.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 323.19 MiB is free. Process 39438 has 72.62 GiB memory in use. Including non-PyTorch memory, this process has 6.18 GiB memory in use. Of the allocated memory 5.53 GiB is allocated by PyTorch, and 1.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m, min_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#\"/home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguided_decoding_backend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlm-format-enforcer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m conversation \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m     {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m ]\n\u001b[1;32m     56\u001b[0m outputs \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mchat(conversation, sampling_params\u001b[38;5;241m=\u001b[39msampling_params)\n",
      "File \u001b[0;32m~/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/utils.py:990\u001b[0m, in \u001b[0;36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    985\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    986\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m    987\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m    988\u001b[0m         )\n\u001b[0;32m--> 990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/entrypoints/llm.py:230\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_engine_class()\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# TODO(rob): enable mp by default (issue with fork vs spawn)\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/engine/llm_engine.py:532\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    530\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    531\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 532\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/engine/llm_engine.py:288\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, mm_registry, use_cached_outputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_registry \u001b[38;5;241m=\u001b[39m input_registry\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_processor \u001b[38;5;241m=\u001b[39m input_registry\u001b[38;5;241m.\u001b[39mcreate_input_processor(\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config)\n\u001b[0;32m--> 288\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mrunner_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpooling\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_kv_caches()\n",
      "File \u001b[0;32m~/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/executor/distributed_gpu_executor.py:26\u001b[0m, in \u001b[0;36mDistributedGPUExecutor.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Updated by implementations that require additional args to be passed\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# to the _run_workers execute_model call\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_execute_model_run_workers_kwargs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/executor/executor_base.py:36\u001b[0m, in \u001b[0;36mExecutorBase.__init__\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_adapter_config \u001b[38;5;241m=\u001b[39m vllm_config\u001b[38;5;241m.\u001b[39mprompt_adapter_config\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;241m=\u001b[39m vllm_config\u001b[38;5;241m.\u001b[39mobservability_config\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/executor/multiproc_gpu_executor.py:83\u001b[0m, in \u001b[0;36mMultiprocessingGPUExecutor._init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_worker(\n\u001b[1;32m     81\u001b[0m     distributed_init_method\u001b[38;5;241m=\u001b[39mdistributed_init_method)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_workers(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_device\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mload_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmax_concurrent_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\n\u001b[1;32m     85\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmax_parallel_loading_workers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/executor/multiproc_gpu_executor.py:157\u001b[0m, in \u001b[0;36mMultiprocessingGPUExecutor._run_workers\u001b[0;34m(self, method, async_run_tensor_parallel_workers_only, max_concurrent_workers, *args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m worker_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    152\u001b[0m     worker\u001b[38;5;241m.\u001b[39mexecute_method(method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m worker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\n\u001b[1;32m    154\u001b[0m ]\n\u001b[1;32m    156\u001b[0m driver_worker_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker, method)\n\u001b[0;32m--> 157\u001b[0m driver_worker_output \u001b[38;5;241m=\u001b[39m \u001b[43mdriver_worker_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Get the results of the workers.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [driver_worker_output\n\u001b[1;32m    161\u001b[0m         ] \u001b[38;5;241m+\u001b[39m [output\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m worker_outputs]\n",
      "File \u001b[0;32m~/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/worker/worker.py:155\u001b[0m, in \u001b[0;36mWorker.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/worker/model_runner.py:1094\u001b[0m, in \u001b[0;36mGPUModelRunnerBase.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1092\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting to load model \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m DeviceMemoryProfiler() \u001b[38;5;28;01mas\u001b[39;00m m:\n\u001b[0;32m-> 1094\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mconsumed_memory\n\u001b[1;32m   1097\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model weights took \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m GB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1098\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m))\n",
      "File \u001b[0;32m~/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py:12\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(vllm_config)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model\u001b[39m(\u001b[38;5;241m*\u001b[39m, vllm_config: VllmConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m nn\u001b[38;5;241m.\u001b[39mModule:\n\u001b[1;32m     11\u001b[0m     loader \u001b[38;5;241m=\u001b[39m get_model_loader(vllm_config\u001b[38;5;241m.\u001b[39mload_config)\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py:364\u001b[0m, in \u001b[0;36mDefaultModelLoader.load_model\u001b[0;34m(self, vllm_config)\u001b[0m\n\u001b[1;32m    361\u001b[0m     model \u001b[38;5;241m=\u001b[39m _initialize_model(vllm_config\u001b[38;5;241m=\u001b[39mvllm_config)\n\u001b[1;32m    363\u001b[0m weights_to_load \u001b[38;5;241m=\u001b[39m {name \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters()}\n\u001b[0;32m--> 364\u001b[0m loaded_weights \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_all_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;66;03m# We only enable strict check for non-quantized models\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# that have loaded weights tracking currently.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_config\u001b[38;5;241m.\u001b[39mquantization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m loaded_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py:594\u001b[0m, in \u001b[0;36mLlamaForCausalLM.load_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, weights: Iterable[Tuple[\u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    588\u001b[0m                                                torch\u001b[38;5;241m.\u001b[39mTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Set[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    589\u001b[0m     loader \u001b[38;5;241m=\u001b[39m AutoWeightsLoader(\n\u001b[1;32m    590\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    591\u001b[0m         skip_prefixes\u001b[38;5;241m=\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm_head.\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    592\u001b[0m                        \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtie_word_embeddings \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    593\u001b[0m     )\n\u001b[0;32m--> 594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_remap_mistral\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/utils.py:237\u001b[0m, in \u001b[0;36mAutoWeightsLoader.load_weights\u001b[0;34m(self, weights, mapper)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mapper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     weights \u001b[38;5;241m=\u001b[39m mapper\u001b[38;5;241m.\u001b[39mapply(weights)\n\u001b[0;32m--> 237\u001b[0m autoloaded_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m autoloaded_weights\n",
      "File \u001b[0;32m~/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/utils.py:198\u001b[0m, in \u001b[0;36mAutoWeightsLoader._load_module\u001b[0;34m(self, base_prefix, module, weights)\u001b[0m\n\u001b[1;32m    194\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping module \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, prefix)\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_module(prefix,\n\u001b[1;32m    199\u001b[0m                                  child_modules[child_prefix],\n\u001b[1;32m    200\u001b[0m                                  child_weights)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m child_prefix \u001b[38;5;129;01min\u001b[39;00m child_params:\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_skip(prefix):\n",
      "File \u001b[0;32m~/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/utils.py:175\u001b[0m, in \u001b[0;36mAutoWeightsLoader._load_module\u001b[0;34m(self, base_prefix, module, weights)\u001b[0m\n\u001b[1;32m    173\u001b[0m module_load_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(module_load_weights):\n\u001b[0;32m--> 175\u001b[0m     loaded_params \u001b[38;5;241m=\u001b[39m \u001b[43mmodule_load_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loaded_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to collect loaded parameters \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor module \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, module)\n",
      "File \u001b[0;32m~/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/llama.py:415\u001b[0m, in \u001b[0;36mLlamaModel.load_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    413\u001b[0m     param \u001b[38;5;241m=\u001b[39m params_dict[name]\n\u001b[1;32m    414\u001b[0m     weight_loader \u001b[38;5;241m=\u001b[39m param\u001b[38;5;241m.\u001b[39mweight_loader\n\u001b[0;32m--> 415\u001b[0m     \u001b[43mweight_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;66;03m# Skip loading extra bias for GPTQ models.\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/layers/linear.py:572\u001b[0m, in \u001b[0;36mMergedColumnParallelLinear.weight_loader\u001b[0;34m(self, param, loaded_weight, loaded_shard_id)\u001b[0m\n\u001b[1;32m    566\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    567\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading a weight without `output_dim` attribute in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMergedColumnParallelLinear, assume the weight is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe same for all partitions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m param_data\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m loaded_weight\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 572\u001b[0m \u001b[43mparam_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Literal\n",
    "from datetime import datetime, timedelta\n",
    "from pydantic import BaseModel\n",
    "from lmformatenforcer import JsonSchemaParser\n",
    "from openai import OpenAI\n",
    "from lmformatenforcer.integrations.transformers import build_transformers_prefix_allowed_tokens_fn\n",
    "import pprint\n",
    "import json\n",
    "import random\n",
    "from fundus import PublisherCollection, Crawler\n",
    "from transformers import AutoTokenizer\n",
    "from llama_index.readers.papers import ArxivReader\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "sampling_params = SamplingParams(temperature=2.0, min_p=0.3)\n",
    "#\"/home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411\"\n",
    "llm = LLM(model=\"/home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411\", tensor_parallel_size=4, guided_decoding_backend = \"lm-format-enforcer\")\n",
    "\n",
    "conversation = [\n",
    "\n",
    "    {\n",
    "\n",
    "        \"role\": \"system\",\n",
    "\n",
    "        \"content\": \"You are a helpful assistant\"\n",
    "\n",
    "    },\n",
    "\n",
    "    {\n",
    "\n",
    "        \"role\": \"user\",\n",
    "\n",
    "        \"content\": \"Hello\"\n",
    "\n",
    "    },\n",
    "\n",
    "    {\n",
    "\n",
    "        \"role\": \"assistant\",\n",
    "\n",
    "        \"content\": \"Hello! How can I assist you today?\"\n",
    "\n",
    "    },\n",
    "\n",
    "    {\n",
    "\n",
    "        \"role\": \"user\",\n",
    "\n",
    "        \"content\": \"Write an essay about the importance of higher education.\",\n",
    "\n",
    "    },\n",
    "\n",
    "]\n",
    "\n",
    "outputs = llm.chat(conversation, sampling_params=sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    print(output.outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8804a2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.84s/it, est. speed input: 1.32 toks/s, output: 19.35 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ### The Importance of Higher Education\n",
      "\n",
      "In an increasingly interconnected and competitive world, the significance of higher education cannot be overstated. Higher education serves as a catalyst for personal growth, intellectual development, and economic prosperity. It equips individuals with the knowledge, skills, and credentials necessary to navigate the complexities of modern society and contribute meaningfully to their communities and the global economy.\n",
      "\n",
      "#### Personal Growth and Intellectual Development\n",
      "\n",
      "Higher education fosters personal growth by providing a structured environment for individuals to explore their interests, develop critical thinking skills, and gain a deeper understanding of the world. Universities and colleges offer a diverse range of courses and extracurricular activities that encourage students to step out of their comfort zones and engage with new ideas and perspectives. This exposure broadens their horizons and cultivates a sense of curiosity and lifelong learning.\n",
      "\n",
      "Moreover, higher education promotes intellectual development by teaching students how to analyze information critically, solve complex problems, and communicate effectively. These skills are essential not only for academic success but also for navigating the challenges of everyday life. By engaging in rigorous academic pursuits, students learn to question assumptions, evaluate evidence, and form well-reasoned arguments, all of which are crucial for informed decision-making and civic engagement.\n",
      "\n",
      "#### Economic Prosperity and Career Advancement\n",
      "\n",
      "Higher education is closely linked to economic prosperity and career advancement. Individuals with a college degree or higher generally have better employment prospects and earn higher wages than those without. According to the U.S. Bureau of Labor Statistics, workers with a bachelor's degree earn, on average, 67% more than those with only a high school diploma. This wage premium reflects the value that employers place on the skills and knowledge acquired through higher education.\n",
      "\n",
      "Furthermore, higher education opens doors to specialized and high-demand fields such as healthcare, technology, engineering, and finance. These fields often require advanced degrees and certifications, making higher education a prerequisite for entry. By pursuing higher education, individuals can position themselves for rewarding and fulfilling careers that offer opportunities for growth and advancement.\n",
      "\n",
      "#### Social and Civic Responsibility\n",
      "\n",
      "Higher education also plays a critical role in fostering social and civic responsibility. Universities and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=2.0, min_p=0.3, min_tokens=200, max_tokens=500, top_p=1, top_k=-1)\n",
    "\n",
    "outputs = llm.chat(conversation, sampling_params=sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    print(output.outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7cc3331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-24 21:06:39 multiproc_worker_utils.py:140] Terminating local vLLM worker processes\n",
      "Successfully delete the llm pipeline and free the GPU memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=38872)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=38874)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=38873)\u001b[0;0m INFO 12-24 21:06:39 multiproc_worker_utils.py:247] Worker exiting\n",
      "INFO 12-24 21:06:39 multiproc_worker_utils.py:247] Worker exiting\n",
      "INFO 12-24 21:06:39 multiproc_worker_utils.py:247] Worker exiting\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61568d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M3P: Learning Universal Representations via Multitask Multilingual\n",
      "Multimodal Pre-training\n",
      "Minheng Ni1*† Haoyang Huang2† Lin Su3† Edward Cui3 Taroon Bharti3 Lijuan Wang4\n",
      "Jianfeng Gao5 Dongdong Zhang2 Nan Duan2‡\n",
      "1 Research Center for Social Computing and Information Retrieval\n",
      "Harbin Institute of Technology, China\n",
      "2 Natural Language Computing, Microsoft Research Asia, China\n",
      "3 Bing Multimedia Team, Microsoft, China\n",
      "4 Cloud+AI, Microsoft, United States\n",
      "5 Deep Learning, Microsoft Research Redmond, United States\n",
      "mhni@ir.hit.edu.cn\n",
      "{haohua, lins, edwac, tbharti, lijuanw, jfgao, Dongdong.Zhang, nanduan}@microsoft.com\n",
      "Abstract\n",
      "We present M3P, a Multitask Multilingual Multimodal\n",
      "Pre-trained model that combines multilingual pre-training\n",
      "and multimodal pre-training into a uniﬁed framework via\n",
      "multitask pre-training. Our goal is to learn universal repre-\n",
      "sentations that can map objects occurred in different modal-\n",
      "ities or texts expressed in different languages into a com-\n",
      "mon semantic space. In addition, to explicitly encourage\n",
      "ﬁne-grained alignment between images and non-English lan-\n",
      "guages, we also propose Multimodal Code-switched Train-\n",
      "ing (MCT) to combine monolingual pre-training and multi-\n",
      "modal pre-training via a code-switch strategy. Experiments\n",
      "are performed on the multilingual image retrieval task across\n",
      "two benchmark datasets, including MSCOCO and Multi30K.\n",
      "M3P can achieve comparable results for English and new\n",
      "state-of-the-art results for non-English languages.\n",
      "1. Introduction\n",
      "Recently, we witness the rise of a new paradigm of natu-\n",
      "ral language processing (NLP), where general knowledge is\n",
      "learned from raw texts by self-supervised pre-training and\n",
      "then applied to downstream tasks by task-speciﬁc ﬁne-tuning.\n",
      "Now, these state-of-the-art monolingual pre-trained language\n",
      "models, such as BERT [7], RoBERTa [23] and GPT-2 [28],\n",
      "have been expanded to multilingual scenarios, such as Mul-\n",
      "tilingual BERT [ 7], XLM/XLM-R [ 5, 4], Unicoder [ 13].\n",
      "*Work is done during an internship at Microsoft Research Asia.\n",
      "†These authors contributed equally to this work.\n",
      "‡Corresponding Author.\n",
      "Moreover, some pre-training models under multimodal sce-\n",
      "narios, such as Unicoder-VL [19], UNITER [3], ERNIE-ViL\n",
      "[36], VILLA [10] and Oscar [21], also come out.\n",
      "However, it is still challenging to extend these pre-trained\n",
      "models to multilingual-multimodal scenarios. The multilin-\n",
      "gual pre-trained language models cannot handle vision data\n",
      "(e.g., images or videos) directly, whereas many pre-trained\n",
      "multimodal models are trained on English corpora thus can-\n",
      "not perform very well on non-English languages. Therefore,\n",
      "high quality multilingual multimodal training corpus is es-\n",
      "sential to combine multilingual pre-training and multimodal\n",
      "pre-training. However, there are only a few multilingual\n",
      "multimodal corpora exist, and they also have low language\n",
      "coverage. Moreover, relying on high-quality machine transla-\n",
      "tion engines to generate such data from English multimodal\n",
      "corpora is both time-consuming and computationally ex-\n",
      "pensive. Learning explicit alignments between vision and\n",
      "non-English languages during pre-training is lacking.\n",
      "To address these challenges, this paper presents M3P,\n",
      "a Multitask Multilingual Multimodal Pre-trained model,\n",
      "which aims to learn universal representations that can map\n",
      "objects occurred in different modalities or texts expressed in\n",
      "different languages into a common semantic space. In order\n",
      "to alleviate the issue of lacking enough non-English labeled\n",
      "data for multimodal pre-training, we introduce Multimodal\n",
      "Code-switched Training (MCT) to enforce the explicit align-\n",
      "ments between images and non-English languages. The goal\n",
      "is achieved by (i) learning to represent multilingual data us-\n",
      "ing multilingual corpora (e.g., sentences from Wikipedia cov-\n",
      "ering 100 languages) by multilingual pre-training, (ii) learn-\n",
      "ing multilingual-multimodal representations by randomly\n",
      "replacing some English words with their translations in other\n",
      "1\n",
      "arXiv:2006.02635v4  [cs.CL]  1 Apr 2021\n",
      "languages from multimodal corpora (e.g., image-caption\n",
      "pairs labeled in English), and (iii) generalizing these rep-\n",
      "resentations to deal with multilingual-multimodal tasks by\n",
      "Multitask learning.\n",
      "In summary, the main contributions of this paper are:\n",
      "• We present M3P, the ﬁrst known effort on combining\n",
      "multilingual pre-training and multimodal pre-training\n",
      "into a uniﬁed framework.\n",
      "• We propose a novel Multimodal Code-switched Train-\n",
      "ing (MCT) method, an effective way to enhance the\n",
      "multilingual transfer ability of M 3P in the zero-shot\n",
      "and few-shot settings.\n",
      "• We achieve new state-of-the-art results for the multi-\n",
      "lingual image-text retrieval task on both Multi30K and\n",
      "MSCOCO for non-English languages, outperforming\n",
      "existing multilingual methods by a large margin. The\n",
      "proposed model can also achieve comparable results\n",
      "for English on these two datasets, compared to the state-\n",
      "of-the-art monolingual multimodal models.\n",
      "• Last but not least, we conduct extensive experiments\n",
      "and analysis to provide insights on the effectiveness of\n",
      "using Multimodal Code-switched Training (MCT) and\n",
      "each pre-training task.\n",
      "2. Related Work\n",
      "Multilingual Pre-trained Models Multilingual BERT\n",
      "(M-BERT) [7] demonstrates that by performing masked lan-\n",
      "guage modeling on a multilingual corpus with shared vo-\n",
      "cabulary and weights for 102 languages, surprisingly good\n",
      "results can be achieved on the cross-lingual natural language\n",
      "inference (XNLI) [6] task in 15 languages. XLM [ 5] and\n",
      "Unicoder [13] further improve the multilingual BERT by\n",
      "introducing new pre-training tasks based on a bilingual cor-\n",
      "pus. However, all such models work for NLP tasks and are\n",
      "not well designed for multimodal tasks such as Multilingual\n",
      "Image-text Retrieval or Multimodal Machine Translation.\n",
      "Multimodal Pre-trained Models Recently, a large num-\n",
      "ber of multimodal pre-trained models, such as ViLBERT\n",
      "[24], Unicoder-VL [19], UNITER [3], VLP [37] and Oscar\n",
      "[21], are developed for vision-language tasks using multi-\n",
      "layer Transformer as the backbone. However, as it is not\n",
      "easy to collect well-aligned visual-linguistic training data in\n",
      "multiple languages, all these models are pre-trained for En-\n",
      "glish only based on monolingual multimodal corpora, such\n",
      "as Conceptual Captions [ 29], SBU Captions [ 26], Visual\n",
      "Genome [17] and MSCOCO [2]. Hence, it is not feasible to\n",
      "apply them into multimodal tasks with non-English inputs.\n",
      "Code-switched Training Code-switched training [ 27]\n",
      "[33] converts the original training corpus to code-switched\n",
      "corpus, which can help the model explicitly model the rela-\n",
      "tionship among corresponding words in different languages.\n",
      "Existing work uses a rule-based word replacement strategy\n",
      "to replace the original word with translated word randomly\n",
      "by bilingual dictionaries. This approach provides a signiﬁ-\n",
      "cant improvement to the low-resource language. However,\n",
      "existing works use Code-switching for text-only tasks and ig-\n",
      "nore its application on multimodal pre-training model under\n",
      "multilingual-multimodal scenarios.\n",
      "3. M3P: Multitask Multilingual Multimodal\n",
      "Pre-training\n",
      "In this section, we describe how we train M 3P using a\n",
      "multilingual-monomodal corpus (e.g., sentences extracted\n",
      "from Wikipedia) and a monolingual-multimodal corpus (e.g.,\n",
      "English image-caption pairs). As outlined in Figure 1, we\n",
      "use the self-attentive transformer architecture of BERT, and\n",
      "design two pre-training objectives with three types of data\n",
      "streams. Multitask training is employed into the pre-training\n",
      "stage to optimize all pre-training objectives simultaneously\n",
      "for better performance. We optimize the accumulated loss\n",
      "of both pre-training objectives with the same weight in each\n",
      "iteration to train them by turns.\n",
      "3.1. Data Stream\n",
      "We use two basic data streams, Multilingual Monomodal\n",
      "Stream and Monolingual Multimodal Stream, from the multi-\n",
      "lingual corpus and multimodal corpus, respectively. We also\n",
      "design Multimodal Code-switched Stream to utilize multi-\n",
      "lingual data and multimodal data at the same time. Details\n",
      "regarding the three data streams are introduced below.\n",
      "Multilingual Monomodal Stream To apply multilingual\n",
      "pre-training, we use Multilingual Monomodal Stream as\n",
      "model input. Given an input text in any language w[li], we\n",
      "ﬁrst tokenize it into a sequence of BPE tokens via Sentence\n",
      "Piece [18]. Then we can obtain a text representation se-\n",
      "quence by summing up the text embedding and the position\n",
      "embedding of each BPE token. Moreover, a language em-\n",
      "bedding [5] is added to each token to indicate its language\n",
      "attribute. Speciﬁcally, the input data is deﬁned as:\n",
      "{\n",
      "w[li]\n",
      "}\n",
      "=\n",
      "{\n",
      "(w[li]\n",
      "1 ,w[li]\n",
      "2 ,..., w[li]\n",
      "M )\n",
      "}\n",
      "where M denotes the length of w[li] and li denotes a lan-\n",
      "guage in the language set L. We denote this stream as D[X].\n",
      "Monolingual Multimodal Stream To apply multimodal\n",
      "pre-training, we use Monolingual Multimodal Stream as\n",
      "model input. Given a pair of English text and image\n",
      "2\n",
      "Figure 1: Three data streams and four pre-training tasks used in M3P. Blue blocks denote English text, and Yellow, Green and\n",
      "Orange blocks denote non-English text.\n",
      "(w[EN],v), the text representation sequence of w[EN] is ob-\n",
      "tained similarly as we described in Multilingual Monomodal\n",
      "Stream section, where English is used as the language em-\n",
      "bedding. For the image v, we use Faster-RCNN [ 12] to\n",
      "detect image regions and use corresponding visual features\n",
      "in each region as a visual feature sequence. We also add a\n",
      "spatial embedding to each visual token, which is a 5-D vector\n",
      "based on its normalized top-left, bottom-right coordinates,\n",
      "and the fraction of the image area covered. We project these\n",
      "two vectors to the same dimension of the text representation\n",
      "using two fully-connected (FC) layers. Therefore, the im-\n",
      "age representation sequence is obtained by summing up its\n",
      "projected visual feature vector and spatial embedding vector\n",
      "of each region in the image. Furthermore, we add a stream\n",
      "tag [IMG] at the beginning of the image region sequence to\n",
      "separate text tokens and image tokens, and concatenate them\n",
      "to form an input stream:\n",
      "{\n",
      "w[EN],v\n",
      "}\n",
      "=\n",
      "{\n",
      "(w[EN]\n",
      "1 ,w[EN]\n",
      "2 ,..., w[EN]\n",
      "M ),(v1,v2,..., vN)\n",
      "}\n",
      "We denote this stream as D[EN].\n",
      "Multimodal Code-switched Stream We generate Multi-\n",
      "modal Code-switched Stream from Monolingual Multimodal\n",
      "Stream by code-switched method, given English text and\n",
      "image pairs (w[EN],v), the set of code-switched languages\n",
      "C = {c1,c2,...,c k}, and bilingual dictionaries which can\n",
      "translate a word from English to any language ci. Follow-\n",
      "ing [27], for each word w[EN]\n",
      "i in English text w[EN], we\n",
      "replace it with a translated word with a probability of β. If\n",
      "a word has multiple translations, we choose a random one.\n",
      "Similar to the generation process of Multilingual Monolin-\n",
      "gual Stream, we obtain the text representation sequence of\n",
      "the Code-switched text w[C] in the same way while keep-\n",
      "ing the original language embedding.* Similar with Mono-\n",
      "lingual Multimodal Stream, the text and image representa-\n",
      "tion sequences are concatenated as the ﬁnal input stream:{\n",
      "(w[d1]\n",
      "1 ,w[d2]\n",
      "2 ,..., w[dM]\n",
      "M ),(v1,v2,..., vN)\n",
      "}\n",
      ", where di is a\n",
      "random language in {EN}∪C. We simplify the input se-\n",
      "quence as:\n",
      "{\n",
      "w[C],v\n",
      "}\n",
      "=\n",
      "{\n",
      "(w[C]\n",
      "1 ,w[C]\n",
      "2 ,..., w[C]\n",
      "M ),(v1,v2,..., vN)\n",
      "}\n",
      "We denote this stream as D[C].\n",
      "3.2. Pre-training Objectives\n",
      "To pre-train M3P under multilingual-multimodal scenario,\n",
      "we designed two types of pre-training objectives. Multilin-\n",
      "gual Training aims to learn grammar or syntax from well-\n",
      "formed multilingual sentences. Multimodal Code-switched\n",
      "Training (MCT) aims to learn different languages from the\n",
      "shared vision modal and the alignment between vision and\n",
      "non-English texts.\n",
      "3.2.1 Multilingual Training\n",
      "Multilingual Masked Language Modeling (xMLM)\n",
      "Similar to Multilingual BERT [7], XLM [5] and Unicoder\n",
      "[13], this task performs masked language modeling based\n",
      "on the multilingual corpus. At each iteration, a batch is\n",
      "composed of sentences sampled from different languages.\n",
      "The sampling probability of a language li is deﬁned as\n",
      "λli = pα\n",
      "li/∑\n",
      "li pα\n",
      "li, where pli is the percentage of li in the\n",
      "entire multilingual corpus, and the smoothing factor αis set\n",
      "to 0.3. In each batch, we randomly sample 15% of the words\n",
      "and (i) replace them with a special symbol [MASK], (ii) re-\n",
      "place them with random tokens, or (iii) keep them unchanged\n",
      "*We have tried to change language embedding in Code-switched Stream,\n",
      "but no signiﬁcant gain was obtained.\n",
      "3\n",
      "with a probability of 80%, 10% and 10%, respectively. We\n",
      "only use Multilingual Monomodal Stream D[X] for we do\n",
      "not need to use Code-switching to extend it to multilingual\n",
      "corpus. The loss function is deﬁned as:\n",
      "LxMLM(θ) =−Ew[li]∼D[X] log qθ(w[li]\n",
      "m |w[li]\n",
      "\\m)\n",
      ", where w[li]\n",
      "m is the masked token and w[li]\n",
      "\\m is its context.\n",
      "3.2.2 Multimodal Code-switched Training\n",
      "Because of the lack of labeled data for the non-English mul-\n",
      "timodal scenario, the model can only learn multilingualism\n",
      "and multimodality independently. To help the model learn\n",
      "different language representations under the shared vision\n",
      "modal, we propose three Multimodal Code-switched Train-\n",
      "ing tasks: MC-MLM, MC-MRM and MC-VLM. We mix\n",
      "Multimodal Code-switched Stream D[C] and Monolingual\n",
      "Multimodal Stream D[EN] with a proportion ratio of αand\n",
      "1 −α, respectively, in train these tasks. To simplify the\n",
      "symbols, we denote the mixed data stream as Dand omit\n",
      "the mask [EN] or [C] as [·].\n",
      "Multimodal Code-switched Masked Language Model-\n",
      "ing (MC-MLM) Different from the pre-training tasks in\n",
      "ViLBERT [24] and Unicoder-VL [19], this task aims to learn\n",
      "the representation of different languages based on the shared\n",
      "vision modal. Mixed data stream Dis used for training this\n",
      "objective. Speciﬁcally, the model predicts each masked to-\n",
      "ken w[·]\n",
      "m in the caption w[·] based on its surrounding tokens\n",
      "w[·]\n",
      "\\m and all image regions v. We follow the same masking\n",
      "strategy used in xMLM to mask tokens in the input caption.\n",
      "The loss function is deﬁned as:\n",
      "LMC−MLM(θ) =−E(w[·],v)∼Dlog pθ(w[·]\n",
      "m|w[·]\n",
      "\\m,v)\n",
      ", where Dis the mixed data stream.\n",
      "Multimodal Code-switched Masked Region Modeling\n",
      "(MC-MRM) This task aims to learn vision representations\n",
      "with multilingual text as the context in mixed data stream\n",
      "D. The model reconstructs each masked image region vn\n",
      "based on the remaining regions v\\n and all caption tokens\n",
      "w[·]. We randomly mask image regions with a probability of\n",
      "15%. The input representation of each masked image region\n",
      "is set to zeros or kept as the original values with a probability\n",
      "of 90% and 10%, respectively. We apply an FC layer to con-\n",
      "vert the Transformer output of each masked region vk into a\n",
      "vector hθ(vk) of the same dimension with the visual feature\n",
      "f(vk). We use cross-entropy loss CE(gθ(vk),C(vk)) to pre-\n",
      "dict the object category of each masked region vk. We also\n",
      "apply another FC layer to convert the Transformer output\n",
      "of each masked region vk to predict the scores of Kobject\n",
      "Dataset Images Texts Langauges\n",
      "Pre-training Corpus\n",
      "Wikipedia - 101G 100\n",
      "Conceptual Captions [29] 3.3M 3.3M 1\n",
      "Fine-tuning and Evaluation Corpus\n",
      "Multi30K [35] 32K 384K 5\n",
      "MSCOCO [2] [34] [20] 120K 1.5M 3\n",
      "Table 1: Statistics of datasets.\n",
      "classes, which further go through a softmax function to be\n",
      "transformed into a normalized distribution gθ(vk). We take\n",
      "the predicted object category with the highest conﬁdence\n",
      "score outputted by Faster-RCNN as the ground-truth label of\n",
      "vk, and convert it into a one-hot vectorC(vk) ∈RK. Due to\n",
      "the top-1 category predicted by Faster-RCNN is not always\n",
      "correct, we leave minimizing the KL divergence between\n",
      "two distributions for our future work. The loss function can\n",
      "be deﬁned as:\n",
      "LMC−MRM(θ) =−E(w[·],v)∼D\n",
      "∑\n",
      "k\n",
      "[MSE(hθ(vk),f(vk))+\n",
      "CE(gθ(vk),C(vk))]\n",
      "where k enumerates the index of each masked image re-\n",
      "gion and MSE(hθ(vk),f(vk)) denotes the mean-square-\n",
      "error loss that regresses the Transformer output of each\n",
      "masked region vk to its visual feature f(vk).\n",
      "Multimodal Code-switched Visual-Linguistic Matching\n",
      "(MC-VLM) This task aims to learn alignment between\n",
      "multilingual texts and images with mixed data stream D. An\n",
      "FC layer sθ(w[·],v) is applied on the Transformer output of\n",
      "[CLS] to predict whether the input image v and the input\n",
      "English or Code-switched text w[·] are semantically matched.\n",
      "Negative image-caption pairs are created by replacing the\n",
      "image or text in a matched sample with a randomly-selected\n",
      "image or text from other samples. We use Binary Cross-\n",
      "Entropy as the loss function:\n",
      "LMC−VLM(θ) =−E(w[·],v)∼D[BCE(sθ(w[·],v),y)]\n",
      "where y∈{0,1}indicates whether the input image-text pair\n",
      "is matched and BCE indicates binary-cross-entropy loss.\n",
      "4. Experiments\n",
      "In this section, we describe detailed experimental settings\n",
      "during pre-training, ﬁne-tuning and evaluating M3P model.\n",
      "4.1. Dataset Description\n",
      "As shown in Table 1, we construct our pre-training dataset\n",
      "based on multimodal corpus, Conceptual Captions [29], and\n",
      "4\n",
      "multilingual corpus, Wikipedia. We evaluate M3P on multi-\n",
      "lingual image-text retrieval task on two datasets: Multi30K\n",
      "[9, 8] and MSCOCO [2, 25, 20]. Panlex† is used as the bilin-\n",
      "gual dictionary during Multimodal Code-switched Training.\n",
      "4.1.1 Pre-training Corpus\n",
      "Conceptual Captions We use Conceptual Captions [29]\n",
      "as the multimodal corpus. It contains 3.3 million English\n",
      "image-caption pairs harvested from the Web and does not\n",
      "contain any non-English text.\n",
      "Wikipedia We use sentences extracted from the Wikipedia\n",
      "dump as the multilingual corpus. It includes 101G sentences\n",
      "covering 100 languages without any vision information.\n",
      "4.1.2 Fine-tuning and Evaluation Corpus\n",
      "Multi30K This dataset extended Flickr30K [35] from En-\n",
      "glish (en) to German (de), French (fr) and Czech (cs). It\n",
      "contains 31,783 images and provides ﬁve captions per image\n",
      "in English and German and one caption per image in French\n",
      "and Czech. The train, dev, and test splits are deﬁned in [35].\n",
      "MSCOCO This dataset contains 123,287 images and pro-\n",
      "vides ﬁve captions per image in English ( en), but fewer\n",
      "in Chinese (zh) and Japanese (ja). STAIR Captions [34]\n",
      "extended MSCOCO [2] with 820K Japanese captions for\n",
      "COCO images. [20] extended MSCOCO [2] with Chinese\n",
      "captions for 20K images. We use the same train, dev, and\n",
      "test splits for English and Japanese as deﬁned in [14]. As for\n",
      "Chinese, we use the COCO-CN split [20].\n",
      "4.1.3 Code-switched Dictionary\n",
      "The word-level bilingual dictionaries used by Code-switched\n",
      "training are from Panlex, the world’s largest open-source\n",
      "lexical translation database. We extract top 50 scale English\n",
      "to other language bilingual dictionaries.\n",
      "4.2. Training Details\n",
      "Pre-training Details Similar to previous vision-language\n",
      "pre-trained models, the M 3P model uses the same model\n",
      "architecture as BERT [7]. We initialize M3P with XLM-R\n",
      "[4] and continue pre-training on our data. We use the same\n",
      "vocabulary as XLM-R [4], which includes 250K BPE tokens\n",
      "and covers 100 languages. We set the dropout rate to 0.1\n",
      "and the max input length to 128. We use Adam Optimizer\n",
      "[16] with a linear warm-up [ 30] and set the learning rate\n",
      "to 1 ×10−4. The total batch size is 1,024 after gradient\n",
      "accumulation. The pre-training stage takes about seven days\n",
      "†https://panlex.org\n",
      "to converge on 8 V100 GPUs. We use Multimodal Code-\n",
      "switched Training with all top 50 languages from Panlex.\n",
      "Fine-tuning Details The batch size is set to 512, and we\n",
      "sample three negative cases for each positive case in VLM.\n",
      "We experiment with different numbers of negative samples\n",
      "in {1,3,5}, and ﬁnd three yields the best results. We use\n",
      "Adam Optimizer with β1 = 0.9, β2 = 0.98 and 5 ×10−5 as\n",
      "the hyper-parameters of learning rate.\n",
      "4.3. Baselines\n",
      "We compare our work with several related work [15, 31,\n",
      "11, 32, 1], which are trained on downstream task datasets\n",
      "(MSCOCO and Multi30K) directly without pre-training. In\n",
      "addition, to make the comparison as fair as possible, we take\n",
      "Unicoder-VL as another baseline, as it employs the same\n",
      "pre-training data during image-language pre-training.\n",
      "Among the baselines, SMALR [1] uses machine transla-\n",
      "tion to augment Multi30K and MSCOCO. But considering\n",
      "that applying machine translation to translate English to all\n",
      "other supported languages lacks generalization and requires\n",
      "a large amount of translators, we leave this as an option for\n",
      "future work. Moreover, note that MULE is using different\n",
      "dev/test splits of MSCOCO compared with other models.\n",
      "It is also worth noticing that word-level dictionaries are\n",
      "only used in M3P, as theMultimodal Code-switched Training\n",
      "is ﬁrstly used in multilingual multimodal pre-training.\n",
      "4.4. Evaluation Settings\n",
      "Multilingual image-text retrieval is the task of ﬁnding the\n",
      "most relevant images given input texts in different languages,\n",
      "or vice versa. We use mean Recall (mR) as our metric, which\n",
      "is an averaged score of Recall@1, Recall@5, and Recall@10\n",
      "on image-to-text retrieval and text-to-image retrieval tasks.\n",
      "We compare M3P with baseline methods on multilingual\n",
      "image-text retrieval in four different settings:\n",
      "(i) w/o ﬁne-tune: apply M 3P to all test sets directly to\n",
      "obtain the evaluation results without ﬁne-tuning.\n",
      "(ii) w/ ﬁne-tune on en : ﬁne-tune M 3P on English and\n",
      "then apply the ﬁne-tuned model to all test sets.\n",
      "(iii) w/ ﬁne-tune on each : ﬁne-tune M 3P on each lan-\n",
      "guage and apply each model to the test set of this language.\n",
      "(iv) w/ ﬁne-tune on all: ﬁne-tune M3P for all languages\n",
      "using the merged labeled data and then apply the ﬁne-tuned\n",
      "model to all test sets.\n",
      "5. Results and Analysis\n",
      "In this section, we show the evaluation results of M 3P\n",
      "compared with existing work and conduct ablation studies\n",
      "in order to better understand the effect of the model.\n",
      "5\n",
      "Model Multi30K MSCOCO\n",
      "en de fr cs en ja zh\n",
      "Monolingual supervised results\n",
      "EmbN [31] 72.0 60.3 54.8 46.3 76.8 73.2 73.5\n",
      "PAR. EmbN [11] 69.0 62.6 60.6 54.1 78.3 76.0 74.8\n",
      "S-LIWE [32] 76.3 72.1 63.4 59.4 80.9 73.6 70.0\n",
      "MULE [15] 70.3 64.1 62.3 57.7 79.0 75.9 75.6\n",
      "SMALR [1] 74.5 69.8 65.9 64.8 81.5 77.5 76.7\n",
      "Monolingual results with multimodal pre-training\n",
      "Unicoder-VL (w/o ﬁne-tune) [19] 72.0 - - - 63.7 - -\n",
      "Unicoder-VL (w/ ﬁne-tune on en) [19] 88.1 - - - 89.2 - -\n",
      "Multilingual results with multimodal pre-training\n",
      "M3P (w/o ﬁne-tune) 57.9 36.8 27.1 20.4 63.1 33.3 32.3\n",
      "M3P (w/ ﬁne-tune on en) 87.4 58.5 46.0 36.8 88.6 53.8 56.0\n",
      "M3P (w/ ﬁne-tune on each) 87.4 82.1 67.3 65.0 88.6 80.1 75.8\n",
      "M3P (w/ ﬁne-tune on all) 87.7 82.7 73.9 72.2 88.7 87.9 86.2\n",
      "Table 2: Multilingual image-text retrieval results on Multi30K and MSCOCO. The metric is the mean Recall (mR). Each bold\n",
      "number indicates the best mR score in that column. We report the mR results of Unicoder-VL on the English test set, as it is\n",
      "pre-trained based on the same image-caption corpus (i.e., Conceptual Captions) with M3P .\n",
      "5.1. Overall Results\n",
      "From Table 2, we have several observations: (1) Our M3P\n",
      "model obtains the state-of-the-art results in all non-English\n",
      "languages, which shows its exciting multilingual multimodal\n",
      "transfer capability. (2) Similar to the observations reported\n",
      "in Unicoder [13, 22], the two fully-supervised settings (iii)\n",
      "w/ ﬁne-tune on each and (iv) w/ ﬁne-tune on all can lead to\n",
      "the best results. This means the same sentence in different\n",
      "languages may capture complementary information to help\n",
      "improve performance. (3) Comparing to Unicoder-VL that is\n",
      "pre-trained using English image-caption corpus (i.e. Concep-\n",
      "tual Captions) only, M3P performs worse on the English test\n",
      "set. The possible reason could be that, M3P needs to balance\n",
      "its multilingual capability over 100+ languages, rather than\n",
      "on English only. (4) In both setting (i) w/o ﬁne-tune and\n",
      "setting (ii) w/ ﬁne-tune on en, integrating Multimodal Code-\n",
      "switched Training (MCT) into M 3P can bring signiﬁcant\n",
      "gains on non-English datasets, which demonstrates good\n",
      "multilingual transfer ability of Multimodal Code-switched\n",
      "Training in the zero-shot setting. It is expected to see such\n",
      "gains become smaller in setting (iii) w/ ﬁne-tune on each and\n",
      "setting (iv) w/ ﬁne-tune on all, as M3P can learn alignments\n",
      "between images and languages from labeled data directly.\n",
      "5.2. Ablation Studies\n",
      "Although we achieve good results under different settings,\n",
      "we want to deep dive into more aspects of M3P: (1) whether\n",
      "Multimodal Code-switched Training (MCT) can provide a\n",
      "positive effect under all settings; (2) whether the number of\n",
      "languages used in MCT affects the performance; (3) whether\n",
      "different pre-training tasks affect the performance.\n",
      "5.2.1 The Impact of MCT\n",
      "To verify whether the Multimodal Code-switched Training\n",
      "(MCT) strategy can provide a positive effect in different\n",
      "settings, we compare the performance of M3P without MCT\n",
      "and M3P with MCT under all ﬁne-tuning settings.\n",
      "For each setting in Table 3, we observe: (1) MCT im-\n",
      "proves the performance on almost all languages, which\n",
      "shows its exciting robustness and expansibility, and (2) in\n",
      "both setting (i) and setting (ii), integrating MCT into M3P\n",
      "can bring signiﬁcant gains on non-English datasets, which\n",
      "demonstrates the good multilingual transferability of MCT.\n",
      "It is expected to see such gains become smaller in settings\n",
      "(iii) and (iv), as M3P can learn alignments between images\n",
      "and languages from labeled data directly.\n",
      "5.2.2 The Impact of Number of Languages in MCT\n",
      "To verify whether the number of languages inﬂuences the\n",
      "performance of Multimodal Code-switched Training (MCT),\n",
      "we conduct an experiment by pre-training M 3P by MCT\n",
      "with different numbers of languages and evaluate the model\n",
      "directly without ﬁne-tuning. We pre-train M 3P with the\n",
      "following settings: pre-train M 3P without MCT, pre-train\n",
      "M3P with MCT on 3 languages (de, fr, cs), 5 languages\n",
      "(de, fr, cs, ja, zh), and all 50 languages.\n",
      "6\n",
      "Setting Multi30K\n",
      "en de fr cs\n",
      "w/o ﬁne-tune\n",
      "M3P w/o MCT 54.9 28.9 25.2 13.5\n",
      "w/ MCT 57.9 36.8 27.1 20.4\n",
      "w/ ﬁne-tune on en\n",
      "M3P w/o MCT 86.0 48.6 37.1 34.6\n",
      "w/ MCT 87.4 58.5 46.0 36.8\n",
      "w/ ﬁne-tune on each\n",
      "M3P w/o MCT 86.0 80.2 67.1 66.2\n",
      "w/ MCT 87.4 82.1 67.3 65.0\n",
      "w/ ﬁne-tune on all\n",
      "M3P w/o MCT 86.7 82.0 73.5 70.2\n",
      "w/ MCT 87.7 82.7 73.9 72.2\n",
      "Table 3: The impact of MCT for multilingual image-text\n",
      "retrieval. The metric is the mean Recall (mR). Each bold\n",
      "number indicates the best mR score.\n",
      "Setting Multi30K\n",
      "en de fr cs\n",
      "M3P w/o MCT 54.9 28.9 25.2 13.5\n",
      "w/ 3 languages MCT 56.4 37.1 28.7 23.0\n",
      "w/ 5 languages MCT 58.2 36.7 26.9 23.6\n",
      "w/ 50 languages MCT (Full) 57.9 36.8 27.1 20.4\n",
      "Table 4: Impact of number of languages inMultimodal Code-\n",
      "switched Training (MCT). The metric is the mean Recall\n",
      "(mR). \"Full\" represents the model pre-trained with all Code-\n",
      "switching languages.\n",
      "In Table 4, we can ﬁnd that, for languages likede and fr,\n",
      "there is no signiﬁcant difference under different settings. On\n",
      "the contrary, for languages likeen and cs, M3P achieves the\n",
      "best performance when MCT is activated with 5 languages.\n",
      "This implies that activating MCT on more languages can\n",
      "lead to more noise due to a higher probability of inaccurate\n",
      "translation. This noise may improve the robustness of the\n",
      "model but make the model harder to be well-trained.\n",
      "5.2.3 The Impact of Proposed Tasks\n",
      "We want to ﬁnd whether each component during pre-training\n",
      "positively affects the performance and try to explain how\n",
      "they gain the performance by conducting several ablation\n",
      "experiments. Since Multimodal Code-switched Training\n",
      "(MCT) inﬂuences each task’s target, we conduct the ablation\n",
      "experiments on M3P without MCT and ﬁne-tune each model\n",
      "on the dataset of each language to compare the performance.\n",
      "Setting Multi30K\n",
      "en de fr cs\n",
      "M3P 86.0 80.2 67.1 66.2\n",
      "w/o xMLM 79.6 70.8 56.4 54.3\n",
      "w/o MC-MLM 84.3 76.2 64.1 62.2\n",
      "w/o MC-MRM 85.5 77.9 65.0 63.9\n",
      "w/o MC-VLM 75.4 68.3 52.7 50.9\n",
      "Table 5: Ablation study on multilingual image-text retrieval.\n",
      "The metric is the mean Recall (mR). Each bold number\n",
      "indicates the best mR score in that column.\n",
      "As shown in Table 5, we can observe that: (1) MC-VLM\n",
      "provides the most considerable improvement (+10.6 on en)\n",
      "to the model among all four sub-tasks during the pre-training\n",
      "stage. We suggest this is because the MC-VLM sub-task\n",
      "successfully models the relationship between image and text.\n",
      "(2) xMLM shows a great impact on non-English results com-\n",
      "pared with English results, which shows that xMLM will\n",
      "improve the capability of multilinguality. (3) MC-MLM and\n",
      "MC-MRM also show good support to the results in all lan-\n",
      "guages, which we suggest these two tasks will help the model\n",
      "learn the knowledge of multimodality. (4) When combining\n",
      "all tasks, we obtain the highest gain in all languages.\n",
      "5.3. Expanding MCT to Fine-tuning\n",
      "Setting Multi30K\n",
      "en de fr cs\n",
      "Pre-trained without MCT\n",
      "M3P (w/ Normal Fine-tune) 86.0 48.6 37.1 34.6\n",
      "M3P (w/ MCT Fine-tune) 85.4 67.8 59.2 54.0\n",
      "Pre-trained with MCT\n",
      "M3P (w/ Normal Fine-tune) 87.4 58.5 46.0 36.8\n",
      "M3P (w/ MCT Fine-tune) 86.4 71.8 62.3 59.6\n",
      "Table 6: The results of expanding MCT to ﬁne-tuning for\n",
      "multilingual image-text retrieval. The metric is the mean\n",
      "Recall (mR). Each bold numberindicates the best mR score\n",
      "under the setting. Normal Fine-tune represents ﬁne-tuning\n",
      "with English data directly and MCT Fine-tune represents\n",
      "ﬁne-tuning with Code-switched English data.\n",
      "Similar to MC-VLM, we use Code-switched data to ﬁne-\n",
      "tune M3P on Multi30K. The results in Table 6 show: (1)\n",
      "Multimodal Code-switched Training(MCT) can bring a large\n",
      "margin for non-English language probably because of the\n",
      "lack of labeled image-non English caption pairs during the\n",
      "pre-training stage or ﬁne-tuning stage. (2) Employing MCT\n",
      "into the ﬁne-tuning stage for the model, whatever pre-trained\n",
      "by, will achieve a large increase in non-English performance.\n",
      "7\n",
      "(3) MCT in ﬁne-tuning is more effective than MCT in pre-\n",
      "training, which may be explained by that the model can learn\n",
      "multilinguality in a more speciﬁc task. (4) The best results\n",
      "can be achieved when MCT replaces English in both the\n",
      "pre-training and ﬁne-tuning stages.\n",
      "5.4. Qualitative Studies on MCT\n",
      "To further explore how Multimodal Code-switched Train-\n",
      "ing (MCT) affects the model, we randomly select some\n",
      "text-image pairs generated from it. We want to ﬁgure out\n",
      "why Multimodal Code-switched Trainingis very effective on\n",
      "non-English languages and whether it has any limitations.\n",
      "Figure 2: Qualitative study for Multimodal Code-switched\n",
      "Training (MCT). The ﬁrst row in each table is the original\n",
      "text, and the second row in each table is the Code-switched\n",
      "text. We add the meaning of the Code-switched text in\n",
      "English in the third row of each table.\n",
      "As Figure 2 (a) shows, the meaning of the code-switched\n",
      "text generated by Multimodal Code-switched Training\n",
      "(MCT) is almost the same as that of the original text. Al-\n",
      "though there are some small differences between the original\n",
      "text (ﬁrst row) and the generated text translated back to En-\n",
      "glish (third row), it has no inﬂuence on the training quality,\n",
      "which demonstrates the reason why MCT brings gains. The\n",
      "key idea of using MCT in M3P is to let the model see more\n",
      "Code-switched text and image pairs and learn the joint mul-\n",
      "tilingual multimodal representations from such pairs directly.\n",
      "We guess this helps the model learn richer information of\n",
      "each token from the multilingual context.\n",
      "We did not consider the grammar or syntax correctness of\n",
      "the Code-switched sentences generated by replacing words\n",
      "in the English sentences with their word translations in\n",
      "other languages. The pre-trained models can learn such\n",
      "knowledge from well-formed multilingual sentences and En-\n",
      "glish caption sentences. Since we don’t have image-caption\n",
      "pairs or high-quality machine translation engines to generate\n",
      "such data for most languages, generating Code-switched sen-\n",
      "tences is the most effective way to let M3P directly see more\n",
      "alignments between non-English languages and images.\n",
      "Hence, because of the high accuracy of translation from\n",
      "MCT, multilingual results will signiﬁcantly increase when\n",
      "no non-English multimodal data is available. However, when\n",
      "the model can access high-quality multilingual multimodal\n",
      "data, the noise from MCT may limit its performance. In\n",
      "Figure 2 (b), we show a negative case in Code-switched text.\n",
      "MCT faultily changes the meaning of the original text. We\n",
      "leave this as future work to solve this problem.\n",
      "6. Conclusion\n",
      "We have presented in this paper a new pre-trained model\n",
      "M3P which combines Multilingual Pre-training and Multi-\n",
      "modal Pre-training into a uniﬁed framework via Multitask\n",
      "Pre-training for multilingual multimodal scenarios. We pro-\n",
      "posed Multimodal Code-switched Training to further allevi-\n",
      "ate the issue of lacking enough labeled data for non-English\n",
      "multimodal tasks and avoid the tendency to model the rela-\n",
      "tionship between vision and English text.\n",
      "References\n",
      "[1] Andrea Burns, Donghyun Kim, Derry Wijaya, Kate Saenko,\n",
      "and Bryan A Plummer. Learning to scale multilingual\n",
      "representations for vision-language tasks. arXiv preprint\n",
      "arXiv:2004.04312, 2020. 5, 6\n",
      "[2] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan-\n",
      "tam, Saurabh Gupta, Piotr Dollár, and C Lawrence Zitnick.\n",
      "Microsoft coco captions: Data collection and evaluation\n",
      "server. arXiv preprint arXiv:1504.00325, 2015. 2, 4, 5\n",
      "[3] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy,\n",
      "Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter:\n",
      "Learning universal image-text representations. arXiv preprint\n",
      "arXiv:1909.11740, 2019. 1, 2\n",
      "[4] Alexis Conneau, Kartikay Khandelwal, Naman Goyal,\n",
      "Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán,\n",
      "Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin\n",
      "Stoyanov. Unsupervised cross-lingual representation learning\n",
      "at scale. arXiv preprint arXiv:1911.02116, 2019. 1, 5\n",
      "[5] Alexis Conneau and Guillaume Lample. Cross-lingual lan-\n",
      "guage model pretraining. In Advances in Neural Information\n",
      "Processing Systems, pages 7057–7067, 2019. 1, 2, 3\n",
      "[6] Alexis Conneau, Guillaume Lample, Ruty Rinott, Adina\n",
      "Williams, Samuel R Bowman, Holger Schwenk, and Veselin\n",
      "Stoyanov. Xnli: Evaluating cross-lingual sentence representa-\n",
      "tions. arXiv preprint arXiv:1809.05053, 2018. 2\n",
      "[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\n",
      "Toutanova. Bert: Pre-training of deep bidirectional transform-\n",
      "ers for language understanding. In Proceedings of the 2019\n",
      "8\n",
      "Conference of the North American Chapter of the Association\n",
      "for Computational Linguistics, pages 4171–4186, 2019. 1, 2,\n",
      "3, 5\n",
      "[8] Desmond Elliott, Stella Frank, Loïc Barrault, Fethi Bougares,\n",
      "and Lucia Specia. Findings of the second shared task on\n",
      "multimodal machine translation and multilingual image de-\n",
      "scription. arXiv preprint arXiv:1710.07177, 2017. 5\n",
      "[9] Desmond Elliott, Stella Frank, Khalil Sima’an, and Lucia\n",
      "Specia. Multi30k: Multilingual english-german image de-\n",
      "scriptions. arXiv preprint arXiv:1605.00459, 2016. 5\n",
      "[10] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng,\n",
      "and Jingjing Liu. Large-scale adversarial training for\n",
      "vision-and-language representation learning. arXiv preprint\n",
      "arXiv:2006.06195, 2020. 1\n",
      "[11] Spandana Gella, Rico Sennrich, Frank Keller, and Mirella\n",
      "Lapata. Image pivoting for learning multilingual multimodal\n",
      "representations. In: Empirical Methods in Natural Language\n",
      "Processing (EMNLP) (2017), 2017. 5, 6\n",
      "[12] Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr\n",
      "Dollár, and Kaiming He. Detectron. 2018. 3\n",
      "[13] Haoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Lin-\n",
      "jun Shou, Daxin Jiang, and Ming Zhou. Unicoder: A universal\n",
      "language encoder by pre-training with multiple cross-lingual\n",
      "tasks. arXiv preprint arXiv:1909.00964, 2019. 1, 2, 3, 6\n",
      "[14] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic align-\n",
      "ments for generating image descriptions. In Proceedings of\n",
      "the IEEE conference on computer vision and pattern recogni-\n",
      "tion, pages 3128–3137, 2015. 5\n",
      "[15] Donghyun Kim, Kuniaki Saito, Kate Saenko, Stan Sclaroff,\n",
      "and Bryan A Plummer. Mule: Multimodal universal language\n",
      "embedding. In: AAAI Conference on Artiﬁcial Intelligence\n",
      "(2020), 2020. 5, 6\n",
      "[16] Diederik P Kingma and Jimmy Ba. Adam: A method for\n",
      "stochastic optimization. international conference on learning\n",
      "representations, 2015. 5\n",
      "[17] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\n",
      "Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\n",
      "tidis, Li-Jia Li, David A Shamma, et al. Visual genome:\n",
      "Connecting language and vision using crowdsourced dense\n",
      "image annotations. International Journal of Computer Vision,\n",
      "123(1):32–73, 2017. 2\n",
      "[18] Taku Kudo and John Richardson. Sentencepiece: A simple\n",
      "and language independent subword tokenizer and detokenizer\n",
      "for neural text processing. EMNLP, 2018. 2\n",
      "[19] Gen Li, Nan Duan, Yuejian Fang, Daxin Jiang, and Ming\n",
      "Zhou. Unicoder-vl: A universal encoder for vision and lan-\n",
      "guage by cross-modal pre-training. AAAI, 2020. 1, 2, 4,\n",
      "6\n",
      "[20] Xirong Li, Chaoxi Xu, Xiaoxu Wang, Weiyu Lan, Zhengx-\n",
      "iong Jia, Gang Yang, and Jieping Xu. Coco-cn for cross-\n",
      "lingual image tagging, captioning and retrieval. In IEEE\n",
      "Transactions on Multimedia, 2019. 4, 5\n",
      "[21] Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan\n",
      "Zhang, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu\n",
      "Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semantics\n",
      "aligned pre-training for vision-language tasks. arXiv preprint\n",
      "arXiv:2004.06165, 2020. 1, 2\n",
      "[22] Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo,\n",
      "Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong\n",
      "Cao, et al. Xglue: A new benchmark dataset for cross-lingual\n",
      "pre-training, understanding and generation. arXiv preprint\n",
      "arXiv:2004.01401, 2020. 6\n",
      "[23] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\n",
      "Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-\n",
      "moyer, and Veselin Stoyanov. Roberta: A robustly optimized\n",
      "bert pretraining approach. arXiv preprint arXiv:1907.11692,\n",
      "2019. 1\n",
      "[24] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert:\n",
      "Pretraining task-agnostic visiolinguistic representations for\n",
      "vision-and-language tasks. In Advances in Neural Information\n",
      "Processing Systems, pages 13–23, 2019. 2, 4\n",
      "[25] Takashi Miyazaki and Nobuyuki Shimizu. Cross-lingual\n",
      "image caption generation. In ACL, 2016. 5\n",
      "[26] Vicente Ordonez, Girish Kulkarni, and Tamara L Berg.\n",
      "Im2text: Describing images using 1 million captioned pho-\n",
      "tographs. In Advances in neural information processing sys-\n",
      "tems, pages 1143–1151, 2011. 2\n",
      "[27] Libo Qin, Minheng Ni, Yue Zhang, and Wanxiang Che.\n",
      "Cosda-ml: Multi-lingual code-switching data augmen-\n",
      "tation for zero-shot cross-lingual nlp. arXiv preprint\n",
      "arXiv:2006.06402, 2020. 2, 3\n",
      "[28] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\n",
      "Amodei, and Ilya Sutskever. Language models are unsuper-\n",
      "vised multitask learners. OpenAI blog, 1(8):9, 2019. 1\n",
      "[29] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\n",
      "Soricut. Conceptual captions: A cleaned, hypernymed, im-\n",
      "age alt-text dataset for automatic image captioning. In Pro-\n",
      "ceedings of the 56th Annual Meeting of the Association for\n",
      "Computational Linguistics (Volume 1: Long Papers), pages\n",
      "2556–2565, 2018. 2, 4, 5\n",
      "[30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\n",
      "reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\n",
      "Polosukhin. Attention is all you need. In Advances in neural\n",
      "information processing systems, pages 5998–6008, 2017. 5\n",
      "[31] Liwei Wang, Yin Li, Jing Huang, and Svetlana Lazebnik.\n",
      "Learning two-branch neural networks for image-text match-\n",
      "ing tasks. IEEE Transactions on Pattern Analysis and Ma-\n",
      "chine Intelligence, 41(2):394–407, 2018. 5, 6\n",
      "[32] Jônatas Wehrmann, Douglas M Souza, Mauricio A Lopes,\n",
      "and Rodrigo C Barros. Language-agnostic visual-semantic\n",
      "embeddings. In Proceedings of the IEEE International Con-\n",
      "ference on Computer Vision, pages 5804–5813, 2019. 5, 6\n",
      "[33] Zhen Yang, Bojie Hu, Ambyera Han, Shen Huang, and Qi Ju.\n",
      "Csp: Code-switching pre-training for neural machine trans-\n",
      "lation. In Proceedings of the 2020 Conference on Empirical\n",
      "Methods in Natural Language Processing (EMNLP), pages\n",
      "2624–2636, 2020. 2\n",
      "[34] Yuya Yoshikawa, Yutaro Shigeto, and Akikazu Takeuchi.\n",
      "Stair captions: Constructing a large-scale japanese image\n",
      "caption dataset. arXiv preprint arXiv:1705.00823, 2017. 4, 5\n",
      "[35] Peter Young, Alice Lai, Micah Hodosh, and Julia Hocken-\n",
      "maier. From image descriptions to visual denotations: New\n",
      "similarity metrics for semantic inference over event descrip-\n",
      "tions. Transactions of the Association for Computational\n",
      "Linguistics, 2:67–78, 2014. 4, 5\n",
      "9\n",
      "[36] Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu,\n",
      "and Haifeng Wang. Ernie-vil: Knowledge enhanced vision-\n",
      "language representations through scene graph. arXiv preprint\n",
      "arXiv:2006.16934, 2020. 1\n",
      "[37] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Ja-\n",
      "son J Corso, and Jianfeng Gao. Uniﬁed vision-language\n",
      "pre-training for image captioning and vqa. AAAI, 2020. 2\n",
      "10\n",
      "The following is a summary of the paper: M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training\n",
      "\n",
      "Summary: We present M3P, a Multitask Multilingual Multimodal Pre-trained model that\n",
      "combines multilingual pre-training and multimodal pre-training into a unified\n",
      "framework via multitask pre-training. Our goal is to learn universal\n",
      "representations that can map objects occurred in different modalities or texts\n",
      "expressed in different languages into a common semantic space. In addition, to\n",
      "explicitly encourage fine-grained alignment between images and non-English\n",
      "languages, we also propose Multimodal Code-switched Training (MCT) to combine\n",
      "monolingual pre-training and multimodal pre-training via a code-switch\n",
      "strategy. Experiments are performed on the multilingual image retrieval task\n",
      "across two benchmark datasets, including MSCOCO and Multi30K. M3P can achieve\n",
      "comparable results for English and new state-of-the-art results for non-English\n",
      "languages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "client = OpenAI(\n",
    "    base_url=\"http://20.81.188.27:8000/v1\",\n",
    "    api_key=\"token-abc123\",\n",
    ")\n",
    "'''\n",
    "\n",
    "def load_arxiv_papers(search_query: str, max_results: int = 1):    \n",
    "    loader = ArxivReader()\n",
    "    documents = loader.load_data(search_query=search_query, max_results=max_results)\n",
    "    return documents\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-Large-Instruct-2411\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "documents = load_arxiv_papers(\"(multimodality OR multitask learning) AND ((low-resource languages) OR (code-switching)) -SoupLM\", max_results=1)\n",
    "\n",
    "for document in documents:\n",
    "    print(document.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f2b268fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='a44d6c99-896d-4657-8b84-5d9016aa665f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The following is a summary of the paper: M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training\\n\\nSummary: We present M3P, a Multitask Multilingual Multimodal Pre-trained model that\\ncombines multilingual pre-training and multimodal pre-training into a unified\\nframework via multitask pre-training. Our goal is to learn universal\\nrepresentations that can map objects occurred in different modalities or texts\\nexpressed in different languages into a common semantic space. In addition, to\\nexplicitly encourage fine-grained alignment between images and non-English\\nlanguages, we also propose Multimodal Code-switched Training (MCT) to combine\\nmonolingual pre-training and multimodal pre-training via a code-switch\\nstrategy. Experiments are performed on the multilingual image retrieval task\\nacross two benchmark datasets, including MSCOCO and Multi30K. M3P can achieve\\ncomparable results for English and new state-of-the-art results for non-English\\nlanguages.', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49986c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.21s/it, est. speed input: 25.83 toks/s, output: 18.80 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=2, prompt='<s>[SYSTEM_PROMPT] You are an expert at creating high-quality training data for language models.\\nGiven academic research content, extract the key concepts and convert them into general knowledge questions and answers.\\nQuestions should:\\n- Be general and broadly applicable\\n- Focus on core concepts and ideas\\n- Be written as standalone questions without referencing any specific research\\n- Test understanding of the topic area\\n\\nAnswers should:\\n- Provide clear, factual information\\n- Be written as standalone knowledge\\n- Focus on general concepts rather than specific research\\n- Be useful for general learning about the topic[/SYSTEM_PROMPT][INST] Based on these concepts, generate 3 general knowledge questions and answers about: The following is a summary of the paper: M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training\\n\\nSummary: We present M3P, a Multitask Multilingual Multimodal Pre-trained model that\\ncombines multilingual pre-training and multimodal pre-training into a unified\\nframework via multitask pre-training. Our goal is to learn universal\\nrepresentations that can map objects occurred in different modalities or texts\\nexpressed in different languages into a common semantic space. In addition, to\\nexplicitly encourage fine-grained alignment between images and non-English\\nlanguages, we also propose Multimodal Code-switched Training (MCT) to combine\\nmonolingual pre-training and multimodal pre-training via a code-switch\\nstrategy. Experiments are performed on the multilingual image retrieval task\\nacross two benchmark datasets, including MSCOCO and Multi30K. M3P can achieve\\ncomparable results for English and new state-of-the-art results for non-English\\nlanguages.[/INST]', prompt_token_ids=[1, 1, 16, 1763, 1228, 1164, 8351, 1206, 7586, 2254, 29501, 15585, 4922, 1946, 1122, 4610, 5762, 29491, 781, 29545, 5835, 12628, 4100, 3804, 29493, 9899, 1040, 2713, 17350, 1072, 7371, 1474, 1546, 3720, 5556, 4992, 1072, 11962, 29491, 781, 15697, 1362, 1791, 29515, 781, 29501, 2507, 3720, 1072, 6609, 1114, 9575, 781, 29501, 25176, 1124, 7189, 17350, 1072, 6534, 781, 29501, 2507, 5009, 1158, 2644, 25947, 4992, 2439, 4063, 11463, 1475, 3716, 4100, 781, 29501, 4503, 7167, 1070, 1040, 9835, 3466, 781, 781, 3588, 2384, 1172, 1791, 29515, 781, 29501, 7901, 1315, 3849, 29493, 2407, 1608, 2639, 781, 29501, 2507, 5009, 1158, 2644, 25947, 5556, 781, 29501, 25176, 1124, 3720, 17350, 3978, 1589, 3716, 4100, 781, 29501, 2507, 6625, 1122, 3720, 5936, 1452, 1040, 9835, 17, 3, 17926, 1124, 1935, 17350, 29493, 9038, 29473, 29538, 3720, 5556, 4992, 1072, 11962, 1452, 29515, 1183, 3064, 1117, 1032, 14828, 1070, 1040, 4598, 29515, 1119, 29538, 29521, 29515, 18272, 22642, 18659, 1465, 4981, 10481, 1047, 1920, 10481, 6476, 1608, 10481, 1089, 1118, 1050, 5026, 29501, 26482, 781, 781, 18358, 29515, 1584, 2937, 1119, 29538, 29521, 29493, 1032, 10481, 1047, 1920, 10481, 6476, 1608, 10481, 1089, 1118, 1050, 5026, 29501, 29015, 2997, 1137, 781, 18073, 2071, 3299, 6476, 1608, 1478, 29501, 26482, 1072, 3299, 1089, 1118, 1050, 1478, 29501, 26482, 1546, 1032, 1289, 2567, 781, 7738, 4981, 3299, 1047, 1920, 1478, 29501, 26482, 29491, 4257, 6309, 1117, 1066, 3590, 15366, 781, 9925, 2812, 1465, 1137, 1309, 4109, 7465, 10662, 1065, 2349, 23714, 1986, 1210, 20105, 781, 1488, 12809, 1065, 2349, 14796, 1546, 1032, 4066, 4314, 6932, 3532, 29491, 1328, 5286, 29493, 1066, 781, 1488, 6794, 1114, 13597, 5201, 29501, 1588, 2506, 20260, 2212, 6971, 1072, 2611, 29501, 28639, 781, 29482, 11900, 29493, 1246, 1603, 20101, 10481, 1089, 1118, 1050, 8364, 29501, 2384, 14977, 16162, 1093, 29523, 2154, 29499, 1066, 18238, 781, 3408, 1096, 1056, 1608, 1478, 29501, 26482, 1072, 3299, 1089, 1118, 1050, 1478, 29501, 26482, 4981, 1032, 3464, 29501, 8020, 781, 1810, 5935, 29491, 11903, 9215, 1228, 8653, 1124, 1040, 3299, 6476, 1608, 4237, 18681, 1050, 4406, 781, 1091, 2324, 1757, 12752, 4093, 27650, 29493, 3258, 9626, 2766, 2766, 1072, 19085, 29538, 29502, 29564, 29491, 1119, 29538, 29521, 1309, 7387, 781, 24986, 1290, 3671, 1122, 5068, 1072, 1401, 2433, 29501, 1777, 29501, 2005, 29501, 1212, 3671, 1122, 2611, 29501, 28639, 781, 29482, 11900, 29491, 4], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='  {\\n  \"question1\": \"What is the primary goal of multimodal pre-training in language models?\",\\n  \"answer1\": \"The primary goal of multimodal pre-training in language models is to learn universal representations that can map objects occurring in different modalities, such as images and text, into a common semantic space. This allows the model to understand and relate information across different types of input.\"\\n\\n  ,\\n  \"question2\": \"What is the purpose of multilingual pre-training in language models?\",\\n  \"answer2\": \"The purpose of multilingual pre-training in language models is to learn universal representations that can map texts expressed in different languages into a common semantic space. This enables the model to understand and process multiple languages effectively, facilitating multilingual tasks such as cross-lingual retrieval.\"\\n\\n  ,\\n  \"question3\": \"What is a common task used to evaluate multilingual multimodal language models?\",\\n  \"answer3\": \"A common task used to evaluate multilingual multimodal language models is multilingual image retrieval. This task involves retrieving relevant images based on text queries in multiple languages, and it helps assess the model\\'s ability to understand and align visual and linguistic information across different languages.\"\\n}', token_ids=(29473, 1139, 781, 29473, 1113, 7679, 1459, 29479, 29508, 2032, 1113, 3963, 1117, 1040, 7026, 6309, 1070, 3299, 1089, 1118, 1050, 1478, 29501, 26482, 1065, 4610, 5762, 29572, 1316, 781, 29473, 1113, 24883, 29508, 2032, 1113, 1782, 7026, 6309, 1070, 3299, 1089, 1118, 1050, 1478, 29501, 26482, 1065, 4610, 5762, 1117, 1066, 3590, 15366, 24152, 1137, 1309, 4109, 7465, 29033, 1065, 2349, 23714, 1986, 29493, 2027, 1158, 6971, 1072, 3013, 29493, 1546, 1032, 4066, 4314, 6932, 3532, 29491, 1619, 6744, 1040, 2997, 1066, 3148, 1072, 22274, 2639, 3441, 2349, 5282, 1070, 3555, 1379, 781, 781, 29473, 1968, 781, 29473, 1113, 18264, 29518, 2032, 1113, 3963, 1117, 1040, 6800, 1070, 3299, 6476, 1608, 1478, 29501, 26482, 1065, 4610, 5762, 29572, 1316, 781, 29473, 1113, 24883, 29518, 2032, 1113, 1782, 6800, 1070, 3299, 6476, 1608, 1478, 29501, 26482, 1065, 4610, 5762, 1117, 1066, 3590, 15366, 24152, 1137, 1309, 4109, 20105, 12326, 1065, 2349, 14796, 1546, 1032, 4066, 4314, 6932, 3532, 29491, 1619, 18924, 1040, 2997, 1066, 3148, 1072, 2527, 5934, 14796, 12234, 29493, 13799, 24019, 3299, 6476, 1608, 10564, 2027, 1158, 4661, 29501, 2673, 1608, 18681, 1050, 1379, 781, 781, 29473, 1968, 781, 29473, 1113, 18264, 29538, 2032, 1113, 3963, 1117, 1032, 4066, 4406, 2075, 1066, 16395, 3299, 6476, 1608, 3299, 1089, 1118, 1050, 4610, 5762, 29572, 1316, 781, 29473, 1113, 24883, 29538, 2032, 1113, 29509, 4066, 4406, 2075, 1066, 16395, 3299, 6476, 1608, 3299, 1089, 1118, 1050, 4610, 5762, 1117, 3299, 6476, 1608, 4237, 18681, 1050, 29491, 1619, 4406, 15425, 18681, 1056, 9366, 6971, 3586, 1124, 3013, 24449, 1065, 5934, 14796, 29493, 1072, 1146, 8031, 8852, 1040, 2997, 29510, 29481, 6305, 1066, 3148, 1072, 9521, 9577, 1072, 24785, 4088, 2639, 3441, 2349, 14796, 1379, 781, 29520, 2), cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1735075432.0527751, last_token_time=1735075432.0527751, first_scheduled_time=1735075432.0561054, first_token_time=1735075432.3742526, time_in_queue=0.003330230712890625, finished_time=1735075447.267877, scheduler_time=0.044779819931136444, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "qa_response = generate_qa(document.text)\n",
    "my_response = \"\"\n",
    "for response in qa_response:\n",
    "    my_response += response.outputs[0].text\n",
    "\n",
    "my_response\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bef6996c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:34<00:00, 34.13s/it, est. speed input: 20.92 toks/s, output: 18.87 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t{\\n\\t\"qa_pair1_arguments_for\":\\n\\t[\\n\\t\"The question directly addresses the core concept of multimodal pre-training, which is a central theme in the paper.\"\\n\\t,\"The answer accurately and clearly summarizes the primary goal of multimodal pre-training, making it a valuable addition to the dataset.\"\\n\\t,\"The pair emphasizes the importance of learning universal representations, which is a key contribution of the M3P model.\"\\n\\t]\\n\\t,\"qa_pair1_arguments_against\":\\n\\t[\\n\\t\"The answer is quite general and might not capture the specific nuances of the M3P model\\'s approach to multimodal pre-training.\"\\n\\t,\"The phrase \\'such as images and text\\' is not explicitly mentioned in the original text and might introduce slight inaccuracies.\"\\n\\t]\\n\\t,\"qa_pair1_include\": \"y\"\\n\\t,\"qa_pair2_arguments_for\":\\n\\t[\\n\\t\"The question focuses on another crucial aspect of the M3P model—multilingual pre-training.\"\\n\\t,\"The answer accurately explains the purpose of multilingual pre-training, which is to map texts in different languages into a common semantic space.\"\\n\\t,\"This pair complements the first pair by highlighting the multilingual capabilities of the model.\"\\n\\t]\\n\\t,\"qa_pair2_arguments_against\":\\n\\t[\\n\\t\"Similar to the first pair, the answer is general and does not delve into the specifics of the M3P model\\'s approach to multilingual pre-training.\"\\n\\t,\"The phrase \\'facilitating multilingual tasks such as cross-lingual retrieval\\' goes beyond the information presented in the original text and might introduce additional assumptions.\"\\n\\t]\\n\\t,\"qa_pair2_include\": \"y\"\\n\\t,\"qa_pair3_arguments_for\":\\n\\t[\\n\\t\"The question addresses an important evaluation metric for multilingual multimodal language models, making it relevant for understanding the practical applications of the M3P model.\"\\n\\t,\"The answer correctly identifies multilingual image retrieval as a common task for evaluating such models, which is supported by the original text.\"\\n\\t]\\n\\t,\"qa_pair3_arguments_against\":\\n\\t[\\n\\t\"The answer does not specify that the evaluation was performed on benchmark datasets like MSCOCO and Multi30K, which are mentioned in the original text.\"\\n\\t,\"The phrase \\'helps assess the model\\'s ability to understand and align visual and linguistic information across different languages\\' is somewhat inferential and not directly stated in the original text.\"\\n\\t]\\n\\t,\"qa_pair3_include\": \"y\"\\n\\t,\"next_search_query\": \"Cross-modal contrastive learning for zero-shot image-to-text translation\"\\n}'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "evaluation_response = evaluate_qa(document.text, QA.parse_raw(my_response))\n",
    "evaluation_response\n",
    "\n",
    "my_eval_response = \"\"\n",
    "for response in evaluation_response:\n",
    "    my_eval_response += response.outputs[0].text\n",
    "\n",
    "my_eval_response\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73851ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-25 08:11:17 config.py:2167] Downcasting torch.float32 to torch.float16.\n",
      "INFO 12-25 08:11:23 config.py:478] This model supports multiple tasks: {'classify', 'reward', 'score', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 12-25 08:11:23 config.py:1216] Defaulting to use mp for distributed inference\n",
      "WARNING 12-25 08:11:23 arg_utils.py:1086] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 12-25 08:11:23 config.py:1364] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 12-25 08:11:23 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='/home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411', speculative_config=None, tokenizer='/home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='lm-format-enforcer'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 12-25 08:11:23 multiproc_worker_utils.py:312] Reducing Torch parallelism from 96 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 12-25 08:11:23 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85253)\u001b[0;0m INFO 12-25 08:11:24 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85253)\u001b[0;0m INFO 12-25 08:11:24 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85255)\u001b[0;0m INFO 12-25 08:11:24 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85255)\u001b[0;0m INFO 12-25 08:11:24 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85254)\u001b[0;0m INFO 12-25 08:11:24 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85254)\u001b[0;0m INFO 12-25 08:11:24 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "INFO 12-25 08:11:24 selector.py:120] Using Flash Attention backend.\n",
      "INFO 12-25 08:11:26 utils.py:922] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85253)\u001b[0;0m INFO 12-25 08:11:26 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85255)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=85254)\u001b[0;0m INFO 12-25 08:11:26 utils.py:922] Found nccl from library libnccl.so.2\n",
      "INFO 12-25 08:11:26 utils.py:922] Found nccl from library libnccl.so.2\n",
      "INFO 12-25 08:11:26 utils.py:922] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85253)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=85255)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=85254)\u001b[0;0m INFO 12-25 08:11:26 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-25 08:11:26 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-25 08:11:26 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "WARNING 12-25 08:11:26 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85253)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=85254)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=85255)\u001b[0;0m WARNING 12-25 08:11:26 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 12-25 08:11:26 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 12-25 08:11:26 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 12-25 08:11:27 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_1bdb05c6'), local_subscribe_port=47753, remote_subscribe_port=None)\n",
      "INFO 12-25 08:11:27 model_runner.py:1092] Starting to load model /home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85253)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=85254)\u001b[0;0m INFO 12-25 08:11:27 model_runner.py:1092] Starting to load model /home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411...\n",
      "INFO 12-25 08:11:27 model_runner.py:1092] Starting to load model /home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85255)\u001b[0;0m INFO 12-25 08:11:27 model_runner.py:1092] Starting to load model /home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1886516b9094f9cb386d32eb86af24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/51 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=85255)\u001b[0;0m INFO 12-25 08:12:03 model_runner.py:1097] Loading model weights took 57.1291 GB\n",
      "INFO 12-25 08:12:03 model_runner.py:1097] Loading model weights took 57.1291 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85254)\u001b[0;0m INFO 12-25 08:12:04 model_runner.py:1097] Loading model weights took 57.1291 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85253)\u001b[0;0m INFO 12-25 08:12:04 model_runner.py:1097] Loading model weights took 57.1291 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85254)\u001b[0;0m INFO 12-25 08:12:07 worker.py:241] Memory profiling takes 2.76 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85254)\u001b[0;0m INFO 12-25 08:12:07 worker.py:241] the current vLLM instance can use total_gpu_memory (79.14GiB) x gpu_memory_utilization (0.90) = 71.22GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85254)\u001b[0;0m INFO 12-25 08:12:07 worker.py:241] model weights take 57.13GiB; non_torch_memory takes 0.36GiB; PyTorch activation peak memory takes 0.27GiB; the rest of the memory reserved for KV Cache is 13.46GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85255)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=85253)\u001b[0;0m INFO 12-25 08:12:07 worker.py:241] Memory profiling takes 2.79 seconds\n",
      "INFO 12-25 08:12:07 worker.py:241] Memory profiling takes 2.79 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85255)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=85253)\u001b[0;0m INFO 12-25 08:12:07 worker.py:241] the current vLLM instance can use total_gpu_memory (79.14GiB) x gpu_memory_utilization (0.90) = 71.22GiB\n",
      "INFO 12-25 08:12:07 worker.py:241] the current vLLM instance can use total_gpu_memory (79.14GiB) x gpu_memory_utilization (0.90) = 71.22GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85255)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=85253)\u001b[0;0m INFO 12-25 08:12:07 worker.py:241] model weights take 57.13GiB; non_torch_memory takes 0.36GiB; PyTorch activation peak memory takes 0.27GiB; the rest of the memory reserved for KV Cache is 13.46GiB.\n",
      "INFO 12-25 08:12:07 worker.py:241] model weights take 57.13GiB; non_torch_memory takes 0.37GiB; PyTorch activation peak memory takes 0.27GiB; the rest of the memory reserved for KV Cache is 13.45GiB.\n",
      "INFO 12-25 08:12:07 worker.py:241] Memory profiling takes 2.84 seconds\n",
      "INFO 12-25 08:12:07 worker.py:241] the current vLLM instance can use total_gpu_memory (79.14GiB) x gpu_memory_utilization (0.90) = 71.22GiB\n",
      "INFO 12-25 08:12:07 worker.py:241] model weights take 57.13GiB; non_torch_memory takes 0.38GiB; PyTorch activation peak memory takes 0.35GiB; the rest of the memory reserved for KV Cache is 13.37GiB.\n",
      "INFO 12-25 08:12:07 distributed_gpu_executor.py:57] # GPU blocks: 9954, # CPU blocks: 2978\n",
      "INFO 12-25 08:12:07 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 1.22x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85255)\u001b[0;0m INFO 12-25 08:12:11 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85255)\u001b[0;0m INFO 12-25 08:12:11 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-25 08:12:11 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-25 08:12:11 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85253)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=85254)\u001b[0;0m INFO 12-25 08:12:11 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-25 08:12:11 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85253)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=85254)\u001b[0;0m INFO 12-25 08:12:11 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-25 08:12:11 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85254)\u001b[0;0m INFO 12-25 08:12:41 model_runner.py:1527] Graph capturing finished in 30 secs, took 1.02 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85253)\u001b[0;0m INFO 12-25 08:12:41 model_runner.py:1527] Graph capturing finished in 30 secs, took 1.02 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85255)\u001b[0;0m INFO 12-25 08:12:41 model_runner.py:1527] Graph capturing finished in 31 secs, took 1.02 GiB\n",
      "INFO 12-25 08:12:41 model_runner.py:1527] Graph capturing finished in 30 secs, took 1.02 GiB\n",
      "INFO 12-25 08:12:41 llm_engine.py:446] init engine (profile, create kv cache, warmup model) took 37.28 seconds\n",
      "\n",
      "🎓 WAND UNIVERSITY KNOWLEDGE ACQUISITION SYSTEM INITIALIZED 🎓\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SEARCHING:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 SEARCHING ARCHIVES WITH FOCUS: large language models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PROCESSING MANUSCRIPTS: 100%|██████████| 51/51 [00:00<00:00, 32142.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📜 ANALYZING MAGICAL MANUSCRIPT:\n",
      "================================================================================\n",
      "\n",
      "Lost in Translation\n",
      "May 2023\n",
      "A report from\n",
      "Gabriel Nicholas \n",
      "Aliya Bhatia\n",
      "Large Language Models in \n",
      "Non-English Content AnalysisGABRIEL NICHOLAS\n",
      "Research Fellow at the Center for Democracy & T echnology.\n",
      "ALIYA BHATIA\n",
      "Policy Analyst, Free Expression Project at the Center for \n",
      "Democracy & T echnology.\n",
      "The Center for Democracy & T echnology (CDT) is the leading \n",
      "nonpartisan, nonprofit organization fighting to advance civil rights and \n",
      "civil liberties in the digital age. W e shape technology policy,......\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "🔮 INITIATING KNOWLEDGE SYNTHESIS AND EVALUATION 🔮\n",
      "📖 GENERATING INITIAL KNOWLEDGE EXCHANGES...\n",
      "INFO 12-25 08:12:43 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts: 100%|██████████| 1/1 [00:56<00:00, 56.78s/it, est. speed input: 702.28 toks/s, output: 8.29 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎭 DEBATE ROUND 1 OF 3\n",
      "⚖️ EVALUATING CURRENT KNOWLEDGE EXCHANGES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:53<00:00, 53.64s/it, est. speed input: 754.42 toks/s, output: 7.66 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 GENERATING IMPROVED KNOWLEDGE EXCHANGES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [01:05<00:00, 65.71s/it, est. speed input: 613.17 toks/s, output: 9.22 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎭 DEBATE ROUND 2 OF 3\n",
      "⚖️ EVALUATING CURRENT KNOWLEDGE EXCHANGES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:54<00:00, 54.45s/it, est. speed input: 745.92 toks/s, output: 7.82 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 GENERATING IMPROVED KNOWLEDGE EXCHANGES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:58<00:00, 58.07s/it, est. speed input: 693.68 toks/s, output: 8.37 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎭 DEBATE ROUND 3 OF 3\n",
      "⚖️ EVALUATING CURRENT KNOWLEDGE EXCHANGES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:51<00:00, 51.68s/it, est. speed input: 783.40 toks/s, output: 7.41 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌟 INITIATING ARCHIVAL PROCESS OF ENCHANTED DIALOGUES 🌟\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALIDATING KNOWLEDGE EXCHANGES: 100%|██████████| 3/3 [00:00<00:00, 21363.18it/s]\n",
      "SEARCHING:  50%|█████     | 1/2 [05:43<05:43, 343.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚠️ DISCARDED KNOWLEDGE EXCHANGE 1:\n",
      "📝 INQUIRY: What are multilingual language models, and how do they work?\n",
      "💭 RESPONSE: Multilingual language models are a type of language model trained on text from multiple languages simultaneously. They work by inferring connections between languages, allowing them to apply word associations and grammatical rules learned from languages with more data to those with less. For example, a model trained on text from dozens of languages, including Hindi, might perform better in Hindi contexts than a model just trained on Hindi text. These models can be used for various natural language processing tasks, such as sentiment analysis, text summarization, and hate speech detection.\n",
      "❌ CRITICAL FEEDBACK:\n",
      "  • The answer oversimplifies the complexities and nuances of how multilingual language models work, which could lead to misunderstandings.\n",
      "\n",
      "✨ ACCEPTED KNOWLEDGE EXCHANGE 2:\n",
      "📝 INQUIRY: What are some of the challenges faced by multilingual language models?\n",
      "💭 RESPONSE: Multilingual language models face several challenges. They often rely on machine-translated text, which can contain errors or terms that native language speakers don’t actually use. This is known as the 'translationese' problem. Additionally, these models struggle to account for the contexts of local language speakers and may not work equally well in all languages due to the 'curse of multilinguality,' where the more languages a model is trained on, the less it can capture unique traits of any specific language. When these models fail, their problems are hard to identify, diagnose, and fix because of their complex and opaque nature.\n",
      "✅ POSITIVE FEEDBACK:\n",
      "  • The answer provides a clear overview of the main challenges faced by multilingual language models, as discussed in the paper.\n",
      "\n",
      "⚠️ DISCARDED KNOWLEDGE EXCHANGE 3:\n",
      "📝 INQUIRY: What recommendations does the report provide for companies deploying large language models?\n",
      "💭 RESPONSE: The report recommends that companies deploying large language models should disclose when, how, and in what languages they use these models to better understand potential problems and challenges. They should also provide adequate remedial channels and mechanisms to ensure individuals can appeal outcomes and decisions made by these systems. Additionally, companies should invest in improving language model performance in individual languages by involving language and context experts and conducting human rights impact assessments at different phases of the model’s life cycle.\n",
      "❌ CRITICAL FEEDBACK:\n",
      "  • The answer does not capture the full scope of recommendations provided in the paper, such as the need for better transparency and accountability mechanisms.\n",
      "\n",
      "📚 TRANSFORMING KNOWLEDGE INTO STRUCTURED FORMAT...\n",
      "💾 PRESERVING IN THE GRAND ARCHIVES...\n",
      "✨ SUCCESSFULLY ARCHIVED 1 EXCHANGES ✨\n",
      "📊 PRESERVED 1 MAGICAL EXCHANGES. TOTAL IN ARCHIVES: 1\n",
      "🎯 NEXT RESEARCH FOCUS: impact of language models on indigenous languages\n",
      "\n",
      "🔄 COMPLETED RESEARCH CYCLE 1. NEXT FOCUS: impact of language models on indigenous languages\n",
      "\n",
      "🔍 SEARCHING ARCHIVES WITH FOCUS: impact of language models on indigenous languages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PROCESSING MANUSCRIPTS: 100%|██████████| 6/6 [00:00<00:00, 56425.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📜 ANALYZING MAGICAL MANUSCRIPT:\n",
      "================================================================================\n",
      "\n",
      "Evaluating Machine Perception of Indigeneity: An Analysis of ChatGPT’s\n",
      "Perceptions of Indigenous Roles in Diverse Scenarios\n",
      "CECILIA DELGADO SOLORZANO,Clemson University, United States\n",
      "CARLOS TOXTLI,Clemson University, United States\n",
      "Large Language Models (LLMs), like ChatGPT, are fundamentally tools trained on vast data, reflecting diverse societal impressions.\n",
      "This paper aims to investigate LLMs’ self-perceived bias concerning indigeneity when simulating scenarios of indigenous people\n",
      "performing......\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "🔮 INITIATING KNOWLEDGE SYNTHESIS AND EVALUATION 🔮\n",
      "📖 GENERATING INITIAL KNOWLEDGE EXCHANGES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.81s/it, est. speed input: 180.04 toks/s, output: 16.93 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎭 DEBATE ROUND 1 OF 3\n",
      "⚖️ EVALUATING CURRENT KNOWLEDGE EXCHANGES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:28<00:00, 28.14s/it, est. speed input: 185.41 toks/s, output: 16.84 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 GENERATING IMPROVED KNOWLEDGE EXCHANGES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:29<00:00, 29.42s/it, est. speed input: 173.76 toks/s, output: 17.00 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎭 DEBATE ROUND 2 OF 3\n",
      "⚖️ EVALUATING CURRENT KNOWLEDGE EXCHANGES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.79s/it, est. speed input: 205.24 toks/s, output: 16.56 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 GENERATING IMPROVED KNOWLEDGE EXCHANGES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:28<00:00, 28.78s/it, est. speed input: 175.57 toks/s, output: 16.96 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎭 DEBATE ROUND 3 OF 3\n",
      "⚖️ EVALUATING CURRENT KNOWLEDGE EXCHANGES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.87s/it, est. speed input: 333.35 toks/s, output: 15.18 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌟 INITIATING ARCHIVAL PROCESS OF ENCHANTED DIALOGUES 🌟\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALIDATING KNOWLEDGE EXCHANGES: 100%|██████████| 3/3 [00:00<00:00, 17213.29it/s]\n",
      "SEARCHING: 100%|██████████| 2/2 [08:23<00:00, 251.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✨ ACCEPTED KNOWLEDGE EXCHANGE 1:\n",
      "📝 INQUIRY: What is the primary aim of the paper 'Evaluating Machine Perception of Indigeneity: An Analysis of ChatGPT’s Perceptions of Indigenous Roles in Diverse Scenarios'?\n",
      "💭 RESPONSE: The primary aim of the paper is to investigate Large Language Models' (LLMs) self-perceived bias concerning indigeneity when simulating scenarios of indigenous people performing various roles. The study seeks to understand how technology perceives and potentially amplifies societal biases related to indigeneity in social computing, offering insights into the broader implications for critical computing.\n",
      "✅ POSITIVE FEEDBACK:\n",
      "\n",
      "✨ ACCEPTED KNOWLEDGE EXCHANGE 2:\n",
      "📝 INQUIRY: What methodology was used in the research to evaluate ChatGPT's perceptions of indigeneity?\n",
      "💭 RESPONSE: The research employed a scenario evaluation approach using a Large Language Model (LLM) to generate and analyze text samples. The study considered three scenarios—a field, a technology store, and a subway—with two ethnicities: White/Caucasian as the control group and Indigenous as the treatment group. For each combination, the LLM generated 30 unique text samples, which were then analyzed for stereotypes and biases. The analysis involved labeling potential stereotypes based on societal perceptions and comparing these labels to human-provided ground truth labels.\n",
      "✅ POSITIVE FEEDBACK:\n",
      "\n",
      "✨ ACCEPTED KNOWLEDGE EXCHANGE 3:\n",
      "📝 INQUIRY: What were the key findings of the study regarding ChatGPT's perceptions of indigeneity in different scenarios?\n",
      "💭 RESPONSE: The study found that ChatGPT tended to reinforce subtle biases and stereotypes in different scenarios. For instance, in the field scenario, indigenous individuals were strongly stereotyped in 80% of samples, reinforcing tropes like deep nature connections and ancestral practices. In the technology store scenario, 26% of samples linked indigenous customers to cultural products. In the subway scenario, 93% of samples contained exoticized stereotypes about indigenous riders. These findings highlight how the LLM internalizes and projects societal biases, often 'othering' indigenous individuals.\n",
      "✅ POSITIVE FEEDBACK:\n",
      "\n",
      "📚 TRANSFORMING KNOWLEDGE INTO STRUCTURED FORMAT...\n",
      "💾 PRESERVING IN THE GRAND ARCHIVES...\n",
      "✨ SUCCESSFULLY ARCHIVED 3 EXCHANGES ✨\n",
      "📊 PRESERVED 3 MAGICAL EXCHANGES. TOTAL IN ARCHIVES: 4\n",
      "🎯 NEXT RESEARCH FOCUS: The intersection of AI and indigenous knowledge systems: Exploring the ethical implications of AI in preserving and promoting indigenous languages\n",
      "\n",
      "🔄 COMPLETED RESEARCH CYCLE 2. NEXT FOCUS: The intersection of AI and indigenous knowledge systems: Exploring the ethical implications of AI in preserving and promoting indigenous languages\n",
      "\n",
      "🏆 GRAND ARCHIVE COMPLETE! TOTAL MAGICAL EXCHANGES PRESERVED: 4 🏆\n",
      "INFO 12-25 08:21:05 multiproc_worker_utils.py:140] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=85253)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=85254)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=85255)\u001b[0;0m INFO 12-25 08:21:05 multiproc_worker_utils.py:247] Worker exiting\n",
      "INFO 12-25 08:21:05 multiproc_worker_utils.py:247] Worker exiting\n",
      "INFO 12-25 08:21:05 multiproc_worker_utils.py:247] Worker exiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully delete the llm pipeline and free the GPU memory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrainsilves\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/lain/wandb/run-20241225_082107-dk5yze9y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rainsilves/wand-university-lora/runs/dk5yze9y' target=\"_blank\">arcane-knowledge-transfer-incremental</a></strong> to <a href='https://wandb.ai/rainsilves/wand-university-lora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rainsilves/wand-university-lora' target=\"_blank\">https://wandb.ai/rainsilves/wand-university-lora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rainsilves/wand-university-lora/runs/dk5yze9y' target=\"_blank\">https://wandb.ai/rainsilves/wand-university-lora/runs/dk5yze9y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔮 SUMMONING TRAINING DATA FROM THE ANCIENT GRIMOIRE... 📚 [Model: mistralai/Mistral-7B-v0.1]\n",
      "⚡ INVOKING THE SACRED TOKENIZER AND MODEL... 🤖 [Sequence Length: 4096]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450aa099b3cc4f2ca91b3fefd89581cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 LOADING EXISTING LORA ENCHANTMENT FROM ./wand_university_lora...\n",
      "trainable params: 0 || all params: 7,262,992,384 || trainable%: 0.0000\n",
      "📜 PREPARING THE SACRED TEXTS... ✨ [Dataset Size: 278 entries]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683c1092281b4cf6a6273d9a60ad6fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/278 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌟 COMMENCING THE SACRED LORA TRAINING RITUAL... 🧙‍♂️ [Epochs: 1, Learning Rate: 1e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='278' max='278' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [278/278 00:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 PRESERVING THE ENHANCED MODEL IN THE ARCANE ARCHIVES... 📦 [Output Directory: ./wand_university_lora]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 CONCLUDING THE WANDB LOGGING RITUAL... 🔮\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1028922153799680.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>278</td></tr><tr><td>train_loss</td><td>1.42083</td></tr><tr><td>train_runtime</td><td>57.6542</td></tr><tr><td>train_samples_per_second</td><td>4.822</td></tr><tr><td>train_steps_per_second</td><td>4.822</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">arcane-knowledge-transfer-incremental</strong> at: <a href='https://wandb.ai/rainsilves/wand-university-lora/runs/dk5yze9y' target=\"_blank\">https://wandb.ai/rainsilves/wand-university-lora/runs/dk5yze9y</a><br/> View project at: <a href='https://wandb.ai/rainsilves/wand-university-lora' target=\"_blank\">https://wandb.ai/rainsilves/wand-university-lora</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241225_082107-dk5yze9y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 THE SACRED LORA TRAINING RITUAL IS COMPLETE! 🎊 [Total Training Steps: 278]\n",
      "🧹 CLEANING THE LORA ENCHANTMENT ARTIFACTS... ✨\n",
      "🔍 FOUND 1 NON-LORA KEYS TO REMOVE...\n",
      "💾 SAVING CLEANED LORA ENCHANTMENT... 📦\n",
      "🧪 BEGINNING MODEL EVALUATION RITUAL... 📊\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrainsilves\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/lain/wandb/run-20241225_082226-bssulwkp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33myouthful-meadow-96\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rainsilves/rebuttal-response\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rainsilves/rebuttal-response/runs/bssulwkp\u001b[0m\n",
      "2024-12-25:08:22:26,753 INFO     [__main__.py:279] Verbosity set to INFO\n",
      "2024-12-25:08:22:26,854 INFO     [__init__.py:491] `group` and `group_alias` keys in TaskConfigs are deprecated and will be removed in v0.4.5 of lm_eval. The new `tag` field will be used to allow for a shortcut to a group of tasks one does not wish to aggregate metrics across. `group`s which aggregate across subtasks must be only defined in a separate group config file, which will be the official way to create groups that support cross-task aggregation as in `mmlu`. Please see the v0.4.4 patch notes and our documentation: https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/new_task_guide.md#advanced-group-configs for more information.\n",
      "2024-12-25:08:22:31,790 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
      "2024-12-25:08:22:31,791 INFO     [__main__.py:376] Selected Tasks: ['gsm8k_cot']\n",
      "2024-12-25:08:22:31,794 INFO     [evaluator.py:161] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-12-25:08:22:31,794 WARNING  [evaluator.py:172] generation_kwargs specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\n",
      "2024-12-25:08:22:31,794 INFO     [evaluator.py:198] Initializing vllm model, with arguments: {'pretrained': 'mistralai/Mistral-7B-v0.1', 'dtype': 'auto', 'tensor_parallel_size': 4, 'distributed_executor_backend': 'ray', 'enable_lora': True, 'lora_local_path': '/home/lain/wand_university_lora'}\n",
      "INFO 12-25 08:22:37 config.py:478] This model supports multiple tasks: {'classify', 'generate', 'embed', 'reward', 'score'}. Defaulting to 'generate'.\n",
      "2024-12-25 08:22:39,968\tINFO worker.py:1786 -- Started a local Ray instance.\n",
      "INFO 12-25 08:22:41 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='mistralai/Mistral-7B-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=1234, served_model_name=mistralai/Mistral-7B-v0.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/transformers_utils/tokenizer_group/tokenizer_group.py:23: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer_mode \"mistral\"` to ensure correct encoding and decoding.\n",
      "  self.tokenizer = get_tokenizer(self.tokenizer_id, **tokenizer_config)\n",
      "INFO 12-25 08:22:41 ray_gpu_executor.py:122] use_ray_spmd_worker: False\n",
      "\u001b[36m(RayWorkerWrapper pid=87923)\u001b[0m INFO 12-25 08:22:59 selector.py:120] Using Flash Attention backend.\n",
      "INFO 12-25 08:22:59 selector.py:120] Using Flash Attention backend.\n",
      "INFO 12-25 08:23:00 utils.py:922] Found nccl from library libnccl.so.2\n",
      "INFO 12-25 08:23:00 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[36m(RayWorkerWrapper pid=87923)\u001b[0m INFO 12-25 08:23:00 utils.py:922] Found nccl from library libnccl.so.2\n",
      "\u001b[36m(RayWorkerWrapper pid=87923)\u001b[0m INFO 12-25 08:23:00 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "WARNING 12-25 08:23:01 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 12-25 08:23:01 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_5ad90478'), local_subscribe_port=59241, remote_subscribe_port=None)\n",
      "\u001b[36m(RayWorkerWrapper pid=87923)\u001b[0m WARNING 12-25 08:23:01 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 12-25 08:23:01 model_runner.py:1092] Starting to load model mistralai/Mistral-7B-v0.1...\n",
      "\u001b[36m(RayWorkerWrapper pid=87923)\u001b[0m INFO 12-25 08:23:01 model_runner.py:1092] Starting to load model mistralai/Mistral-7B-v0.1...\n",
      "INFO 12-25 08:23:01 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[36m(RayWorkerWrapper pid=87923)\u001b[0m INFO 12-25 08:23:01 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  3.73it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.08it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.23it/s]\n",
      "\n",
      "\u001b[36m(RayWorkerWrapper pid=86463)\u001b[0m INFO 12-25 08:23:03 model_runner.py:1097] Loading model weights took 3.3833 GB\n",
      "\u001b[36m(RayWorkerWrapper pid=86463)\u001b[0m INFO 12-25 08:23:03 punica_selector.py:11] Using PunicaWrapperGPU.\n",
      "INFO 12-25 08:23:03 model_runner.py:1097] Loading model weights took 3.3833 GB\n",
      "INFO 12-25 08:23:03 punica_selector.py:11] Using PunicaWrapperGPU.\n",
      "\u001b[36m(RayWorkerWrapper pid=86463)\u001b[0m INFO 12-25 08:23:07 worker.py:241] Memory profiling takes 4.53 seconds\n",
      "\u001b[36m(RayWorkerWrapper pid=86463)\u001b[0m INFO 12-25 08:23:07 worker.py:241] the current vLLM instance can use total_gpu_memory (79.14GiB) x gpu_memory_utilization (0.90) = 71.22GiB\n",
      "\u001b[36m(RayWorkerWrapper pid=86463)\u001b[0m INFO 12-25 08:23:07 worker.py:241] model weights take 3.38GiB; non_torch_memory takes 0.41GiB; PyTorch activation peak memory takes 1.48GiB; the rest of the memory reserved for KV Cache is 65.96GiB.\n",
      "\u001b[36m(RayWorkerWrapper pid=86463)\u001b[0m INFO 12-25 08:22:59 selector.py:120] Using Flash Attention backend.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=86463)\u001b[0m INFO 12-25 08:23:00 utils.py:922] Found nccl from library libnccl.so.2\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=86463)\u001b[0m INFO 12-25 08:23:00 pynccl.py:69] vLLM is using nccl==2.21.5\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=86463)\u001b[0m WARNING 12-25 08:23:01 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=86463)\u001b[0m INFO 12-25 08:23:01 model_runner.py:1092] Starting to load model mistralai/Mistral-7B-v0.1...\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=86463)\u001b[0m INFO 12-25 08:23:01 weight_utils.py:243] Using model weights format ['*.safetensors']\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=87923)\u001b[0m INFO 12-25 08:23:03 model_runner.py:1097] Loading model weights took 3.3833 GB\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=87923)\u001b[0m INFO 12-25 08:23:03 punica_selector.py:11] Using PunicaWrapperGPU.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "INFO 12-25 08:23:08 worker.py:241] Memory profiling takes 5.14 seconds\n",
      "INFO 12-25 08:23:08 worker.py:241] the current vLLM instance can use total_gpu_memory (79.14GiB) x gpu_memory_utilization (0.90) = 71.22GiB\n",
      "INFO 12-25 08:23:08 worker.py:241] model weights take 3.38GiB; non_torch_memory takes 0.42GiB; PyTorch activation peak memory takes 1.48GiB; the rest of the memory reserved for KV Cache is 65.94GiB.\n",
      "INFO 12-25 08:23:08 distributed_gpu_executor.py:57] # GPU blocks: 135054, # CPU blocks: 8192\n",
      "INFO 12-25 08:23:08 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 65.94x\n",
      "INFO 12-25 08:23:11 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-25 08:23:11 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerWrapper pid=87923)\u001b[0m INFO 12-25 08:23:12 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerWrapper pid=87923)\u001b[0m INFO 12-25 08:23:12 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerWrapper pid=87549)\u001b[0m INFO 12-25 08:23:40 model_runner.py:1527] Graph capturing finished in 28 secs, took 0.61 GiB\n",
      "\u001b[36m(RayWorkerWrapper pid=87923)\u001b[0m INFO 12-25 08:23:08 worker.py:241] Memory profiling takes 4.65 seconds\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=87923)\u001b[0m INFO 12-25 08:23:08 worker.py:241] the current vLLM instance can use total_gpu_memory (79.14GiB) x gpu_memory_utilization (0.90) = 71.22GiB\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=87923)\u001b[0m INFO 12-25 08:23:08 worker.py:241] model weights take 3.38GiB; non_torch_memory takes 0.42GiB; PyTorch activation peak memory takes 1.48GiB; the rest of the memory reserved for KV Cache is 65.95GiB.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=86463)\u001b[0m INFO 12-25 08:23:12 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=86463)\u001b[0m INFO 12-25 08:23:12 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "INFO 12-25 08:23:40 model_runner.py:1527] Graph capturing finished in 29 secs, took 0.61 GiB\n",
      "INFO 12-25 08:23:40 llm_engine.py:446] init engine (profile, create kv cache, warmup model) took 37.06 seconds\n",
      "/home/lain/lm-evaluation-harness/lm_eval/models/vllm_causallms.py:117: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer_mode \"mistral\"` to ensure correct encoding and decoding.\n",
      "  self.tokenizer = get_tokenizer(\n",
      "2024-12-25:08:23:43,446 WARNING  [evaluator.py:267] Overwriting default num_fewshot of gsm8k_cot from 8 to 1\n",
      "2024-12-25:08:23:43,447 INFO     [evaluator.py:279] Setting fewshot random generator seed to 1234\n",
      "2024-12-25:08:23:43,447 WARNING  [model.py:422] model.chat_template was called with the chat_template set to False or None. Therefore no chat template will be applied. Make sure this is an intended behavior.\n",
      "2024-12-25:08:23:43,447 INFO     [task.py:428] Building contexts for gsm8k_cot on rank 0...\n",
      "100%|██████████████████████████████████████████| 20/20 [00:00<00:00, 613.74it/s]\n",
      "2024-12-25:08:23:43,481 INFO     [evaluator.py:485] Running generate_until requests\n",
      "Running generate_until requests:   0%|                   | 0/20 [00:00<?, ?it/s]WARNING 12-25 08:23:43 tokenizer.py:191] No tokenizer found in /home/lain/wand_university_lora, using base model tokenizer instead. (Exception: Unrecognized model in /home/lain/wand_university_lora. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, deta, detr, dinat, dinov2, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, git, glm, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, graphormer, grounding-dino, groupvit, hiera, hubert, ibert, idefics, idefics2, idefics3, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, siglip, siglip_vision_model, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zoedepth)\n",
      "Running generate_until requests: 100%|██████████| 20/20 [00:05<00:00,  3.51it/s]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "2024-12-25:08:23:54,601 INFO     [evaluation_tracker.py:206] Saving results aggregated\n",
      "2024-12-25:08:23:54,602 INFO     [evaluation_tracker.py:287] Saving per-sample results for: gsm8k_cot\n",
      "vllm (pretrained=mistralai/Mistral-7B-v0.1,dtype=auto,tensor_parallel_size=4,distributed_executor_backend=ray,enable_lora=True,lora_local_path=/home/lain/wand_university_lora), gen_kwargs: (min_p=0.1,temperature=1.0,do_sample=True), limit: 20.0, num_fewshot: 1, batch_size: 4\n",
      "|  Tasks  |Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n",
      "|---------|------:|----------------|-----:|-----------|---|----:|---|-----:|\n",
      "|gsm8k_cot|      3|flexible-extract|     1|exact_match|↑  | 0.25|±  |0.0993|\n",
      "|         |       |strict-match    |     1|exact_match|↑  | 0.15|±  |0.0819|\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        gsm8k_cot/exact_match,flexible-extract ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            gsm8k_cot/exact_match,strict-match ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: gsm8k_cot/exact_match_stderr,flexible-extract ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     gsm8k_cot/exact_match_stderr,strict-match ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                               gsm8k_cot/alias gsm8k_cot\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        gsm8k_cot/exact_match,flexible-extract 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            gsm8k_cot/exact_match,strict-match 0.15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: gsm8k_cot/exact_match_stderr,flexible-extract 0.09934\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     gsm8k_cot/exact_match_stderr,strict-match 0.08192\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33myouthful-meadow-96\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rainsilves/rebuttal-response/runs/bssulwkp\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/rainsilves/rebuttal-response\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 8 artifact file(s) and 2 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20241225_082226-bssulwkp/logs\u001b[0m\n",
      "\u001b[0m\u001b[36m(RayWorkerWrapper pid=87923)\u001b[0m INFO 12-25 08:23:40 model_runner.py:1527] Graph capturing finished in 28 secs, took 0.61 GiB\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "[rank0]:[W1225 08:23:59.812677768 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "🧪 MODEL EVALUATION RITUAL COMPLETE! 📊\n",
      "INFO 12-25 08:24:01 config.py:2167] Downcasting torch.float32 to torch.float16.\n",
      "INFO 12-25 08:24:01 config.py:478] This model supports multiple tasks: {'classify', 'reward', 'score', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 12-25 08:24:01 config.py:1216] Defaulting to use mp for distributed inference\n",
      "WARNING 12-25 08:24:01 arg_utils.py:1086] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 12-25 08:24:01 config.py:1364] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 12-25 08:24:01 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='/home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411', speculative_config=None, tokenizer='/home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='lm-format-enforcer'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 12-25 08:24:01 multiproc_worker_utils.py:280] CUDA was previously initialized. We must use the `spawn` multiprocessing start method. Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/debugging.html#python-multiprocessing for more information.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92576)\u001b[0;0m INFO 12-25 08:24:06 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92576)\u001b[0;0m INFO 12-25 08:24:06 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92577)\u001b[0;0m INFO 12-25 08:24:07 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92577)\u001b[0;0m INFO 12-25 08:24:07 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92578)\u001b[0;0m INFO 12-25 08:24:07 selector.py:120] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92578)\u001b[0;0m INFO 12-25 08:24:07 multiproc_worker_utils.py:222] Worker ready; awaiting tasks\n",
      "INFO 12-25 08:24:07 utils.py:922] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92578)\u001b[0;0m INFO 12-25 08:24:07 utils.py:922] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92577)\u001b[0;0m INFO 12-25 08:24:07 utils.py:922] Found nccl from library libnccl.so.2\n",
      "INFO 12-25 08:24:07 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92578)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=92577)\u001b[0;0m INFO 12-25 08:24:07 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 12-25 08:24:07 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92576)\u001b[0;0m INFO 12-25 08:24:07 utils.py:922] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92576)\u001b[0;0m INFO 12-25 08:24:07 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "WARNING 12-25 08:24:08 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92577)\u001b[0;0m WARNING 12-25 08:24:08 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92578)\u001b[0;0m WARNING 12-25 08:24:08 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92576)\u001b[0;0m WARNING 12-25 08:24:08 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 12-25 08:24:08 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_194e61dd'), local_subscribe_port=48035, remote_subscribe_port=None)\n",
      "INFO 12-25 08:24:08 model_runner.py:1092] Starting to load model /home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92577)\u001b[0;0m INFO 12-25 08:24:08 model_runner.py:1092] Starting to load model /home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92576)\u001b[0;0m INFO 12-25 08:24:08 model_runner.py:1092] Starting to load model /home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92578)\u001b[0;0m INFO 12-25 08:24:08 model_runner.py:1092] Starting to load model /home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5480409ab42d4195a93cbad4fca8c90c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/51 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-25 08:24:45 model_runner.py:1097] Loading model weights took 57.0978 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92576)\u001b[0;0m INFO 12-25 08:24:45 model_runner.py:1097] Loading model weights took 57.1291 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92578)\u001b[0;0m INFO 12-25 08:24:45 model_runner.py:1097] Loading model weights took 57.1291 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92577)\u001b[0;0m INFO 12-25 08:24:45 model_runner.py:1097] Loading model weights took 57.1291 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92578)\u001b[0;0m INFO 12-25 08:24:48 worker.py:241] Memory profiling takes 2.71 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92578)\u001b[0;0m INFO 12-25 08:24:48 worker.py:241] the current vLLM instance can use total_gpu_memory (79.14GiB) x gpu_memory_utilization (0.90) = 71.22GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92578)\u001b[0;0m INFO 12-25 08:24:48 worker.py:241] model weights take 57.13GiB; non_torch_memory takes 0.36GiB; PyTorch activation peak memory takes 0.27GiB; the rest of the memory reserved for KV Cache is 13.46GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92577)\u001b[0;0m INFO 12-25 08:24:48 worker.py:241] Memory profiling takes 2.75 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92577)\u001b[0;0m INFO 12-25 08:24:48 worker.py:241] the current vLLM instance can use total_gpu_memory (79.14GiB) x gpu_memory_utilization (0.90) = 71.22GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92577)\u001b[0;0m INFO 12-25 08:24:48 worker.py:241] model weights take 57.13GiB; non_torch_memory takes 0.36GiB; PyTorch activation peak memory takes 0.27GiB; the rest of the memory reserved for KV Cache is 13.46GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92576)\u001b[0;0m INFO 12-25 08:24:48 worker.py:241] Memory profiling takes 2.77 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92576)\u001b[0;0m INFO 12-25 08:24:48 worker.py:241] the current vLLM instance can use total_gpu_memory (79.14GiB) x gpu_memory_utilization (0.90) = 71.22GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92576)\u001b[0;0m INFO 12-25 08:24:48 worker.py:241] model weights take 57.13GiB; non_torch_memory takes 0.38GiB; PyTorch activation peak memory takes 0.27GiB; the rest of the memory reserved for KV Cache is 13.45GiB.\n",
      "INFO 12-25 08:24:48 worker.py:241] Memory profiling takes 2.85 seconds\n",
      "INFO 12-25 08:24:48 worker.py:241] the current vLLM instance can use total_gpu_memory (79.14GiB) x gpu_memory_utilization (0.90) = 71.22GiB\n",
      "INFO 12-25 08:24:48 worker.py:241] model weights take 57.10GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 0.34GiB; the rest of the memory reserved for KV Cache is 13.69GiB.\n",
      "INFO 12-25 08:24:48 distributed_gpu_executor.py:57] # GPU blocks: 10015, # CPU blocks: 2978\n",
      "INFO 12-25 08:24:48 distributed_gpu_executor.py:61] Maximum concurrency for 131072 tokens per request: 1.22x\n",
      "INFO 12-25 08:24:48 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-25 08:24:48 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92576)\u001b[0;0m INFO 12-25 08:24:52 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92576)\u001b[0;0m INFO 12-25 08:24:52 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92578)\u001b[0;0m INFO 12-25 08:24:52 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92578)\u001b[0;0m INFO 12-25 08:24:52 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92577)\u001b[0;0m INFO 12-25 08:24:52 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92577)\u001b[0;0m INFO 12-25 08:24:52 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92576)\u001b[0;0m INFO 12-25 08:25:23 model_runner.py:1527] Graph capturing finished in 32 secs, took 1.02 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92578)\u001b[0;0m INFO 12-25 08:25:23 model_runner.py:1527] Graph capturing finished in 32 secs, took 1.02 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92577)\u001b[0;0m INFO 12-25 08:25:23 model_runner.py:1527] Graph capturing finished in 32 secs, took 1.02 GiB\n",
      "INFO 12-25 08:25:24 model_runner.py:1527] Graph capturing finished in 35 secs, took 0.07 GiB\n",
      "INFO 12-25 08:25:24 llm_engine.py:446] init engine (profile, create kv cache, warmup model) took 38.71 seconds\n",
      "\n",
      "🎓 WAND UNIVERSITY KNOWLEDGE ACQUISITION SYSTEM INITIALIZED 🎓\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SEARCHING:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 SEARCHING ARCHIVES WITH FOCUS: large language models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PROCESSING MANUSCRIPTS: 100%|██████████| 51/51 [00:00<00:00, 29439.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📜 ANALYZING MAGICAL MANUSCRIPT:\n",
      "================================================================================\n",
      "\n",
      "Lost in Translation\n",
      "May 2023\n",
      "A report from\n",
      "Gabriel Nicholas \n",
      "Aliya Bhatia\n",
      "Large Language Models in \n",
      "Non-English Content AnalysisGABRIEL NICHOLAS\n",
      "Research Fellow at the Center for Democracy & T echnology.\n",
      "ALIYA BHATIA\n",
      "Policy Analyst, Free Expression Project at the Center for \n",
      "Democracy & T echnology.\n",
      "The Center for Democracy & T echnology (CDT) is the leading \n",
      "nonpartisan, nonprofit organization fighting to advance civil rights and \n",
      "civil liberties in the digital age. W e shape technology policy,......\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "🔮 INITIATING KNOWLEDGE SYNTHESIS AND EVALUATION 🔮\n",
      "📖 GENERATING INITIAL KNOWLEDGE EXCHANGES...\n",
      "INFO 12-25 08:25:26 chat_utils.py:333] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts: 100%|██████████| 1/1 [00:56<00:00, 56.35s/it, est. speed input: 707.66 toks/s, output: 8.36 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎭 DEBATE ROUND 1 OF 3\n",
      "⚖️ EVALUATING CURRENT KNOWLEDGE EXCHANGES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:53<00:00, 53.45s/it, est. speed input: 757.08 toks/s, output: 7.69 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 GENERATING IMPROVED KNOWLEDGE EXCHANGES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [01:05<00:00, 65.47s/it, est. speed input: 615.41 toks/s, output: 9.26 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎭 DEBATE ROUND 2 OF 3\n",
      "⚖️ EVALUATING CURRENT KNOWLEDGE EXCHANGES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:54<00:00, 54.55s/it, est. speed input: 744.57 toks/s, output: 7.81 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 GENERATING IMPROVED KNOWLEDGE EXCHANGES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:58<00:00, 58.05s/it, est. speed input: 693.94 toks/s, output: 8.37 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎭 DEBATE ROUND 3 OF 3\n",
      "⚖️ EVALUATING CURRENT KNOWLEDGE EXCHANGES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:51<00:00, 51.84s/it, est. speed input: 781.07 toks/s, output: 7.39 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌟 INITIATING ARCHIVAL PROCESS OF ENCHANTED DIALOGUES 🌟\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALIDATING KNOWLEDGE EXCHANGES: 100%|██████████| 3/3 [00:00<00:00, 16958.10it/s]\n",
      "SEARCHING:  50%|█████     | 1/2 [05:43<05:43, 343.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚠️ DISCARDED KNOWLEDGE EXCHANGE 1:\n",
      "📝 INQUIRY: What are multilingual language models, and how do they work?\n",
      "💭 RESPONSE: Multilingual language models are a type of language model trained on text from multiple languages simultaneously. They work by inferring connections between languages, allowing them to apply word associations and grammatical rules learned from languages with more data to those with less. For example, a model trained on text from dozens of languages, including Hindi, might perform better in Hindi contexts than a model just trained on Hindi text. These models can be used for various natural language processing tasks, such as sentiment analysis, text summarization, and hate speech detection.\n",
      "❌ CRITICAL FEEDBACK:\n",
      "  • The answer oversimplifies the complexities and nuances of how multilingual language models work, which could lead to misunderstandings.\n",
      "\n",
      "✨ ACCEPTED KNOWLEDGE EXCHANGE 2:\n",
      "📝 INQUIRY: What are some of the challenges faced by multilingual language models?\n",
      "💭 RESPONSE: Multilingual language models face several challenges. They often rely on machine-translated text, which can contain errors or terms that native language speakers don’t actually use. This is known as the 'translationese' problem. Additionally, these models struggle to account for the contexts of local language speakers and may not work equally well in all languages due to the 'curse of multilinguality,' where the more languages a model is trained on, the less it can capture unique traits of any specific language. When these models fail, their problems are hard to identify, diagnose, and fix because of their complex and opaque nature.\n",
      "✅ POSITIVE FEEDBACK:\n",
      "  • The answer provides a clear overview of the main challenges faced by multilingual language models, as discussed in the paper.\n",
      "\n",
      "⚠️ DISCARDED KNOWLEDGE EXCHANGE 3:\n",
      "📝 INQUIRY: What recommendations does the report provide for companies deploying large language models?\n",
      "💭 RESPONSE: The report recommends that companies deploying large language models should disclose when, how, and in what languages they use these models to better understand potential problems and challenges. They should also provide adequate remedial channels and mechanisms to ensure individuals can appeal outcomes and decisions made by these systems. Additionally, companies should invest in improving language model performance in individual languages by involving language and context experts and conducting human rights impact assessments at different phases of the model’s life cycle.\n",
      "❌ CRITICAL FEEDBACK:\n",
      "  • The answer does not capture the full scope of recommendations provided in the paper, such as the need for better transparency and accountability mechanisms.\n",
      "\n",
      "📚 TRANSFORMING KNOWLEDGE INTO STRUCTURED FORMAT...\n",
      "💾 PRESERVING IN THE GRAND ARCHIVES...\n",
      "✨ SUCCESSFULLY ARCHIVED 1 EXCHANGES ✨\n",
      "📊 PRESERVED 1 MAGICAL EXCHANGES. TOTAL IN ARCHIVES: 1\n",
      "🎯 NEXT RESEARCH FOCUS: impact of language models on indigenous languages\n",
      "\n",
      "🔄 COMPLETED RESEARCH CYCLE 1. NEXT FOCUS: impact of language models on indigenous languages\n",
      "\n",
      "🔍 SEARCHING ARCHIVES WITH FOCUS: impact of language models on indigenous languages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PROCESSING MANUSCRIPTS: 100%|██████████| 6/6 [00:00<00:00, 47843.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📜 ANALYZING MAGICAL MANUSCRIPT:\n",
      "================================================================================\n",
      "\n",
      "Evaluating Machine Perception of Indigeneity: An Analysis of ChatGPT’s\n",
      "Perceptions of Indigenous Roles in Diverse Scenarios\n",
      "CECILIA DELGADO SOLORZANO,Clemson University, United States\n",
      "CARLOS TOXTLI,Clemson University, United States\n",
      "Large Language Models (LLMs), like ChatGPT, are fundamentally tools trained on vast data, reflecting diverse societal impressions.\n",
      "This paper aims to investigate LLMs’ self-perceived bias concerning indigeneity when simulating scenarios of indigenous people\n",
      "performing......\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "🔮 INITIATING KNOWLEDGE SYNTHESIS AND EVALUATION 🔮\n",
      "📖 GENERATING INITIAL KNOWLEDGE EXCHANGES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.94s/it, est. speed input: 179.12 toks/s, output: 16.85 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎭 DEBATE ROUND 1 OF 3\n",
      "⚖️ EVALUATING CURRENT KNOWLEDGE EXCHANGES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:28<00:00, 28.25s/it, est. speed input: 184.69 toks/s, output: 16.78 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 GENERATING IMPROVED KNOWLEDGE EXCHANGES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:29<00:00, 29.55s/it, est. speed input: 172.96 toks/s, output: 16.92 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎭 DEBATE ROUND 2 OF 3\n",
      "⚖️ EVALUATING CURRENT KNOWLEDGE EXCHANGES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.90s/it, est. speed input: 204.38 toks/s, output: 16.49 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 GENERATING IMPROVED KNOWLEDGE EXCHANGES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:28<00:00, 28.89s/it, est. speed input: 174.86 toks/s, output: 16.89 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎭 DEBATE ROUND 3 OF 3\n",
      "⚖️ EVALUATING CURRENT KNOWLEDGE EXCHANGES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.95s/it, est. speed input: 331.70 toks/s, output: 15.11 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌟 INITIATING ARCHIVAL PROCESS OF ENCHANTED DIALOGUES 🌟\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALIDATING KNOWLEDGE EXCHANGES: 100%|██████████| 3/3 [00:00<00:00, 19660.80it/s]\n",
      "SEARCHING: 100%|██████████| 2/2 [08:19<00:00, 249.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✨ ACCEPTED KNOWLEDGE EXCHANGE 1:\n",
      "📝 INQUIRY: What is the primary aim of the paper 'Evaluating Machine Perception of Indigeneity: An Analysis of ChatGPT’s Perceptions of Indigenous Roles in Diverse Scenarios'?\n",
      "💭 RESPONSE: The primary aim of the paper is to investigate Large Language Models' (LLMs) self-perceived bias concerning indigeneity when simulating scenarios of indigenous people performing various roles. The study seeks to understand how technology perceives and potentially amplifies societal biases related to indigeneity in social computing, offering insights into the broader implications for critical computing.\n",
      "✅ POSITIVE FEEDBACK:\n",
      "\n",
      "✨ ACCEPTED KNOWLEDGE EXCHANGE 2:\n",
      "📝 INQUIRY: What methodology was used in the research to evaluate ChatGPT's perceptions of indigeneity?\n",
      "💭 RESPONSE: The research employed a scenario evaluation approach using a Large Language Model (LLM) to generate and analyze text samples. The study considered three scenarios—a field, a technology store, and a subway—with two ethnicities: White/Caucasian as the control group and Indigenous as the treatment group. For each combination, the LLM generated 30 unique text samples, which were then analyzed for stereotypes and biases. The analysis involved labeling potential stereotypes based on societal perceptions and comparing these labels to human-provided ground truth labels.\n",
      "✅ POSITIVE FEEDBACK:\n",
      "\n",
      "✨ ACCEPTED KNOWLEDGE EXCHANGE 3:\n",
      "📝 INQUIRY: What were the key findings of the study regarding ChatGPT's perceptions of indigeneity in different scenarios?\n",
      "💭 RESPONSE: The study found that ChatGPT tended to reinforce subtle biases and stereotypes in different scenarios. For instance, in the field scenario, indigenous individuals were strongly stereotyped in 80% of samples, reinforcing tropes like deep nature connections and ancestral practices. In the technology store scenario, 26% of samples linked indigenous customers to cultural products. In the subway scenario, 93% of samples contained exoticized stereotypes about indigenous riders. These findings highlight how the LLM internalizes and projects societal biases, often 'othering' indigenous individuals.\n",
      "✅ POSITIVE FEEDBACK:\n",
      "\n",
      "📚 TRANSFORMING KNOWLEDGE INTO STRUCTURED FORMAT...\n",
      "💾 PRESERVING IN THE GRAND ARCHIVES...\n",
      "✨ SUCCESSFULLY ARCHIVED 3 EXCHANGES ✨\n",
      "📊 PRESERVED 3 MAGICAL EXCHANGES. TOTAL IN ARCHIVES: 4\n",
      "🎯 NEXT RESEARCH FOCUS: The intersection of AI and indigenous knowledge systems: Exploring the ethical implications of AI in preserving and promoting indigenous languages\n",
      "\n",
      "🔄 COMPLETED RESEARCH CYCLE 2. NEXT FOCUS: The intersection of AI and indigenous knowledge systems: Exploring the ethical implications of AI in preserving and promoting indigenous languages\n",
      "\n",
      "🏆 GRAND ARCHIVE COMPLETE! TOTAL MAGICAL EXCHANGES PRESERVED: 4 🏆\n",
      "INFO 12-25 08:33:44 multiproc_worker_utils.py:140] Terminating local vLLM worker processes\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92576)\u001b[0;0m INFO 12-25 08:33:44 multiproc_worker_utils.py:247] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92577)\u001b[0;0m INFO 12-25 08:33:44 multiproc_worker_utils.py:247] Worker exiting\n",
      "\u001b[1;36m(VllmWorkerProcess pid=92578)\u001b[0;0m INFO 12-25 08:33:44 multiproc_worker_utils.py:247] Worker exiting\n",
      "Successfully delete the llm pipeline and free the GPU memory.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/lain/wandb/run-20241225_083345-ialqc4a1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rainsilves/wand-university-lora/runs/ialqc4a1' target=\"_blank\">arcane-knowledge-transfer-incremental</a></strong> to <a href='https://wandb.ai/rainsilves/wand-university-lora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rainsilves/wand-university-lora' target=\"_blank\">https://wandb.ai/rainsilves/wand-university-lora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rainsilves/wand-university-lora/runs/ialqc4a1' target=\"_blank\">https://wandb.ai/rainsilves/wand-university-lora/runs/ialqc4a1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔮 SUMMONING TRAINING DATA FROM THE ANCIENT GRIMOIRE... 📚 [Model: mistralai/Mistral-7B-v0.1]\n",
      "⚡ INVOKING THE SACRED TOKENIZER AND MODEL... 🤖 [Sequence Length: 4096]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00bf71fae04946758f8de431fc73d69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 LOADING EXISTING LORA ENCHANTMENT FROM ./wand_university_lora...\n",
      "trainable params: 0 || all params: 7,262,992,384 || trainable%: 0.0000\n",
      "📜 PREPARING THE SACRED TEXTS... ✨ [Dataset Size: 279 entries]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b78b200d054aa3a2b7fbe0e8fc5269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/279 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌟 COMMENCING THE SACRED LORA TRAINING RITUAL... 🧙‍♂️ [Epochs: 1, Learning Rate: 1e-05]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='279' max='279' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [279/279 01:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 PRESERVING THE ENHANCED MODEL IN THE ARCANE ARCHIVES... 📦 [Output Directory: ./wand_university_lora]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 CONCLUDING THE WANDB LOGGING RITUAL... 🔮\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1035383673667584.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>279</td></tr><tr><td>train_loss</td><td>1.42211</td></tr><tr><td>train_runtime</td><td>60.693</td></tr><tr><td>train_samples_per_second</td><td>4.597</td></tr><tr><td>train_steps_per_second</td><td>4.597</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">arcane-knowledge-transfer-incremental</strong> at: <a href='https://wandb.ai/rainsilves/wand-university-lora/runs/ialqc4a1' target=\"_blank\">https://wandb.ai/rainsilves/wand-university-lora/runs/ialqc4a1</a><br/> View project at: <a href='https://wandb.ai/rainsilves/wand-university-lora' target=\"_blank\">https://wandb.ai/rainsilves/wand-university-lora</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241225_083345-ialqc4a1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 THE SACRED LORA TRAINING RITUAL IS COMPLETE! 🎊 [Total Training Steps: 279]\n",
      "🧹 CLEANING THE LORA ENCHANTMENT ARTIFACTS... ✨\n",
      "🔍 FOUND 1 NON-LORA KEYS TO REMOVE...\n",
      "💾 SAVING CLEANED LORA ENCHANTMENT... 📦\n",
      "🧪 BEGINNING MODEL EVALUATION RITUAL... 📊\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrainsilves\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/lain/wandb/run-20241225_083507-9tsim5e0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33methereal-mountain-97\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rainsilves/rebuttal-response\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rainsilves/rebuttal-response/runs/9tsim5e0\u001b[0m\n",
      "2024-12-25:08:35:08,032 INFO     [__main__.py:279] Verbosity set to INFO\n",
      "2024-12-25:08:35:08,132 INFO     [__init__.py:491] `group` and `group_alias` keys in TaskConfigs are deprecated and will be removed in v0.4.5 of lm_eval. The new `tag` field will be used to allow for a shortcut to a group of tasks one does not wish to aggregate metrics across. `group`s which aggregate across subtasks must be only defined in a separate group config file, which will be the official way to create groups that support cross-task aggregation as in `mmlu`. Please see the v0.4.4 patch notes and our documentation: https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/new_task_guide.md#advanced-group-configs for more information.\n",
      "2024-12-25:08:35:12,974 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
      "2024-12-25:08:35:12,975 INFO     [__main__.py:376] Selected Tasks: ['gsm8k_cot']\n",
      "2024-12-25:08:35:12,978 INFO     [evaluator.py:161] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-12-25:08:35:12,978 WARNING  [evaluator.py:172] generation_kwargs specified through cli, these settings will update set parameters in yaml tasks. Ensure 'do_sample=True' for non-greedy decoding!\n",
      "2024-12-25:08:35:12,978 INFO     [evaluator.py:198] Initializing vllm model, with arguments: {'pretrained': 'mistralai/Mistral-7B-v0.1', 'dtype': 'auto', 'tensor_parallel_size': 4, 'distributed_executor_backend': 'ray', 'enable_lora': True, 'lora_local_path': '/home/lain/wand_university_lora'}\n",
      "INFO 12-25 08:35:19 config.py:478] This model supports multiple tasks: {'score', 'classify', 'embed', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "2024-12-25 08:35:21,048\tINFO worker.py:1786 -- Started a local Ray instance.\n",
      "INFO 12-25 08:35:22 llm_engine.py:249] Initializing an LLM engine (v0.6.5) with config: model='mistralai/Mistral-7B-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=1234, served_model_name=mistralai/Mistral-7B-v0.1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, mm_cache_preprocessor=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/vllm/transformers_utils/tokenizer_group/tokenizer_group.py:23: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer_mode \"mistral\"` to ensure correct encoding and decoding.\n",
      "  self.tokenizer = get_tokenizer(self.tokenizer_id, **tokenizer_config)\n",
      "INFO 12-25 08:35:22 ray_gpu_executor.py:122] use_ray_spmd_worker: False\n",
      "\u001b[36m(RayWorkerWrapper pid=95009)\u001b[0m INFO 12-25 08:35:40 selector.py:120] Using Flash Attention backend.\n",
      "INFO 12-25 08:35:41 selector.py:120] Using Flash Attention backend.\n",
      "INFO 12-25 08:35:42 utils.py:922] Found nccl from library libnccl.so.2\n",
      "INFO 12-25 08:35:42 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[36m(RayWorkerWrapper pid=95009)\u001b[0m INFO 12-25 08:35:42 utils.py:922] Found nccl from library libnccl.so.2\n",
      "\u001b[36m(RayWorkerWrapper pid=95009)\u001b[0m INFO 12-25 08:35:42 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "WARNING 12-25 08:35:43 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 12-25 08:35:43 shm_broadcast.py:255] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_c24bc16b'), local_subscribe_port=60705, remote_subscribe_port=None)\n",
      "INFO 12-25 08:35:43 model_runner.py:1092] Starting to load model mistralai/Mistral-7B-v0.1...\n",
      "\u001b[36m(RayWorkerWrapper pid=95009)\u001b[0m WARNING 12-25 08:35:43 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[36m(RayWorkerWrapper pid=95009)\u001b[0m INFO 12-25 08:35:43 model_runner.py:1092] Starting to load model mistralai/Mistral-7B-v0.1...\n",
      "INFO 12-25 08:35:43 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[36m(RayWorkerWrapper pid=95009)\u001b[0m INFO 12-25 08:35:43 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  3.85it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  1.94it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.10it/s]\n",
      "\n",
      "INFO 12-25 08:35:44 model_runner.py:1097] Loading model weights took 3.3833 GB\n",
      "INFO 12-25 08:35:44 punica_selector.py:11] Using PunicaWrapperGPU.\n",
      "\u001b[36m(RayWorkerWrapper pid=94908)\u001b[0m INFO 12-25 08:35:44 model_runner.py:1097] Loading model weights took 3.3833 GB\n",
      "\u001b[36m(RayWorkerWrapper pid=94908)\u001b[0m INFO 12-25 08:35:44 punica_selector.py:11] Using PunicaWrapperGPU.\n",
      "\u001b[36m(RayWorkerWrapper pid=95009)\u001b[0m INFO 12-25 08:35:49 worker.py:241] Memory profiling takes 4.70 seconds\n",
      "\u001b[36m(RayWorkerWrapper pid=95009)\u001b[0m INFO 12-25 08:35:49 worker.py:241] the current vLLM instance can use total_gpu_memory (79.14GiB) x gpu_memory_utilization (0.90) = 71.22GiB\n",
      "\u001b[36m(RayWorkerWrapper pid=95009)\u001b[0m INFO 12-25 08:35:49 worker.py:241] model weights take 3.38GiB; non_torch_memory takes 0.42GiB; PyTorch activation peak memory takes 1.48GiB; the rest of the memory reserved for KV Cache is 65.95GiB.\n",
      "\u001b[36m(RayWorkerWrapper pid=94908)\u001b[0m INFO 12-25 08:35:40 selector.py:120] Using Flash Attention backend.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=94908)\u001b[0m INFO 12-25 08:35:42 utils.py:922] Found nccl from library libnccl.so.2\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=94908)\u001b[0m INFO 12-25 08:35:42 pynccl.py:69] vLLM is using nccl==2.21.5\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=94908)\u001b[0m WARNING 12-25 08:35:43 custom_all_reduce.py:134] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=94908)\u001b[0m INFO 12-25 08:35:43 model_runner.py:1092] Starting to load model mistralai/Mistral-7B-v0.1...\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=94908)\u001b[0m INFO 12-25 08:35:43 weight_utils.py:243] Using model weights format ['*.safetensors']\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=94920)\u001b[0m INFO 12-25 08:35:45 model_runner.py:1097] Loading model weights took 3.3833 GB\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=94920)\u001b[0m INFO 12-25 08:35:45 punica_selector.py:11] Using PunicaWrapperGPU.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "INFO 12-25 08:35:50 worker.py:241] Memory profiling takes 5.18 seconds\n",
      "INFO 12-25 08:35:50 worker.py:241] the current vLLM instance can use total_gpu_memory (79.14GiB) x gpu_memory_utilization (0.90) = 71.22GiB\n",
      "INFO 12-25 08:35:50 worker.py:241] model weights take 3.38GiB; non_torch_memory takes 0.42GiB; PyTorch activation peak memory takes 1.48GiB; the rest of the memory reserved for KV Cache is 65.94GiB.\n",
      "INFO 12-25 08:35:50 distributed_gpu_executor.py:57] # GPU blocks: 135054, # CPU blocks: 8192\n",
      "INFO 12-25 08:35:50 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 65.94x\n",
      "INFO 12-25 08:35:53 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-25 08:35:53 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerWrapper pid=95009)\u001b[0m INFO 12-25 08:35:53 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerWrapper pid=95009)\u001b[0m INFO 12-25 08:35:53 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-25 08:36:23 model_runner.py:1527] Graph capturing finished in 30 secs, took 0.61 GiB\n",
      "\u001b[36m(RayWorkerWrapper pid=94920)\u001b[0m INFO 12-25 08:36:23 model_runner.py:1527] Graph capturing finished in 29 secs, took 0.61 GiB\n",
      "\u001b[36m(RayWorkerWrapper pid=94908)\u001b[0m INFO 12-25 08:35:49 worker.py:241] Memory profiling takes 4.69 seconds\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=94908)\u001b[0m INFO 12-25 08:35:49 worker.py:241] the current vLLM instance can use total_gpu_memory (79.14GiB) x gpu_memory_utilization (0.90) = 71.22GiB\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=94908)\u001b[0m INFO 12-25 08:35:49 worker.py:241] model weights take 3.38GiB; non_torch_memory takes 0.41GiB; PyTorch activation peak memory takes 1.48GiB; the rest of the memory reserved for KV Cache is 65.96GiB.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=94908)\u001b[0m INFO 12-25 08:35:53 model_runner.py:1413] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerWrapper pid=94908)\u001b[0m INFO 12-25 08:35:53 model_runner.py:1417] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "INFO 12-25 08:36:23 llm_engine.py:446] init engine (profile, create kv cache, warmup model) took 38.25 seconds\n",
      "/home/lain/lm-evaluation-harness/lm_eval/models/vllm_causallms.py:117: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer_mode \"mistral\"` to ensure correct encoding and decoding.\n",
      "  self.tokenizer = get_tokenizer(\n",
      "2024-12-25:08:36:26,343 WARNING  [evaluator.py:267] Overwriting default num_fewshot of gsm8k_cot from 8 to 1\n",
      "2024-12-25:08:36:26,343 INFO     [evaluator.py:279] Setting fewshot random generator seed to 1234\n",
      "2024-12-25:08:36:26,343 WARNING  [model.py:422] model.chat_template was called with the chat_template set to False or None. Therefore no chat template will be applied. Make sure this is an intended behavior.\n",
      "2024-12-25:08:36:26,344 INFO     [task.py:428] Building contexts for gsm8k_cot on rank 0...\n",
      "100%|██████████████████████████████████████████| 20/20 [00:00<00:00, 620.07it/s]\n",
      "2024-12-25:08:36:26,378 INFO     [evaluator.py:485] Running generate_until requests\n",
      "Running generate_until requests:   0%|                   | 0/20 [00:00<?, ?it/s]WARNING 12-25 08:36:26 tokenizer.py:191] No tokenizer found in /home/lain/wand_university_lora, using base model tokenizer instead. (Exception: Unrecognized model in /home/lain/wand_university_lora. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, deta, detr, dinat, dinov2, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, git, glm, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, graphormer, grounding-dino, groupvit, hiera, hubert, ibert, idefics, idefics2, idefics3, ijepa, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, siglip, siglip_vision_model, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zoedepth)\n",
      "Running generate_until requests: 100%|██████████| 20/20 [00:05<00:00,  3.45it/s]\n",
      "fatal: not a git repository (or any of the parent directories): .git\n",
      "2024-12-25:08:36:37,241 INFO     [evaluation_tracker.py:206] Saving results aggregated\n",
      "2024-12-25:08:36:37,242 INFO     [evaluation_tracker.py:287] Saving per-sample results for: gsm8k_cot\n",
      "vllm (pretrained=mistralai/Mistral-7B-v0.1,dtype=auto,tensor_parallel_size=4,distributed_executor_backend=ray,enable_lora=True,lora_local_path=/home/lain/wand_university_lora), gen_kwargs: (min_p=0.1,temperature=1.0,do_sample=True), limit: 20.0, num_fewshot: 1, batch_size: 4\n",
      "|  Tasks  |Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n",
      "|---------|------:|----------------|-----:|-----------|---|----:|---|-----:|\n",
      "|gsm8k_cot|      3|flexible-extract|     1|exact_match|↑  | 0.25|±  |0.0993|\n",
      "|         |       |strict-match    |     1|exact_match|↑  | 0.10|±  |0.0688|\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        gsm8k_cot/exact_match,flexible-extract ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            gsm8k_cot/exact_match,strict-match ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: gsm8k_cot/exact_match_stderr,flexible-extract ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     gsm8k_cot/exact_match_stderr,strict-match ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                               gsm8k_cot/alias gsm8k_cot\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        gsm8k_cot/exact_match,flexible-extract 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            gsm8k_cot/exact_match,strict-match 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: gsm8k_cot/exact_match_stderr,flexible-extract 0.09934\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     gsm8k_cot/exact_match_stderr,strict-match 0.06882\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33methereal-mountain-97\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/rainsilves/rebuttal-response/runs/9tsim5e0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/rainsilves/rebuttal-response\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 8 artifact file(s) and 2 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20241225_083507-9tsim5e0/logs\u001b[0m\n",
      "\u001b[0m\u001b[36m(RayWorkerWrapper pid=95009)\u001b[0m INFO 12-25 08:36:23 model_runner.py:1527] Graph capturing finished in 30 secs, took 0.62 GiB\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "[rank0]:[W1225 08:36:42.087789871 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\n",
      "🧪 MODEL EVALUATION RITUAL COMPLETE! 📊\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import contextlib\n",
    "import ray\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.distributed.parallel_state import (\n",
    "    destroy_model_parallel,\n",
    "    destroy_distributed_environment,\n",
    ")\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Literal\n",
    "from datetime import datetime, timedelta\n",
    "from lmformatenforcer import JsonSchemaParser\n",
    "from openai import OpenAI\n",
    "from lmformatenforcer.integrations.transformers import build_transformers_prefix_allowed_tokens_fn\n",
    "import pprint\n",
    "import random\n",
    "from fundus import PublisherCollection, Crawler\n",
    "from transformers import AutoTokenizer, TrainingArguments, AutoModelForCausalLM\n",
    "from llama_index.readers.papers import ArxivReader\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "import wandb\n",
    "import os\n",
    "from enum import Enum\n",
    "from pydantic import BaseModel\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.sampling_params import GuidedDecodingParams\n",
    "\n",
    "def load_arxiv_papers(search_query: str, max_results: int = 1):    \n",
    "    loader = ArxivReader()\n",
    "    documents = loader.load_data(search_query=search_query, max_results=max_results)\n",
    "    return documents\n",
    "\n",
    "class QA(BaseModel):\n",
    "    question1: str = Field(description=\"First general knowledge question about the core topic and concepts\")\n",
    "    answer1: str = Field(description=\"Clear, factual answer focused on general knowledge\")\n",
    "    question2: str = Field(description=\"Second general knowledge question about the core topic and concepts\") \n",
    "    answer2: str = Field(description=\"Clear, factual answer focused on general knowledge\")\n",
    "    question3: str = Field(description=\"Third general knowledge question about the core topic and concepts\")\n",
    "    answer3: str = Field(description=\"Clear, factual answer focused on general knowledge\")\n",
    "\n",
    "DEFAULT_SYSTEM_CONTENT = \"\"\"You are an expert at creating high-quality training data for language models.\n",
    "Given academic research content, extract the key concepts and convert them into general knowledge questions and answers.\n",
    "Questions should:\n",
    "- Be general and broadly applicable\n",
    "- Focus on core concepts and ideas\n",
    "- Be written as standalone questions without referencing any specific research\n",
    "- Test understanding of the topic area\n",
    "\n",
    "Answers should:\n",
    "- Provide clear, factual information\n",
    "- Be written as standalone knowledge\n",
    "- Focus on general concepts rather than specific research\n",
    "- Be useful for general learning about the topic\"\"\"\n",
    "\n",
    "def generate_qa(document_text: str, system_content: str = DEFAULT_SYSTEM_CONTENT, feedback: str = None):\n",
    "    json_schema = QA.model_json_schema()\n",
    "    guided_decoding_params = GuidedDecodingParams(json=json_schema)\n",
    "    sampling_params = SamplingParams(\n",
    "        guided_decoding=guided_decoding_params,\n",
    "        temperature=1.5,\n",
    "        max_tokens=4000,\n",
    "        presence_penalty=0.1,\n",
    "        frequency_penalty=0.1,\n",
    "        min_p=0.4,\n",
    "        stop=[\"<|eot_id|>\"]\n",
    "    )\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_content}\n",
    "    ]\n",
    "    \n",
    "    if feedback:\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Based on this feedback, please generate improved questions and answers:\\n\\n{feedback}\\n\\nOriginal text: {document_text}\"})\n",
    "    else:\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Based on these concepts, generate 3 general knowledge questions and answers about: {document_text}\"})\n",
    "    \n",
    "    return llm.chat(\n",
    "        messages=messages,\n",
    "        sampling_params=sampling_params\n",
    "    )\n",
    "\n",
    "class QAEvaluation(BaseModel):\n",
    "    qa_pair1_arguments_for: list[str] = Field(description=\"Arguments in favor of including first QA pair\")\n",
    "    qa_pair1_arguments_against: list[str] = Field(description=\"Arguments against including first QA pair\") \n",
    "    qa_pair1_include: Literal[\"y\", \"n\"] = Field(description=\"Whether to include first QA pair\")\n",
    "\n",
    "    qa_pair2_arguments_for: list[str] = Field(description=\"Arguments in favor of including second QA pair\")\n",
    "    qa_pair2_arguments_against: list[str] = Field(description=\"Arguments against including second QA pair\")\n",
    "    qa_pair2_include: Literal[\"y\", \"n\"] = Field(description=\"Whether to include second QA pair\")\n",
    "\n",
    "    qa_pair3_arguments_for: list[str] = Field(description=\"Arguments in favor of including third QA pair\")\n",
    "    qa_pair3_arguments_against: list[str] = Field(description=\"Arguments against including third QA pair\")\n",
    "    qa_pair3_include: Literal[\"y\", \"n\"] = Field(description=\"Whether to include third QA pair\")\n",
    "    \n",
    "    next_search_query: str = Field(description=\"Recommended next search query for finding related but diverse papers\")\n",
    "\n",
    "def evaluate_qa(document_text: str, qa_output: QA, round_num: int):\n",
    "    json_schema = QAEvaluation.model_json_schema()\n",
    "    guided_decoding_params = GuidedDecodingParams(json=json_schema)\n",
    "    sampling_params = SamplingParams(\n",
    "        guided_decoding=guided_decoding_params,\n",
    "        temperature=1.5,\n",
    "        max_tokens=4000,\n",
    "        presence_penalty=0.1,\n",
    "        frequency_penalty=0.1,\n",
    "        min_p=0.4,\n",
    "        stop=[\"<|eot_id|>\"]\n",
    "    )\n",
    "    \n",
    "    eval_system_prompt = f\"\"\"You are an expert evaluator of training data quality for language models.\n",
    "    This is debate round {round_num}. Be increasingly critical with each round.\n",
    "    For each question-answer pair:\n",
    "    1. List key arguments for and against including it (considering accuracy, clarity, generalizability)\n",
    "    2. Make a binary decision (y/n) if it should be included in the final dataset\n",
    "    \n",
    "    Additionally, recommend a search query for finding the next paper to analyze. The query should:\n",
    "    - Be related to but very distinct from the current paper's topic\n",
    "    - Help build a diverse dataset while maintaining topical coherence\n",
    "    - Potentially focus on an interesting direction suggested by the current paper\"\"\"\n",
    "    \n",
    "    eval_user_prompt = f\"\"\"Evaluate these QA pairs generated from the paper:\n",
    "    \n",
    "    Original Paper Text: {document_text}\n",
    "    \n",
    "    QA Pair 1:\n",
    "    Q: {qa_output.question1}\n",
    "    A: {qa_output.answer1}\n",
    "    \n",
    "    QA Pair 2:\n",
    "    Q: {qa_output.question2}\n",
    "    A: {qa_output.answer2}\n",
    "    \n",
    "    QA Pair 3:\n",
    "    Q: {qa_output.question3}\n",
    "    A: {qa_output.answer3}\n",
    "    \n",
    "    Provide a structured evaluation following the schema, including a recommended next search query.\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": eval_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": eval_user_prompt}\n",
    "    ]\n",
    "    \n",
    "    return llm.chat(\n",
    "        messages=messages,\n",
    "        sampling_params=sampling_params\n",
    "    )\n",
    "\n",
    "def archive_enchanted_dialogues(knowledge_exchange_data, assessment_data, archive_path=\"wand_university_training_grimoire.csv\"):\n",
    "    \"\"\"Archive validated magical dialogues and their scholarly assessments for the Wand University curriculum\"\"\"\n",
    "    print(\"\\n🌟 INITIATING ARCHIVAL PROCESS OF ENCHANTED DIALOGUES 🌟\")\n",
    "    \n",
    "    # Transform assessment data from arcane notation to comprehensible format\n",
    "    assessment_grimoire = json.loads(assessment_data)\n",
    "    \n",
    "    # Initialize collection of validated knowledge exchanges\n",
    "    validated_exchanges = []\n",
    "    \n",
    "    # Evaluate each knowledge exchange through our rigorous magical standards\n",
    "    for exchange_id in tqdm(range(1,4), desc=\"VALIDATING KNOWLEDGE EXCHANGES\"):\n",
    "        if assessment_grimoire[f'qa_pair{exchange_id}_include'] == 'y':\n",
    "            # Synthesize supporting and challenging arguments\n",
    "            supporting_thesis = assessment_grimoire.get(f'qa_pair{exchange_id}_arguments_for', [])\n",
    "            counterpoints = assessment_grimoire.get(f'qa_pair{exchange_id}_arguments_against', [])\n",
    "            \n",
    "            print(f\"\\n✨ ACCEPTED KNOWLEDGE EXCHANGE {exchange_id}:\")\n",
    "            print(f\"📝 INQUIRY: {getattr(knowledge_exchange_data, f'question{exchange_id}')}\")\n",
    "            print(f\"💭 RESPONSE: {getattr(knowledge_exchange_data, f'answer{exchange_id}')}\")\n",
    "            print(\"✅ POSITIVE FEEDBACK:\")\n",
    "            for strength in supporting_thesis:\n",
    "                print(f\"  • {strength}\")\n",
    "            \n",
    "            validated_exchanges.append({\n",
    "                'inquiry': getattr(knowledge_exchange_data, f'question{exchange_id}'),\n",
    "                'wisdom': getattr(knowledge_exchange_data, f'answer{exchange_id}'),\n",
    "                'supporting_thesis': ', '.join(supporting_thesis),\n",
    "                'counterpoints': ', '.join(counterpoints),\n",
    "                'search_query': assessment_grimoire['next_search_query']\n",
    "            })\n",
    "        else:\n",
    "            print(f\"\\n⚠️ DISCARDED KNOWLEDGE EXCHANGE {exchange_id}:\")\n",
    "            print(f\"📝 INQUIRY: {getattr(knowledge_exchange_data, f'question{exchange_id}')}\")\n",
    "            print(f\"💭 RESPONSE: {getattr(knowledge_exchange_data, f'answer{exchange_id}')}\")\n",
    "            print(\"❌ CRITICAL FEEDBACK:\")\n",
    "            for critique in assessment_grimoire.get(f'qa_pair{exchange_id}_arguments_against', []):\n",
    "                print(f\"  • {critique}\")\n",
    "    \n",
    "    print(\"\\n📚 TRANSFORMING KNOWLEDGE INTO STRUCTURED FORMAT...\")\n",
    "    # Transform into structured knowledge format\n",
    "    knowledge_codex = pd.DataFrame(validated_exchanges)\n",
    "    \n",
    "    # Preserve in the grand archives\n",
    "    print(\"💾 PRESERVING IN THE GRAND ARCHIVES...\")\n",
    "    if Path(archive_path).exists():\n",
    "        knowledge_codex.to_csv(archive_path, mode='a', header=False, index=False, sep='|')\n",
    "    else:\n",
    "        knowledge_codex.to_csv(archive_path, index=False, sep='|')\n",
    "    \n",
    "    print(f\"✨ SUCCESSFULLY ARCHIVED {len(validated_exchanges)} EXCHANGES ✨\")\n",
    "    return len(validated_exchanges)\n",
    "\n",
    "def synthesize_and_evaluate_knowledge(source_manuscript, research_focus, debate_rounds=3):\n",
    "    \"\"\"Synthesize and evaluate magical knowledge exchanges through our proprietary arcane processes\"\"\"\n",
    "    print(\"\\n🔮 INITIATING KNOWLEDGE SYNTHESIS AND EVALUATION 🔮\")\n",
    "    \n",
    "    # Initial generation\n",
    "    print(\"📖 GENERATING INITIAL KNOWLEDGE EXCHANGES...\")\n",
    "    knowledge_response = generate_qa(source_manuscript, research_focus)\n",
    "    my_response = \"\"\n",
    "    for response in knowledge_response:\n",
    "        my_response += response.outputs[0].text\n",
    "    knowledge_data = QA.parse_raw(my_response)\n",
    "    \n",
    "    # Debate rounds\n",
    "    for round_num in range(1, debate_rounds + 1):\n",
    "        print(f\"\\n🎭 DEBATE ROUND {round_num} OF {debate_rounds}\")\n",
    "        \n",
    "        # Evaluate current QA pairs\n",
    "        print(\"⚖️ EVALUATING CURRENT KNOWLEDGE EXCHANGES...\")\n",
    "        assessment_response = evaluate_qa(source_manuscript, knowledge_data, round_num)\n",
    "        my_assessment = \"\"\n",
    "        for response in assessment_response:\n",
    "            my_assessment += response.outputs[0].text\n",
    "        \n",
    "        if round_num < debate_rounds:\n",
    "            # Generate improved QA pairs based on feedback\n",
    "            print(\"🔄 GENERATING IMPROVED KNOWLEDGE EXCHANGES...\")\n",
    "            knowledge_response = generate_qa(source_manuscript, research_focus, my_assessment)\n",
    "            my_response = \"\"\n",
    "            for response in knowledge_response:\n",
    "                my_response += response.outputs[0].text\n",
    "            knowledge_data = QA.parse_raw(my_response)\n",
    "    \n",
    "    # Archive final round exchanges\n",
    "    exchanges_preserved = archive_enchanted_dialogues(knowledge_data, my_assessment)\n",
    "    \n",
    "    # Extract next research direction from evaluation\n",
    "    assessment_grimoire = json.loads(my_assessment)\n",
    "    next_research_focus = assessment_grimoire['next_search_query']\n",
    "    \n",
    "    return exchanges_preserved, next_research_focus\n",
    "#-------------------------------------------------------------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------------------------------\n",
    "# MAIN LOOP - Iterates N=2 times through the entire knowledge acquisition and training process\n",
    "#-------------------------------------------------------------------------------------------\n",
    "for iteration in range(2):\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    # LLM INITIALIZATION - Set up the language model with specific parameters\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    sampling_params = SamplingParams(temperature=2.0, min_p=0.3)\n",
    "    llm = LLM(model=\"/home/lain/text-generation-webui/models/mistralai_Mistral-Large-Instruct-2411\", tensor_parallel_size=4, guided_decoding_backend = \"lm-format-enforcer\")\n",
    "    current_research_focus = \"large language models\"\n",
    "    preserved_exchanges_count = 0\n",
    "\n",
    "    print(\"\\n🎓 WAND UNIVERSITY KNOWLEDGE ACQUISITION SYSTEM INITIALIZED 🎓\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    # KNOWLEDGE ACQUISITION LOOP - Search and process research papers\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    research_cycle = 0\n",
    "    while research_cycle < 2:\n",
    "        try:\n",
    "            # Search the grand archives\n",
    "            for i in tqdm(range(0,2), desc=\"SEARCHING\"):\n",
    "                print(f\"\\n🔍 SEARCHING ARCHIVES WITH FOCUS: {current_research_focus}\")\n",
    "                ancient_manuscripts = load_arxiv_papers(current_research_focus, max_results=1)\n",
    "                the_text = \"\"            \n",
    "                for manuscript in tqdm(ancient_manuscripts, desc=\"PROCESSING MANUSCRIPTS\"):\n",
    "                    the_text += manuscript.text\n",
    "            \n",
    "                print(\"\\n\" + \"=\"*80)\n",
    "                print(f\"📜 ANALYZING MAGICAL MANUSCRIPT:\")\n",
    "                print(\"=\"*80 + \"\\n\")\n",
    "                print(the_text[:500] + \"......\")  \n",
    "                print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "                \n",
    "                exchanges_preserved, next_focus = synthesize_and_evaluate_knowledge(the_text, current_research_focus)\n",
    "                preserved_exchanges_count += exchanges_preserved\n",
    "                print(f\"📊 PRESERVED {exchanges_preserved} MAGICAL EXCHANGES. TOTAL IN ARCHIVES: {preserved_exchanges_count}\")\n",
    "                print(f\"🎯 NEXT RESEARCH FOCUS: {next_focus}\\n\")\n",
    "                \n",
    "                current_research_focus = next_focus\n",
    "                print(f\"🔄 COMPLETED RESEARCH CYCLE {research_cycle + 1}. NEXT FOCUS: {current_research_focus}\")\n",
    "                research_cycle += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚡ MAGICAL ANOMALY DETECTED: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\n🏆 GRAND ARCHIVE COMPLETE! TOTAL MAGICAL EXCHANGES PRESERVED: {preserved_exchanges_count} 🏆\")\n",
    "\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    # CLEANUP - Free GPU memory and shutdown services so that we can do lora training\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    destroy_model_parallel()\n",
    "    destroy_distributed_environment()\n",
    "    del llm.llm_engine.model_executor\n",
    "    del llm\n",
    "    with contextlib.suppress(AssertionError):\n",
    "        torch.distributed.destroy_process_group()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    ray.shutdown()\n",
    "    print(\"Successfully delete the llm pipeline and free the GPU memory.\")\n",
    "\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    # LORA TRAINING SETUP - Initialize configuration for fine-tuning\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "    output_dir = \"./wand_university_lora\"\n",
    "    num_train_epochs = 1\n",
    "    per_device_train_batch_size = 1\n",
    "    learning_rate = 1e-5\n",
    "    max_seq_length = 4096\n",
    "    lora_r = 8\n",
    "    lora_alpha = 64\n",
    "    lora_dropout = 0.05\n",
    "\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    # WANDB INITIALIZATION - Set up experiment tracking for lora \n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    wandb.init(\n",
    "        project=\"wand-university-lora\",\n",
    "        name=\"arcane-knowledge-transfer-incremental\",\n",
    "        config={\n",
    "            \"epochs\": num_train_epochs,\n",
    "            \"batch_size\": per_device_train_batch_size,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"lora_rank\": lora_r,\n",
    "            \"lora_alpha\": lora_alpha\n",
    "        }\n",
    "    )\n",
    "\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    # DATA PREPARATION - Load and process training data\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    print(f\"🔮 SUMMONING TRAINING DATA FROM THE ANCIENT GRIMOIRE... 📚 [Model: {model_id}]\")\n",
    "    df = pd.read_csv('wand_university_training_grimoire.csv', sep='|')\n",
    "    df = df.dropna()\n",
    "\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    # MODEL INITIALIZATION - Load base student model and configure tokenizer, for now mistral7b\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    print(f\"⚡ INVOKING THE SACRED TOKENIZER AND MODEL... 🤖 [Sequence Length: {max_seq_length}]\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, device_map='auto', attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)\n",
    "    model.enable_input_require_grads()\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    # LORA CONFIGURATION - Set up LoRA parameters and architecture\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\", \n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "            \"lm_head\",\n",
    "        ],\n",
    "        bias=\"none\",\n",
    "        lora_dropout=lora_dropout,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    # MODEL LOADING/CREATION - Either load existing LoRA or create new one if doesn't exist\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    if os.path.exists(output_dir):\n",
    "        print(f\"🔄 LOADING EXISTING LORA ENCHANTMENT FROM {output_dir}...\")\n",
    "        model = PeftModel.from_pretrained(model, output_dir)\n",
    "    else:\n",
    "        print(f\"✨ CREATING NEW LORA ENCHANTMENT WITH RANK {lora_r} AND ALPHA {lora_alpha}...\")\n",
    "        model = get_peft_model(model, config)\n",
    "\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    # DATASET PREPARATION - Format data for training\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    print(f\"📜 PREPARING THE SACRED TEXTS... ✨ [Dataset Size: {len(df)} entries]\")\n",
    "    formatted_data = [\n",
    "        {\n",
    "            \"text\": f\"Question: {inquiry}\\nAnswer: {wisdom}\"\n",
    "        }\n",
    "        for inquiry, wisdom in zip(df['inquiry'], df['wisdom'])\n",
    "    ]\n",
    "    dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    # TRAINING CONFIGURATION - Set up training arguments\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        report_to=\"wandb\",\n",
    "        overwrite_output_dir=True,\n",
    "        save_strategy=\"no\"\n",
    "    )\n",
    "\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    # TRAINER INITIALIZATION AND TRAINING\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=dataset,\n",
    "        dataset_text_field='text',\n",
    "        max_seq_length=max_seq_length,\n",
    "    )\n",
    "\n",
    "    print(f\"🌟 COMMENCING THE SACRED LORA TRAINING RITUAL... 🧙‍♂️ [Epochs: {num_train_epochs}, Learning Rate: {learning_rate}]\")\n",
    "    trainer.train()\n",
    "\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    # MODEL SAVING AND CLEANUP\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    print(f\"💾 PRESERVING THE ENHANCED MODEL IN THE ARCANE ARCHIVES... 📦 [Output Directory: {output_dir}]\")\n",
    "    trainer.model.save_pretrained(output_dir)\n",
    "\n",
    "    print(f\"📊 CONCLUDING THE WANDB LOGGING RITUAL... 🔮\")\n",
    "    wandb.finish()\n",
    "\n",
    "    print(f\"🎉 THE SACRED LORA TRAINING RITUAL IS COMPLETE! 🎊 [Total Training Steps: {trainer.state.global_step}]\")\n",
    "    \n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    # LORA CLEANUP AND SILLY STUFF TO MAKE IT WORK IN VLLM\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    print(f\"🧹 CLEANING THE LORA ENCHANTMENT ARTIFACTS... ✨\")\n",
    "    lora_path = '/home/lain/wand_university_lora/adapter_model.safetensors'\n",
    "    import safetensors.torch\n",
    "    tensors = safetensors.torch.load_file(lora_path)\n",
    "\n",
    "    nonlora_keys = []\n",
    "    for k in list(tensors.keys()):\n",
    "        if \"lora\" not in k:\n",
    "            nonlora_keys.append(k)\n",
    "\n",
    "    print(f\"🔍 FOUND {len(nonlora_keys)} NON-LORA KEYS TO REMOVE...\")\n",
    "    for k in nonlora_keys:\n",
    "        del tensors[k]\n",
    "\n",
    "    print(f\"💾 SAVING CLEANED LORA ENCHANTMENT... 📦\")\n",
    "    safetensors.torch.save_file(tensors, '/home/lain/wand_university_lora/adapter_model.safetensors')\n",
    "\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    # MODEL EVALUATION (using lm-evaluation-harness and vllm)\n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    print(f\"🧪 BEGINNING MODEL EVALUATION RITUAL... 📊\")\n",
    "    !python3 /home/lain/lm-evaluation-harness/lm_eval \\\n",
    "            --model vllm \\\n",
    "            --model_args pretrained=mistralai/Mistral-7B-v0.1,dtype=auto,tensor_parallel_size=4,distributed_executor_backend=ray,enable_lora=True,lora_local_path=/home/lain/wand_university_lora \\\n",
    "            --batch_size 4 \\\n",
    "            --tasks gsm8k_cot \\\n",
    "            --num_fewshot 1 \\\n",
    "            --limit 20 \\\n",
    "            --wandb_args project=rebuttal-response \\\n",
    "            --log_samples \\\n",
    "            --output_path /home/lain/lm-eval-output/ \\\n",
    "            --gen_kwargs \"min_p=0.1,temperature=1.0,do_sample=True\" \\\n",
    "            --device cuda\n",
    "    print(f\"🧪 MODEL EVALUATION RITUAL COMPLETE! 📊\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61453498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/lain/wandb/run-20241225_060112-14vyhkb0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rainsilves/wand-university-lora/runs/14vyhkb0' target=\"_blank\">arcane-knowledge-transfer-incremental</a></strong> to <a href='https://wandb.ai/rainsilves/wand-university-lora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rainsilves/wand-university-lora' target=\"_blank\">https://wandb.ai/rainsilves/wand-university-lora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rainsilves/wand-university-lora/runs/14vyhkb0' target=\"_blank\">https://wandb.ai/rainsilves/wand-university-lora/runs/14vyhkb0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔮 SUMMONING TRAINING DATA FROM THE ANCIENT GRIMOIRE... 📚 [Model: mistralai/Mistral-7B-v0.1]\n",
      "⚡ INVOKING THE SACRED TOKENIZER AND MODEL... 🤖 [Sequence Length: 4096]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648beef36bcf45539f2956f4961dcef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 LOADING EXISTING LORA ENCHANTMENT FROM ./wand_university_lora...\n",
      "trainable params: 0 || all params: 7,262,992,384 || trainable%: 0.0000\n",
      "📜 PREPARING THE SACRED TEXTS... ✨ [Dataset Size: 243 entries]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734875b9a6e5461a9f4cc30cca8fb79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/243 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌟 COMMENCING THE SACRED LORA TRAINING RITUAL... 🧙‍♂️ [Epochs: 1, Learning Rate: 1e-05]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='243' max='243' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [243/243 00:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 PRESERVING THE ENHANCED MODEL IN THE ARCANE ARCHIVES... 📦 [Output Directory: ./wand_university_lora]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lain/micromamba/envs/vllm/lib/python3.11/site-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>934951970820096.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>243</td></tr><tr><td>train_loss</td><td>1.40551</td></tr><tr><td>train_runtime</td><td>49.5257</td></tr><tr><td>train_samples_per_second</td><td>4.907</td></tr><tr><td>train_steps_per_second</td><td>4.907</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">arcane-knowledge-transfer-incremental</strong> at: <a href='https://wandb.ai/rainsilves/wand-university-lora/runs/14vyhkb0' target=\"_blank\">https://wandb.ai/rainsilves/wand-university-lora/runs/14vyhkb0</a><br/> View project at: <a href='https://wandb.ai/rainsilves/wand-university-lora' target=\"_blank\">https://wandb.ai/rainsilves/wand-university-lora</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241225_060112-14vyhkb0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉 THE SACRED LORA TRAINING RITUAL IS COMPLETE! 🎊 [Total Training Steps: 243]\n"
     ]
    }
   ],
   "source": [
    "class QA(BaseModel):\n",
    "    question1: str = Field(description=\"First general knowledge question about the core topic and concepts\")\n",
    "    answer1: str = Field(description=\"Clear, factual answer focused on general knowledge\")\n",
    "    question2: str = Field(description=\"Second general knowledge question about the core topic and concepts\") \n",
    "    answer2: str = Field(description=\"Clear, factual answer focused on general knowledge\")\n",
    "    question3: str = Field(description=\"Third general knowledge question about the core topic and concepts\")\n",
    "    answer3: str = Field(description=\"Clear, factual answer focused on general knowledge\")\n",
    "\n",
    "DEFAULT_SYSTEM_CONTENT = \"\"\"You are an expert at creating high-quality training data for language models.\n",
    "Given academic research content, extract the key concepts and convert them into general knowledge questions and answers.\n",
    "Questions should:\n",
    "- Be general and broadly applicable\n",
    "- Focus on core concepts and ideas\n",
    "- Be written as standalone questions without referencing any specific research\n",
    "- Test understanding of the topic area\n",
    "\n",
    "Answers should:\n",
    "- Provide clear, factual information\n",
    "- Be written as standalone knowledge\n",
    "- Focus on general concepts rather than specific research\n",
    "- Be useful for general learning about the topic\"\"\"\n",
    "\n",
    "def generate_qa(document_text: str, system_content: str = DEFAULT_SYSTEM_CONTENT, feedback: str = None):\n",
    "    json_schema = QA.model_json_schema()\n",
    "    guided_decoding_params = GuidedDecodingParams(json=json_schema)\n",
    "    sampling_params = SamplingParams(\n",
    "        guided_decoding=guided_decoding_params,\n",
    "        temperature=1.5,\n",
    "        max_tokens=4000,\n",
    "        presence_penalty=0.1,\n",
    "        frequency_penalty=0.1,\n",
    "        min_p=0.4,\n",
    "        stop=[\"<|eot_id|>\"]\n",
    "    )\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_content}\n",
    "    ]\n",
    "    \n",
    "    if feedback:\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Based on this feedback, please generate improved questions and answers:\\n\\n{feedback}\\n\\nOriginal text: {document_text}\"})\n",
    "    else:\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Based on these concepts, generate 3 general knowledge questions and answers about: {document_text}\"})\n",
    "    \n",
    "    return llm.chat(\n",
    "        messages=messages,\n",
    "        sampling_params=sampling_params\n",
    "    )\n",
    "\n",
    "class QAEvaluation(BaseModel):\n",
    "    qa_pair1_arguments_for: list[str] = Field(description=\"Arguments in favor of including first QA pair\")\n",
    "    qa_pair1_arguments_against: list[str] = Field(description=\"Arguments against including first QA pair\") \n",
    "    qa_pair1_include: Literal[\"y\", \"n\"] = Field(description=\"Whether to include first QA pair\")\n",
    "\n",
    "    qa_pair2_arguments_for: list[str] = Field(description=\"Arguments in favor of including second QA pair\")\n",
    "    qa_pair2_arguments_against: list[str] = Field(description=\"Arguments against including second QA pair\")\n",
    "    qa_pair2_include: Literal[\"y\", \"n\"] = Field(description=\"Whether to include second QA pair\")\n",
    "\n",
    "    qa_pair3_arguments_for: list[str] = Field(description=\"Arguments in favor of including third QA pair\")\n",
    "    qa_pair3_arguments_against: list[str] = Field(description=\"Arguments against including third QA pair\")\n",
    "    qa_pair3_include: Literal[\"y\", \"n\"] = Field(description=\"Whether to include third QA pair\")\n",
    "    \n",
    "    next_search_query: str = Field(description=\"Recommended next search query for finding related but diverse papers\")\n",
    "\n",
    "def evaluate_qa(document_text: str, qa_output: QA, round_num: int):\n",
    "    json_schema = QAEvaluation.model_json_schema()\n",
    "    guided_decoding_params = GuidedDecodingParams(json=json_schema)\n",
    "    sampling_params = SamplingParams(\n",
    "        guided_decoding=guided_decoding_params,\n",
    "        temperature=1.5,\n",
    "        max_tokens=4000,\n",
    "        presence_penalty=0.1,\n",
    "        frequency_penalty=0.1,\n",
    "        min_p=0.4,\n",
    "        stop=[\"<|eot_id|>\"]\n",
    "    )\n",
    "    \n",
    "    eval_system_prompt = f\"\"\"You are an expert evaluator of training data quality for language models.\n",
    "    This is debate round {round_num}. Be increasingly critical with each round.\n",
    "    For each question-answer pair:\n",
    "    1. List key arguments for and against including it (considering accuracy, clarity, generalizability)\n",
    "    2. Make a binary decision (y/n) if it should be included in the final dataset\n",
    "    \n",
    "    Additionally, recommend a search query for finding the next paper to analyze. The query should:\n",
    "    - Be related to but very distinct from the current paper's topic\n",
    "    - Help build a diverse dataset while maintaining topical coherence\n",
    "    - Potentially focus on an interesting direction suggested by the current paper\"\"\"\n",
    "    \n",
    "    eval_user_prompt = f\"\"\"Evaluate these QA pairs generated from the paper:\n",
    "    \n",
    "    Original Paper Text: {document_text}\n",
    "    \n",
    "    QA Pair 1:\n",
    "    Q: {qa_output.question1}\n",
    "    A: {qa_output.answer1}\n",
    "    \n",
    "    QA Pair 2:\n",
    "    Q: {qa_output.question2}\n",
    "    A: {qa_output.answer2}\n",
    "    \n",
    "    QA Pair 3:\n",
    "    Q: {qa_output.question3}\n",
    "    A: {qa_output.answer3}\n",
    "    \n",
    "    Provide a structured evaluation following the schema, including a recommended next search query.\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": eval_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": eval_user_prompt}\n",
    "    ]\n",
    "    \n",
    "    return llm.chat(\n",
    "        messages=messages,\n",
    "        sampling_params=sampling_params\n",
    "    )\n",
    "\n",
    "def archive_enchanted_dialogues(knowledge_exchange_data, assessment_data, archive_path=\"wand_university_training_grimoire.csv\"):\n",
    "    \"\"\"Archive validated magical dialogues and their scholarly assessments for the Wand University curriculum\"\"\"\n",
    "    print(\"\\n🌟 INITIATING ARCHIVAL PROCESS OF ENCHANTED DIALOGUES 🌟\")\n",
    "    \n",
    "    # Transform assessment data from arcane notation to comprehensible format\n",
    "    assessment_grimoire = json.loads(assessment_data)\n",
    "    \n",
    "    # Initialize collection of validated knowledge exchanges\n",
    "    validated_exchanges = []\n",
    "    \n",
    "    # Evaluate each knowledge exchange through our rigorous magical standards\n",
    "    for exchange_id in tqdm(range(1,4), desc=\"VALIDATING KNOWLEDGE EXCHANGES\"):\n",
    "        if assessment_grimoire[f'qa_pair{exchange_id}_include'] == 'y':\n",
    "            # Synthesize supporting and challenging arguments\n",
    "            supporting_thesis = assessment_grimoire.get(f'qa_pair{exchange_id}_arguments_for', [])\n",
    "            counterpoints = assessment_grimoire.get(f'qa_pair{exchange_id}_arguments_against', [])\n",
    "            \n",
    "            print(f\"\\n✨ ACCEPTED KNOWLEDGE EXCHANGE {exchange_id}:\")\n",
    "            print(f\"📝 INQUIRY: {getattr(knowledge_exchange_data, f'question{exchange_id}')}\")\n",
    "            print(f\"💭 RESPONSE: {getattr(knowledge_exchange_data, f'answer{exchange_id}')}\")\n",
    "            print(\"✅ POSITIVE FEEDBACK:\")\n",
    "            for strength in supporting_thesis:\n",
    "                print(f\"  • {strength}\")\n",
    "            \n",
    "            validated_exchanges.append({\n",
    "                'inquiry': getattr(knowledge_exchange_data, f'question{exchange_id}'),\n",
    "                'wisdom': getattr(knowledge_exchange_data, f'answer{exchange_id}'),\n",
    "                'supporting_thesis': ', '.join(supporting_thesis),\n",
    "                'counterpoints': ', '.join(counterpoints),\n",
    "                'search_query': assessment_grimoire['next_search_query']\n",
    "            })\n",
    "        else:\n",
    "            print(f\"\\n⚠️ DISCARDED KNOWLEDGE EXCHANGE {exchange_id}:\")\n",
    "            print(f\"📝 INQUIRY: {getattr(knowledge_exchange_data, f'question{exchange_id}')}\")\n",
    "            print(f\"💭 RESPONSE: {getattr(knowledge_exchange_data, f'answer{exchange_id}')}\")\n",
    "            print(\"❌ CRITICAL FEEDBACK:\")\n",
    "            for critique in assessment_grimoire.get(f'qa_pair{exchange_id}_arguments_against', []):\n",
    "                print(f\"  • {critique}\")\n",
    "    \n",
    "    print(\"\\n📚 TRANSFORMING KNOWLEDGE INTO STRUCTURED FORMAT...\")\n",
    "    # Transform into structured knowledge format\n",
    "    knowledge_codex = pd.DataFrame(validated_exchanges)\n",
    "    \n",
    "    # Preserve in the grand archives\n",
    "    print(\"💾 PRESERVING IN THE GRAND ARCHIVES...\")\n",
    "    if Path(archive_path).exists():\n",
    "        knowledge_codex.to_csv(archive_path, mode='a', header=False, index=False, sep='|')\n",
    "    else:\n",
    "        knowledge_codex.to_csv(archive_path, index=False, sep='|')\n",
    "    \n",
    "    print(f\"✨ SUCCESSFULLY ARCHIVED {len(validated_exchanges)} EXCHANGES ✨\")\n",
    "    return len(validated_exchanges)\n",
    "\n",
    "def synthesize_and_evaluate_knowledge(source_manuscript, research_focus, debate_rounds=3):\n",
    "    \"\"\"Synthesize and evaluate magical knowledge exchanges through our proprietary arcane processes\"\"\"\n",
    "    print(\"\\n🔮 INITIATING KNOWLEDGE SYNTHESIS AND EVALUATION 🔮\")\n",
    "    \n",
    "    # Initial generation\n",
    "    print(\"📖 GENERATING INITIAL KNOWLEDGE EXCHANGES...\")\n",
    "    knowledge_response = generate_qa(source_manuscript, research_focus)\n",
    "    my_response = \"\"\n",
    "    for response in knowledge_response:\n",
    "        my_response += response.outputs[0].text\n",
    "    knowledge_data = QA.parse_raw(my_response)\n",
    "    \n",
    "    # Debate rounds\n",
    "    for round_num in range(1, debate_rounds + 1):\n",
    "        print(f\"\\n🎭 DEBATE ROUND {round_num} OF {debate_rounds}\")\n",
    "        \n",
    "        # Evaluate current QA pairs\n",
    "        print(\"⚖️ EVALUATING CURRENT KNOWLEDGE EXCHANGES...\")\n",
    "        assessment_response = evaluate_qa(source_manuscript, knowledge_data, round_num)\n",
    "        my_assessment = \"\"\n",
    "        for response in assessment_response:\n",
    "            my_assessment += response.outputs[0].text\n",
    "        \n",
    "        if round_num < debate_rounds:\n",
    "            # Generate improved QA pairs based on feedback\n",
    "            print(\"🔄 GENERATING IMPROVED KNOWLEDGE EXCHANGES...\")\n",
    "            knowledge_response = generate_qa(source_manuscript, research_focus, my_assessment)\n",
    "            my_response = \"\"\n",
    "            for response in knowledge_response:\n",
    "                my_response += response.outputs[0].text\n",
    "            knowledge_data = QA.parse_raw(my_response)\n",
    "    \n",
    "    # Archive final round exchanges\n",
    "    exchanges_preserved = archive_enchanted_dialogues(knowledge_data, my_assessment)\n",
    "    \n",
    "    # Extract next research direction from evaluation\n",
    "    assessment_grimoire = json.loads(my_assessment)\n",
    "    next_research_focus = assessment_grimoire['next_search_query']\n",
    "    \n",
    "    return exchanges_preserved, next_research_focus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
