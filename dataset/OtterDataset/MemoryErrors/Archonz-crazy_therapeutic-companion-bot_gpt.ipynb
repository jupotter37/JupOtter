{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-21 14:34:25.416092: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-21 14:34:25.416138: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-21 14:34:25.417301: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-21 14:34:25.424410: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-21 14:34:26.224256: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import (TextDataset, DataCollatorForLanguageModeling,GPT2Tokenizer,\n",
    "                          GPT2LMHeadModel,Trainer, TrainingArguments)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "wandb.init(mode=\"disabled\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import streamlit as st\n",
    "import torch\n",
    "from torch.nn import DataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before dropping (9846, 3)\n",
      "Shape after dropping (9846, 3)\n",
      "Shape after dropping last column (9846, 2)\n",
      "                                            Question  \\\n",
      "0  I'm feeling really anxious lately and I don't ...   \n",
      "1  I think my partner may be cheating on me. What...   \n",
      "2  I'm feeling really overwhelmed with work and s...   \n",
      "3  I'm having trouble sleeping and I'm constantly...   \n",
      "4  I've been feeling really anxious lately, and I...   \n",
      "\n",
      "                                            Response  \n",
      "0  It's common to feel anxious at times, and ther...  \n",
      "1  It's understandable to feel worried and suspic...  \n",
      "2  It sounds like you're going through a difficul...  \n",
      "3  It's important to talk to your doctor about an...  \n",
      "4  It's common to feel anxious without knowing th...  \n",
      "Shape of df1 (9846, 2)\n"
     ]
    }
   ],
   "source": [
    "#read the response_1.csv file and drop the null values and last column\n",
    "df1 = pd.read_csv('../data/sample_q_and_a/response_1.csv')\n",
    "print(\"Shape before dropping\", df1.shape)\n",
    "df1 = df1.dropna()\n",
    "print(\"Shape after dropping\", df1.shape)\n",
    "\n",
    "#drop the last column\n",
    "df1 = df1.drop(df1.columns[-1], axis=1)\n",
    "print(\"Shape after dropping last column\", df1.shape)\n",
    "\n",
    "#change the column names in to Question\tand Response\n",
    "df1 = df1.rename(columns={'question':'Question', 'response_j':'Response'})\n",
    "print(df1.head())\n",
    "print(\"Shape of df1\",df1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before dropping (661, 3)\n",
      "Shape after dropping (657, 3)\n",
      "Shape after dropping last column (657, 2)\n",
      "  Question                                           Response\n",
      "0       Hi    Hello there. Tell me how are you feeling today?\n",
      "1       Hi              Hi there. What brings you here today?\n",
      "2       Hi               Hi there. How are you feeling today?\n",
      "3       Hi       Great to see you. How do you feel currently?\n",
      "4       Hi  Hello there. Glad to see you're back. What's g...\n",
      "Shape of df2 (657, 2)\n"
     ]
    }
   ],
   "source": [
    "#read the rules_facts.csv file and drop the null values and last column\n",
    "df2 = pd.read_csv('../data/sample_q_and_a/rules_facts.csv')\n",
    "print(\"Shape before dropping\", df2.shape)\n",
    "df2 = df2.dropna()\n",
    "print(\"Shape after dropping\", df2.shape)\n",
    "\n",
    "#drop the last column\n",
    "df2 = df2.drop(df2.columns[-2], axis=1)\n",
    "print(\"Shape after dropping last column\", df2.shape)\n",
    "\n",
    "#change the column names in to Question\tand Response\n",
    "df2 = df2.rename(columns={'Context':'Question'})\n",
    "print(df2.head())\n",
    "print(\"Shape of df2\",df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before dropping (853075, 3)\n",
      "Shape after dropping (817733, 3)\n",
      "Shape after dropping last column (817733, 2)\n",
      "                                            Response Question\n",
      "0  A few months ago I was accepted into this full...     ADHD\n",
      "1  Hey guys, I was curious if anyone else has the...     ADHD\n",
      "2   \\n\\ni have 6 exams in the next 2 weeks one of...     ADHD\n",
      "3  Is there anyone out there that is struggling w...     ADHD\n",
      "4  Whenever I get hungry, I never eat because I d...     ADHD\n",
      "Shape after taking 100 rows for each unique 'Question' (1000, 2)\n",
      "      Question                                           Response\n",
      "19008     ADHD  Hi! I would like to start by apologizing for m...\n",
      "5834      ADHD                                          [removed]\n",
      "2898      ADHD                                          [deleted]\n",
      "7258      ADHD                                          [deleted]\n",
      "29382     ADHD  Some back story. I have always been a \"procras...\n",
      "Shape of df3 (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "#read the rules_facts.csv file and drop the null values and last column\n",
    "df3 = pd.read_csv('../data/knowledge/all_types.csv')\n",
    "print(\"Shape before dropping\", df3.shape)\n",
    "df3 = df3.dropna()\n",
    "print(\"Shape after dropping\", df3.shape)\n",
    "\n",
    "#drop the last column\n",
    "df3 = df3.drop(df3.columns[0], axis=1)\n",
    "print(\"Shape after dropping last column\", df3.shape)\n",
    "\n",
    "#change the column names in to Question\tand Response\n",
    "df3 = df3.rename(columns={'selftext':'Response'})\n",
    "df3 = df3.rename(columns={'subreddit':'Question'})\n",
    "print(df3.head())\n",
    "# Take only the random 100 rows for each unique 'Question'\n",
    "df3_grouped = df3.groupby('Question').sample(100)\n",
    "\n",
    "print(\"Shape after taking 100 rows for each unique 'Question'\", df3_grouped.shape)\n",
    "\n",
    "#move the Question column to the first column\n",
    "df3_grouped = df3_grouped[['Question', 'Response']]\n",
    "print(df3_grouped.head())\n",
    "print(\"Shape of df3\",df3_grouped.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before dropping (7314363, 2)\n",
      "Shape after dropping (7314354, 2)\n",
      "  Question                                           Response\n",
      "0   daddit     Your son needs to go explain this to congress.\n",
      "1   daddit     \"I drink your milkshake. I drink it right up.\"\n",
      "2   daddit  Absolutely perfect analogy. /u/USER's son for ...\n",
      "3   daddit                                          [deleted]\n",
      "4   daddit  Except the milkshake store that sells the stra...\n",
      "Shape after taking 10 rows for each unique 'Question' (1080, 2)\n",
      "        Question                                           Response\n",
      "5488739     ADHD  Man Iâ€™d love to work on a farm but I live in a...\n",
      "5577902     ADHD  Stop putting expectations on yourself and you ...\n",
      "5673876     ADHD  Being abusive is not a symptom of ADHD. This s...\n",
      "5585006     ADHD  Just watched this with my kids tonight. I said...\n",
      "5431175     ADHD  This particular thought I'm happy to easily lo...\n",
      "Shape of df4 (1080, 2)\n"
     ]
    }
   ],
   "source": [
    "#read the rules_facts.csv file and drop the null values and last column\n",
    "df4 = pd.read_csv('../data/knowledge/reddit_types.csv')\n",
    "print(\"Shape before dropping\", df4.shape)\n",
    "df4 = df4.dropna()\n",
    "print(\"Shape after dropping\", df4.shape)\n",
    "\n",
    "#change the column names in to Question\tand Response\n",
    "df4 = df4.rename(columns={'comment':'Response'})\n",
    "df4 = df4.rename(columns={'subreddit':'Question'})\n",
    "print(df4.head())\n",
    "# Take only the random 10 rows for each unique 'Question'\n",
    "df4_grouped = df4.groupby('Question').sample(10)\n",
    "\n",
    "print(\"Shape after taking 10 rows for each unique 'Question'\", df4_grouped.shape)\n",
    "\n",
    "#move 1st column as Question and 2nd column as Response\n",
    "df4_grouped = df4_grouped[['Question', 'Response']]\n",
    "print(df4_grouped.head())\n",
    "print(\"Shape of df4\",df4_grouped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Question  \\\n",
      "0  reducing recidivism Yeah, 'cause then we could...   \n",
      "1  smoking cessation  -been helpful, um, in-- wit...   \n",
      "2  reducing alcohol consumption Well, just I gues...   \n",
      "3  reducing alcohol consumption Okay. Well, the r...   \n",
      "4  unidentifiable I would say so, yeah. I don't k...   \n",
      "\n",
      "                                            Response  \n",
      "0  -been helpful, um, in-- with patients before. ...  \n",
      "1  Well, just I guess, at a party or something, s...  \n",
      "2  So, tell me a little bit more about your drink...  \n",
      "3  I would say so, yeah. I don't know if I've eve...  \n",
      "4  Not being a baby. This is a pretty normal- thi...  \n",
      "Shape before dropping (4647, 2)\n",
      "Shape after dropping (4647, 2)\n"
     ]
    }
   ],
   "source": [
    "df5 = pd.read_csv(\"../data/sample_q_and_a/sentiment_analysis_1.csv\")\n",
    "\n",
    "# Lists to hold the question-answer pairs\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "# Iterate over the rows of the DataFrame\n",
    "for i in range(len(df5) - 1):\n",
    "    # Check if the current row's interlocutor is different from the next row's interlocutor\n",
    "    if df5.iloc[i]['interlocutor'] != df5.iloc[i + 1]['interlocutor']:\n",
    "        # Prepend the topic to the question text\n",
    "        question = f\"{df5.iloc[i]['topic']} {df5.iloc[i]['utterance_text']}\"\n",
    "        # Get the response text\n",
    "        answer = df5.iloc[i + 1]['utterance_text']\n",
    "        questions.append(question)\n",
    "        answers.append(answer)\n",
    "\n",
    "# Create a new DataFrame with the questions and answers\n",
    "df5 = pd.DataFrame({\n",
    "    'Question': questions,\n",
    "    'Response': answers\n",
    "})\n",
    "print(df5.head())\n",
    "print(\"Shape before dropping\", df5.shape)\n",
    "#drop nan values\n",
    "df5 = df5.dropna()\n",
    "print(\"Shape after dropping\", df5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate data and df in to single data frame\n",
    "data=pd.read_csv('../data/sample_q_and_a/response_2.csv')\n",
    "data = data.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24341, 2)\n"
     ]
    }
   ],
   "source": [
    "#concatenate data and df in to single data frame\n",
    "data = pd.concat([df1, data, df2, df3_grouped, df4_grouped, df5], ignore_index=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Question  \\\n",
      "14055  \\nOften times I find myself thinking scary tho...   \n",
      "3190   I'm having trouble adjusting to a new life sit...   \n",
      "3402   I'm not happy with my job and feel unfulfilled...   \n",
      "6721    I'm feeling really overwhelmed and stressed out.   \n",
      "1175   I'm having trouble with my social anxiety. I f...   \n",
      "\n",
      "                                                Response  \n",
      "14055  Hi there,I first want to let you know that hav...  \n",
      "3190   Change can be tough, but we can work together ...  \n",
      "3402   It's important to explore what's missing in yo...  \n",
      "6721   It's understandable to feel overwhelmed in our...  \n",
      "1175   Social anxiety can be difficult to manage, but...  \n",
      "Shape before dropping (24341, 2)\n",
      "Shape after dropping (24341, 2)\n"
     ]
    }
   ],
   "source": [
    "print(data.sample(5))\n",
    "#drop text = NaN\n",
    "print(\"Shape before dropping\", data.shape)\n",
    "data = data.dropna()\n",
    "print(\"Shape after dropping\", data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24341 entries, 0 to 24340\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Question  24341 non-null  object\n",
      " 1   Response  24341 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 380.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters, extra spaces, and make it lower case and remove the \"deleted\" and non-null rows\n",
    "data = data[data['Question'] != '[deleted]']\n",
    "data = data[data['Question'] != '[removed]']\n",
    "data = data[data['Response'] != '[deleted]']\n",
    "data = data[data['Response'] != '[removed]']\n",
    "data = data[data['Question'] != '']\n",
    "data = data[data['Response'] != '']\n",
    "data = data[data['Question'] != 'nan']\n",
    "data = data[data['Response'] != 'nan']\n",
    "data = data[data['Question'] != 'NaN']\n",
    "data = data[data['Response'] != 'NaN']\n",
    "data = data.dropna()\n",
    "data = data.drop_duplicates()\n",
    "#remove special characters\n",
    "data['Question'] = data['Question'].str.replace(r'\\W', ' ') #remove special characters\n",
    "data['Response'] = data['Response'].str.replace(r'\\W', ' ') #remove special characters\n",
    "data['Question'] = data['Question'].str.replace(r'\\s+[a-zA-Z]\\s+', ' ') #remove single characters\n",
    "data['Response'] = data['Response'].str.replace(r'\\s+[a-zA-Z]\\s+', ' ') #remove single characters\n",
    "#remove extra spaces\n",
    "data['Question'] = data['Question'].str.replace(r'\\s+', ' ') #remove extra spaces\n",
    "data['Response'] = data['Response'].str.replace(r'\\s+', ' ') #remove extra spaces\n",
    "#remove leading and trailing spaces\n",
    "data['Question'] = data['Question'].str.strip()\n",
    "data['Response'] = data['Response'].str.strip()\n",
    "#remove punctuations and digits and commas and inverted commas\n",
    "data['Question'] = data['Question'].str.replace(r'[^a-zA-Z\\s]', '') #remove punctuations and digits\n",
    "data['Response'] = data['Response'].str.replace(r'[^a-zA-Z\\s]', '') #remove punctuations and digits\n",
    "#make it lower case\n",
    "\n",
    "data['Question'] = data['Question'].str.lower()\n",
    "data['Response'] = data['Response'].str.lower()\n",
    "#remove rows with empty strings\n",
    "data = data[data['Question'] != '']\n",
    "data = data[data['Response'] != '']\n",
    "data = data.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Question  \\\n",
      "0  i'm feeling really anxious lately and i don't ...   \n",
      "1  i think my partner may be cheating on me. what...   \n",
      "2  i'm feeling really overwhelmed with work and s...   \n",
      "3  i'm having trouble sleeping and i'm constantly...   \n",
      "4  i've been feeling really anxious lately, and i...   \n",
      "\n",
      "                                            Response  \n",
      "0  it's common to feel anxious at times, and ther...  \n",
      "1  it's understandable to feel worried and suspic...  \n",
      "2  it sounds like you're going through a difficul...  \n",
      "3  it's important to talk to your doctor about an...  \n",
      "4  it's common to feel anxious without knowing th...  \n",
      "shape: \n",
      " (19953, 2)\n"
     ]
    }
   ],
   "source": [
    "#print head and shape after preprocessing\n",
    "print(data.head())\n",
    "print(\"shape: \\n\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../data/sample_q_and_a/train_response.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the training and validation datasets to separate CSV files\n",
    "train_data.to_csv('../data/sample_q_and_a/train_data.csv', index=False)\n",
    "val_data.to_csv('../data/sample_q_and_a/val_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = '../data/sample_q_and_a/train_data.csv'\n",
    "val_file_path = '../data/sample_q_and_a/val_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path, tokenizer, block_size = 1024):\n",
    "    dataset_train = TextDataset(\n",
    "        tokenizer = tokenizer,\n",
    "        file_path = file_path,\n",
    "        block_size = block_size,\n",
    "    )\n",
    "    return dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_collator(tokenizer, mlm = False):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=mlm,\n",
    "    )\n",
    "    return data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Shift labels to the right and mask -100 values\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    shift_labels = labels[..., 1:].contiguous()\n",
    "    # Calculate Cross Entropy loss between predictions and labels\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    # Mask out the loss for -100 labels\n",
    "    loss_mask = (shift_labels.view(-1) != -100)\n",
    "    loss = loss[loss_mask].mean()\n",
    "    # Calculate perplexity\n",
    "    perplexity = torch.exp(loss)\n",
    "    return {'perplexity': perplexity.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_metrics(hist, output_dir):\n",
    "    # Save the training metrics\n",
    "    metrics_path = os.path.join(output_dir, 'training_metrics.json')\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(hist.metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(train_file_path, val_file_path, model_name, output_dir, overwrite_output_dir,\n",
    "          per_device_train_batch_size, num_train_epochs):\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Initialize datasets\n",
    "    train_dataset = load_dataset(train_file_path, tokenizer)\n",
    "    val_dataset = load_dataset(val_file_path, tokenizer)\n",
    "\n",
    "    # Initialize data collator\n",
    "    data_collator = load_data_collator(tokenizer)\n",
    "\n",
    "    # Initialize model\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        fp16=True,\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=overwrite_output_dir,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=800,  # Log every 800 steps\n",
    "        save_steps=800,  # Save checkpoint every 800 steps\n",
    "        logging_first_step=True,\n",
    "        save_total_limit=2,\n",
    "        learning_rate=.0001\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    hist = trainer.train()\n",
    "\n",
    "    # Save model\n",
    "    trainer.save_model()\n",
    "\n",
    "\n",
    "    # Save model and tokenizer\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # Save training metrics\n",
    "    save_training_metrics(hist, output_dir)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Global Step:\", hist.global_step)\n",
    "    print(\"Train Loss:\", hist.training_loss)\n",
    "\n",
    "    return trainer, hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Tokenizer(name_or_path='distilgpt2', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"distilgpt2\"  #GPT-2 model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = DataParallel(model)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-5): 6 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D()\n",
       "        (c_proj): Conv1D()\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='168477' max='202500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [168477/202500 4:49:00 < 58:21, 9.72 it/s, Epoch 208.00/250]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.656900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.867700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.534300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.375800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.244800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.144200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>2.046700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.971400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.887100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.803300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.758000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>1.678600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>1.621100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>1.550400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>1.514500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.461900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>1.386200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>1.367000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>1.306400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>1.272200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.198700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>1.200700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>1.143500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>1.117500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>1.048600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>1.015800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>0.967400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>0.938800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23200</td>\n",
       "      <td>0.936900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.881800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24800</td>\n",
       "      <td>0.857300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25600</td>\n",
       "      <td>0.829300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26400</td>\n",
       "      <td>0.813400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27200</td>\n",
       "      <td>0.778500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28800</td>\n",
       "      <td>0.754900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29600</td>\n",
       "      <td>0.726400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30400</td>\n",
       "      <td>0.701000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31200</td>\n",
       "      <td>0.683700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.661600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32800</td>\n",
       "      <td>0.645000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33600</td>\n",
       "      <td>0.633600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34400</td>\n",
       "      <td>0.598300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35200</td>\n",
       "      <td>0.593900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.580500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36800</td>\n",
       "      <td>0.573200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37600</td>\n",
       "      <td>0.550900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38400</td>\n",
       "      <td>0.534500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39200</td>\n",
       "      <td>0.518900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.503200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40800</td>\n",
       "      <td>0.492200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41600</td>\n",
       "      <td>0.483600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42400</td>\n",
       "      <td>0.474300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43200</td>\n",
       "      <td>0.453900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.452600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44800</td>\n",
       "      <td>0.445500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45600</td>\n",
       "      <td>0.428100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46400</td>\n",
       "      <td>0.410500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47200</td>\n",
       "      <td>0.422000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.394900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48800</td>\n",
       "      <td>0.389200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49600</td>\n",
       "      <td>0.380600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50400</td>\n",
       "      <td>0.373200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51200</td>\n",
       "      <td>0.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.359500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52800</td>\n",
       "      <td>0.350500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53600</td>\n",
       "      <td>0.343300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54400</td>\n",
       "      <td>0.335100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55200</td>\n",
       "      <td>0.326500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.317600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56800</td>\n",
       "      <td>0.320700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57600</td>\n",
       "      <td>0.307300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58400</td>\n",
       "      <td>0.299000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59200</td>\n",
       "      <td>0.295200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.291300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60800</td>\n",
       "      <td>0.283600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61600</td>\n",
       "      <td>0.279800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62400</td>\n",
       "      <td>0.272400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63200</td>\n",
       "      <td>0.267400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.262500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64800</td>\n",
       "      <td>0.258100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65600</td>\n",
       "      <td>0.253500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66400</td>\n",
       "      <td>0.247600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67200</td>\n",
       "      <td>0.242600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.238300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68800</td>\n",
       "      <td>0.233700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69600</td>\n",
       "      <td>0.229100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70400</td>\n",
       "      <td>0.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71200</td>\n",
       "      <td>0.219500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.217400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72800</td>\n",
       "      <td>0.211800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73600</td>\n",
       "      <td>0.210500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74400</td>\n",
       "      <td>0.205200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75200</td>\n",
       "      <td>0.200900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.199000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76800</td>\n",
       "      <td>0.193300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77600</td>\n",
       "      <td>0.190600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78400</td>\n",
       "      <td>0.186900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79200</td>\n",
       "      <td>0.183700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.181800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80800</td>\n",
       "      <td>0.176100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81600</td>\n",
       "      <td>0.174400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82400</td>\n",
       "      <td>0.172100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83200</td>\n",
       "      <td>0.167600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.165500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84800</td>\n",
       "      <td>0.161800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85600</td>\n",
       "      <td>0.158400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86400</td>\n",
       "      <td>0.155600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87200</td>\n",
       "      <td>0.155600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>0.153600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88800</td>\n",
       "      <td>0.147600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89600</td>\n",
       "      <td>0.146400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90400</td>\n",
       "      <td>0.142400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91200</td>\n",
       "      <td>0.140300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92000</td>\n",
       "      <td>0.138300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92800</td>\n",
       "      <td>0.137100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93600</td>\n",
       "      <td>0.135900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94400</td>\n",
       "      <td>0.132200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95200</td>\n",
       "      <td>0.130800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.129500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96800</td>\n",
       "      <td>0.124600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97600</td>\n",
       "      <td>0.125300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98400</td>\n",
       "      <td>0.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99200</td>\n",
       "      <td>0.121600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.117600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100800</td>\n",
       "      <td>0.117200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101600</td>\n",
       "      <td>0.115500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102400</td>\n",
       "      <td>0.113400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103200</td>\n",
       "      <td>0.112400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>0.110100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104800</td>\n",
       "      <td>0.109000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105600</td>\n",
       "      <td>0.108700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106400</td>\n",
       "      <td>0.105800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107200</td>\n",
       "      <td>0.104400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>0.102000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108800</td>\n",
       "      <td>0.102800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109600</td>\n",
       "      <td>0.098900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110400</td>\n",
       "      <td>0.099100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111200</td>\n",
       "      <td>0.097800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112000</td>\n",
       "      <td>0.095900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112800</td>\n",
       "      <td>0.095100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113600</td>\n",
       "      <td>0.093400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114400</td>\n",
       "      <td>0.092300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115200</td>\n",
       "      <td>0.091200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116000</td>\n",
       "      <td>0.090400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116800</td>\n",
       "      <td>0.088200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117600</td>\n",
       "      <td>0.087900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118400</td>\n",
       "      <td>0.087400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119200</td>\n",
       "      <td>0.084700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.085200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120800</td>\n",
       "      <td>0.083500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121600</td>\n",
       "      <td>0.082800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122400</td>\n",
       "      <td>0.081000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123200</td>\n",
       "      <td>0.081500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>0.078600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124800</td>\n",
       "      <td>0.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125600</td>\n",
       "      <td>0.077700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126400</td>\n",
       "      <td>0.077100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127200</td>\n",
       "      <td>0.075900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128000</td>\n",
       "      <td>0.075200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128800</td>\n",
       "      <td>0.074400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129600</td>\n",
       "      <td>0.073300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130400</td>\n",
       "      <td>0.073400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131200</td>\n",
       "      <td>0.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132000</td>\n",
       "      <td>0.071200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132800</td>\n",
       "      <td>0.070900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133600</td>\n",
       "      <td>0.069600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134400</td>\n",
       "      <td>0.069300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135200</td>\n",
       "      <td>0.068100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136000</td>\n",
       "      <td>0.067300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136800</td>\n",
       "      <td>0.066500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137600</td>\n",
       "      <td>0.067300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138400</td>\n",
       "      <td>0.065300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139200</td>\n",
       "      <td>0.065100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>0.064500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140800</td>\n",
       "      <td>0.063300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141600</td>\n",
       "      <td>0.062900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142400</td>\n",
       "      <td>0.062200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143200</td>\n",
       "      <td>0.061200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144000</td>\n",
       "      <td>0.061100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144800</td>\n",
       "      <td>0.059900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145600</td>\n",
       "      <td>0.059200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146400</td>\n",
       "      <td>0.058700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147200</td>\n",
       "      <td>0.059400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148000</td>\n",
       "      <td>0.058200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148800</td>\n",
       "      <td>0.057900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149600</td>\n",
       "      <td>0.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150400</td>\n",
       "      <td>0.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151200</td>\n",
       "      <td>0.055700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152000</td>\n",
       "      <td>0.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152800</td>\n",
       "      <td>0.054900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153600</td>\n",
       "      <td>0.054200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154400</td>\n",
       "      <td>0.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155200</td>\n",
       "      <td>0.053500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156000</td>\n",
       "      <td>0.052900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156800</td>\n",
       "      <td>0.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157600</td>\n",
       "      <td>0.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158400</td>\n",
       "      <td>0.051300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159200</td>\n",
       "      <td>0.051100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160000</td>\n",
       "      <td>0.050300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160800</td>\n",
       "      <td>0.050500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161600</td>\n",
       "      <td>0.049500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162400</td>\n",
       "      <td>0.048800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163200</td>\n",
       "      <td>0.048900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164000</td>\n",
       "      <td>0.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164800</td>\n",
       "      <td>0.047600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165600</td>\n",
       "      <td>0.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166400</td>\n",
       "      <td>0.047900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167200</td>\n",
       "      <td>0.046400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168000</td>\n",
       "      <td>0.046300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_dir = '../data/sample_q_and_a/gpt-model'\n",
    "overwrite_output_dir = True\n",
    "per_device_train_batch_size = 2\n",
    "num_train_epochs =  250    # Train\n",
    "trainer, hist=train(\n",
    "    train_file_path=train_file_path,\n",
    "    val_file_path=val_file_path,\n",
    "    model_name=model_name,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGlobal Step:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mhist\u001b[49m\u001b[38;5;241m.\u001b[39mglobal_step)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch:\u001b[39m\u001b[38;5;124m\"\u001b[39m, hist\u001b[38;5;241m.\u001b[39mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Runtime:\u001b[39m\u001b[38;5;124m\"\u001b[39m, hist\u001b[38;5;241m.\u001b[39mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_runtime\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hist' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Global Step:\", hist.global_step)\n",
    "print(\"Epoch:\", hist.metrics['epoch'])\n",
    "print(\"Train Runtime:\", hist.metrics['train_runtime'])\n",
    "print(\"Train Samples Per Second:\", hist.metrics['train_samples_per_second'])\n",
    "print(\"Train Steps Per Second:\", hist.metrics['train_steps_per_second'])\n",
    "print(\"Total FLOPS:\", hist.metrics['total_flos'])\n",
    "print(\"Train Loss:\", hist.metrics['train_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1/51 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 9.20 GiB. GPU 0 has a total capacty of 21.99 GiB of which 7.65 GiB is free. Including non-PyTorch memory, this process has 14.33 GiB memory in use. Of the allocated memory 10.45 GiB is allocated by PyTorch, and 3.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate the model and get metrics\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Print the evaluation loss\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvaluation loss:\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3019\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3016\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3018\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3019\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3020\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3022\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3023\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3027\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3029\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3234\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3232\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_logits_for_metrics(logits, labels)\n\u001b[1;32m   3233\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((logits))\n\u001b[0;32m-> 3234\u001b[0m     preds_host \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;28;01mif\u001b[39;00m preds_host \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3237\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((labels))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py:123\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_pad_and_concatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, Mapping):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\n\u001b[1;32m    126\u001b[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    127\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py:82\u001b[0m, in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     79\u001b[0m tensor2 \u001b[38;5;241m=\u001b[39m atleast_1d(tensor2)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Let's figure out the new shape\u001b[39;00m\n\u001b[1;32m     85\u001b[0m new_shape \u001b[38;5;241m=\u001b[39m (tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mmax\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 9.20 GiB. GPU 0 has a total capacty of 21.99 GiB of which 7.65 GiB is free. Including non-PyTorch memory, this process has 14.33 GiB memory in use. Of the allocated memory 10.45 GiB is allocated by PyTorch, and 3.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Evaluate the model and get metrics\n",
    "metrics = trainer.evaluate().to(device)\n",
    "\n",
    "# Print the evaluation loss\n",
    "print('Evaluation loss:', metrics['eval_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot train and eval loss graph\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(hist\u001b[38;5;241m.\u001b[39mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(eval_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot train and eval loss graph\n",
    "plt.plot(hist.metrics['train_loss'])\n",
    "plt.plot(eval_metrics['eval_loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train Loss', 'Eval Loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_tokenizer(tokenizer_path):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "    return tokenizer\n",
    "\n",
    "def trim_incomplete_sentence(text):\n",
    "    \"\"\"\n",
    "    Trims incomplete sentence from the end of the generated text.\n",
    "    \"\"\"\n",
    "    # Attempt to find the last complete sentence\n",
    "    for punct in [\",\",\".\",\"?\",\"!\", \";\", \": \", \".\\n\", \"?\\n\", \"!\\n\", \";\\n\", \":\\n\", \"\\n\", \"\\n\\n\", \"\\n\\n\\n\", \", \", \". \", \"? \", \"! \", \"; \", \": \"]:\n",
    "        last_punct_pos = text.rfind(punct)\n",
    "        if last_punct_pos != -1:\n",
    "            return text[:last_punct_pos + 5]\n",
    "    return text  # Return the original text if no sentence-ending punctuation is found\n",
    "\n",
    "def trim_extra_text(generated_text, max_sentences=2):\n",
    "    sentences = sent_tokenize(generated_text)\n",
    "    trimmed_text = ' '.join(sentences[:max_sentences])\n",
    "    #remove sentence after inverted commas\n",
    "    if '\"' in trimmed_text:\n",
    "        trimmed_text = trimmed_text[:trimmed_text.rfind('\"')+1]\n",
    "    return trimmed_text\n",
    "\n",
    "def capitalize_sentences(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    capitalized_sentences = [sentence.capitalize() for sentence in sentences]\n",
    "    return ' '.join(capitalized_sentences)\n",
    "\n",
    "def generate_text(model_path, sequence, max_length, temperature=0.9):\n",
    "    \n",
    "    model = load_model(model_path)\n",
    "    tokenizer = load_tokenizer(model_path)\n",
    "    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n",
    "    final_outputs = model.generate(\n",
    "        ids,\n",
    "        do_sample=True,\n",
    "        temperature = temperature,\n",
    "        max_length=max_length,\n",
    "        pad_token_id=model.config.eos_token_id,\n",
    "        eos_token_id=model.config.eos_token_id,\n",
    "        top_k=100,\n",
    "        top_p=0.6,\n",
    "        early_stopping=True,  # Stops generation when EOS token is produced\n",
    "        no_repeat_ngram_size=2\n",
    "    )\n",
    "    # Decoding the generated text\n",
    "    generated_text = tokenizer.decode(final_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    #removing question part from generated text\n",
    "    clean_generated_text = generated_text[len(tokenizer.decode(ids[0], skip_special_tokens=True)):].strip()\n",
    "    \n",
    "    # Post-generation cleanup\n",
    "    clean_generated_text = clean_generated_text.replace('\",\"', '')\n",
    "    clean_generated_text = clean_generated_text.replace(',', '')# Removing specific unwanted characters\n",
    "    clean_generated_text = trim_incomplete_sentence(clean_generated_text)  # Trimming incomplete sentences\n",
    "    clean_generated_text = trim_extra_text(clean_generated_text, max_sentences=3)  # Trimming extra text\n",
    "    \n",
    "    #captialize the words at start of sentence\n",
    "    clean_generated_text = capitalize_sentences(clean_generated_text)    \n",
    "    \n",
    "    return clean_generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q :  i feel like i'm not good enough at my job and it's causing me a lot of stress. what can i do to feel more confident?\n",
      "\n",
      "A :  it's common to feel insecure about our abilities from time to time. let's work together to identify your strengths and accomplishments, and develop a plan for building your confidence. we can also explore any underlying beliefs or thought patterns that may be contributing to your self-doubt.\n",
      "\n",
      "G :  \"it's common to experience self-doubt at work but it important to remember that these thoughts are not always accurate. Try to focus on your strengths and accomplishments and practice self selfcompassion. You may also benefit from talking to a therapist who can help you work through these feelings and develop strategies for building your confidence.\"\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../data/sample_q_and_a/gpt-model\"\n",
    "sequence = data['Question'].iloc[10]\n",
    "max_len = 100\n",
    "print(\"Q : \",data['Question'].iloc[10])\n",
    "print()\n",
    "print(\"A : \",data['Response'].iloc[10])\n",
    "print()\n",
    "print(\"G : \",generate_text(model_path, sequence, max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sequence = input(\"Enter the question : \")\n",
    "max_len = 500\n",
    "print()\n",
    "print(\"G : \",generate_text(model_path, sequence, max_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STREAMLIT-APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define keywords for detecting crisis situations\n",
    "CRISIS_KEYWORDS = ['suicide', 'selfharm', 'self-harm', 'assault', 'suicidal']\n",
    "\n",
    "# Define a function to check for crisis keywords and return appropriate responses\n",
    "def check_for_crisis(text):\n",
    "    if any(keyword in text.lower() for keyword in CRISIS_KEYWORDS):\n",
    "        return (\"If you're in crisis or think you may have an emergency, call your doctor or 911 immediately. \"\n",
    "                \"If you're having suicidal thoughts, call 1-800-273-TALK (1-800-273-8255) in the U.S. \"\n",
    "                \"to talk to a skilled, trained counselor at a crisis center in your area at any time (National Suicide Prevention Lifeline). \"\n",
    "                \"If you are located outside the United States, call your local emergency line immediately.\")\n",
    "    return None\n",
    "\n",
    "st.title('Mental Health Support Chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greeting message at the start of the chat\n",
    "if 'start_message' not in st.session_state:\n",
    "    st.session_state['start_message'] = \"Hi! How are you feeling today?\"\n",
    "\n",
    "# Goodbye message at the end of the chat\n",
    "if 'end_message' not in st.session_state:\n",
    "    st.session_state['end_message'] = \"It was nice talking to you. Have a good day!\"\n",
    "\n",
    "# Display the start message once at the beginning\n",
    "if 'start_message' in st.session_state and st.session_state['start_message']:\n",
    "    st.write(st.session_state['start_message'])\n",
    "    st.session_state['start_message'] = None  # Clear the message so it doesn't show again\n",
    "\n",
    "# Text input for user's message\n",
    "user_input = st.text_input(\"Enter your message or type 'bye' to end the chat:\")\n",
    "\n",
    "# Main chat function\n",
    "if st.button('Send'):\n",
    "    # End the chat if the user says goodbye\n",
    "    if user_input.lower() in [\"bye\", \"goodbye\", \"quit\"]:\n",
    "        st.write(st.session_state['end_message'])\n",
    "        st.stop()\n",
    "\n",
    "    # Check for crisis situations and respond accordingly\n",
    "    crisis_response = check_for_crisis(user_input)\n",
    "    if crisis_response:\n",
    "        st.write(crisis_response)\n",
    "        st.stop()\n",
    "\n",
    "    # Otherwise, generate a response using the model\n",
    "    else:\n",
    "        # Set the model path (adjust as necessary)\n",
    "        model_path = \"../data/sample_q_and_a/gpt-model\"\n",
    "        generated_response = generate_text(model_path, user_input, max_len=100)\n",
    "        st.text_area(\"AI Response:\", value=generated_response, height=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
