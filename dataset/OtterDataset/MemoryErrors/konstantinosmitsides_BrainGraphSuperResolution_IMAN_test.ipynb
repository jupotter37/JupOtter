{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Sequential, Linear, ReLU, Sigmoid, Tanh, Dropout, Upsample\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import NNConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import BatchNorm\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch.autograd import Variable\n",
    "import networkx as nx\n",
    "\n",
    "import os.path as osp\n",
    "import pickle\n",
    "from scipy.linalg import sqrtm\n",
    "import argparse\n",
    "from scipy.stats import wasserstein_distance\n",
    "from torch.distributions import normal, kl\n",
    "\n",
    "\n",
    "import argparse\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, GAE, VGAE, InnerProductDecoder, ARGVA\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preparation import load_data_tensor\n",
    "\n",
    "lr_train, lr_test, hr_train = load_data_tensor(\"dgl-icl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of subjects in simulated data \n",
    "N_SUBJECTS = 167\n",
    "\n",
    "# Number of ROIs in source brain graph for simulated data \n",
    "N_SOURCE_NODES = 160\n",
    "\n",
    "# Number of ROIs in target brain graph for simulated data\n",
    "N_TARGET_NODES = 268\n",
    "\n",
    "# Number of traning epochs\n",
    "N_EPOCHS = 10\n",
    "\n",
    "\n",
    "####** DO NOT MODIFY BELOW **####\n",
    "N_SOURCE_NODES_F =int((N_SOURCE_NODES*(N_SOURCE_NODES-1))/2)\n",
    "N_TARGET_NODES_F =int((N_TARGET_NODES*(N_TARGET_NODES-1))/2)\n",
    "###**************************####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aligner(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Aligner, self).__init__()\n",
    "\n",
    "        nn = Sequential(Linear(1, N_SOURCE_NODES*N_SOURCE_NODES), ReLU())\n",
    "        self.conv1 = NNConv(N_SOURCE_NODES, N_SOURCE_NODES, nn, aggr='mean', root_weight=True, bias=True)\n",
    "        self.conv11 = BatchNorm(N_SOURCE_NODES, eps=1e-03, momentum=0.1, affine=True, track_running_stats=True)\n",
    "\n",
    "        nn = Sequential(Linear(1, N_SOURCE_NODES), ReLU())\n",
    "        self.conv2 = NNConv(N_SOURCE_NODES, 1, nn, aggr='mean', root_weight=True, bias=True)\n",
    "        self.conv22 = BatchNorm(1, eps=1e-03, momentum=0.1, affine=True, track_running_stats=True)\n",
    "\n",
    "        nn = Sequential(Linear(1, N_SOURCE_NODES), ReLU())\n",
    "        self.conv3 = NNConv(1, N_SOURCE_NODES, nn, aggr='mean', root_weight=True, bias=True)\n",
    "        self.conv33 = BatchNorm(N_SOURCE_NODES, eps=1e-03, momentum=0.1, affine=True, track_running_stats=True)\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.pos_edge_index, data.edge_attr\n",
    "\n",
    "        x1 = F.sigmoid(self.conv11(self.conv1(x, edge_index, edge_attr)))\n",
    "        x1 = F.dropout(x1, training=self.training)\n",
    "\n",
    "        x2 = F.sigmoid(self.conv22(self.conv2(x1, edge_index, edge_attr)))\n",
    "\n",
    "        x2 = F.dropout(x2, training=self.training)\n",
    "\n",
    "        x3 = torch.cat([F.sigmoid(self.conv33(self.conv3(x2, edge_index, edge_attr))), x1], dim=1)\n",
    "        # x3 = torch.cat([F.sigmoid(self.conv3(x2, edge_index, edge_attr)), x1], dim=1)\n",
    "        x4 = x3[:, 0:N_SOURCE_NODES]\n",
    "        x5 = x3[:, N_SOURCE_NODES:2*N_SOURCE_NODES]\n",
    "\n",
    "        x6 = (x4 + x5) / 2\n",
    "        return x6\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        nn = Sequential(Linear(1, N_SOURCE_NODES*N_SOURCE_NODES),ReLU())\n",
    "        self.conv1 = NNConv(N_SOURCE_NODES, N_SOURCE_NODES, nn, aggr='mean', root_weight=True, bias=True)\n",
    "        self.conv11 = BatchNorm(N_SOURCE_NODES, eps=1e-03, momentum=0.1, affine=True, track_running_stats=True)\n",
    "\n",
    "        nn = Sequential(Linear(1, N_TARGET_NODES*N_SOURCE_NODES), ReLU())\n",
    "        self.conv2 = NNConv(N_TARGET_NODES, N_SOURCE_NODES, nn, aggr='mean', root_weight=True, bias=True)\n",
    "        self.conv22 = BatchNorm(N_SOURCE_NODES, eps=1e-03, momentum=0.1, affine=True, track_running_stats=True)\n",
    "\n",
    "        nn = Sequential(Linear(1, N_TARGET_NODES*N_SOURCE_NODES), ReLU())\n",
    "        self.conv3 = NNConv(N_SOURCE_NODES, N_TARGET_NODES, nn, aggr='mean', root_weight=True, bias=True)\n",
    "        self.conv33 = BatchNorm(N_TARGET_NODES, eps=1e-03, momentum=0.1, affine=True, track_running_stats=True)\n",
    "\n",
    "\n",
    "        # self.layer= torch.nn.ConvTranspose2d(N_TARGET_NODES, N_TARGET_NODES,5)\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.pos_edge_index, data.edge_attr\n",
    "        # x = torch.squeeze(x)\n",
    "\n",
    "        x1 = F.sigmoid(self.conv11(self.conv1(x, edge_index, edge_attr)))\n",
    "        x1 = F.dropout(x1, training=self.training)\n",
    "\n",
    "        # x2 = F.sigmoid(self.conv22(self.conv2(x1, edge_index, edge_attr)))\n",
    "        # x2 = F.dropout(x2, training=self.training)\n",
    "\n",
    "        x3 = F.sigmoid(self.conv33(self.conv3(x1, edge_index, edge_attr)))\n",
    "        x3 = F.dropout(x3, training=self.training)\n",
    "\n",
    "\n",
    "\n",
    "        x4  = torch.matmul(x3.t(), x3)\n",
    "\n",
    "        return x4\n",
    "\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = GCNConv(N_TARGET_NODES, N_TARGET_NODES, cached=True)\n",
    "        self.conv2 = GCNConv(N_TARGET_NODES, 1, cached=True)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.pos_edge_index, data.edge_attr\n",
    "        x = torch.squeeze(x)\n",
    "        x1 = F.sigmoid(self.conv1(x, edge_index))\n",
    "        x1 = F.dropout(x1, training=self.training)\n",
    "        x2 = F.sigmoid(self.conv2(x1, edge_index))\n",
    "        #         # x2 = F.dropout(x2, training=self.training)\n",
    "\n",
    "\n",
    "        return x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put it back into a 2D symmetric array\n",
    "\n",
    "\n",
    "def topological_measures(data):\n",
    "    # ROI is the number of brain regions (i.e.,35 in our case)\n",
    "    ROI = 160\n",
    "\n",
    "    topology = []\n",
    "\n",
    "\n",
    "\n",
    "    # A = to_2d(data)\n",
    "    np.fill_diagonal(data, 0)\n",
    "\n",
    "    # create a graph from similarity matrix\n",
    "    G = nx.from_numpy_matrix(np.absolute(data))\n",
    "    U = G.to_undirected()\n",
    "\n",
    "    # Centrality #\n",
    "\n",
    "    # compute closeness centrality and transform the output to vector\n",
    "    cc = nx.closeness_centrality(U, distance=\"weight\")\n",
    "    closeness_centrality = np.array([cc[g] for g in U])\n",
    "    # compute betweeness centrality and transform the output to vector\n",
    "    # bc = nx.betweenness_centrality(U, weight='weight')\n",
    "    # bc = (nx.betweenness_centrality(U))\n",
    "    betweenness_centrality = np.array([cc[g] for g in U])\n",
    "    # # compute egeinvector centrality and transform the output to vector\n",
    "    ec = nx.eigenvector_centrality_numpy(U)\n",
    "    eigenvector_centrality = np.array([ec[g] for g in U])\n",
    "\n",
    "\n",
    "    topology.append(closeness_centrality)  # 0\n",
    "    topology.append(betweenness_centrality)  # 1\n",
    "    topology.append(eigenvector_centrality)  # 2\n",
    "\n",
    "    return topology\n",
    "# put it back into a 2D symmetric array\n",
    "\n",
    "def eigen_centrality(data):\n",
    "    # ROI is the number of brain regions (i.e.,35 in our case)\n",
    "    ROI = 268\n",
    "\n",
    "    topology_eigen = []\n",
    "\n",
    "\n",
    "    data = data.squeeze()\n",
    "    # A = to_2d(data)\n",
    "    np.fill_diagonal(data, 0)\n",
    "\n",
    "    # create a graph from similarity matrix\n",
    "    G = nx.from_numpy_array(np.absolute(data))\n",
    "    U = G.to_undirected()\n",
    "\n",
    "    # Centrality #\n",
    "\n",
    "\n",
    "    # # compute egeinvector centrality and transform the output to vector\n",
    "    ec = nx.eigenvector_centrality_numpy(U)\n",
    "    eigenvector_centrality = np.array([ec[g] for g in U])\n",
    "\n",
    "\n",
    "\n",
    "    topology_eigen.append(eigenvector_centrality)  # 2\n",
    "\n",
    "    return topology_eigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"running on GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"running on CPU\")\n",
    "\n",
    "l1_loss = torch.nn.L1Loss()\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "adversarial_loss.to(device)\n",
    "l1_loss.to(device)\n",
    "\n",
    "\n",
    "def pearson_coor(input, target):\n",
    "    vx = input - torch.mean(input)\n",
    "    vy = target - torch.mean(target)\n",
    "    cost = torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)))\n",
    "    return cost\n",
    "\n",
    "\n",
    "def GT_loss(target, predicted):\n",
    "\n",
    "    # l1_loss\n",
    "    loss_pix2pix = l1_loss(target, predicted)\n",
    "\n",
    "    # topological_loss\n",
    "    target_n = target.detach().cpu().clone().numpy()\n",
    "    predicted_n = predicted.detach().cpu().clone().numpy()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    target_t = eigen_centrality(target_n)\n",
    "    real_topology = torch.tensor(target_t)\n",
    "    predicted_t = eigen_centrality(predicted_n)\n",
    "    fake_topology = torch.tensor(predicted_t)\n",
    "    topo_loss = l1_loss(fake_topology, real_topology)\n",
    "\n",
    "    pc_loss = pearson_coor(target, predicted).to(device)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    G_loss = loss_pix2pix + (1 - pc_loss) + topo_loss\n",
    "\n",
    "    return G_loss\n",
    "\n",
    "\n",
    "def Alignment_loss(target, predicted):\n",
    "    # l_loss1 = torch.abs(nn.KLDivLoss()(F.softmax(zt1), F.softmax(z_s1.t())))\n",
    "\n",
    "    kl_loss = torch.abs(F.kl_div(F.softmax(target), F.softmax(predicted), None, None, 'sum'))\n",
    "    kl_loss = (1/350) * kl_loss\n",
    "    return kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "def convert_vector_to_graph_FC(data):\n",
    "    \"\"\"\n",
    "        convert subject vector to adjacency matrix then use it to create a graph\n",
    "        edge_index:\n",
    "        edge_attr:\n",
    "        x:\n",
    "    \"\"\"\n",
    "\n",
    "    data.reshape(1, N_TARGET_NODES_F)\n",
    "    # create adjacency matrix\n",
    "    tri = np.zeros((N_TARGET_NODES, N_TARGET_NODES))\n",
    "    tri[np.triu_indices(N_TARGET_NODES, 1)] = data\n",
    "    tri = tri + tri.T\n",
    "    tri[np.diag_indices(N_TARGET_NODES)] = 1\n",
    "\n",
    "    edge_attr = torch.Tensor(tri).view(N_TARGET_NODES**2, 1)\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    counter = 0\n",
    "    pos_counter = 0\n",
    "    neg_counter = 0\n",
    "    N_ROI = N_TARGET_NODES\n",
    "\n",
    "    pos_edge_index = torch.zeros(2, N_ROI * N_ROI)\n",
    "    neg_edge_indexe = []\n",
    "    # pos_edge_indexe = []\n",
    "    for i in range(N_ROI):\n",
    "        for j in range(N_ROI):\n",
    "            pos_edge_index[:, counter] = torch.tensor([i, j])\n",
    "            counter += 1\n",
    "\n",
    "        # xx = torch.ones(160, 160, dtype=torch.float)\n",
    "\n",
    "        x = torch.tensor(tri, dtype=torch.float)\n",
    "        pos_edge_index = torch.tensor(pos_edge_index, dtype=torch.long)\n",
    "\n",
    "\n",
    "    return Data(x=x, pos_edge_index=pos_edge_index, edge_attr=edge_attr)\n",
    "\n",
    "def convert_vector_to_graph_RH(data):\n",
    "    \"\"\"\n",
    "        convert subject vector to adjacency matrix then use it to create a graph\n",
    "        edge_index:\n",
    "        edge_attr:\n",
    "        x:\n",
    "    \"\"\"\n",
    "\n",
    "    data.reshape(1, N_SOURCE_NODES_F)\n",
    "    # create adjacency matrix\n",
    "    tri = np.zeros((N_SOURCE_NODES, N_SOURCE_NODES))\n",
    "    tri[np.triu_indices(N_SOURCE_NODES, 1)] = data\n",
    "    tri = tri + tri.T\n",
    "    tri[np.diag_indices(N_SOURCE_NODES)] = 1\n",
    "\n",
    "    edge_attr = torch.Tensor(tri).view(N_SOURCE_NODES**2, 1)\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    counter = 0\n",
    "    pos_counter = 0\n",
    "    neg_counter = 0\n",
    "    N_ROI = N_SOURCE_NODES\n",
    "\n",
    "    pos_edge_index = torch.zeros(2, N_ROI * N_ROI)\n",
    "    neg_edge_indexe = []\n",
    "    # pos_edge_indexe = []\n",
    "    for i in range(N_ROI):\n",
    "        for j in range(N_ROI):\n",
    "            pos_edge_index[:, counter] = torch.tensor([i, j])\n",
    "            counter += 1\n",
    "\n",
    "        # xx = torch.ones(160, 160, dtype=torch.float)\n",
    "\n",
    "        x = torch.tensor(tri, dtype=torch.float)\n",
    "        pos_edge_index = torch.tensor(pos_edge_index, dtype=torch.long)\n",
    "\n",
    "        return Data(x=x, pos_edge_index=pos_edge_index, edge_attr=edge_attr)\n",
    "\n",
    "\n",
    "def cast_data_vector_RH(dataset):\n",
    "    \"\"\"\n",
    "        convert subject vectors to graph and append it in a list\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_g = []\n",
    "\n",
    "    for subj in range(dataset.shape[0]):\n",
    "        dataset_g.append(convert_vector_to_graph_RH(dataset[subj]))\n",
    "\n",
    "    return dataset_g\n",
    "\n",
    "def cast_data_vector_FC(dataset):\n",
    "    \"\"\"\n",
    "        convert subject vectors to graph and append it in a list\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_g = []\n",
    "\n",
    "    for subj in range(dataset.shape[0]):\n",
    "        dataset_g.append(convert_vector_to_graph_FC(dataset[subj]))\n",
    "\n",
    "def convert_vector_to_graph_RH_2(data):\n",
    "    \"\"\"\n",
    "        convert subject vector to adjacency matrix then use it to create a graph\n",
    "        edge_index:\n",
    "        edge_attr:\n",
    "        x:\n",
    "    \"\"\"\n",
    "    tri = data\n",
    "\n",
    "    edge_attr = torch.Tensor(tri).view(N_SOURCE_NODES**2, 1)\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    counter = 0\n",
    "    pos_counter = 0\n",
    "    neg_counter = 0\n",
    "    N_ROI = N_SOURCE_NODES\n",
    "\n",
    "    pos_edge_index = torch.zeros(2, N_ROI * N_ROI)\n",
    "    neg_edge_indexe = []\n",
    "    # pos_edge_indexe = []\n",
    "    for i in range(N_ROI):\n",
    "        for j in range(N_ROI):\n",
    "            pos_edge_index[:, counter] = torch.tensor([i, j])\n",
    "            counter += 1\n",
    "\n",
    "        # xx = torch.ones(160, 160, dtype=torch.float)\n",
    "\n",
    "        x = torch.tensor(tri, dtype=torch.float)\n",
    "        pos_edge_index = torch.tensor(pos_edge_index, dtype=torch.long)\n",
    "\n",
    "        return Data(x=x, pos_edge_index=pos_edge_index, edge_attr=edge_attr)\n",
    "\n",
    "\n",
    "#Copied the above functions and adapted the beginning by deleting the part where they\n",
    "#transformed the vector to a matrix\n",
    "def convert_vector_to_graph_FC_2(data):\n",
    "    \"\"\"\n",
    "        convert subject vector to adjacency matrix then use it to create a graph\n",
    "        edge_index:\n",
    "        edge_attr:\n",
    "        x:\n",
    "    \"\"\"\n",
    "    tri = data\n",
    "    edge_attr = torch.Tensor(tri).view(N_TARGET_NODES**2, 1)\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    counter = 0\n",
    "    pos_counter = 0\n",
    "    neg_counter = 0\n",
    "    N_ROI = N_TARGET_NODES\n",
    "\n",
    "    pos_edge_index = torch.zeros(2, N_ROI * N_ROI)\n",
    "    neg_edge_indexe = []\n",
    "    # pos_edge_indexe = []\n",
    "    for i in range(N_ROI):\n",
    "        for j in range(N_ROI):\n",
    "            pos_edge_index[:, counter] = torch.tensor([i, j])\n",
    "            counter += 1\n",
    "\n",
    "        # xx = torch.ones(160, 160, dtype=torch.float)\n",
    "\n",
    "        x = torch.tensor(tri, dtype=torch.float)\n",
    "        pos_edge_index = torch.tensor(pos_edge_index, dtype=torch.long)\n",
    "\n",
    "\n",
    "    return Data(x=x, pos_edge_index=pos_edge_index, edge_attr=edge_attr)\n",
    "\n",
    "\n",
    "def cast_data_vector_RH_2(dataset):\n",
    "    \"\"\"\n",
    "        convert subject vectors to graph and append it in a list\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_g = []\n",
    "\n",
    "    for subj in range(dataset.shape[0]):\n",
    "        dataset_g.append(convert_vector_to_graph_RH_2(dataset[subj]))\n",
    "\n",
    "    return dataset_g\n",
    "\n",
    "def cast_data_vector_FC_2(dataset):\n",
    "    \"\"\"\n",
    "        convert subject vectors to graph and append it in a list\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_g = []\n",
    "\n",
    "    for subj in range(dataset.shape[0]):\n",
    "        dataset_g.append(convert_vector_to_graph_FC_2(dataset[subj]))\n",
    "    return dataset_g\n",
    "\n",
    "\n",
    "\n",
    "def convert_generated_to_graph(data):\n",
    "    \"\"\"\n",
    "        convert generated output from G to a graph\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "# for data in data1:\n",
    "    counter = 0\n",
    "    N_ROI = N_TARGET_NODES\n",
    "    pos_edge_index = torch.zeros(2, N_ROI * N_ROI, dtype=torch.long)\n",
    "    for i in range(N_ROI):\n",
    "        for j in range(N_ROI):\n",
    "            pos_edge_index[:, counter] = torch.tensor([i, j])\n",
    "            counter += 1\n",
    "\n",
    "    x = data\n",
    "    pos_edge_index = torch.tensor(pos_edge_index, dtype=torch.long)\n",
    "    data = Data(x=x, pos_edge_index= pos_edge_index, edge_attr=data.view(N_TARGET_NODES**2, 1))\n",
    "    dataset.append(data)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def convert_generated_to_graph_Al(data1):\n",
    "    \"\"\"\n",
    "        convert generated output from G to a graph\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    # for data in data1:\n",
    "    counter = 0\n",
    "    N_ROI = N_SOURCE_NODES\n",
    "    pos_edge_index = torch.zeros(2, N_ROI * N_ROI, dtype=torch.long)\n",
    "    for i in range(N_ROI):\n",
    "        for j in range(N_ROI):\n",
    "            pos_edge_index[:, counter] = torch.tensor([i, j])\n",
    "            counter += 1\n",
    "\n",
    "    # x = data\n",
    "    pos_edge_index = torch.tensor(pos_edge_index, dtype=torch.long)\n",
    "    data = Data(x=data1, pos_edge_index=pos_edge_index, edge_attr=data1.view(N_SOURCE_NODES*N_SOURCE_NODES, 1))\n",
    "    dataset.append(data)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "import torch_geometric\n",
    "def convert_func_data_to_graph_list(data, num_nodes=160, h=True):\n",
    "    batch_size = data.shape[0]\n",
    "    dataset = []# Create an array to store the adjacency matrices\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        x = data[i]\n",
    "        dense_adj = torch.ones((num_nodes, num_nodes))\n",
    "\n",
    "        # Convert the dense adjacency matrix to a sparse edge index\n",
    "        edge_index, _ = torch_geometric.utils.dense_to_sparse(dense_adj)\n",
    "\n",
    "        edge_attribute = x.view(num_nodes*num_nodes, 1)\n",
    "\n",
    "        d = Data(x=x, pos_edge_index=edge_index, edge_attr=edge_attribute, edge_index=edge_index)\n",
    "\n",
    "        dataset.append(d)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def anti_vectorize_tensor(arr_2d, num_nodes=160, h=True):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    batch_size = arr_2d.shape[0]  # Extract the batch size\n",
    "    adj_matrices = torch.zeros((batch_size, num_nodes, num_nodes)) # Create an array to store the adjacency matrices\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        arr_1d = arr_2d[i].to(device)  # Extract the 1D array for this batch element\n",
    "        adj_matrix = torch.zeros((num_nodes, num_nodes)).to(device)  # Create a matrix to store the adjacency matrix\n",
    "\n",
    "        if h:\n",
    "            cv, cb = h_i(num_nodes)\n",
    "            adj_matrix[cv, cb] = arr_1d\n",
    "\n",
    "        else:\n",
    "            adj_matrix[np.triu_indices(n=num_nodes, k=1)] = arr_1d  # Assign the values to the upper triangular part\n",
    "        adj_matrix_t = torch.transpose(adj_matrix,0,1)\n",
    "        adj_matrix = (adj_matrix_t + adj_matrix)/2 # Add the transpose to make the matrix symmetric\n",
    "        adj_matrix.fill_diagonal_(1)  # Set diagonal values to 1\n",
    "\n",
    "        adj_matrices[i] = adj_matrix  # Add the adjacency matrix to the output array\n",
    "\n",
    "    return adj_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.data import Batch,Data\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "class ListDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        # Do any preprocessing you need on the sample here\n",
    "        return sample\n",
    "\n",
    "def collate_fn(batch):\n",
    "    a = [x[0] for x in batch]\n",
    "    b = [x[1] for x in batch]\n",
    "    return [a,b]\n",
    "\n",
    "def freeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def unfreeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47334162432, 47622258688)\n"
     ]
    }
   ],
   "source": [
    "aligner = Aligner()\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "# Losses\n",
    "adversarial_loss1 = torch.nn.BCELoss()\n",
    "l1_loss = torch.nn.L1Loss()\n",
    "\n",
    "# send 1st GAN to GPU\n",
    "aligner.to(device)\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "adversarial_loss1.to(device)\n",
    "l1_loss.to(device)\n",
    "\n",
    "print(torch.cuda.mem_get_info())\n",
    "\n",
    "Aligner_optimizer = torch.optim.AdamW(aligner.parameters(), lr=0.025, betas=(0.5, 0.999))\n",
    "generator_optimizer = torch.optim.AdamW(generator.parameters(), lr=0.025, betas=(0.5, 0.999))\n",
    "discriminator_optimizer = torch.optim.AdamW(discriminator.parameters(), lr=0.025, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "Batch_Size = 1\n",
    "\n",
    "X_casted_train_source = convert_func_data_to_graph_list(lr_train, N_SOURCE_NODES, h=False)\n",
    "#X_casted_test_source = convert_func_data_to_graph_list(X_test_source, N_SOURCE_NODES, h=False)\n",
    "X_casted_train_target = convert_func_data_to_graph_list(hr_train, N_TARGET_NODES, h=True)\n",
    "#X_casted_test_target = convert_func_data_to_graph_list(X_test_target, N_TARGET_NODES, h=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data_Loader: 0it [00:00, ?it/s]00:00<?, ?it/s]\n",
      "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.09 GiB. GPU 0 has a total capacity of 44.35 GiB of which 3.07 GiB is free. Including non-PyTorch memory, this process has 41.27 GiB memory in use. Of the allocated memory 29.73 GiB is allocated by PyTorch, and 11.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 72\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m############################ Train Discriminator #################################################\u001b[39;00m\n\u001b[1;32m     69\u001b[0m targett \u001b[38;5;241m=\u001b[39m data_batch_target\u001b[38;5;241m.\u001b[39medge_attr\u001b[38;5;241m.\u001b[39mview(cur_batch_size, N_TARGET_NODES, N_TARGET_NODES)\n\u001b[0;32m---> 72\u001b[0m G_output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_batch_source\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m new_data \u001b[38;5;241m=\u001b[39m Data(x\u001b[38;5;241m=\u001b[39mG_output\u001b[38;5;241m.\u001b[39mview(cur_batch_size \u001b[38;5;241m*\u001b[39m N_TARGET_NODES, N_TARGET_NODES),\n\u001b[1;32m     74\u001b[0m                 edge_index\u001b[38;5;241m=\u001b[39mdata_batch_target\u001b[38;5;241m.\u001b[39medge_index,\n\u001b[1;32m     75\u001b[0m                 edge_attr\u001b[38;5;241m=\u001b[39mG_output\u001b[38;5;241m.\u001b[39mview(cur_batch_size \u001b[38;5;241m*\u001b[39m N_TARGET_NODES \u001b[38;5;241m*\u001b[39m N_TARGET_NODES, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     76\u001b[0m                 pos_edge_index\u001b[38;5;241m=\u001b[39mdata_batch_target\u001b[38;5;241m.\u001b[39medge_index)\n\u001b[1;32m     78\u001b[0m freeze_model(generator)\n",
      "File \u001b[0;32m/vol/bitbucket/se223/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/se223/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 74\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     69\u001b[0m x1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x1, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# x2 = F.sigmoid(self.conv22(self.conv2(x1, edge_index, edge_attr)))\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# x2 = F.dropout(x2, training=self.training)\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m x3 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv33(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     75\u001b[0m x3 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x3, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m     79\u001b[0m x4  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(x3\u001b[38;5;241m.\u001b[39mt(), x3)\n",
      "File \u001b[0;32m/vol/bitbucket/se223/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/se223/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/vol/bitbucket/se223/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch_geometric/nn/conv/nn_conv.py:108\u001b[0m, in \u001b[0;36mNNConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, size)\u001b[0m\n\u001b[1;32m    105\u001b[0m     x \u001b[38;5;241m=\u001b[39m (x, x)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m x_r \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_weight:\n",
      "File \u001b[0;32m~/.cache/pyg/message_passing/torch_geometric.nn.conv.nn_conv_NNConv_propagate.py:167\u001b[0m, in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, x, edge_attr, size)\u001b[0m\n\u001b[1;32m    158\u001b[0m             kwargs \u001b[38;5;241m=\u001b[39m CollectArgs(\n\u001b[1;32m    159\u001b[0m                 x_j\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_j\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    160\u001b[0m                 edge_attr\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_attr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 dim_size\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mdim_size,\n\u001b[1;32m    164\u001b[0m             )\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# End Message Forward Pre Hook #########################################\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_j\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_j\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# Begin Message Forward Hook ###########################################\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[0;32m/vol/bitbucket/se223/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch_geometric/nn/conv/nn_conv.py:120\u001b[0m, in \u001b[0;36mNNConv.message\u001b[0;34m(self, x_j, edge_attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmessage\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_j: Tensor, edge_attr: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 120\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels_l, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels)\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmatmul(x_j\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), weight)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/vol/bitbucket/se223/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/se223/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/vol/bitbucket/se223/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/vol/bitbucket/se223/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/se223/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/vol/bitbucket/se223/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/activation.py:101\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/se223/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/functional.py:1473\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1471\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1473\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.09 GiB. GPU 0 has a total capacity of 44.35 GiB of which 3.07 GiB is free. Including non-PyTorch memory, this process has 41.27 GiB memory in use. Of the allocated memory 29.73 GiB is allocated by PyTorch, and 11.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "aligner.train()\n",
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "nbre_epochs = N_EPOCHS\n",
    "\n",
    "\n",
    "\n",
    "dataset = ListDataset(list(zip(X_casted_train_source, X_casted_train_target)))\n",
    "#test_dataset = ListDataset(list(zip(X_casted_test_source, X_casted_test_target)))\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=Batch_Size, collate_fn=collate_fn, drop_last=False)\n",
    "#test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=Batch_Size, collate_fn=collate_fn, drop_last=False)\n",
    "\n",
    "\n",
    "cv = KFold(n_splits=3, random_state=42, shuffle=True)\n",
    "X, Y = lr_train, hr_train\n",
    "\n",
    "for train_index, test_index in cv.split(X):\n",
    "    X_casted_test_source = convert_func_data_to_graph_list(lr_train[test_index], N_SOURCE_NODES, h=False)\n",
    "    X_casted_test_target = convert_func_data_to_graph_list(hr_train[test_index], N_TARGET_NODES, h=True)\n",
    "    X_casted_train_source = convert_func_data_to_graph_list(lr_train[train_index], N_SOURCE_NODES, h=False)\n",
    "    X_casted_train_target = convert_func_data_to_graph_list(hr_train[train_index], N_TARGET_NODES, h=True)\n",
    "    dataset = ListDataset(list(zip(X_casted_train_source, X_casted_train_target)))\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=Batch_Size, collate_fn=collate_fn, drop_last=False)\n",
    "    test_dataset = ListDataset(list(zip(X_casted_test_source, X_casted_test_target)))\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=Batch_Size, collate_fn=collate_fn, drop_last=False)\n",
    "\n",
    "    for epochs in tqdm(range(nbre_epochs), desc=\"Epochs\"):\n",
    "\n",
    "        for i, x in tqdm(enumerate(train_loader), desc=\"Data_Loader\"):\n",
    "\n",
    "            Aligner_optimizer.zero_grad()\n",
    "            generator_optimizer.zero_grad()\n",
    "            discriminator_optimizer.zero_grad()\n",
    "\n",
    "            data_source = x[0]\n",
    "            data_target = x[1]\n",
    "\n",
    "            cur_batch_size = len(data_source)\n",
    "\n",
    "            data_batch_source = Batch.from_data_list(data_source).to(device)\n",
    "            data_batch_target = Batch.from_data_list(data_target).to(device)\n",
    "\n",
    "\n",
    "            A_output = aligner(data_batch_source)\n",
    "\n",
    "            data_batch_source.x = A_output\n",
    "\n",
    "            data_batch_source.edge_attr = A_output.view(cur_batch_size * N_SOURCE_NODES * N_SOURCE_NODES, 1)\n",
    "\n",
    "            targett = data_batch_target.edge_attr.view(cur_batch_size, N_TARGET_NODES, N_TARGET_NODES)\n",
    "\n",
    "            target_mean = torch.mean(targett, dim=(1, 2))\n",
    "            target_std = torch.std(targett, dim=(1, 2))\n",
    "\n",
    "            vectors = torch.empty(cur_batch_size, N_SOURCE_NODES_F)\n",
    "            for j in range(cur_batch_size):\n",
    "                vectors[j] = torch.normal(target_mean[j], target_std[j], size=(N_SOURCE_NODES_F,))\n",
    "\n",
    "            d_target = vectors.to(device)\n",
    "\n",
    "            target_d = anti_vectorize_tensor(d_target, num_nodes=N_SOURCE_NODES, h=False).to(device)\n",
    "\n",
    "            kl_loss = Alignment_loss(target_d, A_output.view(cur_batch_size, N_SOURCE_NODES, N_SOURCE_NODES))\n",
    "\n",
    "\n",
    "            ############################ Train Discriminator #################################################\n",
    "\n",
    "            targett = data_batch_target.edge_attr.view(cur_batch_size, N_TARGET_NODES, N_TARGET_NODES)\n",
    "\n",
    "\n",
    "            G_output = generator(data_batch_source)\n",
    "            new_data = Data(x=G_output.view(cur_batch_size * N_TARGET_NODES, N_TARGET_NODES),\n",
    "                            edge_index=data_batch_target.edge_index,\n",
    "                            edge_attr=G_output.view(cur_batch_size * N_TARGET_NODES * N_TARGET_NODES, 1),\n",
    "                            pos_edge_index=data_batch_target.edge_index)\n",
    "\n",
    "            freeze_model(generator)\n",
    "            freeze_model(aligner)\n",
    "            unfreeze_model(discriminator)\n",
    "\n",
    "            D_real = discriminator(data_batch_target)\n",
    "            D_real_loss = adversarial_loss(D_real, (torch.ones_like(D_real)))\n",
    "            D_fake = discriminator(new_data)\n",
    "            D_fake_loss = adversarial_loss(D_fake, torch.zeros_like(D_fake))\n",
    "            D_loss = (D_real_loss + D_fake_loss) / 2\n",
    "\n",
    "\n",
    "            D_loss.backward(retain_graph=True)\n",
    "            discriminator_optimizer.step()\n",
    "\n",
    "\n",
    "            unfreeze_model(generator)\n",
    "            unfreeze_model(aligner)\n",
    "            freeze_model(discriminator)\n",
    "\n",
    "            Gg_loss = GT_loss(targett, G_output.view(cur_batch_size, N_TARGET_NODES, N_TARGET_NODES))\n",
    "\n",
    "\n",
    "            D_fake = discriminator(new_data)\n",
    "\n",
    "            D_fake = D_fake.view(-1)\n",
    "\n",
    "            G_adversarial = adversarial_loss(D_fake, (torch.ones_like(D_fake)))\n",
    "\n",
    "            G_loss = Gg_loss + G_adversarial + kl_loss\n",
    "\n",
    "\n",
    "            G_loss.backward()\n",
    "\n",
    "            generator_optimizer.step()\n",
    "\n",
    "        print(\"[Epoch: %d]| [Ge loss: %f]| [D loss: %f]\" % (\n",
    "            epochs, G_loss, D_loss))\n",
    "\n",
    "        aligner.eval()\n",
    "        generator.eval()\n",
    "\n",
    "\n",
    "        l1_l = []\n",
    "\n",
    "        gen = []\n",
    "        tar = []\n",
    "        sou = []\n",
    "\n",
    "        for it,x in tqdm(enumerate(test_loader),desc=\"Testing\"):\n",
    "            data_source = x[0]\n",
    "            data_target = x[1]\n",
    "            cur_batch_size = len(data_source)\n",
    "\n",
    "            data_batch_source = Batch.from_data_list(data_source).to(device)\n",
    "\n",
    "            sou.append(data_batch_source.x.view(cur_batch_size, N_SOURCE_NODES, N_SOURCE_NODES).detach().cpu())\n",
    "\n",
    "            data_batch_target = Batch.from_data_list(data_target).to(device)\n",
    "            A_test = aligner(data_batch_source)\n",
    "            data_batch_source.x = A_test\n",
    "            data_batch_source.edge_attr = A_test.view(cur_batch_size * N_SOURCE_NODES * N_SOURCE_NODES, 1)\n",
    "            G_test = generator(data_batch_source)\n",
    "            target = data_batch_target.edge_attr.view(cur_batch_size, N_TARGET_NODES, N_TARGET_NODES).detach().cpu()\n",
    "            G_test = G_test.view(cur_batch_size, N_TARGET_NODES, N_TARGET_NODES).detach().cpu()\n",
    "            G_test = post(G_test)\n",
    "            l1 = F.l1_loss(G_test, target)\n",
    "            l1_l.append(l1)\n",
    "\n",
    "            gen.append(G_test.cpu())\n",
    "\n",
    "            tar.append(target.cpu())\n",
    "\n",
    "\n",
    "            G_test = G_test.numpy()\n",
    "            target = target.numpy()\n",
    "\n",
    "        final_l1 = np.mean(l1_l)\n",
    "\n",
    "        final_gen = gen\n",
    "        final_tar = tar\n",
    "        final_sou = sou\n",
    "\n",
    "        #with open(f\"source_{fold_number + 1}.pkl\", \"wb\") as f:\n",
    "            #pickle.dump(final_sou, f)\n",
    "            #f.close()\n",
    "\n",
    "        #with open(f\"target_{fold_number + 1}.pkl\", \"wb\") as f:\n",
    "            #pickle.dump(final_tar, f)\n",
    "            #f.close()\n",
    "\n",
    "        #with open(f\"gen_{fold_number + 1}.pkl\", \"wb\") as f:\n",
    "            #pickle.dump(final_gen, f)\n",
    "            #f.close()\n",
    "\n",
    "        print(G_test[0])\n",
    "        print(target[0])\n",
    "        print(final_l1)\n",
    "\n",
    "\n",
    "# #     ######################################### TESTING PART #########################################\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post(x):\n",
    "\n",
    "    for i in range(x.shape[1]):\n",
    "        x[:, i, i] = 1\n",
    "\n",
    "    x[x<0] = 0\n",
    "    x[x>1] = 1\n",
    "\n",
    "    x_tr = torch.transpose(x, 1, 2)\n",
    "    x = (x + x_tr)/2\n",
    "\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 112it [00:12,  9.10it/s]\n"
     ]
    }
   ],
   "source": [
    "res = torch.empty((lr_test.shape[0], 268, 268))\n",
    "\n",
    "\n",
    "X_casted_test_source = convert_func_data_to_graph_list(lr_test, N_SOURCE_NODES, h=False)\n",
    "\n",
    "X_casted_test_target = convert_func_data_to_graph_list(lr_test, N_SOURCE_NODES, h=False)\n",
    "\n",
    "\n",
    "test_dataset = ListDataset(list(zip(X_casted_test_source, X_casted_test_target)))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=Batch_Size, collate_fn=collate_fn, drop_last=False)\n",
    "\n",
    "\n",
    "\n",
    "aligner.eval()\n",
    "generator.eval()\n",
    "gen = []\n",
    "\n",
    "for it,x in tqdm(enumerate(test_loader),desc=\"Testing\"):\n",
    "    data_source = x[0]\n",
    "    cur_batch_size = len(data_source)\n",
    "\n",
    "    data_batch_source = Batch.from_data_list(data_source).to(device)\n",
    "\n",
    "   \n",
    "    A_test = aligner(data_batch_source)\n",
    "    data_batch_source.x = A_test\n",
    "    data_batch_source.edge_attr = A_test.view(cur_batch_size * N_SOURCE_NODES * N_SOURCE_NODES, 1)\n",
    "    G_test = generator(data_batch_source)\n",
    "    \n",
    "    G_test = G_test.view(cur_batch_size, N_TARGET_NODES, N_TARGET_NODES).detach().cpu()\n",
    "    G_test = post(G_test)\n",
    "    \n",
    "\n",
    "    res[it] = G_test.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"\"\" restore_aligner = \"./weight\" + \"aligner_fold\" + \"_\" + \".model\"\n",
    "        restore_generator = \"./weight\" + \"generator_fold\" + \"_\" + \".model\"\n",
    "\n",
    "        aligner.load_state_dict(torch.load(restore_aligner))\n",
    "        generator.load_state_dict(torch.load(restore_generator))\n",
    "\n",
    "        aligner.eval()\n",
    "        generator.eval()\n",
    "\n",
    "        i = 0\n",
    "        predicted_test_graphs = []\n",
    "        losses_test = []\n",
    "        eigenvector_losses_test = []\n",
    "        l1_tests = []\n",
    "        Closeness_test = []\n",
    "        Eigenvector_test = []\n",
    "        for data_source, data_target in zip(X_casted_test_source, X_casted_test_target):\n",
    "            # print(i)\n",
    "            data_source_test = data_source.x.view(N_SOURCE_NODES, N_SOURCE_NODES)\n",
    "            data_target_test = data_target.x.view(N_TARGET_NODES, N_TARGET_NODES)\n",
    "\n",
    "\n",
    "            A_test = aligner(data_source)\n",
    "            A_test_casted = convert_generated_to_graph_Al(A_test)\n",
    "            A_test_casted = A_test_casted[0]\n",
    "            data_target = data_target_test.detach().cpu().clone().numpy()\n",
    "            # ************     Super-resolution    ************\n",
    "            G_output_test = generator(A_test_casted)  # 35 x35\n",
    "            G_output_test_casted = convert_generated_to_graph(G_output_test)\n",
    "            G_output_test_casted = G_output_test_casted[0]\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            L1_test = l1_loss(data_target_test, G_output_test)\n",
    "            # fold= 1\n",
    "            target_test = data_target_test.detach().cpu().clone().numpy()\n",
    "            predicted_test = G_output_test.detach().cpu().clone().numpy()\n",
    "            source_test = data_source_test.detach().cpu().clone().numpy()\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            fake_topology_test = torch.tensor(topological_measures(predicted_test))\n",
    "            real_topology_test = torch.tensor(topological_measures(target_test))\n",
    "\n",
    "            eigenvector_test = (l1_loss(fake_topology_test[2], real_topology_test[2]))\n",
    "\n",
    "\n",
    "            l1_tests.append(L1_test.detach().cpu().numpy())\n",
    "            Eigenvector_test.append(eigenvector_test.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "\n",
    "        mean_l1 = np.mean(l1_tests)\n",
    "        mean_eigenvector = np.mean(Eigenvector_test)\n",
    "\n",
    "        # print(\"Mean L1 Loss Test: \", fold_mean_l1_loss)\n",
    "        # print()\n",
    "\n",
    "        losses_test.append(mean_l1)\n",
    "        eigenvector_losses_test.append(mean_eigenvector) \"\"\"\"\"\"\n",
    "\n",
    "# fold += 1\n",
    "#return (aligner, generator, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_preparation import generate_submission_file\n",
    "generate_submission_file(res, 'submission_files/trial_submission1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(aligner.state_dict(), \"./weights/weight\" + \"aligner_fold\" + f\"_{fold_number}\" + \".model\")\n",
    "#torch.save(generator.state_dict(), \"./weights/weight\" + \"generator_fold\" + f\"_{fold_number}\" + \".model\")\n",
    "#torch.save(discriminator.state_dict(), \"./weights/weight\" + \"discriminator_fold\" + f\"_{fold_number}\" + \".model\")\n",
    "\n",
    "####################################################TESTING PART####################################################\n",
    "\n",
    "#restore_aligner = \"./weights/weight\" + \"aligner_fold\" + f\"_{fold_number}\" + \".model\"\n",
    "#restore_generator = \"./weights/weight\" + \"generator_fold\" + f\"_{fold_number}\" + \".model\"\n",
    "#aligner.load_state_dict(torch.load(restore_aligner, map_location=device))\n",
    "#generator.load_state_dict(torch.load(restore_generator, map_location=device))\n",
    "\n",
    "\n",
    "X_casted_test_source = convert_func_data_to_graph_list(lr_test, N_SOURCE_NODES, h=False)\n",
    "\n",
    "X_casted_test_target = convert_func_data_to_graph_list(X_test_target, N_TARGET_NODES, h=True)\n",
    "\n",
    "\n",
    "test_dataset = ListDataset(list(zip(X_casted_test_source, X_casted_test_target)))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=Batch_Size, collate_fn=collate_fn, drop_last=False)\n",
    "\n",
    "\n",
    "aligner.eval()\n",
    "generator.eval()\n",
    "\n",
    "\n",
    "l1_loss = []\n",
    "\n",
    "gen = []\n",
    "tar = []\n",
    "sou = []\n",
    "\n",
    "for it,x in tqdm(enumerate(test_loader),desc=\"Testing\"):\n",
    "    data_source = x[0]\n",
    "    data_target = x[1]\n",
    "    cur_batch_size = len(data_source)\n",
    "\n",
    "    data_batch_source = Batch.from_data_list(data_source).to(device)\n",
    "\n",
    "    sou.append(data_batch_source.x.view(cur_batch_size, N_SOURCE_NODES, N_SOURCE_NODES).detach().cpu())\n",
    "\n",
    "    data_batch_target = Batch.from_data_list(data_target).to(device)\n",
    "    A_test = aligner(data_batch_source)\n",
    "    data_batch_source.x = A_test\n",
    "    data_batch_source.edge_attr = A_test.view(cur_batch_size * N_SOURCE_NODES * N_SOURCE_NODES, 1)\n",
    "    G_test = generator(data_batch_source)\n",
    "    target = data_batch_target.edge_attr.view(cur_batch_size, N_TARGET_NODES, N_TARGET_NODES).detach().cpu()\n",
    "    G_test = G_test.view(cur_batch_size, N_TARGET_NODES, N_TARGET_NODES).detach().cpu()\n",
    "    G_test = post(G_test)\n",
    "    l1 = F.l1_loss(G_test, target)\n",
    "    l1_loss.append(l1)\n",
    "\n",
    "    gen.append(G_test.cpu())\n",
    "\n",
    "    tar.append(target.cpu())\n",
    "\n",
    "\n",
    "    G_test = G_test.numpy()\n",
    "    target = target.numpy()\n",
    "\n",
    "final_l1 = np.mean(l1_loss)\n",
    "\n",
    "final_gen = gen\n",
    "final_tar = tar\n",
    "final_sou = sou\n",
    "\n",
    "with open(f\"source_{fold_number + 1}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(final_sou, f)\n",
    "    f.close()\n",
    "\n",
    "with open(f\"target_{fold_number + 1}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(final_tar, f)\n",
    "    f.close()\n",
    "\n",
    "with open(f\"gen_{fold_number + 1}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(final_gen, f)\n",
    "    f.close()\n",
    "\n",
    "print(G_test[0])\n",
    "print(target[0])\n",
    "print(final_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data_Loader: 0it [00:00, ?it/s]00:00<?, ?it/s]\n",
      "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.44 GiB. GPU 0 has a total capacity of 15.70 GiB of which 1.10 GiB is free. Process 204151 has 5.68 GiB memory in use. Process 210271 has 3.73 GiB memory in use. Process 214347 has 4.05 GiB memory in use. Including non-PyTorch memory, this process has 896.00 MiB memory in use. Of the allocated memory 615.99 MiB is allocated by PyTorch, and 8.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 31\u001b[0m\n\u001b[1;32m     25\u001b[0m eigenvector_losses_test \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#for train_index, test_index in kf.split(source_data):\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# print( * \"#\" + \" FOLD \" + str(fold) + \" \" +  * \"#\")\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m#X_train_source, X_test_source, X_train_target, X_test_target = source_data[train_index], source_data[test_index], target_data[train_index], target_data[test_index]\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m aligner, generator, discriminator \u001b[38;5;241m=\u001b[39m \u001b[43mIMANGraphNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mhr_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m test_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(l1_test)\n\u001b[1;32m     37\u001b[0m Eigenvector_test_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(eigenvector_test)\n",
      "Cell \u001b[0;32mIn[49], line 110\u001b[0m, in \u001b[0;36mIMANGraphNet\u001b[0;34m(X_train_source, X_train_target, fold_number)\u001b[0m\n\u001b[1;32m    106\u001b[0m data_batch_source \u001b[38;5;241m=\u001b[39m Batch\u001b[38;5;241m.\u001b[39mfrom_data_list(data_source)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    107\u001b[0m data_batch_target \u001b[38;5;241m=\u001b[39m Batch\u001b[38;5;241m.\u001b[39mfrom_data_list(data_target)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 110\u001b[0m A_output \u001b[38;5;241m=\u001b[39m \u001b[43maligner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_batch_source\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m data_batch_source\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m A_output\n\u001b[1;32m    114\u001b[0m data_batch_source\u001b[38;5;241m.\u001b[39medge_attr \u001b[38;5;241m=\u001b[39m A_output\u001b[38;5;241m.\u001b[39mview(cur_batch_size \u001b[38;5;241m*\u001b[39m N_SOURCE_NODES \u001b[38;5;241m*\u001b[39m N_SOURCE_NODES, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/vol/bitbucket/se223/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/se223/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[45], line 23\u001b[0m, in \u001b[0;36mAligner.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     20\u001b[0m x, edge_index, edge_attr \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39mpos_edge_index, data\u001b[38;5;241m.\u001b[39medge_attr\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# x1 = F.sigmoid(self.conv11(self.conv1(x, edge_index, edge_attr)))\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m x1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     24\u001b[0m x1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x1, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# x2 = F.sigmoid(self.conv22(self.conv2(x1, edge_index, edge_attr)))\u001b[39;00m\n",
      "File \u001b[0;32m/vol/bitbucket/se223/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/se223/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/vol/bitbucket/se223/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch_geometric/nn/conv/nn_conv.py:108\u001b[0m, in \u001b[0;36mNNConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, size)\u001b[0m\n\u001b[1;32m    105\u001b[0m     x \u001b[38;5;241m=\u001b[39m (x, x)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor, edge_attr: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m x_r \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_weight:\n",
      "File \u001b[0;32m~/.cache/pyg/message_passing/torch_geometric.nn.conv.nn_conv_NNConv_propagate.py:167\u001b[0m, in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, x, edge_attr, size)\u001b[0m\n\u001b[1;32m    158\u001b[0m             kwargs \u001b[38;5;241m=\u001b[39m CollectArgs(\n\u001b[1;32m    159\u001b[0m                 x_j\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_j\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    160\u001b[0m                 edge_attr\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_attr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 dim_size\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mdim_size,\n\u001b[1;32m    164\u001b[0m             )\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# End Message Forward Pre Hook #########################################\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_j\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_j\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# Begin Message Forward Hook ###########################################\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[0;32m/vol/bitbucket/se223/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch_geometric/nn/conv/nn_conv.py:120\u001b[0m, in \u001b[0;36mNNConv.message\u001b[0;34m(self, x_j, edge_attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmessage\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_j: Tensor, edge_attr: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 120\u001b[0m     weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels_l, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels)\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmatmul(x_j\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), weight)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/vol/bitbucket/se223/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/se223/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/vol/bitbucket/se223/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/vol/bitbucket/se223/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/se223/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/vol/bitbucket/se223/DGL24-Group-Project/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.44 GiB. GPU 0 has a total capacity of 15.70 GiB of which 1.10 GiB is free. Process 204151 has 5.68 GiB memory in use. Process 210271 has 3.73 GiB memory in use. Process 214347 has 4.05 GiB memory in use. Including non-PyTorch memory, this process has 896.00 MiB memory in use. Of the allocated memory 615.99 MiB is allocated by PyTorch, and 8.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\"\"\"#Training\"\"\"\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"running on GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"running on CPU\")\n",
    "\n",
    "source_data = np.random.normal(0, 0.5, (N_SUBJECTS, N_SOURCE_NODES_F))\n",
    "target_data = np.random.normal(0, 0.5, (N_SUBJECTS, N_TARGET_NODES_F))\n",
    "\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=1773)\n",
    "\n",
    "fold = 0\n",
    "losses_test = []\n",
    "closeness_losses_test = []\n",
    "# betweenness_losses_test = []\n",
    "eigenvector_losses_test = []\n",
    "\n",
    "#for train_index, test_index in kf.split(source_data):\n",
    "    # print( * \"#\" + \" FOLD \" + str(fold) + \" \" +  * \"#\")\n",
    "    #X_train_source, X_test_source, X_train_target, X_test_target = source_data[train_index], source_data[test_index], target_data[train_index], target_data[test_index]\n",
    "   \n",
    "aligner, generator, discriminator = IMANGraphNet(lr_train,  hr_train, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_mean = np.mean(l1_test)\n",
    "Eigenvector_test_mean = np.mean(eigenvector_test)\n",
    "plot_source(source_test)\n",
    "plot_target(data_target)\n",
    "plot_target(predicted_test)\n",
    "\n",
    "print(\"Mean L1 Test\", test_mean)\n",
    "\n",
    "print(\"Mean Eigenvector Test\", Eigenvector_test_mean)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
