{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "280042f8-5f0d-4b7c-9b1f-925a966df1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/angle/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [06:00<00:00, 120.23s/it]\n",
      "/opt/conda/envs/angle/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/angle/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/angle/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/angle/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 106.94 MiB is free. Process 16124 has 23.58 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 2.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m config \u001b[38;5;241m=\u001b[39m PeftConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(peft_model_id)\n\u001b[1;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(config\u001b[38;5;241m.\u001b[39mbase_model_name_or_path)\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model_name_or_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, peft_model_id)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_text\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/envs/angle/lib/python3.8/site-packages/transformers/modeling_utils.py:2528\u001b[0m, in \u001b[0;36mPreTrainedModel.cuda\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2523\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2524\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling `cuda()` is not supported for `4-bit` or `8-bit` quantized models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2526\u001b[0m     )\n\u001b[1;32m   2527\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/angle/lib/python3.8/site-packages/torch/nn/modules/module.py:911\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    895\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \n\u001b[1;32m    897\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 911\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/angle/lib/python3.8/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/angle/lib/python3.8/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 802 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/angle/lib/python3.8/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/angle/lib/python3.8/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/opt/conda/envs/angle/lib/python3.8/site-packages/torch/nn/modules/module.py:911\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    895\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \n\u001b[1;32m    897\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 911\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 106.94 MiB is free. Process 16124 has 23.58 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 2.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# todo:换个RTX8000（显存48g）的跑\n",
    "from angle_emb import AnglE\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# peft_model_id = 'SeanLee97/angle-llama-7b-nli-v2'\n",
    "peft_model_id = './models/angle-llama-13b-nli'\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path).bfloat16().cuda()\n",
    "model = PeftModel.from_pretrained(model, peft_model_id).cuda()\n",
    "\n",
    "def decorate_text(text: str):\n",
    "    return Prompts.A.format(text=text)\n",
    "\n",
    "inputs = 'hello world!'\n",
    "tok = tokenizer([decorate_text(inputs)], return_tensors='pt')\n",
    "for k, v in tok.items():\n",
    "    tok[k] = v.cuda()\n",
    "vec = model(output_hidden_states=True, **tok).hidden_states[-1][:, -1].float().detach().cpu().numpy()\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ac58e61-b652-4f05-a5c9-e65014d0c44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:AnglE:LLM detected, automatically set is_llm=True.If it is wrong, you can manually set `is_llm`.\n",
      "INFO:AnglE:LLM detected, automatically set apply_lora=True.If it is wrong, you can manually set `apply_lora`.\n",
      "INFO:AnglE:LLM detected, automatically set prompt. You can change this setting by manually configuring the `set_prompt()` function.\n",
      "INFO:AnglE:Prompt is set, the prompt will be automatically applied during the encoding phase. To disable prompt setting, please configure set_prompt(prompt=None)\n",
      "INFO:AnglE:LLaMA detected, automatically set `apply_bfloat16=True`. You can change this setting by manually configuring the `apply_bfloat16`.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:49<00:00, 16.43s/it]\n",
      "/opt/conda/envs/angle/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/angle/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/angle/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/angle/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "INFO:AnglE:Load lora weight from ./models/angle-llama-13b-nli\n",
      "INFO:AnglE:Prompt is set, the prompt will be automatically applied during the encoding phase. To disable prompt setting, please configure set_prompt(prompt=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts.A = 'Summarize sentence \"{text}\" in one word:\"'\n",
      "Prompts.B = 'You can only output one word. Summarize \"{text}\":\"'\n",
      "Prompts.C = 'Represent this sentence for searching relevant passages: {text}'\n",
      "All predefined prompts: None\n",
      "prompt: Summarize sentence \"{text}\" in one word:\"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m angle\u001b[38;5;241m.\u001b[39mset_prompt(prompt\u001b[38;5;241m=\u001b[39mPrompts\u001b[38;5;241m.\u001b[39mA)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt:\u001b[39m\u001b[38;5;124m'\u001b[39m, angle\u001b[38;5;241m.\u001b[39mprompt)\n\u001b[0;32m---> 10\u001b[0m vec \u001b[38;5;241m=\u001b[39m \u001b[43mangle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhello world\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(vec)\n\u001b[1;32m     12\u001b[0m vecs \u001b[38;5;241m=\u001b[39m angle\u001b[38;5;241m.\u001b[39mencode([{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhello world1\u001b[39m\u001b[38;5;124m'\u001b[39m}, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhello world2\u001b[39m\u001b[38;5;124m'\u001b[39m}], to_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/angle/lib/python3.8/site-packages/angle_emb/angle.py:1508\u001b[0m, in \u001b[0;36mAnglE.encode\u001b[0;34m(self, inputs, max_length, end_with_eos, to_numpy, layer_index, embedding_size, device)\u001b[0m\n\u001b[1;32m   1506\u001b[0m tok\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m-> 1508\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpooler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m to_numpy:\n\u001b[1;32m   1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/opt/conda/envs/angle/lib/python3.8/site-packages/angle_emb/angle.py:694\u001b[0m, in \u001b[0;36mPooler.__call__\u001b[0;34m(self, inputs, layer_index, embedding_size)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Dict, layer_index: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, embedding_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    689\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m    :param inputs: Dict. Model inputs.\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m    :param layer_index: int. Get embeddings from specific layer.\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;124;03m    :param embedding_size: int. Set embedding size for sentence embeddings for 2DMSE models.\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhidden_states[layer_index]\n\u001b[1;32m    695\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_llm:\n\u001b[1;32m    696\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/angle/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/angle/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/angle/lib/python3.8/site-packages/peft/peft_model.py:1091\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;241m==\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mPOLY:\n\u001b[1;32m   1090\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task_ids\n\u001b[0;32m-> 1091\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1102\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/angle/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/angle/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/angle/lib/python3.8/site-packages/peft/tuners/tuners_utils.py:160\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/angle/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:1176\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1173\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1176\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/angle/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/angle/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/angle/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:977\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    974\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 977\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    979\u001b[0m past_seen_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:  \u001b[38;5;66;03m# kept for BC (cache positions)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/angle/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/angle/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/angle/lib/python3.8/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/angle/lib/python3.8/site-packages/torch/nn/functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "from angle_emb import AnglE, Prompts\n",
    "\n",
    "\n",
    "peft_model_id = './models/angle-llama-13b-nli'\n",
    "angle = AnglE.from_pretrained('models/NousResearch/Llama-2-13b-hf', pretrained_lora_path=peft_model_id)\n",
    "\n",
    "print('All predefined prompts:', Prompts.list_prompts())\n",
    "angle.set_prompt(prompt=Prompts.A)\n",
    "print('prompt:', angle.prompt)\n",
    "vec = angle.encode({'text': 'hello world'}, to_numpy=True)\n",
    "print(vec)\n",
    "vecs = angle.encode([{'text': 'hello world1'}, {'text': 'hello world2'}], to_numpy=True)\n",
    "print(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f15591c-9a46-4f4e-8681-6fe65d38e3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.27171564e-01  1.75740778e-01  4.33561146e-01 -1.67173013e-01\n",
      "   2.28020459e-01 -3.13170254e-01  3.23471576e-01  2.04031318e-01\n",
      "  -4.91903007e-01 -3.83959204e-01 -3.45154583e-01 -2.40107596e-01\n",
      "   2.18672417e-02  2.15509579e-01 -3.58560443e-01  1.87191486e-01\n",
      "  -3.58546674e-01  4.50014293e-01 -6.13204837e-01  1.48928672e-01\n",
      "  -1.20886594e-01 -2.03186542e-01  1.72497153e-01 -2.65070587e-01\n",
      "  -2.46300057e-01 -1.50559723e-01  1.90194294e-01 -1.52787238e-01\n",
      "  -1.75320074e-01 -1.45873457e-01 -5.22409439e-01 -4.29163456e-01\n",
      "   3.16311032e-01 -4.92238142e-02 -5.73321223e-01  7.30248019e-02\n",
      "  -4.08607781e-01  5.06453454e-01 -4.11549956e-01 -3.83612365e-01\n",
      "   4.96911667e-02 -5.86191535e-01  3.62863153e-01  2.61388421e-01\n",
      "  -5.26935518e-01 -6.30573869e-01 -8.98442984e-01  2.97703028e-01\n",
      "  -4.99761760e-01 -1.85546607e-01  5.68788290e-01  2.64061615e-03\n",
      "  -8.24039876e-02 -3.32786441e-01  1.18184894e-01 -2.99841851e-01\n",
      "  -7.83518776e-02 -4.66939867e-01  2.29209363e-01 -3.58953774e-01\n",
      "   2.86294892e-03 -2.81176001e-01  5.20282388e-01 -6.12773001e-03\n",
      "   4.27944243e-01  6.34107232e-01  1.83049858e-01 -6.65160567e-02\n",
      "   4.52754386e-02  6.02951527e-01  9.70304385e-03 -4.32461888e-01\n",
      "   8.28891248e-02 -1.51260674e-01  7.89795890e-02 -7.40268886e-01\n",
      "   8.49848315e-02  3.58909965e-01  8.95131454e-02  3.96291614e-01\n",
      "  -1.07108243e-01 -4.61858511e-01  8.01512957e-01  3.35397989e-01\n",
      "   2.95069069e-04 -5.55790178e-02  2.41071761e-01  3.20984274e-01\n",
      "  -2.64288455e-01  2.98609138e-02  2.21270442e-01 -2.39910726e-02\n",
      "   6.44853711e-03  1.35768861e-01  6.54157281e-01 -3.91321659e-01\n",
      "  -2.16710530e-02 -7.17845857e-02 -1.76328659e-01 -5.41296840e-01\n",
      "  -4.17515218e-01 -1.16669238e-01 -3.88798006e-02 -4.79640871e-01\n",
      "   1.61252916e-01  6.92400932e-02  4.08931971e-01 -9.08906221e-01\n",
      "  -5.67505956e-02  1.43793553e-01  1.81271762e-01 -1.06992394e-01\n",
      "   1.79962561e-01 -1.48348719e-01  1.27309978e-01  7.71980882e-01\n",
      "   6.14848495e-01 -4.09798890e-01 -1.05253354e-01 -1.02240577e-01\n",
      "  -1.33019343e-01  3.53638530e-01  1.00043945e-01  3.38469744e-02\n",
      "   6.25921190e-01  2.58441716e-01 -6.53964281e-01  1.64679959e-01\n",
      "  -3.26884925e-01 -2.33524874e-01 -1.37523025e-01 -3.03682804e-01\n",
      "   2.90983617e-01  8.65396559e-02  2.80809462e-01  1.94490090e-01\n",
      "   2.80408770e-01 -4.73236173e-01 -3.32650751e-01 -2.10165381e-01\n",
      "  -1.68552622e-01  1.83745012e-01 -4.86773372e-01  6.04573265e-02\n",
      "   9.91974622e-02  2.81687677e-01 -1.28005803e-01  2.36869618e-01\n",
      "  -9.87807751e-01 -5.70943475e-01 -4.62960243e-01  1.41424090e-01\n",
      "  -3.35698962e-01 -2.25744098e-01  5.62671795e-02  3.16771269e-01\n",
      "  -4.07059282e-01 -1.17124692e-02  4.36680079e-01  3.50898623e-01\n",
      "   2.98922062e-01  4.16919708e-01 -2.28072479e-01  4.27731335e-01\n",
      "  -1.65575668e-01  4.55592811e-01  5.06879926e-01  2.20399290e-01\n",
      "   2.46289447e-01  4.28290106e-02  7.86995068e-02 -4.61630642e-01\n",
      "   3.64000678e-01  3.94027352e-01 -2.25989759e-01 -4.96398985e-01\n",
      "   1.32939017e+00 -1.36194095e-01  3.98291796e-02  2.07575813e-01\n",
      "  -1.31314802e+00 -9.09906030e-01 -3.07966352e-01  2.40621507e-01\n",
      "   1.48223221e-01  3.76481414e-01 -1.70218963e-02 -2.34945923e-01\n",
      "   1.59926564e-01  9.08531696e-02  3.83274913e-01 -2.33671710e-01\n",
      "  -3.61586928e-01 -4.37143743e-01  3.76093298e-01  3.31844747e-01\n",
      "  -1.98573038e-01  4.88617808e-01  1.05622754e-01  6.27593696e-01\n",
      "   3.41958642e-01  6.61504149e-01  1.42659128e-01  1.88805193e-01\n",
      "   1.49003595e-01 -2.02689499e-01  5.97366765e-02  3.60922515e-01\n",
      "  -2.74881050e-02  3.94853294e-01 -1.89247966e-01  3.30513477e-01\n",
      "   4.69557881e-01 -2.70775765e-01  3.99091661e-01  1.95879519e-01\n",
      "   1.09558277e-01  1.47380278e-01 -4.51705903e-01  3.72194350e-01\n",
      "   3.45905244e-01 -1.93409517e-01  2.78508157e-01 -1.16188303e-01\n",
      "   8.93715918e-01 -3.62391323e-02  8.03735971e-01  7.04839975e-02\n",
      "   1.92281306e-01  2.64791310e-01 -7.40401149e-02 -3.98390740e-01\n",
      "   2.60013103e-01  2.61296272e-01  6.39559478e-02  6.87146068e-01\n",
      "   7.49473214e-01 -2.74217755e-01  1.44951865e-02 -2.35149711e-02\n",
      "  -2.71385401e-01 -3.29094157e-02 -1.11071482e-01  2.50166655e-01\n",
      "  -2.20006242e-01 -7.32421279e-01  3.77849936e-01 -3.07877243e-01\n",
      "   2.18978882e-01  1.23789310e-01  6.00847192e-02 -3.07368301e-02\n",
      "   7.38647699e-01  5.88953197e-01  5.52768588e-01 -6.26822829e-01\n",
      "  -4.69386429e-01 -3.04042436e-02  5.80326557e-01  6.24454856e-01\n",
      "   1.29041404e-01 -4.97972250e-01 -2.36490648e-03 -1.72727734e-01\n",
      "  -2.11911649e-01  1.12751402e-01  2.65474260e-01  8.00357163e-02\n",
      "   2.85732925e-01  2.03880683e-01 -1.92655072e-01  4.92182910e-01\n",
      "   4.67865527e-01  9.95364487e-02  1.48761153e-01  7.05874115e-02\n",
      "   1.48639716e-02 -6.11209758e-02 -1.38148665e-01 -4.49090004e-01\n",
      "  -4.24246609e-01  4.03880417e-01 -7.14791000e-01 -3.24508071e-01\n",
      "   8.47140312e-01 -7.14187920e-01 -2.94655234e-01 -1.94225550e-01\n",
      "  -1.79155916e-02 -4.49467003e-01 -1.22499853e-01  7.76799619e-02\n",
      "   1.08099341e-01 -2.22088508e-02  8.65807652e-01  6.89434260e-02\n",
      "   7.11007357e-01  1.64726257e-01 -3.38981152e-01  4.23120797e-01\n",
      "  -8.68826285e-02  6.09390020e-01 -6.70849085e-01 -4.74667251e-01\n",
      "   4.90700424e-01  2.88845450e-02  2.50849754e-01 -6.50216520e-01\n",
      "  -4.09885216e+00 -3.85786623e-01  4.56304967e-01 -5.42699158e-01\n",
      "   1.69476822e-01 -6.46742880e-02  6.82939410e-01  4.62940425e-01\n",
      "  -2.97728062e-01  6.64944232e-01  1.59970105e-01 -2.48248652e-01\n",
      "  -5.47659338e-01  2.11559609e-03  7.05377758e-03 -3.09352338e-01\n",
      "  -4.87528965e-02 -2.10859865e-01 -3.41605484e-01  1.74237072e-01\n",
      "  -3.68770063e-01 -2.12226361e-01 -1.25825763e-01  1.55556098e-01\n",
      "   7.41463900e-03  2.19949901e-01  8.81285965e-01 -2.28991173e-02\n",
      "  -4.09348845e-01  4.29952115e-01 -5.85362986e-02 -9.05409455e-02\n",
      "  -4.02647853e-02 -2.37082422e-01 -7.17361271e-02  1.73878431e-01\n",
      "   3.21294308e-01 -4.41326618e-01  9.17477489e-01 -2.05976754e-01\n",
      "   3.83973956e-01 -2.10446179e-01  1.33505389e-01 -1.19934142e-01\n",
      "  -5.43342978e-02 -7.12733865e-01  7.68679678e-02 -3.49202693e-01\n",
      "   2.50957087e-02  1.76439613e-01  2.28837933e-02 -3.16970497e-02\n",
      "   8.25304687e-02 -4.43150371e-01  2.08574980e-01  2.24343576e-02\n",
      "  -2.58020647e-02  6.76432133e-01 -5.87140396e-02 -4.41339493e-01\n",
      "   5.63824177e-01  3.83841813e-01 -5.61786117e-03 -8.05593848e-01\n",
      "   4.12857443e-01  4.29903939e-02 -8.03660095e-01  7.87305087e-02\n",
      "  -1.69496179e-01 -2.83427000e-01  3.83440375e-01 -5.32580972e-01\n",
      "  -6.84597045e-02 -8.24708521e-01 -5.33192992e-01 -1.03112664e-02\n",
      "  -5.60913742e-01 -7.19310343e-01 -1.78673744e-01  8.89234394e-02\n",
      "  -3.45155597e-03 -1.75188541e-01 -4.84267443e-01  6.07214928e-01\n",
      "  -1.70537919e-01 -6.09310687e-01  4.94363941e-02 -1.94791883e-01\n",
      "  -2.03222916e-01  3.71521786e-02  3.99172157e-02 -2.74756730e-01\n",
      "  -4.18719828e-01  1.50068641e-01  5.54088593e-01  2.43372560e-01\n",
      "  -9.89356488e-02  5.87462425e-01  1.41168088e-01 -6.06474318e-02\n",
      "  -2.29469836e-01 -1.72339574e-01  2.09766969e-01  8.35348845e-01\n",
      "   1.12060833e+00  9.21978235e-01 -3.73492718e-01 -4.76524740e-01\n",
      "   9.59508061e-01  4.73846821e-03  3.47678721e-01 -9.07402337e-02\n",
      "  -3.41508776e-01  1.45654589e-01  7.22156689e-02 -5.07129431e-02\n",
      "   4.73469794e-01  4.52462256e-01  1.41575292e-01  4.15680766e-01\n",
      "   9.21923071e-02  5.03590703e-01 -5.20715356e-01  7.59420395e-01\n",
      "   1.98938161e-01 -7.09327534e-02 -1.56839624e-01  5.83881259e-01\n",
      "  -5.09582937e-01 -5.21770120e-01  1.08653128e-01 -1.78837419e-01\n",
      "   5.71887672e-01 -1.10055886e-01  5.27667463e-01  4.63349640e-01\n",
      "  -6.03000998e-01 -6.35731459e-01  1.00103885e-01 -1.30631909e-01\n",
      "  -3.68860126e-01 -2.61076033e-01 -7.11878613e-02  2.47210205e-01\n",
      "   1.44013137e-01 -1.46174608e-02 -1.40994340e-01  3.19372416e-02\n",
      "  -3.46648693e-01  2.27888022e-02  5.26623607e-01 -6.35428309e-01\n",
      "   3.69782567e-01 -1.06364883e-01 -6.41280174e-01 -2.66558409e-01\n",
      "   3.68044019e-01 -2.42904425e-01 -3.45331669e-01  1.17907755e-01\n",
      "   4.96516734e-01 -2.60876715e-01  4.39480059e-02  4.34797466e-01\n",
      "  -1.10793367e-01  4.95332956e-01  1.02702931e-01  2.42850453e-01\n",
      "  -2.10041419e-01 -5.56755781e-01  6.79588675e-01  5.06682277e-01\n",
      "  -4.05666947e-01 -1.89267263e-01 -2.74543971e-01  3.20334226e-01\n",
      "  -1.30623430e-01 -5.46179533e-01  3.53626907e-01 -5.33922136e-01\n",
      "  -2.25366764e-02 -2.57660866e-01  1.43361464e-03 -2.13924587e-01\n",
      "   5.39387465e-01 -3.91958922e-01  4.71004546e-01 -1.50555000e-01\n",
      "   4.04307604e-01 -1.86272919e-01 -2.51376152e-01  7.45019555e-01\n",
      "   6.04862832e-02 -4.37738270e-01 -7.04893887e-01 -4.76239860e-01\n",
      "   5.20469323e-02 -2.84768999e-01 -6.04615986e-01  1.54116124e-01\n",
      "  -4.93843377e-01 -5.23458242e-01 -2.29575410e-01 -3.01747501e-01\n",
      "   9.41013098e-02 -7.10730791e-01  3.79742146e-01 -1.31914988e-01\n",
      "   1.25748247e-01  9.10076797e-02  3.06022726e-02 -3.22645366e-01\n",
      "  -1.60476267e-02 -9.04396594e-01 -1.00674987e-01  1.24089047e-02\n",
      "  -2.47924007e-03  1.16923489e-01 -3.11303139e-03 -3.02284062e-01\n",
      "  -1.14934951e-01  3.28891486e-01 -1.12417519e-01 -2.07686156e-01\n",
      "  -6.27256036e-01 -4.59816873e-01 -3.59166741e-01  2.16361314e-01\n",
      "   1.17674455e-01 -1.33394286e-01  1.07941013e-02 -2.01264203e-01\n",
      "   4.17209864e-01 -1.05653778e-01 -3.54669631e-01 -2.03607857e-01\n",
      "  -4.76609915e-01 -2.58627862e-01 -4.02310610e-01  8.55234981e-01\n",
      "  -1.97568446e-01 -5.59928477e-01 -4.62239459e-02 -6.55317366e-01\n",
      "  -4.64966781e-02  1.22919351e-01 -3.12656581e-01  5.44233084e-01\n",
      "   2.22962826e-01  1.77375138e-01 -1.85795631e-02  5.49070477e-01\n",
      "   2.00459361e-01  1.88287318e-01  5.85571639e-02 -5.66371083e-01\n",
      "  -5.16346812e-01 -3.34905051e-02 -3.13563526e-01 -4.04140770e-01\n",
      "  -2.85972625e-01  2.60112584e-01 -3.92225981e-01  2.21290588e-01\n",
      "   2.97075629e-01  4.85486388e-01 -1.86674252e-01  6.28163442e-02\n",
      "  -6.11856356e-02  3.66958141e-01  2.97326803e-01  5.47615528e-01\n",
      "  -9.48761106e-02 -2.24283904e-01  3.67032625e-02  4.70248349e-02\n",
      "  -1.42854080e-01 -4.48179573e-01  1.73474342e-01 -3.95135343e-01\n",
      "  -2.08277538e-01  4.04578745e-01  2.14191616e-01 -2.43163735e-01\n",
      "   1.08078182e-01 -6.33559704e-01 -1.15948856e-01 -7.43692517e-01\n",
      "   6.69353977e-02  2.66251683e-01  1.22809991e-01  6.54164553e-01\n",
      "  -2.01518342e-01  5.47450423e-01  1.18660487e-01  2.18911499e-01\n",
      "  -1.31970316e-01  2.12758332e-01 -4.11360204e-01 -2.91084886e-01\n",
      "   5.90261482e-02  1.16893247e-01  5.44485629e-01 -5.16402483e-01\n",
      "   2.65591919e-01  1.70551956e-01 -1.07352391e-01  1.29943890e-02\n",
      "  -9.37070400e-02 -1.15343250e-01 -3.44024301e-01 -1.37475073e-01\n",
      "   1.14707589e-01  5.06681085e-01 -3.24239582e-02  6.98146343e-01\n",
      "   2.78438777e-01 -7.19675004e-01 -1.32548973e-01  3.21696460e-01\n",
      "   8.30910578e-02 -2.52523899e-01  2.16446757e-01  9.69898179e-02\n",
      "   1.95928812e-01  1.42776877e-01 -7.57833242e-01 -1.18409246e-02\n",
      "   4.15826470e-01  4.24776644e-01  3.53682071e-01  1.19322665e-01\n",
      "  -9.14258212e-02  3.81615832e-02  3.02626729e-01 -2.85140872e-01\n",
      "   1.49027109e-01 -9.40696001e-02 -8.35470617e-01  8.26719761e-01\n",
      "  -2.43656859e-02  6.84619188e-01  2.10598886e-01 -1.72516972e-01\n",
      "  -2.28780266e-02 -6.98995590e-02 -4.26955342e-01  9.51494813e-01\n",
      "   5.83711624e-01 -1.70755535e-01  3.88790965e-01 -7.85537586e-02\n",
      "  -2.89518654e-01 -4.07349408e-01  1.41508222e-01 -1.96321860e-01\n",
      "   2.96374261e-01 -3.81292999e-01 -3.14260036e-01  1.53377634e-02\n",
      "  -1.32593382e-02  3.45858745e-02  3.59097958e-01  3.11558813e-01\n",
      "   1.53262600e-01 -4.87927757e-02  5.22438101e-02  1.75333545e-01\n",
      "   5.99626899e-02 -1.90047063e-02  6.73967823e-02  2.30165608e-02\n",
      "  -3.25141966e-01  6.31205559e-01 -4.28555727e-01  4.05117452e-01\n",
      "  -2.91198909e-01 -4.37460452e-01 -7.42325559e-03 -3.58179808e-01\n",
      "  -3.39660831e-02  4.24042717e-03  2.78878510e-01  4.93887700e-02\n",
      "  -2.49127373e-02  4.22589242e-01  1.82520777e-01  7.99294487e-02\n",
      "  -2.75798470e-01 -2.12589234e-01  4.23594892e-01 -9.86283422e-01\n",
      "  -3.93501818e-01  7.36465603e-02  1.04932263e-01 -4.91609991e-01\n",
      "   6.48580045e-02 -1.45266235e-01 -2.81392764e-02  5.13528526e-01\n",
      "   5.59167385e-01 -6.32752329e-02 -1.21057704e-02 -1.14493988e-01\n",
      "  -5.01734912e-02  4.19266850e-01  1.35725141e-01 -1.11469746e-01\n",
      "  -2.33928829e-01 -2.02424377e-01  1.38380796e-01  1.05402187e-01\n",
      "   2.47254372e-02  3.23439032e-01  1.25533819e-01  2.17248499e-03\n",
      "   2.62334913e-01  2.64651477e-01  4.18681651e-01 -4.52590436e-01\n",
      "  -4.43926871e-01 -1.87801868e-01  1.04960896e-01  1.87380686e-02\n",
      "   6.31317794e-02  4.32113618e-01 -3.98161441e-01  1.13305017e-01\n",
      "  -5.92043996e-01  6.57465383e-02  4.20550644e-01  5.15914634e-02\n",
      "   9.44726542e-03 -1.02815114e-01 -4.64351773e-02  1.97357424e-02\n",
      "   1.10731825e-01 -1.45117283e-01  8.96853358e-02 -3.90625387e-01\n",
      "  -2.25016236e-01 -1.60462216e-01  2.61444211e-01 -4.76893038e-02\n",
      "  -8.67418885e-01 -1.42774269e-01  2.07413554e-01 -1.26596577e-02\n",
      "  -4.93036568e-01 -5.35370469e-01 -2.37964332e-01 -7.65708566e-01\n",
      "  -9.38109532e-02 -1.94720536e-01  1.65610969e-01  4.19810005e-02\n",
      "  -4.13286269e-01  1.89353928e-01  1.21501289e-01  2.98995793e-01]]\n",
      "[[-0.3794228   0.26642293  0.2784773  ...  0.06655556  0.15811956\n",
      "   0.47939137]\n",
      " [-0.25529113  0.02767812  0.14917977 ...  0.041011    0.3529467\n",
      "   0.38041154]]\n"
     ]
    }
   ],
   "source": [
    "# bert\n",
    "from angle_emb import AnglE\n",
    "\n",
    "model_id = 'models/angle-bert-base-uncased-nli-en-v1'\n",
    "angle = AnglE.from_pretrained(model_id, pooling_strategy='cls_avg').cuda()\n",
    "vec = angle.encode('hello world', to_numpy=True)\n",
    "print(vec)\n",
    "vecs = angle.encode(['hello world1', 'hello world2'], to_numpy=True)\n",
    "print(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8001eb25-1a1b-42e5-a110-e85a2b892ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-angle",
   "language": "python",
   "name": "conda-angle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
