{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import sys\n",
    "import torch \n",
    "import numpy as np\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from accelerate.utils import tqdm\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from functools import partial\n",
    "from importlib import reload\n",
    "from transformers import pipeline as pipe\n",
    "from transformers import (\n",
    "                          DataCollatorWithPadding,\n",
    "                          get_scheduler)\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from IPython.display import clear_output\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "# custom modules\n",
    "import utils.preprocessing as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "model_path = \"meta-llama/Meta-Llama-3-8B\"\n",
    "dataset_path = \"allenai/peS2o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_path, dataset_path):\n",
    "\n",
    "    # for distributed training\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    # load dataset\n",
    "    raw_dataset = load_dataset(dataset_path, \"v2\", streaming=True, trust_remote_code=True)\n",
    "\n",
    "    ## MODEL LOADING\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    "    )\n",
    "\n",
    "    # load tokenizer and model\n",
    "    pipeline = pipe('text-generation', \n",
    "                        model=model_path,\n",
    "                        model_kwargs={'torch_dtype': torch.bfloat16},\n",
    "                        device_map = accelerator.device \n",
    "                        )\n",
    "\n",
    "    pipeline.model = get_peft_model(pipeline.model, peft_config)\n",
    "    pipeline.tokenizer.pad_token = pipeline.tokenizer.eos_token\n",
    "    pipeline.tokenizer.pad_token_id = pipeline.tokenizer.eos_token_id\n",
    "    pipeline.model.generation_config.pad_token_id = pipeline.tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "    ## PREPROCESSING\n",
    "    # add special tokens to tokenizer\n",
    "    pipeline.tokenizer.pad_token = pipeline.tokenizer.eos_token\n",
    "    pipeline.model.resize_token_embeddings(len(pipeline.tokenizer))\n",
    "\n",
    "    tokenize_fn = partial(pp.tokenize_data, \n",
    "                        type = \"nextchar\",\n",
    "                        pipeline_name = pipeline,\n",
    "                        max_length = 100)\n",
    "\n",
    "    tokenized_dataset = raw_dataset.map(tokenize_fn,\n",
    "                                        batched=True,\n",
    "                                        remove_columns=raw_dataset['train'].column_names,)\n",
    "    tokenized_dataset.with_format(\"torch\")\n",
    "\n",
    "    # instantiate data collator\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=pipeline.tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(tokenized_dataset['train'],\n",
    "                                batch_size=8, \n",
    "                                collate_fn=data_collator,\n",
    "                                num_workers=20)\n",
    "\n",
    "    val_dataloader = DataLoader(tokenized_dataset['validation'],\n",
    "                            batch_size=8,\n",
    "                            collate_fn=data_collator,\n",
    "                            num_workers=2)\n",
    "    \n",
    "    ## TRAINING\n",
    "\n",
    "    # options\n",
    "    num_batches = 10\n",
    "    num_epochs = 1\n",
    "    checkpoint_path = '../checkpoints/checkpoint_8b_{0}epochs.pt'\n",
    "    log_path = '../logs/log.csv'\n",
    "\n",
    "    # init optimizer\n",
    "    optimizer = AdamW(pipeline.model.parameters(), lr=1e-5)\n",
    "\n",
    "    # init scheduler\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=1000,\n",
    "        num_training_steps=num_epochs * num_batches,\n",
    "    )\n",
    "\n",
    "    pipeline.model, optimizer, train_dataloader, val_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        pipeline.model, optimizer, train_dataloader, val_dataloader, lr_scheduler)\n",
    "\n",
    "    # init parameters\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    with open(log_path, 'w') as f: \n",
    "        f.write(f'epoch,iter_num,train_loss,val_loss\\n')\n",
    "\n",
    "    # loop\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        clear_output(wait=False)\n",
    "\n",
    "        running_train_loss = 0.0\n",
    "        running_val_loss = 0.0\n",
    "\n",
    "        accelerator.print(\"=====================\")\n",
    "        accelerator.print(f\"Epoch {epoch + 1}\")\n",
    "        accelerator.print(\"=====================\")\n",
    "\n",
    "        # loop through train data\n",
    "        accelerator.print(\"Training...\")\n",
    "        with tqdm(range(num_batches), disable=not accelerator.is_local_main_process) as pbar:\n",
    "            for i, (train_batch, val_batch) in enumerate(zip(train_dataloader, val_dataloader)):\n",
    "                \n",
    "                ## training\n",
    "                # set model to train mode\n",
    "                pipeline.model.train()\n",
    "\n",
    "                # grab batch and map to device\n",
    "                train_batch = {k: v.to(accelerator.device) for k, v in train_batch.items()}\n",
    "\n",
    "                # forward pass\n",
    "                outputs = pipeline.model(train_batch['input_ids'], \n",
    "                                        labels=train_batch['input_ids'],\n",
    "                                        attention_mask=train_batch['attention_mask'])\n",
    "                train_loss = outputs.loss\n",
    "\n",
    "                running_train_loss += train_loss.item()\n",
    "\n",
    "                # backward pass\n",
    "                # train_loss.backward()\n",
    "                accelerator.backward(train_loss)\n",
    "\n",
    "                # clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(pipeline.model.parameters(), 1.0)\n",
    "\n",
    "                # update optimizer, scheduler\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                # zero gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                ## validation\n",
    "                # set model to eval mode\n",
    "                pipeline.model.eval()\n",
    "                # loop through val data\n",
    "                val_batch = {k: v.to(accelerator.device) for k, v in val_batch.items()}\n",
    "                with torch.no_grad():\n",
    "                    outputs = pipeline.model(val_batch['input_ids'], \n",
    "                                            labels=val_batch['input_ids'],\n",
    "                                            attention_mask=val_batch['attention_mask'])\n",
    "                    val_loss = outputs.loss\n",
    "                    running_val_loss += val_loss.item()\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                \n",
    "                accelerator.print(f\"Train Batch Loss: {train_loss:.4f} | Val Batch Loss: {val_loss:.4f} | Best Val. Loss: {best_val_loss:.4f}\\r\", end=\"\")\n",
    "\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # write to log\n",
    "                with open(log_path, 'a') as f: \n",
    "                    f.write(f'{epoch},{i},{train_loss},{val_loss}\\n')\n",
    "                \n",
    "                if i == num_batches:\n",
    "                    accelerator.print(f\"Reached {num_batches} batches; starting next epoch...\")\n",
    "                    \n",
    "                    # break out of batching loop\n",
    "                    break\n",
    "        \n",
    "        train_loss = running_train_loss / num_batches\n",
    "        val_loss = running_val_loss / num_batches\n",
    "        train_loss = running_train_loss / num_batches\n",
    "        accelerator.print(f\"Avg. Train Loss: {train_loss:.4f}, Avg. Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    accelerator.print(f\"Saving model checkpoint to {checkpoint_path.format(epoch)}\")\n",
    "    # save model checkpoint\n",
    "    checkpoint = {'model': pipeline.model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                }\n",
    "    accelerator.save(checkpoint, checkpoint_path.format(epoch))\n",
    "\n",
    "    accelerator.print(\"Training Complete!\")\n",
    "\n",
    "    # save model to hub after training\n",
    "    pipeline.model.push_to_hub(f\"Semantic-Scholar-{model_path.split('/')[-1]}\")\n",
    "    pipeline.tokenizer.push_to_hub(f\"Semantic-Scholar-{model_path.split('/')[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "Epoch 1\n",
      "=====================\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0a87e5a86144dba8cfcadb0840ba00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1010 15:22:31.582478 140610585851712 torch/multiprocessing/spawn.py:146] Terminating process 393007 via signal SIGTERM\n",
      "W1010 15:23:01.614235 140610585851712 torch/multiprocessing/spawn.py:154] Unable to shutdown process 393007 via SIGTERM , forcefully exiting via SIGKILL\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702] failed (exitcode: 1) local_rank: 1 (pid: 393009) of fn: train_model (start_method: fork)\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702] Traceback (most recent call last):\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/api.py\", line 659, in _poll\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]     self._pc.join(-1)\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 189, in join\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]     raise ProcessRaisedException(msg, error_index, failed_process.pid)\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702] torch.multiprocessing.spawn.ProcessRaisedException: \n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702] \n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702] -- Process 1 terminated with the following error:\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702] Traceback (most recent call last):\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 76, in _wrap\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]     fn(i, *args)\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/api.py\", line 583, in _wrap\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]     ret = record(fn)(*args_)\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 348, in wrapper\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]     return f(*args, **kwargs)\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/tmp/ipykernel_373195/2832777836.py\", line 109, in train_model\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]     outputs = pipeline.model(train_batch['input_ids'],\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]     return self._call_impl(*args, **kwargs)\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]     return forward_call(*args, **kwargs)\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1636, in forward\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]     else self._run_ddp_forward(*inputs, **kwargs)\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1454, in _run_ddp_forward\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]     return self.module(*inputs, **kwargs)  # type: ignore[index]\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]     return self._call_impl(*args, **kwargs)\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]     return forward_call(*args, **kwargs)\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/peft/peft_model.py\", line 1577, in forward\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]     return self.base_model(\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]     return self._call_impl(*args, **kwargs)\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]     return forward_call(*args, **kwargs)\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/peft/tuners/tuners_utils.py\", line 188, in forward\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]     return self.model.forward(*args, **kwargs)\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]   File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\", line 1161, in forward\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702]     logits = logits.float()\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 392.00 MiB. GPU 1 has a total capacity of 39.50 GiB of which 317.81 MiB is free. Process 305467 has 17.94 GiB memory in use. Including non-PyTorch memory, this process has 21.23 GiB memory in use. Of the allocated memory 19.82 GiB is allocated by PyTorch, and 180.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "E1010 15:23:01.620146 140610585851712 torch/distributed/elastic/multiprocessing/api.py:702] \n"
     ]
    },
    {
     "ename": "ChildFailedError",
     "evalue": "\n============================================================\ntrain_model FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2024-10-10_15:22:31\n  host      : bsd-a100\n  rank      : 1 (local_rank: 1)\n  exitcode  : 1 (pid: 393009)\n  error_file: /tmp/torchelastic_5ol151nh/none_dn7ruawi/attempt_0/1/error.json\n  traceback : Traceback (most recent call last):\n    File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 348, in wrapper\n      return f(*args, **kwargs)\n    File \"/tmp/ipykernel_373195/2832777836.py\", line 109, in train_model\n      outputs = pipeline.model(train_batch['input_ids'],\n    File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n      return self._call_impl(*args, **kwargs)\n    File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n      return forward_call(*args, **kwargs)\n    File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1636, in forward\n      else self._run_ddp_forward(*inputs, **kwargs)\n    File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1454, in _run_ddp_forward\n      return self.module(*inputs, **kwargs)  # type: ignore[index]\n    File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n      return self._call_impl(*args, **kwargs)\n    File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n      return forward_call(*args, **kwargs)\n    File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/peft/peft_model.py\", line 1577, in forward\n      return self.base_model(\n    File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n      return self._call_impl(*args, **kwargs)\n    File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n      return forward_call(*args, **kwargs)\n    File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/peft/tuners/tuners_utils.py\", line 188, in forward\n      return self.model.forward(*args, **kwargs)\n    File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\", line 1161, in forward\n      logits = logits.float()\n  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 392.00 MiB. GPU 1 has a total capacity of 39.50 GiB of which 317.81 MiB is free. Process 305467 has 17.94 GiB memory in use. Including non-PyTorch memory, this process has 21.23 GiB memory in use. Of the allocated memory 19.82 GiB is allocated by PyTorch, and 180.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n  \n============================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mChildFailedError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# accelerate notebook launcher to run\u001b[39;00m\n\u001b[1;32m      2\u001b[0m args \u001b[38;5;241m=\u001b[39m (model_path, dataset_path)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/accelerate/launchers.py:245\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes, rdzv_backend, rdzv_endpoint, rdzv_conf, rdzv_id, max_restarts, monitor_interval, log_line_prefix_template)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;124m\"\u001b[39m, ELASTIC_LOG_LINE_PREFIX_TEMPLATE_PYTORCH_VERSION):\n\u001b[1;32m    244\u001b[0m         launch_config_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_line_prefix_template\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m log_line_prefix_template\n\u001b[0;32m--> 245\u001b[0m     \u001b[43melastic_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLaunchConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlaunch_config_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentrypoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProcessRaisedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py:133\u001b[0m, in \u001b[0;36melastic_launch.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlaunch_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_entrypoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/distributed/launcher/api.py:264\u001b[0m, in \u001b[0;36mlaunch_agent\u001b[0;34m(config, entrypoint, args)\u001b[0m\n\u001b[1;32m    257\u001b[0m     events\u001b[38;5;241m.\u001b[39mrecord(agent\u001b[38;5;241m.\u001b[39mget_event_succeeded())\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mis_failed():\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;66;03m# ChildFailedError is treated specially by @record\u001b[39;00m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;66;03m# if the error files for the failed children exist\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;66;03m# @record will copy the first error (root cause)\u001b[39;00m\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;66;03m# to the error file of the launcher process.\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChildFailedError(\n\u001b[1;32m    265\u001b[0m             name\u001b[38;5;241m=\u001b[39mentrypoint_name,\n\u001b[1;32m    266\u001b[0m             failures\u001b[38;5;241m=\u001b[39mresult\u001b[38;5;241m.\u001b[39mfailures,\n\u001b[1;32m    267\u001b[0m         )\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreturn_values\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ChildFailedError:\n",
      "\u001b[0;31mChildFailedError\u001b[0m: \n============================================================\ntrain_model FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2024-10-10_15:22:31\n  host      : bsd-a100\n  rank      : 1 (local_rank: 1)\n  exitcode  : 1 (pid: 393009)\n  error_file: /tmp/torchelastic_5ol151nh/none_dn7ruawi/attempt_0/1/error.json\n  traceback : Traceback (most recent call last):\n    File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 348, in wrapper\n      return f(*args, **kwargs)\n    File \"/tmp/ipykernel_373195/2832777836.py\", line 109, in train_model\n      outputs = pipeline.model(train_batch['input_ids'],\n    File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n      return self._call_impl(*args, **kwargs)\n    File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n      return forward_call(*args, **kwargs)\n    File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1636, in forward\n      else self._run_ddp_forward(*inputs, **kwargs)\n    File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1454, in _run_ddp_forward\n      return self.module(*inputs, **kwargs)  # type: ignore[index]\n    File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n      return self._call_impl(*args, **kwargs)\n    File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n      return forward_call(*args, **kwargs)\n    File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/peft/peft_model.py\", line 1577, in forward\n      return self.base_model(\n    File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n      return self._call_impl(*args, **kwargs)\n    File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n      return forward_call(*args, **kwargs)\n    File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/peft/tuners/tuners_utils.py\", line 188, in forward\n      return self.model.forward(*args, **kwargs)\n    File \"/mnt/DGX01/Personal/krusepi/.venv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\", line 1161, in forward\n      logits = logits.float()\n  torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 392.00 MiB. GPU 1 has a total capacity of 39.50 GiB of which 317.81 MiB is free. Process 305467 has 17.94 GiB memory in use. Including non-PyTorch memory, this process has 21.23 GiB memory in use. Of the allocated memory 19.82 GiB is allocated by PyTorch, and 180.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n  \n============================================================"
     ]
    }
   ],
   "source": [
    "# accelerate notebook launcher to run\n",
    "args = (model_path, dataset_path)\n",
    "notebook_launcher(train_model, args = args, num_processes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a test prediction\n",
    "outputs = pipe(\n",
    "    text,\n",
    "    max_new_tokens=1024,\n",
    "    eos_token_id=terminators,\n",
    "    no_repeat_ngram_size=3,       \n",
    "    do_sample=True, \n",
    "    top_k=100, \n",
    "    top_p=0.9,\n",
    "    temperature=0.6\n",
    ")\n",
    "print(outputs[0][0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
