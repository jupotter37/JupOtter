{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36273039-10ff-4a44-9e97-7807c7a33fbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| distributed init (rank 0, world 1): env://\n",
      "2024-04-16 21:39:35,394 [INFO] \n",
      "=====  Running Parameters    =====\n",
      "2024-04-16 21:39:35,395 [INFO] {\n",
      "    \"amp\": true,\n",
      "    \"device\": \"cuda\",\n",
      "    \"dist_backend\": \"nccl\",\n",
      "    \"dist_url\": \"env://\",\n",
      "    \"distributed\": true,\n",
      "    \"evaluate\": false,\n",
      "    \"gpu\": 0,\n",
      "    \"init_lr\": 1e-05,\n",
      "    \"iters_per_epoch\": 176948,\n",
      "    \"job_name\": \"pmcvqa_pretrain_w_bigger_mid_proj_w_trainable_visual_qv_w_sophia_w_dora\",\n",
      "    \"lr_sched\": \"linear_warmup_cosine_lr\",\n",
      "    \"max_epoch\": 2,\n",
      "    \"min_lr\": 1e-06,\n",
      "    \"num_workers\": 6,\n",
      "    \"output_dir\": \"outputs\",\n",
      "    \"rank\": 0,\n",
      "    \"resume_ckpt_path\": null,\n",
      "    \"seed\": 42,\n",
      "    \"task\": \"image_text_pretrain\",\n",
      "    \"train_splits\": [\n",
      "        \"train\"\n",
      "    ],\n",
      "    \"wandb_log\": true,\n",
      "    \"warmup_lr\": 1e-06,\n",
      "    \"warmup_steps\": 1000,\n",
      "    \"weight_decay\": 0.05,\n",
      "    \"world_size\": 1\n",
      "}\n",
      "2024-04-16 21:39:35,395 [INFO] \n",
      "======  Dataset Attributes  ======\n",
      "2024-04-16 21:39:35,395 [INFO] \n",
      "======== pmcvqa =======\n",
      "2024-04-16 21:39:35,395 [INFO] {\n",
      "    \"batch_size\": 2,\n",
      "    \"build_info\": {\n",
      "        \"ann_path\": \"/ibex/user/sabbam0a/grad/MiniGPT-4/medvqa_dataset/PMC-VQA/train.csv\",\n",
      "        \"image_path\": \"/ibex/user/sabbam0a/grad/MiniGPT-4/medvqa_dataset/PMC-VQA/images/images\"\n",
      "    },\n",
      "    \"data_type\": \"images\",\n",
      "    \"sample_ratio\": 14,\n",
      "    \"text_processor\": {\n",
      "        \"train\": {\n",
      "            \"name\": \"blip_caption\"\n",
      "        }\n",
      "    },\n",
      "    \"vis_processor\": {\n",
      "        \"train\": {\n",
      "            \"image_size\": 448,\n",
      "            \"name\": \"blip2_image_train\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "2024-04-16 21:39:35,395 [INFO] \n",
      "======  Model Attributes  ======\n",
      "2024-04-16 21:39:35,395 [INFO] {\n",
      "    \"arch\": \"minigpt_v2\",\n",
      "    \"chat_template\": true,\n",
      "    \"ckpt\": \"\",\n",
      "    \"drop_path_rate\": 0,\n",
      "    \"end_sym\": \"</s>\",\n",
      "    \"freeze_vit\": false,\n",
      "    \"image_size\": 448,\n",
      "    \"llama_model\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
      "    \"lora_alpha\": 16,\n",
      "    \"lora_r\": 64,\n",
      "    \"max_txt_len\": 1024,\n",
      "    \"model_type\": \"pretrain\",\n",
      "    \"prompt\": \"\",\n",
      "    \"use_grad_checkpoint\": true,\n",
      "    \"vit_precision\": \"fp16\"\n",
      "}\n",
      "2024-04-16 21:39:35,395 [INFO] Building datasets...\n",
      "2024-04-16 21:39:36,087 [INFO] Loading LLAMA\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.30it/s]\n",
      "trainable params: 33,816,576 || all params: 6,772,232,192 || trainable%: 0.49934165045237716\n",
      "2024-04-16 21:39:42,562 [INFO] Loading LLAMA Done\n",
      "2024-04-16 21:39:42,563 [INFO] Loading VIT\n",
      "Position interpolate from 16x16 to 32x32\n",
      "2024-04-16 21:39:53,395 [INFO] freeze vision encoder\n",
      "2024-04-16 21:39:53,395 [INFO] Loading VIT Done\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msabbagh\u001b[0m (\u001b[33mhaxpa\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/ibex/user/sabbam0a/grad/MiniGPT-4/wandb/run-20240416_213955-hmpzdj8j\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpmcvqa_pretrain_w_bigger_mid_proj_w_trainable_visual_qv_w_sophia_w_dora\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/haxpa/MiniMedGPT\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/haxpa/MiniMedGPT/runs/hmpzdj8j\u001b[0m\n",
      "2024-04-16 21:40:02,554 [INFO] Start training\n",
      "Traceback (most recent call last):\n",
      "  File \"/ibex/user/sabbam0a/grad/MiniGPT-4/train.py\", line 104, in <module>\n",
      "    main()\n",
      "  File \"/ibex/user/sabbam0a/grad/MiniGPT-4/train.py\", line 100, in main\n",
      "    runner.train()\n",
      "  File \"/ibex/user/sabbam0a/grad/MiniGPT-4/minigpt4/runners/runner_base.py\", line 377, in train\n",
      "    train_stats = self.train_epoch(cur_epoch)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ibex/user/sabbam0a/grad/MiniGPT-4/minigpt4/runners/runner_base.py\", line 435, in train_epoch\n",
      "    self.model.train()\n",
      "    ^^^^^^^^^^\n",
      "  File \"/ibex/user/sabbam0a/grad/MiniGPT-4/minigpt4/runners/runner_base.py\", line 85, in model\n",
      "    self._model = self._model.to(self.device)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sabbam0a/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1160, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sabbam0a/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/sabbam0a/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/sabbam0a/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"/home/sabbam0a/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/home/sabbam0a/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacty of 79.15 GiB of which 32.00 MiB is free. Process 1247012 has 49.15 GiB memory in use. Including non-PyTorch memory, this process has 29.95 GiB memory in use. Of the allocated memory 29.25 GiB is allocated by PyTorch, and 152.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mpmcvqa_pretrain_w_bigger_mid_proj_w_trainable_visual_qv_w_sophia_w_dora\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/haxpa/MiniMedGPT/runs/hmpzdj8j\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ô∏è‚ö° View job at \u001b[34m\u001b[4mhttps://wandb.ai/haxpa/MiniMedGPT/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE1NDUwMjg3MQ==/version_details/v16\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240416_213955-hmpzdj8j/logs\u001b[0m\n",
      "[2024-04-16 21:40:19,597] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 1260707) of binary: /home/sabbam0a/miniconda3/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sabbam0a/miniconda3/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/home/sabbam0a/miniconda3/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sabbam0a/miniconda3/lib/python3.11/site-packages/torch/distributed/run.py\", line 806, in main\n",
      "    run(args)\n",
      "  File \"/home/sabbam0a/miniconda3/lib/python3.11/site-packages/torch/distributed/run.py\", line 797, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/sabbam0a/miniconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/sabbam0a/miniconda3/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "train.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-04-16_21:40:19\n",
      "  host      : gpu201-23-r.ibex.kaust.edu.sa\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 1260707)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!torchrun train.py --cfg-path train_configs/minigptv2_finetune.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fd03963-2911-4b30-827a-4aa3ee84a96f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Chat\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.01s/it]\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "trainable params: 33,554,432 || all params: 6,771,970,048 || trainable%: 0.49548996469513035\n",
      "Position interpolate from 16x16 to 32x32\n",
      "Load Minigpt-4-LLM Checkpoint: minigpt4/outputs/slake_finetune_w_mid_proj/checkpoint_0.pth\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/gradio/layouts/column.py:55: UserWarning: 'scale' value should be an integer. Using 0.5 will cause issues.\n",
      "  warnings.warn(\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://ac9adf7e8049e549d3.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/sabbam0a/miniconda3/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "^C\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://ac9adf7e8049e549d3.gradio.live\n"
     ]
    }
   ],
   "source": [
    "!python /ibex/user/sabbam0a/grad/MiniGPT-4/demo_v2.py --cfg-path /ibex/user/sabbam0a/grad/MiniGPT-4/eval_configs/minigptv2_eval.yaml --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334581df-a99a-43e8-92b0-b1e025c6c813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bitsandbytes.cextension import CUDASetup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ddf0a4-e0fd-45a2-be9c-2045fa23d788",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lib = CUDASetup.get_instance().lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f41e787-b063-44cf-9e2b-d6a0dcbf56f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73557e62-0561-445e-9851-a490f4118edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(img_size=224,drop_path_rate=0.4,use_checkpoint=False,precision=\"fp16\"):\n",
    "    model = VisionTransformer(\n",
    "        img_size=img_size,\n",
    "        patch_size=14,\n",
    "        use_mean_pooling=False,\n",
    "        embed_dim=1408,\n",
    "        depth=39,\n",
    "        num_heads=1408//88,\n",
    "        mlp_ratio=4.3637,\n",
    "        qkv_bias=True,\n",
    "        drop_path_rate=drop_path_rate,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
    "        use_checkpoint=use_checkpoint,\n",
    "    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff324381-d981-4b21-8d13-64cb634fdf20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7613a4c3-70e7-47c5-8a63-b022c7954981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        # In case langdetect cannot determine the language, consider it non-English\n",
    "        return False\n",
    "\n",
    "def filter_english_questions(json_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    english_questions = [doc for doc in data if is_english(doc.get('question', ''))]\n",
    "\n",
    "    return english_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9507d5-fe34-4ccf-a699-e81606b3a6ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json_file = 'medvqa_dataset/Slake/Slake1.0/test.json'  # Replace with your JSON file path\n",
    "\n",
    "english_questions = filter_english_questions(json_file)\n",
    "\n",
    "# Save the filtered questions back to a new JSON file\n",
    "with open('medvqa_dataset/Slake/Slake1.0/test_en.json', 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(english_questions, output_file, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495336d4-2996-4494-8170-83518d7041ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def merge_json_files(file1, file2, file3, output_file):\n",
    "    with open(file1, 'r', encoding='utf-8') as f1:\n",
    "        data1 = json.load(f1)\n",
    "\n",
    "    with open(file2, 'r', encoding='utf-8') as f2:\n",
    "        data2 = json.load(f2)\n",
    "\n",
    "    with open(file3, 'r', encoding='utf-8') as f3:\n",
    "        data3 = json.load(f3)\n",
    "\n",
    "    # Merge the data from three files\n",
    "    merged_data = data1 + data2 + data3\n",
    "\n",
    "    # Save the merged data to a new JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as output:\n",
    "        json.dump(merged_data, output, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "json_file1 = 'medvqa_dataset/Slake/Slake1.0/train_en.json'  # Replace with the path to your first JSON file\n",
    "json_file2 = 'medvqa_dataset/Slake/Slake1.0/validate_en.json'  # Replace with the path to your second JSON file\n",
    "json_file3 = 'medvqa_dataset/Slake/Slake1.0/test_en.json'  # Replace with the path to your third JSON file\n",
    "output_json_file = 'medvqa_dataset/Slake/Slake1.0/all_annotations_en.json'  # Replace with the desired output file name\n",
    "\n",
    "merge_json_files(json_file1, json_file2, json_file3, output_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b49f816-b558-4f73-a358-f9373b0abb64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3795d6-d2db-4624-94f7-918edf2a6865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
