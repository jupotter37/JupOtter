{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcav88u_I45M"
      },
      "source": [
        "# Prompting Language Models\n",
        "\n",
        "In ths notebook, we'll be evaluating different model prompting strategies on a publicly available language model. We will then perform soft-prompt tuning on GPT-2 and compare it against hard prompting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShJPg2nFI45N"
      },
      "source": [
        "## Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5Cs5P34rU7rT"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers\n",
        "#!pip install inflect\n",
        "#!pip install cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UCd-lD2jU7rX"
      },
      "outputs": [],
      "source": [
        "import pickle as pkl\n",
        "import os\n",
        "import json\n",
        "import cohere\n",
        "import random\n",
        "import inflect\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle as pkl\n",
        "import torch\n",
        "from torch import nn\n",
        "import time\n",
        "from collections import deque\n",
        "\n",
        "random.seed(0)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnghhJjYU7rY"
      },
      "source": [
        "# Load the dataset\n",
        "\n",
        "We will be using the Common Sense QA dataset, which is a collection of questions about everyday life. The cells below download the data from https://www.tau-nlp.sites.tau.ac.il/commonsenseqa ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "N9rldi5WU7rZ"
      },
      "outputs": [],
      "source": [
        "# Load dataset from jsonl file\n",
        "def make_dataset(path):\n",
        "    dataset = []\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            dataset.append(json.loads(line))\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FLwUUEk3U7rZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            " 73 3697k   73 2731k    0     0  2557k      0  0:00:01  0:00:01 --:--:-- 2557k\n",
            "100 3697k  100 3697k    0     0  3205k      0  0:00:01  0:00:01 --:--:-- 3206k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  460k  100  460k    0     0   688k      0 --:--:-- --:--:-- --:--:--  689k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  413k  100  413k    0     0   702k      0 --:--:-- --:--:-- --:--:--  702k\n"
          ]
        }
      ],
      "source": [
        "#@title download the dataset\n",
        "!curl https://s3.amazonaws.com/commensenseqa/train_rand_split.jsonl -o train_rand_split.jsonl\n",
        "!curl https://s3.amazonaws.com/commensenseqa/dev_rand_split.jsonl -o dev_rand_split.jsonl\n",
        "!curl https://s3.amazonaws.com/commensenseqa/test_rand_split_no_answers.jsonl -o test_rand_split_no_answers.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YhoOwcDYU7ra"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9741 1221\n"
          ]
        }
      ],
      "source": [
        "train_set = make_dataset('train_rand_split.jsonl')\n",
        "val_set = make_dataset('dev_rand_split.jsonl')\n",
        "\n",
        "# Print the lengths of the train and validation sets\n",
        "print(len(train_set), len(val_set))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p02tkKfGU7ra"
      },
      "source": [
        "Here are a few examples of the dataset format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "se1rOuGkU7rb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'answerKey': 'A',\n",
              "  'id': '1afa02df02c908a558b4036e80242fac',\n",
              "  'question': {'question_concept': 'revolving door',\n",
              "   'choices': [{'label': 'A', 'text': 'bank'},\n",
              "    {'label': 'B', 'text': 'library'},\n",
              "    {'label': 'C', 'text': 'department store'},\n",
              "    {'label': 'D', 'text': 'mall'},\n",
              "    {'label': 'E', 'text': 'new york'}],\n",
              "   'stem': 'A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?'}},\n",
              " {'answerKey': 'A',\n",
              "  'id': 'a7ab086045575bb497933726e4e6ad28',\n",
              "  'question': {'question_concept': 'people',\n",
              "   'choices': [{'label': 'A', 'text': 'complete job'},\n",
              "    {'label': 'B', 'text': 'learn from each other'},\n",
              "    {'label': 'C', 'text': 'kill animals'},\n",
              "    {'label': 'D', 'text': 'wear hats'},\n",
              "    {'label': 'E', 'text': 'talk to each other'}],\n",
              "   'stem': 'What do people aim to do at work?'}},\n",
              " {'answerKey': 'B',\n",
              "  'id': 'b8c0a4703079cf661d7261a60a1bcbff',\n",
              "  'question': {'question_concept': 'magazines',\n",
              "   'choices': [{'label': 'A', 'text': 'doctor'},\n",
              "    {'label': 'B', 'text': 'bookstore'},\n",
              "    {'label': 'C', 'text': 'market'},\n",
              "    {'label': 'D', 'text': 'train station'},\n",
              "    {'label': 'E', 'text': 'mortuary'}],\n",
              "   'stem': 'Where would you find magazines along side many other printed works?'}}]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_set[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fUcl1PcU7rc"
      },
      "source": [
        "Make an Cohere account, generate a **trial** API key at https://dashboard.cohere.ai/api-keys, and paste it below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "w258J--PU7rc"
      },
      "outputs": [],
      "source": [
        "# Set the api key from https://dashboard.cohere.ai/api-keys\n",
        "# my trial API\n",
        "# MingZwhy\n",
        "\n",
        "co = cohere.Client('DVlh1qOXWwaTCNthG68iZQUR8g4B2YOWW9pxxmS6')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWXXcazzU7rc"
      },
      "source": [
        "In this notebook, we'll explore different hard-prompting strategies. Run the cells below to see a few example strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2caC6yf4U7rd"
      },
      "outputs": [],
      "source": [
        "def make_simple_prompt(data_point):\n",
        "    prompt = f\"\"\"{data_point['question']['stem']}\n",
        " {data_point['question']['choices'][0]['label']} {data_point['question']['choices'][0]['text']}\n",
        " {data_point['question']['choices'][1]['label']} {data_point['question']['choices'][1]['text']}\n",
        " {data_point['question']['choices'][2]['label']} {data_point['question']['choices'][2]['text']}\n",
        " {data_point['question']['choices'][3]['label']} {data_point['question']['choices'][3]['text']}\n",
        " {data_point['question']['choices'][4]['label']} {data_point['question']['choices'][4]['text']}\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "def make_simple_qa_prompt(data_point):\n",
        "    prompt = f\"\"\"Question: {data_point['question']['stem']}\n",
        "Choice {data_point['question']['choices'][0]['label']}: {data_point['question']['choices'][0]['text']}\n",
        "Choice {data_point['question']['choices'][1]['label']}: {data_point['question']['choices'][1]['text']}\n",
        "Choice {data_point['question']['choices'][2]['label']}: {data_point['question']['choices'][2]['text']}\n",
        "Choice {data_point['question']['choices'][3]['label']}: {data_point['question']['choices'][3]['text']}\n",
        "Choice {data_point['question']['choices'][4]['label']}: {data_point['question']['choices'][4]['text']}\n",
        "Answer:\"\"\"\n",
        "    return prompt\n",
        "\n",
        "def get_instruction():\n",
        "    return \"Answer the following question with A, B, C, D, or E.\\n\"\n",
        "\n",
        "def make_qa_instruction_prompt(data_point):\n",
        "    prompt = get_instruction()\n",
        "    prompt += make_simple_qa_prompt(data_point)\n",
        "    return prompt\n",
        "\n",
        "def make_few_shot_prompt(data_point, num_shots):\n",
        "    prompt = get_instruction()\n",
        "    for i in range(num_shots):\n",
        "        prompt += make_simple_qa_prompt(train_set[i])\n",
        "        prompt += f\" {train_set[i]['answerKey']}\\n\"\n",
        "    prompt += make_simple_qa_prompt(data_point)\n",
        "    return prompt\n",
        "\n",
        "# This is like the prompt above, but the answers in the examples given are random, not the correct answer\n",
        "def make_incorrect_few_shot_prompt(data_point, num_shots):\n",
        "    prompt = get_instruction()\n",
        "    for i in range(num_shots):\n",
        "        prompt += make_simple_prompt(train_set[i])\n",
        "        valid_answers = ['A', 'B', 'C', 'D', 'E']\n",
        "        valid_answers.remove(train_set[i]['answerKey'])\n",
        "        # Randomly choose an incorrect answer\n",
        "        random_answer = random.choice(valid_answers)\n",
        "        prompt += f\"{random_answer}\\n\"\n",
        "    prompt += make_simple_qa_prompt(data_point)\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UrXbp2iMU7re"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================== Simple Prompt ========================================\n",
            "The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
            " A ignore\n",
            " B enforce\n",
            " C authoritarian\n",
            " D yell at\n",
            " E avoid\n",
            "\n",
            "======================================== Simple QA Prompt ========================================\n",
            "Question: The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
            "Choice A: ignore\n",
            "Choice B: enforce\n",
            "Choice C: authoritarian\n",
            "Choice D: yell at\n",
            "Choice E: avoid\n",
            "Answer:\n",
            "======================================== QA Instruction Prompt ========================================\n",
            "Answer the following question with A, B, C, D, or E.\n",
            "Question: The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
            "Choice A: ignore\n",
            "Choice B: enforce\n",
            "Choice C: authoritarian\n",
            "Choice D: yell at\n",
            "Choice E: avoid\n",
            "Answer:\n",
            "======================================== Few Shot Prompt ========================================\n",
            "Answer the following question with A, B, C, D, or E.\n",
            "Question: The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
            "Choice A: ignore\n",
            "Choice B: enforce\n",
            "Choice C: authoritarian\n",
            "Choice D: yell at\n",
            "Choice E: avoid\n",
            "Answer: A\n",
            "Question: Sammy wanted to go to where the people were.  Where might he go?\n",
            "Choice A: race track\n",
            "Choice B: populated areas\n",
            "Choice C: the desert\n",
            "Choice D: apartment\n",
            "Choice E: roadblock\n",
            "Answer: B\n",
            "Question: To locate a choker not located in a jewelry box or boutique where would you go?\n",
            "Choice A: jewelry store\n",
            "Choice B: neck\n",
            "Choice C: jewlery box\n",
            "Choice D: jewelry box\n",
            "Choice E: boutique\n",
            "Answer: A\n",
            "Question: Google Maps and other highway and street GPS services have replaced what?\n",
            "Choice A: united states\n",
            "Choice B: mexico\n",
            "Choice C: countryside\n",
            "Choice D: atlas\n",
            "Choice E: oceans\n",
            "Answer: D\n",
            "Question: What do people use to absorb extra ink from a fountain pen?\n",
            "Choice A: shirt pocket\n",
            "Choice B: calligrapher's hand\n",
            "Choice C: inkwell\n",
            "Choice D: desk drawer\n",
            "Choice E: blotter\n",
            "Answer:\n",
            "======================================== Incorrect Few Shot Prompt ========================================\n",
            "Answer the following question with A, B, C, D, or E.\n",
            "The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
            " A ignore\n",
            " B enforce\n",
            " C authoritarian\n",
            " D yell at\n",
            " E avoid\n",
            "E\n",
            "Sammy wanted to go to where the people were.  Where might he go?\n",
            " A race track\n",
            " B populated areas\n",
            " C the desert\n",
            " D apartment\n",
            " E roadblock\n",
            "E\n",
            "To locate a choker not located in a jewelry box or boutique where would you go?\n",
            " A jewelry store\n",
            " B neck\n",
            " C jewlery box\n",
            " D jewelry box\n",
            " E boutique\n",
            "B\n",
            "Google Maps and other highway and street GPS services have replaced what?\n",
            " A united states\n",
            " B mexico\n",
            " C countryside\n",
            " D atlas\n",
            " E oceans\n",
            "C\n",
            "Question: What do people use to absorb extra ink from a fountain pen?\n",
            "Choice A: shirt pocket\n",
            "Choice B: calligrapher's hand\n",
            "Choice C: inkwell\n",
            "Choice D: desk drawer\n",
            "Choice E: blotter\n",
            "Answer:\n"
          ]
        }
      ],
      "source": [
        "# Print one example of each prompt type\n",
        "print('='*40, 'Simple Prompt', '='*40)\n",
        "print(make_simple_prompt(train_set[0]))\n",
        "print('='*40, 'Simple QA Prompt', '='*40)\n",
        "print(make_simple_qa_prompt(train_set[0]))\n",
        "print('='*40, 'QA Instruction Prompt', '='*40)\n",
        "print(make_qa_instruction_prompt(train_set[0]))\n",
        "print('='*40, 'Few Shot Prompt', '='*40)\n",
        "print(make_few_shot_prompt(train_set[8], 4))\n",
        "print('='*40, 'Incorrect Few Shot Prompt', '='*40)\n",
        "print(make_incorrect_few_shot_prompt(train_set[8], 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NxHU7rQU7re"
      },
      "source": [
        "# Running the model\n",
        "\n",
        "The cells below contain code to query Cohere's command-xlarge-nightly model. You can read more about it here: https://docs.cohere.ai/reference/generate.\n",
        "\n",
        "The algorithm is as follows:\n",
        "1. Format the multiple-choice question as a prompt such that the expected continuation is the answer to the question.\n",
        "2. Query the model with the prompt.\n",
        "3. Parse the model's response to extract the answer.\n",
        "\n",
        "\n",
        "Since we are using the trial keys, we are rate limited to 5 queries per minute. We have implemented the code to wait for a minute if you get close to the rate limit. If you run into rate limits, just wait a bit and retry. (Previous queries will stay in the cache, so you won't have to re-query them.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "i87UtnWVU7rf"
      },
      "outputs": [],
      "source": [
        "# Parameters for the query.\n",
        "# Run with the defaults first, then if you want try changing them to see how it affects the results\n",
        "model = \"command-xlarge-nightly\"  # This is the biggest (and most expensive) model. You can also try \"xlarge\" or \"medium\"\n",
        "temperature = 0 # Control randomness. For more randomness, set to a higher value. For QA, we recommend 0.0\n",
        "max_tokens = 1 # Only generate 1 token (the answer). The Cohere API\n",
        "num_generations = 5 # How many outputs to generate\n",
        "return_likelihoods = 'GENERATION'  # Return the likelihoods for the generations\n",
        "\n",
        "# To avoid errors due to hitting API rate limits, we'll maintain a running tracker of queries made in the last minute\n",
        "MAX_QUERIES = 5   # 5 queries per 60s in the cohere free tier\n",
        "QUERY_TIMEWINDOW = 60 # seconds\n",
        "timestamps = deque()\n",
        "\n",
        "def query_cohere(prompt):\n",
        "    response = co.generate(\n",
        "      model=model,\n",
        "      prompt=prompt,\n",
        "      temperature=temperature,\n",
        "      max_tokens=max_tokens,\n",
        "      num_generations=num_generations,\n",
        "      return_likelihoods=return_likelihoods,\n",
        "    )\n",
        "    return response\n",
        "\n",
        "\n",
        "# We will store the results in a cache so we don't have to query Cohere every time\n",
        "# This will avoid hitting the API rate limit\n",
        "cache = {}\n",
        "if os.path.exists('cache.pkl'):\n",
        "    with open('cache.pkl', 'rb') as f:\n",
        "        cache = pkl.load(f)\n",
        "\n",
        "def post_process_response_text(response):\n",
        "    \"\"\"Removes trailing and preceding whitespace and newlines. Returns the first character of the response.\"\"\"\n",
        "    r = response.strip()\n",
        "    if r:\n",
        "        return r[0]\n",
        "    return r\n",
        "\n",
        "# Return the log probability of the correct answer and the most probable answer\n",
        "def query_model(prompt, correct_answer):\n",
        "    global query_count, timestamps\n",
        "    # Check if the query is in the cache\n",
        "    inputs = (prompt, correct_answer, model, temperature, max_tokens, return_likelihoods)\n",
        "    if inputs in cache:\n",
        "        response = cache[inputs]\n",
        "    else:\n",
        "        # If more than 5 queries have been run in the last 60s, wait for cooldown\n",
        "        if len(timestamps) >= MAX_QUERIES and time.time() - timestamps[0] <= QUERY_TIMEWINDOW:\n",
        "            print(\"Sleeping for a minute to cooldown API limits.\")\n",
        "            time.sleep(60)\n",
        "            timestamps.clear()\n",
        "\n",
        "        # Run query\n",
        "        response = query_cohere(prompt)\n",
        "\n",
        "        # Update timestamps\n",
        "        timestamps.append(time.time())\n",
        "        if len(timestamps) > MAX_QUERIES:\n",
        "            timestamps.popleft()\n",
        "\n",
        "        # Cache inputs\n",
        "        cache[inputs] = response\n",
        "        # Save cache to file\n",
        "        with open('cache.pkl', 'wb') as f:\n",
        "            pkl.dump(cache, f)\n",
        "    log_prob = response.data[0].likelihood\n",
        "    most_probable = post_process_response_text(response.data[0].text)\n",
        "    return log_prob, most_probable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6q2Q8Sf6U7rf"
      },
      "outputs": [],
      "source": [
        "# For simplicity (and to save time), we'll only use the first 10 data points\n",
        "# You can get more reliable results by using more data points, but it will\n",
        "# take longer to run because of API rate limits.\n",
        "num_points = 10\n",
        "mini_train = val_set[:num_points]\n",
        "num_shots = 4\n",
        "\n",
        "# Consider adding additional prompts of your own here.\n",
        "prompt_strategies = {\n",
        "    'simple': make_simple_prompt,\n",
        "    'simple_qa': make_simple_qa_prompt,\n",
        "    'qa_instruction': make_qa_instruction_prompt,\n",
        "    'few_shot': lambda x: make_few_shot_prompt(x, num_shots),\n",
        "    'incorrect_few_shot': lambda x: make_incorrect_few_shot_prompt(x, num_shots)\n",
        "}\n",
        "\n",
        "def compute_acc(data_points, prompt_strategy):\n",
        "    accuracies = []\n",
        "    valid_responses = []\n",
        "    for data_point in data_points:\n",
        "        print(f'Question: {data_point[\"question\"][\"stem\"]}, Answer: {data_point[\"answerKey\"]}, Choices: {[choice[\"text\"] for choice in data_point[\"question\"][\"choices\"]]}')\n",
        "        prompt = prompt_strategy(data_point)\n",
        "        correct_answer = f'{data_point[\"answerKey\"]}'\n",
        "\n",
        "        log_prob, most_probable = query_model(prompt, correct_answer)\n",
        "        accuracies += [int(most_probable == correct_answer)]\n",
        "        valid_responses += [most_probable in ['A', 'B', 'C', 'D', 'E']]\n",
        "        print(f'   LM predicted |{most_probable}|, accuracy: {accuracies[-1]}')\n",
        "    return np.mean(accuracies), np.mean(valid_responses)\n",
        "\n",
        "def plot_all_accs(data_points, prompt_strategies):\n",
        "    accuracies = []\n",
        "    valid_responses = []\n",
        "    for prompt_name, prompt_strategy in prompt_strategies.items():\n",
        "        accuracy, valid_rate = compute_acc(data_points, prompt_strategy)\n",
        "        accuracies += [accuracy]\n",
        "        valid_responses += [valid_rate]\n",
        "    # Plot a bar chart of the accuracies\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(prompt_strategies.keys(), accuracies)\n",
        "    plt.title('Accuracies')\n",
        "    plt.show()\n",
        "    # Plot a bar chart of the valid responses\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(prompt_strategies.keys(), valid_responses)\n",
        "    plt.title('Valid Responses')\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NlZ9rNdCU7rg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?, Answer: A, Choices: ['bank', 'library', 'department store', 'mall', 'new york']\n",
            "   LM predicted |E|, accuracy: 0\n",
            "Question: What do people aim to do at work?, Answer: A, Choices: ['complete job', 'learn from each other', 'kill animals', 'wear hats', 'talk to each other']\n",
            "   LM predicted |A|, accuracy: 1\n",
            "Question: Where would you find magazines along side many other printed works?, Answer: B, Choices: ['doctor', 'bookstore', 'market', 'train station', 'mortuary']\n",
            "   LM predicted |B|, accuracy: 1\n",
            "Question: Where are  you likely to find a hamburger?, Answer: A, Choices: ['fast food restaurant', 'pizza', 'ground up dead cows', 'mouth', 'cow carcus']\n",
            "   LM predicted |A|, accuracy: 1\n",
            "Question: James was looking for a good place to buy farmland.  Where might he look?, Answer: A, Choices: ['midwest', 'countryside', 'estate', 'farming areas', 'illinois']\n",
            "   LM predicted |E|, accuracy: 0\n",
            "Question: What island country is ferret popular?, Answer: C, Choices: ['own home', 'north carolina', 'great britain', 'hutch', 'outdoors']\n",
            "   LM predicted |E|, accuracy: 0\n",
            "Question: In what Spanish speaking North American country can you get a great cup of coffee?, Answer: B, Choices: [\"mildred's coffee shop\", 'mexico', 'diner', 'kitchen', 'canteen']\n",
            "   LM predicted |B|, accuracy: 1\n",
            "Question: What do animals do when an enemy is approaching?, Answer: D, Choices: ['feel pleasure', 'procreate', 'pass water', 'listen to each other', 'sing']\n",
            "   LM predicted |D|, accuracy: 1\n",
            "Question: Reading newspaper one of many ways to practice your what?, Answer: A, Choices: ['literacy', 'knowing how to read', 'money', 'buying', 'money bank']\n",
            "   LM predicted |A|, accuracy: 1\n",
            "Question: What do people typically do while playing guitar?, Answer: C, Choices: ['cry', 'hear sounds', 'singing', 'arthritis', 'making music']\n",
            "   LM predicted |E|, accuracy: 0\n",
            "Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?, Answer: A, Choices: ['bank', 'library', 'department store', 'mall', 'new york']\n",
            "   LM predicted |A|, accuracy: 1\n",
            "Question: What do people aim to do at work?, Answer: A, Choices: ['complete job', 'learn from each other', 'kill animals', 'wear hats', 'talk to each other']\n",
            "   LM predicted |C|, accuracy: 0\n",
            "Question: Where would you find magazines along side many other printed works?, Answer: B, Choices: ['doctor', 'bookstore', 'market', 'train station', 'mortuary']\n",
            "   LM predicted |B|, accuracy: 1\n",
            "Question: Where are  you likely to find a hamburger?, Answer: A, Choices: ['fast food restaurant', 'pizza', 'ground up dead cows', 'mouth', 'cow carcus']\n",
            "   LM predicted |A|, accuracy: 1\n",
            "Question: James was looking for a good place to buy farmland.  Where might he look?, Answer: A, Choices: ['midwest', 'countryside', 'estate', 'farming areas', 'illinois']\n",
            "   LM predicted |C|, accuracy: 0\n",
            "Question: What island country is ferret popular?, Answer: C, Choices: ['own home', 'north carolina', 'great britain', 'hutch', 'outdoors']\n",
            "   LM predicted |C|, accuracy: 1\n",
            "Question: In what Spanish speaking North American country can you get a great cup of coffee?, Answer: B, Choices: [\"mildred's coffee shop\", 'mexico', 'diner', 'kitchen', 'canteen']\n",
            "   LM predicted |B|, accuracy: 1\n",
            "Question: What do animals do when an enemy is approaching?, Answer: D, Choices: ['feel pleasure', 'procreate', 'pass water', 'listen to each other', 'sing']\n",
            "   LM predicted |D|, accuracy: 1\n",
            "Question: Reading newspaper one of many ways to practice your what?, Answer: A, Choices: ['literacy', 'knowing how to read', 'money', 'buying', 'money bank']\n",
            "   LM predicted |A|, accuracy: 1\n",
            "Question: What do people typically do while playing guitar?, Answer: C, Choices: ['cry', 'hear sounds', 'singing', 'arthritis', 'making music']\n",
            "   LM predicted |E|, accuracy: 0\n",
            "Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?, Answer: A, Choices: ['bank', 'library', 'department store', 'mall', 'new york']\n",
            "   LM predicted |A|, accuracy: 1\n",
            "Question: What do people aim to do at work?, Answer: A, Choices: ['complete job', 'learn from each other', 'kill animals', 'wear hats', 'talk to each other']\n",
            "   LM predicted |B|, accuracy: 0\n",
            "Question: Where would you find magazines along side many other printed works?, Answer: B, Choices: ['doctor', 'bookstore', 'market', 'train station', 'mortuary']\n",
            "   LM predicted |B|, accuracy: 1\n",
            "Question: Where are  you likely to find a hamburger?, Answer: A, Choices: ['fast food restaurant', 'pizza', 'ground up dead cows', 'mouth', 'cow carcus']\n",
            "   LM predicted |A|, accuracy: 1\n",
            "Question: James was looking for a good place to buy farmland.  Where might he look?, Answer: A, Choices: ['midwest', 'countryside', 'estate', 'farming areas', 'illinois']\n",
            "   LM predicted |D|, accuracy: 0\n",
            "Question: What island country is ferret popular?, Answer: C, Choices: ['own home', 'north carolina', 'great britain', 'hutch', 'outdoors']\n",
            "   LM predicted |C|, accuracy: 1\n",
            "Question: In what Spanish speaking North American country can you get a great cup of coffee?, Answer: B, Choices: [\"mildred's coffee shop\", 'mexico', 'diner', 'kitchen', 'canteen']\n",
            "   LM predicted |B|, accuracy: 1\n",
            "Question: What do animals do when an enemy is approaching?, Answer: D, Choices: ['feel pleasure', 'procreate', 'pass water', 'listen to each other', 'sing']\n",
            "   LM predicted |D|, accuracy: 1\n",
            "Question: Reading newspaper one of many ways to practice your what?, Answer: A, Choices: ['literacy', 'knowing how to read', 'money', 'buying', 'money bank']\n",
            "   LM predicted |A|, accuracy: 1\n",
            "Question: What do people typically do while playing guitar?, Answer: C, Choices: ['cry', 'hear sounds', 'singing', 'arthritis', 'making music']\n",
            "   LM predicted |E|, accuracy: 0\n",
            "Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?, Answer: A, Choices: ['bank', 'library', 'department store', 'mall', 'new york']\n",
            "   LM predicted |A|, accuracy: 1\n",
            "Question: What do people aim to do at work?, Answer: A, Choices: ['complete job', 'learn from each other', 'kill animals', 'wear hats', 'talk to each other']\n",
            "   LM predicted |A|, accuracy: 1\n",
            "Question: Where would you find magazines along side many other printed works?, Answer: B, Choices: ['doctor', 'bookstore', 'market', 'train station', 'mortuary']\n",
            "   LM predicted |B|, accuracy: 1\n",
            "Question: Where are  you likely to find a hamburger?, Answer: A, Choices: ['fast food restaurant', 'pizza', 'ground up dead cows', 'mouth', 'cow carcus']\n",
            "   LM predicted |A|, accuracy: 1\n",
            "Question: James was looking for a good place to buy farmland.  Where might he look?, Answer: A, Choices: ['midwest', 'countryside', 'estate', 'farming areas', 'illinois']\n",
            "   LM predicted |D|, accuracy: 0\n",
            "Question: What island country is ferret popular?, Answer: C, Choices: ['own home', 'north carolina', 'great britain', 'hutch', 'outdoors']\n",
            "   LM predicted |C|, accuracy: 1\n",
            "Question: In what Spanish speaking North American country can you get a great cup of coffee?, Answer: B, Choices: [\"mildred's coffee shop\", 'mexico', 'diner', 'kitchen', 'canteen']\n",
            "   LM predicted |B|, accuracy: 1\n",
            "Question: What do animals do when an enemy is approaching?, Answer: D, Choices: ['feel pleasure', 'procreate', 'pass water', 'listen to each other', 'sing']\n",
            "   LM predicted |D|, accuracy: 1\n",
            "Question: Reading newspaper one of many ways to practice your what?, Answer: A, Choices: ['literacy', 'knowing how to read', 'money', 'buying', 'money bank']\n",
            "   LM predicted |A|, accuracy: 1\n",
            "Question: What do people typically do while playing guitar?, Answer: C, Choices: ['cry', 'hear sounds', 'singing', 'arthritis', 'making music']\n",
            "   LM predicted |E|, accuracy: 0\n",
            "Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?, Answer: A, Choices: ['bank', 'library', 'department store', 'mall', 'new york']\n",
            "   LM predicted |A|, accuracy: 1\n",
            "Question: What do people aim to do at work?, Answer: A, Choices: ['complete job', 'learn from each other', 'kill animals', 'wear hats', 'talk to each other']\n",
            "   LM predicted |B|, accuracy: 0\n",
            "Question: Where would you find magazines along side many other printed works?, Answer: B, Choices: ['doctor', 'bookstore', 'market', 'train station', 'mortuary']\n",
            "   LM predicted |B|, accuracy: 1\n",
            "Question: Where are  you likely to find a hamburger?, Answer: A, Choices: ['fast food restaurant', 'pizza', 'ground up dead cows', 'mouth', 'cow carcus']\n",
            "   LM predicted |A|, accuracy: 1\n",
            "Question: James was looking for a good place to buy farmland.  Where might he look?, Answer: A, Choices: ['midwest', 'countryside', 'estate', 'farming areas', 'illinois']\n",
            "   LM predicted |D|, accuracy: 0\n",
            "Question: What island country is ferret popular?, Answer: C, Choices: ['own home', 'north carolina', 'great britain', 'hutch', 'outdoors']\n",
            "   LM predicted |C|, accuracy: 1\n",
            "Question: In what Spanish speaking North American country can you get a great cup of coffee?, Answer: B, Choices: [\"mildred's coffee shop\", 'mexico', 'diner', 'kitchen', 'canteen']\n",
            "   LM predicted |A|, accuracy: 0\n",
            "Question: What do animals do when an enemy is approaching?, Answer: D, Choices: ['feel pleasure', 'procreate', 'pass water', 'listen to each other', 'sing']\n",
            "   LM predicted |D|, accuracy: 1\n",
            "Question: Reading newspaper one of many ways to practice your what?, Answer: A, Choices: ['literacy', 'knowing how to read', 'money', 'buying', 'money bank']\n",
            "   LM predicted |A|, accuracy: 1\n",
            "Question: What do people typically do while playing guitar?, Answer: C, Choices: ['cry', 'hear sounds', 'singing', 'arthritis', 'making music']\n",
            "   LM predicted |E|, accuracy: 0\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_all_accs(mini_train, prompt_strategies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic7JH8WTU7rg"
      },
      "source": [
        "# Analysis\n",
        "\n",
        "TODO: in the cells below, implement code to analyze the model's performance.\n",
        "* What kinds of failures do you see with different prompting strategies?\n",
        "* Does providing correct labels in few-shot prompting have a significant impact on accuracy?\n",
        "* Observe the models log probabilities. Does it seem more confident when it is correct than when it is incorrect?\n",
        "\n",
        "A function to plot the model's confidence has been implemented for you, but you should feel free to write code to do additional analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BWnuRosfU7rg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\n",
            "Choice A: bank\n",
            "Choice B: library\n",
            "Choice C: department store\n",
            "Choice D: mall\n",
            "Choice E: new york\n",
            "Answer:\n",
            "LM predicted |A|, correct answer: A\n",
            "Prompt: Question: What do people aim to do at work?\n",
            "Choice A: complete job\n",
            "Choice B: learn from each other\n",
            "Choice C: kill animals\n",
            "Choice D: wear hats\n",
            "Choice E: talk to each other\n",
            "Answer:\n",
            "LM predicted |C|, correct answer: A\n",
            "Prompt: Question: Where would you find magazines along side many other printed works?\n",
            "Choice A: doctor\n",
            "Choice B: bookstore\n",
            "Choice C: market\n",
            "Choice D: train station\n",
            "Choice E: mortuary\n",
            "Answer:\n",
            "LM predicted |B|, correct answer: B\n",
            "Prompt: Question: Where are  you likely to find a hamburger?\n",
            "Choice A: fast food restaurant\n",
            "Choice B: pizza\n",
            "Choice C: ground up dead cows\n",
            "Choice D: mouth\n",
            "Choice E: cow carcus\n",
            "Answer:\n",
            "LM predicted |A|, correct answer: A\n",
            "Prompt: Question: James was looking for a good place to buy farmland.  Where might he look?\n",
            "Choice A: midwest\n",
            "Choice B: countryside\n",
            "Choice C: estate\n",
            "Choice D: farming areas\n",
            "Choice E: illinois\n",
            "Answer:\n",
            "LM predicted |C|, correct answer: A\n",
            "Prompt: Question: What island country is ferret popular?\n",
            "Choice A: own home\n",
            "Choice B: north carolina\n",
            "Choice C: great britain\n",
            "Choice D: hutch\n",
            "Choice E: outdoors\n",
            "Answer:\n",
            "LM predicted |C|, correct answer: C\n",
            "Prompt: Question: In what Spanish speaking North American country can you get a great cup of coffee?\n",
            "Choice A: mildred's coffee shop\n",
            "Choice B: mexico\n",
            "Choice C: diner\n",
            "Choice D: kitchen\n",
            "Choice E: canteen\n",
            "Answer:\n",
            "LM predicted |B|, correct answer: B\n",
            "Prompt: Question: What do animals do when an enemy is approaching?\n",
            "Choice A: feel pleasure\n",
            "Choice B: procreate\n",
            "Choice C: pass water\n",
            "Choice D: listen to each other\n",
            "Choice E: sing\n",
            "Answer:\n",
            "LM predicted |D|, correct answer: D\n",
            "Prompt: Question: Reading newspaper one of many ways to practice your what?\n",
            "Choice A: literacy\n",
            "Choice B: knowing how to read\n",
            "Choice C: money\n",
            "Choice D: buying\n",
            "Choice E: money bank\n",
            "Answer:\n",
            "LM predicted |A|, correct answer: A\n",
            "Prompt: Question: What do people typically do while playing guitar?\n",
            "Choice A: cry\n",
            "Choice B: hear sounds\n",
            "Choice C: singing\n",
            "Choice D: arthritis\n",
            "Choice E: making music\n",
            "Answer:\n",
            "LM predicted |E|, correct answer: C\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\n",
            "Choice A: bank\n",
            "Choice B: library\n",
            "Choice C: department store\n",
            "Choice D: mall\n",
            "Choice E: new york\n",
            "Answer:\n",
            "LM predicted |A|, correct answer: A\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "Question: What do people aim to do at work?\n",
            "Choice A: complete job\n",
            "Choice B: learn from each other\n",
            "Choice C: kill animals\n",
            "Choice D: wear hats\n",
            "Choice E: talk to each other\n",
            "Answer:\n",
            "LM predicted |B|, correct answer: A\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "Question: Where would you find magazines along side many other printed works?\n",
            "Choice A: doctor\n",
            "Choice B: bookstore\n",
            "Choice C: market\n",
            "Choice D: train station\n",
            "Choice E: mortuary\n",
            "Answer:\n",
            "LM predicted |B|, correct answer: B\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "Question: Where are  you likely to find a hamburger?\n",
            "Choice A: fast food restaurant\n",
            "Choice B: pizza\n",
            "Choice C: ground up dead cows\n",
            "Choice D: mouth\n",
            "Choice E: cow carcus\n",
            "Answer:\n",
            "LM predicted |A|, correct answer: A\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "Question: James was looking for a good place to buy farmland.  Where might he look?\n",
            "Choice A: midwest\n",
            "Choice B: countryside\n",
            "Choice C: estate\n",
            "Choice D: farming areas\n",
            "Choice E: illinois\n",
            "Answer:\n",
            "LM predicted |D|, correct answer: A\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "Question: What island country is ferret popular?\n",
            "Choice A: own home\n",
            "Choice B: north carolina\n",
            "Choice C: great britain\n",
            "Choice D: hutch\n",
            "Choice E: outdoors\n",
            "Answer:\n",
            "LM predicted |C|, correct answer: C\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "Question: In what Spanish speaking North American country can you get a great cup of coffee?\n",
            "Choice A: mildred's coffee shop\n",
            "Choice B: mexico\n",
            "Choice C: diner\n",
            "Choice D: kitchen\n",
            "Choice E: canteen\n",
            "Answer:\n",
            "LM predicted |B|, correct answer: B\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "Question: What do animals do when an enemy is approaching?\n",
            "Choice A: feel pleasure\n",
            "Choice B: procreate\n",
            "Choice C: pass water\n",
            "Choice D: listen to each other\n",
            "Choice E: sing\n",
            "Answer:\n",
            "LM predicted |D|, correct answer: D\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "Question: Reading newspaper one of many ways to practice your what?\n",
            "Choice A: literacy\n",
            "Choice B: knowing how to read\n",
            "Choice C: money\n",
            "Choice D: buying\n",
            "Choice E: money bank\n",
            "Answer:\n",
            "LM predicted |A|, correct answer: A\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "Question: What do people typically do while playing guitar?\n",
            "Choice A: cry\n",
            "Choice B: hear sounds\n",
            "Choice C: singing\n",
            "Choice D: arthritis\n",
            "Choice E: making music\n",
            "Answer:\n",
            "LM predicted |E|, correct answer: C\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "Question: The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
            "Choice A: ignore\n",
            "Choice B: enforce\n",
            "Choice C: authoritarian\n",
            "Choice D: yell at\n",
            "Choice E: avoid\n",
            "Answer: A\n",
            "Question: Sammy wanted to go to where the people were.  Where might he go?\n",
            "Choice A: race track\n",
            "Choice B: populated areas\n",
            "Choice C: the desert\n",
            "Choice D: apartment\n",
            "Choice E: roadblock\n",
            "Answer: B\n",
            "Question: To locate a choker not located in a jewelry box or boutique where would you go?\n",
            "Choice A: jewelry store\n",
            "Choice B: neck\n",
            "Choice C: jewlery box\n",
            "Choice D: jewelry box\n",
            "Choice E: boutique\n",
            "Answer: A\n",
            "Question: Google Maps and other highway and street GPS services have replaced what?\n",
            "Choice A: united states\n",
            "Choice B: mexico\n",
            "Choice C: countryside\n",
            "Choice D: atlas\n",
            "Choice E: oceans\n",
            "Answer: D\n",
            "Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\n",
            "Choice A: bank\n",
            "Choice B: library\n",
            "Choice C: department store\n",
            "Choice D: mall\n",
            "Choice E: new york\n",
            "Answer:\n",
            "LM predicted |A|, correct answer: A\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "Question: The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
            "Choice A: ignore\n",
            "Choice B: enforce\n",
            "Choice C: authoritarian\n",
            "Choice D: yell at\n",
            "Choice E: avoid\n",
            "Answer: A\n",
            "Question: Sammy wanted to go to where the people were.  Where might he go?\n",
            "Choice A: race track\n",
            "Choice B: populated areas\n",
            "Choice C: the desert\n",
            "Choice D: apartment\n",
            "Choice E: roadblock\n",
            "Answer: B\n",
            "Question: To locate a choker not located in a jewelry box or boutique where would you go?\n",
            "Choice A: jewelry store\n",
            "Choice B: neck\n",
            "Choice C: jewlery box\n",
            "Choice D: jewelry box\n",
            "Choice E: boutique\n",
            "Answer: A\n",
            "Question: Google Maps and other highway and street GPS services have replaced what?\n",
            "Choice A: united states\n",
            "Choice B: mexico\n",
            "Choice C: countryside\n",
            "Choice D: atlas\n",
            "Choice E: oceans\n",
            "Answer: D\n",
            "Question: What do people aim to do at work?\n",
            "Choice A: complete job\n",
            "Choice B: learn from each other\n",
            "Choice C: kill animals\n",
            "Choice D: wear hats\n",
            "Choice E: talk to each other\n",
            "Answer:\n",
            "LM predicted |A|, correct answer: A\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "Question: The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
            "Choice A: ignore\n",
            "Choice B: enforce\n",
            "Choice C: authoritarian\n",
            "Choice D: yell at\n",
            "Choice E: avoid\n",
            "Answer: A\n",
            "Question: Sammy wanted to go to where the people were.  Where might he go?\n",
            "Choice A: race track\n",
            "Choice B: populated areas\n",
            "Choice C: the desert\n",
            "Choice D: apartment\n",
            "Choice E: roadblock\n",
            "Answer: B\n",
            "Question: To locate a choker not located in a jewelry box or boutique where would you go?\n",
            "Choice A: jewelry store\n",
            "Choice B: neck\n",
            "Choice C: jewlery box\n",
            "Choice D: jewelry box\n",
            "Choice E: boutique\n",
            "Answer: A\n",
            "Question: Google Maps and other highway and street GPS services have replaced what?\n",
            "Choice A: united states\n",
            "Choice B: mexico\n",
            "Choice C: countryside\n",
            "Choice D: atlas\n",
            "Choice E: oceans\n",
            "Answer: D\n",
            "Question: Where would you find magazines along side many other printed works?\n",
            "Choice A: doctor\n",
            "Choice B: bookstore\n",
            "Choice C: market\n",
            "Choice D: train station\n",
            "Choice E: mortuary\n",
            "Answer:\n",
            "LM predicted |B|, correct answer: B\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "Question: The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
            "Choice A: ignore\n",
            "Choice B: enforce\n",
            "Choice C: authoritarian\n",
            "Choice D: yell at\n",
            "Choice E: avoid\n",
            "Answer: A\n",
            "Question: Sammy wanted to go to where the people were.  Where might he go?\n",
            "Choice A: race track\n",
            "Choice B: populated areas\n",
            "Choice C: the desert\n",
            "Choice D: apartment\n",
            "Choice E: roadblock\n",
            "Answer: B\n",
            "Question: To locate a choker not located in a jewelry box or boutique where would you go?\n",
            "Choice A: jewelry store\n",
            "Choice B: neck\n",
            "Choice C: jewlery box\n",
            "Choice D: jewelry box\n",
            "Choice E: boutique\n",
            "Answer: A\n",
            "Question: Google Maps and other highway and street GPS services have replaced what?\n",
            "Choice A: united states\n",
            "Choice B: mexico\n",
            "Choice C: countryside\n",
            "Choice D: atlas\n",
            "Choice E: oceans\n",
            "Answer: D\n",
            "Question: Where are  you likely to find a hamburger?\n",
            "Choice A: fast food restaurant\n",
            "Choice B: pizza\n",
            "Choice C: ground up dead cows\n",
            "Choice D: mouth\n",
            "Choice E: cow carcus\n",
            "Answer:\n",
            "LM predicted |A|, correct answer: A\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "Question: The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
            "Choice A: ignore\n",
            "Choice B: enforce\n",
            "Choice C: authoritarian\n",
            "Choice D: yell at\n",
            "Choice E: avoid\n",
            "Answer: A\n",
            "Question: Sammy wanted to go to where the people were.  Where might he go?\n",
            "Choice A: race track\n",
            "Choice B: populated areas\n",
            "Choice C: the desert\n",
            "Choice D: apartment\n",
            "Choice E: roadblock\n",
            "Answer: B\n",
            "Question: To locate a choker not located in a jewelry box or boutique where would you go?\n",
            "Choice A: jewelry store\n",
            "Choice B: neck\n",
            "Choice C: jewlery box\n",
            "Choice D: jewelry box\n",
            "Choice E: boutique\n",
            "Answer: A\n",
            "Question: Google Maps and other highway and street GPS services have replaced what?\n",
            "Choice A: united states\n",
            "Choice B: mexico\n",
            "Choice C: countryside\n",
            "Choice D: atlas\n",
            "Choice E: oceans\n",
            "Answer: D\n",
            "Question: James was looking for a good place to buy farmland.  Where might he look?\n",
            "Choice A: midwest\n",
            "Choice B: countryside\n",
            "Choice C: estate\n",
            "Choice D: farming areas\n",
            "Choice E: illinois\n",
            "Answer:\n",
            "LM predicted |D|, correct answer: A\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "Question: The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
            "Choice A: ignore\n",
            "Choice B: enforce\n",
            "Choice C: authoritarian\n",
            "Choice D: yell at\n",
            "Choice E: avoid\n",
            "Answer: A\n",
            "Question: Sammy wanted to go to where the people were.  Where might he go?\n",
            "Choice A: race track\n",
            "Choice B: populated areas\n",
            "Choice C: the desert\n",
            "Choice D: apartment\n",
            "Choice E: roadblock\n",
            "Answer: B\n",
            "Question: To locate a choker not located in a jewelry box or boutique where would you go?\n",
            "Choice A: jewelry store\n",
            "Choice B: neck\n",
            "Choice C: jewlery box\n",
            "Choice D: jewelry box\n",
            "Choice E: boutique\n",
            "Answer: A\n",
            "Question: Google Maps and other highway and street GPS services have replaced what?\n",
            "Choice A: united states\n",
            "Choice B: mexico\n",
            "Choice C: countryside\n",
            "Choice D: atlas\n",
            "Choice E: oceans\n",
            "Answer: D\n",
            "Question: What island country is ferret popular?\n",
            "Choice A: own home\n",
            "Choice B: north carolina\n",
            "Choice C: great britain\n",
            "Choice D: hutch\n",
            "Choice E: outdoors\n",
            "Answer:\n",
            "LM predicted |C|, correct answer: C\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "Question: The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
            "Choice A: ignore\n",
            "Choice B: enforce\n",
            "Choice C: authoritarian\n",
            "Choice D: yell at\n",
            "Choice E: avoid\n",
            "Answer: A\n",
            "Question: Sammy wanted to go to where the people were.  Where might he go?\n",
            "Choice A: race track\n",
            "Choice B: populated areas\n",
            "Choice C: the desert\n",
            "Choice D: apartment\n",
            "Choice E: roadblock\n",
            "Answer: B\n",
            "Question: To locate a choker not located in a jewelry box or boutique where would you go?\n",
            "Choice A: jewelry store\n",
            "Choice B: neck\n",
            "Choice C: jewlery box\n",
            "Choice D: jewelry box\n",
            "Choice E: boutique\n",
            "Answer: A\n",
            "Question: Google Maps and other highway and street GPS services have replaced what?\n",
            "Choice A: united states\n",
            "Choice B: mexico\n",
            "Choice C: countryside\n",
            "Choice D: atlas\n",
            "Choice E: oceans\n",
            "Answer: D\n",
            "Question: In what Spanish speaking North American country can you get a great cup of coffee?\n",
            "Choice A: mildred's coffee shop\n",
            "Choice B: mexico\n",
            "Choice C: diner\n",
            "Choice D: kitchen\n",
            "Choice E: canteen\n",
            "Answer:\n",
            "LM predicted |B|, correct answer: B\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "Question: The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
            "Choice A: ignore\n",
            "Choice B: enforce\n",
            "Choice C: authoritarian\n",
            "Choice D: yell at\n",
            "Choice E: avoid\n",
            "Answer: A\n",
            "Question: Sammy wanted to go to where the people were.  Where might he go?\n",
            "Choice A: race track\n",
            "Choice B: populated areas\n",
            "Choice C: the desert\n",
            "Choice D: apartment\n",
            "Choice E: roadblock\n",
            "Answer: B\n",
            "Question: To locate a choker not located in a jewelry box or boutique where would you go?\n",
            "Choice A: jewelry store\n",
            "Choice B: neck\n",
            "Choice C: jewlery box\n",
            "Choice D: jewelry box\n",
            "Choice E: boutique\n",
            "Answer: A\n",
            "Question: Google Maps and other highway and street GPS services have replaced what?\n",
            "Choice A: united states\n",
            "Choice B: mexico\n",
            "Choice C: countryside\n",
            "Choice D: atlas\n",
            "Choice E: oceans\n",
            "Answer: D\n",
            "Question: What do animals do when an enemy is approaching?\n",
            "Choice A: feel pleasure\n",
            "Choice B: procreate\n",
            "Choice C: pass water\n",
            "Choice D: listen to each other\n",
            "Choice E: sing\n",
            "Answer:\n",
            "LM predicted |D|, correct answer: D\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "Question: The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
            "Choice A: ignore\n",
            "Choice B: enforce\n",
            "Choice C: authoritarian\n",
            "Choice D: yell at\n",
            "Choice E: avoid\n",
            "Answer: A\n",
            "Question: Sammy wanted to go to where the people were.  Where might he go?\n",
            "Choice A: race track\n",
            "Choice B: populated areas\n",
            "Choice C: the desert\n",
            "Choice D: apartment\n",
            "Choice E: roadblock\n",
            "Answer: B\n",
            "Question: To locate a choker not located in a jewelry box or boutique where would you go?\n",
            "Choice A: jewelry store\n",
            "Choice B: neck\n",
            "Choice C: jewlery box\n",
            "Choice D: jewelry box\n",
            "Choice E: boutique\n",
            "Answer: A\n",
            "Question: Google Maps and other highway and street GPS services have replaced what?\n",
            "Choice A: united states\n",
            "Choice B: mexico\n",
            "Choice C: countryside\n",
            "Choice D: atlas\n",
            "Choice E: oceans\n",
            "Answer: D\n",
            "Question: Reading newspaper one of many ways to practice your what?\n",
            "Choice A: literacy\n",
            "Choice B: knowing how to read\n",
            "Choice C: money\n",
            "Choice D: buying\n",
            "Choice E: money bank\n",
            "Answer:\n",
            "LM predicted |A|, correct answer: A\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "Question: The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
            "Choice A: ignore\n",
            "Choice B: enforce\n",
            "Choice C: authoritarian\n",
            "Choice D: yell at\n",
            "Choice E: avoid\n",
            "Answer: A\n",
            "Question: Sammy wanted to go to where the people were.  Where might he go?\n",
            "Choice A: race track\n",
            "Choice B: populated areas\n",
            "Choice C: the desert\n",
            "Choice D: apartment\n",
            "Choice E: roadblock\n",
            "Answer: B\n",
            "Question: To locate a choker not located in a jewelry box or boutique where would you go?\n",
            "Choice A: jewelry store\n",
            "Choice B: neck\n",
            "Choice C: jewlery box\n",
            "Choice D: jewelry box\n",
            "Choice E: boutique\n",
            "Answer: A\n",
            "Question: Google Maps and other highway and street GPS services have replaced what?\n",
            "Choice A: united states\n",
            "Choice B: mexico\n",
            "Choice C: countryside\n",
            "Choice D: atlas\n",
            "Choice E: oceans\n",
            "Answer: D\n",
            "Question: What do people typically do while playing guitar?\n",
            "Choice A: cry\n",
            "Choice B: hear sounds\n",
            "Choice C: singing\n",
            "Choice D: arthritis\n",
            "Choice E: making music\n",
            "Answer:\n",
            "LM predicted |E|, correct answer: C\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
            " A ignore\n",
            " B enforce\n",
            " C authoritarian\n",
            " D yell at\n",
            " E avoid\n",
            "B\n",
            "Sammy wanted to go to where the people were.  Where might he go?\n",
            " A race track\n",
            " B populated areas\n",
            " C the desert\n",
            " D apartment\n",
            " E roadblock\n",
            "A\n",
            "To locate a choker not located in a jewelry box or boutique where would you go?\n",
            " A jewelry store\n",
            " B neck\n",
            " C jewlery box\n",
            " D jewelry box\n",
            " E boutique\n",
            "D\n",
            "Google Maps and other highway and street GPS services have replaced what?\n",
            " A united states\n",
            " B mexico\n",
            " C countryside\n",
            " D atlas\n",
            " E oceans\n",
            "E\n",
            "Question: A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\n",
            "Choice A: bank\n",
            "Choice B: library\n",
            "Choice C: department store\n",
            "Choice D: mall\n",
            "Choice E: new york\n",
            "Answer:\n",
            "LM predicted |A|, correct answer: A\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
            " A ignore\n",
            " B enforce\n",
            " C authoritarian\n",
            " D yell at\n",
            " E avoid\n",
            "B\n",
            "Sammy wanted to go to where the people were.  Where might he go?\n",
            " A race track\n",
            " B populated areas\n",
            " C the desert\n",
            " D apartment\n",
            " E roadblock\n",
            "D\n",
            "To locate a choker not located in a jewelry box or boutique where would you go?\n",
            " A jewelry store\n",
            " B neck\n",
            " C jewlery box\n",
            " D jewelry box\n",
            " E boutique\n",
            "D\n",
            "Google Maps and other highway and street GPS services have replaced what?\n",
            " A united states\n",
            " B mexico\n",
            " C countryside\n",
            " D atlas\n",
            " E oceans\n",
            "A\n",
            "Question: What do people aim to do at work?\n",
            "Choice A: complete job\n",
            "Choice B: learn from each other\n",
            "Choice C: kill animals\n",
            "Choice D: wear hats\n",
            "Choice E: talk to each other\n",
            "Answer:\n",
            "LM predicted |B|, correct answer: A\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
            " A ignore\n",
            " B enforce\n",
            " C authoritarian\n",
            " D yell at\n",
            " E avoid\n",
            "D\n",
            "Sammy wanted to go to where the people were.  Where might he go?\n",
            " A race track\n",
            " B populated areas\n",
            " C the desert\n",
            " D apartment\n",
            " E roadblock\n",
            "C\n",
            "To locate a choker not located in a jewelry box or boutique where would you go?\n",
            " A jewelry store\n",
            " B neck\n",
            " C jewlery box\n",
            " D jewelry box\n",
            " E boutique\n",
            "D\n",
            "Google Maps and other highway and street GPS services have replaced what?\n",
            " A united states\n",
            " B mexico\n",
            " C countryside\n",
            " D atlas\n",
            " E oceans\n",
            "E\n",
            "Question: Where would you find magazines along side many other printed works?\n",
            "Choice A: doctor\n",
            "Choice B: bookstore\n",
            "Choice C: market\n",
            "Choice D: train station\n",
            "Choice E: mortuary\n",
            "Answer:\n",
            "LM predicted |B|, correct answer: B\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
            " A ignore\n",
            " B enforce\n",
            " C authoritarian\n",
            " D yell at\n",
            " E avoid\n",
            "B\n",
            "Sammy wanted to go to where the people were.  Where might he go?\n",
            " A race track\n",
            " B populated areas\n",
            " C the desert\n",
            " D apartment\n",
            " E roadblock\n",
            "E\n",
            "To locate a choker not located in a jewelry box or boutique where would you go?\n",
            " A jewelry store\n",
            " B neck\n",
            " C jewlery box\n",
            " D jewelry box\n",
            " E boutique\n",
            "D\n",
            "Google Maps and other highway and street GPS services have replaced what?\n",
            " A united states\n",
            " B mexico\n",
            " C countryside\n",
            " D atlas\n",
            " E oceans\n",
            "B\n",
            "Question: Where are  you likely to find a hamburger?\n",
            "Choice A: fast food restaurant\n",
            "Choice B: pizza\n",
            "Choice C: ground up dead cows\n",
            "Choice D: mouth\n",
            "Choice E: cow carcus\n",
            "Answer:\n",
            "LM predicted |A|, correct answer: A\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
            " A ignore\n",
            " B enforce\n",
            " C authoritarian\n",
            " D yell at\n",
            " E avoid\n",
            "D\n",
            "Sammy wanted to go to where the people were.  Where might he go?\n",
            " A race track\n",
            " B populated areas\n",
            " C the desert\n",
            " D apartment\n",
            " E roadblock\n",
            "C\n",
            "To locate a choker not located in a jewelry box or boutique where would you go?\n",
            " A jewelry store\n",
            " B neck\n",
            " C jewlery box\n",
            " D jewelry box\n",
            " E boutique\n",
            "C\n",
            "Google Maps and other highway and street GPS services have replaced what?\n",
            " A united states\n",
            " B mexico\n",
            " C countryside\n",
            " D atlas\n",
            " E oceans\n",
            "B\n",
            "Question: James was looking for a good place to buy farmland.  Where might he look?\n",
            "Choice A: midwest\n",
            "Choice B: countryside\n",
            "Choice C: estate\n",
            "Choice D: farming areas\n",
            "Choice E: illinois\n",
            "Answer:\n",
            "LM predicted |D|, correct answer: A\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
            " A ignore\n",
            " B enforce\n",
            " C authoritarian\n",
            " D yell at\n",
            " E avoid\n",
            "B\n",
            "Sammy wanted to go to where the people were.  Where might he go?\n",
            " A race track\n",
            " B populated areas\n",
            " C the desert\n",
            " D apartment\n",
            " E roadblock\n",
            "D\n",
            "To locate a choker not located in a jewelry box or boutique where would you go?\n",
            " A jewelry store\n",
            " B neck\n",
            " C jewlery box\n",
            " D jewelry box\n",
            " E boutique\n",
            "E\n",
            "Google Maps and other highway and street GPS services have replaced what?\n",
            " A united states\n",
            " B mexico\n",
            " C countryside\n",
            " D atlas\n",
            " E oceans\n",
            "A\n",
            "Question: What island country is ferret popular?\n",
            "Choice A: own home\n",
            "Choice B: north carolina\n",
            "Choice C: great britain\n",
            "Choice D: hutch\n",
            "Choice E: outdoors\n",
            "Answer:\n",
            "Sleeping for a minute to cooldown API limits.\n",
            "LM predicted |C|, correct answer: C\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
            " A ignore\n",
            " B enforce\n",
            " C authoritarian\n",
            " D yell at\n",
            " E avoid\n",
            "B\n",
            "Sammy wanted to go to where the people were.  Where might he go?\n",
            " A race track\n",
            " B populated areas\n",
            " C the desert\n",
            " D apartment\n",
            " E roadblock\n",
            "C\n",
            "To locate a choker not located in a jewelry box or boutique where would you go?\n",
            " A jewelry store\n",
            " B neck\n",
            " C jewlery box\n",
            " D jewelry box\n",
            " E boutique\n",
            "C\n",
            "Google Maps and other highway and street GPS services have replaced what?\n",
            " A united states\n",
            " B mexico\n",
            " C countryside\n",
            " D atlas\n",
            " E oceans\n",
            "A\n",
            "Question: In what Spanish speaking North American country can you get a great cup of coffee?\n",
            "Choice A: mildred's coffee shop\n",
            "Choice B: mexico\n",
            "Choice C: diner\n",
            "Choice D: kitchen\n",
            "Choice E: canteen\n",
            "Answer:\n",
            "LM predicted |B|, correct answer: B\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
            " A ignore\n",
            " B enforce\n",
            " C authoritarian\n",
            " D yell at\n",
            " E avoid\n",
            "B\n",
            "Sammy wanted to go to where the people were.  Where might he go?\n",
            " A race track\n",
            " B populated areas\n",
            " C the desert\n",
            " D apartment\n",
            " E roadblock\n",
            "E\n",
            "To locate a choker not located in a jewelry box or boutique where would you go?\n",
            " A jewelry store\n",
            " B neck\n",
            " C jewlery box\n",
            " D jewelry box\n",
            " E boutique\n",
            "D\n",
            "Google Maps and other highway and street GPS services have replaced what?\n",
            " A united states\n",
            " B mexico\n",
            " C countryside\n",
            " D atlas\n",
            " E oceans\n",
            "B\n",
            "Question: What do animals do when an enemy is approaching?\n",
            "Choice A: feel pleasure\n",
            "Choice B: procreate\n",
            "Choice C: pass water\n",
            "Choice D: listen to each other\n",
            "Choice E: sing\n",
            "Answer:\n",
            "LM predicted |D|, correct answer: D\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
            " A ignore\n",
            " B enforce\n",
            " C authoritarian\n",
            " D yell at\n",
            " E avoid\n",
            "C\n",
            "Sammy wanted to go to where the people were.  Where might he go?\n",
            " A race track\n",
            " B populated areas\n",
            " C the desert\n",
            " D apartment\n",
            " E roadblock\n",
            "E\n",
            "To locate a choker not located in a jewelry box or boutique where would you go?\n",
            " A jewelry store\n",
            " B neck\n",
            " C jewlery box\n",
            " D jewelry box\n",
            " E boutique\n",
            "D\n",
            "Google Maps and other highway and street GPS services have replaced what?\n",
            " A united states\n",
            " B mexico\n",
            " C countryside\n",
            " D atlas\n",
            " E oceans\n",
            "E\n",
            "Question: Reading newspaper one of many ways to practice your what?\n",
            "Choice A: literacy\n",
            "Choice B: knowing how to read\n",
            "Choice C: money\n",
            "Choice D: buying\n",
            "Choice E: money bank\n",
            "Answer:\n",
            "LM predicted |A|, correct answer: A\n",
            "Prompt: Answer the following question with A, B, C, D, or E.\n",
            "The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
            " A ignore\n",
            " B enforce\n",
            " C authoritarian\n",
            " D yell at\n",
            " E avoid\n",
            "E\n",
            "Sammy wanted to go to where the people were.  Where might he go?\n",
            " A race track\n",
            " B populated areas\n",
            " C the desert\n",
            " D apartment\n",
            " E roadblock\n",
            "D\n",
            "To locate a choker not located in a jewelry box or boutique where would you go?\n",
            " A jewelry store\n",
            " B neck\n",
            " C jewlery box\n",
            " D jewelry box\n",
            " E boutique\n",
            "B\n",
            "Google Maps and other highway and street GPS services have replaced what?\n",
            " A united states\n",
            " B mexico\n",
            " C countryside\n",
            " D atlas\n",
            " E oceans\n",
            "C\n",
            "Question: What do people typically do while playing guitar?\n",
            "Choice A: cry\n",
            "Choice B: hear sounds\n",
            "Choice C: singing\n",
            "Choice D: arthritis\n",
            "Choice E: making music\n",
            "Answer:\n",
            "LM predicted |E|, correct answer: C\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Printing the log probs for the generated tokens\n",
        "prompt_strategies = {\n",
        "    # 'simple': make_simple_prompt, # removed since it's always incorrect\n",
        "    'simple_qa': make_simple_qa_prompt,\n",
        "    'qa_instruction': make_qa_instruction_prompt,\n",
        "    'few_shot': lambda x: make_few_shot_prompt(x, num_shots),\n",
        "    'incorrect_few_shot': lambda x: make_incorrect_few_shot_prompt(x, num_shots)\n",
        "}\n",
        "\n",
        "argmax_lp_correct = []\n",
        "argmax_lp_incorrect = []\n",
        "for prompt_name, prompt_strategy in prompt_strategies.items():\n",
        "    argmax_lp_correct_prompt = []\n",
        "    argmax_lp_incorrect_prompt = []\n",
        "    for data_point in val_set[:num_points]:\n",
        "        prompt = prompt_strategy(data_point)\n",
        "        correct_answer = f'{data_point[\"answerKey\"]}'\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "        log_probs, most_probable = query_model(prompt, correct_answer)\n",
        "        log_prob_argmax = log_probs\n",
        "        if most_probable == correct_answer:\n",
        "            argmax_lp_correct_prompt += [log_prob_argmax]\n",
        "        else:\n",
        "            argmax_lp_incorrect_prompt += [log_prob_argmax]\n",
        "        print(f'LM predicted |{most_probable}|, correct answer: {correct_answer}')\n",
        "\n",
        "    argmax_lp_correct += [np.mean(argmax_lp_correct_prompt)]\n",
        "    argmax_lp_incorrect += [np.mean(argmax_lp_incorrect_prompt)]\n",
        "\n",
        "# Plot a bar chart of the accuracies. The bar chart has two bars for each prompt strategy.\n",
        "# These bars are placed side by side, so you can compare the accuracies of the two strategies.\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(np.arange(len(prompt_strategies)) - 0.2, np.exp(argmax_lp_correct), width=0.4, label='Correct')\n",
        "plt.bar(np.arange(len(prompt_strategies)) + 0.2, np.exp(argmax_lp_incorrect), width=0.4, label='Incorrect')\n",
        "plt.title('Confidence (Probability of most likely token)')\n",
        "plt.xticks(np.arange(len(prompt_strategies)), prompt_strategies.keys())\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z16OJFJfU7rh"
      },
      "source": [
        "# Training GPT-2 for soft prompt tuning\n",
        "\n",
        "GPT-2 is the smaller predecessor model to GPT-3. We will use GPT-2 for soft prompt tuning as it is\n",
        "publicly available(unlike GPT-3) and small enough to train on the free version of the colab GPU\n",
        "(unlike GPT-J).\n",
        "\n",
        "Soft prompt tuning is described in this [paper](https://arxiv.org/abs/2104.08691v1), which we encourage you to learn more about. In essence, instead of generating answers by putting in token prompts, we use fine tuning to train the embeddings of new learned tokens. This allows us to generate answers by putting in the new learned tokens instead of tokens which correspond to real words.\n",
        "\n",
        "Most of the code has been implemented for you, but you should still read through the code to understand what it's doing. There is one TODO which asks you to set up the optimizer. Think about which parameters should get passed into the optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wKSHHwAXU7rh"
      },
      "outputs": [],
      "source": [
        "#@title Define soft embedding for GPT-2\n",
        "#@markdown code adapted from [this github repo](https://github.com/kipgparker/soft-prompt-tuning) implementing the [soft prompt tuning paper](https://arxiv.org/abs/2104.08691v1)\n",
        "class SoftEmbedding(nn.Module):\n",
        "    def __init__(self,\n",
        "                wte: nn.Embedding,\n",
        "                n_tokens: int = 10,\n",
        "                random_range: float = 0.5,\n",
        "                initialize_from_vocab: bool = True):\n",
        "        \"\"\"\n",
        "        Here, we concatentate a new task-specific learned embedding to the existing GPT-2 embedding.\n",
        "        Args:\n",
        "            wte (nn.Embedding): original transformer word embedding\n",
        "            n_tokens (int, optional): number of tokens for task. Defaults to 10.\n",
        "            random_range (float, optional): range to init embedding (if not initialize from vocab). Defaults to 0.5.\n",
        "            initialize_from_vocab (bool, optional): initalizes from default vocab. Defaults to True.\n",
        "        \"\"\"\n",
        "        super(SoftEmbedding, self).__init__()\n",
        "        self.wte = wte\n",
        "        self.n_tokens = n_tokens\n",
        "        self.learned_embedding = nn.parameter.Parameter(self.initialize_embedding(wte,\n",
        "                                                                               n_tokens,\n",
        "                                                                               random_range,\n",
        "                                                                               initialize_from_vocab))\n",
        "\n",
        "    def initialize_embedding(self,\n",
        "                             wte: nn.Embedding,\n",
        "                             n_tokens: int = 10,\n",
        "                             random_range: float = 0.5,\n",
        "                             initialize_from_vocab: bool = True):\n",
        "        \"\"\"initializes learned embedding\n",
        "        Args:\n",
        "            same as __init__\n",
        "        Returns:\n",
        "            torch.float: initialized using original schemes\n",
        "        \"\"\"\n",
        "        if initialize_from_vocab:\n",
        "            return self.wte.weight[:n_tokens].clone().detach()\n",
        "        return torch.FloatTensor(n_tokens, wte.weight.size(1)).uniform_(-random_range, random_range)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        \"\"\"run forward pass\n",
        "        Args:\n",
        "            tokens (torch.long): input tokens before encoding\n",
        "        Returns:\n",
        "            torch.float: encoding of text concatenated with learned task specifc embedding\n",
        "        \"\"\"\n",
        "        # The first n_tokens embeddings are reserved for the learned embeddings\n",
        "        # The rest of the embeddings are the original GPT-2 embeddings\n",
        "        input_embedding = self.wte(tokens[:, self.n_tokens:])\n",
        "        learned_embedding = self.learned_embedding.repeat(input_embedding.size(0), 1, 1)\n",
        "        return torch.cat([learned_embedding, input_embedding], 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3TWAX7kEU7ri"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9f4ad38ad484db2bb3b1b8385e201ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading ()olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Anaconda\\Anaconda_setup\\envs\\malning\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\cyt\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83fbf2e283cf4ca1afaceb54e6726fc2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading ()olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ded95180b5cc4300950a3d36fcbe8317",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading ()/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88ab6a4b497345af8b9ff328ea7df96a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading ()lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@markdown Set up a soft embedding version of GPT-2\n",
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\", padding_side='left')\n",
        "tokenizer.pad_token=tokenizer.eos_token\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_tokens = 100\n",
        "\n",
        "def initialize_soft_model():\n",
        "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "    initialize_from_vocab = True\n",
        "\n",
        "    # Set the input embeddings to the GPT2 model\n",
        "    s_wte = SoftEmbedding(model.get_input_embeddings(),\n",
        "                        n_tokens=n_tokens,\n",
        "                        initialize_from_vocab=initialize_from_vocab)\n",
        "    model.set_input_embeddings(s_wte)\n",
        "    model.to(device)\n",
        "    return model\n",
        "\n",
        "# While we didn't need to do this for GPT-3 earlier, training the model means we need to turn the text\n",
        "# into tokens that the model can understand via the embedding layer.\n",
        "def process_dataset(dataset, mapper_fn, pad_length=119):\n",
        "    mapped_dataset = [mapper_fn(item) for item in dataset]\n",
        "    if pad_length is None:\n",
        "        out = tokenizer(mapped_dataset, return_tensors='pt', padding=True)\n",
        "    else:\n",
        "        out = tokenizer(mapped_dataset, return_tensors='pt', padding=\"max_length\", max_length=pad_length)\n",
        "    # Need to add a space as GPT differentiates between \" A\" and \"A\" and it will be predicting \" A\".\n",
        "    answerkey = [' ' + item['answerKey'] for item in dataset]\n",
        "    out['answerkey'] = tokenizer(answerkey, return_tensors='pt', max_length=1)['input_ids']\n",
        "    return out\n",
        "\n",
        "def pad_soft_inputs(inputs):\n",
        "    \"\"\"\n",
        "    We need to pad the attention_mask and input_ids with an extra n_learned_tokens\n",
        "    It does not matter what you pad input_ids with since these will be overwritten by learned embeddings\n",
        "    \"\"\"\n",
        "    batch = len(inputs['input_ids'])\n",
        "    inputs['input_ids'] = torch.cat([torch.full((batch, n_tokens), 50256).to(device), inputs['input_ids'].to(device)], 1).to(device)\n",
        "    inputs['attention_mask'] = torch.cat([torch.full((batch, n_tokens), 1).to(device), inputs['attention_mask'].to(device)], 1).to(device)\n",
        "    return inputs\n",
        "\n",
        "# Train the model\n",
        "def train_model(model, train_set, val_set, dataset_processor, batch_size=8, epochs=1, lr=1e-4, print_every=100):\n",
        "    train_dataset = process_dataset(train_set, dataset_processor)\n",
        "    val_dataset = process_dataset(val_set, dataset_processor)\n",
        "    \n",
        "    # Get the parameters of the new embedding layer\n",
        "    new_embedding_params = list(model.get_input_embeddings().parameters())\n",
        "    parameters_to_train = new_embedding_params\n",
        "    \n",
        "    optimizer = torch.optim.Adam(parameters_to_train, lr=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    epoch_train_losses = []\n",
        "    for i in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for j in range(0, len(train_dataset['input_ids']), batch_size):\n",
        "            # Calculate cross entropy loss between predicted last token and actual last token\n",
        "            optimizer.zero_grad()\n",
        "            inputs = {k: v[j:j+batch_size].to(device) for k, v in train_dataset.items()}\n",
        "            inputs = pad_soft_inputs(inputs)\n",
        "            labels = inputs.pop('answerkey')\n",
        "            outputs = model(**inputs).logits[:, -1, :] # (batch_size, vocab_size)\n",
        "            loss = criterion(outputs, labels.squeeze())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # loss calculated by criterion is averaged over batch, so multiply by batch size to get total loss\n",
        "            epoch_loss += loss.item() * labels.shape[0]\n",
        "            if j % print_every == 0:\n",
        "                print(f'Epoch {i}, Item {j}, loss: {loss.item()}')\n",
        "        epoch_loss /= train_dataset['input_ids'].shape[0]\n",
        "        epoch_train_losses.append(epoch_loss)\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        for j in range(0, len(val_dataset['input_ids']), batch_size):\n",
        "            inputs = {k: v[j:j+batch_size].to(device) for k, v in val_dataset.items()}\n",
        "            inputs = pad_soft_inputs(inputs)\n",
        "            labels = inputs.pop('answerkey')\n",
        "            outputs = model(**inputs).logits[:, -1, :]\n",
        "            if j == 0: print(f'decoding {tokenizer.decode(outputs.argmax(dim=-1))}')\n",
        "            loss = criterion(outputs, labels.squeeze())\n",
        "            val_loss += loss.item() * labels.shape[0]\n",
        "        val_loss /= val_dataset['input_ids'].shape[0]\n",
        "        print('-'*20)\n",
        "        print(f'Epoch {i}, Validation loss: {val_loss}')\n",
        "\n",
        "\n",
        "# This function lets us sample the next token (or, in our case, the next answer) from the model.\n",
        "def generate_output(model, inputs, pad_soft=True):\n",
        "    \"\"\"\n",
        "    Given a string text or a tokenized input (or list of these, if batched), returns the model's prediction for the\n",
        "    next token in the sequence.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    if type(inputs) is str or type(inputs) is list and type(inputs[0]) is str:\n",
        "        inputs = tokenizer(inputs, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    if pad_soft:\n",
        "        inputs = pad_soft_inputs(inputs)\n",
        "    outputs = model(**inputs).logits[0, -1, :]\n",
        "    outputs = outputs.argmax(dim=-1)\n",
        "    return tokenizer.decode(outputs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "E4YnMpBqI45R"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0866e8b2c1b4e88b45d18d29a0ca414",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0f879bb15d142eb8c79f68a9bcb09b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading ()neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " important\n",
            "Embedding object SoftEmbedding(\n",
            "  (wte): Embedding(50257, 768)\n",
            ")\n",
            "Learned embeddings torch.Size([100, 768])\n",
            "Original vocab embeddings torch.Size([50257, 768])\n"
          ]
        }
      ],
      "source": [
        "model = initialize_soft_model()\n",
        "hard_embedding_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
        "print(generate_output(hard_embedding_model, 'Deep learning is an', pad_soft=False))\n",
        "\n",
        "# Print out the embeddings so you can see their shpaes\n",
        "print('Embedding object', model.get_input_embeddings())\n",
        "print('Learned embeddings', model.get_input_embeddings().learned_embedding.shape)\n",
        "print('Original vocab embeddings', model.get_input_embeddings().wte.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UCZQZU17XuJi"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Item 0, loss: 5.640458106994629\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">6</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">'prompt strategy must be one of'</span>, [i <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> prompt_strategies])                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>prompt_strategy = prompt_strategies[prompt_strategy]                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>6 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>train_model(model, train_set, val_set, prompt_strategy, batch_size=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">8</span>, epochs=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>) <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># yo</span>     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">7 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train_model</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">68</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 65 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>labels = inputs.pop(<span style=\"color: #808000; text-decoration-color: #808000\">'answerkey'</span>)                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 66 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>outputs = model(**inputs).logits[:, -<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, :] <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># (batch_size, vocab_size)</span>          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 67 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>loss = criterion(outputs, labels.squeeze())                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 68 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>loss.backward()                                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 69 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>optimizer.step()                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 70 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># loss calculated by criterion is averaged over batch, so multiply by batch </span>   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 71 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>epoch_loss += loss.item() * labels.shape[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">d:\\Anaconda\\Anaconda_setup\\envs\\malning\\lib\\site-packages\\torch\\_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">489</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 486 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>inputs=inputs,                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 487 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 488 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>torch.autograd.backward(                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 489 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 490 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 491 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 492 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">register_hook</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, hook):                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000\">d:\\Anaconda\\Anaconda_setup\\envs\\malning\\lib\\site-packages\\torch\\autograd\\__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">199</span> in      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">196 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">197 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">198 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>199 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">200 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">grad</span>(                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">202 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>outputs: _TensorOrTensors,                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">OutOfMemoryError: </span>CUDA out of memory. Tried to allocate <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">336.00</span> MiB <span style=\"font-weight: bold\">(</span>GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.00</span> GiB total capacity; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.57</span> GiB \n",
              "already allocated; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> bytes free; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.15</span> GiB reserved in total by PyTorch<span style=\"font-weight: bold\">)</span> If reserved memory is &gt;&gt; allocated memory \n",
              "try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
              "PYTORCH_CUDA_ALLOC_CONF\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m6\u001b[0m                                                                                    \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m3 \u001b[0m\u001b[2m   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m'\u001b[0m\u001b[33mprompt strategy must be one of\u001b[0m\u001b[33m'\u001b[0m, [i \u001b[94mfor\u001b[0m i \u001b[95min\u001b[0m prompt_strategies])                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m4 \u001b[0m\u001b[94melse\u001b[0m:                                                                                        \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m5 \u001b[0m\u001b[2m   \u001b[0mprompt_strategy = prompt_strategies[prompt_strategy]                                     \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m6 \u001b[2m   \u001b[0mtrain_model(model, train_set, val_set, prompt_strategy, batch_size=\u001b[94m8\u001b[0m, epochs=\u001b[94m2\u001b[0m) \u001b[2m# yo\u001b[0m     \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m7 \u001b[0m                                                                                             \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m in \u001b[92mtrain_model\u001b[0m:\u001b[94m68\u001b[0m                                                                                \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 65 \u001b[0m\u001b[2m         \u001b[0mlabels = inputs.pop(\u001b[33m'\u001b[0m\u001b[33manswerkey\u001b[0m\u001b[33m'\u001b[0m)                                               \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 66 \u001b[0m\u001b[2m         \u001b[0moutputs = model(**inputs).logits[:, -\u001b[94m1\u001b[0m, :] \u001b[2m# (batch_size, vocab_size)\u001b[0m          \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 67 \u001b[0m\u001b[2m         \u001b[0mloss = criterion(outputs, labels.squeeze())                                    \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 68 \u001b[2m         \u001b[0mloss.backward()                                                                \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 69 \u001b[0m\u001b[2m         \u001b[0moptimizer.step()                                                               \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 70 \u001b[0m\u001b[2m         \u001b[0m\u001b[2m# loss calculated by criterion is averaged over batch, so multiply by batch \u001b[0m   \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 71 \u001b[0m\u001b[2m         \u001b[0mepoch_loss += loss.item() * labels.shape[\u001b[94m0\u001b[0m]                                    \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[33md:\\Anaconda\\Anaconda_setup\\envs\\malning\\lib\\site-packages\\torch\\_tensor.py\u001b[0m:\u001b[94m489\u001b[0m in \u001b[92mbackward\u001b[0m       \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 486 \u001b[0m\u001b[2m            \u001b[0minputs=inputs,                                                            \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 487 \u001b[0m\u001b[2m         \u001b[0m)                                                                             \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 488 \u001b[0m\u001b[2m      \u001b[0mtorch.autograd.backward(                                                          \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 489 \u001b[2m         \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 490 \u001b[0m\u001b[2m      \u001b[0m)                                                                                 \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 491 \u001b[0m\u001b[2m   \u001b[0m                                                                                      \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m 492 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mregister_hook\u001b[0m(\u001b[96mself\u001b[0m, hook):                                                        \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[33md:\\Anaconda\\Anaconda_setup\\envs\\malning\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m:\u001b[94m199\u001b[0m in      \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[92mbackward\u001b[0m                                                                                         \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m196 \u001b[0m\u001b[2m   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m      \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m \u001b[31m \u001b[0m199 \u001b[2m      \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m200 \u001b[0m                                                                                           \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m201 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mgrad\u001b[0m(                                                                                  \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m   \u001b[0moutputs: _TensorOrTensors,                                                             \u001b[31m\u001b[0m\n",
              "\u001b[31m\u001b[0m\n",
              "\u001b[1;91mOutOfMemoryError: \u001b[0mCUDA out of memory. Tried to allocate \u001b[1;36m336.00\u001b[0m MiB \u001b[1m(\u001b[0mGPU \u001b[1;36m0\u001b[0m; \u001b[1;36m6.00\u001b[0m GiB total capacity; \u001b[1;36m4.57\u001b[0m GiB \n",
              "already allocated; \u001b[1;36m0\u001b[0m bytes free; \u001b[1;36m5.15\u001b[0m GiB reserved in total by PyTorch\u001b[1m)\u001b[0m If reserved memory is >> allocated memory \n",
              "try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
              "PYTORCH_CUDA_ALLOC_CONF\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prompt_strategy = 'qa_instruction'\n",
        "if prompt_strategy not in prompt_strategies:\n",
        "    print('prompt strategy must be one of', [i for i in prompt_strategies])\n",
        "else:\n",
        "    prompt_strategy = prompt_strategies[prompt_strategy]\n",
        "    train_model(model, train_set, val_set, prompt_strategy, batch_size=8, epochs=2) # you can reduce the batch size if you run out of memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWzIveVzI45R"
      },
      "outputs": [],
      "source": [
        "# Save the model to a pickle file (If your runtime crashes, you can load the model from this file)\n",
        "with open('soft_embeddings_model_qa.pkl', 'wb') as f:\n",
        "    pkl.dump(model, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TorOKrwaI45R"
      },
      "source": [
        "Compare the performance of the model with hard prompting and with soft prompt tuning. If your implementation is correct, you should get around 21% correct and 0% invalid with the soft prompt. Answer the analysis questions in the written portion of the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3J3PS-70U7rj"
      },
      "outputs": [],
      "source": [
        "#@title Let's compare a few examples from our prompts\n",
        "def compare_models(model1, model1_pad_soft, model2, model2_pad_soft, dataset, dataset_processor, n_entries=5):\n",
        "    processed_set = process_dataset(dataset, dataset_processor)\n",
        "    for i in range(n_entries):\n",
        "        data_point_idx = np.random.choice(len(dataset))\n",
        "        point = dataset[data_point_idx]\n",
        "        processed_point = processed_set[data_point_idx]\n",
        "        prompt = dataset_processor(point)\n",
        "        print(prompt)\n",
        "        output1 = generate_output(model1, dataset_processor(point), pad_soft=model1_pad_soft)\n",
        "        output2 = generate_output(model2, dataset_processor(point), pad_soft=model2_pad_soft)\n",
        "        print(f'gt: {point[\"answerKey\"]}. model1: {repr(output1)}. model2: {repr(output2)}')\n",
        "        print('-' * 20)\n",
        "\n",
        "\n",
        "compare_models(model, True, hard_embedding_model, False, val_set, prompt_strategy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfXUEeJmU7rj"
      },
      "outputs": [],
      "source": [
        "#@title Bar plot the distribution of incorrect answers, invalid answers, and correct answers\n",
        "def get_answer_stats(model, dataset, dataset_processor, verbose, pad_soft):\n",
        "    processed_set = process_dataset(dataset, dataset_processor)\n",
        "    correct = 0\n",
        "    incorrect = 0\n",
        "    invalid = 0\n",
        "    for i in range(len(dataset)):\n",
        "        inputs = {k: v[i:i+1].to(device) for k, v in processed_set.items()}\n",
        "        point = dataset[i]\n",
        "        prompt = dataset_processor(point)\n",
        "        label = inputs.pop('answerkey')\n",
        "        output = generate_output(model, inputs, pad_soft).strip()\n",
        "        if verbose: print(f'Prompt: {prompt}, output: |{output}|, answerkey |{point[\"answerKey\"]}|')\n",
        "        if output == point[\"answerKey\"]:\n",
        "            correct += 1\n",
        "        elif output in ['A', 'B', 'C', 'D', 'E'] or not point['answerKey'] in ['A', 'B', 'C', 'D', 'E']:\n",
        "            incorrect += 1\n",
        "        else:\n",
        "            invalid += 1\n",
        "    correct, incorrect, invalid = correct/len(dataset), incorrect/len(dataset), invalid/len(dataset)\n",
        "    return correct, incorrect, invalid\n",
        "\n",
        "def plot_answer_stats(model, dataset, dataset_processor, verbose=False, pad_soft=True):\n",
        "    correct, incorrect, invalid = get_answer_stats(model, dataset, dataset_processor, verbose, pad_soft)\n",
        "    plt.bar(['correct', 'incorrect', 'invalid'], [correct, incorrect, invalid])\n",
        "    plt.title('Answer distribution')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4F-l72jcU7rj"
      },
      "outputs": [],
      "source": [
        "plot_answer_stats(model, val_set, prompt_strategy, verbose=True, pad_soft=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZY_SuePeBK4"
      },
      "outputs": [],
      "source": [
        "plot_answer_stats(hard_embedding_model, val_set, prompt_strategies['qa_instruction'], pad_soft=False, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGiiewORI45R"
      },
      "outputs": [],
      "source": [
        "model_soft_qa = model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZGRQUvdI45R"
      },
      "source": [
        "# Pluralize task\n",
        "\n",
        "As you can see above, the soft embedding model does not perform very well on this task. We'll show how soft prompting does better on a second, very simple task - pluralizing a word.\n",
        "\n",
        "The dataset we use was found here, and consists of a list of English nouns: https://www.kaggle.com/datasets/leite0407/list-of-nouns?select=nounlist.csv.\n",
        "For simplicity, we will only consider words where the output is a single token (to avoid needing to deal with sequential generation for evaluation), but you could adapt the code to generate arbitrarily long outputs.\n",
        "\n",
        "If you get memory errors when running this part, re-run the notebook while skipping loading the previous dataset and soft model.\n",
        "\n",
        "\n",
        "!! If you run into an error during training complaining about batch size dimensions, this is an edge-case issue where we get errors when the last batch in an epoch length 1. You can fix this by removing the item in the train set. !!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBi3QVllI45R"
      },
      "outputs": [],
      "source": [
        "!curl https://inst.eecs.berkeley.edu/~cs182/fa22/assets/assignments/nounlist.csv -o nounlist.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzAeBRBFI45R"
      },
      "source": [
        "We'll create targets for this dataset using the inflect library, which is a Python library for inflecting English words. You can read more about it here: https://pypi.org/project/inflect/. This library can convert word to plural forms (though it is not 100% reliable)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2gUrggh_XAl"
      },
      "outputs": [],
      "source": [
        "# Set of words with unusual plurals\n",
        "noun_test = ['foot', 'man', 'person', 'self', 'wife', 'wolf', 'woman']\n",
        "\n",
        "engine = inflect.engine()\n",
        "\n",
        "# Load new noun_list dataset from csv file\n",
        "with open('nounlist.csv', 'r') as f:\n",
        "    noun_list = f.read().splitlines()\n",
        "    noun_list = [i.strip() for i in noun_list]\n",
        "random.seed(0)\n",
        "# shuffle the noun list\n",
        "random.shuffle(noun_list)\n",
        "# Remove all list items which are in the nouns list (our test set)\n",
        "noun_list = [i for i in noun_list if i not in noun_test]\n",
        "# Remove the last 10% for validation\n",
        "noun_train = noun_list[:-int(len(noun_list)*0.1)]\n",
        "noun_val = noun_list[-int(len(noun_list)*0.1):]\n",
        "\n",
        "# Plural task\n",
        "def format_dataset(noun_list):\n",
        "    dataset = []\n",
        "    for noun in noun_list:\n",
        "        plural = engine.plural(noun)\n",
        "        dataset.append({'answerKey': plural, 'input': noun})\n",
        "    return dataset\n",
        "\n",
        "\n",
        "noun_train = format_dataset(noun_train)\n",
        "noun_val = format_dataset(noun_val)\n",
        "noun_test = format_dataset(noun_test)\n",
        "\n",
        "\n",
        "# Only include nouns where the plural is a single token\n",
        "noun_train = [i for i in noun_train if len(tokenizer(i['answerKey'])['input_ids']) == 1]\n",
        "noun_val = [i for i in noun_val if len(tokenizer(i['answerKey'])['input_ids']) == 1]\n",
        "noun_test = [i for i in noun_test if len(tokenizer(i['answerKey'])['input_ids']) == 1]\n",
        "\n",
        "# Print the first 10 items in the dataset\n",
        "print([point['input'] for point in noun_train[:10]])\n",
        "print([point['answerKey'] for point in noun_train[:10]])\n",
        "\n",
        "print(f'Lengths: train: {len(noun_train)}, val: {len(noun_val)}, test: {len(noun_test)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCpYm0JEI45S"
      },
      "source": [
        "Compare hard prompting with soft prompting on this task, then answer the analysis questions in the written part of this homework. You should get over 60% correct on the val set with soft prompting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZE5sX6DSI45S"
      },
      "outputs": [],
      "source": [
        "def basic_format(point):\n",
        "    if isinstance(point, dict):\n",
        "        point = point['input']\n",
        "    return f\"The plural of {point} is\"\n",
        "\n",
        "examples = ['pasta', 'sweater', 'wave', 'mouse', 'attorney', 'bottle', 'phone', 'grass', 'evening', 'candy', 'flower', 'planet', 'architect', 'washer',\n",
        "            'keyhole', 'economy', 'grace', 'finance', 'midnight', 'cushion', 'plateau', 'mouse', 'chord', 'cactus', 'swap', 'tremor', 'criterion', 'sink', 'bandana', 'trade'\n",
        "            ]\n",
        "\n",
        "def make_few_shot(i):\n",
        "    def few_shot(point):\n",
        "        prompt = ''\n",
        "        for j in range(i):\n",
        "            prompt += basic_format(examples[j]) + ' ' + engine.plural(examples[j]) + '.\\n'\n",
        "        prompt += basic_format(point)\n",
        "        return prompt\n",
        "    return few_shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-f3LXo1pI45S"
      },
      "outputs": [],
      "source": [
        "model_pluralize = initialize_soft_model()\n",
        "train_model(model_pluralize, noun_train, noun_val, basic_format, batch_size=8, epochs=40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-Fm5R2OI45S"
      },
      "outputs": [],
      "source": [
        "# Save the model to a pickle file (If your runtime crashes, you can load the model from this file)\n",
        "with open('soft_embeddings_model_pluralize.pkl', 'wb') as f:\n",
        "    pkl.dump(model, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3A80lhQjI45S"
      },
      "outputs": [],
      "source": [
        "plot_answer_stats(model_pluralize, noun_val, basic_format, verbose=True, pad_soft=True)\n",
        "plot_answer_stats(model_pluralize, noun_test, basic_format, verbose=True, pad_soft=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byiaYr9WI45S"
      },
      "outputs": [],
      "source": [
        "# Plot results with hard prompts of various lengths\n",
        "for num_shots in range(10):\n",
        "    plot_answer_stats(hard_embedding_model, noun_val, make_few_shot(num_shots), verbose=False, pad_soft=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnEosJ5BI45S"
      },
      "source": [
        "# Deliverables\n",
        "\n",
        "Please submit this completed notebook and complete all the written questions."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "malning",
      "language": "python",
      "name": "malning"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "24c732284262cd41d749de7c2ab26fba11a555a98d922bf877eca389ddd668a1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
