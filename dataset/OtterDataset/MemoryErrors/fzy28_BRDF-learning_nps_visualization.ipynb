{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ -0.14555131  -0.06219748  -0.06519722   0.07246056   0.04400999\n",
      "    -0.01289852   0.03283399]]\n",
      "\n",
      " [[-10.855956    -9.535606   -11.067599    -9.128112    -8.964688\n",
      "   -10.615193   -10.389152  ]]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from nps.nps import NPsDecoder\n",
    "\n",
    "z = np.load(\"./nps/latent_vectors/aventurnine_latentVector.npy\")\n",
    "print(z)\n",
    "z = torch.from_numpy(z)\n",
    "z = z.to('cuda').float()\n",
    "module = NPsDecoder()\n",
    "module.load_state_dict(torch.load('nps/model/decoder.pt'))\n",
    "\n",
    "module = module.to('cuda').eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 875.99 GiB (GPU 0; 23.65 GiB total capacity; 21.10 MiB already allocated; 20.62 GiB free; 46.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 43\u001b[0m\n\u001b[1;32m     38\u001b[0m conca_in_fixed_h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([theta_h_in, theta_d_in, phi_d_in], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     40\u001b[0m gt_fixed_h \u001b[38;5;241m=\u001b[39m fit_brdf\u001b[38;5;241m.\u001b[39mhalf_diff_look_up_brdf(\n\u001b[1;32m     41\u001b[0m     repeated_array, plane1_theta_h, plane2_theta_h, use_interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     42\u001b[0m )\n\u001b[0;32m---> 43\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_split\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m pred_fixed_h \u001b[38;5;241m=\u001b[39m module(z[\u001b[38;5;241m0\u001b[39m],z[\u001b[38;5;241m1\u001b[39m],phi_d_in, theta_h_in, theta_d_in)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     45\u001b[0m pred_fixed_h\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(pred_fixed_h,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 875.99 GiB (GPU 0; 23.65 GiB total capacity; 21.10 MiB already allocated; 20.62 GiB free; 46.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "## test the learned model and visualize the results\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filename = \"alum-bronze.binary\"\n",
    "model_path = \"be_simple_mlp_\"+ filename + \".pth\"\n",
    "\n",
    "fit_brdf = MeasuredBRDF(filename)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "n_split = 360\n",
    "\n",
    "theta_h = np.linspace(0, np.pi / 2, n_split)\n",
    "theta_d = np.linspace(0, np.pi / 2, n_split)\n",
    "phi_d = np.linspace(0, np.pi, n_split)\n",
    "\n",
    "plane1_phi_d, plane2_phi_d = np.meshgrid(theta_h, theta_d)\n",
    "plane1_theta_d, plane2_theta_d = np.meshgrid(theta_h, phi_d)\n",
    "plane1_theta_h, plane2_theta_h = np.meshgrid(theta_d, phi_d)\n",
    "\n",
    "plane1_phi_d, plane2_phi_d = plane1_phi_d.flatten(), plane2_phi_d.flatten()\n",
    "plane1_theta_d, plane2_theta_d = plane1_theta_d.flatten(), plane2_theta_d.flatten()\n",
    "plane1_theta_h, plane2_theta_h = plane1_theta_h.flatten(), plane2_theta_h.flatten()\n",
    "\n",
    "\n",
    "fixed_angle = \"np.pi * 0.01\"\n",
    "repeated_array = np.full(n_split**2, eval(fixed_angle))\n",
    "\n",
    "\n",
    "\n",
    "theta_h_in = torch.from_numpy(repeated_array).float().to(device)\n",
    "theta_d_in = torch.from_numpy(plane1_theta_h).float().to(device)\n",
    "phi_d_in = torch.from_numpy(plane2_theta_h).float().to(device)\n",
    "conca_in_fixed_h = torch.stack([theta_h_in, theta_d_in, phi_d_in], dim=1)\n",
    "\n",
    "gt_fixed_h = fit_brdf.half_diff_look_up_brdf(\n",
    "    repeated_array, plane1_theta_h, plane2_theta_h, use_interpolation=False\n",
    ")\n",
    "z = z.repeat(1 ,n_split**2,1)\n",
    "pred_fixed_h = module(z[0],z[1],phi_d_in, theta_h_in, theta_d_in).detach().cpu().numpy()\n",
    "pred_fixed_h= np.clip(pred_fixed_h,0,None)\n",
    "gt_fixed_h = gt_fixed_h.reshape(n_split, n_split,3)\n",
    "pred_fixed_h = pred_fixed_h.reshape(n_split, n_split,3)\n",
    "\n",
    "#visualize R channel\n",
    "\n",
    "R_pred = pred_fixed_h[:,:,0]\n",
    "R_gt = gt_fixed_h[:,:,0]\n",
    "vmin = R_pred.min()\n",
    "vmax = R_pred.max()\n",
    "levels = np.linspace(vmin, vmax, 100)\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.contourf(theta_d, phi_d, R_pred, cmap='viridis', levels=levels,vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='pred_fixed_h value')\n",
    "plt.xlabel('theta_d (x)')\n",
    "plt.ylabel('phi_d (y)')\n",
    "plt.title('2D Function Visualization (pred) fixed_angle = '+ fixed_angle)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.contourf(theta_d, phi_d, R_gt, cmap='viridis', levels=levels,vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(label='gt_fixed_h value')\n",
    "plt.xlabel('theta_d (x)')\n",
    "plt.ylabel('phi_d (y)')\n",
    "plt.title('2D Function Visualization (gt) fixed_angle = '+fixed_angle)\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch21",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
