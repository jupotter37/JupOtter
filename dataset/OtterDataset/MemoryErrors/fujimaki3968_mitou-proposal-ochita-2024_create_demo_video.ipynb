{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a2e3Q7Xk_yI1",
      "metadata": {
        "id": "a2e3Q7Xk_yI1"
      },
      "source": [
        "## 試行錯誤編"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iVGqxyylRz8JkDqkPz6JOvYg",
      "metadata": {
        "id": "iVGqxyylRz8JkDqkPz6JOvYg",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/facebookresearch/segment-anything.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7i28O6e4ESc6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i28O6e4ESc6",
        "outputId": "c223b1e9-efec-4cfb-85d6-7932ba6f52cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AIULV2hfEVrK",
      "metadata": {
        "id": "AIULV2hfEVrK"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sA6HPMooEV-s",
      "metadata": {
        "id": "sA6HPMooEV-s"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib\n",
        "from functools import lru_cache\n",
        "from random import randint\n",
        "from typing import Any, Callable, Dict, List, Tuple\n",
        "\n",
        "import clip\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import torch\n",
        "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
        "\n",
        "CHECKPOINT_PATH = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"SAM\")\n",
        "CHECKPOINT_NAME = \"sam_vit_h_4b8939.pth\"\n",
        "CHECKPOINT_URL = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\n",
        "MODEL_TYPE = \"default\"\n",
        "MAX_WIDTH = MAX_HEIGHT = 1024\n",
        "TOP_K_OBJ = 100\n",
        "THRESHOLD = 0.85\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "@lru_cache\n",
        "def load_mask_generator() -> SamAutomaticMaskGenerator:\n",
        "    if not os.path.exists(CHECKPOINT_PATH):\n",
        "        os.makedirs(CHECKPOINT_PATH)\n",
        "    checkpoint = os.path.join(CHECKPOINT_PATH, CHECKPOINT_NAME)\n",
        "    if not os.path.exists(checkpoint):\n",
        "        urllib.request.urlretrieve(CHECKPOINT_URL, checkpoint)\n",
        "    sam = sam_model_registry[MODEL_TYPE](checkpoint=checkpoint).to(device)\n",
        "    mask_generator = SamAutomaticMaskGenerator(sam)\n",
        "    return mask_generator\n",
        "\n",
        "\n",
        "@lru_cache\n",
        "def load_clip(\n",
        "    name: str = \"ViT-B/32\",\n",
        ") -> Tuple[torch.nn.Module, Callable[[PIL.Image.Image], torch.Tensor]]:\n",
        "    model, preprocess = clip.load(name, device=device)\n",
        "    return model.to(device), preprocess\n",
        "\n",
        "\n",
        "def adjust_image_size(image: np.ndarray) -> np.ndarray:\n",
        "    height, width = image.shape[:2]\n",
        "    if height > width:\n",
        "        if height > MAX_HEIGHT:\n",
        "            height, width = MAX_HEIGHT, int(MAX_HEIGHT / height * width)\n",
        "    else:\n",
        "        if width > MAX_WIDTH:\n",
        "            height, width = int(MAX_WIDTH / width * height), MAX_WIDTH\n",
        "    image = cv2.resize(image, (width, height))\n",
        "    return image\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_score(crop: PIL.Image.Image, texts: List[str]) -> torch.Tensor:\n",
        "    model, preprocess = load_clip()\n",
        "    preprocessed = preprocess(crop).unsqueeze(0).to(device)\n",
        "    tokens = clip.tokenize(texts).to(device)\n",
        "    logits_per_image, _ = model(preprocessed, tokens)\n",
        "    similarity = logits_per_image.softmax(-1).cpu()\n",
        "    return similarity[0, 0]\n",
        "\n",
        "\n",
        "def crop_image(image: np.ndarray, mask: Dict[str, Any]) -> PIL.Image.Image:\n",
        "    x, y, w, h = mask[\"bbox\"]\n",
        "    masked = image * np.expand_dims(mask[\"segmentation\"], -1)\n",
        "    crop = masked[y : y + h, x : x + w]\n",
        "    if h > w:\n",
        "        top, bottom, left, right = 0, 0, (h - w) // 2, (h - w) // 2\n",
        "    else:\n",
        "        top, bottom, left, right = (w - h) // 2, (w - h) // 2, 0, 0\n",
        "    # padding\n",
        "    crop = cv2.copyMakeBorder(\n",
        "        crop,\n",
        "        top,\n",
        "        bottom,\n",
        "        left,\n",
        "        right,\n",
        "        cv2.BORDER_CONSTANT,\n",
        "        value=(0, 0, 0),\n",
        "    )\n",
        "    crop = PIL.Image.fromarray(crop)\n",
        "    return crop\n",
        "\n",
        "\n",
        "def get_texts(query: str) -> List[str]:\n",
        "    return [f\"a picture of {query}\", \"a picture of background\"]\n",
        "\n",
        "\n",
        "def filter_masks(\n",
        "    image: np.ndarray,\n",
        "    masks: List[Dict[str, Any]],\n",
        "    predicted_iou_threshold: float,\n",
        "    stability_score_threshold: float,\n",
        "    query: str,\n",
        "    clip_threshold: float,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    filtered_masks: List[Dict[str, Any]] = []\n",
        "\n",
        "    for mask in sorted(masks, key=lambda mask: mask[\"area\"])[-TOP_K_OBJ:]:\n",
        "        if (\n",
        "            mask[\"predicted_iou\"] < predicted_iou_threshold\n",
        "            or mask[\"stability_score\"] < stability_score_threshold\n",
        "            or image.shape[:2] != mask[\"segmentation\"].shape[:2]\n",
        "            or query\n",
        "            and get_score(crop_image(image, mask), get_texts(query)) < clip_threshold\n",
        "        ):\n",
        "            continue\n",
        "\n",
        "        filtered_masks.append(mask)\n",
        "\n",
        "    return filtered_masks\n",
        "\n",
        "\n",
        "def remove_small_segments(segmentation: np.ndarray) -> np.ndarray:\n",
        "    # ブール配列を整数型に変換（OpenCVの関数はブール型を直接扱えないため）\n",
        "    segmentation_int = segmentation.astype(np.uint8)  # Trueを1に、Falseを0に変換\n",
        "\n",
        "    # すべての連結成分を見つけ、ラベル付けする\n",
        "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(segmentation_int)\n",
        "\n",
        "    # 最大の連結成分（背景を除く）のラベルを見つける\n",
        "    # 面積はstatsの5番目の列に格納されています（index=4）\n",
        "    # 背景の成分（ラベル0）を除外して最大のものを見つける\n",
        "    largest_label = 1 + np.argmax(stats[1:, 4])  # 背景を除く最大領域\n",
        "    # 最大の連結成分のみを保持\n",
        "    cleaned_segmentation = (labels == largest_label)\n",
        "\n",
        "    return cleaned_segmentation\n",
        "\n",
        "def remove_contained_masks(masks: List[np.ndarray]) -> List[np.ndarray]:\n",
        "    # マスクが他のマスクに完全に含まれているかどうかをチェック\n",
        "    remaining_masks = []\n",
        "    for i, mask_i in enumerate(masks):\n",
        "        fully_contained = False\n",
        "        for j, mask_j in enumerate(masks):\n",
        "            if i != j and np.all(mask_i[\"segmentation\"] <= mask_j[\"segmentation\"]):\n",
        "                fully_contained = True\n",
        "                break\n",
        "        if not fully_contained:\n",
        "            remaining_masks.append(mask_i)\n",
        "    return remaining_masks\n",
        "\n",
        "def remove_overlapping_masks(masks: List[np.ndarray], overlap_threshold: float = 0.8) -> List[np.ndarray]:\n",
        "    # マスクが他のマスクと大きく重複しているかどうかをチェックし、重複している場合は小さい方を削除\n",
        "    remaining_masks = []\n",
        "    removed_indices = set()  # 削除されたマスクのインデックスを保持\n",
        "\n",
        "    for i, mask_i in enumerate(masks):\n",
        "        if i in removed_indices:\n",
        "            continue  # すでに削除されているマスクはスキップ\n",
        "\n",
        "        for j, mask_j in enumerate(masks):\n",
        "            if i != j and j not in removed_indices:\n",
        "                # 両マスク間の重複領域を計算\n",
        "                intersection = np.logical_and(mask_i[\"segmentation\"], mask_j[\"segmentation\"])\n",
        "                intersection_area = np.sum(intersection)\n",
        "\n",
        "                # 小さい方のマスクの面積を計算\n",
        "                area_i = np.sum(mask_i[\"segmentation\"])\n",
        "                area_j = np.sum(mask_j[\"segmentation\"])\n",
        "                min_area = min(area_i, area_j)\n",
        "\n",
        "                # 重複領域が小さい方のマスクの面積の特定の割合以上なら、小さい方のマスクを削除\n",
        "                if intersection_area / min_area > overlap_threshold:\n",
        "                    if area_i < area_j:\n",
        "                        removed_indices.add(i)\n",
        "                        break  # 現在のマスクiを削除し、次のマスクに進む\n",
        "                    else:\n",
        "                        removed_indices.add(j)\n",
        "                        # マスクjを削除しても、マスクiの処理は続ける\n",
        "\n",
        "    # 削除されていないマスクのみを保持\n",
        "    for i, mask in enumerate(masks):\n",
        "        if i not in removed_indices:\n",
        "            remaining_masks.append(mask)\n",
        "\n",
        "    return remaining_masks\n",
        "\n",
        "\n",
        "def draw_masks(\n",
        "    image: np.ndarray, masks: List[np.ndarray], alpha: float = 0.7\n",
        ") -> np.ndarray:\n",
        "    masks = remove_overlapping_masks(masks)\n",
        "    surfaces = []\n",
        "    transparent_mask = np.zeros_like(image)\n",
        "\n",
        "    for mask in masks:\n",
        "        segmentation = remove_small_segments(mask[\"segmentation\"])\n",
        "        area = np.sum(segmentation)\n",
        "\n",
        "        if mask[\"segmentation\"].size * 0.01 > area:\n",
        "          continue\n",
        "\n",
        "        color = [randint(127, 255) for _ in range(3)]\n",
        "\n",
        "        # draw mask overlay\n",
        "        colored_mask = np.expand_dims(segmentation, 0).repeat(3, axis=0)\n",
        "        colored_mask = np.moveaxis(colored_mask, 0, -1)\n",
        "\n",
        "        # masked = np.ma.MaskedArray(image, mask=colored_mask, fill_value=color)\n",
        "        # image_overlay = masked.filled()\n",
        "        # image = cv2.addWeighted(image, 1 - alpha, image_overlay, alpha, 0)\n",
        "\n",
        "        # draw contour\n",
        "        print(np.uint8(segmentation))\n",
        "        contours, _ = cv2.findContours(\n",
        "            np.uint8(segmentation), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "        cv2.drawContours(image, contours, -1, (0, 0, 255), 2)\n",
        "\n",
        "        for contour in contours:\n",
        "            # Calculate the perimeter of the contour\n",
        "            perimeter = cv2.arcLength(contour, True)\n",
        "            # Approximate the contour to a polygon\n",
        "            epsilon = 0.02 * perimeter  # 2% of the perimeter\n",
        "            approx = cv2.approxPolyDP(contour, epsilon, True)\n",
        "            if len(approx) == 4:\n",
        "              # Draw the approximated polygon (should be a quadrilateral if the shape is close to a rectangle)\n",
        "              cv2.drawContours(image, [approx], 0, (255, 0, 0), 2)\n",
        "              cv2.drawContours(transparent_mask, [approx], 0, (255, 0, 0), -1)\n",
        "\n",
        "              surfaces.append(np.squeeze(approx, axis=1))\n",
        "\n",
        "    image = cv2.addWeighted(transparent_mask, 1 - alpha, image, alpha, 0)\n",
        "\n",
        "    return image, surfaces\n",
        "\n",
        "def crop_and_affine_transform_quadrilateral(original_image: np.ndarray, src_pts: np.ndarray) -> np.ndarray:\n",
        "    # src_ptsが平行四辺形であると仮定して、アフィン変換を適用\n",
        "    # 変換後の点を定義 (左上、右上、左下の順)\n",
        "    width_a = np.sqrt(((src_pts[0][0][0] - src_pts[1][0][0]) ** 2) + ((src_pts[0][0][1] - src_pts[1][0][1]) ** 2))\n",
        "    width_b = np.sqrt(((src_pts[2][0][0] - src_pts[3][0][0]) ** 2) + ((src_pts[2][0][1] - src_pts[3][0][1]) ** 2))\n",
        "    height_a = np.sqrt(((src_pts[0][0][0] - src_pts[3][0][0]) ** 2) + ((src_pts[0][0][1] - src_pts[3][0][1]) ** 2))\n",
        "    height_b = np.sqrt(((src_pts[1][0][0] - src_pts[2][0][0]) ** 2) + ((src_pts[1][0][1] - src_pts[2][0][1]) ** 2))\n",
        "    max_width = max(int(width_a), int(width_b))\n",
        "    max_height = max(int(height_a), int(height_b))\n",
        "    dst_pts = np.array([[0, 0], [max_width - 1, 0], [0, max_height - 1]], dtype='float32')\n",
        "\n",
        "    # 3つの点からアフィン変換行列を計算\n",
        "    M = cv2.getAffineTransform(np.float32(src_pts[:3]), dst_pts)\n",
        "\n",
        "    # アフィン変換を適用して画像を変換\n",
        "    transformed = cv2.warpAffine(original_image, M, (max_width, max_height))\n",
        "\n",
        "    return transformed\n",
        "\n",
        "def crop_test(img, points):\n",
        "    points = sorted(points, key=lambda x:x[1])  # yが小さいもの順に並び替え。\n",
        "    top = sorted(points[:2], key=lambda x:x[0])  # 前半二つは四角形の上。xで並び替えると左右も分かる。\n",
        "    bottom = sorted(points[2:], key=lambda x:x[0], reverse=True)  # 後半二つは四角形の下。同じくxで並び替え。\n",
        "    points = np.array(top + bottom, dtype='float32')  # 分離した二つを再結合。\n",
        "\n",
        "    width = max(np.sqrt(((points[0][0]-points[2][0])**2)*2), np.sqrt(((points[1][0]-points[3][0])**2)*2))\n",
        "    height = max(np.sqrt(((points[0][1]-points[2][1])**2)*2), np.sqrt(((points[1][1]-points[3][1])**2)*2))\n",
        "\n",
        "    dst = np.array([\n",
        "            np.array([0, 0]),\n",
        "            np.array([width-1, 0]),\n",
        "            np.array([width-1, height-1]),\n",
        "            np.array([0, height-1]),\n",
        "            ], np.float32)\n",
        "\n",
        "    trans = cv2.getPerspectiveTransform(points, dst)  # 変換前の座標と変換後の座標の対応を渡すと、透視変換行列を作ってくれる。\n",
        "    return cv2.warpPerspective(img, trans, (int(width), int(height)))\n",
        "\n",
        "def normalize_surface(surface, image_width, image_height):\n",
        "    # surfaceの各頂点を正規化（0から1の範囲に変換）\n",
        "    normalized_surface = np.zeros_like(surface, dtype=np.float32)\n",
        "    normalized_surface[:, 0] = surface[:, 0] / image_width  # x座標を正規化\n",
        "    normalized_surface[:, 1] = surface[:, 1] / image_height  # y座標を正規化\n",
        "\n",
        "    return normalized_surface\n",
        "\n",
        "def denormalize_approx(surface, image_width, image_height):\n",
        "    denormalized_approx = np.zeros_like(surface, dtype=np.int32)\n",
        "    denormalized_approx[:, 0] = np.round(surface[:, 0] * image_width).astype(np.int32)  # x座標を元に戻す\n",
        "    denormalized_approx[:, 1] = np.round(surface[:, 1] * image_height).astype(np.int32)  # y座標を元に戻す\n",
        "\n",
        "    return denormalized_approx\n",
        "\n",
        "def segment_frame(predicted_iou_threshold: float,\n",
        "    stability_score_threshold: float,\n",
        "    clip_threshold: float,\n",
        "    frame: str,\n",
        "    query: str,\n",
        "    mask_generator,\n",
        "):\n",
        "    # mask_generator = load_mask_generator()\n",
        "    # ori_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # reduce the size to save gpu memory\n",
        "    image = adjust_image_size(frame)\n",
        "    try:\n",
        "      masks = mask_generator.generate(image)\n",
        "      masks = filter_masks(\n",
        "          image,\n",
        "          masks,\n",
        "          predicted_iou_threshold,\n",
        "          stability_score_threshold,\n",
        "          query,\n",
        "          clip_threshold,\n",
        "      )\n",
        "      masked_image, _ = draw_masks(image, masks)\n",
        "      # masked_image = PIL.Image.fromarray(masked_image)\n",
        "      return masked_image\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      print(\"error occur\")\n",
        "      return image\n",
        "\n",
        "def segment(\n",
        "    predicted_iou_threshold: float,\n",
        "    stability_score_threshold: float,\n",
        "    clip_threshold: float,\n",
        "    image_path: str,\n",
        "    query: str,\n",
        ") -> PIL.ImageFile.ImageFile:\n",
        "    mask_generator = load_mask_generator()\n",
        "    ori_image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "    ori_image = cv2.cvtColor(ori_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # reduce the size to save gpu memory\n",
        "    image = adjust_image_size(ori_image)\n",
        "    masks = mask_generator.generate(image)\n",
        "    masks = filter_masks(\n",
        "        image,\n",
        "        masks,\n",
        "        predicted_iou_threshold,\n",
        "        stability_score_threshold,\n",
        "        query,\n",
        "        clip_threshold,\n",
        "    )\n",
        "    masked_image, surfaces = draw_masks(image, masks)\n",
        "\n",
        "    # cropped_images = []\n",
        "    normalized_surfaces = []\n",
        "    for surface in surfaces:\n",
        "      height, width, _ = image.shape\n",
        "      nor_surface = normalize_surface(surface, width, height)\n",
        "      normalized_surfaces.append(nor_surface)\n",
        "    #   ori_h, ori_w, _ = ori_image.shape\n",
        "    #   sca_surface = denormalize_approx(surface, ori_w, ori_h)\n",
        "\n",
        "      # cropped_img = crop_test(ori_image, sca_surface)\n",
        "      # cropped_images.append(cropped_img)\n",
        "\n",
        "    masked_image = PIL.Image.fromarray(masked_image)\n",
        "    return masked_image, surfaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ng80KjGsEn3I",
      "metadata": {
        "id": "ng80KjGsEn3I"
      },
      "outputs": [],
      "source": [
        "filename = \"/content/samples/demo01.jpg\"\n",
        "\n",
        "image, surfaces = segment(0.8, 0.8, 0.96, filename, \"display\")\n",
        "\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-YihU5qTFIxc",
      "metadata": {
        "id": "-YihU5qTFIxc"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "video_path = \"/content/samples/MOVIE.mp4\"\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# 動画のフレームレートを取得\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "_, first_frame = cap.read()\n",
        "\n",
        "first_frame = adjust_image_size(first_frame.copy())\n",
        "height, width = first_frame.shape[:2]\n",
        "\n",
        "# 10秒間に相当するフレーム数を計算\n",
        "frames_to_process = int(20 * fps)\n",
        "\n",
        "# 出力用の動画ファイルを準備\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter('output_video.mp4', fourcc, fps, (width, height))\n",
        "\n",
        "# 処理するフレーム数をカウント\n",
        "frame_count = 0\n",
        "\n",
        "mask_generator = load_mask_generator()\n",
        "\n",
        "for _ in range(int(5 * fps)):\n",
        "  cap.read()\n",
        "\n",
        "with tqdm(total=100) as pbar:\n",
        "  while(cap.isOpened()):\n",
        "      ret, frame = cap.read()\n",
        "      if not ret or frame_count >= frames_to_process:\n",
        "          break\n",
        "\n",
        "      # フレームごとに物体検出を実行\n",
        "      detected_frame = segment_frame(0.8, 0.8, 0.96, frame.copy(), \"display\", mask_generator)\n",
        "      # detected_frame = segment_frame(0.8, 0.8, 0.96, frame.copy(), \"display\")\n",
        "\n",
        "      # 出力用の動画ファイルに書き込む\n",
        "      out.write(detected_frame)\n",
        "\n",
        "      pbar.update(100/frames_to_process)\n",
        "      frame_count += 1\n",
        "\n",
        "cap.release()\n",
        "out.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AVgom4vSG1MT",
      "metadata": {
        "id": "AVgom4vSG1MT"
      },
      "source": [
        "## FAST SAM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2QRCmuwvVygK",
      "metadata": {
        "id": "2QRCmuwvVygK"
      },
      "source": [
        "### image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "czsKkDI7GN5u",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czsKkDI7GN5u",
        "outputId": "0b36d2bb-2498-4303-87cb-6645ab83e460"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.1.27-py3-none-any.whl (721 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.2/721.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.16.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Collecting thop>=0.1.1 (from ultralytics)\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Installing collected packages: thop, ultralytics\n",
            "Successfully installed thop-0.1.1.post2209072238 ultralytics-8.1.27\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T6jvi8CmZDFF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6jvi8CmZDFF",
        "outputId": "44f5b94c-5b0b-4e4d-bdca-3376864580e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YB-h1_i4HFdF",
      "metadata": {
        "id": "YB-h1_i4HFdF"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from ultralytics import FastSAM\n",
        "from ultralytics.models.fastsam import FastSAMPrompt\n",
        "\n",
        "# Define an inference source\n",
        "source = '/content/drive/MyDrive/未踏/prototype/pdemo.jpg'\n",
        "\n",
        "# Create a FastSAM model\n",
        "model = FastSAM('FastSAM-x.pt')  # or FastSAM-x.pt\n",
        "\n",
        "# Run inference on an image\n",
        "everything_results = model(source, device='cuda', retina_masks=True, imgsz=1024, conf=0.4, iou=0.9)\n",
        "\n",
        "# Prepare a Prompt Process object\n",
        "prompt_process = FastSAMPrompt(source, everything_results, device='cuda')\n",
        "\n",
        "# Everything prompt\n",
        "ann = prompt_process.everything_prompt()\n",
        "\n",
        "# Bbox default shape [0,0,0,0] -> [x1,y1,x2,y2]\n",
        "# ann = prompt_process.box_prompt(bbox=[200, 200, 300, 300])\n",
        "\n",
        "# Text prompt\n",
        "# ann = prompt_process.text_prompt(text='the screen and laptop screen')\n",
        "\n",
        "# Point prompt\n",
        "# points default [[0,0]] [[x1,y1],[x2,y2]]\n",
        "# point_label default [0] [1,0] 0:background, 1:foreground\n",
        "# ann = prompt_process.point_prompt(points=[[200, 200]], pointlabel=[1])\n",
        "prompt_process.plot(annotations=ann, output='./result')\n",
        "\n",
        "result = cv2.imread('./result/demo01.jpg', cv2.IMREAD_UNCHANGED)\n",
        "cv2_imshow(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H2CVYXockOOU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2CVYXockOOU",
        "outputId": "8ad6a61c-ee3e-41b3-a083-d61e40a47b81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
          ]
        }
      ],
      "source": [
        "%reset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E3HvPgUjkv-Q",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3HvPgUjkv-Q",
        "outputId": "a641e227-835f-4925-8c72-dc4680e4b36a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.58.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.41.1)\n",
            "Requirement already satisfied: numpy<1.27,>=1.22 in /usr/local/lib/python3.10/dist-packages (from numba) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numba\n",
        "\n",
        "from numba import cuda\n",
        "device = cuda.get_current_device()\n",
        "device.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JDD83QpeajUW",
      "metadata": {
        "id": "JDD83QpeajUW"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from ultralytics import FastSAM\n",
        "from ultralytics.models.fastsam import FastSAMPrompt\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class CustomFastSAMPrompt(FastSAMPrompt):\n",
        "    def __init__(self, device='cuda'):\n",
        "        try:\n",
        "            import clip\n",
        "        except ImportError:\n",
        "            from ultralytics.utils.checks import check_requirements\n",
        "\n",
        "            check_requirements(\"git+https://github.com/openai/CLIP.git\")\n",
        "            import clip\n",
        "        self.clip = clip\n",
        "        self.device = device\n",
        "        with torch.no_grad():\n",
        "            self.clip_model, self.preprocess = self.clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def pre_make_text_features(self, search_text: str, device) -> int:\n",
        "        tokenized_text = self.clip.tokenize([search_text]).to(device)\n",
        "        text_features = self.clip_model.encode_text(tokenized_text)\n",
        "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "        return text_features\n",
        "\n",
        "    def _crop_image(self, format_results):\n",
        "        image = Image.fromarray(cv2.cvtColor(self.results[0].orig_img, cv2.COLOR_BGR2RGB))\n",
        "        ori_w, ori_h = image.size\n",
        "        annotations = format_results\n",
        "        mask_h, mask_w = annotations[0][\"segmentation\"].shape\n",
        "        if ori_w != mask_w or ori_h != mask_h:\n",
        "            image = image.resize((mask_w, mask_h))\n",
        "        cropped_boxes = []\n",
        "        cropped_images = []\n",
        "        not_crop = []\n",
        "        filter_id = []\n",
        "        for _, mask in enumerate(annotations):\n",
        "            if np.sum(mask[\"segmentation\"]) <= 100:\n",
        "                filter_id.append(_)\n",
        "                continue\n",
        "            bbox = self._get_bbox_from_mask(mask[\"segmentation\"])  # bbox from mask\n",
        "            cropped_boxes.append(self._segment_image(image, bbox))  # save cropped image\n",
        "            cropped_images.append(bbox)  # save cropped image bbox\n",
        "\n",
        "        return cropped_boxes, cropped_images, not_crop, filter_id, annotations\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def fast_retrieve(self, model, preprocess, elements, text_features: str, device) -> int:\n",
        "        \"\"\"Processes images and text with a model, calculates similarity, and returns softmax score.\"\"\"\n",
        "        preprocessed_images = [preprocess(image).to(device) for image in elements]\n",
        "        stacked_images = torch.stack(preprocessed_images)\n",
        "        image_features = model.encode_image(stacked_images)\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "        probs = 100.0 * image_features @ text_features.T\n",
        "        return probs[:, 0].softmax(dim=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _normalize_mask(mask, width, height):\n",
        "        normalized_coordinates = np.copy(mask).astype(np.float32)\n",
        "        normalized_coordinates[:, 0] /= width\n",
        "        normalized_coordinates[:, 1] /= height\n",
        "        return normalized_coordinates\n",
        "\n",
        "    def _custom_crop_image(self, format_results, orig_img):\n",
        "        image = Image.fromarray(cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB))\n",
        "        ori_w, ori_h = image.size\n",
        "        annotations = format_results\n",
        "        mask_h, mask_w = annotations[0][\"segmentation\"].shape\n",
        "        if ori_w != mask_w or ori_h != mask_h:\n",
        "            image = image.resize((mask_w, mask_h))\n",
        "\n",
        "        cropped_boxes = []\n",
        "        cropped_images = []\n",
        "        not_crop = []\n",
        "        filter_id = []\n",
        "\n",
        "        for _, mask in enumerate(annotations):\n",
        "            segmentation = mask[\"segmentation\"]\n",
        "\n",
        "            if np.sum(segmentation) <= mask_w * mask_h * 0.01:\n",
        "                filter_id.append(_)\n",
        "                continue\n",
        "\n",
        "            # ディスプレイぽいのを検出\n",
        "            contours, _ = cv2.findContours(\n",
        "                np.uint8(segmentation), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "            )\n",
        "\n",
        "            for contour in contours:\n",
        "                  perimeter = cv2.arcLength(contour, True)\n",
        "                  epsilon = 0.02 * perimeter\n",
        "                  approx = cv2.approxPolyDP(contour, epsilon, True)\n",
        "                  if len(approx) == 4:\n",
        "                      bbox = [\n",
        "                          min(approx[:, 0, 0]),\n",
        "                          min(approx[:, 0, 1]),\n",
        "                          max(approx[:, 0, 0]),\n",
        "                          max(approx[:, 0, 1]),\n",
        "                      ]\n",
        "                      cropped_boxes.append(self._segment_image(image, bbox))\n",
        "                      approx = np.squeeze(approx, axis=1)\n",
        "                      cropped_images.append(self._normalize_mask(approx, mask_w, mask_h))\n",
        "                  else:\n",
        "                      filter_id.append(_)\n",
        "                      continue\n",
        "\n",
        "        return cropped_boxes, cropped_images, not_crop, filter_id, annotations\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def custom_filter_mask(self, text, source, results, threshold=0.5, text_features=None, device=\"cuda\"):\n",
        "        self.results = results\n",
        "        self.source = source\n",
        "        self.device = device\n",
        "\n",
        "        new_results = []\n",
        "        for result in self.results:\n",
        "          if result.masks is not None:\n",
        "            format_results = self._format_results(result, 0)\n",
        "            cropped_boxes, cropped_images, not_crop, filter_id, annotations = self._custom_crop_image(format_results, result.orig_img)\n",
        "\n",
        "            if text_features is None:\n",
        "                scores = self.retrieve(self.clip_model, self.preprocess, cropped_boxes, text, device=self.device)\n",
        "            else:\n",
        "                scores = self.fast_retrieve(self.clip_model, self.preprocess, cropped_boxes, text_features, device=self.device)\n",
        "\n",
        "            scores = scores.cpu().numpy()\n",
        "            valid_indices = np.where(scores >= threshold)[0]\n",
        "            valid_cropped_images = np.array(cropped_images)[valid_indices]\n",
        "\n",
        "            new_results.append(valid_cropped_images)\n",
        "\n",
        "        if len(new_results) == 1:\n",
        "          return new_results[0]\n",
        "        return new_results\n",
        "\n",
        "\n",
        "        # if self.results[0].masks is not None:\n",
        "        #     format_results = self._format_results(self.results[0], 0)\n",
        "        #     cropped_boxes, cropped_images, not_crop, filter_id, annotations = self._custom_crop_image(format_results)\n",
        "\n",
        "        #     if text_features is None:\n",
        "        #         scores = self.retrieve(self.clip_model, self.preprocess, cropped_boxes, text, device=self.device)\n",
        "        #     else:\n",
        "        #         scores = self.fast_retrieve(self.clip_model, self.preprocess, cropped_boxes, text_features, device=self.device)\n",
        "\n",
        "        #     scores = scores.cpu().numpy()\n",
        "        #     valid_indices = np.where(scores >= threshold)[0]\n",
        "        #     valid_cropped_images = np.array(cropped_images)[valid_indices]\n",
        "\n",
        "        #     return valid_cropped_images\n",
        "        # return []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mv4iGNziitx_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "mv4iGNziitx_",
        "outputId": "c8643927-9795-4f68-eb33-9eafe1c3416d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 7.56 GiB. GPU 0 has a total capacty of 14.75 GiB of which 4.75 GiB is free. Process 72869 has 9.99 GiB memory in use. Of the allocated memory 9.79 GiB is allocated by PyTorch, and 25.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d8107822fa9f>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;31m# Run inference on an image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0meverything_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretina_masks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgsz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0multralytics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencapsulated\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mResults\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \"\"\"\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprompts\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"set_prompts\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# for SAM-type models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_cli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_cli\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     def track(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# merge list of Result into one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_cli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mgenerator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# Issuing `None` to a generator fires it up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/predictor.py\u001b[0m in \u001b[0;36mstream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0;31m# Postprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mprofilers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim0s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_predict_postprocess_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/models/fastsam/predict.py\u001b[0m in \u001b[0;36mpostprocess\u001b[0;34m(self, preds, img, orig_imgs)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina_masks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_mask_native\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# HWC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# HWC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/utils/ops.py\u001b[0m in \u001b[0;36mprocess_mask_native\u001b[0;34m(protos, masks_in, bboxes, shape)\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmasks_in\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mprotos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# CHW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m     \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrop_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# CHW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgt_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/utils/ops.py\u001b[0m in \u001b[0;36mcrop_mask\u001b[0;34m(masks, boxes)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# cols shape(1,h,1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 7.56 GiB. GPU 0 has a total capacty of 14.75 GiB of which 4.75 GiB is free. Process 72869 has 9.99 GiB memory in use. Of the allocated memory 9.79 GiB is allocated by PyTorch, and 25.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from ultralytics import FastSAM\n",
        "from ultralytics.models.fastsam import FastSAMPrompt\n",
        "\n",
        "# Define an inference source\n",
        "source = '/content/drive/MyDrive/未踏/prototype/pdemo.jpg'\n",
        "\n",
        "# Create a FastSAM model\n",
        "with torch.no_grad():\n",
        "  model = FastSAM('FastSAM-x.pt')  # or FastSAM-x.pt\n",
        "\n",
        "  # Run inference on an image\n",
        "  everything_results = model(source, device='cuda', retina_masks=True, imgsz=1024, conf=0.4, iou=0.9)\n",
        "\n",
        "  del model\n",
        "  prompt_process = CustomFastSAMPrompt(device='cuda')\n",
        "\n",
        "  # Prepare a Prompt Process object\n",
        "\n",
        "  prompt = 'operation system, windows, macos, browser, screen with text'\n",
        "  text_features = prompt_process.pre_make_text_features(search_text=prompt, device='cuda')\n",
        "  masks = prompt_process.custom_filter_mask(text=prompt, source=source, results=everything_results, text_features=text_features, threshold=0.1)\n",
        "\n",
        "del prompt_process\n",
        "\n",
        "len(masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebQFrT2dRJko",
      "metadata": {
        "id": "ebQFrT2dRJko"
      },
      "outputs": [],
      "source": [
        "def denomarize_mask(mask, width, height):\n",
        "    denomarized_mask = np.round(mask * np.array([width, height])).astype(np.int32)\n",
        "    return denomarized_mask\n",
        "\n",
        "def denomarize_masks(masks, width, height):\n",
        "    denomarized_mask = np.round(masks * np.array([[width, height]])).astype(np.int32)\n",
        "    return denomarized_mask\n",
        "\n",
        "def draw_mask_result(masks, ori_image):\n",
        "  masks = denomarize_masks(masks, ori_image.shape[1], ori_image.shape[0])\n",
        "  for mask in masks:\n",
        "    cv2.drawContours(ori_image, [mask], 0, (255, 0, 0), -1)\n",
        "  return ori_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AWVAo4QJt4GQ",
      "metadata": {
        "id": "AWVAo4QJt4GQ"
      },
      "outputs": [],
      "source": [
        "ori_image = cv2.imread(source, cv2.IMREAD_COLOR)\n",
        "ori_image = draw_mask_result(masks, ori_image)\n",
        "\n",
        "cv2_imshow(ori_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FhOsmn04V18R",
      "metadata": {
        "id": "FhOsmn04V18R"
      },
      "outputs": [],
      "source": [
        "def order_points_clockwise(pts):\n",
        "    x_sorted = pts[np.argsort(pts[:, 0]), :]\n",
        "    left_most = x_sorted[:2, :]\n",
        "    right_most = x_sorted[2:, :]\n",
        "    left_most = left_most[np.argsort(left_most[:, 1]), :]\n",
        "    (tl, bl) = left_most\n",
        "    D = np.linalg.norm(right_most - tl, axis=1)\n",
        "    (br, tr) = right_most[np.argsort(D)[::-1], :]\n",
        "    return np.array([tl, tr, br, bl], dtype=\"float32\")\n",
        "\n",
        "def calculate_output_size(src_coords):\n",
        "    widths = [np.linalg.norm(src_coords[0] - src_coords[1]), np.linalg.norm(src_coords[2] - src_coords[3])]\n",
        "    heights = [np.linalg.norm(src_coords[0] - src_coords[3]), np.linalg.norm(src_coords[1] - src_coords[2])]\n",
        "\n",
        "    max_width = int(max(widths))\n",
        "    max_height = int(max(heights))\n",
        "\n",
        "    return (max_height, max_width)\n",
        "\n",
        "def crop_and_affine_display(mask, ori_image):\n",
        "    mask = denomarize_mask(mask, ori_image.shape[1], ori_image.shape[0])\n",
        "    output_size = calculate_output_size(mask)\n",
        "\n",
        "    dst_coords = np.array([[0, 0], [output_size[0], 0], [output_size[0], output_size[1]], [0, output_size[1]]], dtype=np.float32)\n",
        "\n",
        "    # Convert source coordinates to numpy array\n",
        "    src_coords = np.array(mask, dtype=np.float32)\n",
        "    src_coords = order_points_clockwise(src_coords)\n",
        "    transform_matrix = cv2.getPerspectiveTransform(src_coords, dst_coords)\n",
        "\n",
        "    # Apply the perspective transformation\n",
        "    transformed_image = cv2.warpPerspective(ori_image, transform_matrix, output_size)\n",
        "\n",
        "    return transformed_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9s7QHCQgcSr4",
      "metadata": {
        "id": "9s7QHCQgcSr4"
      },
      "outputs": [],
      "source": [
        "!sudo apt -y install tesseract-ocr tesseract-ocr-jpn libtesseract-dev libleptonica-dev tesseract-ocr-script-jpan tesseract-ocr-script-jpan-vert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gjuKOKF0c7tK",
      "metadata": {
        "id": "gjuKOKF0c7tK"
      },
      "outputs": [],
      "source": [
        "!pip install pytesseract pyocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Army3rKAdAlo",
      "metadata": {
        "id": "Army3rKAdAlo"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "from pytesseract import Output\n",
        "import pyocr\n",
        "import pyocr.builders\n",
        "from PIL import Image\n",
        "\n",
        "def add_text_detection_boxes(image):\n",
        "    tools = pyocr.get_available_tools()\n",
        "    if len(tools) == 0:\n",
        "        print(\"No OCR tool found\")\n",
        "        return\n",
        "    tool = tools[0]  # 利用可能なツールの最初のものを使用\n",
        "\n",
        "    pil_image = Image.fromarray(image)\n",
        "\n",
        "    # テキストの検出領域を取得\n",
        "    boxes = tool.image_to_string(\n",
        "        pil_image,\n",
        "        lang='jpn',\n",
        "        builder=pyocr.builders.WordBoxBuilder(tesseract_layout=1)\n",
        "    )\n",
        "\n",
        "    # 検出したテキストの領域に対して枠を描画\n",
        "    for box in boxes:\n",
        "        top_left = box.position[0]\n",
        "        bottom_right = box.position[1]\n",
        "        image = cv2.rectangle(image, top_left, bottom_right, (0, 255, 0), 2)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "voUDbI4XgQCu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voUDbI4XgQCu",
        "outputId": "c863e4cb-30fd-4cb5-9e68-27ed94d74548"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google-cloud-vision in /usr/local/lib/python3.10/dist-packages (3.7.2)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision) (2.11.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision) (1.23.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision) (3.20.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (1.62.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2.31.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (1.51.3)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-cloud-vision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ECiSGEVXkBdf",
      "metadata": {
        "id": "ECiSGEVXkBdf"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user(project_id=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i2vnDFRtf8LQ",
      "metadata": {
        "id": "i2vnDFRtf8LQ"
      },
      "outputs": [],
      "source": [
        "from google.cloud import vision\n",
        "from PIL import Image\n",
        "import io\n",
        "from google.cloud.vision_v1 import types\n",
        "\n",
        "def add_text_detection_boxes_google(image):\n",
        "    # クライアントを初期化\n",
        "    client = vision.ImageAnnotatorClient()\n",
        "\n",
        "\n",
        "    success, encoded_image = cv2.imencode('.jpg', image)\n",
        "    if not success:\n",
        "        raise Exception(\"Image encoding failed\")\n",
        "\n",
        "    # エンコードされたバイト列をPythonのバイト配列に変換\n",
        "    byte_io = io.BytesIO(encoded_image.tobytes())\n",
        "\n",
        "    # Google Cloud Vision APIに渡すための画像オブジェクトを作成\n",
        "    g_image = types.Image(content=byte_io.getvalue())\n",
        "\n",
        "    # OCR実行\n",
        "    response = client.text_detection(image=g_image)\n",
        "    texts = response.text_annotations\n",
        "\n",
        "    # 検出されたテキストの領域を描画\n",
        "    for text in texts[1:]:  # 最初の要素は全テキストを含むためスキップ\n",
        "        vertices = [(vertex.x, vertex.y) for vertex in text.bounding_poly.vertices]\n",
        "        cv2.polylines(image, [np.array(vertices)], True, (0, 255, 0), 2)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jZWe4xNwfCDJ",
      "metadata": {
        "id": "jZWe4xNwfCDJ"
      },
      "outputs": [],
      "source": [
        "def overlay_on_four_points(base_image, overlay_image, dst_points):\n",
        "    \"\"\"\n",
        "    base_image: 背景画像\n",
        "    overlay_image: 上に貼り付ける画像\n",
        "    dst_points: 背景画像上の四点の座標 (numpy array of shape (4, 2))\n",
        "    \"\"\"\n",
        "    # 貼り付ける画像の四隅の座標 (左上, 右上, 右下, 左下)\n",
        "    src_points = np.float32([[0, 0], [overlay_image.shape[1], 0], [overlay_image.shape[1], overlay_image.shape[0]], [0, overlay_image.shape[0]]])\n",
        "    dst_points = denomarize_mask(dst_points, base_image.shape[1], base_image.shape[0])\n",
        "    dst_points = order_points_clockwise(dst_points)\n",
        "    # 変換行列を計算\n",
        "    M = cv2.getPerspectiveTransform(src_points, dst_points)\n",
        "\n",
        "    # 変換行列を使用して画像を変形\n",
        "    transformed_overlay = cv2.warpPerspective(overlay_image, M, (base_image.shape[1], base_image.shape[0]))\n",
        "\n",
        "    # 貼り付ける領域を作成 (マスクを使用)\n",
        "    mask = np.zeros_like(base_image, dtype=np.uint8)\n",
        "    cv2.fillConvexPoly(mask, np.int32(dst_points), (255,) * base_image.shape[2])\n",
        "\n",
        "    # 背景画像から該当領域を削除\n",
        "    base_image = cv2.bitwise_and(base_image, cv2.bitwise_not(mask))\n",
        "\n",
        "    # 変形した画像を背景画像に貼り付け\n",
        "    result_image = cv2.add(base_image, cv2.bitwise_and(transformed_overlay, mask))\n",
        "    return result_image\n",
        "\n",
        "\n",
        "def render_ocr_result(image, boxes):\n",
        "    for box in boxes:\n",
        "        transformed_image = crop_and_affine_display(box, image)\n",
        "        ocr_image = add_text_detection_boxes(transformed_image)\n",
        "        image = overlay_on_four_points(image, ocr_image, box)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8BcPYuxI3udK",
      "metadata": {
        "id": "8BcPYuxI3udK"
      },
      "source": [
        "### 映像処理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GQ9leKeV3B6N",
      "metadata": {
        "id": "GQ9leKeV3B6N"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from ultralytics import FastSAM\n",
        "from ultralytics.models.fastsam import FastSAMPrompt\n",
        "\n",
        "import logging\n",
        "\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.ERROR)\n",
        "\n",
        "video_path = \"/content/samples/demo03.mp4\"\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# 動画のフレームレートを取得\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "_, first_frame = cap.read()\n",
        "\n",
        "height, width = first_frame.shape[:2]\n",
        "\n",
        "# 10秒間に相当するフレーム数を計算\n",
        "frames_to_process = int(60 * fps)\n",
        "\n",
        "# 出力用の動画ファイルを準備\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter('output_video-demo03.mp4', fourcc, fps, (width, height))\n",
        "\n",
        "# 処理するフレーム数をカウント\n",
        "frame_count = 0\n",
        "batch_frame_size = 2\n",
        "\n",
        "# for _ in range(int(5 * fps)):\n",
        "#   cap.read()\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  prompt_process = CustomFastSAMPrompt(device='cuda')\n",
        "  model = FastSAM('FastSAM-x.pt')  # or FastSAM-x.pt\n",
        "  prompt = 'operation system, windows, macos, browser, display word'\n",
        "  text_features = prompt_process.pre_make_text_features(search_text=prompt, device='cuda')\n",
        "\n",
        "  with tqdm(total=100) as pbar:\n",
        "    while(cap.isOpened()):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret or frame_count >= frames_to_process:\n",
        "            break\n",
        "\n",
        "        # batch_frames = []\n",
        "        # for _ in range(batch_frame_size):\n",
        "        #     ret, frame = cap.read()\n",
        "        #     if not ret:\n",
        "        #         break\n",
        "        #     batch_frames.append(frame.copy())\n",
        "\n",
        "        # everything_results = model(batch_frames, device='cuda', retina_masks=True, imgsz=1024, conf=0.4, iou=0.9, verbose=True)\n",
        "        # batch_masks = prompt_process.custom_filter_mask(text=prompt, source=\"\", results=everything_results, text_features=text_features, threshold=0.1)\n",
        "\n",
        "        # for i, masks in enumerate(batch_masks):\n",
        "        #     image = batch_frames[i]\n",
        "        #     image = render_ocr_result(image, masks)\n",
        "        #     # image = draw_mask_result(masks, image)\n",
        "        #     out.write(image)\n",
        "\n",
        "        # pbar.update(batch_frame_size * 100/frames_to_process)\n",
        "        # frame_count += batch_frame_size\n",
        "\n",
        "\n",
        "        image = frame.copy()\n",
        "        everything_results = model(image, device='cuda', retina_masks=True, imgsz=1024, conf=0.4, iou=0.9)\n",
        "        masks = prompt_process.custom_filter_mask(text=prompt, source=\"\", results=everything_results, threshold=0.1)\n",
        "\n",
        "        # image = draw_mask_result(masks, image)\n",
        "        image = render_ocr_result(image, masks)\n",
        "\n",
        "        out.write(image)\n",
        "        pbar.update(100/frames_to_process)\n",
        "        frame_count += 1\n",
        "\n",
        "\n",
        "del model\n",
        "del prompt_process\n",
        "\n",
        "cap.release()\n",
        "out.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GOmxAqGoAlr6",
      "metadata": {
        "id": "GOmxAqGoAlr6"
      },
      "source": [
        "### tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DsgSmZE3AlQv",
      "metadata": {
        "id": "DsgSmZE3AlQv"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def denomarize_mask(mask, width, height):\n",
        "    denomarized_mask = np.round(mask * np.array([width, height])).astype(np.int32)\n",
        "    return denomarized_mask\n",
        "\n",
        "\n",
        "with open(\"/content/samples/outputs/demo04/mask_data.jsonl\") as f:\n",
        "    first_mask = json.loads(f.readline())\n",
        "    masks = first_mask[\"masks\"]\n",
        "\n",
        "# 動画ファイルを読み込む\n",
        "video_path = \"/content/samples/outputs/demo04/demo04.mp4\"\n",
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "# 出力する動画の設定\n",
        "fps = int(video.get(cv2.CAP_PROP_FPS))\n",
        "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "output = cv2.VideoWriter(\"output_tracker_video_04.mp4\", fourcc, fps, (width, height))\n",
        "\n",
        "# トラッキングする領域を選択する\n",
        "_, frame = video.read()\n",
        "\n",
        "mask = denomarize_mask(masks[0], width, height)\n",
        "\n",
        "# maskは4点の座標を持つため、それを使ってbboxを作成する\n",
        "# ここでは、単純に最小のx, yと最大のx, yを使ってbboxを作成します\n",
        "x, y = mask.min(axis=0)\n",
        "w, h = mask.max(axis=0) - mask.min(axis=0)\n",
        "\n",
        "bbox = (x, y, w, h)\n",
        "\n",
        "# トラッキングメソッドを初期化する（ここではMILを使用しますが、他にもいくつか選択肢があります）\n",
        "tracker = cv2.TrackerMIL_create()\n",
        "tracker.init(frame, bbox)\n",
        "\n",
        "\n",
        "# 動画を通じてトラッキングを行う\n",
        "while video.isOpened():\n",
        "    ret, frame = video.read()\n",
        "    if not ret:\n",
        "        break  # 動画が終了したらループを抜ける\n",
        "\n",
        "    # 物体のトラッキングを更新する\n",
        "    success, bbox = tracker.update(frame)\n",
        "\n",
        "    if success:\n",
        "        x, y, w, h = [int(v) for v in bbox]\n",
        "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2, 1)\n",
        "    else:\n",
        "        cv2.putText(\n",
        "            frame,\n",
        "            \"Tracking failure detected\",\n",
        "            (100, 80),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            0.75,\n",
        "            (0, 0, 255),\n",
        "            2,\n",
        "        )\n",
        "\n",
        "    # 処理したフレームを出力動画に追加する\n",
        "    output.write(frame)\n",
        "\n",
        "\n",
        "# リソースの解放\n",
        "video.release()\n",
        "output.release()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z3mUiEmMi8Rl",
      "metadata": {
        "id": "z3mUiEmMi8Rl"
      },
      "outputs": [],
      "source": [
        "!pip install -U opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Er75HGkxj32D",
      "metadata": {
        "id": "Er75HGkxj32D"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/opencv/opencv_zoo/blob/main/models/object_tracking_vittrack/object_tracking_vittrack_2023sep.onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g3cAsyUXSuw3",
      "metadata": {
        "id": "g3cAsyUXSuw3"
      },
      "outputs": [],
      "source": [
        "# multi object tracking\n",
        "import json\n",
        "\n",
        "import clip\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def fast_retrieve(model, preprocess, elements, text_features: str, device) -> int:\n",
        "    \"\"\"Processes images and text with a model, calculates similarity, and returns softmax score.\"\"\"\n",
        "    print(elements)\n",
        "    preprocessed_images = [preprocess(image).to(device) for image in elements]\n",
        "    stacked_images = torch.stack(preprocessed_images)\n",
        "    image_features = model.encode_image(stacked_images)\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    probs = 100.0 * image_features @ text_features.T\n",
        "    return probs[:, 0].softmax(dim=0)\n",
        "\n",
        "\n",
        "def crop_boxes(orig_img, boxes):\n",
        "    image = Image.fromarray(cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB))\n",
        "    cropped_boxes = []\n",
        "    for box in boxes:\n",
        "        x, y, w, h = box\n",
        "        cropped_boxes.append(image.crop((x, y, x + w, y + h)))\n",
        "    return cropped_boxes\n",
        "\n",
        "\n",
        "def filter_trackers(\n",
        "    trackers,\n",
        "    frame,\n",
        "    clip_model,\n",
        "    clip_preprocess,\n",
        "    text_features,\n",
        "    threshold=0.5,\n",
        "    device=\"cuda\",\n",
        "):\n",
        "    if len(trackers) == 0:\n",
        "        return []\n",
        "\n",
        "    # trackerのbboxが小さいものは除外\n",
        "    trackers = [\n",
        "        tracker for tracker in trackers if tracker[\"bbox\"][2] * tracker[\"bbox\"][3] > 100\n",
        "    ]\n",
        "\n",
        "    boxes = [tracker[\"bbox\"] for tracker in trackers]\n",
        "\n",
        "    box_images = crop_boxes(frame, boxes)\n",
        "\n",
        "    # print(len(box_images))\n",
        "    # for box_image in box_images:\n",
        "    #     print(box_image.size)\n",
        "\n",
        "    # 画像とテキストの特徴量を取得\n",
        "    image_features = fast_retrieve(\n",
        "        clip_model, clip_preprocess, box_images, text_features, device\n",
        "    )\n",
        "\n",
        "    # 類似度が低いトラッカーを削除\n",
        "    new_trackers = []\n",
        "    for tracker, similarity in zip(trackers, image_features):\n",
        "        if similarity > threshold:\n",
        "            new_trackers.append(tracker)\n",
        "        else:\n",
        "            print(\"similarity is low\")\n",
        "\n",
        "    return new_trackers\n",
        "\n",
        "\n",
        "def random_color():\n",
        "    return np.random.randint(0, 255, (3,)).tolist()\n",
        "\n",
        "\n",
        "def denomarize_masks(masks, width, height):\n",
        "    denomarized_mask = np.round(masks * np.array([[width, height]])).astype(np.int32)\n",
        "    return denomarized_mask\n",
        "\n",
        "\n",
        "def detect_objects_from_file(f, width, height):\n",
        "    if not f.seekable():\n",
        "        raise ValueError(\"File must be seekable\")\n",
        "\n",
        "    data = f.readline()\n",
        "    if not data:\n",
        "        return []\n",
        "\n",
        "    first_mask = json.loads(data)\n",
        "    if not first_mask:\n",
        "        return []\n",
        "    masks = first_mask[\"masks\"]\n",
        "    masks = denomarize_masks(masks, width, height)\n",
        "\n",
        "    # maskは4点の座標を持つため、それを使ってbboxを作成する\n",
        "    # ここでは、単純に最小のx, yと最大のx, yを使ってbboxを作成します\n",
        "    bboxes = []\n",
        "    for mask in masks:\n",
        "        try:\n",
        "            x, y = mask.min(axis=0)\n",
        "            w, h = mask.max(axis=0) - mask.min(axis=0)\n",
        "            bboxes.append((x, y, w, h))\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            pass\n",
        "\n",
        "    return bboxes  # [(x1, y1, w1, h1), (x2, y2, w2, h2), ...]\n",
        "\n",
        "\n",
        "# 重複をチェックする関数\n",
        "def is_overlapping(new_box, existing_boxes, threshold=0.5):\n",
        "    x1, y1, w1, h1 = new_box\n",
        "    for box in existing_boxes:\n",
        "        x2, y2, w2, h2 = box\n",
        "        # 重なっている領域を計算\n",
        "        dx = min(x1 + w1, x2 + w2) - max(x1, x2)\n",
        "        dy = min(y1 + h1, y2 + h2) - max(y1, y2)\n",
        "        if dx >= 0 and dy >= 0:\n",
        "            intersect_area = dx * dy\n",
        "            area1 = w1 * h1\n",
        "            area2 = w2 * h2\n",
        "            # 重なっている領域が両方のバウンディングボックスのいずれかの一定の割合以上であるか確認\n",
        "            if intersect_area > threshold * area1 or intersect_area > threshold * area2:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "\n",
        "# 動画ファイルを読み込む\n",
        "video_path = \"/content/samples/outputs/demo02/demo02.mp4\"\n",
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "# 出力する動画の設定\n",
        "fps = int(video.get(cv2.CAP_PROP_FPS))\n",
        "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "output = cv2.VideoWriter(\n",
        "    \"output_tracker_video_02_multi.mp4\", fourcc, fps, (width, height)\n",
        ")\n",
        "\n",
        "trackers = []\n",
        "\n",
        "\n",
        "mask_data_file = open(\"/content/samples/outputs/demo02/mask_data.jsonl\")\n",
        "\n",
        "vit_params = cv2.TrackerVit_Params()\n",
        "vit_params.net = \"/content/samples/object_tracking_vittrack_2023sep.onnx\"\n",
        "\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=\"cuda\")\n",
        "text_features = clip_model.encode_text(\n",
        "    clip.tokenize([\"operation system, windows, macos, browser, display word\"]).to(\n",
        "        \"cuda\"\n",
        "    )\n",
        ")\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "frame_count = 0\n",
        "target_secods = 10\n",
        "while video.isOpened():\n",
        "    ret, frame = video.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    if frame_count > (fps * target_secods) == 0:\n",
        "        break\n",
        "\n",
        "    # トラッカーの更新\n",
        "    trackers = filter_trackers(\n",
        "        trackers, frame, clip_model, clip_preprocess, text_features, threshold=0.1\n",
        "    )\n",
        "    existing_boxes = [\n",
        "        tracker[\"bbox\"] for tracker in trackers\n",
        "    ]  # 既存のバウンディングボックスを取得\n",
        "\n",
        "    new_objects = detect_objects_from_file(mask_data_file, width, height)\n",
        "\n",
        "    # 新しい物体が既に追跡されていないか確認\n",
        "    for new_box in new_objects:\n",
        "        if not is_overlapping(new_box, existing_boxes):\n",
        "            print(\"new object\")\n",
        "            tracker = cv2.TrackerVit.create(vit_params)\n",
        "            tracker.init(frame, new_box)\n",
        "            new_color = random_color()\n",
        "            tracker_item = {\"tracker\": tracker, \"color\": new_color, \"bbox\": new_box}\n",
        "            trackers.append(tracker_item)\n",
        "\n",
        "    # トラッカーの更新と描画\n",
        "    for tracker_item in trackers:\n",
        "        success, bbox = tracker_item[\"tracker\"].update(frame)\n",
        "        if success:\n",
        "            x, y, w, h = [int(v) for v in bbox]\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), tracker_item[\"color\"], 3, 1)\n",
        "            # トラッキングしたバウンディングボックスを更新\n",
        "            tracker_item[\"bbox\"] = (x, y, w, h)\n",
        "        else:\n",
        "            # トラッキングに失敗した場合はトラッカーを削除\n",
        "            print(\"lost object\")\n",
        "            trackers.remove(tracker_item)\n",
        "\n",
        "    # 処理したフレームを出力動画に追加する\n",
        "    output.write(frame)\n",
        "    frame_count += 1\n",
        "\n",
        "    if frame_count % fps == 0:\n",
        "        print(f\"{frame_count} frames processed\")\n",
        "\n",
        "video.release()\n",
        "output.release()\n",
        "mask_data_file.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lPPqF_K2ip3Q",
      "metadata": {
        "id": "lPPqF_K2ip3Q"
      },
      "source": [
        "## 下記テスト"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HHUE3u75bRHy",
      "metadata": {
        "id": "HHUE3u75bRHy"
      },
      "outputs": [],
      "source": [
        "ann[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Uo5seBb_Pzkz",
      "metadata": {
        "id": "Uo5seBb_Pzkz"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "def denomarize_mask(mask, width, height):\n",
        "    denomarized_mask = np.round(mask.xyn * np.array([width, height])).astype(np.int32)\n",
        "    return denomarized_mask\n",
        "\n",
        "def segment_image(image, bbox):\n",
        "    \"\"\"Segments the given image according to the provided bounding box coordinates.\"\"\"\n",
        "    image_array = np.array(image)\n",
        "    segmented_image_array = np.zeros_like(image_array)\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    segmented_image_array[y1:y2, x1:x2] = image_array[y1:y2, x1:x2]\n",
        "    segmented_image = Image.fromarray(segmented_image_array)\n",
        "    black_image = Image.new(\"RGB\", image.size, (255, 255, 255))\n",
        "    # transparency_mask = np.zeros_like((), dtype=np.uint8)\n",
        "    transparency_mask = np.zeros((image_array.shape[0], image_array.shape[1]), dtype=np.uint8)\n",
        "    transparency_mask[y1:y2, x1:x2] = 255\n",
        "    transparency_mask_image = Image.fromarray(transparency_mask, mode=\"L\")\n",
        "    black_image.paste(segmented_image, mask=transparency_mask_image)\n",
        "    return black_image\n",
        "\n",
        "def filter_mask(image, masks, clip_model, preprocess):\n",
        "    results = []\n",
        "    height, width = image.shape[:2]\n",
        "\n",
        "    mask_h, mask_w = masks[0].cpu().numpy().shape[:2]\n",
        "    if mask_h != height or mask_w != width:\n",
        "          image = image.resize((mask_w, mask_h))\n",
        "\n",
        "    cropped_boxes = []\n",
        "    cropped_images = []\n",
        "    not_crop = []\n",
        "    # filter_id = []\n",
        "\n",
        "    for mask in masks:\n",
        "      if np.sum(mask[\"segmentation\"]) <= 100:\n",
        "          continue\n",
        "\n",
        "      segmentation = mask.cpu().numpy()\n",
        "\n",
        "      # ディスプレイぽいのを検出\n",
        "      contours, _ = cv2.findContours(\n",
        "          np.uint8(segmentation), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "      )\n",
        "\n",
        "      for contour in contours:\n",
        "            perimeter = cv2.arcLength(contour, True)\n",
        "            epsilon = 0.02 * perimeter\n",
        "            approx = cv2.approxPolyDP(contour, epsilon, True)\n",
        "            if len(approx) == 4:\n",
        "                cropped_boxes.append(segment_image(image, approx.squeeze().tolist()))\n",
        "                cropped_images.append(approx)\n",
        "\n",
        "\n",
        "\n",
        "def draw_fastsam_masks(\n",
        "    image: np.ndarray, masks: List[np.ndarray], alpha: float = 0.7\n",
        ") -> np.ndarray:\n",
        "    # masks = remove_overlapping_masks(masks)\n",
        "    surfaces = []\n",
        "    transparent_mask = np.zeros_like(image)\n",
        "    height, width = image.shape[:2]\n",
        "\n",
        "    for mask in masks:\n",
        "        segmentation = mask.cpu().numpy()\n",
        "\n",
        "        color = [randint(127, 255) for _ in range(3)]\n",
        "\n",
        "        # draw contour\n",
        "        contours, _ = cv2.findContours(\n",
        "            np.uint8(segmentation), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "        )\n",
        "        # cv2.drawContours(image, contours, -1, (0, 0, 255), 2)\n",
        "\n",
        "        for contour in contours:\n",
        "            # Calculate the perimeter of the contour\n",
        "            perimeter = cv2.arcLength(contour, True)\n",
        "            # Approximate the contour to a polygon\n",
        "            epsilon = 0.02 * perimeter  # 2% of the perimeter\n",
        "            approx = cv2.approxPolyDP(contour, epsilon, True)\n",
        "            if len(approx) == 4:\n",
        "              # Draw the approximated polygon (should be a quadrilateral if the shape is close to a rectangle)\n",
        "              cv2.drawContours(image, [approx], 0, (255, 0, 0), 2)\n",
        "              # cv2.drawContours(transparent_mask, [approx], 0, (255, 0, 0), -1)\n",
        "\n",
        "              surfaces.append(np.squeeze(approx, axis=1))\n",
        "\n",
        "    # image = cv2.addWeighted(transparent_mask, 1 - alpha, image, alpha, 0)\n",
        "\n",
        "    return image, surfaces\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GZsk8cQ7KGl-",
      "metadata": {
        "id": "GZsk8cQ7KGl-"
      },
      "outputs": [],
      "source": [
        "ori_image = cv2.imread(source, cv2.IMREAD_COLOR)\n",
        "# ori_image = cv2.cvtColor(ori_image, cv2.COLOR_BGR2RGB)\n",
        "# print(ann[0].masks)\n",
        "image, surfaces = draw_fastsam_masks(ori_image, ann[0].masks.data)\n",
        "\n",
        "cv2_imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30mi7GMiOq1l",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30mi7GMiOq1l",
        "outputId": "6d1dc5e6-18a0-404a-a71e-a2c1705a4d84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([132, 3072, 4080])"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ann[0].masks.data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1ACnJawdLs5",
      "metadata": {
        "id": "b1ACnJawdLs5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "a2e3Q7Xk_yI1",
        "8BcPYuxI3udK",
        "lPPqF_K2ip3Q"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
