{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80e44a88-7379-4443-b83b-bcfdd64ca9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn transformers matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25ac7084-ccb0-4078-9bbb-340b795673d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertModel\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f2f6f92-5deb-4477-ad00-4dcfa7a8d70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def digit_to_word(digit):\n",
    "    word_dict = {\n",
    "        '0': 'zero',\n",
    "        '1': 'one',\n",
    "        '2': 'two',\n",
    "        '3': 'three',\n",
    "        '4': 'four',\n",
    "        '5': 'five',\n",
    "        '6': 'six',\n",
    "        '7': 'seven',\n",
    "        '8': 'eight',\n",
    "        '9': 'nine',\n",
    "    }\n",
    "\n",
    "    # Convert tensor elements to strings and map using the dictionary\n",
    "    return [word_dict.get(str(d.item()), 'unknown') for d in digit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac797c93-ec32-40ed-8002-b88770c4b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset\n",
    "dataset = digit_to_word(torch.tensor([0,1,2,3,4,5,6,7,8,9,9,9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cc9c3cf-1dc0-4701-8e91-521b45b2e5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zero',\n",
       " 'one',\n",
       " 'two',\n",
       " 'three',\n",
       " 'four',\n",
       " 'five',\n",
       " 'six',\n",
       " 'seven',\n",
       " 'eight',\n",
       " 'nine',\n",
       " 'nine',\n",
       " 'nine']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35f3f2c1-0b6c-44d3-92d6-d12295d8bc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced Embeddings shape: (12, 10)\n",
      "Reduced Embeddings: [[-1.35483017e+00  2.83730442e+00 -1.59382267e+00 -1.78759244e-01\n",
      "   4.16839147e-01  2.54847272e-01  1.00929160e-01  7.60469733e-02\n",
      "   1.26888512e-02  6.77622292e-16]\n",
      " [ 1.47180889e+00  1.53807397e+00  1.89075082e+00 -1.42928837e+00\n",
      "  -3.67352037e-01 -1.91922436e-01  5.00334733e-02 -2.30013847e-02\n",
      "  -1.13014739e-02  6.77622292e-16]\n",
      " [ 5.25073335e+00  6.48742269e-01  4.59993114e-01  1.68743296e+00\n",
      "   2.29138813e-01  1.70471491e-01 -2.95726369e-02 -2.42961520e-02\n",
      "   1.36572639e-02  6.77622292e-16]\n",
      " [ 3.32678364e+00 -9.73974156e-01 -1.52830707e+00 -7.23986340e-01\n",
      "  -1.21546778e+00 -1.43203784e-01  1.48581590e-01  3.36304400e-03\n",
      "  -8.18041223e-02  6.77622292e-16]\n",
      " [ 1.45015289e+00 -1.06861063e+00 -4.10892292e-01 -7.47370913e-01\n",
      "   1.45622298e+00 -7.66172731e-01 -1.08374393e-01 -1.96262548e-01\n",
      "   1.99971215e-01  6.77622292e-16]\n",
      " [-5.93001380e-02 -9.07339529e-01  8.70469226e-02 -5.43175394e-01\n",
      "   3.32905089e-01  1.21072944e+00 -7.15861017e-01  1.50907845e-01\n",
      "  -2.20915952e-01  6.77622292e-16]\n",
      " [-1.39156921e+00 -7.81365776e-01  3.01847737e-01  6.78588169e-02\n",
      "   2.29013307e-01  6.23584367e-01  9.00285599e-01 -6.23505557e-01\n",
      "  -1.51014013e-01  6.77622292e-16]\n",
      " [-1.34141290e+00 -5.89344454e-01  2.36882002e-01  1.57916531e-01\n",
      "  -2.71857695e-01  3.38166693e-01  1.93370291e-01  4.69335743e-01\n",
      "   7.89409066e-01  6.77622292e-16]\n",
      " [-1.29908264e+00 -5.38987910e-01  2.62380191e-01  3.01517224e-01\n",
      "   2.42972639e-01 -4.26169281e-01  3.65427453e-01  8.99634178e-01\n",
      "  -4.60867951e-01  6.77622292e-16]\n",
      " [-2.01776124e+00 -5.48327361e-02  9.80404174e-02  4.69284908e-01\n",
      "  -3.50804822e-01 -3.56777010e-01 -3.01606507e-01 -2.44074047e-01\n",
      "  -2.99409616e-02  5.25102722e-16]\n",
      " [-2.01776124e+00 -5.48327361e-02  9.80404174e-02  4.69284908e-01\n",
      "  -3.50804822e-01 -3.56777010e-01 -3.01606507e-01 -2.44074047e-01\n",
      "  -2.99409616e-02  7.05421802e-16]\n",
      " [-2.01776124e+00 -5.48327361e-02  9.80404174e-02  4.69284908e-01\n",
      "  -3.50804822e-01 -3.56777010e-01 -3.01606507e-01 -2.44074047e-01\n",
      "  -2.99409616e-02  8.02342351e-16]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# # Sample dataset\n",
    "# dataset = [\n",
    "#     'one boy and two dogs', 'two','three','one', 'two','three','one', 'two','three','one', 'two','three','one', 'two','three'\n",
    "#     # Add more sentences as needed\n",
    "# ]\n",
    "# Step 1: Tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_texts = tokenizer(dataset, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Step 2: Obtaining Embeddings\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokenized_texts)\n",
    "\n",
    "# Extract the embeddings for each sentence\n",
    "embeddings = outputs.last_hidden_state[:, 0, :]  # Extract embeddings for [CLS] token\n",
    "\n",
    "# Step 3: Dimensionality Reduction with PCA\n",
    "n_components = min(10, embeddings.shape[1])  # Set n_components to be less than or equal to the number of features\n",
    "pca = PCA(n_components=n_components)\n",
    "embeddings_reduced = pca.fit_transform(embeddings)\n",
    "\n",
    "# Print the reduced embeddings\n",
    "print(\"Reduced Embeddings shape:\", embeddings_reduced.shape)\n",
    "print(\"Reduced Embeddings:\", embeddings_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ecb84de-f380-487e-b10f-1ffe34f5e851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size (excluding special tokens): 30520\n"
     ]
    }
   ],
   "source": [
    "# Get the vocabulary size excluding special tokens\n",
    "vocab_size_excluding_special = tokenizer.vocab_size - tokenizer.num_special_tokens_to_add()\n",
    "\n",
    "# Print the vocabulary size excluding special tokens\n",
    "print(\"Vocabulary Size (excluding special tokens):\", vocab_size_excluding_special)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9b998f5-62f3-4853-8df0-7dd0cd41eaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5]) becomes ['one', 'two', 'three', 'four', 'five']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def digit_to_word(digit):\n",
    "    word_dict = {\n",
    "        '0': 'zero',\n",
    "        '1': 'one',\n",
    "        '2': 'two',\n",
    "        '3': 'three',\n",
    "        '4': 'four',\n",
    "        '5': 'five',\n",
    "        '6': 'six',\n",
    "        '7': 'seven',\n",
    "        '8': 'eight',\n",
    "        '9': 'nine',\n",
    "    }\n",
    "\n",
    "    # Convert tensor elements to strings and map using the dictionary\n",
    "    string_representations = [word_dict.get(str(d.item()), 'unknown') for d in digit]\n",
    "\n",
    "    # Return the list of strings\n",
    "    return string_representations\n",
    "\n",
    "# Example usage:\n",
    "digits = torch.tensor([1, 2, 3, 4, 5])\n",
    "word_representations = digit_to_word(digits)\n",
    "print(f\"{digits} becomes {word_representations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88964008-79a8-4ef0-8444-f72bdddeec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import label_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e29d289-d83a-41e2-97a7-5c307c45576d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.15837019e-01,  2.55886055e+00,  8.54042530e-01,\n",
       "         2.82013066e-01,  1.01663013e-15],\n",
       "       [ 3.54278325e+00,  5.49610405e-01, -9.03860827e-01,\n",
       "        -2.83396282e-01,  1.01663013e-15],\n",
       "       [ 9.08933572e-01, -1.76249892e+00,  1.55149586e+00,\n",
       "        -1.48567446e-01,  1.01663013e-15],\n",
       "       [-1.10762063e+00, -9.61675566e-01, -8.11972534e-01,\n",
       "         1.33378403e+00,  1.01663013e-15],\n",
       "       [-2.42825918e+00, -3.84296470e-01, -6.89705028e-01,\n",
       "        -1.18383337e+00,  1.01663013e-15]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_embedding.label_embedding(word_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29bed3ad-362f-410a-bd5c-9aaec3c5a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def draw_graph(image_idx):\n",
    "    colors = []\n",
    "    # Data for the pie chart\n",
    "    # labels = ['Category A', 'Category B', 'Category C', 'Category D']\n",
    "    sizes = [.25, .30, .20, .25]  # Percentage distribution for each category\n",
    "    colors_blue = ['blue', 'lightblue', 'skyblue', 'deepskyblue']\n",
    "    colors_red = ['red', 'lightcoral', 'tomato', 'darkred']\n",
    "    colors_green = ['green', 'limegreen', 'forestgreen', 'mediumseagreen']\n",
    "    colors_purple = ['purple', 'mediumorchid', 'darkorchid', 'rebeccapurple']\n",
    "    colors_orange = ['orange', 'darkorange', 'coral', 'darkgoldenrod']\n",
    "    colors_yellow = ['yellow', 'gold', 'khaki', 'darkkhaki']\n",
    "    colors_pink = ['pink', 'lightpink', 'hotpink', 'deeppink']\n",
    "    colors_cyan = ['cyan', 'lightcyan', 'darkcyan', 'mediumturquoise']\n",
    "    colors_gray = ['gray', 'darkgray', 'lightgray', 'dimgray']\n",
    "    colors_brown = ['brown', 'saddlebrown', 'sienna', 'chocolate']\n",
    "    colors = [colors_blue, colors_red, colors_green, colors_purple, colors_orange, colors_yellow, colors_pink, colors_cyan, colors_gray, colors_brown]\n",
    "    # Plotting the pie chart\n",
    "    \n",
    "    color_choice = random.choice([0,1,2,3,4,5,6,7,8,9])\n",
    "    num1 = random.random()\n",
    "    num2 = random.random()\n",
    "    num3 = random.random()\n",
    "    normalization_factor = 1 / (num1 + num2 + num3)\n",
    "    \n",
    "    # Normalize the numbers\n",
    "    num1 *= normalization_factor\n",
    "    num2 *= normalization_factor\n",
    "    num3 *= normalization_factor\n",
    "    sizes = [num1, num2, num3]\n",
    "    plt.pie(sizes, colors=colors[color_choice], autopct='%1.1f%%', startangle=90)\n",
    "    # plt.title('Blue Pie Chart')\n",
    "    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    \n",
    "    image_path = os.path.join(output_dir, f\"{colors[color_choice][0]}_{image_idx}_{color_choice}.png\")\n",
    "    plt.savefig(image_path)\n",
    "    \n",
    "    # Close the current figure to free up resources\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd0411bb-df43-42dc-b92d-2c6c7460d532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Create a directory to save the images\n",
    "output_dir = \"image_dataset\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to generate and save images\n",
    "def generate_and_save_image(image_idx):\n",
    "\n",
    "    # Save the plot as an image using PIL\n",
    "    draw_graph(image_idx)\n",
    "\n",
    "# Generate and save a few images\n",
    "num_images = 15000\n",
    "for i in range(num_images):\n",
    "    generate_and_save_image(i)\n",
    "\n",
    "# Optionally, you can use the following code to load and display an image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc5bf4b8-c047-4a6f-8259-f9781567fdc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=640x480>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 480, 640])\n",
      "torch.Size([3, 480, 640])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "output_dir = \"image_dataset\"\n",
    "image_path = os.path.join(output_dir, \"red_17200_1.png\")\n",
    "img = Image.open(image_path)\n",
    "img.show()\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Apply the transformation to the image\n",
    "tensor_image = transform(img)\n",
    "\n",
    "# Now, tensor_image is a PyTorch tensor\n",
    "print(tensor_image.shape)\n",
    "rgb_image = tensor_image[:3, :, :]\n",
    "\n",
    "# Convert tensor to PIL Image\n",
    "pil_image = transforms.ToPILImage()(rgb_image)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(pil_image)\n",
    "print(rgb_image.size())\n",
    "plt.axis('off')  # Turn off axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08fb9239-a471-42e4-a7f0-9dc4ba6bab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplot_dataset\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43d18e74-eb85-4bfc-9f44-b9a8f93c7a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = matplot_dataset.matplot_dataset(\"image_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fe5af4b-a04b-4142-b132-a0dc99f32c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "image, label = image_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "710d20d7-f7bd-4b17-8b1f-6a72513feaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = image_dataset\n",
    "batch_size = 10\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81e0ddd3-2bf3-4dad-94a8-d77646bad98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 240, 320])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d4ddc2-0fd4-490f-b2f8-bfaa5c518afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63065227-a80b-4747-a768-db38682e2d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shape: torch.Size([4, 480, 640])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "colors = []\n",
    "# Data for the pie chart\n",
    "# labels = ['Category A', 'Category B', 'Category C', 'Category D']\n",
    "colors_blue = ['blue', 'lightblue', 'skyblue', 'deepskyblue']\n",
    "colors_red = ['red', 'lightcoral', 'tomato', 'darkred']\n",
    "colors_green = ['green', 'limegreen', 'forestgreen', 'mediumseagreen']\n",
    "colors_purple = ['purple', 'mediumorchid', 'darkorchid', 'rebeccapurple']\n",
    "colors_orange = ['orange', 'darkorange', 'coral', 'darkgoldenrod']\n",
    "colors_yellow = ['yellow', 'gold', 'khaki', 'darkkhaki']\n",
    "colors_pink = ['pink', 'lightpink', 'hotpink', 'deeppink']\n",
    "colors_cyan = ['cyan', 'lightcyan', 'darkcyan', 'mediumturquoise']\n",
    "colors_gray = ['gray', 'darkgray', 'lightgray', 'dimgray']\n",
    "colors_brown = ['brown', 'saddlebrown', 'sienna', 'chocolate']\n",
    "colors = [colors_blue, colors_red, colors_green, colors_purple, colors_orange, colors_yellow, colors_pink, colors_cyan, colors_gray, colors_brown]\n",
    "# Plotting the pie chart\n",
    "\n",
    "color_choice = random.choice([0,9])\n",
    "num1 = random.random()\n",
    "num2 = random.random()\n",
    "num3 = random.random()\n",
    "normalization_factor = 1 / (num1 + num2 + num3)\n",
    "\n",
    "# Normalize the numbers\n",
    "num1 *= normalization_factor\n",
    "num2 *= normalization_factor\n",
    "num3 *= normalization_factor\n",
    "sizes = [num1, num2, num3]\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie(sizes, colors=colors[color_choice], autopct='%1.1f%%', startangle=90)\n",
    "# plt.title('Blue Pie Chart')\n",
    "ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "fig.canvas.draw()\n",
    "image_np = np.array(fig.canvas.renderer.buffer_rgba())\n",
    "\n",
    "# Close the plot to prevent it from being displayed\n",
    "plt.close(fig)\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "tensor_image = torch.from_numpy(image_np).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "# Reshape the tensor to have a batch dimension (optional)\n",
    "# tensor_image = tensor_image.unsqueeze(0)\n",
    "\n",
    "print(\"Tensor shape:\", tensor_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6e8b983-9846-4dbb-ad9d-e2a0198ca605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3e02dea-750b-4653-b7ee-8a968511ef4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/330 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "  0%|          | 0/330 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 450.00 MiB. GPU 0 has a total capacty of 15.73 GiB of which 247.00 MiB is free. Process 810865 has 15.48 GiB memory in use. Of the allocated memory 15.21 GiB is allocated by PyTorch, and 73.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 413\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved model at \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m save_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 413\u001b[0m     \u001b[43mtrain_mnist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 358\u001b[0m, in \u001b[0;36mtrain_mnist\u001b[0;34m()\u001b[0m\n\u001b[1;32m    356\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    357\u001b[0m c \u001b[38;5;241m=\u001b[39m c\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 358\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mddpm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss_ema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 251\u001b[0m, in \u001b[0;36mDDPM.forward\u001b[0;34m(self, x, c)\u001b[0m\n\u001b[1;32m    249\u001b[0m context_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbernoulli(torch\u001b[38;5;241m.\u001b[39mzeros_like(c)\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_prob)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# return MSE between added noise, and our predicted noise\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_mse(noise, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_ts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_T\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_mask\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 162\u001b[0m, in \u001b[0;36mContextUnet.forward\u001b[0;34m(self, x, c, t, context_mask)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, c, t, context_mask):\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# x is (noisy) image, c is context label, t is timestep, \u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# context_mask says which samples to block the context on\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     down1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown1(x)\n\u001b[1;32m    164\u001b[0m     down2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown2(down1)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 55\u001b[0m, in \u001b[0;36mResidualConvBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_res:\n\u001b[0;32m---> 55\u001b[0m         x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m         x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x1)\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;66;03m# this adds on correct residual in case channels have increased\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 450.00 MiB. GPU 0 has a total capacty of 15.73 GiB of which 247.00 MiB is free. Process 810865 has 15.48 GiB memory in use. Of the allocated memory 15.21 GiB is allocated by PyTorch, and 73.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "''' \n",
    "This script does conditional image generation on MNIST, using a diffusion model\n",
    "\n",
    "This code is modified from,\n",
    "https://github.com/cloneofsimo/minDiffusion\n",
    "\n",
    "Diffusion model is based on DDPM,\n",
    "https://arxiv.org/abs/2006.11239\n",
    "\n",
    "The conditioning idea is taken from 'Classifier-Free Diffusion Guidance',\n",
    "https://arxiv.org/abs/2207.12598\n",
    "\n",
    "This technique also features in ImageGen 'Photorealistic Text-to-Image Diffusion Modelswith Deep Language Understanding',\n",
    "https://arxiv.org/abs/2205.11487\n",
    "\n",
    "'''\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import numpy as np\n",
    "import matplot_dataset\n",
    "\n",
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, is_res: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        '''\n",
    "        standard ResNet style convolutional block\n",
    "        '''\n",
    "        self.same_channels = in_channels==out_channels\n",
    "        self.is_res = is_res\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.is_res:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            # this adds on correct residual in case channels have increased\n",
    "            if self.same_channels:\n",
    "                out = x + x2\n",
    "            else:\n",
    "                out = x1 + x2 \n",
    "            return out / 1.414\n",
    "        else:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            return x2\n",
    "\n",
    "\n",
    "class UnetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetDown, self).__init__()\n",
    "        '''\n",
    "        process and downscale the image feature maps\n",
    "        '''\n",
    "        layers = [ResidualConvBlock(in_channels, out_channels), nn.MaxPool2d(2)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class UnetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UnetUp, self).__init__()\n",
    "        '''\n",
    "        process and upscale the image feature maps\n",
    "        '''\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        print(x.size())\n",
    "        print(skip.size())\n",
    "        x = torch.cat((x, skip), 1)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EmbedFC(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        super(EmbedFC, self).__init__()\n",
    "        '''\n",
    "        generic one layer FC NN for embedding things  \n",
    "        '''\n",
    "        self.input_dim = input_dim\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat = 256, n_classes=10):\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.n_feat = n_feat\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        self.down1 = UnetDown(n_feat, n_feat)\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat)\n",
    "\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "        self.contextembed1 = EmbedFC(n_classes, 2*n_feat)\n",
    "        self.contextembed2 = EmbedFC(n_classes, 1*n_feat)\n",
    "\n",
    "        self.up0 = nn.Sequential(\n",
    "            # nn.ConvTranspose2d(6 * n_feat, 2 * n_feat, 7, 7), # when concat temb and cemb end up w 6*n_feat\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, 7, 7), # otherwise just have 2*n_feat\n",
    "            nn.GroupNorm(8, 2 * n_feat),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1),\n",
    "            nn.GroupNorm(8, n_feat),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, c, t, context_mask):\n",
    "        # x is (noisy) image, c is context label, t is timestep, \n",
    "        # context_mask says which samples to block the context on\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "        down1 = self.down1(x)\n",
    "        down2 = self.down2(down1)\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "\n",
    "        # convert context to one hot embedding\n",
    "        c = nn.functional.one_hot(c, num_classes=self.n_classes).type(torch.float)\n",
    "        # mask out context if context_mask == 1\n",
    "        context_mask = context_mask[:, None]\n",
    "        context_mask = context_mask.repeat(1,self.n_classes)\n",
    "        context_mask = (-1*(1-context_mask)) # need to flip 0 <-> 1\n",
    "        c = c * context_mask\n",
    "        # embed context, time step\n",
    "        cemb1 = self.contextembed1(c).view(-1, self.n_feat * 2, 1, 1)\n",
    "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
    "        cemb2 = self.contextembed2(c).view(-1, self.n_feat, 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
    "        # could concatenate the context embedding here instead of adaGN\n",
    "        # hiddenvec = torch.cat((hiddenvec, temb1, cemb1), 1)\n",
    "\n",
    "        up1 = self.up0(hiddenvec)\n",
    "        # up2 = self.up1(up1, down2) # if want to avoid add and multiply embeddings\n",
    "        up2 = self.up1(cemb1*up1+ temb1, down2)  # add and multiply embeddings\n",
    "        up3 = self.up2(cemb2*up2+ temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out\n",
    "\n",
    "\n",
    "def ddpm_schedules(beta1, beta2, T):\n",
    "    \"\"\"\n",
    "    Returns pre-computed schedules for DDPM sampling, training process.\n",
    "    \"\"\"\n",
    "    assert beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n",
    "\n",
    "    beta_t = (beta2 - beta1) * torch.arange(0, T + 1, dtype=torch.float32) / T + beta1\n",
    "    sqrt_beta_t = torch.sqrt(beta_t)\n",
    "    alpha_t = 1 - beta_t\n",
    "    log_alpha_t = torch.log(alpha_t)\n",
    "    alphabar_t = torch.cumsum(log_alpha_t, dim=0).exp()\n",
    "\n",
    "    sqrtab = torch.sqrt(alphabar_t)\n",
    "    oneover_sqrta = 1 / torch.sqrt(alpha_t)\n",
    "\n",
    "    sqrtmab = torch.sqrt(1 - alphabar_t)\n",
    "    mab_over_sqrtmab_inv = (1 - alpha_t) / sqrtmab\n",
    "\n",
    "    return {\n",
    "        \"alpha_t\": alpha_t,  # \\alpha_t\n",
    "        \"oneover_sqrta\": oneover_sqrta,  # 1/\\sqrt{\\alpha_t}\n",
    "        \"sqrt_beta_t\": sqrt_beta_t,  # \\sqrt{\\beta_t}\n",
    "        \"alphabar_t\": alphabar_t,  # \\bar{\\alpha_t}\n",
    "        \"sqrtab\": sqrtab,  # \\sqrt{\\bar{\\alpha_t}}\n",
    "        \"sqrtmab\": sqrtmab,  # \\sqrt{1-\\bar{\\alpha_t}}\n",
    "        \"mab_over_sqrtmab\": mab_over_sqrtmab_inv,  # (1-\\alpha_t)/\\sqrt{1-\\bar{\\alpha_t}}\n",
    "    }\n",
    "\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self, nn_model, betas, n_T, device, drop_prob=0.1):\n",
    "        super(DDPM, self).__init__()\n",
    "        self.nn_model = nn_model.to(device)\n",
    "\n",
    "        # register_buffer allows accessing dictionary produced by ddpm_schedules\n",
    "        # e.g. can access self.sqrtab later\n",
    "        for k, v in ddpm_schedules(betas[0], betas[1], n_T).items():\n",
    "            self.register_buffer(k, v)\n",
    "\n",
    "        self.n_T = n_T\n",
    "        self.device = device\n",
    "        self.drop_prob = drop_prob\n",
    "        self.loss_mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        \"\"\"\n",
    "        this method is used in training, so samples t and noise randomly\n",
    "        \"\"\"\n",
    "\n",
    "        _ts = torch.randint(1, self.n_T+1, (x.shape[0],)).to(self.device)  # t ~ Uniform(0, n_T)\n",
    "        noise = torch.randn_like(x)  # eps ~ N(0, 1)\n",
    "\n",
    "        x_t = (\n",
    "            self.sqrtab[_ts, None, None, None] * x\n",
    "            + self.sqrtmab[_ts, None, None, None] * noise\n",
    "        )  # This is the x_t, which is sqrt(alphabar) x_0 + sqrt(1-alphabar) * eps\n",
    "        # We should predict the \"error term\" from this x_t. Loss is what we return.\n",
    "\n",
    "        # dropout context with some probability\n",
    "        context_mask = torch.bernoulli(torch.zeros_like(c)+self.drop_prob).to(self.device)\n",
    "        # return MSE between added noise, and our predicted noise\n",
    "        return self.loss_mse(noise, self.nn_model(x_t, c, _ts / self.n_T, context_mask))\n",
    "\n",
    "    def sample(self, n_sample, size, device, guide_w = 0.0):\n",
    "        # we follow the guidance sampling scheme described in 'Classifier-Free Diffusion Guidance'\n",
    "        # to make the fwd passes efficient, we concat two versions of the dataset,\n",
    "        # one with context_mask=0 and the other context_mask=1\n",
    "        # we then mix the outputs with the guidance scale, w\n",
    "        # where w>0 means more guidance\n",
    "\n",
    "        x_i = torch.randn(n_sample, *size).to(device)  # x_T ~ N(0, 1), sample initial noise\n",
    "        c_i = torch.arange(0,10).to(device) # context for us just cycles throught the mnist labels\n",
    "        c_i = c_i.repeat(int(n_sample/c_i.shape[0]))\n",
    "\n",
    "        # don't drop context at test time\n",
    "        context_mask = torch.zeros_like(c_i).to(device)\n",
    "\n",
    "        # double the batch\n",
    "        c_i = c_i.repeat(2)\n",
    "        context_mask = context_mask.repeat(2)\n",
    "        context_mask[n_sample:] = 1. # makes second half of batch context free\n",
    "\n",
    "        x_i_store = [] # keep track of generated steps in case want to plot something \n",
    "        print()\n",
    "        for i in range(self.n_T, 0, -1):\n",
    "            print(f'sampling timestep {i}',end='\\r')\n",
    "            t_is = torch.tensor([i / self.n_T]).to(device)\n",
    "            t_is = t_is.repeat(n_sample,1,1,1)\n",
    "\n",
    "            # double batch\n",
    "            x_i = x_i.repeat(2,1,1,1)\n",
    "            t_is = t_is.repeat(2,1,1,1)\n",
    "\n",
    "            z = torch.randn(n_sample, *size).to(device) if i > 1 else 0\n",
    "\n",
    "            # split predictions and compute weighting\n",
    "            eps = self.nn_model(x_i, c_i, t_is, context_mask)\n",
    "            eps1 = eps[:n_sample]\n",
    "            eps2 = eps[n_sample:]\n",
    "            eps = (1+guide_w)*eps1 - guide_w*eps2\n",
    "            x_i = x_i[:n_sample]\n",
    "            x_i = (\n",
    "                self.oneover_sqrta[i] * (x_i - eps * self.mab_over_sqrtmab[i])\n",
    "                + self.sqrt_beta_t[i] * z\n",
    "            )\n",
    "            if i%20==0 or i==self.n_T or i<8:\n",
    "                x_i_store.append(x_i.detach().cpu().numpy())\n",
    "        \n",
    "        x_i_store = np.array(x_i_store)\n",
    "        return x_i, x_i_store\n",
    "\n",
    "def digit_to_word(digit):\n",
    "    word_dict = {\n",
    "        '0': 'zero',\n",
    "        '1': 'one',\n",
    "        '2': 'two',\n",
    "        '3': 'three',\n",
    "        '4': 'four',\n",
    "        '5': 'five',\n",
    "        '6': 'six',\n",
    "        '7': 'seven',\n",
    "        '8': 'eight',\n",
    "        '9': 'nine',\n",
    "    }\n",
    "\n",
    "    # Convert tensor elements to strings and map using the dictionary\n",
    "    return [word_dict.get(str(d.item()), 'unknown') for d in digit]\n",
    "\n",
    "\n",
    "def train_mnist():\n",
    "\n",
    "    # hardcoding these here\n",
    "    n_epoch = 20\n",
    "    batch_size = 16\n",
    "    n_T = 400 # 500\n",
    "    device = \"cuda:0\"\n",
    "    n_classes = 2\n",
    "    n_feat = 128 # 128 ok, 256 better (but slower)\n",
    "    lrate = 1e-4\n",
    "    save_model = False\n",
    "    save_dir = './data/'\n",
    "    ws_test = [0.0, 0.5, 2.0] # strength of generative guidance\n",
    "\n",
    "    ddpm = DDPM(nn_model=ContextUnet(in_channels=3, n_feat=n_feat, n_classes=n_classes), betas=(1e-4, 0.02), n_T=n_T, device=device, drop_prob=0.1)\n",
    "    ddpm.to(device)\n",
    "\n",
    "    # optionally load a model\n",
    "    # ddpm.load_state_dict(torch.load(\"./data/diffusion_outputs/ddpm_unet01_mnist_9.pth\"))\n",
    "\n",
    "    tf = transforms.Compose([transforms.ToTensor()]) # mnist is already normalised 0 to 1\n",
    "\n",
    "    dataset = matplot_dataset.matplot_dataset(\"image_dataset\")\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=5)\n",
    "    optim = torch.optim.Adam(ddpm.parameters(), lr=lrate)\n",
    "\n",
    "    for ep in range(n_epoch):\n",
    "        print(f'epoch {ep}')\n",
    "        ddpm.train()\n",
    "\n",
    "        # linear lrate decay\n",
    "        optim.param_groups[0]['lr'] = lrate*(1-ep/n_epoch)\n",
    "\n",
    "        pbar = tqdm(dataloader)\n",
    "        loss_ema = None\n",
    "        for x, c in pbar:\n",
    "            optim.zero_grad()\n",
    "            x = x.to(device)\n",
    "            c = c.to(device)\n",
    "            loss = ddpm(x, c)\n",
    "            loss.backward()\n",
    "            if loss_ema is None:\n",
    "                loss_ema = loss.item()\n",
    "            else:\n",
    "                loss_ema = 0.95 * loss_ema + 0.05 * loss.item()\n",
    "            pbar.set_description(f\"loss: {loss_ema:.4f}\")\n",
    "            optim.step()\n",
    "        \n",
    "        # for eval, save an image of currently generated samples (top rows)\n",
    "        # followed by real images (bottom rows)\n",
    "        ddpm.eval()\n",
    "        with torch.no_grad():\n",
    "            n_sample = 4*n_classes\n",
    "            for w_i, w in enumerate(ws_test):\n",
    "                x_gen, x_gen_store = ddpm.sample(n_sample, (1, 28, 28), device, guide_w=w)\n",
    "\n",
    "                # append some real images at bottom, order by class also\n",
    "                x_real = torch.Tensor(x_gen.shape).to(device)\n",
    "                for k in range(n_classes):\n",
    "                    for j in range(int(n_sample/n_classes)):\n",
    "                        try: \n",
    "                            idx = torch.squeeze((c == k).nonzero())[j]\n",
    "                        except:\n",
    "                            idx = 0\n",
    "                        x_real[k+(j*n_classes)] = x[idx]\n",
    "\n",
    "                x_all = torch.cat([x_gen, x_real])\n",
    "                grid = make_grid(x_all*-1 + 1, nrow=10)\n",
    "                save_image(grid, save_dir + f\"image_ep{ep}_w{w}.png\")\n",
    "                print('saved image at ' + save_dir + f\"image_ep{ep}_w{w}.png\")\n",
    "\n",
    "                if ep%5==0 or ep == int(n_epoch-1):\n",
    "                    # create gif of images evolving over time, based on x_gen_store\n",
    "                    fig, axs = plt.subplots(nrows=int(n_sample/n_classes), ncols=n_classes,sharex=True,sharey=True,figsize=(8,3))\n",
    "                    def animate_diff(i, x_gen_store):\n",
    "                        print(f'gif animating frame {i} of {x_gen_store.shape[0]}', end='\\r')\n",
    "                        plots = []\n",
    "                        for row in range(int(n_sample/n_classes)):\n",
    "                            for col in range(n_classes):\n",
    "                                axs[row, col].clear()\n",
    "                                axs[row, col].set_xticks([])\n",
    "                                axs[row, col].set_yticks([])\n",
    "                                # plots.append(axs[row, col].imshow(x_gen_store[i,(row*n_classes)+col,0],cmap='gray'))\n",
    "                                plots.append(axs[row, col].imshow(-x_gen_store[i,(row*n_classes)+col,0],cmap='gray',vmin=(-x_gen_store[i]).min(), vmax=(-x_gen_store[i]).max()))\n",
    "                        return plots\n",
    "                    ani = FuncAnimation(fig, animate_diff, fargs=[x_gen_store],  interval=200, blit=False, repeat=True, frames=x_gen_store.shape[0])    \n",
    "                    ani.save(save_dir + f\"gif_ep{ep}_w{w}.gif\", dpi=100, writer=PillowWriter(fps=5))\n",
    "                    print('saved image at ' + save_dir + f\"gif_ep{ep}_w{w}.gif\")\n",
    "        # optionally save model\n",
    "        if save_model and ep == int(n_epoch-1):\n",
    "            torch.save(ddpm.state_dict(), save_dir + f\"model_{ep}.pth\")\n",
    "            print('saved model at ' + save_dir + f\"model_{ep}.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_mnist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be4dd7a2-e2e3-4bee-bcd8-1a90d8d01a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced3b9f3-3c07-449c-a8b2-2e1af697e90a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
