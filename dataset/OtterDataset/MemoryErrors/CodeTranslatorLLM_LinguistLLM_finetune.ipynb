{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV5dVMma2xTZ",
        "outputId": "e846807e-a420-4d89-a6f6-f69da9e05a2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /scratch/5404940.1.academic-gpu/pip-req-build-b64xtmme\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /scratch/5404940.1.academic-gpu/pip-req-build-b64xtmme\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Resolved https://github.com/huggingface/transformers.git to commit 8e64ba2890bd3231916cddcec77ba6331c306031\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: filelock in ./env_folder/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in ./env_folder/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.21.4)\n",
            "Requirement already satisfied: numpy>=1.17 in ./env_folder/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./env_folder/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./env_folder/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in ./env_folder/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in ./env_folder/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./env_folder/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (2024.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./env_folder/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./env_folder/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env_folder/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./env_folder/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (2024.2.2)\n",
            "Collecting accelerate\n",
            "  Using cached accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in ./env_folder/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
            "Requirement already satisfied: pyyaml in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in ./env_folder/lib/python3.10/site-packages (from accelerate) (2.2.1)\n",
            "Requirement already satisfied: huggingface-hub in ./env_folder/lib/python3.10/site-packages (from accelerate) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in ./env_folder/lib/python3.10/site-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in ./env_folder/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
            "Requirement already satisfied: sympy in ./env_folder/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in ./env_folder/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in ./env_folder/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./env_folder/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./env_folder/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./env_folder/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./env_folder/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./env_folder/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./env_folder/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./env_folder/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./env_folder/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./env_folder/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./env_folder/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./env_folder/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in ./env_folder/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./env_folder/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.99)\n",
            "Requirement already satisfied: requests in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in ./env_folder/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./env_folder/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./env_folder/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env_folder/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./env_folder/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in ./env_folder/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: accelerate\n",
            "Successfully installed accelerate-0.28.0\n",
            "Looking in indexes: https://pypi.org/simple/\n",
            "Collecting bitsandbytes\n",
            "  Using cached bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: torch in ./env_folder/lib/python3.10/site-packages (from bitsandbytes) (2.2.1)\n",
            "Requirement already satisfied: numpy in ./env_folder/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in ./env_folder/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (4.10.0)\n",
            "Requirement already satisfied: sympy in ./env_folder/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\n",
            "Requirement already satisfied: networkx in ./env_folder/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.3)\n",
            "Requirement already satisfied: fsspec in ./env_folder/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./env_folder/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./env_folder/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./env_folder/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./env_folder/lib/python3.10/site-packages (from torch->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./env_folder/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./env_folder/lib/python3.10/site-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./env_folder/lib/python3.10/site-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./env_folder/lib/python3.10/site-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./env_folder/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./env_folder/lib/python3.10/site-packages (from torch->bitsandbytes) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./env_folder/lib/python3.10/site-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in ./env_folder/lib/python3.10/site-packages (from torch->bitsandbytes) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./env_folder/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.4.99)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in ./env_folder/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Using cached bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl (102.2 MB)\n",
            "Installing collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.43.0\n",
            "Collecting peft==0.8.2\n",
            "  Using cached peft-0.8.2-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in ./env_folder/lib/python3.10/site-packages (from peft==0.8.2) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from peft==0.8.2) (24.0)\n",
            "Requirement already satisfied: psutil in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from peft==0.8.2) (5.9.8)\n",
            "Requirement already satisfied: pyyaml in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from peft==0.8.2) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in ./env_folder/lib/python3.10/site-packages (from peft==0.8.2) (2.2.1)\n",
            "Requirement already satisfied: transformers in ./env_folder/lib/python3.10/site-packages (from peft==0.8.2) (4.39.0.dev0)\n",
            "Requirement already satisfied: tqdm in ./env_folder/lib/python3.10/site-packages (from peft==0.8.2) (4.66.2)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in ./env_folder/lib/python3.10/site-packages (from peft==0.8.2) (0.28.0)\n",
            "Requirement already satisfied: safetensors in ./env_folder/lib/python3.10/site-packages (from peft==0.8.2) (0.4.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in ./env_folder/lib/python3.10/site-packages (from peft==0.8.2) (0.21.4)\n",
            "Requirement already satisfied: filelock in ./env_folder/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.8.2) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./env_folder/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.8.2) (2024.2.0)\n",
            "Requirement already satisfied: requests in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.8.2) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.8.2) (4.10.0)\n",
            "Requirement already satisfied: sympy in ./env_folder/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2) (1.12)\n",
            "Requirement already satisfied: networkx in ./env_folder/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./env_folder/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./env_folder/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./env_folder/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./env_folder/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./env_folder/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./env_folder/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./env_folder/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./env_folder/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./env_folder/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./env_folder/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./env_folder/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in ./env_folder/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.8.2) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./env_folder/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft==0.8.2) (12.4.99)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./env_folder/lib/python3.10/site-packages (from transformers->peft==0.8.2) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./env_folder/lib/python3.10/site-packages (from transformers->peft==0.8.2) (0.15.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.8.2) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./env_folder/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.8.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./env_folder/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.8.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env_folder/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.8.2) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./env_folder/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.8.2) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in ./env_folder/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.8.2) (1.3.0)\n",
            "Using cached peft-0.8.2-py3-none-any.whl (183 kB)\n",
            "Installing collected packages: peft\n",
            "Successfully installed peft-0.8.2\n",
            "Collecting wandb==0.16.3\n",
            "  Using cached wandb-0.16.3-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting Click!=8.0.0,>=7.1 (from wandb==0.16.3)\n",
            "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb==0.16.3)\n",
            "  Using cached GitPython-3.1.42-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from wandb==0.16.3) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from wandb==0.16.3) (5.9.8)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb==0.16.3)\n",
            "  Using cached sentry_sdk-1.41.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb==0.16.3)\n",
            "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: PyYAML in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from wandb==0.16.3) (6.0.1)\n",
            "Collecting setproctitle (from wandb==0.16.3)\n",
            "  Using cached setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in ./env_folder/lib/python3.10/site-packages (from wandb==0.16.3) (68.2.2)\n",
            "Collecting appdirs>=1.4.3 (from wandb==0.16.3)\n",
            "  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting protobuf!=4.21.0,<5,>=3.19.0 (from wandb==0.16.3)\n",
            "  Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: six>=1.4.0 in ./env_folder/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb==0.16.3) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb==0.16.3)\n",
            "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./env_folder/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb==0.16.3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./env_folder/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb==0.16.3) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env_folder/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb==0.16.3) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./env_folder/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb==0.16.3) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.16.3)\n",
            "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Using cached wandb-0.16.3-py3-none-any.whl (2.2 MB)\n",
            "Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
            "Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Using cached GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "Using cached sentry_sdk-1.41.0-py2.py3-none-any.whl (258 kB)\n",
            "Using cached setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: appdirs, smmap, setproctitle, sentry-sdk, protobuf, docker-pycreds, Click, gitdb, GitPython, wandb\n",
            "Successfully installed Click-8.1.7 GitPython-3.1.42 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.11 protobuf-4.25.3 sentry-sdk-1.41.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.3\n",
            "Collecting huggingface_hub==0.20.3\n",
            "  Using cached huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: filelock in ./env_folder/lib/python3.10/site-packages (from huggingface_hub==0.20.3) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./env_folder/lib/python3.10/site-packages (from huggingface_hub==0.20.3) (2024.2.0)\n",
            "Requirement already satisfied: requests in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from huggingface_hub==0.20.3) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in ./env_folder/lib/python3.10/site-packages (from huggingface_hub==0.20.3) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from huggingface_hub==0.20.3) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from huggingface_hub==0.20.3) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr4/cs640/charoori/.local/lib/python3.10/site-packages (from huggingface_hub==0.20.3) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./env_folder/lib/python3.10/site-packages (from requests->huggingface_hub==0.20.3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./env_folder/lib/python3.10/site-packages (from requests->huggingface_hub==0.20.3) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env_folder/lib/python3.10/site-packages (from requests->huggingface_hub==0.20.3) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./env_folder/lib/python3.10/site-packages (from requests->huggingface_hub==0.20.3) (2024.2.2)\n",
            "Using cached huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
            "Installing collected packages: huggingface_hub\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.21.4\n",
            "    Uninstalling huggingface-hub-0.21.4:\n",
            "      Successfully uninstalled huggingface-hub-0.21.4\n",
            "Successfully installed huggingface_hub-0.20.3\n"
          ]
        }
      ],
      "source": [
        "!pip install -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -U accelerate\n",
        "!pip install -U datasets>=2.16.1\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
        "!pip install -U peft==0.8.2\n",
        "# !pip install -U trl==0.7.10\n",
        "!pip install -U wandb==0.16.3\n",
        "!pip install -U huggingface_hub==0.20.3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration already exists at /usr4/cs640/charoori/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n"
          ]
        }
      ],
      "source": [
        "!python -c \"from accelerate.utils import write_basic_config; write_basic_config(mixed_precision='fp16')\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "# Specify your desired directory for the transformers cache\n",
        "cache_dir = r\"/projectnb/ds598/students/charoori\"\n",
        "\n",
        "# Set the environment variable for this session and for any subprocesses launched from here\n",
        "os.environ[\"TRANSFORMERS_CACHE\"] = cache_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def formatting_func(example):\n",
        "    text = \"Program to be Translated : \" + example['program'] + \"\\n\" + example[\"fortran_code_explanation \"] + \"\\n\" + example[\"rust_code_translation\"]\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c58SjiOE2jvz",
        "outputId": "87b244fd-2597-4670-a7ac-96a9073bfd20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bigcode/starcoder2-3b\n",
            "trainable params: 151369728 || all params: 1591200768 || trainable%: 9.5129245186488\n",
            "trainable params: 4,546,560 || all params: 3,034,917,888 || trainable%: 0.14980833642903488\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 13/13 [00:00<00:00, 295.80 examples/s]\n",
            "/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training...\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 15.62 MiB is free. Including non-PyTorch memory, this process has 15.75 GiB memory in use. Of the allocated memory 15.33 GiB is allocated by PyTorch, and 49.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[1;32m/projectnb/ds598/students/charoori/finetune.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://scc-ondemand2.bu.edu/projectnb/ds598/students/charoori/finetune.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=176'>177</a>\u001b[0m os\u001b[39m.\u001b[39mmakedirs(args\u001b[39m.\u001b[39moutput_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell://scc-ondemand2.bu.edu/projectnb/ds598/students/charoori/finetune.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=178'>179</a>\u001b[0m logging\u001b[39m.\u001b[39mset_verbosity_error()\n\u001b[0;32m--> <a href='vscode-notebook-cell://scc-ondemand2.bu.edu/projectnb/ds598/students/charoori/finetune.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=180'>181</a>\u001b[0m main(args)\n",
            "\u001b[1;32m/projectnb/ds598/students/charoori/finetune.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://scc-ondemand2.bu.edu/projectnb/ds598/students/charoori/finetune.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=163'>164</a>\u001b[0m \u001b[39m# launch\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://scc-ondemand2.bu.edu/projectnb/ds598/students/charoori/finetune.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=164'>165</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://scc-ondemand2.bu.edu/projectnb/ds598/students/charoori/finetune.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=165'>166</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    <a href='vscode-notebook-cell://scc-ondemand2.bu.edu/projectnb/ds598/students/charoori/finetune.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=167'>168</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSaving the last checkpoint of the model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://scc-ondemand2.bu.edu/projectnb/ds598/students/charoori/finetune.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=168'>169</a>\u001b[0m model\u001b[39m.\u001b[39msave_pretrained(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(args\u001b[39m.\u001b[39moutput_dir, \u001b[39m\"\u001b[39m\u001b[39mfinal_checkpoint/\u001b[39m\u001b[39m\"\u001b[39m))\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/transformers/trainer.py:1637\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1635\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1636\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1637\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1638\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1639\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1640\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1641\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1642\u001b[0m     )\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/transformers/trainer.py:1974\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1971\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1973\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1974\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1976\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1977\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1978\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   1979\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1980\u001b[0m ):\n\u001b[1;32m   1981\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1982\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/transformers/trainer.py:2888\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2885\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2887\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2888\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2890\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2891\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/transformers/trainer.py:2911\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2909\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2910\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2911\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2912\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2913\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2914\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/accelerate/utils/operations.py:822\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 822\u001b[0m     \u001b[39mreturn\u001b[39;00m model_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/accelerate/utils/operations.py:810\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 810\u001b[0m     \u001b[39mreturn\u001b[39;00m convert_to_fp32(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/torch/amp/autocast_mode.py:16\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_autocast\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     15\u001b[0m     \u001b[39mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 16\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/peft/peft_model.py:1083\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m     \u001b[39mif\u001b[39;00m peft_config\u001b[39m.\u001b[39mpeft_type \u001b[39m==\u001b[39m PeftType\u001b[39m.\u001b[39mPOLY:\n\u001b[1;32m   1082\u001b[0m         kwargs[\u001b[39m\"\u001b[39m\u001b[39mtask_ids\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m task_ids\n\u001b[0;32m-> 1083\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model(\n\u001b[1;32m   1084\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1085\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1086\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1087\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   1088\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1089\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1090\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1091\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1092\u001b[0m     )\n\u001b[1;32m   1094\u001b[0m batch_size \u001b[39m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1095\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1096\u001b[0m     \u001b[39m# concat prompt attention mask\u001b[39;00m\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:161\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 161\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/transformers/models/starcoder2/modeling_starcoder2.py:1148\u001b[0m, in \u001b[0;36mStarcoder2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1145\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1147\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1148\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1149\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1150\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1151\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1152\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1153\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1154\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1155\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1156\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1157\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1158\u001b[0m )\n\u001b[1;32m   1160\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1161\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/transformers/models/starcoder2/modeling_starcoder2.py:1032\u001b[0m, in \u001b[0;36mStarcoder2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1023\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m   1024\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1029\u001b[0m         use_cache,\n\u001b[1;32m   1030\u001b[0m     )\n\u001b[1;32m   1031\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1032\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m   1033\u001b[0m         hidden_states,\n\u001b[1;32m   1034\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1035\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1036\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1037\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1038\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1039\u001b[0m     )\n\u001b[1;32m   1041\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/transformers/models/starcoder2/modeling_starcoder2.py:758\u001b[0m, in \u001b[0;36mStarcoder2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    756\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    757\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 758\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states)\n\u001b[1;32m    759\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    761\u001b[0m outputs \u001b[39m=\u001b[39m (hidden_states,)\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/transformers/models/starcoder2/modeling_starcoder2.py:157\u001b[0m, in \u001b[0;36mStarcoder2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: Optional[Tuple[torch\u001b[39m.\u001b[39mFloatTensor]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor:\n\u001b[0;32m--> 157\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc_fc(hidden_states)\n\u001b[1;32m    158\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(hidden_states)\n\u001b[1;32m    159\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(hidden_states)\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:429\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    426\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_dtype)\n\u001b[1;32m    428\u001b[0m bias \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 429\u001b[0m out \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39;49mmatmul_4bit(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mt(), bias\u001b[39m=\u001b[39;49mbias, quant_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mquant_state)\n\u001b[1;32m    431\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mto(inp_dtype)\n\u001b[1;32m    433\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:577\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n\u001b[1;32m    576\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m     \u001b[39mreturn\u001b[39;00m MatMul4Bit\u001b[39m.\u001b[39;49mapply(A, B, out, bias, quant_state)\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
            "File \u001b[0;32m/projectnb/ds598/students/charoori/env_folder/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:516\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mempty(A\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m B_shape[:\u001b[39m1\u001b[39m], dtype\u001b[39m=\u001b[39mA\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mA\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    514\u001b[0m \u001b[39m# 1. Dequantize\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[39m# 2. MatmulnN\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mlinear(A, F\u001b[39m.\u001b[39;49mdequantize_4bit(B, quant_state)\u001b[39m.\u001b[39;49mto(A\u001b[39m.\u001b[39;49mdtype)\u001b[39m.\u001b[39mt(), bias)\n\u001b[1;32m    518\u001b[0m \u001b[39m# 3. Save state\u001b[39;00m\n\u001b[1;32m    519\u001b[0m ctx\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m quant_state\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 15.62 MiB is free. Including non-PyTorch memory, this process has 15.75 GiB memory in use. Of the allocated memory 15.33 GiB is allocated by PyTorch, and 49.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "# Code adapted from https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama/scripts/supervised_finetuning.py\n",
        "# and https://huggingface.co/blog/gemma-peft\n",
        "import argparse\n",
        "import multiprocessing\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import Trainer\n",
        "from datasets import Dataset\n",
        "from accelerate import PartialState\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    logging,\n",
        "    set_seed,\n",
        ")\n",
        "# from trl import SFTTrainer\n",
        "\n",
        "\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\"--model_id\", type=str, default=\"bigcode/starcoder2-3b\")\n",
        "    parser.add_argument(\"--dataset_name\", type=str, default=\"CodeTranslatorLLM/Code-Translation\")\n",
        "    # parser.add_argument(\"--dataset_name\", type=str, default=\"bigcode/the-stack-smol-xs\")\n",
        "    parser.add_argument(\"--subset\", type=str, default=\"data/rust\")\n",
        "    parser.add_argument(\"--split\", type=str, default=\"train\")\n",
        "    parser.add_argument(\"--dataset_text_field\", type=str, default=\"content\")\n",
        "\n",
        "    parser.add_argument(\"--max_seq_length\", type=int, default=1024)\n",
        "    parser.add_argument(\"--max_steps\", type=int, default=10)\n",
        "    parser.add_argument(\"--micro_batch_size\", type=int, default=1)\n",
        "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=4)\n",
        "    parser.add_argument(\"--weight_decay\", type=float, default=0.01)\n",
        "    parser.add_argument(\"--bf16\", type=bool, default=True)\n",
        "\n",
        "    parser.add_argument(\"--attention_dropout\", type=float, default=0.1)\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=2e-4)\n",
        "    parser.add_argument(\"--lr_scheduler_type\", type=str, default=\"cosine\")\n",
        "    parser.add_argument(\"--warmup_steps\", type=int, default=100)\n",
        "    parser.add_argument(\"--seed\", type=int, default=0)\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"finetune_starcoder2\")\n",
        "    parser.add_argument(\"--num_proc\", type=int, default=None)\n",
        "    parser.add_argument(\"--push_to_hub\", type=bool, default=True)\n",
        "    return parser.parse_args(args=[])\n",
        "\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    # config\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    )\n",
        "    lora_config = LoraConfig(\n",
        "        r=8,\n",
        "        target_modules=[\n",
        "            \"q_proj\",\n",
        "            \"o_proj\",\n",
        "            \"k_proj\",\n",
        "            \"v_proj\",\n",
        "            \"gate_proj\",\n",
        "            \"up_proj\",\n",
        "            \"down_proj\",\n",
        "        ],\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "    # load model and dataset\n",
        "    token = os.getenv(\"HF_TOKEN\")\n",
        "    print(args.model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        args.model_id,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map={\"\": PartialState().process_index},\n",
        "        attention_dropout=args.attention_dropout,\n",
        "    )\n",
        "    print_trainable_parameters(model)\n",
        "\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    data = load_dataset(\n",
        "        args.dataset_name,\n",
        "        # data_dir=args.subset,\n",
        "        split=args.split,\n",
        "        token=token,\n",
        "        # num_proc=args.num_proc if args.num_proc else multiprocessing.cpu_count(),\n",
        "    ) \n",
        "\n",
        "    # data = data['content']\n",
        "    # data = Dataset.from_dict({'content': data})\n",
        "    # data[\"content\"] = data[\"content\"].shuffle().map(generate_and_tokenize_prompt, batched = False)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        args.model_id,\n",
        "        # padding_side=\"left\",\n",
        "        # add_eos_token=True,\n",
        "        # add_bos_token=True,\n",
        "    )\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    def generate_and_tokenize_prompt(prompt):\n",
        "        result =  tokenizer(\n",
        "            formatting_func(prompt),\n",
        "            truncation=True,\n",
        "            max_length=args.max_seq_length,\n",
        "            padding=\"max_length\",\n",
        "            )\n",
        "        \n",
        "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "        return result\n",
        "    \n",
        "    tokenized_train_dataset = data.map(generate_and_tokenize_prompt)\n",
        "\n",
        "\n",
        "    # setup the trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        train_dataset=tokenized_train_dataset,\n",
        "        # max_seq_length=args.max_seq_length,\n",
        "        args=transformers.TrainingArguments(\n",
        "            # per_device_train_batch_size=args.micro_batch_size,\n",
        "            per_device_train_batch_size=2,\n",
        "            gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "            warmup_steps=args.warmup_steps,\n",
        "            max_steps=args.max_steps,\n",
        "            learning_rate=args.learning_rate,\n",
        "            lr_scheduler_type=args.lr_scheduler_type,\n",
        "            weight_decay=args.weight_decay,\n",
        "            fp16 =True,\n",
        "            logging_strategy=\"steps\",\n",
        "            logging_steps=1,\n",
        "            output_dir=args.output_dir,\n",
        "            optim=\"paged_adamw_8bit\",\n",
        "            seed=args.seed,\n",
        "            run_name=f\"train-{args.model_id.split('/')[-1]}\",\n",
        "            report_to=\"wandb\",\n",
        "            # remove_unused_columns=False\n",
        "        ),\n",
        "        # peft_config=lora_config,\n",
        "        # dataset_text_field=args.dataset_text_field,\n",
        "    )\n",
        "\n",
        "    # launch\n",
        "    print(\"Training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"Saving the last checkpoint of the model\")\n",
        "    model.save_pretrained(os.path.join(args.output_dir, \"final_checkpoint/\"))\n",
        "    if args.push_to_hub:\n",
        "        trainer.push_to_hub(\"Upload model\")\n",
        "    print(\"Training Done! 💥\")\n",
        "\n",
        "\n",
        "args = get_args()\n",
        "set_seed(args.seed)\n",
        "os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "main(args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OD7qwlG57e9"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoder2-3b\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigcode/starcoder2-3b\")\n",
        "device = \"cuda\"\n",
        "\n",
        "inputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(inputs)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
