{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f13c7a3-6722-4da4-a696-65bc14a8a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72a4ec37-1f4a-45b1-9f2a-42ed82041c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset, Audio, Value\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    ")\n",
    "\n",
    "from speechtokenizer import SpeechTokenizer\n",
    "from audiotools import AudioSignal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae2542db-d1d8-4e4d-8c0e-a973f3b0d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "n_special_tokens = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a31c3da7-4cc1-421f-8778-c9eba23272ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ffebb45ca84f448fbef6a67908fac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/368k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a4888f09f947619b80b501ba3ecc02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/18.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9231bc6d03b248f7a1b85913f2c65dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/339 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc2d6b1e1444090829ef3ec4a26a674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/721 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd6e662a5844638a3a0bb6482d20c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce3ed6aeb9e430a9335de52996bf27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8e3ed0a8314472a78193ac7c9de145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f96c157b4b4163b2cfd328f52d77bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.91G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5290e658d71d40378af411fab4f4ec14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.83G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6ea098d5eb41deb69dadb63ea09652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.16G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1120214528724753abdd87dcc1b15d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.94 GiB. GPU 0 has a total capacity of 44.55 GiB of which 1.90 GiB is free. Process 79061 has 10.86 GiB memory in use. Process 93396 has 18.82 GiB memory in use. Including non-PyTorch memory, this process has 12.95 GiB memory in use. Of the allocated memory 12.69 GiB is allocated by PyTorch, and 2.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlexWortega/nemo_asr_mixed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path, cache_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meager\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py:3941\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3932\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3934\u001b[0m     (\n\u001b[1;32m   3935\u001b[0m         model,\n\u001b[1;32m   3936\u001b[0m         missing_keys,\n\u001b[1;32m   3937\u001b[0m         unexpected_keys,\n\u001b[1;32m   3938\u001b[0m         mismatched_keys,\n\u001b[1;32m   3939\u001b[0m         offload_index,\n\u001b[1;32m   3940\u001b[0m         error_msgs,\n\u001b[0;32m-> 3941\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3948\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3949\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3952\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3953\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3960\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3961\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py:4415\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[1;32m   4411\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   4412\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4413\u001b[0m                 )\n\u001b[1;32m   4414\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4415\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4416\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4417\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4418\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4419\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4420\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4421\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4422\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4423\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4424\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4425\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4426\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4427\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4428\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4429\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4430\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4431\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4432\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4434\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/transformers/modeling_utils.py:936\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    925\u001b[0m     state_dict_index \u001b[38;5;241m=\u001b[39m offload_weight(param, param_name, state_dict_folder, state_dict_index)\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m is_quantized\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m hf_quantizer\u001b[38;5;241m.\u001b[39mrequires_parameters_quantization)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m ):\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 936\u001b[0m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    938\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/accelerate/utils/modeling.py:416\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    414\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 416\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.94 GiB. GPU 0 has a total capacity of 44.55 GiB of which 1.90 GiB is free. Process 79061 has 10.86 GiB memory in use. Process 93396 has 18.82 GiB memory in use. Including non-PyTorch memory, this process has 12.95 GiB memory in use. Of the allocated memory 12.69 GiB is allocated by PyTorch, and 2.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model_path = \"AlexWortega/nemo_asr_mixed\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, cache_dir=\".\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, cache_dir=\".\", attn_implementation=\"eager\", device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e7de30-6ec2-4cb9-9b23-c8d53472e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vikhr4oDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, quantizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.soa = tokenizer(start_audio_token, return_tensors=\"pt\")[\"input_ids\"][:, -1:].to(device)\n",
    "        self.eoa = tokenizer(end_audio_token, return_tensors=\"pt\")[\"input_ids\"][:, -1:].to(device)\n",
    "        self.eos = tokenizer(end_sequence_token, return_tensors=\"pt\")[\"input_ids\"][:, -1:].to(device)\n",
    "\n",
    "        self.n_original_tokens = len(tokenizer) - 1024\n",
    "        self.quantizer = quantizer \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def quantize(self, example):\n",
    "        audio_data, sample_rate = example[\"audio\"][\"array\"], example[\"audio\"][\"sampling_rate\"]\n",
    "    \n",
    "        # audio -> discrete codes\n",
    "        audio = torch.tensor(audio_data).view(1, 1, len(audio_data)).float()\n",
    "        audio = audio.to(device)\n",
    "        codes = self.quantizer.encode(audio)\n",
    "        codes = codes.squeeze(1)\n",
    "    \n",
    "        # Move tensor back to CPU and delete it to free GPU memory\n",
    "        del audio\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "        # increment tokens' ids \n",
    "        return codes + self.n_original_tokens\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset[idx]\n",
    "\n",
    "        # get text tokens \n",
    "        text = row[\"text\"]\n",
    "        text_tokenized = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        text_input_tokens = text_tokenized[\"input_ids\"].to(device)\n",
    "\n",
    "        # quantize audio \n",
    "        codes = self.quantize(row)\n",
    "        raw_audio_tokens = codes[:n_codebooks]\n",
    "        \n",
    "        audio_input_tokens = raw_audio_tokens.contiguous().view(1, -1)\n",
    "\n",
    "        # determine number of audio tokens given max_seq_length \n",
    "        audio_length = min(max_seq_length - text_input_tokens.shape[-1] - n_special_tokens, audio_input_tokens.shape[-1])\n",
    "        audio_length -= audio_length % n_codebooks\n",
    "\n",
    "        audio_tokens = torch.cat([self.soa, audio_input_tokens[:, :audio_length], self.eoa], dim=1)\n",
    "        text_tokens = torch.cat([text_input_tokens, self.soa], dim=1)\n",
    "\n",
    "        return {\n",
    "            \"audio_tokens\": audio_tokens, \n",
    "            \"text_tokens\": text_tokens,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ea99e2-a79b-4268-bc1b-e6bde6ab4fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_padding_tokens(quantizer):\n",
    "    # create audio without any sounds \n",
    "    # seems to work better than radom padding if \n",
    "    # length of generated audio is not devisible by n_codebooks\n",
    "    audio = torch.zeros((1, 1, 1))\n",
    "    audio = audio.to(device)\n",
    "    \n",
    "    codes = quantizer.encode(audio)\n",
    "\n",
    "    # Move tensor back to CPU and delete it to free GPU memory\n",
    "    del audio\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\"audio_tokens\": codes.squeeze(1)}\n",
    "    \n",
    "\n",
    "\n",
    "def decode_audio(tokens, quantizer, pad_tokens, n_original_tokens):\n",
    "    # find start and end indices of audio tokens \n",
    "    start = torch.nonzero(tokens == start_audio_token_id)\n",
    "    end = torch.nonzero(tokens == end_audio_token_id)\n",
    "    \n",
    "    start = start[0, -1] + 1 if len(start) else 0\n",
    "    end = end[0, -1] if len(end) else tokens.shape[-1]\n",
    "    \n",
    "    # substract length of original vocabulary -> tokens in range [0, 1024)\n",
    "    audio_tokens = tokens[start:end] % n_original_tokens\n",
    "    reminder = audio_tokens.shape[-1] % n_codebooks\n",
    "    \n",
    "    if reminder:\n",
    "        # pad if last frame is incomplete \n",
    "        audio_tokens = torch.cat([audio_tokens, pad_tokens[reminder:n_codebooks]], dim=0)\n",
    "\n",
    "    transposed = audio_tokens.view(-1, n_codebooks).t()\n",
    "    codes = transposed.view(n_codebooks, 1, -1).to(device)\n",
    "\n",
    "    audio = quantizer.decode(codes).squeeze(0)\n",
    "\n",
    "    del tokens \n",
    "    del audio_tokens \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return AudioSignal(audio.detach().cpu().numpy(), quantizer.sample_rate)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb2279c-e9ef-48a0-8360-eaeda1a485fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_entire_model(model):\n",
    "    for n, p in model.named_parameters():\n",
    "        p.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee784812-cac8-444e-bda3-e2a889770ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_codebooks = 3\n",
    "\n",
    "config_path = \"./audiotokenizer/speechtokenizer_hubert_avg_config.json\"\n",
    "ckpt_path = \"./audiotokenizer/SpeechTokenizer.pt\"\n",
    "quantizer = SpeechTokenizer.load_from_checkpoint(config_path, ckpt_path)\n",
    "quantizer.eval()\n",
    "\n",
    "for n, child in quantizer.named_children():\n",
    "    child.to(model.device)\n",
    "    child = freeze_entire_model(child)\n",
    "\n",
    "codebook_size = quantizer.quantizer.bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24eed71-70e0-4322-b59a-59fa1182763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_librispeech():\n",
    "    raw = load_dataset(\"openslr/librispeech_asr\", \"clean\", cache_dir=\".\")\n",
    "    processed = raw.remove_columns([\"chapter_id\"])\n",
    "    processed = processed.cast_column('speaker_id', Value('string'))\n",
    "    return processed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740248c9-75d2-4b24-815c-1c5be51a3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_audio_token = \"<soa>\"\n",
    "end_audio_token = \"<eoa>\"\n",
    "end_sequence_token = \"<eos>\"\n",
    "\n",
    "n_tokens = len(tokenizer) - 1024\n",
    "\n",
    "dataset = prepare_librispeech()\n",
    "val_data = dataset[\"validation\"]\n",
    "\n",
    "val_dataset = Vikhr4oDataset(val_data, tokenizer, quantizer)\n",
    "\n",
    "padding_tokens = get_audio_padding_tokens(quantizer)[\"audio_tokens\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e445b21-1128-496a-8e02-70cb2a561055",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_examples = 5\n",
    "max_seq_length = 1024\n",
    "start_audio_token_id = tokenizer(start_audio_token)[\"input_ids\"][-1]\n",
    "end_audio_token_id = tokenizer(end_audio_token)[\"input_ids\"][-1]\n",
    "\n",
    "for i in range(n_examples):\n",
    "    row = val_dataset[i]\n",
    "\n",
    "    for k, v in row.items():\n",
    "        row[k] = v.to(\"cuda:0\")\n",
    "\n",
    "    # GT\n",
    "    print(\"GT:\", tokenizer.decode(row[\"text_tokens\"][0], skip_special_tokens=True))\n",
    "\n",
    "    # audio\n",
    "    attention_mask=torch.ones(row[\"text_tokens\"].size(), device=model.device)\n",
    "    output_audio = model.generate(row[\"text_tokens\"], attention_mask=attention_mask, max_new_tokens=max_seq_length, top_k=20, do_sample=True)\n",
    "    audio = decode_audio(output_audio[0], quantizer, padding_tokens.t()[0], n_tokens)\n",
    "    audio.write(f\"tests/audio_mixed_{i}.wav\")\n",
    "\n",
    "    # text\n",
    "    attention_mask=torch.ones(row[\"audio_tokens\"].size(), device=model.device)\n",
    "    output_text = model.generate(row[\"audio_tokens\"], attention_mask=attention_mask, max_new_tokens=max_seq_length, top_k=20, do_sample=True)\n",
    "    output_text = output_text.cpu()[0]\n",
    "    output_text = output_text[output_text < start_audio_token_id]\n",
    "    text = tokenizer.decode(output_text, skip_special_tokens=True)\n",
    "    print(text)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c95e0ac7-e3a5-4bf8-9432-01d51eaa4df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<audiotools.core.audio_signal.AudioSignal at 0x79293b99a4e0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_audio(output_audio[0], quantizer, padding_tokens.t()[0], n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac36a636-757f-472b-abc7-1eee352bdc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 328,   15,  842,  573,  612,  345, 1003,  399]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_tokens.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d62af33-1914-4505-a157-de6832aa01f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
