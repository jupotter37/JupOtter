{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "batch_size = 2\n",
    "image_size = 128\n",
    "num_classes = 21  # Replace with the number of classes in your dataset\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class DataProcessor:\n",
    "    @staticmethod\n",
    "    def get_data(directory_path, flag):\n",
    "        images = []\n",
    "        for img_path in glob.glob(os.path.join(directory_path, \"*.tif\")):\n",
    "            img = cv2.imread(img_path, flag)\n",
    "            images.append(img)\n",
    "            \n",
    "        images = np.array(images)\n",
    "        return images\n",
    "\n",
    "    @staticmethod\n",
    "    def shuffle_data(images, masks):\n",
    "        images, masks = shuffle(images, masks, random_state=0)\n",
    "        return images, masks\n",
    "\n",
    "    def preprocess_data(self, train_images, train_masks, val_images, val_masks, test_images, test_masks):\n",
    "        train_images = np.array(train_images)\n",
    "        train_masks = np.array(train_masks)\n",
    "        val_images = np.array(val_images)\n",
    "        val_masks = np.array(val_masks)\n",
    "        test_images = np.array(test_images)\n",
    "        test_masks = np.array(test_masks)\n",
    "\n",
    "        # Label encoding for training masks\n",
    "        labelencoder = LabelEncoder()\n",
    "        n, h, w = train_masks.shape\n",
    "        train_masks_reshaped = train_masks.reshape(-1, 1)\n",
    "        train_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped)\n",
    "        train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n",
    "\n",
    "        X_train = train_images\n",
    "        y_train = train_masks_encoded_original_shape\n",
    "        X_test = test_images\n",
    "        y_test = test_masks\n",
    "        X_val = val_images\n",
    "        y_val = val_masks\n",
    "        X_train = torch.tensor(train_images.transpose(0, 3, 1, 2), dtype=torch.float32)  # Transpose image dimensions\n",
    "        y_train = torch.tensor(train_masks_encoded_original_shape, dtype=torch.long)\n",
    "        X_test = torch.tensor(test_images.transpose(0, 3, 1, 2), dtype=torch.float32)    # Transpose image dimensions\n",
    "        y_test = torch.tensor(test_masks, dtype=torch.long)\n",
    "        X_val = torch.tensor(val_images.transpose(0, 3, 1, 2), dtype=torch.float32)      # Transpose image dimensions\n",
    "        y_val = torch.tensor(val_masks, dtype=torch.long)\n",
    "        n_classes = len(np.unique(y_train))\n",
    "        print(f\"Number of classes = {n_classes}\")\n",
    "\n",
    "        return X_train, y_train, X_test, y_test, X_val, y_val, n_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cipher73/.local/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes = 11\n"
     ]
    }
   ],
   "source": [
    "data_processor = DataProcessor()\n",
    "\n",
    "# Load and preprocess data\n",
    "train_images = np.array(data_processor.get_data(\"Dataset/train/noisy_images/\", 1))\n",
    "train_masks = np.array(data_processor.get_data(\"Dataset/train/noisy_masks/\", 0))\n",
    "val_images = np.array(data_processor.get_data(\"Dataset/val/noisy_images/\", 1))\n",
    "val_masks = np.array(data_processor.get_data(\"Dataset/val/noisy_masks/\", 0))\n",
    "test_images = np.array(data_processor.get_data(\"Dataset/test/noisy_images/\", 1))\n",
    "test_masks = np.array(data_processor.get_data(\"Dataset/test/noisy_masks/\", 0))\n",
    "\n",
    "# Shuffle the data\n",
    "train_images, train_masks = data_processor.shuffle_data(train_images, train_masks)\n",
    "val_images, val_masks = data_processor.shuffle_data(val_images, val_masks)\n",
    "test_images, test_masks = data_processor.shuffle_data(test_images, test_masks)\n",
    "\n",
    "# Preprocess the data\n",
    "X_train, y_train, X_test, y_test, X_val, y_val, n_classes = data_processor.preprocess_data(\n",
    "    train_images, train_masks, val_images, val_masks, test_images, test_masks\n",
    ")\n",
    "\n",
    "# Combine your data into a single DataLoader\n",
    "train_data = list(zip(X_train, y_train))\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True,pin_memory=True)\n",
    "\n",
    "# Validation DataLoader (similarly, create a DataLoader for test data if needed)\n",
    "val_data = list(zip(X_val, y_val))\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Define a custom ViT model for segmentation\n",
    "class CustomViT(nn.Module):\n",
    "    def __init__(self, num_classes, image_size, hidden_size=256, num_heads=4, num_layers=6):\n",
    "        super(CustomViT, self).__init__()\n",
    "\n",
    "        # Convolutional backbone\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, hidden_size, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_size, hidden_size, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        num_patches = (image_size // 16) ** 2\n",
    "        self.embedding = nn.Embedding(num_patches, hidden_size)\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(hidden_size, num_heads)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.segmentation_head = nn.Conv2d(hidden_size, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.permute(0, 2, 3, 1).reshape(x.shape[0], -1, x.shape[1])\n",
    "        x = self.embedding(x.long())\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)\n",
    "        segmentation_map = self.segmentation_head(x.permute(0, 2, 1).reshape(-1, x.shape[2], x.shape[1], x.shape[3]))\n",
    "        return segmentation_map\n",
    "\n",
    "# Initialize the custom ViT model\n",
    "model = CustomViT(num_classes=num_classes, image_size=image_size)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 8.00 GiB (GPU 0; 3.82 GiB total capacity; 195.04 MiB already allocated; 2.64 GiB free; 226.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/cipher73/Downloads/Honours/Research/CNN/train.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cipher73/Downloads/Honours/Research/CNN/train.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m masks \u001b[39m=\u001b[39m masks\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cipher73/Downloads/Honours/Research/CNN/train.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mwith\u001b[39;00m autocast():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/cipher73/Downloads/Honours/Research/CNN/train.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cipher73/Downloads/Honours/Research/CNN/train.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, masks)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cipher73/Downloads/Honours/Research/CNN/train.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m scaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/cipher73/Downloads/Honours/Research/CNN/train.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cipher73/Downloads/Honours/Research/CNN/train.ipynb#W4sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cipher73/Downloads/Honours/Research/CNN/train.ipynb#W4sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mreshape(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/cipher73/Downloads/Honours/Research/CNN/train.ipynb#W4sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(x\u001b[39m.\u001b[39;49mlong())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cipher73/Downloads/Honours/Research/CNN/train.ipynb#W4sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_layers:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cipher73/Downloads/Honours/Research/CNN/train.ipynb#W4sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     x \u001b[39m=\u001b[39m layer(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    161\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 8.00 GiB (GPU 0; 3.82 GiB total capacity; 195.04 MiB already allocated; 2.64 GiB free; 226.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Initialize the scaler for mixed precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, masks in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] | Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
