{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "from hierataxo.util import (\n",
    "    OrderManager)\n",
    "from hierataxo import (\n",
    "    OrderManager,ConcatProteinDataset,\n",
    "    HierarchicalLossNetwork,\n",
    "    cal_accuracy,set_seed)\n",
    "from hierataxo.dataset import ConcatProteinDataModule\n",
    "from hierataxo.model import HierarESM\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset, DistributedSampler,random_split\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint,LearningRateMonitor\n",
    "\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.loggers.tensorboard import TensorBoardLogger\n",
    "\n",
    "# L.Trainer()\n",
    "max_length=500\n",
    "max_domain=15\n",
    "train_bs=2\n",
    "infer_bs=30\n",
    "seed=42\n",
    "seed_everything(seed, workers=True)\n",
    "order_manager=OrderManager(pkl.load(open('taxo_data/hierarchy_order.pkl','rb'))['Riboviria'],\n",
    "                        level_names=['Kingdom','Phylum','Class','Order'])\n",
    "model=HierarESM(order_manager,max_length=max_length,max_domain=max_domain,\n",
    "    optimizer_kwargs={'backbone_lr':1e-4,'head_lr':1e-3,'weight_decay':0.01},\n",
    "    scheduler_kwargs={'warmup_iter_1':20,'warmup_iter_2':30,'warmup_lr':1e-10})\n",
    "\n",
    "datamodule=ConcatProteinDataModule(order_manager,'taxo_data/proseq_taxo_1.pkl',\n",
    "    max_length=max_length,max_domain=max_domain,train_bs=train_bs,infer_bs=infer_bs)\n",
    "# datamodule.setup('fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from lightning.pytorch.callbacks import BasePredictionWriter\n",
    "from typing import List,Dict\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import torch\n",
    "class PredictWriter(BasePredictionWriter):\n",
    "    def __init__(self, outname:str):\n",
    "        super().__init__(write_interval='epoch')\n",
    "        self.outname = outname\n",
    "\n",
    "    def write_on_epoch_end(self, trainer, pl_module, \n",
    "        predictions:List[Dict[str,List[str]|torch.Tensor]], batch_indices):\n",
    "        o={}\n",
    "        dtypes={k:type(v) for k,v in predictions[0].items()}\n",
    "        for k,dt in dtypes:\n",
    "            if dt is list:\n",
    "                o[k]=sum([i[k] for i in predictions],start=[])\n",
    "            elif dt is torch.Tensor:\n",
    "                o[k]=torch.concat([i[k] for i in predictions]).tolist()\n",
    "        pd.DataFrame(o).to_pickle(trainer.default_root_dir+'/'+self.outname)\n",
    "        # TODO use hdf5 or lmdb to write on batch\n",
    "        # torch.save(predictions, os.path.join(self.output_dir, \"predictions.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35453/2596810752.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(i,map_location='cpu'),strict=False)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/taxo_playground/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  23%|██▎       | 52/222 [00:41<02:16,  1.25it/s]"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.34 GiB. GPU 0 has a total capacity of 14.57 GiB of which 2.28 GiB is free. Including non-PyTorch memory, this process has 12.28 GiB memory in use. Of the allocated memory 6.42 GiB is allocated by PyTorch, and 5.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 27\u001b[0m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(i,map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m),strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m      9\u001b[0m     default_root_dir\u001b[38;5;241m=\u001b[39mdefault_root_dir, \n\u001b[1;32m     10\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# logger=TensorBoardLogger(save_dir=default_root_dir,name=exp_dir)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     )\n\u001b[0;32m---> 27\u001b[0m trainer\u001b[38;5;241m.\u001b[39mpredict(model,datamodule)\n\u001b[1;32m     28\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_checkpoint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/seed-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-last.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:858\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 858\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_impl, model, dataloaders, datamodule, return_predictions, ckpt_path\n\u001b[1;32m    860\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:897\u001b[0m, in \u001b[0;36mTrainer._predict_impl\u001b[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    894\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn, ckpt_path, model_provided\u001b[38;5;241m=\u001b[39mmodel_provided, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    896\u001b[0m )\n\u001b[0;32m--> 897\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_stage()\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    986\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1020\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_loop\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[0;32m-> 1020\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_loop\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py:178\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/lightning/pytorch/loops/prediction_loop.py:124\u001b[0m, in \u001b[0;36m_PredictionLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/lightning/pytorch/loops/prediction_loop.py:253\u001b[0m, in \u001b[0;36m_PredictionLoop._predict_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# configure step_kwargs\u001b[39;00m\n\u001b[1;32m    248\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_step\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    252\u001b[0m )\n\u001b[0;32m--> 253\u001b[0m predictions \u001b[38;5;241m=\u001b[39m call\u001b[38;5;241m.\u001b[39m_call_strategy_hook(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39mstep_args)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predictions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict returned None if it was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    322\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py:437\u001b[0m, in \u001b[0;36mStrategy.predict_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mpredict_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/PGG/taxo_sandbox/hierataxo/model.py:316\u001b[0m, in \u001b[0;36mHierarESM.predict_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch:\u001b[38;5;28mdict\u001b[39m,\n\u001b[1;32m    315\u001b[0m         batch_idx, dataloader_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 316\u001b[0m     predictions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m    317\u001b[0m     predict_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morder_manager\u001b[38;5;241m.\u001b[39midx_to_order(predictions,need_argmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m level_name,predict \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morder_manager\u001b[38;5;241m.\u001b[39mlevel_names,predictions):\n",
      "File \u001b[0;32m/PGG/taxo_sandbox/hierataxo/model.py:248\u001b[0m, in \u001b[0;36mHierarESM.forward\u001b[0;34m(self, attention_mask, sentence_mask, input_ids, inputs_embeds, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m sentence_mask\u001b[38;5;241m=\u001b[39msentence_mask\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    247\u001b[0m ipt\u001b[38;5;241m=\u001b[39m{k:v[sentence_mask\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mbool)] \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m ipt\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 248\u001b[0m ori_ebs:torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mipt)\u001b[38;5;241m.\u001b[39mpooler_output,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    249\u001b[0m ebs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mzeros(sentence_mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],ori_ebs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    250\u001b[0m                 dtype\u001b[38;5;241m=\u001b[39mori_ebs\u001b[38;5;241m.\u001b[39mdtype,device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    251\u001b[0m ebs[sentence_mask\u001b[38;5;241m.\u001b[39mbool()]\u001b[38;5;241m=\u001b[39mori_ebs\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/models/esm/modeling_esm.py:907\u001b[0m, in \u001b[0;36mEsmModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    898\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    900\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    901\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    902\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    905\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    906\u001b[0m )\n\u001b[0;32m--> 907\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    908\u001b[0m     embedding_output,\n\u001b[1;32m    909\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m    910\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m    911\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m    912\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[1;32m    913\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    914\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    915\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    916\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    917\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    918\u001b[0m )\n\u001b[1;32m    919\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    920\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/models/esm/modeling_esm.py:612\u001b[0m, in \u001b[0;36mEsmEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    601\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    602\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    603\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m         output_attentions,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 612\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m    613\u001b[0m         hidden_states,\n\u001b[1;32m    614\u001b[0m         attention_mask,\n\u001b[1;32m    615\u001b[0m         layer_head_mask,\n\u001b[1;32m    616\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    617\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    618\u001b[0m         past_key_value,\n\u001b[1;32m    619\u001b[0m         output_attentions,\n\u001b[1;32m    620\u001b[0m     )\n\u001b[1;32m    622\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/models/esm/modeling_esm.py:502\u001b[0m, in \u001b[0;36mEsmLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    492\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    499\u001b[0m ):\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[1;32m    503\u001b[0m         hidden_states,\n\u001b[1;32m    504\u001b[0m         attention_mask,\n\u001b[1;32m    505\u001b[0m         head_mask,\n\u001b[1;32m    506\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    507\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mself_attn_past_key_value,\n\u001b[1;32m    508\u001b[0m     )\n\u001b[1;32m    509\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/models/esm/modeling_esm.py:436\u001b[0m, in \u001b[0;36mEsmAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    427\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    433\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    434\u001b[0m ):\n\u001b[1;32m    435\u001b[0m     hidden_states_ln \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states)\n\u001b[0;32m--> 436\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[1;32m    437\u001b[0m         hidden_states_ln,\n\u001b[1;32m    438\u001b[0m         attention_mask,\n\u001b[1;32m    439\u001b[0m         head_mask,\n\u001b[1;32m    440\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    441\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    442\u001b[0m         past_key_value,\n\u001b[1;32m    443\u001b[0m         output_attentions,\n\u001b[1;32m    444\u001b[0m     )\n\u001b[1;32m    445\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    446\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/models/esm/modeling_esm.py:360\u001b[0m, in \u001b[0;36mEsmSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    356\u001b[0m         attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m relative_position_scores_query \u001b[38;5;241m+\u001b[39m relative_position_scores_key\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask is (precomputed for all layers in EsmModel forward() function)\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(attention_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.34 GiB. GPU 0 has a total capacity of 14.57 GiB of which 2.28 GiB is free. Including non-PyTorch memory, this process has 12.28 GiB memory in use. Of the allocated memory 6.42 GiB is allocated by PyTorch, and 5.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from pathlib import Path\n",
    "default_root_dir='infer'\n",
    "exp_dir='reload_ckpt'\n",
    "for i in Path('tmp_poster_ana/used_models/').iterdir():\n",
    "    seed=i.stem.split('-')[1]\n",
    "    model.load_state_dict(torch.load(i,map_location='cpu'),strict=False)\n",
    "    trainer = L.Trainer(\n",
    "        default_root_dir=default_root_dir, \n",
    "        accelerator='gpu',\n",
    "        strategy='auto',\n",
    "        max_epochs=100,\n",
    "        check_val_every_n_epoch=1,\n",
    "        log_every_n_steps=1,\n",
    "        precision='32',\n",
    "        max_steps=0,\n",
    "        callbacks=PredictWriter(f'{exp_dir}/seed-{seed}.pkl')\n",
    "        # model.on_save_checkpoint\n",
    "        # limit_predict_batches=4,\n",
    "        # limit_train_batches=100,\n",
    "        # limit_val_batches=30,\n",
    "        # callbacks=[ModelCheckpoint(dirpath=default_root_dir+'/'+exp_dir,monitor='val_loss',\n",
    "        #                 filename='checkpoint-{step:06d}-{val_loss:.2f}',save_top_k=2),\n",
    "        #            LearningRateMonitor('epoch',log_weight_decay=True)],\n",
    "        # logger=TensorBoardLogger(save_dir=default_root_dir,name=exp_dir)\n",
    "        )\n",
    "    trainer.predict(model,datamodule)\n",
    "    trainer.save_checkpoint(f'{exp_dir}/seed-{seed}-last.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taxo_playground/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 4/4 [00:00<00:00,  5.73it/s]\n"
     ]
    }
   ],
   "source": [
    "o=trainer.predict(model,datamodule)\n",
    "# trainer.save_checkpoint('seed-7-last.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=o\n",
    "o_={}\n",
    "dtypes={k:type(v) for k,v in predictions[0].items()}\n",
    "for k,dt in dtypes.items():\n",
    "    if dt is list:\n",
    "        o_[k]=sum([i[k] for i in predictions],start=[])\n",
    "    elif dt is torch.Tensor:\n",
    "        o_[k]=torch.concat([i[k] for i in predictions],).tolist()\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Orthornavirae', 'Lenarviricota', 'Miaviricetes', 'Ourlivirales'],\n",
       " ['Orthornavirae', 'Kitrinoviricota', 'Alsuviricetes', 'Tymovirales'],\n",
       " ['Orthornavirae', 'Pisuviricota', 'Pisoniviricetes', 'Nidovirales'],\n",
       " ['Orthornavirae', 'Negarnaviricota', 'Monjiviricetes', 'Mononegavirales'],\n",
       " ['Orthornavirae', 'Negarnaviricota', 'Monjiviricetes', 'Mononegavirales'],\n",
       " ['Orthornavirae', 'Pisuviricota', 'Stelpaviricetes', 'Stellavirales'],\n",
       " ['Orthornavirae', 'Pisuviricota', 'Stelpaviricetes', 'Stellavirales'],\n",
       " ['Orthornavirae', 'Pisuviricota', 'Stelpaviricetes', 'Stellavirales'],\n",
       " ['Orthornavirae', 'Negarnaviricota', 'Monjiviricetes', 'Mononegavirales'],\n",
       " ['Orthornavirae', 'Negarnaviricota', 'Monjiviricetes', 'Mononegavirales'],\n",
       " ['Orthornavirae', 'Negarnaviricota', 'Ellioviricetes', 'Bunyavirales'],\n",
       " ['Orthornavirae', 'Negarnaviricota', 'Ellioviricetes', 'Bunyavirales']]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_['predict_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>seq</th>\n",
       "      <th>taxo_label</th>\n",
       "      <th>domain_label</th>\n",
       "      <th>taxo</th>\n",
       "      <th>sentence_mask</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>Kingdom_predict</th>\n",
       "      <th>Phylum_predict</th>\n",
       "      <th>Class_predict</th>\n",
       "      <th>Order_predict</th>\n",
       "      <th>predict_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6PavOLV7||MN532594</td>\n",
       "      <td>[RKLKPSQLLQVETGGKYRIVSKSDLGMNSLRPLHSAIYNHLSRFS...</td>\n",
       "      <td>[Orthornavirae, Lenarviricota, Miaviricetes, O...</td>\n",
       "      <td>[Mitovir_RNA_pol, , , , , , , , , , , , , , ]</td>\n",
       "      <td>[1, 3, 11, 14]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0, 10, 15, 4, 15, 14, 8, 16, 4, 4, 16, 7, 9,...</td>\n",
       "      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[-17.12173080444336, 7.148506164550781, -11.82...</td>\n",
       "      <td>[-16.606761932373047, -12.017447471618652, -13...</td>\n",
       "      <td>[-18.014789581298828, -18.111919403076172, -16...</td>\n",
       "      <td>[-16.4014949798584, -20.295434951782227, -19.6...</td>\n",
       "      <td>[Orthornavirae, Lenarviricota, Miaviricetes, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAV1||KF421905</td>\n",
       "      <td>[HYNMPIEAATKLESFGIITNPYSIALHSHAAAKVCENKLLDTVGH...</td>\n",
       "      <td>[Orthornavirae, Kitrinoviricota, Alsuviricetes...</td>\n",
       "      <td>[Vmethyltransf, FTO_NTD, Viral_helicase1, RdRP...</td>\n",
       "      <td>[1, 2, 4, 6]</td>\n",
       "      <td>[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0, 21, 19, 17, 20, 14, 12, 9, 5, 5, 11, 15, ...</td>\n",
       "      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[-15.91338062286377, 4.330357074737549, -10.42...</td>\n",
       "      <td>[-17.33768653869629, -15.704703330993652, -1.6...</td>\n",
       "      <td>[-10.867238998413086, -12.22330093383789, -11....</td>\n",
       "      <td>[-13.037378311157227, -16.32259750366211, -12....</td>\n",
       "      <td>[Orthornavirae, Kitrinoviricota, Alsuviricetes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAbV||GBBW01007738</td>\n",
       "      <td>[GLCSGVLVGENLVLTAAHCLVQPAQVLRFCLGDDCYQILSYEVVD...</td>\n",
       "      <td>[Orthornavirae, Pisuviricota, Pisoniviricetes,...</td>\n",
       "      <td>[DUF316, DUF2075, RdRP_2, CoV_RPol_N, LAP1_C, ...</td>\n",
       "      <td>[1, 5, 19, 23]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0, 6, 4, 23, 8, 6, 7, 4, 7, 6, 9, 17, 4, 7, ...</td>\n",
       "      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[-15.944304466247559, 4.202620983123779, -11.1...</td>\n",
       "      <td>[-12.274317741394043, -12.27157974243164, -9.0...</td>\n",
       "      <td>[-16.270158767700195, -14.265697479248047, -14...</td>\n",
       "      <td>[-11.492676734924316, -11.494980812072754, -10...</td>\n",
       "      <td>[Orthornavirae, Pisuviricota, Pisoniviricetes,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAnV1|0|BK059208</td>\n",
       "      <td>[NGSFSFRLGKNLHALEVMGTNKLYVGNNNSSLLLLDTLGQRLCLE...</td>\n",
       "      <td>[Orthornavirae, Negarnaviricota, Monjiviricete...</td>\n",
       "      <td>[Mononeg_RNA_pol, Mononeg_mRNAcap, Methyltrans...</td>\n",
       "      <td>[1, 4, 14, 18]</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0, 17, 6, 8, 18, 8, 18, 10, 4, 6, 15, 17, 4,...</td>\n",
       "      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[-12.73079776763916, 12.338814735412598, -7.33...</td>\n",
       "      <td>[-16.030885696411133, -10.683784484863281, -13...</td>\n",
       "      <td>[-17.23851776123047, -15.97236156463623, -14.7...</td>\n",
       "      <td>[-17.72771644592285, -16.68787384033203, -12.2...</td>\n",
       "      <td>[Orthornavirae, Negarnaviricota, Monjiviricete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAnV1|1|BK059209</td>\n",
       "      <td>[WPDSRMHGKTGIRLIYLSTANRLRIGKRVINDLHTGISDKTIGGI...</td>\n",
       "      <td>[Orthornavirae, Negarnaviricota, Monjiviricete...</td>\n",
       "      <td>[Rhabdo_ncap_2, , , , , , , , , , , , , , ]</td>\n",
       "      <td>[1, 4, 14, 18]</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0, 22, 14, 13, 8, 10, 20, 21, 6, 15, 11, 6, ...</td>\n",
       "      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[-14.70251178741455, 8.073880195617676, -3.283...</td>\n",
       "      <td>[-11.194315910339355, -7.476253509521484, -8.6...</td>\n",
       "      <td>[-7.324009895324707, -4.696815013885498, -3.05...</td>\n",
       "      <td>[-10.98635196685791, -6.811470985412598, -3.93...</td>\n",
       "      <td>[Orthornavirae, Negarnaviricota, Monjiviricete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AAstV1||Y15936</td>\n",
       "      <td>[SIVVVEGQGGSGVGWRFMNSIFTAGHVVQGSKFVTIKSESTQVKV...</td>\n",
       "      <td>[Orthornavirae, Pisuviricota, Stelpaviricetes,...</td>\n",
       "      <td>[Peptidase_C62, Astro_VPg, RdRP_1, Astro_capsi...</td>\n",
       "      <td>[1, 5, 20, 27]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0, 8, 12, 7, 7, 7, 9, 6, 16, 6, 6, 8, 6, 7, ...</td>\n",
       "      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[-13.615700721740723, 2.0321462154388428, -5.1...</td>\n",
       "      <td>[-10.854243278503418, -9.116840362548828, -8.3...</td>\n",
       "      <td>[-7.915549278259277, -13.054832458496094, -11....</td>\n",
       "      <td>[-10.215134620666504, -12.144798278808594, -12...</td>\n",
       "      <td>[Orthornavirae, Pisuviricota, Stelpaviricetes,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AAstV2||AB033998</td>\n",
       "      <td>[FPSKADNIVKIEVDVDGGSAGVGFRLGNYIYTAGHVVGEAKIAKI...</td>\n",
       "      <td>[Orthornavirae, Pisuviricota, Stelpaviricetes,...</td>\n",
       "      <td>[Peptidase_S32, Astro_VPg, RdRP_2, Astro_capsi...</td>\n",
       "      <td>[1, 5, 20, 27]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0, 18, 14, 8, 15, 5, 13, 17, 12, 7, 15, 12, ...</td>\n",
       "      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[-12.0418062210083, 0.6968989372253418, -2.754...</td>\n",
       "      <td>[-9.824309349060059, -7.311359882354736, -6.79...</td>\n",
       "      <td>[-7.18323278427124, -11.163854598999023, -10.4...</td>\n",
       "      <td>[-8.415350914001465, -8.647527694702148, -11.0...</td>\n",
       "      <td>[Orthornavirae, Pisuviricota, Stelpaviricetes,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AAstV3||AF206663</td>\n",
       "      <td>[TASIVIVEGKNGTGVGFRFMNYILTAEHVVQGSDIATLKNGSVSV...</td>\n",
       "      <td>[Orthornavirae, Pisuviricota, Stelpaviricetes,...</td>\n",
       "      <td>[Peptidase_S3, Astro_VPg, RdRP_1, Astro_capsid...</td>\n",
       "      <td>[1, 5, 20, 27]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0, 11, 5, 8, 12, 7, 12, 7, 9, 6, 15, 17, 6, ...</td>\n",
       "      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[-12.167651176452637, 1.0686155557632446, -5.1...</td>\n",
       "      <td>[-10.126836776733398, -7.9227614402771, -7.140...</td>\n",
       "      <td>[-5.771191596984863, -11.366390228271484, -10....</td>\n",
       "      <td>[-7.9608306884765625, -9.540933609008789, -11....</td>\n",
       "      <td>[Orthornavirae, Pisuviricota, Stelpaviricetes,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ABBV1||KF578398</td>\n",
       "      <td>[RPTHLPKLPGTFLQYTTGGGDPHPGIGDEKDIKKNALALLDPARR...</td>\n",
       "      <td>[Orthornavirae, Negarnaviricota, Monjiviricete...</td>\n",
       "      <td>[BDV_P40, BDV_G, Mononeg_RNA_pol, Mononeg_mRNA...</td>\n",
       "      <td>[1, 4, 14, 18]</td>\n",
       "      <td>[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0, 10, 14, 11, 21, 4, 14, 15, 4, 14, 6, 11, ...</td>\n",
       "      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[-12.777323722839355, 12.1207914352417, -7.129...</td>\n",
       "      <td>[-15.485483169555664, -9.798325538635254, -12....</td>\n",
       "      <td>[-18.301544189453125, -16.48464584350586, -15....</td>\n",
       "      <td>[-18.400583267211914, -17.23122787475586, -13....</td>\n",
       "      <td>[Orthornavirae, Negarnaviricota, Monjiviricete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ABBV2||KJ756399</td>\n",
       "      <td>[TPAHLPKLPGTFLQYTTGGGDPHPGIGDEKDIKKNALAFLDPTRR...</td>\n",
       "      <td>[Orthornavirae, Negarnaviricota, Monjiviricete...</td>\n",
       "      <td>[BDV_P40, BDV_G, Mononeg_RNA_pol, Mononeg_mRNA...</td>\n",
       "      <td>[1, 4, 14, 18]</td>\n",
       "      <td>[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0, 11, 14, 5, 21, 4, 14, 15, 4, 14, 6, 11, 1...</td>\n",
       "      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[-12.802946090698242, 12.065025329589844, -7.0...</td>\n",
       "      <td>[-15.797685623168945, -10.068511009216309, -13...</td>\n",
       "      <td>[-18.28931427001953, -16.935344696044922, -15....</td>\n",
       "      <td>[-18.409753799438477, -17.483552932739258, -13...</td>\n",
       "      <td>[Orthornavirae, Negarnaviricota, Monjiviricete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ABEV|L|KX611382</td>\n",
       "      <td>[STIGSTLKPECVITGVKLSNFVHDFTYGHLCGSSDQKFMSVFPIM...</td>\n",
       "      <td>[Orthornavirae, Negarnaviricota, Ellioviricete...</td>\n",
       "      <td>[L_protein_N, DUF3770, Bunya_RdRp, , , , , , ,...</td>\n",
       "      <td>[1, 4, 16, 20]</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0, 8, 11, 12, 6, 8, 11, 4, 15, 14, 9, 23, 7,...</td>\n",
       "      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[-12.521162986755371, 10.41726303100586, -3.51...</td>\n",
       "      <td>[-19.238157272338867, -15.32852840423584, -17....</td>\n",
       "      <td>[-15.584421157836914, -15.835116386413574, -15...</td>\n",
       "      <td>[-14.387609481811523, -16.658790588378906, -13...</td>\n",
       "      <td>[Orthornavirae, Negarnaviricota, Ellioviricete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ABEV|M|KX611383</td>\n",
       "      <td>[AVLLTQSVIAEGLFMLKSFKPLSSAQVCFSSRTPLDGIGVYWISE...</td>\n",
       "      <td>[Orthornavirae, Negarnaviricota, Ellioviricete...</td>\n",
       "      <td>[Phlebovirus_NSM, Tenui_PVC2, , , , , , , , , ...</td>\n",
       "      <td>[1, 4, 16, 20]</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[0, 5, 7, 4, 4, 11, 16, 8, 7, 12, 5, 9, 6, 4,...</td>\n",
       "      <td>[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "      <td>[-11.007511138916016, 9.273067474365234, -1.40...</td>\n",
       "      <td>[-18.348670959472656, -14.04471492767334, -16....</td>\n",
       "      <td>[-15.71420669555664, -16.18773651123047, -15.5...</td>\n",
       "      <td>[-16.346206665039062, -19.009645462036133, -15...</td>\n",
       "      <td>[Orthornavirae, Negarnaviricota, Ellioviricete...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name                                                seq  \\\n",
       "0   6PavOLV7||MN532594  [RKLKPSQLLQVETGGKYRIVSKSDLGMNSLRPLHSAIYNHLSRFS...   \n",
       "1       AAV1||KF421905  [HYNMPIEAATKLESFGIITNPYSIALHSHAAAKVCENKLLDTVGH...   \n",
       "2   AAbV||GBBW01007738  [GLCSGVLVGENLVLTAAHCLVQPAQVLRFCLGDDCYQILSYEVVD...   \n",
       "3     AAnV1|0|BK059208  [NGSFSFRLGKNLHALEVMGTNKLYVGNNNSSLLLLDTLGQRLCLE...   \n",
       "4     AAnV1|1|BK059209  [WPDSRMHGKTGIRLIYLSTANRLRIGKRVINDLHTGISDKTIGGI...   \n",
       "5       AAstV1||Y15936  [SIVVVEGQGGSGVGWRFMNSIFTAGHVVQGSKFVTIKSESTQVKV...   \n",
       "6     AAstV2||AB033998  [FPSKADNIVKIEVDVDGGSAGVGFRLGNYIYTAGHVVGEAKIAKI...   \n",
       "7     AAstV3||AF206663  [TASIVIVEGKNGTGVGFRFMNYILTAEHVVQGSDIATLKNGSVSV...   \n",
       "8      ABBV1||KF578398  [RPTHLPKLPGTFLQYTTGGGDPHPGIGDEKDIKKNALALLDPARR...   \n",
       "9      ABBV2||KJ756399  [TPAHLPKLPGTFLQYTTGGGDPHPGIGDEKDIKKNALAFLDPTRR...   \n",
       "10     ABEV|L|KX611382  [STIGSTLKPECVITGVKLSNFVHDFTYGHLCGSSDQKFMSVFPIM...   \n",
       "11     ABEV|M|KX611383  [AVLLTQSVIAEGLFMLKSFKPLSSAQVCFSSRTPLDGIGVYWISE...   \n",
       "\n",
       "                                           taxo_label  \\\n",
       "0   [Orthornavirae, Lenarviricota, Miaviricetes, O...   \n",
       "1   [Orthornavirae, Kitrinoviricota, Alsuviricetes...   \n",
       "2   [Orthornavirae, Pisuviricota, Pisoniviricetes,...   \n",
       "3   [Orthornavirae, Negarnaviricota, Monjiviricete...   \n",
       "4   [Orthornavirae, Negarnaviricota, Monjiviricete...   \n",
       "5   [Orthornavirae, Pisuviricota, Stelpaviricetes,...   \n",
       "6   [Orthornavirae, Pisuviricota, Stelpaviricetes,...   \n",
       "7   [Orthornavirae, Pisuviricota, Stelpaviricetes,...   \n",
       "8   [Orthornavirae, Negarnaviricota, Monjiviricete...   \n",
       "9   [Orthornavirae, Negarnaviricota, Monjiviricete...   \n",
       "10  [Orthornavirae, Negarnaviricota, Ellioviricete...   \n",
       "11  [Orthornavirae, Negarnaviricota, Ellioviricete...   \n",
       "\n",
       "                                         domain_label            taxo  \\\n",
       "0       [Mitovir_RNA_pol, , , , , , , , , , , , , , ]  [1, 3, 11, 14]   \n",
       "1   [Vmethyltransf, FTO_NTD, Viral_helicase1, RdRP...    [1, 2, 4, 6]   \n",
       "2   [DUF316, DUF2075, RdRP_2, CoV_RPol_N, LAP1_C, ...  [1, 5, 19, 23]   \n",
       "3   [Mononeg_RNA_pol, Mononeg_mRNAcap, Methyltrans...  [1, 4, 14, 18]   \n",
       "4         [Rhabdo_ncap_2, , , , , , , , , , , , , , ]  [1, 4, 14, 18]   \n",
       "5   [Peptidase_C62, Astro_VPg, RdRP_1, Astro_capsi...  [1, 5, 20, 27]   \n",
       "6   [Peptidase_S32, Astro_VPg, RdRP_2, Astro_capsi...  [1, 5, 20, 27]   \n",
       "7   [Peptidase_S3, Astro_VPg, RdRP_1, Astro_capsid...  [1, 5, 20, 27]   \n",
       "8   [BDV_P40, BDV_G, Mononeg_RNA_pol, Mononeg_mRNA...  [1, 4, 14, 18]   \n",
       "9   [BDV_P40, BDV_G, Mononeg_RNA_pol, Mononeg_mRNA...  [1, 4, 14, 18]   \n",
       "10  [L_protein_N, DUF3770, Bunya_RdRp, , , , , , ,...  [1, 4, 16, 20]   \n",
       "11  [Phlebovirus_NSM, Tenui_PVC2, , , , , , , , , ...  [1, 4, 16, 20]   \n",
       "\n",
       "                                    sentence_mask  \\\n",
       "0   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "1   [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2   [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "3   [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "5   [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "6   [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "7   [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "8   [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "9   [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "10  [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "11  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                            input_ids  \\\n",
       "0   [[0, 10, 15, 4, 15, 14, 8, 16, 4, 4, 16, 7, 9,...   \n",
       "1   [[0, 21, 19, 17, 20, 14, 12, 9, 5, 5, 11, 15, ...   \n",
       "2   [[0, 6, 4, 23, 8, 6, 7, 4, 7, 6, 9, 17, 4, 7, ...   \n",
       "3   [[0, 17, 6, 8, 18, 8, 18, 10, 4, 6, 15, 17, 4,...   \n",
       "4   [[0, 22, 14, 13, 8, 10, 20, 21, 6, 15, 11, 6, ...   \n",
       "5   [[0, 8, 12, 7, 7, 7, 9, 6, 16, 6, 6, 8, 6, 7, ...   \n",
       "6   [[0, 18, 14, 8, 15, 5, 13, 17, 12, 7, 15, 12, ...   \n",
       "7   [[0, 11, 5, 8, 12, 7, 12, 7, 9, 6, 15, 17, 6, ...   \n",
       "8   [[0, 10, 14, 11, 21, 4, 14, 15, 4, 14, 6, 11, ...   \n",
       "9   [[0, 11, 14, 5, 21, 4, 14, 15, 4, 14, 6, 11, 1...   \n",
       "10  [[0, 8, 11, 12, 6, 8, 11, 4, 15, 14, 9, 23, 7,...   \n",
       "11  [[0, 5, 7, 4, 4, 11, 16, 8, 7, 12, 5, 9, 6, 4,...   \n",
       "\n",
       "                                       attention_mask  \\\n",
       "0   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "1   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "2   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "3   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "4   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "5   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "6   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "7   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "8   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "9   [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "10  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "11  [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...   \n",
       "\n",
       "                                      Kingdom_predict  \\\n",
       "0   [-17.12173080444336, 7.148506164550781, -11.82...   \n",
       "1   [-15.91338062286377, 4.330357074737549, -10.42...   \n",
       "2   [-15.944304466247559, 4.202620983123779, -11.1...   \n",
       "3   [-12.73079776763916, 12.338814735412598, -7.33...   \n",
       "4   [-14.70251178741455, 8.073880195617676, -3.283...   \n",
       "5   [-13.615700721740723, 2.0321462154388428, -5.1...   \n",
       "6   [-12.0418062210083, 0.6968989372253418, -2.754...   \n",
       "7   [-12.167651176452637, 1.0686155557632446, -5.1...   \n",
       "8   [-12.777323722839355, 12.1207914352417, -7.129...   \n",
       "9   [-12.802946090698242, 12.065025329589844, -7.0...   \n",
       "10  [-12.521162986755371, 10.41726303100586, -3.51...   \n",
       "11  [-11.007511138916016, 9.273067474365234, -1.40...   \n",
       "\n",
       "                                       Phylum_predict  \\\n",
       "0   [-16.606761932373047, -12.017447471618652, -13...   \n",
       "1   [-17.33768653869629, -15.704703330993652, -1.6...   \n",
       "2   [-12.274317741394043, -12.27157974243164, -9.0...   \n",
       "3   [-16.030885696411133, -10.683784484863281, -13...   \n",
       "4   [-11.194315910339355, -7.476253509521484, -8.6...   \n",
       "5   [-10.854243278503418, -9.116840362548828, -8.3...   \n",
       "6   [-9.824309349060059, -7.311359882354736, -6.79...   \n",
       "7   [-10.126836776733398, -7.9227614402771, -7.140...   \n",
       "8   [-15.485483169555664, -9.798325538635254, -12....   \n",
       "9   [-15.797685623168945, -10.068511009216309, -13...   \n",
       "10  [-19.238157272338867, -15.32852840423584, -17....   \n",
       "11  [-18.348670959472656, -14.04471492767334, -16....   \n",
       "\n",
       "                                        Class_predict  \\\n",
       "0   [-18.014789581298828, -18.111919403076172, -16...   \n",
       "1   [-10.867238998413086, -12.22330093383789, -11....   \n",
       "2   [-16.270158767700195, -14.265697479248047, -14...   \n",
       "3   [-17.23851776123047, -15.97236156463623, -14.7...   \n",
       "4   [-7.324009895324707, -4.696815013885498, -3.05...   \n",
       "5   [-7.915549278259277, -13.054832458496094, -11....   \n",
       "6   [-7.18323278427124, -11.163854598999023, -10.4...   \n",
       "7   [-5.771191596984863, -11.366390228271484, -10....   \n",
       "8   [-18.301544189453125, -16.48464584350586, -15....   \n",
       "9   [-18.28931427001953, -16.935344696044922, -15....   \n",
       "10  [-15.584421157836914, -15.835116386413574, -15...   \n",
       "11  [-15.71420669555664, -16.18773651123047, -15.5...   \n",
       "\n",
       "                                        Order_predict  \\\n",
       "0   [-16.4014949798584, -20.295434951782227, -19.6...   \n",
       "1   [-13.037378311157227, -16.32259750366211, -12....   \n",
       "2   [-11.492676734924316, -11.494980812072754, -10...   \n",
       "3   [-17.72771644592285, -16.68787384033203, -12.2...   \n",
       "4   [-10.98635196685791, -6.811470985412598, -3.93...   \n",
       "5   [-10.215134620666504, -12.144798278808594, -12...   \n",
       "6   [-8.415350914001465, -8.647527694702148, -11.0...   \n",
       "7   [-7.9608306884765625, -9.540933609008789, -11....   \n",
       "8   [-18.400583267211914, -17.23122787475586, -13....   \n",
       "9   [-18.409753799438477, -17.483552932739258, -13...   \n",
       "10  [-14.387609481811523, -16.658790588378906, -13...   \n",
       "11  [-16.346206665039062, -19.009645462036133, -15...   \n",
       "\n",
       "                                       predict_labels  \n",
       "0   [Orthornavirae, Lenarviricota, Miaviricetes, O...  \n",
       "1   [Orthornavirae, Kitrinoviricota, Alsuviricetes...  \n",
       "2   [Orthornavirae, Pisuviricota, Pisoniviricetes,...  \n",
       "3   [Orthornavirae, Negarnaviricota, Monjiviricete...  \n",
       "4   [Orthornavirae, Negarnaviricota, Monjiviricete...  \n",
       "5   [Orthornavirae, Pisuviricota, Stelpaviricetes,...  \n",
       "6   [Orthornavirae, Pisuviricota, Stelpaviricetes,...  \n",
       "7   [Orthornavirae, Pisuviricota, Stelpaviricetes,...  \n",
       "8   [Orthornavirae, Negarnaviricota, Monjiviricete...  \n",
       "9   [Orthornavirae, Negarnaviricota, Monjiviricete...  \n",
       "10  [Orthornavirae, Negarnaviricota, Ellioviricete...  \n",
       "11  [Orthornavirae, Negarnaviricota, Ellioviricete...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(o_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Orthornavirae', 'Lenarviricota', 'Miaviricetes', 'Ourlivirales'],\n",
       " ['Orthornavirae', 'Kitrinoviricota', 'Alsuviricetes', 'Tymovirales'],\n",
       " ['Orthornavirae', 'Pisuviricota', 'Pisoniviricetes', 'Nidovirales'],\n",
       " ['Orthornavirae', 'Negarnaviricota', 'Monjiviricetes', 'Mononegavirales'],\n",
       " ['Orthornavirae', 'Negarnaviricota', 'Monjiviricetes', 'Mononegavirales'],\n",
       " ['Orthornavirae', 'Pisuviricota', 'Stelpaviricetes', 'Stellavirales'],\n",
       " ['Orthornavirae', 'Pisuviricota', 'Stelpaviricetes', 'Stellavirales'],\n",
       " ['Orthornavirae', 'Pisuviricota', 'Stelpaviricetes', 'Stellavirales'],\n",
       " ['Orthornavirae', 'Negarnaviricota', 'Monjiviricetes', 'Mononegavirales'],\n",
       " ['Orthornavirae', 'Negarnaviricota', 'Monjiviricetes', 'Mononegavirales'],\n",
       " ['Orthornavirae', 'Negarnaviricota', 'Ellioviricetes', 'Bunyavirales'],\n",
       " ['Orthornavirae', 'Negarnaviricota', 'Ellioviricetes', 'Bunyavirales']]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_['taxo_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36416968008711675"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.98**50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1])\n",
      "tensor([3, 2, 5])\n",
      "tensor([11,  4, 19])\n",
      "tensor([14,  6, 23])\n"
     ]
    }
   ],
   "source": [
    "for i in o[0]['predictions']:\n",
    "    print(model.order_manager.idx_to_order(torch.argmax(i, dim=1).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Orthornavirae', 'Lenarviricota', 'Miaviricetes', 'Ourlivirales'],\n",
       " ['Orthornavirae', 'Kitrinoviricota', 'Alsuviricetes', 'Tymovirales'],\n",
       " ['Orthornavirae', 'Pisuviricota', 'Pisoniviricetes', 'Nidovirales']]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_=model.order_manager.idx_to_order(o[0]['predictions'],need_argmax=True)\n",
    "# [list(row) for row in zip(*_)]\n",
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes={k:type(v) for k,v in o[0].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtypes['attention_mask'] is torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': list,\n",
       " 'seq': list,\n",
       " 'taxo_label': list,\n",
       " 'domain_label': list,\n",
       " 'taxo': torch.Tensor,\n",
       " 'sentence_mask': torch.Tensor,\n",
       " 'input_ids': torch.Tensor,\n",
       " 'attention_mask': torch.Tensor,\n",
       " 'predictions': list,\n",
       " 'acc_Kingdom': torch.Tensor,\n",
       " 'acc_Phylum': torch.Tensor,\n",
       " 'acc_Class': torch.Tensor,\n",
       " 'acc_Order': torch.Tensor}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:type(v) for k,v in o[0].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 12,\n",
       " 'seq': 12,\n",
       " 'taxo_label': 12,\n",
       " 'domain_label': 12,\n",
       " 'taxo': 12,\n",
       " 'sentence_mask': 12,\n",
       " 'input_ids': 12,\n",
       " 'attention_mask': 12,\n",
       " 'Kingdom_predict': 12,\n",
       " 'Phylum_predict': 12,\n",
       " 'Class_predict': 12,\n",
       " 'Order_predict': 12,\n",
       " 'predict_labels': 12}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:len(v) for k,v in o_.items() if not isinstance(v,int) and not 'acc' in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_=[]\n",
    "idx_list=o[0]['predictions']\n",
    "for i,level in enumerate(model.order_manager.levels):\n",
    "    idx_l=torch.argmax(idx_list[i],dim=1).tolist() if isinstance(\n",
    "        idx_list[i],torch.Tensor) else idx_list[i]\n",
    "    o_.append([level[j] for j in idx_l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Orthornavirae', 'Orthornavirae', 'Orthornavirae'],\n",
       " ['Lenarviricota', 'Kitrinoviricota', 'Pisuviricota'],\n",
       " ['Miaviricetes', 'Alsuviricetes', 'Pisoniviricetes'],\n",
       " ['Ourlivirales', 'Tymovirales', 'Nidovirales']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14,  6, 23])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(idx_list[i],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
