{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9182e1ea-d948-496d-9800-1a1a1a550742",
   "metadata": {
    "id": "f9e13780-fcf2-4e66-8958-b47802dead91"
   },
   "source": [
    "# Chapter 5. Introduction to Statistics in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c09c80",
   "metadata": {},
   "source": [
    "Statistics: the practice and study of collecting and analyzing data.\n",
    "\n",
    "Do's\n",
    "- How likely is someone to purchase a product?\n",
    "- Are people more likely to purchase it if they can use a different payment system?\n",
    "- How many occupants will your hotel have?\n",
    "- A/B testing: Which ad is more effective in getting people to purchase a product?\n",
    "\n",
    "Don'ts\n",
    "- Subjective questions (like all reasons behind a person's preference)\n",
    "\n",
    "Types of statistics\n",
    "- Descriptive. Focuses on *describing and summarizing* the data at hand.\n",
    "- Inferential. Uses the data at hand, which is called sample data, to make *inferences* about a larger population.\n",
    "\n",
    "Types of data\n",
    "- Numeric (quantitative) data is made up of numeric values.\n",
    "   - Continuous numeric data: quantities that can be *measured*, like speed or time.\n",
    "   - Discrete numeric data: usually *count* data, like number of pets or number of packages shipped.\n",
    "- Categorical (qualitative) data is made up of values that belong to distinct groups.\n",
    "   - Nominal categorical data: made up of categories with no inherent ordering, like marriage status or country of residence.\n",
    "   - Ordinal categorical data: has an inherent order, like a survey question where you need to indicate the degree to which you agree with a statement.\n",
    "\n",
    "Being able to identify data types is important since the type of data you're working with will dictate what kinds of summary statistics and visualizations make sense for your data. For numerical data, we can use summary statistics like mean, and plots like scatter plots, but these don't make a ton of sense for categorical data. Similarly, things like counts and barplots don't make much sense for numeric data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25809963-a7d3-496f-aa53-ecfca36306a9",
   "metadata": {},
   "source": [
    "# 5.1 Summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56787d6d",
   "metadata": {},
   "source": [
    "Summary statistics: fact about or summary of some data, like an average or a count."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b1b502",
   "metadata": {},
   "source": [
    "## Measures of center"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b78293",
   "metadata": {},
   "source": [
    "### Histograms\n",
    "\n",
    "- A histogram takes a bunch of data points and separates them into bins, or ranges of values.\n",
    "- The heights of the bars represent the number of data points that fall into that bin\n",
    "- Great way to visually summarize the data, but we can use numerical summary statistics to summarize even further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d16d1b",
   "metadata": {},
   "source": [
    "### Mean\n",
    "\n",
    "- Often called the average, is one of the most common ways of summarizing data.\n",
    "- Is more sensitive to extreme values (outliers)\n",
    "- To calculate mean, we add up all the numbers of interest and divide by the total number of data points.\n",
    "- In Python, we can use numpy's function ``np.mean(variable)``, passing it the variable of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02730884",
   "metadata": {},
   "source": [
    "### Median\n",
    "\n",
    "- Is the value where 50% of the data is lower than it, and 50% of the data is higher.\n",
    "- We can calculate this by sorting all the data points and taking the middle one\n",
    "- In Python, we can use ``np.median(variable)`` to do the calculations for us.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5190fe2f",
   "metadata": {},
   "source": [
    "### Mode\n",
    "\n",
    "- Is the most frequent value in the data.\n",
    "- Is often used for categorical variables, since categorical variables can be unordered and often don't have an inherent numerical representation.\n",
    "- We can also find the mode using the ``np.mode(variable)`` function from the statistics module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1858f824",
   "metadata": {},
   "source": [
    "### Which measure should I use?\n",
    "\n",
    "- Since the mean is more sensitive to extreme values, it works better for symmetrical data\n",
    "- If the data is skewed, meaning it's not symmetrical, median is usually better to use.\n",
    "   - Left-skewed data has a tail on the left and data is piled up on the right. Mean < Median\n",
    "   - Right-skewed data has a tail on the right and data is piled up on the left. Mean > Median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3613b3f2",
   "metadata": {},
   "source": [
    "## Measures of spread\n",
    "\n",
    "Describes how spread apart or close together the data points are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df712b6",
   "metadata": {},
   "source": [
    "### Variance\n",
    "\n",
    "- Measures the average distance from each data point to the data's mean. It has squared units.\n",
    "- To calculate the variance:\n",
    "   1. We start by calculating the distance between each point and the mean, so we get one number for every data point.\n",
    "   2. We then square each distance and then add them all together.\n",
    "   3. Finally, we divide the sum of squared distances by the number of data points minus 1, giving us the variance.\n",
    " \n",
    "- The higher the variance, the more spread out the data is.\n",
    "- We can calculate the variance in one step using ``np.var(variable)``, setting the ``ddof`` argument to 1.\n",
    "   - If we don't specify ``ddof=1``, a slightly different formula is used to calculate variance that should only be used on a full population, not a sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad40ed3",
   "metadata": {},
   "source": [
    "### Standard deviation\n",
    "\n",
    "- Calculated by taking the square root of the variance. It has lineal units.\n",
    "- It can be calculated using ``np.std(variable)``, and we need to set ``ddof=1``."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51648c0",
   "metadata": {},
   "source": [
    "### Mean absolute deviation\n",
    "\n",
    "- Takes the absolute value of the distances to the mean, and then takes the mean of those differences.\n",
    "- Standard deviation squares distances, so longer distances are penalized more than shorter ones, while mean absolute deviation penalizes each distance equally.\n",
    "- It can be calculated using ``np.mean(np.abs(distances))``, where ``distances = data - np.mean(data)``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd39743c",
   "metadata": {},
   "source": [
    "### Quantiles\n",
    "\n",
    "- Also called percentiles, split up the data into some number of equal parts.\n",
    "- We call ``np.quantile(df['column_name'], argument)``, passing in the column of interest, followed by point-5.\n",
    "   - Argument: ``0.5`` is equal to median, it can be a list of quantiles like ``[0, 0.25, 0.5, 0.75, 1]``\n",
    "\n",
    "- We can look for quantiles using ``np.quantile(df['column_name'], np.linspace(start, stop, num)``, which takes in the starting number, the stopping number, and the number intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619f50dc",
   "metadata": {},
   "source": [
    "### Boxplots and quartiles\n",
    "\n",
    "- The boxes in box plots represent quartiles.\n",
    "   - The bottom of the box is the first quartile\n",
    "   - The middle line is the second quartile, or the median.\n",
    "   - The top of the box is the third quartile.\n",
    "\n",
    "- How to plot a boxplot\n",
    "\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "plt.boxplot(df['column'])\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51830030",
   "metadata": {},
   "source": [
    "### Interquartile range (IQR)\n",
    "\n",
    "- It's the distance between the 25th and 75th percentile, which is also the height of the box in a boxplot.\n",
    "- We can calculate it using\n",
    "   - The function from NumPy ``np.quantile(df['column_name'], 0.75) - np.quantile(df['column_name'], 0.25)``\n",
    "   - The imported function from Sci-Py:\n",
    "   ```\n",
    "   from scipy-dot-stats import iqr\n",
    "   iqr(df['column_name'])\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7ea7c9",
   "metadata": {},
   "source": [
    "### Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a8db17",
   "metadata": {},
   "source": [
    "Outliers are data points that are substantially different from the others.\n",
    "\n",
    "How do we know what a substantial difference is? Here is an often used rule:\n",
    "- Any data point less than the first quartile minus 1-point-5 times the IQR is an outlier\n",
    "- Any point greater than the third quartile plus 1-point-5 times the IQR.\n",
    "\n",
    "Using Python:\n",
    "```\n",
    "from scipy-dot-stats import iqr\n",
    "iqr(df['column_name'])\n",
    "lower_threshold = np.quantile(df['column_name'], 0.25) - (1.5*iqr)\n",
    "upper_threshold = np.quantile(df['column_name'], 0.75) + (1.5*iqr)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba312b99",
   "metadata": {},
   "source": [
    "### ALL IN ONE GO: ``pd.df['column_name].describe()``\n",
    "\n",
    "It returns:\n",
    "- count of rows\n",
    "- mean\n",
    "- std\n",
    "- 25% (first quartile)\n",
    "- 50% (first quartile)\n",
    "- 75% (first quartile)\n",
    "- maximum value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860ea388",
   "metadata": {},
   "source": [
    "## ```dataframe.melt()```\n",
    "\n",
    "About the method...\n",
    "\n",
    "Syntax\n",
    "- Calling the function while working with 2 DataFrames: ```dataframe.melt(arguments)```\n",
    "\n",
    "Arguments\n",
    "- Selects columns to be used as identifier variables: ```id_vars=[list of columns]```\n",
    "- Will allow us to control which columns are unpivoted: ```value_vars=[list of columns]```\n",
    "- Will allow us to set the name of column in the output: ```var_name='column'```\n",
    "- Will allow us to set the name of the value column in the output: ```var_values='column'```\n",
    "\n",
    "What does it do?\n",
    "- Will unpivot a table from wide to long format. This is often a much more computer-friendly format, therefore making this a valuable method to know.\n",
    "- Wide format: data where every row relates to one subject, and each column has different information about an attribute of that subject.\n",
    "- Long (or tall) format: information about one subject is found over many rows, and each row has one attribute about that subject."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc4a8bf",
   "metadata": {},
   "source": [
    "# 5.2 Random numbers and Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d028466",
   "metadata": {},
   "source": [
    "- We can measure the chances of an event using probability.\n",
    "- Probability is always between 0% and 100%\n",
    "   - If probability is 0%, it's impossible\n",
    "   - If probability is 100%, it will certainly happen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d243d57",
   "metadata": {},
   "source": [
    "## Setting a random seed and sampling from a DataFrame\n",
    "\n",
    "- To ensure we get the same results when we run the script.\n",
    "- The seed is a number that Python's random number generator uses as a starting point, so if we orient it with a seed number, it will generate the same random value each time.\n",
    "- The only thing that matters is that we use the same seed the next time we run the script.\n",
    "\n",
    "Parameters\n",
    "1. ``n``: integer representing the number of items from axis to return.\n",
    "2. ``replace``: ``None`` by default, ``True`` allows sampling of the same row more than once\n",
    "\n",
    "Code\n",
    "```\n",
    "np.random.seed()\n",
    "pd.df.sample(5, replace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4168888a",
   "metadata": {},
   "source": [
    "## Independent events\n",
    "\n",
    "- Two events are independent if the probability of the second event isn't affected by the ouctome of the first event.\n",
    "- Generally when sampling with replacement, each pick is independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6e6a17",
   "metadata": {},
   "source": [
    "## Dependent events\n",
    "\n",
    "- Two events are dependent if the probability of the second event is affected by the outcome of the first event\n",
    "- Generally when sampling without replacement, each pick is dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a47963",
   "metadata": {},
   "source": [
    "## Probability distribution\n",
    "\n",
    "- Describes the probability of each possible outcome in a scenario.\n",
    "- The expected value of a distribution, is the mean of a distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cc652a",
   "metadata": {},
   "source": [
    "### Discrete distributions\n",
    "\n",
    "- They represent situations with discrete outcomes, which can be thought of as counted variables.\n",
    "- We can visualize this using a barplot, where each bar represents an outcome, and each bar's height represents the probability of that outcome.\n",
    "- We can calculate probabilities of different outcomes by taking areas of the probability distribution.\n",
    "\n",
    "Sampling from discrete distributions\n",
    "```\n",
    "np.mean(die['number'])\n",
    "rolls_10 = die.saample(10, replace=True)\n",
    "rolls_10\n",
    "\n",
    "rolls_10['number'].hist(bins=np.linspace(1,7,7))\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "- Law of large numbers: \"As the size of your sample increases, the sample mean will approach the expected value\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c21e85",
   "metadata": {},
   "source": [
    "### Continuous distributions\n",
    "\n",
    "- We can use discrete distributions to model situations that involve discrete or countable variables.\n",
    "- We'll use a continuous line to represent probability.\n",
    "- Just like with discrete distributions, we can take the area to calculate probability.\n",
    "\n",
    "Calculating the probability using the Sci-Py function ``uniform`` with method ``cdf``. Example code: \n",
    "```\n",
    "from scipy.stats import uniform\n",
    "uniform.cdf(array, lower_limit_value, upper_limit_value)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb68564",
   "metadata": {},
   "source": [
    "#### Random numbers generation according to a continuous distribution (i.e. uniform)\n",
    "\n",
    "```\n",
    "from scipy.stats import uniform\n",
    "uniform.rvs(lower_limit_vale, upper_limit_value, size=n)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795c633c",
   "metadata": {},
   "source": [
    "#### Binomial distribution\n",
    "\n",
    "The binomial distribution describes the probability of the number of successes in a sequence of independent trials.\n",
    "\n",
    "- The binomial distribution can be described using two parameters, n and p.\n",
    "   - n represents the total number of trials being performed\n",
    "   - p is the probability of success.\n",
    "- We could also represent these outcomes as a 1 and a 0, a success or a failure, and a win or a loss.\n",
    "- In order for the binomial distribution to apply, each trial must be independent, so the outcome of one trial shouldn't have an effect on the next.\n",
    "- We can simulate this in Python ike this:\n",
    "```\n",
    "from scipy.stats import binom\n",
    "\n",
    "# GETTING RANDOM VARIATES\n",
    "binom.rvs(k, p, size=n)\n",
    "\n",
    "# Single flip\n",
    "binom.rvs(1, 0.5, size=1)\n",
    "\n",
    "# single flip, many times\n",
    "binom.rvs(1, 0.5, size=8)\n",
    "\n",
    "# Many flips, one time\n",
    "binom.rvs(8, 0.5, size=1)\n",
    "\n",
    "# Many flips, many times\n",
    "binom.rvs(8, 0.5, size=10)\n",
    "\n",
    "# Other probabilities\n",
    "binom.rvs(3, 0.25, size=10)\n",
    "\n",
    "# GETTING THE PROBABILITY OF SUCCESS\n",
    "binom.pmf(r, n, p) # of 'r'\n",
    "binom.cdf(r, n, p) # of 'r' or less\n",
    "1 - binom.cdf(r, n, p) # of more than 'r'\n",
    "\n",
    "# EXPECTED VALUE\n",
    "n*p\n",
    "```\n",
    "\n",
    "Parameters\n",
    "- k: number of samples\n",
    "- r: number of successful results\n",
    "- p: probability of success\n",
    "- size/n: number of experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887ed114",
   "metadata": {},
   "source": [
    "# 5.3 More distributions and the Central Limit Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5086915b",
   "metadata": {},
   "source": [
    "#### Normal distribution\n",
    "\n",
    "- Its shape is commonly referred to as a \"bell curve\"\n",
    "- Countless number of statistical methods rely on it, and it applies to more real-world situations than the distributions we've covered so far.\n",
    "- Has important properties\n",
    "   1. Symmetrical: the left side is a mirror image of the right.\n",
    "   2. Area=1 : like any continuous distribution, the area beneath the curve is 1.\n",
    "   3. Curve never hits 0: the probability never hits 0, even if it looks like it does at the tail ends. Only 0.006% of its area is contained beyond the edges of this graph.\n",
    "   4. Described by mean and standard deviation.\n",
    "       - When a normal distribution has mean 0 and a standard deviation of 1, it's a special distribution called the standard normal distribution.\n",
    "       - Other distributions with different values for mean and std will have the same shape, but their axes have different scales\n",
    "   5. Areas under the normal distribution (\"68-95-99.7\" rule)\n",
    "       - 68% of the area is within 1 standard deviation of the mean.\n",
    "       - 95% of the area falls within 2 standard deviations of the mean\n",
    "       - 99.7% of the area falls within three standard deviations.\n",
    "- Approximating data with the normal distribution: since histograms can closely resemble the normal distribution, we can take the area under a normal distribution given certain mean and std values\n",
    "\n",
    "```\n",
    "from scipy.stats import norm\n",
    "\n",
    "# CUMULATIVE DENSITY FUNCTION\n",
    "\n",
    "# Probability of a value being equal or less than \"x\"\n",
    "norm.cdf(x, mean, std)\n",
    "\n",
    "# Probability of a value being greater than \"x\"\n",
    "1 - norm.cdf(x, mean, std)\n",
    "\n",
    "# Probability of a value being between \"x1\" and \"x2\"\n",
    "norm.cdf(x2, mean, std) - norm.cdf(x1, mean, std)\n",
    "\n",
    "# PROBABILITY POINT FUNCTION\n",
    "\n",
    "# Value for which a given percentage of observations fall below\n",
    "norm.ppf(x, mean, std)\n",
    "\n",
    "# Value for which a given percentage of observations fall above\n",
    "norm.ppf((1-x), mean, std)\n",
    "\n",
    "# # Sampling from a Normal distribution\n",
    "norm.rvs(mean, std, size=n)\n",
    "```\n",
    "\n",
    "Parameters\n",
    "- x: value\n",
    "- mean: float\n",
    "- std: float\n",
    "- n: number of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e77157",
   "metadata": {},
   "source": [
    "### Central limit theorem (CLT)\n",
    "\n",
    "- States that a sampling distribution will approach a normal distribution as the number of trials increases.\n",
    "- Samples should be random and independent\n",
    "- Applies to other summary statistics\n",
    "- Useful for estimating characteristics of unkown underlyng distribution\n",
    "- More easily estimate characteristics of large populations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc88f461",
   "metadata": {},
   "source": [
    "#### Poisson distribution\n",
    "\n",
    "A Poisson process is a process where events appear to happen at a certain rate, but completely at random. The time unit like, hours, weeks, or years, is irrelevant as long as it's consistent.\n",
    "\n",
    "The Poisson distribution describes the probability of some number of events happening over a fixed period of time.\n",
    "\n",
    "- The Poisson distribution is described by a value called lambda, which represents the average number of events per time period. \n",
    "   - This value is also the expected value of the distribution\n",
    "   - Lambda changes the shape of the distribution: a Poisson distribution with lambda=1, looks quite different than a Poisson distribution with lambda=8 but no matter what, the distribution's peak is always at its lambda value.\n",
    "- Python code:\n",
    "\n",
    "```\n",
    "from scipy.stats import poisson\n",
    "\n",
    "# Probability of a single value\n",
    "poisson.pmf(k, mu)\n",
    "\n",
    "# Probability of less than or equal to...\n",
    "poisson.cdf(k, mu)\n",
    "\n",
    "# probability of greater than...\n",
    "1 - poisson.cdf(k, mu)\n",
    "\n",
    "# Sampling from a Poisson distribution\n",
    "poisson.rvs(mu, size=n)\n",
    "```\n",
    "\n",
    "Parameters\n",
    "- k: number of ocurrences\n",
    "- mu: average value\n",
    "- n: sample size\n",
    "\n",
    "Side note: CLT still appies!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6705536c",
   "metadata": {},
   "source": [
    "#### Exponential distribution\n",
    "\n",
    "The exponential distribution measures frequency in terms of time between events.\n",
    "\n",
    "- Represents the probability of a certain time passing between Poisson events.\n",
    "- The exponential distribution uses the same lambda value as the Poisson.\n",
    "- The expected value of the exponential distribution can be calculated by taking 1 divided by lambda.\n",
    "- It's continuous, since it represents time\n",
    "\n",
    "```\n",
    "from scipy.stats import expon\n",
    "\n",
    "# Probability of being less than or equal to \"x\"\n",
    "expon.cdf(x, scale=mu)\n",
    "\n",
    "# Probability of being greater than \"x\"\n",
    "1 - expon.cdf(x, scale=mu)\n",
    "\n",
    "# Probability of a value being between \"x1\" and \"x2\"\n",
    "expon.cdf(x2, scale=1/mu) - expon.cdf(x1, scale=1/mu)\n",
    "```\n",
    "\n",
    "Parameters\n",
    "- x: time value\n",
    "- scale: expected value = mu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1de31eb",
   "metadata": {},
   "source": [
    "#### Student's t-distribution\n",
    "\n",
    "- Its shape is similar to the normal distribution\n",
    "- The t-distribution's tails are thicker. This means that in a t-distribution, observations are more likely to fall further from the mean.\n",
    "- Has a parameter called degrees of freedom, which affects the thickness of the distribution's tails.\n",
    "   - Lower degrees of freedom results in thicker tails and a higher standard deviation.\n",
    "   - As the number of degrees of freedom increases, the distribution looks more and more like the normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba83ff2",
   "metadata": {},
   "source": [
    "#### Log-normal distribution\n",
    "\n",
    "- Variables that follow a log-normal distribution have a logarithm that is normally distributed.\n",
    "- This results in distributions that are skewed, unlike the normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594d829b",
   "metadata": {},
   "source": [
    "# 5.4 Correlation and Experimental designs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8e46b1",
   "metadata": {},
   "source": [
    "## Relationships between 2 variables\n",
    "\n",
    "- Relationships between numeric variables can be visualized with scatter plots.\n",
    "- In this scatterplot, we can see the relationship between the \"x\" and \"y\" variables.\n",
    "- The variable on the x-axis is called the explanatory/independent variable, and the variable on the y-axis is called the response/dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01d02f1",
   "metadata": {},
   "source": [
    "## Correlation coefficient\n",
    "\n",
    "- We can also examine relationships between two numeric variables using a number called the correlation coefficient.\n",
    "- This is a number between -1 and 1, where the magnitude corresponds to the strength of the relationship between the variables, and the sign, positive or negative, corresponds to the direction of the relationship.\n",
    "   - Closer to 1 or -1, very strong (or moderate) relationship\n",
    "   - Closer to 0, very weak (or no) relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82a45e5",
   "metadata": {},
   "source": [
    "## Visualizing relationships\n",
    "\n",
    "- We can use a scatterplot.\n",
    "- Also, we'll use seaborn, which is a plotting package built on top of matplotlib.\n",
    "- Python code:\n",
    "```\n",
    "import seabron as sns\n",
    "sns.scatterplot(x=\"column_x\", y=\"column_y\", data=DataFrame)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba35ddba",
   "metadata": {},
   "source": [
    "## Adding a trend\n",
    "\n",
    "- We can add a linear trendline to the scatterplot using seaborn's ```sns.lmplot()`` function.\n",
    "- It takes the same arguments as ``sns.scatterplot()``, but we'll set ci to None so that there aren't any confidence interval margins around the line.\n",
    "   - Trendlines like this can be helpful to more easily see a relationship between two variables.\n",
    "- Python code:\n",
    "```\n",
    "import seabron as sns\n",
    "sns.lmplot(x=\"column_x\", y=\"column_y\", data=DataFrame, ci=None)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27e74c9",
   "metadata": {},
   "source": [
    "## Computing correlation\n",
    "\n",
    "- To calculate the correlation coefficient (Pearson product-moment correlation) between two Series, we can use the ``.corr()`` method.\n",
    "- Note that it doesn't matter which Series the method is invoked on and which is passed in since the correlation between x and y is the same thing as the correlation between y and x.\n",
    "- Python code:\n",
    "```\n",
    "df[\"column_x\"].corr(df[\"column_y\"]) is equal to df[\"column_y\"].corr(df[\"column_x\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb00f8c",
   "metadata": {},
   "source": [
    "## Correlation caveats\n",
    "\n",
    "1. **Non-linear relationships**: Correlation only accounts for linear relationships.\n",
    "   - The correlation coefficient measures the strength of linear relationships only.\n",
    "   - Correlation shouldn't be used blindly, and you should always visualize your data when possible.\n",
    "2. If data is highly skewed, we can use the **log transformation**\n",
    "```\n",
    "import seabron as sns\n",
    "\n",
    "df[\"log_x_column\"] = np.log(df[\"skewed_x_column\"])\n",
    "\n",
    "sns.lmplot(x=\"log_column\", y=\"column_y\", data=DataFrame, ci=None)\n",
    "plt.show()\n",
    "\n",
    "df[\"log_x_column\"].corr(df[\"column_y\"])\n",
    "```\n",
    "3. Other transformations\n",
    "   - Square root tranformation ``sqrt(x)``\n",
    "   - Reciprocal transformation ``(1/x)``\n",
    "   - Combinations, ie.: ``log(x)`` and ``(1/y)`\n",
    "   \n",
    "Why use a transformation?\n",
    "- Certain statistical methods rely on variables having a linear relationship, like calculating a correlation coefficient.\n",
    "- Linear regression is another statistical technique that requires variables to be related in a linear manner\n",
    "\n",
    "4. **CORRELATION DOES NOT IMPLY CAUSATION**\n",
    "- \"x\" being correlated with \"y\" **does not mean** \"x\" causes \"y\"\n",
    "- This kind of correlation is often called a **spurious correlation**.\n",
    "- **Confounding** can lead to spurious correlation\n",
    "   - If two variables are correlated, it may lead us to think one causes the other.\n",
    "   - In reality, there might be a third variabe at play that is known to be the real cause of one, and associated with other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79955354",
   "metadata": {},
   "source": [
    "## Design of experiments\n",
    "\n",
    "Data is created as a result of a study that aims to answer a specific question. However, data needs to be analyzed and interpreted differently depending on how the data was generated and how the study was designed.\n",
    "\n",
    "- Experiments generally aim to answer a question in the form, \"What is the effect of the treatment on the response?\"\n",
    "- Treatment refers to the explanatory or independent variable\n",
    "- Response refers to the response or dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6433b2b0",
   "metadata": {},
   "source": [
    "### Controlled experiments\n",
    "\n",
    "- Participants are randomly assigned to either the treatment group or the control group, where the treatment group receives the treatment and the control group does not.\n",
    "   - Example: A/B test\n",
    "- Other than this difference, the groups should be comparable so that we can determine if seeing an advertisement causes people to buy more.\n",
    "   - If the groups aren't comparable, this could lead to confounding, or bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d4ec05",
   "metadata": {},
   "source": [
    "### The gold standard\n",
    "\n",
    "\"If there are fewer opportunities for bias to creep into your experiment, the more reliably you can conclude whether the treatment affects the response\"\n",
    "\n",
    "- The ideal experiment will eliminate as much bias as possible by using certain tools. \n",
    "   1. Use a randomized controlled trial.\n",
    "      - In it, participants are randomly assigned to the treatment or control group and their assignment isn't based on anything other than chance.\n",
    "      - Random assignment like this helps ensure that the groups are comparable.\n",
    "   2. Use a placebo\n",
    "      - Is something that resembles the treatment, but has no effect.\n",
    "      - This way, participants don't know if they're in the treatment or control group.\n",
    "      - Ensures that the effect of the treatment is due to the treatment itself, not the idea of getting the treatment.\n",
    "   3. Double-blind trial\n",
    "      - The person administering the treatment or running the experiment also doesn't know whether they're administering the actual treatment or the placebo.\n",
    "      - This protects against bias in the response as well as the analysis of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3006f1",
   "metadata": {},
   "source": [
    "### Observational studies\n",
    "\n",
    "- Participants are not randomly assigned to groups.\n",
    "   - Instead, participants assign themselves, usually based on pre-existing characteristics.\n",
    "- Useful for answering questions that aren't conducive to a controlled experiment.\n",
    "- Observational studies can't establish causation, only association."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2633b67f",
   "metadata": {},
   "source": [
    "### Longitudinal vs. cross-sectional studies\n",
    "\n",
    "1. Longitudinal study\n",
    "   - The same participants are followed over a period of time to examine the effect of treatment on the response.\n",
    "   - More expensive, and take longer to perform\n",
    "2. Cross-sectional study\n",
    "   - Data is collected from a single snapshot in time.\n",
    "   - Cheaper, faster, and more convenient."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
