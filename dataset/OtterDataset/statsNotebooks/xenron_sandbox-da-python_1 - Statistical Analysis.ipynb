{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll use selected statistical algorithms to analyze our dataset. Specifically, we'll do the following:\n",
    "\n",
    "* Statistical analysis\n",
    "    * Distribution analysis\n",
    "    * Categorical variable analysis\n",
    "    * Linear Regression\n",
    "    * Time-series analysis\n",
    "    * Outlier detection\n",
    "* Predictive analysis\n",
    "    * Logistic regression\n",
    "    * Random Forest\n",
    "    * Support Vector Machine (SVM)\n",
    "* Save the results\n",
    "    * Save a predictive model for production use\n",
    "\n",
    "First we'll get our data into our dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DataFrame of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the Python libraries we need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a variable for the accidents data file\n",
    "accidents_data_file = '/Users/robertdempsey/Dropbox/private/Python Business Intelligence Cookbook/Data/Stats19-Data1979-2004/Accidents7904.csv'\n",
    "\n",
    "accidents = pd.read_csv(accidents_data_file,\n",
    "                        sep=',',\n",
    "                        header=0,\n",
    "                        index_col=False,\n",
    "                        parse_dates=True,\n",
    "                        tupleize_cols=False,\n",
    "                        error_bad_lines=False,\n",
    "                        warn_bad_lines=True,\n",
    "                        skip_blank_lines=True,\n",
    "                        low_memory=False\n",
    "                        )\n",
    "accidents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a full list of the columns and data types\n",
    "accidents.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a Distribution Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A distribution analysis helps us understand the distribution of various attributes of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a histogram of the weather conditions\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.hist(accidents['Weather_Conditions'], \n",
    "        range = (accidents['Weather_Conditions'].min(),accidents['Weather_Conditions'].max()))\n",
    "counts, bins, patches = ax.hist(accidents['Weather_Conditions'], facecolor='green', edgecolor='gray')\n",
    "ax.set_xticks(bins)\n",
    "plt.title('Weather Conditions Distribution')\n",
    "plt.xlabel('Weather Condition')\n",
    "plt.ylabel('Count of Weather Condition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per the Data Guide provided with the data, here are the corresponding meanings for the weather condition values.\n",
    "\n",
    "* -1 - Data missing or out of range\n",
    "* 1 - Fine no high winds\n",
    "* 2 - Raining no high winds\n",
    "* 3 - Snowing no high winds\n",
    "* 4 - Fine + high winds\n",
    "* 5 - Raining + high winds\n",
    "* 6 - Snowing + high winds\n",
    "* 7 - Fog or mist\n",
    "* 8 - Other\n",
    "* 9 - Unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a box plot of the light conditions\n",
    "# The ';' at the end of the function call suppresses the usual matplotlib output\n",
    "accidents.boxplot(column='Light_Conditions',\n",
    "                  return_type='dict');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a box plot of the light conditions grouped by weather conditions\n",
    "accidents.boxplot(column='Light_Conditions',\n",
    "                  by = 'Weather_Conditions',\n",
    "                  return_type='dict');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Variable Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A categorical variable analysis helps us understands categorical types of data. Categorical types are non-numeric. In this example, we're using day of the week. Technically it's a category as opposed to purely numeric data. The creators of the dataset have already converted the category - the name of the day of the week - to a number. If they hadn't done this, we could use Pandas to do it for us, and then perform our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of casualties by day of the week\n",
    "# Sunday = 1\n",
    "casualty_count = accidents.groupby('Day_of_Week').Number_of_Casualties.count()\n",
    "casualty_probability = accidents.groupby('Day_of_Week').Number_of_Casualties.sum()/accidents.groupby('Day_of_Week').Number_of_Casualties.count()\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.set_xlabel('Day of Week')\n",
    "ax1.set_ylabel('Casualty Count')\n",
    "ax1.set_title(\"Casualties by Day of Week\")\n",
    "casualty_count.plot(kind='bar')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "casualty_probability.plot(kind = 'bar')\n",
    "ax2.set_xlabel('Day of Week')\n",
    "ax2.set_ylabel('Probability of Casualties')\n",
    "ax2.set_title(\"Probability of Casualties by Day of Week\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In statistics, regression analysis is a statistical process for estimating the relationships among variables...More specifically, regression analysis helps one understand how the typical value of the dependent variable (or 'criterion variable') changes when any one of the independent variables is varied, while the other independent variables are held fixed.\"\n",
    "\n",
    "Linear regression is an approach for predicting a quantitative response using a single feature (or \"predictor\" or \"input variable\").\n",
    "\n",
    "For this recipe we are going to use the Advertising dataset from 'An Introduction to Statistical Learning\n",
    "with Applications in R'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the data\n",
    "# Define a variable for the accidents data file\n",
    "data_file = '/Users/robertdempsey/Dropbox/private/Python Business Intelligence Cookbook/Data/ISL/Advertising.csv'\n",
    "\n",
    "ads = pd.read_csv(data_file,\n",
    "                        sep=',',\n",
    "                        header=0,\n",
    "                        index_col=False,\n",
    "                        parse_dates=True,\n",
    "                        tupleize_cols=False,\n",
    "                        error_bad_lines=False,\n",
    "                        warn_bad_lines=True,\n",
    "                        skip_blank_lines=True,\n",
    "                        low_memory=False\n",
    "                        )\n",
    "ads.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How much data do we have?\n",
    "ads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the relationship between sales and TV in a scatterplot\n",
    "ads.plot(kind='scatter',\n",
    "         x='TV',\n",
    "         y='Sales',\n",
    "         figsize=(16, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the Python libraries we need\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create an instance of the LinearRegression model\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Create X and y\n",
    "features = ['TV', 'Radio', 'Newspaper']\n",
    "x = ads[features]\n",
    "y = ads.Sales\n",
    "\n",
    "# Fit the data to the model\n",
    "lm.fit(x, y)\n",
    "\n",
    "# Print the intercept and coefficients\n",
    "# Intercept: the expected mean value of Y when all X=0\n",
    "# Coefficients: \n",
    "\n",
    "print(lm.intercept_)\n",
    "print(lm.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aggregate the feature names and coefficients to create a single object\n",
    "fc = zip(features, lm.coef_)\n",
    "list(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the R-squared value: a statistical measure of how close the data are to the fitted regression line\n",
    "# The closer to 100% this number is the better the model fits the data\n",
    "lm.score(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a sales prediction for a new observation\n",
    "# Given the ad spend for three channels how many thousands of widgets do we predict we will sell\n",
    "# Dollars (in thousands) spent on tv, radio, and newspaper advertising\n",
    "lm.predict([75.60, 132.70, 34])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a dataframe containing the total number of casualties by date\n",
    "casualty_count = accidents.groupby('Date').agg({'Number_of_Casualties': np.sum})\n",
    "\n",
    "# Convert the index to a DateTimeIndex\n",
    "casualty_count.index = pd.to_datetime(casualty_count.index)\n",
    "\n",
    "# Sort the index so the plot looks correct\n",
    "casualty_count.sort_index(inplace=True,\n",
    "                          ascending=True)\n",
    "\n",
    "# Plot the data\n",
    "casualty_count.plot(figsize=(18, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot one year of the data\n",
    "casualty_count['2000'].plot(figsize=(18, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the yearly total casualty count for each year in the 1980's\n",
    "the1980s = casualty_count['1980-01-01':'1989-12-31'].groupby(casualty_count['1980-01-01':'1989-12-31'].index.year).sum()\n",
    "the1980s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show the plot\n",
    "the1980s.plot(kind='bar',\n",
    "              figsize=(18, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the 80's data as a line graph to better see the differences in years\n",
    "the1980s.plot(figsize=(18, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outlier detection is used to find outliers in the data that can throw off your analysis.\n",
    "\n",
    "Outliers come in two flavors: Univariate and Multivariate. Univariate outliers can be seen when looking at a single variable; multivariate outliers are found in multi-dimensional data.\n",
    "\n",
    "For this recipe we are going to use the College dataset from 'An Introduction to Statistical Learning\n",
    "with Applications in R'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "data_file = '/Users/robertdempsey/Dropbox/private/Python Business Intelligence Cookbook/Data/ISL/College.csv'\n",
    "\n",
    "# Use the first column as the index - the dataset is set up to be like this\n",
    "colleges = pd.read_csv(data_file,\n",
    "                        sep=',',\n",
    "                        header=0,\n",
    "                        index_col=0,\n",
    "                        parse_dates=True,\n",
    "                        tupleize_cols=False,\n",
    "                        error_bad_lines=False,\n",
    "                        warn_bad_lines=True,\n",
    "                        skip_blank_lines=True,\n",
    "                        low_memory=False\n",
    "                        )\n",
    "colleges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colleges.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colleges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# View a boxplot of the number of applications and the number of accepted applicants\n",
    "colleges.boxplot(column=['Apps', 'Accept'],\n",
    "                 return_type='axes',\n",
    "                 figsize=(12,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the relationship between the application and acceptance numbers in a scatterplot\n",
    "colleges.plot(kind='scatter',\n",
    "              x='Accept',\n",
    "              y='Apps',\n",
    "              figsize=(16, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Label each point so we can see which points are the outliers\n",
    "# Except for the outliers, this will be completely unreadable\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "colleges.plot(kind='scatter',\n",
    "              x='Accept',\n",
    "              y='Apps',\n",
    "              figsize=(16, 6),\n",
    "              ax=ax)\n",
    "\n",
    "# Label each of the points\n",
    "for k, v in colleges.iterrows():\n",
    "    ax.annotate(k,(v['Accept'],v['Apps']))\n",
    "\n",
    "# Re-draw the scatterplot\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a statistical technique used to predict a binary outcome, for example purchase/no-purchase.\n",
    "\n",
    "For this recipe we are going to use the Heart dataset from 'An Introduction to Statistical Learning with Applications in R'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "data_file = '/Users/robertdempsey/Dropbox/private/Python Business Intelligence Cookbook/Data/ISL/Heart.csv'\n",
    "\n",
    "# Use the first column as the index - the dataset is set up to be like this\n",
    "heart = pd.read_csv(data_file,\n",
    "                        sep=',',\n",
    "                        header=0,\n",
    "                        index_col=0,\n",
    "                        parse_dates=True,\n",
    "                        tupleize_cols=False,\n",
    "                        error_bad_lines=False,\n",
    "                        warn_bad_lines=True,\n",
    "                        skip_blank_lines=True,\n",
    "                        low_memory=False\n",
    "                        )\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heart.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heart.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the ChestPain column to a numeric value\n",
    "t2 = pd.Series({'asymptomatic' : 1,\n",
    "                'nonanginal' : 2,\n",
    "                'nontypical' : 3,\n",
    "                'typical': 4})\n",
    "heart['ChestPain'] = heart['ChestPain'].map(t2)\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the Thal column to a numeric value\n",
    "t = pd.Series({'fixed' : 1,\n",
    "               'normal' : 2,\n",
    "               'reversible' : 3})\n",
    "heart['Thal'] = heart['Thal'].map(t)\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the AHD column to a numeric value\n",
    "t = pd.Series({'No' : 0,\n",
    "               'Yes' : 1})\n",
    "heart['AHD'] = heart['AHD'].map(t)\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill missing values in with 0\n",
    "heart.fillna(0, inplace=True)\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What is the shape of the data?\n",
    "heart.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create two matrices for our model to use\n",
    "heart_data = heart.iloc[:,0:13].values\n",
    "heart_targets = heart['AHD'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "from sklearn import linear_model\n",
    "logClassifier = linear_model.LogisticRegression(C=1, random_state=111)\n",
    "\n",
    "# Add in cross validation for our model\n",
    "from sklearn import cross_validation\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(heart_data,\n",
    "                                                                     heart_targets,\n",
    "                                                                     test_size=0.20,\n",
    "                                                                     random_state=111)\n",
    "logClassifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Estimate the accuracy of the model on our dataset\n",
    "# Splits the data, fits the model and computes the score 12 consecutive times with different splits each time\n",
    "scores = cross_validation.cross_val_score(logClassifier, heart_data, heart_targets, cv=12)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show the mean accuracy score and the standard deviation\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the test data\n",
    "predicted = logClassifier.predict(X_test)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# View the confusion matrix\n",
    "\n",
    "# Confusion matrix - shows the predictions that the model made on the test data\n",
    "# Diagonal from top-left corner to bottom-right corner is number of correct predictions for each row\n",
    "# A number in a non-diagonal row is the count of errors for that row, and the column corresponds to the incorrect prediction\n",
    "\n",
    "metrics.confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest is an ensemble (a group) of decision trees which will output a prediction value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "data_file = '/Users/robertdempsey/Dropbox/private/Python Business Intelligence Cookbook/Data/ISL/Heart.csv'\n",
    "\n",
    "# Use the first column as the index - the dataset is set up to be like this\n",
    "heart = pd.read_csv(data_file,\n",
    "                        sep=',',\n",
    "                        header=0,\n",
    "                        index_col=0,\n",
    "                        parse_dates=True,\n",
    "                        tupleize_cols=False,\n",
    "                        error_bad_lines=False,\n",
    "                        warn_bad_lines=True,\n",
    "                        skip_blank_lines=True,\n",
    "                        low_memory=False\n",
    "                        )\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the ChestPain column to a numeric value\n",
    "t2 = pd.Series({'asymptomatic' : 1,\n",
    "                'nonanginal' : 2,\n",
    "                'nontypical' : 3,\n",
    "                'typical': 4})\n",
    "heart['ChestPain'] = heart['ChestPain'].map(t2)\n",
    "\n",
    "# Convert the Thal column to a numeric value\n",
    "t = pd.Series({'fixed' : 1,\n",
    "               'normal' : 2,\n",
    "               'reversible' : 3})\n",
    "heart['Thal'] = heart['Thal'].map(t)\n",
    "\n",
    "# Convert the AHD column to a numeric value\n",
    "t = pd.Series({'No' : 0,\n",
    "               'Yes' : 1})\n",
    "heart['AHD'] = heart['AHD'].map(t)\n",
    "\n",
    "# Fill missing values in with 0\n",
    "heart.fillna(0, inplace=True)\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the random forest library\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "# Create the random forest object which will include all the parameters\n",
    "# for the fit\n",
    "rfClassifier = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "# Fit the training data to the AHD labels and create the decision trees\n",
    "rfClassifier = rfClassifier.fit(X_train, y_train)\n",
    "rfClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take the same decision trees and run it on the test data\n",
    "predicted = rfClassifier.predict(X_test)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Estimate the accuracy of the model on our dataset\n",
    "scores = cross_validation.cross_val_score(rfClassifier, heart_data, heart_targets, cv=12)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show the mean accuracy score and the standard deviation\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assess the model\n",
    "metrics.accuracy_score(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show the confusion matrix\n",
    "metrics.confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM) are a group of supervised learning methods that can be applied to classification or regression.\n",
    "\n",
    "For this recipe we are going to use the Heart dataset from 'An Introduction to Statistical Learning with Applications in R'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "data_file = '/Users/robertdempsey/Dropbox/private/Python Business Intelligence Cookbook/Data/ISL/Heart.csv'\n",
    "\n",
    "# Use the first column as the index - the dataset is set up to be like this\n",
    "heart = pd.read_csv(data_file,\n",
    "                        sep=',',\n",
    "                        header=0,\n",
    "                        index_col=0,\n",
    "                        parse_dates=True,\n",
    "                        tupleize_cols=False,\n",
    "                        error_bad_lines=False,\n",
    "                        warn_bad_lines=True,\n",
    "                        skip_blank_lines=True,\n",
    "                        low_memory=False\n",
    "                        )\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the ChestPain column to a numeric value\n",
    "t2 = pd.Series({'asymptomatic' : 1,\n",
    "                'nonanginal' : 2,\n",
    "                'nontypical' : 3,\n",
    "                'typical': 4})\n",
    "heart['ChestPain'] = heart['ChestPain'].map(t2)\n",
    "\n",
    "# Convert the Thal column to a numeric value\n",
    "t = pd.Series({'fixed' : 1,\n",
    "               'normal' : 2,\n",
    "               'reversible' : 3})\n",
    "heart['Thal'] = heart['Thal'].map(t)\n",
    "\n",
    "# Convert the AHD column to a numeric value\n",
    "t = pd.Series({'No' : 0,\n",
    "               'Yes' : 1})\n",
    "heart['AHD'] = heart['AHD'].map(t)\n",
    "\n",
    "\n",
    "# Fill missing values in with 0\n",
    "heart.fillna(0, inplace=True)\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an instance of a linear support vector classifier, an SVM classifier\n",
    "from sklearn.svm import LinearSVC\n",
    "svmClassifier = LinearSVC(random_state=111)\n",
    "svmClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the model - the svmClassifier we created earlier - with training data\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(heart_data,\n",
    "                                                                     heart_targets,\n",
    "                                                                     test_size=0.20,\n",
    "                                                                     random_state=111)\n",
    "svmClassifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the test data through our model by feeding it to the predict function of the model \n",
    "predicted = svmClassifier.predict(X_test)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Estimate the accuracy of the model on our dataset\n",
    "scores = cross_validation.cross_val_score(rfClassifier, heart_data, heart_targets, cv=12)\n",
    "# Show the mean accuracy score and the standard deviation\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assess the model\n",
    "metrics.accuracy_score(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show the confusion matrix\n",
    "metrics.confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Models for Production Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the Python libraries we need\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression Model\n",
    "hearts_classifier_file = \"/Users/robertdempsey/Dropbox/private/Python Business Intelligence Cookbook/Models/hearts_lr_classifier_09.27.15.dat\"\n",
    "pickle.dump(logClassifier, open(hearts_classifier_file, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random Forest Model\n",
    "hearts_classifier_file = \"/Users/robertdempsey/Dropbox/private/Python Business Intelligence Cookbook/Models/hearts_rf_classifier_09.27.15.dat\"\n",
    "pickle.dump(rfClassifier, open(hearts_classifier_file, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM Model\n",
    "hearts_classifier_file = \"/Users/robertdempsey/Dropbox/private/Python Business Intelligence Cookbook/Models/hearts_svm_classifier_09.27.15.dat\"\n",
    "pickle.dump(svmClassifier, open(hearts_classifier_file, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reconstitute the logistic regression model as a test\n",
    "model_file = \"/Users/robertdempsey/Dropbox/private/Python Business Intelligence Cookbook/Models/hearts_lr_classifier_09.27.15.dat\"\n",
    "logClassifier2 = pickle.load(open(model_file, \"rb\"))\n",
    "print(logClassifier2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
