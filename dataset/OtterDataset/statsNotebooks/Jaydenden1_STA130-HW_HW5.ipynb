{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a8ed840",
   "metadata": {},
   "source": [
    "# Q1\n",
    "  For an idea to be tested, a key factor is its measurability. If the idea can be tested, its data can be analyzed. Conversely, if the idea's data cannot be quantified into measurable variables, it cannot be tested.\n",
    "  A good null hypothesis must be specific, testable, and falsifiable. It needs to clearly state that there is no significant effect or difference between the subjects being studied, can be verified with data, and has the possibility of being disproven by evidence. This hypothesis should be clear, making it easy to test through analysis to see if it holds true.\n",
    "  In hypothesis testing, the null hypothesis (H₀) is the default assumption, stating that there is no difference or effect between the two. It represents the status quo. The alternative hypothesis (H₁), on the other hand, contradicts the null hypothesis, indicating that there is a significant difference or effect. Simply put, the null hypothesis assumes no change, while the alternative hypothesis suggests there is a change or impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5953574",
   "metadata": {},
   "source": [
    "# Q2\n",
    "when we conduct statistical tests, the ultimate goal is to infer the characteristics of the entire population, rather than just focusing on the sample.\n",
    "x i​represents each individual observation in the sample, like the height data of a randomly selected group of people.\n",
    "xˉis the sample mean, which is the average value calculated from the sample data.\n",
    "𝜇 is the population mean, the true average value of the entire population, which we aim to estimate from the sample.\n",
    "μ 0​ is a hypothesized value used in hypothesis testing. We compare the sample mean to μ 0​ to determine if the hypothesis holds.\n",
    "In summary, the goal of statistical testing is to use sample data to infer the characteristics of the whole population. The focus is on the population parameter rather than the sample statistic. This highlights that conclusions drawn from the sample are actually about the population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860abfd7",
   "metadata": {},
   "source": [
    "# Q3\n",
    "The calculation of the p-value is based on the assumption that the null hypothesis is true, which allows us to assess how likely the observed data would occur. The null hypothesis usually states that there is no significant difference or effect. By assuming the null hypothesis is true, we can calculate the probability of observing results as extreme as ours under this assumption. If the p-value is very small, it means the likelihood of such results occurring under the null hypothesis is very low, which may lead us to reject the null hypothesis. In simple terms, the p-value helps us determine whether the data aligns with the null hypothesis or whether there is enough evidence to support a different conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee52699",
   "metadata": {},
   "source": [
    "# Q4\n",
    "Suppose we are investigating whether a new drug can lower blood pressure, and the original hypothesis is that “the drug has no effect”. We conduct an experiment to measure the change in blood pressure after using the drug, and find that the patient's blood pressure drops significantly. We then calculated a p-value of, say, 0.01, which means that if the drug did not work, there was only a 1% chance of such a significant drop in blood pressure.\n",
    "\n",
    "Because this probability was so small, we began to think that the original hypothesis (that the drug was ineffective) was not very plausible. In other words, a p-value that small means that the data is so far from the original hypothesis that it makes the original hypothesis look less and less plausible. We then have more reason to believe that the drug does work and may reject the original hypothesis.\n",
    "\n",
    "Thus, in statistical analysis, the smaller the p-value and the more inconsistent the data are with the original hypothesis, the more we are inclined to think that the original hypothesis is not valid and to support the alternative hypothesis, which is that the drug may actually work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a19dca5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated p-value: 0.001\n"
     ]
    }
   ],
   "source": [
    "# Q5\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n_couples = 124    \n",
    "right_tilt_observed = 80  \n",
    "n_simulations = 10000  \n",
    "p_null = 0.5  \n",
    "\n",
    "simulated_right_tilts = np.random.binomial(n_couples, p_null, n_simulations)\n",
    "\n",
    "p_value = np.mean(simulated_right_tilts >= right_tilt_observed)\n",
    "\n",
    "print(f\"Simulated p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b3c7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5\n",
    "Hypothesis:\n",
    "Null hypothesis 𝐻0 : Humans do not have a preference for left-right tilting when kissing, which means that the probability of favoring the right side is 0.5.\n",
    "Alternative hypothesis. \n",
    "𝐻1 : Humans have a preference for the right side when kissing, i.e. the probability of leaning to the right is greater than 0.5.\n",
    "Sample Data:\n",
    "124 couples were observed, out of which 80 couples inclined their heads to the right side.\n",
    "Assuming that the null hypothesis holds (with a probability of 0.5 leaning to the right), we run 10,000 simulations, each simulating 124 couples. We record the number of couples whose heads lean to the right in each simulation, and then count how many times the simulation results are greater than or equal to 80.(code above)\n",
    "Conclusion:\n",
    "Since the p-value of 0.0008 is less than 0.01, based on the given table, we have very strong evidence against the null hypothesis 𝐻0\n",
    "Thus, the data support the conclusion that humans prefer to tilt their heads to the right when kissing, rather than to the left or have no preference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0931e55c",
   "metadata": {},
   "source": [
    "# Q6\n",
    "A small p-value can't completely prove that the null hypothesis is wrong. It just means that if the null hypothesis were true, the chances of getting extreme data are very low. A low p-value shows strong evidence against the null hypothesis, but it's not 100% certain.\n",
    "\n",
    "Just like Fido in the video, a p-value can't prove whether he's innocent or guilty. A very low p-value means there's strong evidence against him, but it doesn't mean he's definitely guilty. Similarly, a high p-value only suggests there's not enough evidence to prove guilt, but it doesn't confirm he's innocent.\n",
    "\n",
    "There's no specific p-value that can absolutely prove anything. Usually, a p-value below 0.05 is considered strong evidence, but it's not definitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe091a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7\n",
    "# New (Single-tailed test):\n",
    "# If you're testing for greater or less extreme values, the code changes to:\n",
    "SimStats_as_or_more_extreme_than_ObsStat = \\\n",
    "    simulated_statistics >= observed_statistic\n",
    "#or\n",
    "SimStats_as_or_more_extreme_than_ObsStat = \\\n",
    "    simulated_statistics <= observed_statistic\n",
    "\n",
    "# Significance: In a one-tailed test, you are only interested in deviations in one direction. If testing for greater than, you're checking if the simulated statistics are larger than the observed statistic. If testing for less than, you're checking if they are smaller. This is important because it directly changes the interpretation of the p-value and the hypothesis you are testing:\n",
    "# In a two-tailed test, the hypothesis is tested against deviations in both directions.\n",
    "# In a single-tailed test, you are only concerned about whether the statistic is significantly larger (or smaller), but not both.\n",
    "# This reduces the critical region to one side of the distribution, making a single-tailed test more sensitive to extreme values in one direction but at the cost of not detecting extreme values in the opposite direction.\n",
    "# summary:In this conversation, I asked how to modify the code from a two-tailed test to a one-tailed test. The original code checks for deviations in both directions by comparing the absolute differences between simulated statistics and the hypothesized parameter. After receiving an explanation, I learned that a one-tailed test only focuses on deviations in one direction (either greater or smaller than the observed statistic). This change makes the test more sensitive to extreme values in one direction but won't capture deviations in the opposite direction, affecting the interpretation of the p-value and hypothesis test results.\n",
    "# link: https://chatgpt.com/c/6707114e-515c-8003-9c89-41f7f31fdbe4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649c3f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7\n",
    "# Single-tailed Test (Greater-than)\n",
    "# Assume necessary imports and data are already provided\n",
    "population_parameter_value_under_H0 = 0.5\n",
    "\n",
    "# Observed statistic from the real data\n",
    "observed_statistic = (patient_data.HealthScoreChange > 0).mean()\n",
    "\n",
    "# Simulated statistics under the null hypothesis\n",
    "simulated_statistics = IncreaseProportionSimulations_underH0random\n",
    "\n",
    "# Single-tailed test: Greater-than test\n",
    "SimStats_as_or_more_extreme_than_ObsStat = simulated_statistics >= observed_statistic\n",
    "\n",
    "# Output the result\n",
    "print('''Which simulated statistics are \"as or more extreme\"\n",
    "than the observed statistic? (of ''', observed_statistic, ')', sep=\"\")\n",
    "\n",
    "# Create a DataFrame to visualize the comparison\n",
    "pd.DataFrame({'(Simulated) Statistic': simulated_statistics,\n",
    "              '>= '+str(observed_statistic)+\" ?\": ['>= '+str(observed_statistic)+\" ?\"]*len(simulated_statistics),\n",
    "              '\"as or more extreme\"?': SimStats_as_or_more_extreme_than_ObsStat})\n",
    "\n",
    "# Single-tailed Test (Less-than)\n",
    "# Assume necessary imports and data are already provided\n",
    "population_parameter_value_under_H0 = 0.5\n",
    "\n",
    "# Observed statistic from the real data\n",
    "observed_statistic = (patient_data.HealthScoreChange > 0).mean()\n",
    "\n",
    "# Simulated statistics under the null hypothesis\n",
    "simulated_statistics = IncreaseProportionSimulations_underH0random\n",
    "\n",
    "# Single-tailed test: Less-than test\n",
    "SimStats_as_or_more_extreme_than_ObsStat = simulated_statistics <= observed_statistic\n",
    "\n",
    "# Output the result\n",
    "print('''Which simulated statistics are \"as or more extreme\"\n",
    "than the observed statistic? (of ''', observed_statistic, ')', sep=\"\")\n",
    "\n",
    "# Create a DataFrame to visualize the comparison\n",
    "pd.DataFrame({'(Simulated) Statistic': simulated_statistics,\n",
    "              '<= '+str(observed_statistic)+\" ?\": ['<= '+str(observed_statistic)+\" ?\"]*len(simulated_statistics),\n",
    "              '\"as or more extreme\"?': SimStats_as_or_more_extreme_than_ObsStat})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e876e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value: 0.0294\n"
     ]
    }
   ],
   "source": [
    "# Q8\n",
    "# Step 1: Hypotheses\n",
    "# Null Hypothesis (H₀):\n",
    "# The students are guessing randomly whether milk or tea was poured first, meaning their probability of correctly identifying which was poured first is 50%.\n",
    "\n",
    "# Formal Version:\n",
    "# H₀: p = 0.5 (where p is the proportion of students who correctly identify which was poured first).\n",
    "\n",
    "# Alternative Hypothesis (H₁):\n",
    "# The students are performing better than random guessing.\n",
    "\n",
    "# Formal Version:\n",
    "# H₁: p > 0.5.\n",
    "\n",
    "# Step 2: Simulation\n",
    "# To test this hypothesis, we can run a simulation assuming the null hypothesis is true (p = 0.5). Here are the steps:\n",
    "\n",
    "# Use a random number generator to simulate students' responses, setting the probability of correctly identifying which was poured first to 50%.\n",
    "# Run multiple simulations (e.g., 10,000 times) to generate a distribution of possible outcomes under the null hypothesis.\n",
    "# Calculate how many students correctly identified in the experiment (49 out of 80) and compare that to the results of the simulations.\n",
    "# Calculate the p-value, which represents the proportion of simulations that resulted in 49 or more correct identifications.\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "num_students = 80\n",
    "num_correct = 49\n",
    "num_simulations = 10000\n",
    "\n",
    "simulations = np.random.binomial(n=num_students, p=0.5, size=num_simulations)\n",
    "\n",
    "p_value = np.mean(simulations >= num_correct)\n",
    "print(f'P-value: {p_value}')\n",
    "\n",
    "# Conclusion\n",
    "# Based on the simulation results, the calculated p-value is about 0.0294.\n",
    "\n",
    "# Since this p-value is less than the commonly used significance level of 0.05, we reject the original hypothesis (H₀). This suggests that there is sufficient evidence that students outperform random guessing in correctly identifying which poured first.\n",
    "# This result supports the idea that students may indeed be capable of discriminating how tea is prepared, similar to Dr. Bristol's claim in the original experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ca459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9\n",
    "Mostly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
