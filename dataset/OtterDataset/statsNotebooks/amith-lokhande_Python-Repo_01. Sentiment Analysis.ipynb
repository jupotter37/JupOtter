{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a79357",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# Sentiment Analysis\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6349cb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing transformers and scipy libraries\n",
    "\n",
    "# pip install transformers --quiet        \n",
    "# pip install scipy --quiet   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e16f3",
   "metadata": {},
   "source": [
    "* ***We use the \"transformers\" package to download the model from Hugging Face website*** \n",
    "* ***We use \"scipy\" package to convert the o/p of the model into probability scores*** \n",
    "\n",
    "**`pip`** - pip stands for \"Pip Installs Packages\". It is a package management system and command-line tool for installing,  \n",
    "        upgrading, and managing Python packages and dependencies. Pip is the standard package installer for Python and is \n",
    "        widely used in the Python community. It simplifies the process of installing and managing external libraries and \n",
    "        packages, making it easier to work with third-party code in Python projects.\n",
    "\n",
    "\n",
    "**`transformers`** - The \"transformers\" library simplifies the development of NLP (Natural Language Processing) models by \n",
    "        providing access to powerful pre-trained models, efficient tokenization methods, and a unified API for various NLP \n",
    "        tasks. It has become a go-to library for many NLP practitioners and researchers, enabling them to leverage the latest \n",
    "        advancements in transformer-based architectures for their specific NLP applications.\n",
    "        \n",
    "The main highlight of the \"transformers\" library is its extensive collection of pre-trained models, including \n",
    "        state-of-the-art models like BERT, GPT, RoBERTa, and many others. These models are pre-trained on large corpora and can \n",
    "        be fine-tuned or used directly for a wide range of NLP tasks.\n",
    "\n",
    "\n",
    "**`scipy`** - The SciPy library is a comprehensive scientific computing library in Python that provides a wide range of tools \n",
    "        and functions for various areas of scientific and technical computing. Built on top of NumPy, it offers efficient \n",
    "        numerical operations, advanced mathematical functions, optimization algorithms, interpolation techniques, linear algebra\n",
    "        operations, statistical analysis tools, signal and image processing functions, and more. With its extensive\n",
    "        functionality, SciPy serves as a powerful tool for tasks such as numerical analysis, data processing, optimization, \n",
    "        statistics, signal processing, and beyond. It is widely used in scientific research, engineering, data analysis, \n",
    "        and other domains where complex mathematical and scientific computations are required.\n",
    "\n",
    "\n",
    "**`--quiet`** - The behavior and usage of --quiet can vary depending on the tool or script you are using. Generally, when \n",
    "        \"--quiet\" is specified as a command-line argument or option, it instructs the tool or script to operate silently, \n",
    "        without displaying unnecessary information or verbose output. This can be useful when you want to run a program without \n",
    "        being distracted by excessive output.\n",
    "\n",
    "\n",
    "**`Hugging Face website`** - The website contains a model called \"roBERTa\" for tweet sentiment analysis. The model is developed \n",
    "        by the Facebook AI Team. It is pre-trained on 58 million tweets and is proven to be quite accurate for tweet sentiment \n",
    "        analysis. Website - [https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65e93503",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea8ce3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2968fe23",
   "metadata": {},
   "source": [
    "\n",
    "**`AutoTokenizer`** - The AutoTokenizer is a powerful feature provided by the Hugging Face Transformers library in Python. \n",
    "        It is designed to automatically select and instantiate the appropriate tokenizer based on the chosen pre-trained model. \n",
    "        This eliminates the need to manually download and manage different tokenizers for various models. The AutoTokenizer \n",
    "        class supports a wide range of tokenization tasks, such as text classification, named entity recognition, and machine \n",
    "        translation. It provides methods to tokenize input text, convert tokens to input IDs, handle special tokens, and \n",
    "        perform other tokenization-related operations. The AutoTokenizer simplifies the process of working with different\n",
    "        models and ensures consistent tokenization across various tasks, making it a valuable tool for Natural Language \n",
    "        Processing (NLP) tasks in Python.\n",
    "\n",
    "\n",
    "**`AutoModelForSequenceClassification`** - The \"AutoModelForSequenceClassification\" is a component of the Hugging Face \n",
    "        Transformers library in Python that offers automatic model selection and instantiation based on the specified task. \n",
    "        It is specifically designed for sequence classification tasks, such as sentiment analysis or text categorization. By \n",
    "        using the \"AutoModelForSequenceClassification\" class, developers can easily load and initialize the appropriate \n",
    "        pre-trained model for their task without needing to manually download and manage multiple models. The class provides \n",
    "        a consistent interface for fine-tuning, inference, and evaluation of sequence classification models. It supports various\n",
    "        architectures, such as BERT, RoBERTa, and GPT, and allows users to seamlessly switch between them based on their \n",
    "        specific requirements. The \"AutoModelForSequenceClassification\" class simplifies the process of working with sequence \n",
    "        classification models and enables efficient development and deployment of natural language processing models in Python.\n",
    "\n",
    "\n",
    "**`softmax`** - The softmax function is a mathematical function used in machine learning for multiclass classification tasks. \n",
    "        It takes a vector of real numbers as input and transforms them into a probability distribution. The function \n",
    "        exponentiates each element of the input vector and normalizes the results by dividing them by the sum of all \n",
    "        exponentiated values. This normalization ensures that the resulting probabilities sum up to 1. The softmax function \n",
    "        is commonly used to convert model outputs into probabilities for assigning an input to one of multiple classes. \n",
    "        By mapping the outputs to a probability distribution, the softmax function enables the selection of the most probable \n",
    "        class based on the highest probability value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c9013df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tweet = \"@MehranShakarami today's cold @ home ðŸ˜’ https://mehranshakarami.com\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66f9ff3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@user', \"today's\", 'cold', '@', 'home', 'ðŸ˜’', 'http']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# precprocess tweet                                # Explanation:  \n",
    "\n",
    "tweet_words = []\n",
    "\n",
    "for word in tweet.split(' '):                     # splits the tweet word by word\n",
    "    if word.startswith('@') and len(word) > 1:    # to avoid instances where '@' is used as a letter apart from the username we include length of words. If length of the word > 2  including '@' then the code gets executed.\n",
    "        word = '@user'                            # coverts any word that starts with '@' and has more than 2 characters to a word called \"@user\"    \n",
    "    \n",
    "    elif word.startswith('http'):\n",
    "        word = \"http\"                             # coverts any word that starts with 'http..............' to 'http' \n",
    "    tweet_words.append(word)\n",
    "\n",
    "tweet_proc = \" \".join(tweet_words)                # joins the tweet that was split before with the above changes made to @ and http words\n",
    "\n",
    "tweet_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eea3f035",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load model and tokenizer\n",
    "\n",
    "roberta = \"cardiffnlp/twitter-roberta-base-sentiment\"                # Name of the model copied from Hugging Face website\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(roberta)  # Downloading and loading the \"roBERTa\" model \n",
    "tokenizer = AutoTokenizer.from_pretrained(roberta)                   # Loading the Tokenizer to convert tweet text into appropriate numbers\n",
    "\n",
    "labels = ['Negative', 'Neutral', 'Positive']                         # Labelling the outputs of the model\n",
    "\n",
    "\n",
    "# Note - Downloading is done only initially. The code should execute much faster next time since it only loads then. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfd04cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,  1039, 12105,   452,    18,  2569,   787,   184, 17841, 10659,\n",
      "          2054,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sentiment Analysis\n",
    "\n",
    "encoded_tweet = tokenizer(tweet_proc, return_tensors='pt')     # Converting processed tweet into PyTorch tensors 'pt', so that we can pass that into the model\n",
    "\n",
    "print(encoded_tweet)                                           # Printing the encoded tweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540a2b5e",
   "metadata": {},
   "source": [
    "**`input_ids`** - The code snippet you provided represents a dictionary with a single key-value pair. The key is 'input_ids', and the value is a PyTorch tensor.\n",
    "\n",
    "In this specific case, the tensor is a 2D tensor with a shape of (1, 12). The values within the tensor represent the input token IDs for a sequence of 12 tokens. Token IDs are typically used in natural language processing tasks to represent individual tokens from a vocabulary.\n",
    "\n",
    "In the example, the input token IDs include the values [0, 1039, 12105, 452, 18, 2569, 787, 184, 17841, 10659, 2054, 2]. The specific meaning of each token ID depends on the tokenization scheme and the vocabulary used by the model. Token IDs are often obtained by converting text input into a numerical representation that can be processed by the model.\n",
    "\n",
    "Overall, the dictionary represents an input to a model or a part of a model's output, where the 'input_ids' are provided to represent the tokens of a sequence in a numerical format.\n",
    "\n",
    "\n",
    "\n",
    "**`attention_mask`** - It represents a dictionary with a single key-value pair. The key is 'attention_mask', and the value is a PyTorch tensor.\n",
    "\n",
    "In this specific case, the tensor is a 2D tensor with a shape of (1, 12). The values within the tensor represent the attention mask for a sequence of 12 tokens. An attention mask is commonly used in natural language processing tasks, particularly in transformer-based models like BERT.\n",
    "\n",
    "The attention mask is used to indicate which tokens in the sequence should be attended to (considered) by the model and which tokens should be ignored. In this case, all tokens in the sequence have an attention mask value of 1, indicating that they should be attended to. A value of 0 would typically be used to indicate tokens that should be ignored or masked during processing.\n",
    "\n",
    "Overall, the dictionary represents an input to a model or a part of a model's output, where the 'attention_mask' is provided to guide the model's attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a57c34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.3710,  0.3350, -1.7215]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Passing encoded tweet to the model to do sentiment analysis\n",
    "\n",
    "output = model(encoded_tweet['input_ids'], encoded_tweet['attention_mask'])   # entering the keys i.e. \"input_ids\" & \"attention_mask\"\n",
    "\n",
    "print(output)                                                                 # O/p's the sentiment analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "358cb18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.3710,  0.3350, -1.7215]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Simplyfying \n",
    "\n",
    "\"\"\"\n",
    "Instead of passing the encoded tweet in to the model \n",
    "(i.e. \"output = model(encoded_tweet['input_ids'], encoded_tweet['attention_mask'])\") \n",
    "we can simplify it by writing the below code \n",
    "\"\"\"\n",
    "output = model(**encoded_tweet)\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd8a6400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_outputs.SequenceClassifierOutput'>\n"
     ]
    }
   ],
   "source": [
    "print(type(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd18bc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.3709717   0.33496094 -1.7214578 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Converting the result into probabilites \n",
    "\n",
    "scores = output[0][0].detach().numpy()     # scores = output[0][0].detach() selects the o/p list i.e. [ 1.3710,  0.3350, -1.7215]\n",
    "                                           # scores = output[0][0].detach().numpy() converts the list into a numpy array\n",
    "    \n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3d08373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(scores))         # confirmatinon that the list is converted into a numpy array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3180b635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7141536  0.2534299  0.03241653]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scores = softmax(scores)     # passing scores into the softmax function to covert it into probabilities\n",
    "\n",
    "print(scores)               \n",
    "\n",
    "\"\"\"\n",
    "# Note \n",
    "\n",
    "1. You can multiply the above code by 100 to get proper integers in terms of 100% rather than from 0 to 1. This is totally optional however and is down to the user's choice. \n",
    "2. Make sure to not run this cell over and over again, as the o/p changes everytime the code is run. \n",
    "3. Suppose if we run this cell over and over again for some reason, then to get the original o/p run the program from 2 cells above i.e., from # Converting the result into probabilites\n",
    "\"\"\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5118b9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative 0.7141536\n",
      "Neutral 0.2534299\n",
      "Positive 0.032416526\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Printing output with the corrersponding labels\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    \n",
    "    l = labels[i]\n",
    "    s = scores[i]\n",
    "    print(l,s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f14591",
   "metadata": {},
   "source": [
    "### <span style=\"background-color: yellow;\">Let's look at another example using the entire code in a single cell</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15af0aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative 0.20959757\n",
      "Neutral 1.897672\n",
      "Positive 97.89272\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tweet = 'Great content! subscribed ðŸ˜‰'\n",
    "\n",
    "# precprcess tweet\n",
    "tweet_words = []\n",
    "\n",
    "for word in tweet.split(' '):\n",
    "    if word.startswith('@') and len(word) > 1:\n",
    "        word = '@user'\n",
    "    \n",
    "    elif word.startswith('http'):\n",
    "        word = \"http\"\n",
    "    tweet_words.append(word)\n",
    "\n",
    "tweet_proc = \" \".join(tweet_words)\n",
    "\n",
    "# load model and tokenizer\n",
    "roberta = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
    "tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
    "\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "# sentiment analysis\n",
    "encoded_tweet = tokenizer(tweet_proc, return_tensors='pt')\n",
    "# output = model(encoded_tweet['input_ids'], encoded_tweet['attention_mask'])\n",
    "output = model(**encoded_tweet)\n",
    "\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores) * 100            # printing scores in terms of 100 % by multiplying * 100 \n",
    "\n",
    "# print output with corresponding labels\n",
    "for i in range(len(scores)):\n",
    "    \n",
    "    l = labels[i]\n",
    "    s = scores[i]\n",
    "    print(l,s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443af3b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
