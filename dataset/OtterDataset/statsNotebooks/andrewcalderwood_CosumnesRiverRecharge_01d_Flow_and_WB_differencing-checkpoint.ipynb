{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e3557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python utilities\n",
    "import os\n",
    "from os.path import join, basename,dirname, exists, expanduser\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# standard python plotting utilities\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "# standard geospatial python utilities\n",
    "# import pyproj # for converting proj4string\n",
    "# import shapely\n",
    "import geopandas as gpd\n",
    "# import rasterio\n",
    "\n",
    "# mapping utilities\n",
    "import contextily as ctx\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "import matplotlib.font_manager as fm\n",
    "from matplotlib.ticker import MaxNLocator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f0b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_dir = expanduser('~')\n",
    "doc_dir = join(usr_dir, 'Documents')\n",
    "    \n",
    "# dir of all gwfm data\n",
    "gwfm_dir = dirname(doc_dir)+'/Box/research_cosumnes/GWFlowModel'\n",
    "# dir of stream level data for seepage study\n",
    "proj_dir = gwfm_dir + '/Oneto_Denier/'\n",
    "dat_dir = proj_dir+'Stream_level_data/'\n",
    "\n",
    "fig_dir = proj_dir+'/Streambed_seepage/figures/'\n",
    "hob_dir = join(gwfm_dir, 'HOB_data')\n",
    "sfr_dir = gwfm_dir+'/SFR_data/'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a585fd8-4eeb-4954-aae8-c626e3436f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_path(fxn_dir):\n",
    "    \"\"\" Insert fxn directory into first position on path so local functions supercede the global\"\"\"\n",
    "    if fxn_dir not in sys.path:\n",
    "        sys.path.insert(0, fxn_dir)\n",
    "\n",
    "add_path(doc_dir+'/GitHub/flopy')\n",
    "import flopy \n",
    "py_dir = join(doc_dir,'GitHub/CosumnesRiverRecharge/python_utilities')\n",
    "add_path(py_dir)\n",
    "add_path(join(doc_dir,'GitHub/CosumnesRiverRecharge/tprogs_utilities'))\n",
    "\n",
    "# from importlib import reload\n",
    "# import map_cln\n",
    "# reload(map_cln)\n",
    "\n",
    "from mf_utility import get_dates, get_layer_from_elev, clean_wb\n",
    "from map_cln import gdf_bnds, plt_cln, xy_lab\n",
    "\n",
    "# scenario specific function\n",
    "from flopy_utilities import reach_data_gdf\n",
    "from tprogs_review import get_tprogs_quants\n",
    "\n",
    "add_path(dirname(os.getcwd()))\n",
    "from OD_utility import run_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario = '' # baseline, levee removal occurred in 2014\n",
    "# create identifier for scenario if levee removal didn't occur\n",
    "scenario = 'no_reconnection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ab77f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_dir = 'F:/WRDAPP'\n",
    "c_dir = 'C:/WRDAPP'\n",
    "if os.path.exists(ext_dir):\n",
    "    loadpth = ext_dir \n",
    "elif os.path.exists(c_dir):\n",
    "    loadpth = c_dir \n",
    "loadpth +=  '/GWFlowModel/Cosumnes/Stream_seepage'\n",
    "\n",
    "upscale = 'upscale4x_'\n",
    "# model_nam = 'oneto_denier_'+upscale+'2014_2018'\n",
    "model_nam = 'oneto_denier_'+upscale+'2014_2020'\n",
    "model_ws0 = join(loadpth,model_nam)\n",
    "\n",
    "# if scenario != '':\n",
    "#     model_ws += '_' + scenario\n",
    "model_ws0 = join(loadpth,model_nam) + '_' + scenario\n",
    "\n",
    "\n",
    "# try alternate scenario against regular levee removal\n",
    "model_nam = 'oneto_denier_2014_2020_floodplain4'\n",
    "model_ws = join(loadpth,model_nam)\n",
    "\n",
    "# model_ws = join(loadpth,'parallel_oneto_denier','realization000')\n",
    "load_only = ['DIS','BAS6','UPW','SFR','OC', \"EVT\",'LAK']\n",
    "m = flopy.modflow.Modflow.load('MF.nam', model_ws= model_ws0, \n",
    "                                exe_name='mf-owhm.exe', version='mfnwt',\n",
    "                              load_only=load_only,\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33639cb-5d01-4d4f-85b6-c1a9214b3206",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_baseline = 'Baseline'\n",
    "label_restoration = 'Levee removal'\n",
    "\n",
    "label_baseline = 'Expanded floodplain'\n",
    "label_restoration = 'Levee removal'\n",
    "# subdirectory for alternate floodplain testing\n",
    "fig_dir = join(proj_dir, 'levee_removal','figures', 'expansion')\n",
    "\n",
    "label_baseline = 'Expanded floodplain'\n",
    "label_restoration = 'Baseline'\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "print(model_ws0)\n",
    "print(model_ws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0004888d-19ce-474b-b66f-45bfb30b2a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "strt_date, end_date, dt_ref = get_dates(m.dis, ref='strt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6209198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow, ncol = (m.dis.nrow, m.dis.ncol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c0ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Quantiles: ',[0,0.5,0.6,0.75,1])\n",
    "print('HK :',np.quantile(m.upw.hk.array,[0,0.5,0.6,0.75,1]))\n",
    "print('VKA :',np.quantile(m.upw.vka.array,[0,0.5,0.6,0.75,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563edcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grp = 'inset_oneto_denier'\n",
    "grid_dir = join(gwfm_dir, 'DIS_data/streambed_seepage/grid')\n",
    "grid_fn = join(grid_dir, model_grp,'rm_only_grid.shp')\n",
    "grid_p = gpd.read_file(grid_fn)\n",
    "grid_p.crs='epsg:32610'\n",
    "m_domain = gpd.GeoDataFrame(pd.DataFrame([0]), geometry = [grid_p.unary_union], crs=grid_p.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f20454",
   "metadata": {},
   "outputs": [],
   "source": [
    "XSg = pd.read_csv(join(model_ws,'04_XSg_filled.csv'))\n",
    "XSg = gpd.GeoDataFrame(XSg, geometry = gpd.points_from_xy(XSg.Easting, XSg.Northing), crs='epsg:32610')\n",
    "\n",
    "# overwrite SFR segment/reach input relevant to seepage\n",
    "# sensor_dict = pd.read_csv(join(model_ws, 'sensor_xs_dict.csv'), index_col=0)\n",
    "# XS_params = sensor_dict.join(params.set_index('Sensor'), on='Sensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac30a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = pd.read_csv(model_ws+'/ZonePropertiesInitial.csv', index_col='Zone')\n",
    "# convert from m/s to m/d\n",
    "params['K_m_d'] = params.K_m_s * 86400 \n",
    "tprogs_hist = np.flip([0.590, 0.155, 0.197, 0.058]) \n",
    "tprogs_vals = np.arange(1,5)\n",
    "\n",
    "vka = m.upw.vka.array\n",
    "# def get_tprogs_quants(vka, params, tprogs_vals, tprogs_hist):\n",
    "#     \"\"\"\n",
    "#     Find the upper and lower values of hydraulic parameters in \n",
    "#     a 3D array format based on the histogram values \n",
    "#     \"\"\"\n",
    "#     tprogs_quants = 1-np.append([0], np.cumsum(tprogs_hist)/np.sum(tprogs_hist))\n",
    "#     vka_quants = pd.DataFrame(tprogs_quants[1:], columns=['quant'], index=tprogs_vals)\n",
    "#     # dataframe summarizing dominant facies based on quantiles\n",
    "#     vka_quants['vka_min'] = np.quantile(vka, tprogs_quants[1:])\n",
    "#     vka_quants['vka_max'] = np.quantile(vka, tprogs_quants[:-1])\n",
    "#     vka_quants['facies'] = params.loc[tprogs_vals].Lithology.values\n",
    "#     return(vka_quants)\n",
    "\n",
    "vka_quants = get_tprogs_quants(vka, params, tprogs_vals, tprogs_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615df5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid_sfr = reach_data_gdf(m.sfr, grid_p)\n",
    "\n",
    "# group sfrdf by vka quantiles\n",
    "sfr_vka = vka[grid_sfr.k, grid_sfr.i, grid_sfr.j]\n",
    "\n",
    "for p in vka_quants.index:\n",
    "    facies = vka_quants.loc[p]\n",
    "    grid_sfr.loc[(sfr_vka< facies.vka_max)&(sfr_vka>= facies.vka_min),'facies'] = facies.facies\n",
    "#     # add color for facies plots\n",
    "# grid_sfr = grid_sfr.join(gel_color.set_index('geology')[['color']], on='facies')\n",
    "    \n",
    "grid_sfr_full = grid_sfr.copy()\n",
    "\n",
    "grid_sfr = grid_sfr[grid_sfr.strhc1!=0]\n",
    "grid_sfr['vka'] = vka[grid_sfr.k, grid_sfr.i, grid_sfr.j]\n",
    "grid_sfr['rch_num'] = np.arange(1,len(grid_sfr)+1)\n",
    "grid_sfr['Total distance (m)'] = grid_sfr['rchlen'].cumsum()\n",
    "\n",
    "pd_sfr = grid_sfr.set_index(['iseg','ireach'])[['rchlen','strtop', 'facies', 'strthick', 'slope','Total distance (m)']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b1aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lak_shp = join(gwfm_dir,'LAK_data/floodplain_delineation')\n",
    "# shapefile rectangle of the area surrounding the Dam within about 5 cells\n",
    "lak_gpd = gpd.read_file(join(lak_shp,'LCRFR_ModelDom_2017/LCRFR_2DArea_2015.shp' )).to_crs('epsg:32610')\n",
    "\n",
    "lak_cells = gpd.sjoin(grid_p,lak_gpd,how='right',predicate='within').drop(columns='index_left')\n",
    "\n",
    "# filter zone budget for Blodgett Dam to just within 5 cells or so of the Dam\n",
    "zon_lak = np.zeros((grid_p.row.max(),grid_p.column.max()),dtype=int)\n",
    "zon_lak[lak_cells.row-1,lak_cells.column-1]=1\n",
    "\n",
    "zon_mod = np.ones((grid_p.row.max(),grid_p.column.max()),dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4a7e72-5256-468e-a55f-f119c0729816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the river is next to the floodplain\n",
    "sfr_lak = gpd.sjoin_nearest(grid_sfr, lak_gpd, max_distance=100)\n",
    "lak_up = sfr_lak.rch_num.min()\n",
    "lak_dn = sfr_lak.rch_num.max()\n",
    "print('Floodplain runs along river from reach',lak_up, 'to reach', lak_dn)\n",
    "\n",
    "# identify reaches where floodplain connects\n",
    "fp_in_reach = grid_sfr[grid_sfr.iseg == grid_sfr_full[grid_sfr_full.outseg==-1].iseg.values[0]-2].rch_num.max()\n",
    "fp_out_reach = grid_sfr[grid_sfr.iseg == grid_sfr_full[grid_sfr_full.iupseg==-1].iseg.values[0]+1].rch_num.max()\n",
    "print('River flows into floodplain at reach', fp_in_reach,'and returns to river at reach', fp_out_reach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406ed422-b9c0-4dca-a834-fee6d2e9bca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Floodplain area is %.f hectares' %(lak_gpd.geometry.area.values[0]/1E4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4093def-4aba-4848-9215-c788079a8ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_path = join(dirname(os.getcwd()), 'mf_wb_color_dict.xlsx')\n",
    "zon_color_dict = pd.read_excel(color_path, sheet_name='owhm_wb_dict', header=0, index_col='flux',comment='#').color.to_dict()\n",
    "zon_name_dict = pd.read_excel(color_path, sheet_name='owhm_wb_dict', header=0, index_col='flux',comment='#').name.to_dict()\n",
    "\n",
    "zb_alt = pd.read_excel(color_path, sheet_name='flopy_to_owhm', header=0, index_col='flopy',comment='#').owhm.to_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d80ea",
   "metadata": {},
   "source": [
    "## Sensor data and XS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac3d574",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_grid = pd.read_csv(join(proj_dir, 'mw_hob_cleaned.csv'))\n",
    "rm_grid = gpd.GeoDataFrame(rm_grid, geometry = gpd.points_from_xy(rm_grid.Longitude,rm_grid.Latitude), \n",
    "                           crs='epsg:4326').to_crs(grid_p.crs)\n",
    "# get model layer for heads\n",
    "hob_row = rm_grid.row.values-1\n",
    "hob_col = rm_grid.column.values-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54bbd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XS are every 100 m\n",
    "xs_all = pd.read_csv(dat_dir+'XS_point_elevations.csv',index_col=0)\n",
    "xs_all = gpd.GeoDataFrame(xs_all,geometry = gpd.points_from_xy(xs_all.Easting,xs_all.Northing), crs='epsg:32610')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3732c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# correspond XS to sensors\n",
    "rm_elev = gpd.sjoin_nearest(XSg, rm_grid, how='right',lsuffix='xs', rsuffix='rm')\n",
    "#MW_11, MW_CP1 had doubles with sjoin_nearest due to XS duplicates from Oneto_Denier\n",
    "rm_elev = rm_elev.drop_duplicates(['xs_num','Sensor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e3e108",
   "metadata": {},
   "source": [
    "## Model output - time variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbbab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdobj0 = flopy.utils.HeadFile(model_ws0+'/MF.hds')\n",
    "hdobj = flopy.utils.HeadFile(model_ws+'/MF.hds')\n",
    "spd_stp = hdobj.get_kstpkper()\n",
    "times = hdobj.get_times()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3bed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb0, out_cols, in_cols = clean_wb(model_ws0, dt_ref)\n",
    "wb0_cols = np.append(out_cols, in_cols)\n",
    "\n",
    "fig,ax= plt.subplots(3,1, sharex=True)\n",
    "wb0.plot(y='PERCENT_ERROR', ax=ax[0])\n",
    "wb0.plot(y=out_cols, ax=ax[1], legend=True)\n",
    "wb0.plot(y=in_cols, ax=ax[2], legend=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d80782",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb, out_cols, in_cols = clean_wb(model_ws, dt_ref)\n",
    "wb_cols = np.append(out_cols, in_cols)\n",
    "fig,ax= plt.subplots(3,1, sharex=True)\n",
    "wb.plot(y='PERCENT_ERROR', ax=ax[0])\n",
    "wb.plot(y=out_cols, ax=ax[1], legend=True)\n",
    "wb.plot(y=in_cols, ax=ax[2], legend=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f226dde7",
   "metadata": {},
   "source": [
    "## Water budget change\n",
    "The expected components to change in the water budget are evapotranspiration, groundwater inflow and outflow, and change in storage. Plot each of these on the same plot or with a differenced line.\n",
    "\n",
    "- Amy Yoder performed a t-test with levee-restoration status as the grouping variable and recharge volume as the response variable (she did this for each recharge event, in this case I would do it for each year)+. She also plotted cumulative flow volume against recharge for each event and applied a power regression equation to fit groups or pre- and post-restoration to show how restoration ideally leads to more recharge per cumulative flow.\n",
    "\n",
    "- If I apply a t-test, we are comparing the annual recharge and storage change between the scenarios to see if they have significantly different average values. Should also plot linear regression to use slope as an explanation of how water budget terms change from the scenarios.\n",
    "\n",
    "- **Water budget can be compared with riparian zone water budget stats**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208cb4aa",
   "metadata": {},
   "source": [
    "The statistic is calculated as (np.mean(a) - np.mean(b))/se, where se is the standard error. Therefore, the statistic will be positive when the sample mean of a is greater than the sample mean of b and negative when the sample mean of a is less than the sample mean of b.  We apply a related t-test because samples are paired by datetime or location.  \n",
    "- Standard error is $\\frac{\\sigma}{\\sqrt{n}}$ of the differences between all pairs\n",
    "\n",
    "Two-sample t-test: Decide if the population means for two different groups are equal or not  \n",
    "Paired t-test: Decide if the difference between paired measurements for a population is zero or not  \n",
    "\n",
    "The water budget data is generally log-normally distributed so ideally I should apply a log transform before data analysis.\n",
    "\n",
    "\"The paired t test provides an hypothesis test of the difference between population means for a pair of random samples whose differences are approximately normally distributed. Please note that a pair of samples, each of which are not from normal a distribution, often yields differences that are normally distributed.\" To gain normality one could apply a regular t-test to the differenced data against an expected mean of 0, the results from the 1-sample t-test are the same as the paired t-test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5401f0-a41c-4d87-a934-f0bf130a26eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel, ttest_1samp, linregress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6fa3dd-eb3d-4401-9cdf-b33b5813c0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg_plt(a, b):\n",
    "    \"\"\" Plot scatter plot and linear regression, print ttest results\n",
    "    \"\"\"\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(a, b)\n",
    "\n",
    "    plt.scatter(a, b)\n",
    "    x_range = np.array([[np.min((a,b))], [np.max((a,b))]])\n",
    "    plt.plot(x_range, slope*x_range + intercept, color='black', linewidth=1)\n",
    "    plt.annotate('y = '+str(np.round(slope,3))+'x + '+ str(np.round(intercept,2)), (0.1,0.8), xycoords='axes fraction')\n",
    "    plt.ylabel('No Reconnection')\n",
    "    plt.xlabel(label_baseline)\n",
    "def run_ttest(a, b, term, freq):\n",
    "    \"\"\" Run ttest and summarize to tables\n",
    "    \"\"\"\n",
    "    t_out = ttest_rel(a, b)\n",
    "\n",
    "    t_df = pd.DataFrame([t_out.statistic, t_out.pvalue]).transpose()\n",
    "    t_df.columns=['statistic','pvalue']\n",
    "    t_df['term'] = term\n",
    "    # t_df['freq'] = freq\n",
    "    # t_df['season'] = season\n",
    "    t_df['mean_a'] = np.mean(a)\n",
    "    t_df['mean_b'] = np.mean(b)\n",
    "    t_df['perc_diff_in_means'] = 100*(np.mean(a)-np.mean(b))/np.abs((np.mean(a)+np.mean(b))/2)\n",
    "\n",
    "    # rounding to clean up output\n",
    "    t_df.statistic = t_df.statistic.round(3)\n",
    "    t_df.pvalue = t_df.pvalue.round(4)\n",
    "    t_df.perc_diff_in_means = t_df.perc_diff_in_means.round(2)\n",
    "\n",
    "    # if pvalue is insignificant then don't include\n",
    "    t_df.loc[t_df.pvalue>=0.05,'perc_diff_in_means'] = '-'\n",
    "    return(t_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2552c9e-b976-44f5-984c-05ed278f449a",
   "metadata": {},
   "source": [
    "By cutting off the wet season in april, there is a disconnect in ET. It might be best to switch to Winter, Spring, summer, and fall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37131a17-d463-48a8-8c50-95e09d2a5e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wet_months = [11,12,1,2,3,4]\n",
    "# dry_months = [5,6,7,8,9,10]\n",
    "# fall_months=[9,10,11]\n",
    "\n",
    "def run_stats(wb, wb0, term, season=None, freq='monthly', plot=False):\n",
    "    if season == 'Winter':\n",
    "        months = [12,1,2]\n",
    "    elif season== 'Spring':\n",
    "        months = [3,4,5]\n",
    "    elif season =='Summer':\n",
    "        months = [6,7,8]\n",
    "    elif season=='Fall':\n",
    "        months=[9,10,11]\n",
    "    if season is not None:\n",
    "        wb = wb[wb.index.month.isin(months)]\n",
    "        wb0 = wb0[wb0.index.month.isin(months)]\n",
    "    if freq=='annual':\n",
    "        a = wb0.resample('AS-Oct').sum()[term].values\n",
    "        b = wb.resample('AS-Oct').sum()[term].values\n",
    "    elif freq=='monthly':\n",
    "        a = wb0.resample('MS').sum()[term].values\n",
    "        b = wb.resample('MS').sum()[term].values\n",
    "    elif freq=='daily':\n",
    "        a = wb0.resample('D').sum()[term].values\n",
    "        b = wb.resample('D').sum()[term].values\n",
    "\n",
    "    t_df = run_ttest(a, b, term, freq)\n",
    "    t_df['freq'] = freq\n",
    "    t_df['season'] = season\n",
    "    if plot:\n",
    "        print('T-test statistic: %.2f' %t_df.statistic.iloc[0], 'and pvalue: %.4f' %t_df.pvalue.iloc[0])\n",
    "        lin_reg_plt(a, b)\n",
    "        plt.title(term)\n",
    "        plt.show()\n",
    "\n",
    "    return(t_df)\n",
    "\n",
    "# run_stats(wb, wb0, 'SFR_IN', season='Wet', freq='monthly', plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc53d9f0",
   "metadata": {},
   "source": [
    "Do we want to use the slope of the linear regression as a way to show the relationship at each point versus the relationship on average (t-test)? The slope shows the relationship in a more specific way while the t-test helps decide the net effect. Assuming our water years are representative then the t-test can be a decider of effectiveness while the slope helps show the significance of the benefits?  \n",
    "- linear regression is helpful for runderstanding but should be presented in an appendix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda0a3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_stats(wb, wb0, 'SFR_OUT', freq='annual')\n",
    "# t_out = run_stats(wb, wb0, 'ET_OUT', freq='annual', plot=True)\n",
    "# t_out = run_stats(wb, wb0, 'SFR_IN', freq='annual', plot=True, season = 'Dry')\n",
    "# plt.show()\n",
    "# t_out = run_stats(wb, wb0, 'SFR_IN', freq='annual', plot=True, season = 'Dry')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce103c30",
   "metadata": {},
   "source": [
    " - There is a tight relationship of recharge with and without levee removal, with the slope indicating a reduction in change in storage going from the baseline scenario to a no reconnection scenario. This would go further that when there are losses in storage they are larger than in the baseline. This linear relationship exists both on an annual and monthly scale, the slope is reduced at a monthly scale which shows that on a monthly scale there are larger recharge gains under levee removal. There is not a statistically significant difference in mean change in storage.\n",
    " - The baseflow has a statistically significant difference in means. The slope is 0 because there is no baseflow in the no reconnection scenario. The statistically significant relationship only exists in the wet season.\n",
    " - For streamflow seepage there is relationship in the dry and wet seasons, but the dry season relationship shows that the different scenarios while having different means (t-test), have similar monthly stream seepage. The wet season shows a stronger relationship that stream seepage is large without levee removal.  \n",
    " - Groundwater in/outflow is statistically signficantly different but the slope is almost near one so not worth presenting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15da444-d682-4684-81b1-b22597022d14",
   "metadata": {},
   "source": [
    "Using the daily values for the t-test input keeps most of the percent differences the same and adds a few new significant differences in the dry season for floodplain recharge and baseflow which show much bigger differences because the no reconnection case has zero values. It's helpful to see but not necessary for this first presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9dd6b7-4b97-4299-b032-b917bd92db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_dir = join(fig_dir, 'ttest_results')\n",
    "os.makedirs(ttest_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6ecf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_all = pd.DataFrame()\n",
    "for freq in ['annual','monthly','daily']:\n",
    "    for t in ['dSTORAGE_sum','LAK_IN', 'ET_OUT', 'GHB_NET', 'SFR_IN', 'SFR_OUT']:\n",
    "        for s in ['Winter','Spring','Summer','Fall']:\n",
    "            t_df = run_stats(wb, wb0, t, freq=freq, season=s)\n",
    "\n",
    "            ttest_all = pd.concat((ttest_all, t_df))\n",
    "# replace term with clean name\n",
    "ttest_all['term'] = [zon_name_dict[t] for t in ttest_all.term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfde510",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ttest_out = ttest_all[['freq','term','season','statistic','pvalue', 'perc_diff_in_means']]\n",
    "\n",
    "ttest_out[ttest_out.freq=='monthly'].drop(columns=['freq']).to_csv(join(ttest_dir,'wb_seasonal_monthly.csv'), index=False)\n",
    "ttest_out[ttest_out.freq=='annual'].drop(columns=['freq']).to_csv(join(ttest_dir,'wb_seasonal_annual.csv'), index=False)\n",
    "\n",
    "# ttest_out[ttest_out.freq=='monthly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c0e5a1-9c72-4161-b98f-e3b68253e921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the means\n",
    "\n",
    "ttest_monthly = ttest_all[ttest_all.freq=='monthly']\n",
    "plt_terms = ttest_monthly.term.unique()\n",
    "plt_labels=['Cumulative\\nStorage\\nChange', 'Floodplain\\nRecharge', 'GW ET',\n",
    "            'Net\\nGW Flow','Stream\\nLosses', 'Stream\\nBaseflow']\n",
    "scale=1E3\n",
    "# scale=1E6 # \n",
    "fig,ax = plt.subplots(len(plt_terms),1, figsize=(5,6.5), dpi=300,sharex=True)\n",
    "for n, t in enumerate(plt_terms):\n",
    "    df = ttest_monthly[ttest_monthly.term==t].copy()\n",
    "    df_mean = df.set_index('season')[['mean_a','mean_b']].multiply(1/scale)\n",
    "    # df_mean.plot(y=['mean_a','mean_b'], kind='bar', ax=ax[n], legend=False)\n",
    "    df_mean.plot(y=['mean_a','mean_b'], kind='line', ax=ax[n], legend=False)\n",
    "    df.assign(star = df.mean_a/scale)[df.pvalue<0.05].plot(x='season',y='star', kind='scatter', marker='*', s=100, ax=ax[n])\n",
    "    ax[n].set_ylabel(plt_labels[n])\n",
    "    ax[n].set_xticks(np.arange(0,4))\n",
    "    ax[n].set_xticks([], minor=True)\n",
    "plt.xticks(rotation=0);\n",
    "plt.xlabel(None);\n",
    "fig.supylabel('Flux (thousand $m^3$/day)')\n",
    "fig.tight_layout()\n",
    "fig.legend([label_restoration,label_baseline, 'Significant'], ncol=3, \n",
    "           loc='outside upper center', bbox_to_anchor=(0.5, 1.05),)# 0.4, 0.95 no tight layout\n",
    "# ax[5].legend([label_restoration,label_baseline, 'Significant'], ncol=1, loc='upper right')\n",
    "# fig.legend([label_restoration,label_baseline, 'Significant'], ncol=1, loc='outside center right', bbox_to_anchor=(0.95, 0.5),)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451506e4-feaf-43c2-95fa-f99f73fb3715",
   "metadata": {},
   "source": [
    "### Reference values\n",
    "Use the output saved in the table for the percent changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6abfcc-1e50-4c01-b20a-6508347378fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ttest_monthly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bb9596-0a4f-41ed-9bcd-ca4a1aff673d",
   "metadata": {},
   "source": [
    "# WY comparison\n",
    "- good linear fit all WY: dSTORAGE_sum, ET_OUT, GHB_NET\n",
    "- for LAK_IN the linearity is worse in\n",
    "\n",
    "Need to use the full year to avoid cutting off months with seasonal definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce7edca-0205-4641-b7be-dada0fbf8789",
   "metadata": {},
   "outputs": [],
   "source": [
    "## closer review for understanding\n",
    "# t = 'LAK_IN'\n",
    "# freq='monthly'\n",
    "# s='Wet'\n",
    "# for wy in np.arange(2015,2021):\n",
    "#     yr_strt = str(wy-1)+'-10-1'\n",
    "#     yr_end = str(wy)+'-9-30'\n",
    "#     run_stats(wb.loc[yr_strt:yr_end], wb0.loc[yr_strt:yr_end], t, freq=freq,  plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c7134f-1537-4181-beaa-f874c6f55a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_all = pd.DataFrame()\n",
    "for t in ['dSTORAGE_sum','ET_OUT','SFR_IN', 'LAK_IN', 'SFR_OUT']:\n",
    "    s = 'Wet'\n",
    "    for wy in np.arange(2015,2021):\n",
    "        yr_strt = str(wy-1)+'-10-1'\n",
    "        yr_end = str(wy)+'-9-30'\n",
    "        t_df = run_stats(wb.loc[yr_strt:yr_end], wb0.loc[yr_strt:yr_end], t, freq=freq)\n",
    "        \n",
    "        ttest_all = pd.concat((ttest_all, t_df.assign(wy=wy)))\n",
    "# replace term with clean name\n",
    "ttest_all['term'] = [zon_name_dict[t] for t in ttest_all.term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc3ebcd-a06d-4619-af53-baf0f902f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_out = ttest_all[['term','season','wy','statistic','pvalue', 'perc_diff_in_means']]\n",
    "\n",
    "ttest_out.to_csv(join(ttest_dir, 'wb_wy_monthly.csv'), index=False)\n",
    "# ttest_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e33bc50-168e-4309-b713-9f5f61113c46",
   "metadata": {},
   "source": [
    "## Monthly t-test to plot\n",
    "If the t-tests are done on a monthly scale we can then count the number of months by season and water year that are significantly different while keeping the standard error based on the daily data so these would be truer t-tests since the monthly summed data might be reducing the variance.  \n",
    "\n",
    "We want to know the number of months that are significantly different and in which seasons and years the occur.\n",
    "- histogram by season and year\n",
    "- time series or histogram for percent difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b7ef0-92a0-4f2b-8b94-9f24c6f117c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq='daily'\n",
    "s=None\n",
    "ttest_all = pd.DataFrame()\n",
    "ttest1_all = pd.DataFrame()\n",
    "months = pd.date_range(strt_date, end_date, freq='MS')[:-1]\n",
    "for t in ['dSTORAGE_sum', 'LAK_IN','ET_OUT','GHB_NET', 'SFR_IN',  'SFR_OUT']:\n",
    "    for n, month in enumerate(months):\n",
    "        m_strt = month\n",
    "        m_end = month +pd.offsets.MonthEnd()\n",
    "        t_df = run_stats(wb.loc[m_strt:m_end], wb0.loc[m_strt:m_end], t, freq=freq)\n",
    "        ttest_all = pd.concat((ttest_all, t_df.assign(month=m_strt)))\n",
    "        # t1_df = ttest_1samp((wb.loc[m_strt:m_end,t]-wb0.loc[m_strt:m_end,t]).values, 0) # ttest of difference\n",
    "        # ttest1_all = pd.concat((ttest1_all, pd.DataFrame(t1_df).transpose().assign(month=m_strt, term=t)))\n",
    "# replace term with clean name\n",
    "ttest_all['term_name'] = [zon_name_dict[t] for t in ttest_all.term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d6ad4-dfe1-46c5-b78d-78a0a0a3127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ttest1_all = ttest1_all.rename(columns={0:'statistic',1:'pvalue'})\n",
    "# ttest1_all['sig'] = 0\n",
    "# ttest1_all.loc[ttest1_all.pvalue<0.05,'sig'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5900db5b-663a-4802-b3ba-d27e29aac188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# something simple for the histograms\n",
    "ttest_all['sig'] = 0\n",
    "ttest_all.loc[ttest_all.pvalue<0.05,'sig'] = 1\n",
    "\n",
    "ttest_all['wy'] = ttest_all.month.dt.year\n",
    "ttest_all.loc[ttest_all.month.dt.month>=10, 'wy'] +=1\n",
    "# ttest_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeca6b2-b602-4afc-8f3e-8153fc7498a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ttest_\n",
    "ttest_all['log_pval'] =  np.log10(ttest_all.pvalue)\n",
    "# ttest_all\n",
    "ttest_all['sig_lvl'] = 'NA'\n",
    "# suggested terms for identifying the strength of the evidence <0.1 is weak and >0.1 is little to none\n",
    "# below 1E-4 results should be listed as <1E-4\n",
    "lvl_name = ['Moderate', 'Strong', 'Very Strong']\n",
    "for pn, pl in enumerate([0.05, 0.01, 1E-3]):\n",
    "    ttest_all.loc[ttest_all.pvalue < pl, 'sig_lvl'] = lvl_name[pn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dbc5fc-1c68-48a9-b68e-fb19354ccf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Helen though this was boring, could be represented with a simpler table giving the number of sig months if needed\n",
    "# # very slow with sns.histplot\n",
    "# plt_terms = ['dSTORAGE_sum','LAK_IN', 'ET_OUT','GHB_NET', 'SFR_IN', 'SFR_OUT']\n",
    "# plt_labels=['Cumulative\\nStorage\\nChange', 'Floodplain\\nRecharge', 'GW ET',\n",
    "#             'Net\\nGW Flow','Stream\\nLosses', 'Stream\\nBaseflow']\n",
    "# fig, ax = plt.subplots(len(plt_terms),1, figsize=(6.5,len(plt_terms)*1), dpi=300,sharey=True, sharex=True)\n",
    "\n",
    "# for n,t in enumerate(plt_terms):\n",
    "#     t_df = ttest_all[ttest_all.term==t]\n",
    "#     t_df.groupby('wy').sum(numeric_only=True).plot(y='sig',kind='bar', ax=ax[n], legend=False)\n",
    "# plt.xlabel(None)\n",
    "# ax[n].set_yticks(np.arange(0,14,6));\n",
    "# ax[n].set_yticks(np.arange(0,12,3), minor=True);\n",
    "# for n, t in enumerate(plt_terms):\n",
    "#     ax[n].grid(which='both',axis='y',linestyle='--', alpha=0.8)\n",
    "#     ax[n].set_ylabel(plt_labels[n])\n",
    "\n",
    "# plt.xticks(rotation=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaad1af-639b-4a94-953b-79bb66c15274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sig_fill(t_df, ax):\n",
    "#     xlim = ax.get_xlim()\n",
    "#     ylim = ax.get_ylim()\n",
    "#     t_df['sig_grp'] = (t_df.sig != t_df.sig.shift(1)).cumsum()\n",
    "#     # sig_grps = t_df.sig_grp.dropna().unique()\n",
    "#     sig_grps = t_df[t_df.sig==1].sig_grp.dropna().unique()\n",
    "#     n=0\n",
    "#     for s in sig_grps:\n",
    "#         t_plt = t_df.loc[t_df.sig_grp == s]\n",
    "#         tmin = t_plt.month.min()\n",
    "#         tmax = t_plt.month.max()\n",
    "#         if tmin==tmax:\n",
    "#             tmax = tmin + pd.DateOffset(days=30)\n",
    "#         ax.fill_between([tmin, tmax], ylim[1], ylim[0], \n",
    "#                          color='blue', edgecolor='none', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d472a-25bc-4212-bdbf-71caa0ba6d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sig_fill(ttest_all, ax):\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    for n in ttest_all.month.unique():\n",
    "        t_df = ttest_all.loc[(ttest_all.month==n)]\n",
    "        if t_df.sig.values[0]==1:\n",
    "            t_min = t_df.month.min()-pd.DateOffset(days=15)\n",
    "            t_max = t_df.month.max()+ pd.DateOffset(days=15) #+pd.DateOffset(months=1)\n",
    "            ax.fill_between([t_min, t_max ], \n",
    "                             ylim[1], ylim[0], \n",
    "                            \n",
    "                            color='blue', edgecolor='none', alpha=0.1) # hatch is too busy\n",
    "# step/interpolate doesn't effect the box location\n",
    "# could also use 'where=y > threshold' which would avoid the need for a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5150f5f-9bab-4db4-b1d5-21344df65474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# # for n, var in enumerate(plt_cols):\n",
    "# var='ET_OUT'\n",
    "# ttest_chk = ttest_all[(ttest_all.month>='2015-10-1')&(ttest_all.month<'2016-10-1')]\n",
    "# # plt_wb(wb.resample('MS').sum(), wb0.resample('MS').sum(), plt_cols, plt_labels, ax)\n",
    "\n",
    "# wb.resample('MS').sum().plot(y=var, ax=ax)\n",
    "# ax.set_xlim(ttest_chk.month.min(), ttest_chk.month.max())\n",
    "\n",
    "# # after plotting the water budget lines the fill between is reset the offset\n",
    "# sig_fill(ttest_chk[ttest_chk.term==var], ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cee48e-a609-4f0f-99df-fe64b3bb1986",
   "metadata": {},
   "source": [
    "# Time series comparison\n",
    "Rather than plotting the significance over the monthly summed data (it's not averaged here). It would make sense to plot the difference of the monthly average values and the standard deviation with the highlighting for significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f032c0-d744-4467-a24a-67580cc68b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wb_rip = pd.read_csv(join(model_ws, 'MF_zonebud_riparian_monthly.csv'))\n",
    "def load_zb_cln(filename, zb_alt):\n",
    "    wb_df = pd.read_csv(filename, parse_dates=['totim'])\n",
    "    wb_df.totim-=pd.DateOffset(1) # fix dates\n",
    "    # select and rename relevant columns\n",
    "    extra_cols = ['FROM_ZONE_0','TO_ZONE_0']\n",
    "    wb_df = wb_df.set_index('totim')[list(zb_alt.keys())+extra_cols].rename(columns=zb_alt)\n",
    "    wb_df['GHB_NET'] = wb_df.FROM_ZONE_0 - wb_df.TO_ZONE_0\n",
    "    # long format\n",
    "    # wb_df_long = wb_df.melt(ignore_index=False)\n",
    "    return(wb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc4a6b3-e20c-4a27-a731-44159e82b9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sum GHB_IN and GHB_OUT, LAK_IN and LAK_OUT to show net effect\n",
    "# SFR is separate because of interest in baseflow\n",
    "# the cumulative change in storage is more intuitive to plot than plain change in storage\n",
    "plt_cols = ['dSTORAGE_sum','LAK_IN', 'ET_OUT','GHB_NET', 'SFR_IN', 'SFR_OUT']\n",
    "plt_labels=['Cumulative\\nStorage Change', 'Floodplain\\nRecharge', 'GW ET',\n",
    "            'Net\\nGW Flow','Stream\\nLosses', 'Stream\\nBaseflow']\n",
    "def plt_wb(wb, wb0, plt_cols, plt_labels, ax, scale=1E-6):\n",
    "    for n, var in enumerate(plt_cols):\n",
    "        wb0[var].multiply(scale).plot(ax=ax[n], label=label_restoration, legend=False)\n",
    "        wb[var].multiply(scale).plot(ax=ax[n], label=label_baseline, legend=False)\n",
    "        ax[n].set_ylabel(plt_labels[n])\n",
    "\n",
    "        ax[n].ticklabel_format(style='plain', axis='y')\n",
    "        ax[n].set_xlabel(None)\n",
    "#         ax[n].set_yscale('log')\n",
    "\n",
    "#     fig.savefig(join(fig_dir, 'monthly_wb_lines.png'), bbox_inches='tight')\n",
    "    \n",
    "# plt_wb(wb.resample('AS-Oct').sum(), wb0.resample('AS-Oct').sum())\n",
    "fig,ax= plt.subplots(len(plt_cols),1, sharex=True,  figsize=(6.5, len(plt_cols)*1),dpi=300)\n",
    "# plt_wb(wb.resample('MS').sum(), wb0.resample('MS').sum(), plt_cols, plt_labels, ax)\n",
    "plt_wb(wb.resample('MS').mean(numeric_only=True), wb0.resample('MS').mean(numeric_only=True),\n",
    "       plt_cols, plt_labels, ax, scale=1E-3)\n",
    "\n",
    "# for n, var in enumerate(plt_cols):\n",
    "#     sig_fill(ttest_all[ttest_all.term==var], ax[n])\n",
    "    # sig_fill(ttest1_all[ttest1_all.term==var], ax[n])\n",
    "\n",
    "fig.legend([label_restoration,label_baseline], ncol=2, loc='outside upper center', bbox_to_anchor=(0.5, 1.05),)\n",
    "# fig.supylabel('Flux (MCM)')\n",
    "fig.supylabel('Flux (thousand $m^3$/day)')\n",
    "fig.tight_layout(h_pad=0.1)\n",
    "\n",
    "# plt.savefig(join(fig_dir, 'wb_monthly_timeseries.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf14f94-0c1b-4ee7-b197-634d14c900f9",
   "metadata": {},
   "source": [
    "### Text reference values\n",
    "The monthly values should provide context for the results so it may be most helpful to present the ranges of differences or values rather than average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78da99f6-1f5b-4b80-9660-d99ae5e8e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wb_range(wb, wb0, plt_cols):\n",
    "    months = [12,1,2,3,4,5]\n",
    "    months = [3,4,5]\n",
    "    wb = wb[wb.index.month.isin(months)]\n",
    "    wb0 = wb0[wb0.index.month.isin(months)]\n",
    "    for n, var in enumerate(plt_cols):\n",
    "        wb_frac = wb0[var]/wb[var]\n",
    "        print(var)\n",
    "        print('Min %.2f' %wb_frac.min(), 'Max %.2f' %wb_frac.max())\n",
    "        print(wb_frac.index.date[wb_frac.argmin()], wb_frac.index.date[wb_frac.argmax()])\n",
    "        \n",
    "# wb_range(wb.resample('MS').sum(), wb0.resample('MS').sum(), plt_cols, )\n",
    "wb_range(wb.resample('MS').mean(numeric_only=True), \n",
    "         wb0.resample('MS').mean(numeric_only=True), plt_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d096b0-0768-4156-8aec-c611638eeca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "dif_lgd = [\n",
    "    # Patch(facecolor='tab:blue', alpha=0.5, label='Reconnected Floodplain'),\n",
    "    Line2D([0], [0],color='black',label='Difference'),\n",
    "    # Line2D([0], [0], color='grey', label='Difference $\\pm 1\\sigma$'),\n",
    "    Patch(facecolor='grey', alpha=0.5, label='Difference $\\pm 1\\sigma$'),\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14366b9-9163-45ee-b68a-1e11fa1565b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a254a4f9-5545-4b42-93d6-e70bb9dd063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 1E-3\n",
    "def plt_wb_diff(wb_diff, plt_cols, plt_labels, ax, color, scale = 1E-3):\n",
    "    for n, var in enumerate(plt_cols):\n",
    "        wb_diff[var].multiply(scale).plot(ax=ax[n], label='Difference', legend=False, color=color)\n",
    "        ax[n].set_ylabel(plt_labels[n])\n",
    "\n",
    "        ax[n].ticklabel_format(style='plain', axis='y')\n",
    "        ax[n].set_xlabel(None)\n",
    "fig,ax= plt.subplots(len(plt_cols),1, sharex=True,  figsize=(6.5, len(plt_cols)*1),dpi=300)\n",
    "\n",
    "\n",
    "wb_diff = wb0.resample('MS').mean(numeric_only=True) - wb.resample('MS').mean(numeric_only=True)\n",
    "wb_std = (wb[plt_cols]-wb0[plt_cols]).resample('MS').std(numeric_only=True)\n",
    "\n",
    "plt_wb_diff(wb_diff, plt_cols, plt_labels, ax, color='black')\n",
    "\n",
    "\n",
    "\n",
    "# plt_wb_diff(wb_diff+wb_std, plt_cols, plt_labels, ax, color='gray')\n",
    "# plt_wb_diff(wb_diff-wb_std, plt_cols, plt_labels, ax, color='gray')\n",
    "for n, var in enumerate(plt_cols):\n",
    "    ax[n].fill_between(wb_diff.index, (wb_diff-wb_std)[var].multiply(scale),\n",
    "                       (wb_diff+wb_std)[var].multiply(scale), color='gray', alpha=0.3)\n",
    "\n",
    "for n, var in enumerate(plt_cols):\n",
    "    # sig_fill(ttest_all[ttest_all.term==var], ax[n])\n",
    "    sig_fill(ttest_all[ttest_all.term==var], ax[n])\n",
    "\n",
    "\n",
    "\n",
    "# fig.legend(['Difference','Std Dev'], ncol=2, loc='outside upper center', bbox_to_anchor=(0.5, 1.05),)\n",
    "fig.legend(handles = dif_lgd, ncol=2, loc='outside upper center', bbox_to_anchor=(0.5, 1.05),)\n",
    "#     ax[0].legend(['No Reconnection',label_baseline], ncol=2)\n",
    "# fig.supylabel('Difference in Mean Flux (MCM)')\n",
    "fig.supylabel('Difference in Flux (thousand $m^3$/day)')\n",
    "fig.tight_layout(h_pad=0.1)\n",
    "\n",
    "# plt.savefig(join(fig_dir, 'wb_monthly_timeseries.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2e6bf7-b2c1-4b7a-9c3f-c5af6880c935",
   "metadata": {},
   "source": [
    "In the riparian zone we see:\n",
    "- even smaller differences in storage change because both are trending upward.\n",
    "- The pattern of GW ET is the same with smaller magnitude\n",
    "- Net GW pattern is more pronounced with greater outflow in winter under baseline\n",
    "In the floodplain:\n",
    "- much clearer difference of floodplain storage in dry years\n",
    "- GW ET has a weird dynamic with no reconnection greater\n",
    "- net gw is more pronounced in winter with much greater baseline outflow  \n",
    "*May not be worth showing these separately*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11244c50-39c6-40d4-a42c-4eaa5ee24468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # riparian vs floodplain (spatial)\n",
    "# wb_rip = load_zb_cln(join(model_ws, 'MF_zonebud_riparian_daily.csv'), zb_alt).assign(scenario='no reconnection')\n",
    "# wb_rip0 = load_zb_cln(join(model_ws0, 'MF_zonebud_riparian_daily.csv'), zb_alt).assign(scenario='baseline')\n",
    "# wb_fp = load_zb_cln(join(model_ws, 'MF_zonebud_floodplain_daily.csv'), zb_alt).assign(scenario='no reconnection')\n",
    "# wb_fp0 = load_zb_cln(join(model_ws0, 'MF_zonebud_floodplain_daily.csv'), zb_alt).assign(scenario='baseline')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f37fad-0158-4f6a-bf8d-8f58ee62428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt_cols = ['dSTORAGE_sum','ET_OUT','GHB_NET']\n",
    "# plt_labels=['Cumulative\\nStorage Change', 'GW ET',\n",
    "#             'Net\\nGW Flow']\n",
    "# plt_wb_diff(wb_rip.resample('MS').sum(), wb_rip0.resample('MS').sum(), plt_cols, plt_labels)\n",
    "# plt_wb_diff(wb_fp.resample('MS').sum(), wb_fp0.resample('MS').sum(), plt_cols, plt_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c8b125-f438-48e8-87eb-2a161d4c7ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # decided it wasn't worth showing, better already with mechanism plot earlier on and differences are harder to see\n",
    "# # coarse vs fine comparison (heterogeneity)\n",
    "# wb_fine = load_zb_cln(join(model_ws, 'MF_zonebud_fine_daily.csv'), zb_alt).assign(scenario='no reconnection')\n",
    "# wb_fine0 = load_zb_cln(join(model_ws0, 'MF_zonebud_fine_daily.csv'), zb_alt).assign(scenario='baseline')\n",
    "# wb_coarse = load_zb_cln(join(model_ws, 'MF_zonebud_coarse_daily.csv'), zb_alt).assign(scenario='no reconnection')\n",
    "# wb_coarse0 = load_zb_cln(join(model_ws0, 'MF_zonebud_coarse_daily.csv'), zb_alt).assign(scenario='baseline')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c071658-5077-416d-943d-dbb3b6d43542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt_cols = ['dSTORAGE_sum','LAK_IN', 'ET_OUT','GHB_NET', 'SFR_IN', 'SFR_OUT']\n",
    "# plt_labels=['Cumulative\\nStorage\\nChange', 'Floodplain\\nRecharge', 'GW ET',\n",
    "#             'Net\\nGW Flow','Stream\\nLosses', 'Stream\\nBaseflow']\n",
    "# fig,ax= plt.subplots(len(plt_cols),2, sharex=True,  figsize=(6.5, len(plt_cols)*1),dpi=300, sharey='row')\n",
    "\n",
    "# # fig.legend([label_baseline,'No Reconnection'], ncol=2, loc='outside upper center', bbox_to_anchor=(0.5, 1.05),)\n",
    "# #     ax[0].legend(['No Reconnection',label_baseline], ncol=2)\n",
    "\n",
    "\n",
    "# plt_wb_diff(wb_fine.resample('MS').sum(), wb_fine0.resample('MS').sum(), plt_cols, plt_labels, ax=ax[:,0])\n",
    "\n",
    "# plt_wb_diff(wb_coarse.resample('MS').sum(), wb_coarse0.resample('MS').sum(), plt_cols, plt_labels, ax=ax[:,1])\n",
    "\n",
    "# fig.supylabel('Flux (MCM)')\n",
    "# fig.tight_layout(h_pad=0.1)\n",
    "# ax[0,0].set_title('Fine')\n",
    "# ax[0,1].set_title('Coarse')\n",
    "\n",
    "# plt.savefig(join(fig_dir, 'wb_monthly_by_facies_time_series.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b1a8e0-abad-4b1f-aa8f-5a350bafaf4c",
   "metadata": {},
   "source": [
    "Between coarse and fine there are some interesting patterns such as the Baseflow is nearly all from the fines, the stream recharge in coarse doesn't change much but in fines it is reduced as the mounding remains in place for longer periods of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722f4f5a",
   "metadata": {},
   "source": [
    "# SFR Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7814367c-8568-485a-a77c-e334ddb66322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mf_utility\n",
    "from importlib import reload\n",
    "reload(mf_utility)\n",
    "from mf_utility import clean_sfr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6265c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check the no_reconnection has updated fully\n",
    "\n",
    "sfrdf =  clean_sfr_df(model_ws, dt_ref, pd_sfr, name='MF')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9034a9-f1e5-497e-b01d-bceee2511dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_sfr.groupby('facies').count()['vka']/grid_sfr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca930a2-6da0-4013-86fb-4b5f527e13cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfrdf_full =  clean_sfr_df(model_ws, dt_ref, name='MF')\n",
    "sfrdf0_full =  clean_sfr_df(model_ws0, dt_ref,name='MF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17de4c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sfrdf0=  clean_sfr_df(model_ws0, dt_ref, pd_sfr, name='MF')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7858e711-3220-4c8c-8bf5-7b50d7587fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # troubleshooting\n",
    "# # # review of stream stage and slope\n",
    "# plt_date = '2017-6-1'\n",
    "# fig,ax = plt.subplots(5,1,figsize=(6.5,6), sharex=True)\n",
    "# sfrdf0.loc[plt_date].plot(x='Total distance (m)', y='Qin', ax=ax[0])\n",
    "\n",
    "# sfrdf0.loc[plt_date].plot(x='Total distance (m)', y=['stage', 'strtop'], ax=ax[1])\n",
    "# sfrdf0.loc[plt_date].plot(x='Total distance (m)', y='depth', ax=ax[2])\n",
    "# sfrdf0.loc[plt_date].plot(x='Total distance (m)', y='slope', ax=ax[3])\n",
    "# ax[3].set_ylabel('Bed\\nslope')\n",
    "# ax[4].plot(sfrdf0.loc[plt_date]['Total distance (m)'], sfrdf0.loc[plt_date].stage.diff().bfill().multiply(-1/100))\n",
    "# ax[4].set_ylabel('Friction\\nslope')\n",
    "# ax[4].set_ylim(-0.01, 0.01)\n",
    "# # ax.set_aspect(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0208871d",
   "metadata": {},
   "source": [
    "The mean, median, min depth across reaches doesn't help show a significant change except that the baseline has slightly lower peaks.  \n",
    "\n",
    "The segments with flow is odd because right now it might include the segments that need to be dropped.\n",
    "\n",
    "*The days with flow doesn't greatly change between scenarios so not worth showing*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83da1b2a-76c6-4253-afb2-950a420717c0",
   "metadata": {},
   "source": [
    "The plot of streamflow at the outlet is better visualized for differences without log scale, unless only plotting the summertime flows. Streamflow at the outlet isn't that different, but upstream near the floodplain it is more clearly different.\n",
    "\n",
    "- one can clearly see peak flow reductions along the floodplain and the shift in summer flows is small but perhaps that small shift is more important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f7f39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't use seven day roling average because this smooths out some key points\n",
    "plt_dates = pd.date_range('2019-1-1','2019-11-30')\n",
    "plt_dates = pd.date_range('2017-7-1','2017-8-30')\n",
    "# plt_dates = pd.date_range('2014-10-1','2020-9-30')\n",
    "\n",
    "fig, ax = plt.subplots( figsize=(6.5,3),dpi=300, sharex=True)\n",
    "seg_plt = (sfrdf['Total distance (m)']==sfrdf['Total distance (m)'].max())\n",
    "# seg_plt = (sfrdf['Total distance (m)']==sfrdf['Total distance (m)'].median())\n",
    "# seg_plt = (sfrdf.segment==sfrdf.segment.median())\n",
    "# seg_plt = (sfrdf.segment==33)\n",
    "\n",
    "sfrdf0[seg_plt].loc[plt_dates].plot(y='Qout', ax=ax, label=label_restoration,linewidth=0.5)\n",
    "sfrdf[seg_plt].loc[plt_dates].plot(y='Qout', ax=ax, label=label_baseline,linewidth=0.5)\n",
    "# sfrdf[sfrdf.segment==1].loc[plt_dates].plot(y='Qin', ax=ax, label='Inflow', linewidth=0.5)\n",
    "\n",
    "\n",
    "# plt.yscale('log')\n",
    "ax.set_ylabel('Outlet Streamflow\\n($m^3/day$)')\n",
    "plt.xlabel('Date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd31bc71-30cc-4fb3-9456-ca11806662e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_day_of_flow(sfrdf0, seg_plt, plt_dates):\n",
    "    return sfrdf0[seg_plt].loc[plt_dates][sfrdf0[seg_plt].loc[plt_dates].Qin>0].index.max()\n",
    "\n",
    "last_day_of_flow(sfrdf0, seg_plt, plt_dates), last_day_of_flow(sfrdf, seg_plt, plt_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bdf0f7-5b87-45e4-9d1a-2d73bbf759ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## plot hydrographs by water year\n",
    "# ## still hard to see a shift\n",
    "# fig, axes = plt.subplots(6,1, figsize=(6.5,6.5),dpi=300, sharex=True)\n",
    "# seg_plt = (sfrdf['Total distance (m)']==sfrdf['Total distance (m)'].max())\n",
    "# # seg_plt = (sfrdf.segment==sfrdf.segment.median())\n",
    "# # seg_plt = (sfrdf.segment==33)\n",
    "# for n, y in enumerate(np.arange(2015,2021)):\n",
    "#     ax = axes[n]\n",
    "#     plt_dates = pd.date_range(str(y)+'-1-1', str(y)+'-6-1')\n",
    "#     # plt_dates = pd.date_range(str(y)+'-5-1', str(y)+'-9-30')\n",
    "#     # plt_dates = plt_dates[plt_dates.isin(sfrdf.index)]\n",
    "#     ax.plot(sfrdf0[seg_plt].loc[plt_dates].Qout.values, label=label_restoration,linewidth=0.5)\n",
    "#     ax.plot(sfrdf[seg_plt].loc[plt_dates].Qout.values, label=label_baseline,linewidth=0.5)\n",
    "#     ax.set_yscale('log')\n",
    "#     ax.set_ylabel(y)\n",
    "#     # ax.set_xticks(None)\n",
    "\n",
    "# # sfrdf[sfrdf.segment==1].loc[plt_dates].plot(y='Qin', ax=ax, label='Inflow', linewidth=0.5)\n",
    "\n",
    "# axes[0].legend()\n",
    "# fig.supylabel('Outlet Streamflow ($m^3/day$)')\n",
    "# fig.tight_layout()\n",
    "# plt.xlabel('Date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f61f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## the days with flow doesn't seem to provide anything distinctly new\n",
    "# # the difference is only noticeable at the outlet, not upstream where it doesn't go dry\n",
    "# fig,ax=plt.subplots(figsize=(6.5,6))\n",
    "# seg_plt = (sfrdf['Total distance (m)']==sfrdf['Total distance (m)'].max())\n",
    "# freq = 'MS'\n",
    "# freq='AS-Oct'\n",
    "# flow_yr = sfrdf0[seg_plt].resample(freq).sum(numeric_only=True)\n",
    "# ax.bar(np.arange(0, len(flow_yr)), flow_yr.flowing, label=label_restoration,alpha=1, color='tab:blue')\n",
    "\n",
    "# flow_yr = sfrdf[seg_plt].resample(freq).sum(numeric_only=True)\n",
    "# ax.bar(np.arange(0, len(flow_yr)), flow_yr.flowing, label=label_baseline, color='tab:orange')\n",
    "# plt.ylabel('Days with flow')\n",
    "# plt.xlabel('Date')\n",
    "# plt.xticks(np.arange(0, len(flow_yr)), flow_yr.index.year);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6123de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_all = pd.DataFrame()\n",
    "for freq in ['annual','monthly']:\n",
    "    for s in ['Winter','Spring','Summer','Fall']:\n",
    "        # streamflow isn't relevant to facies really\n",
    "        t_df = run_stats(sfrdf[sfrdf.segment==sfrdf.segment.max()], \n",
    "                 sfrdf0[sfrdf.segment==sfrdf.segment.max()], 'Qout', freq=freq, season=s)\n",
    "        ttest_all = pd.concat((ttest_all, t_df))\n",
    "        for f in ['Gravel','Sand','Sandy Mud','Mud']:\n",
    "            t_df = run_stats(sfrdf[sfrdf.facies==f].groupby('dt').sum(numeric_only=True), \n",
    "                      sfrdf0[sfrdf0.facies==f].groupby('dt').sum(numeric_only=True), 'Qrech', freq=freq, season=s)\n",
    "            ttest_all = pd.concat((ttest_all, t_df.assign(facies=f)))\n",
    "# only flow from the last segment is compared (cumulative impact)\n",
    "\n",
    "# ttest_all.columns=['z_stat','pvalue','term','freq']\n",
    "sfr_name_dict = {'Qout':'Outlet Streamflow', 'Qrech':'Stream Losses'}\n",
    "# replace term with clean name\n",
    "ttest_all['term'] = [sfr_name_dict[t] for t in ttest_all.term]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3894f36f",
   "metadata": {},
   "source": [
    "Need to decide if statistics should be based on comparing daily data for streamflow or monthly. Why should I use monthly instead of daily?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7d5c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_out = ttest_all[['freq','term','season','facies','statistic','pvalue', 'perc_diff_in_means']]\n",
    "\n",
    "ttest_flow = ttest_out[ttest_out.term=='Outlet Streamflow'].drop(columns='facies')\n",
    "ttest_flow[ttest_flow.freq=='monthly'].drop(columns=['freq']).to_csv(join(ttest_dir, 'flow_monthly.csv'), index=False)\n",
    "ttest_flow[ttest_flow.freq=='annual'].drop(columns=['freq']).to_csv(join(ttest_dir, 'flow_annual.csv'), index=False)\n",
    "\n",
    "ttest_facies = ttest_out[~ttest_out.facies.isna()]\n",
    "ttest_facies[ttest_facies.freq=='monthly'].drop(columns=['freq']).to_csv(join(ttest_dir, 'facies_monthly.csv'), index=False)\n",
    "ttest_facies[ttest_facies.freq=='annual'].drop(columns=['freq']).to_csv(join(ttest_dir, 'facies_annual.csv'), index=False)\n",
    "\n",
    "# ttest_facies[ttest_facies.freq=='monthly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfbbfe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63d97d1c-bb9f-49f6-bb17-6af25696ca86",
   "metadata": {},
   "source": [
    "## Flow duration curve\n",
    "Helen suggested plotting the flow duration curves as a way of looking at how probability changes for flow events.\n",
    "\n",
    "The OSU Streamflow guide and USGS method suggest grouping a streamflow record into 20-30 bins and then to count the flow events that fall within each bin to create the flow-duration curve\n",
    "\n",
    "- the flow duration curve shows and increase in the exceedance of smaller flow events (1E1 - 1E4 m3/d) and an increase in high flow events, 1E6-1E8. the increase in low flows is due to lowered summer seepage while the increase in high flow events is because the lake builds up storage which in MODFLOW results in higher peak flows which isn't realistic since it's not a 2D hydraulic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904e4c74-32b0-4241-9ec8-f4ec7fe6ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes, zoomed_inset_axes, mark_inset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71548833-883a-4a5b-9890-188168b8ba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "## inset map\n",
    "def inset_plot(ax, loc, width = '35%', height='35%'):\n",
    "    ax_inset = inset_axes(ax, width=width, height=height, loc=loc,\n",
    "                       bbox_to_anchor=(.0, .0, 1, 1), bbox_transform=ax.transAxes, \n",
    "                      )\n",
    "    return(ax_inset)\n",
    "def clean_inset(ax_inset):\n",
    "    ax_inset.tick_params(labelleft=False, labelbottom=False, left = False, bottom = False)\n",
    "    ax_inset.set_xlabel(None)\n",
    "    ax_inset.set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9df4f6-852f-4ba4-8daf-9a100269ca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ax_lab(xy, text, ax, offset = (0,0), lw=1, fontsize=10, bbox=True, fc='white', ec='black'):\n",
    "    if bbox:\n",
    "        ax.annotate(text=text, xy=xy, xycoords='axes fraction', ha='center', va = 'bottom', xytext = offset, \n",
    "                    textcoords='offset pixels', fontsize = fontsize, \n",
    "            bbox=dict(boxstyle=\"square,pad=0.3\", fc=\"lightgrey\", ec=\"black\", lw=lw))\n",
    "    else:\n",
    "        ax.annotate(text=text, xy=xy, xycoords='axes fraction', ha='center', va = 'bottom', xytext = offset, \n",
    "                                        textcoords='offset pixels', fontsize = fontsize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3a0e26-4661-46e7-baaa-009d76502974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow_curve(sfrdf):\n",
    "    sfr_last = sfrdf[sfrdf.segment==sfrdf.segment.max()].copy()\n",
    "    sfr_last_sort = sfr_last['Qout'].copy().sort_values(ascending=False).reset_index()\n",
    "    sfr_last_sort['P'] = np.arange(0, sfr_last_sort.shape[0])/(sfr_last_sort.shape[0]+1)\n",
    "\n",
    "    return(sfr_last_sort)\n",
    "    \n",
    "wy = [2017]\n",
    "wy = sfrdf0.WY.unique()\n",
    "fig, ax = plt.subplots(figsize=(6.5,4),dpi=300)\n",
    "\n",
    "def plt_flow_duration(sfrdf0, sfrdf, wy, ax, xlim = [0,1], linestyle='-'):\n",
    "    sfr_last_sort0 = flow_curve(sfrdf0.loc[sfrdf0.WY.isin(wy)])\n",
    "    sfr_last_sort0 = sfr_last_sort0[(sfr_last_sort0.P>xlim[0])&(sfr_last_sort0.P<xlim[1])]\n",
    "\n",
    "    sfr_last_sort0.plot(x='P',y='Qout', ax=ax, label=label_restoration, \n",
    "                        color='tab:blue', linestyle=linestyle, legend=False)\n",
    "    \n",
    "    sfr_last_sort = flow_curve(sfrdf.loc[sfrdf.WY.isin(wy)])\n",
    "    # subset to desired x_range\n",
    "    sfr_last_sort = sfr_last_sort[(sfr_last_sort.P>xlim[0])&(sfr_last_sort.P<xlim[1])]\n",
    "    sfr_last_sort.plot(x='P',y='Qout', ax=ax, label=label_baseline, \n",
    "                       color='tab:orange', linestyle=linestyle, legend=False)\n",
    "    return(sfr_last_sort, sfr_last_sort0)\n",
    "    \n",
    "\n",
    "sfr_last_sort, sfr_last_sort0 = plt_flow_duration(sfrdf0, sfrdf, wy, ax)\n",
    "# yoff = 9E3\n",
    "# for wy in np.arange(2015,2021):\n",
    "#     sfr_last_sort = plt_flow_duration(sfrdf0, sfrdf, [wy], ax)\n",
    "#     no_flow = sfr_last_sort[sfr_last_sort.Qout!=0].iloc[-1].P\n",
    "#     yoff -= 1.5E3\n",
    "#     xy_lab((no_flow , 1E3+yoff), str(wy), ax, offset = (0,1), fontsize=10, bbox=True)\n",
    "\n",
    "ax.set_yscale('log')\n",
    "# ax.set_xlim(.7,.8)\n",
    "# ax.set_xlim(0, .7)\n",
    "# ax.set_xlim(0.55,0.65)\n",
    "ax.set_ylabel('Discharge ($m^3/d$)')\n",
    "ax.set_xlabel('Exceedance Probability')\n",
    "# plt_flow_duration(sfrdf0, sfrdf, [2015], ax)\n",
    "fig.legend([label_restoration,label_baseline], ncol=2, loc='outside upper center', bbox_to_anchor=(0.5, 1.0),)\n",
    "\n",
    "### inset plots ###\n",
    "axins = inset_plot(ax, 'upper right', width='15%', height='50%')\n",
    "axins.set_facecolor((0.5, 0.5, 0.5, 0.2))\n",
    "sfr_last_sort, sfr_last_sort0 = plt_flow_duration(sfrdf0, sfrdf, wy, axins, xlim=[0.55, 0.65])\n",
    "mark_inset(ax, axins, loc1=2, loc2=4, fc=\"gray\", alpha=0.2, ec=\"black\")\n",
    "axins.set_yticks([1E4],labels=[''], minor=True)\n",
    "clean_inset(axins)\n",
    "# ax_lab((0.7, 0.6), 'Low Flow\\nInset', axins, offset = (0,0), fontsize=10, bbox=False)\n",
    "\n",
    "axins1 = inset_plot(ax, 'lower left', height='25%')\n",
    "axins1.set_facecolor((0.5, 0.5, 0.5, 0.2))\n",
    "sfr_last_sort, sfr_last_sort0 = plt_flow_duration(sfrdf0, sfrdf, wy, axins1, xlim=[0.01, 0.15])\n",
    "mark_inset(ax, axins1, loc1=2, loc2=1, fc=\"gray\", alpha=0.2, ec=\"black\")\n",
    "clean_inset(axins1)\n",
    "axins1.set_yticks([1E7],labels=[''], minor=True)\n",
    "# ax_lab((0.7, 0.6), 'High Flow\\nInset', axins1, offset = (0,0), fontsize=10, bbox=False)\n",
    "\n",
    "# axins1.tick_params(labelleft=False, labelbottom=False, left = False, bottom = False)\n",
    "# axins1.set_xticks([0.05, 0.1])\n",
    "\n",
    "# switching from daily to monthly streamflow kept the shift upward in discharge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4941fa10-d32b-43e4-bde3-a9aa30c3d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_last_sort0 = flow_curve(sfrdf0.loc[sfrdf0.WY.isin(wy)])\n",
    "sfr_last_sort = flow_curve(sfrdf.loc[sfrdf.WY.isin(wy)])\n",
    "\n",
    "# identify shift in probably of exceeding no flow\n",
    "no_flow = sfr_last_sort[sfr_last_sort.Qout!=0].iloc[-1].P*100\n",
    "no_flow0 = sfr_last_sort0[sfr_last_sort0.Qout!=0].iloc[-1].P*100\n",
    "print('Probably of exceeding no flow is %.2f for the baseline' %no_flow)\n",
    "print('Probably of exceeding no flow is %.2f for the levee removal' %no_flow0)\n",
    "high_flow = sfr_last_sort[(sfr_last_sort.P>0.05)&(sfr_last_sort.P<0.1)]\n",
    "high_flow0 = sfr_last_sort0[(sfr_last_sort0.P>0.05)&(sfr_last_sort0.P<0.1)]\n",
    "hf_red = ((high_flow.Qout/high_flow0.Qout).mean()-1)*100\n",
    "print('High flows 0.05 to 0.1 reduced by %.2f %%' %hf_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab88366-1a04-4089-babd-9d882990d495",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def flow_curve_cat(sfrdf):\n",
    "#     sfr_last = sfrdf[sfrdf.segment==sfrdf.segment.max()].copy()\n",
    "#     # categorized - OSU plotted the exceedance at the upper end, linear\n",
    "#     flow_group = np.linspace(sfr_last.Qin.min(), sfr_last.Qin.max(), 30)\n",
    "#     # logarithmic\n",
    "#     Qmin = np.max((0, np.log10(sfr_last.Qin.min())))\n",
    "#     Qmax = np.log(sfr_last.Qin.max())\n",
    "#     flow_group = np.exp(np.linspace(Qmin, Qmax, 30))\n",
    "#     flow_group[0] = Qmin # reset 0\n",
    "\n",
    "#     sfr_last['flow_group'] = 0\n",
    "#     for n in np.arange(0,len(flow_group)-1):\n",
    "#         sfr_last.loc[(sfr_last.Qin>flow_group[n])&(sfr_last.Qin<flow_group[n+1]), 'flow_group'] = flow_group[n+1]\n",
    "#     # sfr_last['flow_count'] = sfr_last.groupby('flow_group').count()['Qin']\n",
    "#     sfr_last_sort = sfr_last.groupby('flow_group').count()['Qout'].reset_index()\n",
    "#     sfr_last_sort = sfr_last_sort.sort_values('flow_group', ascending=False)\n",
    "#     sfr_last_sort['cum_flow_count'] = sfr_last_sort.Qout.cumsum()\n",
    "#     sfr_last_sort['P'] = sfr_last_sort.cum_flow_count/sfr_last_sort.Qout.sum()\n",
    "#     return(sfr_last_sort)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# flow_curve_cat(sfrdf0).plot(x='P',y='flow_group', ax=ax, label=label_restoration)\n",
    "# flow_curve_cat(sfrdf).plot(x='P',y='flow_group', ax=ax, label=label_baseline)\n",
    "# plt.yscale('log')\n",
    "\n",
    "# # the monthly plots show the same result as the daily so it's not really needed to characterize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e155475-96b2-4dfe-a766-074b58aeba16",
   "metadata": {},
   "source": [
    "### Compare water budget fluxes to cumulative discharge\n",
    "Monthly scatter plots (annual shows similar result but more simplified)\n",
    "- For floodplain recharge there is logarithmic trend in both scenarios as at some point greater discharge doesn't increase recharge. There is a noticeable offset in trends.\n",
    "- the trend is similar for stream recharge but the difference is much less noticeable. big difference in trend for baseflow\n",
    "- ET increases logarithmicaly as well with most differences at larger flows\n",
    "- cumulative storage doesn't have a clear pattern but has increases, similar with GHB_NET where there is more outflow but no trend with discharge\n",
    "- the discharge at the inlet vs outlet suggests that there is an increase in peak flows with restoration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73006cab-32e9-417b-b519-7cd47a66eb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cumulative discharge in is equal but in case we change segments it will be different\n",
    "rs = 'MS' #'AS'\n",
    "sfr_sum = sfrdf[sfrdf.segment==1].resample(rs).sum()\n",
    "sfr0_sum = sfrdf0[sfrdf0.segment==1].resample(rs).sum()\n",
    "\n",
    "sfr_last_sum = sfrdf[sfrdf.segment==sfrdf.segment.max()].resample(rs).sum()\n",
    "sfr0_last_sum = sfrdf0[sfrdf0.segment==sfrdf.segment.max()].resample(rs).sum()\n",
    "\n",
    "var = 'ET_OUT'\n",
    "# var = 'LAK_IN'\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter(sfr_sum.Qin, wb0.resample(rs).sum()[var], label=label_restoration)\n",
    "ax.scatter(sfr0_sum.Qin, wb.resample(rs).sum()[var], label=label_baseline)\n",
    "ax.set_ylabel(var+' Monthly Flux ($m^3/month$)')\n",
    "\n",
    "# plt.scatter(sfr0_sum.Qin, sfr0_last_sum.Qout, label=label_restoration)\n",
    "# plt.scatter(sfr_sum.Qin, sfr_last_sum.Qout, label=label_baseline)\n",
    "# plt.ylabel('Monthly Discharge out ($m^3/month$)')\n",
    "# # the flow in vs flow out shows that restoration increases peak outflows\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel('Monthly Discharge in ($m^3/month$)')\n",
    "# ax.set_xscale('log')\n",
    "# ax.set_yscale('log')\n",
    "# maybe the flow fraction into the floodplain is overestimated \n",
    "# as less flow could go onto the floodplain and the benefits would remain since only 1% is recharged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8dd702-9ba9-4016-8135-2608a0aed572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c075134a-a8e3-4030-9c95-b4659653033d",
   "metadata": {},
   "source": [
    "# Spatial comparison of seepage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b99435-7b8e-4be4-a01d-bf1d8a8fb41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vka = m.upw.vka.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7b1536-3864-4cad-92dd-9e8a866afcbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f288313-a067-486f-b1aa-f98417e90ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdobj = flopy.utils.HeadFile(model_ws+'/MF.hds')\n",
    "hdobj0 = flopy.utils.HeadFile(model_ws0+'/MF.hds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cddeff-cc3c-47cd-88b9-6e219905c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poorly formed function - lots of indirect call\n",
    "def plot_vka(vka, ax, sfr_nodata, k_max):\n",
    "    sfr_hk = vka[:k_max][:, sfr_rows, sfr_cols]\n",
    "    sfr_hk = np.ma.masked_where(sfr_nodata[:k_max], sfr_hk)\n",
    "    im = ax.imshow(sfr_hk, norm = mpl.colors.LogNorm(vmin=vmin, vmax=vmax), \n",
    "                   aspect='auto', cmap='viridis')\n",
    "    # plt.xticks([]);\n",
    "    ax.set_yticks(ticks = np.arange(1,k_max,5), labels=m.dis.botm.array[:,0,0][:k_max:5]);\n",
    "    ax.set_xticks(ticks = np.arange(0, len(plt_segs),10), labels=np.arange(0, len(plt_segs),10), rotation=90)\n",
    "    return sfr_hk, im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e6efcb-4118-4c7b-bf7c-182795056c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_segs = sfrdf.segment.unique()\n",
    "plt_segs = sfrdf['Total distance (m)'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c874e71-1414-4668-a8b9-3d50e6390fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_in_reach\n",
    "# grid_sfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b240b029-3b13-439b-b378-69472a48386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sfr_hk_plt = grid_sfr[~grid_sfr.iseg.isin(drop_iseg)]\n",
    "sfr_hk_plt = grid_sfr.copy()\n",
    "vmin = sfr_hk_plt.vka.min()\n",
    "vmax = sfr_hk_plt.vka.max()\n",
    "\n",
    "sfr_seg = sfr_hk_plt.drop_duplicates('node')\n",
    "sfr_rows = sfr_seg.i.values\n",
    "sfr_cols = sfr_seg.j.values\n",
    "sfr_lays = sfr_seg.k.values\n",
    "\n",
    "k_max = int(sfr_hk_plt.k.max())\n",
    "k_max = m.dis.nlay-1\n",
    "\n",
    "# define by active cells (new: don't include bottom 10, original: don't include bottom)\n",
    "sfr_ibound = ~m.bas6.ibound.array[:-1, sfr_rows, sfr_cols].astype(bool)\n",
    "# identify where data should be removed becaues it's above land\n",
    "sfr_nodata = np.zeros((k_max, len(sfr_lays)), dtype=bool)\n",
    "for n in np.arange(0,len(sfr_lays)):    \n",
    "    sfr_nodata[:sfr_lays[n], n] = True\n",
    "\n",
    "# plot only data below ground\n",
    "fig, ax = plt.subplots(figsize=(6.5,2))\n",
    "k_max = 15 # around -10m\n",
    "k_max = 10\n",
    "sfr_hk, im = plot_vka(vka, ax, sfr_nodata, k_max = k_max)\n",
    "def fp_label(ax, y, xoff):\n",
    "    ax.axvline(fp_in_reach+1)\n",
    "    ax.axvline(fp_out_reach+1)\n",
    "    ax.axvline(lak_up+1)\n",
    "    xy_lab((fp_in_reach + xoff, y), 'Floodplain\\nInflow', ax, offset = (0,5), fontsize=10, bbox=False)\n",
    "    xy_lab((fp_out_reach + xoff, y), 'Floodplain\\n    Outflow+End', ax, offset = (0,5), fontsize=10, bbox=False)\n",
    "    xy_lab((lak_up + xoff, y), 'Floodplain\\nStart', ax, offset = (0,5), fontsize=10, bbox=False)\n",
    "fp_label(ax=ax, y=3, xoff=10)\n",
    "\n",
    "\n",
    "fig.supylabel('Layer')\n",
    "# cbar_ax=ax.ravel().tolist()\n",
    "fig.colorbar(im, ax=ax, orientation='vertical', label='$K_{vert}$\\n($m/day$)', shrink=1, location='right')  \n",
    "# cax = ax.inset_axes([0.6, 0.7, 0.1, 0.04])\n",
    "# fig.colorbar(im, ax=cax, orientation='vertical', label='$K_{vert}$\\n($m/day$)', shrink=1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca6fee-2b37-4f3a-abf2-6cdf5795a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_dates = pd.date_range('2017-1-1','2017-5-30')\n",
    "# plt_dates = pd.date_range('2017-6-1','2017-9-30')\n",
    "\n",
    "def sfr_load_hds(hdobj, plt_dates):\n",
    "    # runs pretty quickly with hdobj.get_data\n",
    "    sfr_heads = np.zeros((len(plt_dates), len(plt_segs)))\n",
    "    avg_heads = np.zeros((len(plt_dates), len(plt_segs)))\n",
    "    for n, plt_date in enumerate(plt_dates):\n",
    "        spd = dt_ref.loc[dt_ref.dt==plt_date, 'kstpkper'].values[0]\n",
    "    \n",
    "        head = hdobj.get_data(spd)\n",
    "        head = np.ma.masked_where(head ==-999.99, head)\n",
    "        sfr_heads[n,:] = head[sfr_lays, sfr_rows, sfr_cols]\n",
    "        # pull head for top 10 layers to compare\n",
    "        avg_heads[n,:] = np.mean(head[:10, sfr_rows, sfr_cols], axis=0)\n",
    "    return(sfr_heads, avg_heads)\n",
    "\n",
    "sfr_heads, avg_heads = sfr_load_hds(hdobj, plt_dates)\n",
    "sfr_heads0, avg_heads0 = sfr_load_hds(hdobj0, plt_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006fa724-622c-4997-ba12-03902370dcfb",
   "metadata": {},
   "source": [
    "The idea that baseflow may be driven more by the decrease in stream stage than the increase in groundwater level is interesting in itself because it is perhaps revealing a natural way that alternate flow paths develop. We can caveat if needed that there is uncertainty in the flow in the channel but either way there would be a signficant flow path from the floodplain to the stream (could look at floodplain loggers vs stream loggers stage or flood rasters from Whipple).   \n",
    "- One way to clarify the baseflow due to stage vs groundwater would be to look at days when the stage is equal between scenarios (e.g., >71.6 cms) then look at how groundwater differs.\n",
    "- The idea of floodplain filtering is interesting as well, but I would need justification for the removal of contaminants or the addition of helpful things like the primary production. MODPATH would give us the residence time of floodplain recharge and the fate more precisely than water budgets, but I'm not sure we can directly link it to quality improvements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a124707-08be-400a-aa63-31bb5f10d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "profile_legend_elements = [\n",
    "    # Patch(facecolor='tab:blue', alpha=0.5, label='Floodplain'),\n",
    "    Line2D([0], [0], color='tab:blue',  linestyle='-', label=label_restoration),\n",
    "    Line2D([0], [0],color='tab:orange',label=label_baseline),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcff564-0c8f-40d7-94bf-639197b12068",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_xticks = np.append(0,np.arange(9,80,10)) # sfr segments to label\n",
    "# plt_xticks = np.sort(np.unique(np.append(plt_xticks, (fp_in_reach, fp_out_reach))))\n",
    "# plt_xticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e79bdd1-6e6d-48a3-a1f4-199dffec93d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baada972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# averaged across all time we see baseflow only in the floodplain, but within that it is variable\n",
    "# and seeapage shows hotter spots\n",
    "fig,ax = plt.subplots(6,1,sharex=True, sharey=False, layout='constrained', dpi=300,\n",
    "                      figsize=(8, 6.5),\n",
    "                      # gridspec_kw={'height_ratios':(3,2, 2,2, 2, 2)}\n",
    "                     )\n",
    "\n",
    "sfr_hk, im = plot_vka(vka, ax[0], sfr_nodata, k_max=10)\n",
    "fp_label(ax=ax[0], y=4, xoff=6)\n",
    "\n",
    "ax[0].set_ylabel('Layer\\nElevation\\n(m AMSL)')\n",
    "# grid_sfr.reset_index().vka.plot(ax=ax[1], color='black')\n",
    "# ax[1].set_ylabel('Stream\\nVKA (m/d)')\n",
    "\n",
    "# grp_col = 'segment'\n",
    "grp_col = 'Total distance (m)'\n",
    "\n",
    "def plt_profile(sfrdf, plt_dates, ax, color):\n",
    "    df_mean = sfrdf.loc[plt_dates].groupby(grp_col).mean(numeric_only=True).reset_index()\n",
    "    df_mean.plot(y='Qrech', ax=ax[-2], legend=False, color=color)\n",
    "    ax[-2].set_ylabel('Losses\\n($m^3/day$)')\n",
    "    ax[-2].set_yscale('log')\n",
    "    df_mean.plot(y='Qbase', ax=ax[-1], legend=False, color=color)\n",
    "    ax[-1].set_ylabel('Baseflow\\n($m^3/day$)')\n",
    "    ax[-1].set_yscale('log')\n",
    "    ax[-3].axhline(y=0, color='black', alpha=0.5) # show transition from gaining to losing\n",
    "    df_mean.plot(y='gradient', ax=ax[-3], legend=False, color=color)\n",
    "    ax[-3].set_ylabel('Vertical\\nGradient')\n",
    "    return(df_mean)\n",
    "\n",
    "# plt_dates = pd.date_range('2017-1-1','2017-5-30')\n",
    "# plt_dates = pd.date_range('2017-3-1','2017-6-30')\n",
    "# plt_dates = pd.date_range('2017-3-1','2017-6-30')\n",
    "# plt_dates = '2017-1-16'\n",
    "sfrdf_mean0 = plt_profile(sfrdf0, plt_dates, ax, color='tab:blue')\n",
    "sfrdf_mean = plt_profile(sfrdf, plt_dates, ax, color='tab:orange')\n",
    "\n",
    "ax[1].plot(sfrdf0.loc[plt_dates].groupby(grp_col).mean(numeric_only=True).stage.values)\n",
    "ax[1].plot(sfrdf.loc[plt_dates].groupby(grp_col).mean(numeric_only=True).stage.values)\n",
    "ax[1].set_ylabel('Stream\\nStage\\n(m)')\n",
    "\n",
    "ax[2].plot(sfr_heads0.mean(axis=0))\n",
    "ax[2].plot(sfr_heads.mean(axis=0))\n",
    "ax[2].set_ylabel('GW\\nElevation\\n(m)')\n",
    "\n",
    "fig.tight_layout(h_pad=0.1)\n",
    "\n",
    "# fig.legend(handles=profile_legend_elements, loc='center', bbox_to_anchor=[0.3, 0.99], ncol=1)\n",
    "ax[1].legend(handles=profile_legend_elements, loc='best',  ncol=1)\n",
    "# fig.colorbar(im, orientation = 'horizontal', location='top', label='$K_{vert}$ ($m/day$)', shrink=0.3)\n",
    "cax = ax[0].inset_axes([0.75, 0.9, 0.2, 0.075])\n",
    "fig.colorbar(im, cax=cax, orientation = 'horizontal', label='$K_{vert}$ ($m/day$)', shrink=0.4)\n",
    "plt.xlabel('Stream Reach')\n",
    "plt.xticks(plt_xticks, plt_xticks+1);\n",
    "plt.xticks([],minor=True);\n",
    "plt.savefig(join(fig_dir, 'longitudinal_profile_stream_aquifer.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844fed71-3f88-4c37-8be5-45ece51a6553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of vertical hydraulic conductivity vs baseflow/gradient to see if there is any relationship\n",
    "sfr_hk_top = sfr_hk[sfr_lays, np.arange(0, len(sfr_lays))]\n",
    "\n",
    "# plt.scatter(sfr_hk_top, sfrdf_mean0.gradient)\n",
    "fig,ax = plt.subplots(1,3, figsize=(6,2), dpi=300)\n",
    "for n, var in enumerate(['gradient', 'Qbase', 'Qrech']):\n",
    "    ax[n].scatter(sfr_hk_top, sfrdf_mean0[var])\n",
    "    ax[n].set_title(var)\n",
    "# there appears to be no relationship to gradient, Qbase and a slight relationship with Qbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108f2856-e505-413d-ac05-c67149017cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify where the shift happens spatially\n",
    "sfrdf_mean.gradient>sfrdf_mean0.gradient\n",
    "var = 'Qrech'\n",
    "var = 'gradient'\n",
    "# var='Qbase'\n",
    "red_rch = 100*(1-(sfrdf_mean0[var]/sfrdf_mean[var]))\n",
    "plt_rch = np.arange(30,50)\n",
    "red_rch.plot()\n",
    "red_rch.iloc[plt_rch].mean()\n",
    "for p in [10,50]:\n",
    "    print(p, np.percentile(np.where(red_rch>p), [0, 100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbb1d23-9c42-4e31-a003-b2b888683810",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt_rch = np.arange(10,50)\n",
    "# plt_rch = np.arange(30,50)\n",
    "for v in ['Qrech', 'gradient']:\n",
    "    # average reduction\n",
    "    red_rch = 100*(1-(sfrdf_mean0[v]/sfrdf_mean[v]))\n",
    "    print('Avg %.2f %% reduction in stream ' %red_rch.iloc[plt_rch].mean(), v)\n",
    "\n",
    "# red_grad = 100*(1-(df_mean0.loc[10:50, 'gradient']/df_mean.loc[10:50, 'gradient']).mean())\n",
    "# red_grad = 100*(1-(df_mean0.loc[10:50, 'gradient']/df_mean.loc[10:50, 'gradient']).mean())\n",
    "# print('Avg %.2f %% reduction in stream gradient' %red_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1766d7-b3b9-4419-a77d-6b19060307ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify how the spatial extent of baseflow changes\n",
    "# quantity doesn't help because it's from 0\n",
    "Qbase_where0= np.where(sfrdf_mean0.Qbase>0)[0]\n",
    "Qbase_where= np.where(sfrdf_mean.Qbase>0)[0]\n",
    "Qbase_where.shape, Qbase_where0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d675510-6cce-4e0a-ac06-e0d7816a99be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
