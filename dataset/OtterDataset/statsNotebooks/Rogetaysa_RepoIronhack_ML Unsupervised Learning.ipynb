{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0225c1f",
   "metadata": {},
   "source": [
    "# Machine Learning: Unsupervised Learning Techniques in Python\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/1*PnrNu1pepOF4euxKx4brGw.png)\n",
    "\n",
    "Machine Learning (ML) is a dynamic and ever-evolving field that sits at the intersection of computer science and statistics. At its core, ML is about creating and using models that can find patterns in data and then use those patterns to make predictions on new, unseen data. Think of it as the process of teaching a computer to make decisions based on evidence rather than following a strict set of programmed instructions.\n",
    "\n",
    "**How Does Machine Learning Work?**\n",
    "\n",
    "1. **Data Ingestion:** ML starts with data - lots of it. This data can be anything from numbers and images to text and sound.\n",
    "2. **Pattern Recognition:** Algorithms are used to search for patterns and relationships within the data. This is akin to finding the pieces of a puzzle.\n",
    "3. **Model Building:** Once patterns are discovered, ML uses these insights to build a predictive model. Like assembling the puzzle pieces to see a part of the picture.\n",
    "4. **Predictions:** The predictive model can now take new data, apply the learned patterns, and make predictions or decisions without being explicitly programmed to perform the task.\n",
    "5. **Learning and Improving:** ML models are designed to learn and improve over time as they are exposed to more data.\n",
    "\n",
    "**The Relationship Between Machine Learning and Artificial Intelligence**\n",
    "\n",
    "Machine Learning is the engine that drives Artificial Intelligence technologies. Here's how they relate:\n",
    "\n",
    "- **Artificial Intelligence (AI):** AI is the broader science of mimicking human abilities. It's about creating machines that can perform tasks that typically require human intelligence.\n",
    "- **Machine Learning:** ML is a subset of AI that equips machines with the ability to learn from data and improve over time. It is one of the ways we achieve AI.\n",
    "\n",
    "**Types of learning**\n",
    "\n",
    "- **Supervised Learning:** The model learns using a labeled dataset, which acts as a teacher giving the correct answers (labels) for the data points.\n",
    "- **Unsupervised Learning:** The model looks for patterns and relationships in unlabeled data, learning from the inherent structure in the data.\n",
    "- **Reinforcement Learning:** The model learns to make decisions by performing actions and receiving feedback in the form of rewards or penalties.\n",
    "\n",
    "**And what is Deep Learning?**\n",
    "\n",
    "Deep Learning is a sophisticated and powerful subset of machine learning. It is inspired by the structure and function of the human brain in the form of a neural network. Deep learning models can automatically learn representations from data such as images, video, or text, without introducing hand-coded rules or human domain knowledge.\n",
    "\n",
    "A neural network, in the context of deep learning, is a complex architecture of interconnected nodes (neurons) arranged in layers. These layers can be thought of as a hierarchy of concepts, where each layer combines the information from the previous ones to create more complex representations.\n",
    "\n",
    "1. **Input Layer:** Receives the raw data. This is the initial contact point between the data and the neural network.\n",
    "2. **Hidden Layers:** These layers perform most of the computational heavy lifting. Each hidden layer transforms the data into more abstract and composite representations.\n",
    "3. **Output Layer:** This layer provides the final output, which could be a classification (like identifying an object in an image) or a value in a regression task.\n",
    "\n",
    "Deep learning networks learn through an iterative process called backpropagation, where the model makes predictions, measures the error of those predictions, and then goes back through the network to adjust the weights. This process is repeated thousands or millions of times, and with each pass, the network gets better at producing accurate predictions.\n",
    "\n",
    "Deep learning models, due to their depth and complexity, require significant computational power. However, with the rise of cloud computing and specialized hardware like GPUs, it has become more accessible, paving the way for groundbreaking research and applications.\n",
    "\n",
    "\n",
    "**Machine Learning, AI, and Beyond**\n",
    "\n",
    "Understanding ML and AI is not just about technology. It's about understanding a fundamental shift in the way we analyze data, make decisions, and build software to engage with the world. As we embark on this journey, we'll explore how these technologies are built, how they can be used, and the ethical considerations they entail.\n",
    "\n",
    "In the next sections, we'll delve deeper into each type of machine learning, explore key algorithms, and examine their impact across various sectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bf1faa",
   "metadata": {},
   "source": [
    "## The Distinction Between Machine Learning and Statistics\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:400/0*0o8cCtW-obWCQBHA.png)\n",
    "\n",
    "Understanding the difference between Machine Learning (ML) and statistics is essential as both fields are fundamental to making sense of data in the modern world. Though they share common ground, their philosophies, objectives, and approaches often differ.\n",
    "\n",
    "**Statistics: The Science of Data**\n",
    "\n",
    "Statistics is a branch of mathematics dealing with the collection, analysis, interpretation, and presentation of masses of numerical data. It's about understanding and quantifying the uncertainty and variability in data.\n",
    "\n",
    "- **Descriptive Statistics** help summarize and visualize data to describe its main features.\n",
    "- **Inferential Statistics** use samples to make generalizations or inferences about a larger population.\n",
    "\n",
    "It offers a suite of established algorithms and methods designed for specific types of data analysis:\n",
    "\n",
    "- **Parametric methods** assume a specific form for the data distribution and use the sample data to infer the population parameters.\n",
    "- **Non-parametric methods** do not assume a fixed form and are more flexible, allowing for analysis without strict distributional assumptions.\n",
    "\n",
    "Statisticians have developed rigorous methods for hypothesis testing, estimation, and drawing conclusions from data. These methods are designed to interpret data and quantify uncertainty.\n",
    "\n",
    "**Machine Learning: The Algorithmic Approach**\n",
    "\n",
    "Machine Learning, on the other hand, is a subfield of computer science that evolved from the study of pattern recognition and computational learning theory in AI. It involves the development of algorithms that can learn from and make predictions on data.\n",
    "\n",
    "- **Supervised Learning** algorithms make predictions based on a set of examples (input-output pairs).\n",
    "- **Unsupervised Learning** algorithms find hidden patterns or intrinsic structures in input data.\n",
    "\n",
    "The algorithms it employs are not fixed but are developed and refined as they are exposed to more data:\n",
    "\n",
    "- **Learning from Data:** Instead of using a predetermined algorithm, ML methods modify and improve their algorithms continually as they learn from more data.\n",
    "- **Predictive Focus:** ML is less about proving a hypothesis and more about making accurate predictions. It uses statistical methods as building blocks to create adaptive algorithms that improve as they ingest more data.\n",
    "\n",
    "The distinction is akin to the difference between using a pre-fabricated building plan (statistics) versus constructing a building that designs itself based on the environment it's in (machine learning).\n",
    "\n",
    "\n",
    "**Complementary Disciplines in Data Analysis**\n",
    "\n",
    "ML doesn't replace statistics; rather, it builds upon statistical theories to create flexible algorithms that can adjust and improve. Here's how they complement each other:\n",
    "\n",
    "- **Statistical Rigor in ML:** ML leverages statistical concepts such as regression, Bayesian methods, and likelihood estimations to understand and build upon the patterns in data.\n",
    "- **ML's Innovation in Statistics:** The adaptive algorithms of ML are enhancing statistical analysis, especially in areas with complex data that do not fit traditional models well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecc1d8b",
   "metadata": {},
   "source": [
    "# Session Plan: Unsupervised Learning Techniques in Python\n",
    "\n",
    "## 1. Introduction to Unsupervised Learning\n",
    "### 1.1 Overview of Unsupervised Learning\n",
    "- Detailed definition, scope, and real-world applications.\n",
    "### 1.2 Distinction Between Supervised and Unsupervised Learning\n",
    "- In-depth comparison with illustrative examples.\n",
    "\n",
    "## 2. Clustering Techniques\n",
    "### 2.1 In-Depth Study of K-means Clustering\n",
    "- Theoretical concepts, algorithm steps, and Python implementation.\n",
    "- **Hands-on Activity:** Applying K-means to a sample dataset, including visualization of clusters.\n",
    "### 2.2 Hierarchical Clustering Explored\n",
    "- Detailed explanation of hierarchical clustering methods and their uses.\n",
    "- **Practical Exercise:** Hierarchical Clustering in Python with dendrogram analysis.\n",
    "### 2.3 Understanding and Implementing DBSCAN\n",
    "- Comprehensive exploration of DBSCAN principles and use cases.\n",
    "- **Interactive Session:** DBSCAN in Python with parameter tuning.\n",
    "\n",
    "## 3. Profiling Clusters\n",
    "### 3.1 Deep Dive into Cluster Profiling\n",
    "- Importance and strategies for effective cluster profiling.\n",
    "### 3.2 Advanced Techniques for Profiling Clusters\n",
    "- Detailed guide to statistical summaries and advanced visualization techniques.\n",
    "- **Interactive Workshop:** Profiling Clusters in Python using various libraries.\n",
    "\n",
    "## 4. Dimensionality Reduction with PCA\n",
    "### 4.1 Mastering PCA\n",
    "- Extensive discussion on PCA concepts, mathematics, and applications.\n",
    "### 4.2 Hands-on PCA in Python\n",
    "- Step-by-step guide to PCA implementation.\n",
    "- **Case Study:** Applying PCA to a multi-dimensional dataset and interpreting results.\n",
    "\n",
    "## 5. Association Rules Mining\n",
    "### 5.1 Foundation of Association Rules\n",
    "- Deep dive into concepts, metrics (support, confidence, lift), and significance in data mining.\n",
    "### 5.2 Apriori Algorithm in Action\n",
    "- Thorough explanation and Python implementation.\n",
    "- **Real-World Application:** Executing Association Rules Mining on a retail dataset.\n",
    "\n",
    "## 6. Anomaly Detection\n",
    "### 6.1 Advanced Anomaly Detection Techniques\n",
    "- Elaborate exploration of methods like Isolation Forest and One-Class SVM.\n",
    "### 6.2 Practical Anomaly Detection in Python\n",
    "- Detailed implementation guide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76acd17",
   "metadata": {},
   "source": [
    "## 1. Introduction to Unsupervised Learning\n",
    "\n",
    "### 1.1 Overview of Unsupervised Learning\n",
    "\n",
    "Unsupervised learning is a type of machine learning that deals with pattern recognition and exploratory data analysis. Unlike supervised learning, unsupervised learning algorithms are not provided with labeled output data. Instead, these algorithms explore the structure of the data to extract meaningful information without guidance. \n",
    "\n",
    "**Key Points:**\n",
    "- **Data Exploration:** Unsupervised learning is often used to discover the underlying structure of data.\n",
    "- **Clustering:** A common unsupervised learning task is to group data into clusters based on similarity or distance metrics.\n",
    "- **Dimensionality Reduction:** Reducing the number of variables under consideration, and can be divided into feature selection and feature extraction.\n",
    "\n",
    "**Real-World Applications:**\n",
    "- **Market Segmentation:** Identifying distinct groups of customers to tailor marketing strategies.\n",
    "- **Genomics:** Classifying genes with similar functionalities without prior knowledge.\n",
    "- **Anomaly Detection:** Identifying unusual patterns that do not conform to expected behavior.\n",
    "\n",
    "### 1.2 Distinction Between Supervised and Unsupervised Learning\n",
    "\n",
    "![](https://www.researchgate.net/publication/351953193/figure/fig3/AS:11431281117150742@1675395484096/Supervised-and-unsupervised-machine-learning-a-Schematic-representation-of-an.png)\n",
    "\n",
    "Supervised and unsupervised learning represent two core approaches to machine learning, each with its unique use cases and methods.\n",
    "\n",
    "**Supervised Learning:**\n",
    "- **Labeled Data:** Supervised learning algorithms are trained on labeled data, meaning each training example is paired with an output label.\n",
    "- **Direct Feedback:** The goal is to learn a mapping from inputs to outputs, with direct feedback on prediction accuracy.\n",
    "- **Prediction Tasks:** Common tasks include classification and regression.\n",
    "\n",
    "**Unsupervised Learning:**\n",
    "- **No Labels:** In contrast, unsupervised learning deals with data that has no historical labels.\n",
    "- **Without Feedback:** The system is not told the 'correct' answer; it must figure out what is being shown.\n",
    "- **Discovery Tasks:** The goal is to explore the data and find some structure within.\n",
    "\n",
    "**Comparative Illustration:**\n",
    "Imagine supervised learning as a student learning with a teacher to solve math problems with a known answer key. In contrast, unsupervised learning is like a detective trying to find patterns and connections in a case without prior clues.\n",
    "\n",
    "**Bridging the Two:**\n",
    "While supervised and unsupervised learning may seem distinct, they can be complementary. Semi-supervised learning, for instance, uses both labeled and unlabeled data, which is particularly useful when acquiring labels is costly.\n",
    "\n",
    "In the next sections, we will explore various unsupervised learning techniques in Python, understand their principles, and apply them to real datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bc4a4b",
   "metadata": {},
   "source": [
    "## 2. Clustering Techniques\n",
    "\n",
    "### 2.1 In-Depth Study of K-means Clustering\n",
    "\n",
    "K-means clustering is a widely used technique in unsupervised machine learning for dividing a dataset into distinct groups, known as 'clusters.' It's particularly useful when you have data that naturally groups together but doesn't have any labels to guide the grouping.\n",
    "\n",
    "[K-means link](https://www.youtube.com/watch?v=4b5d3muPQmA&ab_channel=StatQuestwithJoshStarmer)\n",
    "\n",
    "#### Understanding K-means Clustering:\n",
    "\n",
    "Think of K-means as a way to organize data points into groups based on their similarity. Here's a simple analogy: imagine you have a mix of different colored balls scattered on the floor, and you want to organize them into groups of the same color. K-means clustering would be like finding the central point for each color group and then grouping the balls based on which central point they are closest to.\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1200/1*rw8IUza1dbffBhiA4i0GNQ.png)\n",
    "\n",
    "#### Theoretical Concepts:\n",
    "\n",
    "- **Centroids**: These are the 'central points' of each cluster. Imagine each cluster as a circle of data points, the centroid would be right at the center of this circle.\n",
    "- **Assignment Step**: Here, each data point is checked to see which centroid it is closest to (like each ball finding out which central point is nearest).\n",
    "- **Update Step**: After all data points have been assigned to centroids, the centroids recalibrate — they move to the true center of their respective clusters.\n",
    "- **Iteration**: These steps are repeated. With each iteration, the centroids move, and the data points might shift to a closer centroid, until the centroids stabilize and don't move anymore.\n",
    "\n",
    "#### Algorithm Steps Explained:\n",
    "\n",
    "1. **Initialization**: Imagine randomly placing flags on your dataset. These flags are your initial 'centroids.' \n",
    "2. **Assignment**: Each data point in your dataset 'decides' which flag (centroid) it is closest to and groups there.\n",
    "3. **Update**: Each flag (centroid) then moves to the center of its group of data points.\n",
    "4. **Repeat**: Steps 2 and 3 keep repeating. With each repeat, the groups get more refined, and the flags move less, until they don't need to move anymore.\n",
    "\n",
    "#### The Goal of K-means:\n",
    "\n",
    "The aim is to minimize the distance between points in a cluster and their centroid. When the centroids stop moving (or the movement is minimal), it means the clusters are as tight and accurate as they can be, based on the K-means algorithm.\n",
    "\n",
    "#### How to select the value of K:\n",
    "\n",
    "#####  Silhouette Score Method\n",
    "\n",
    "The Silhouette Score is a measure used to determine the degree of separation between clusters in K-means clustering. It provides insight into the distance between the resulting clusters.\n",
    "\n",
    "- **How it Works**: The Silhouette Score evaluates how close each point in one cluster is to points in the neighboring clusters. It ranges from -1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
    "- **Calculation**: For each sample, the Silhouette Score is computed as `(b - a) / max(a, b)`, where `a` is the mean distance to the other points in the same cluster and `b` is the mean nearest-cluster distance.\n",
    "- **Interpretation**: A score close to +1 indicates that the data points are far away from neighboring clusters, while a score close to 0 indicates that the clusters are overlapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36543d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "# Load the IRIS dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # Feature data\n",
    "y = iris.target  # Target labels (not used in K-means clustering)\n",
    "\n",
    "# Setting up the matplotlib figure with multiple subplots\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15,8))\n",
    "\n",
    "# Loop through different numbers of clusters (from 2 to 5)\n",
    "for i in [2, 3, 4, 5]:\n",
    "    # Create KMeans instance for different number of clusters\n",
    "    # 'k-means++' for smart centroid initialization, 10 different centroid initializations\n",
    "    # 100 iterations max for each run, and set a random state for reproducibility\n",
    "    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)\n",
    "\n",
    "    # Determine the position of the subplot\n",
    "    q, mod = divmod(i, 2)\n",
    "\n",
    "    # Create a SilhouetteVisualizer with the KMeans instance\n",
    "    # Colors are set to 'yellowbrick' palette, and the subplot ax is defined\n",
    "    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])\n",
    "\n",
    "    # Fit the visualizer to the data to produce the silhouette plot\n",
    "    visualizer.fit(X)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0e55a0",
   "metadata": {},
   "source": [
    "- For a particular K, all the clusters should have a Silhouette score greater than the average score of the data set represented by the red-dotted line. The x-axis represents the Silhouette score. The clusters with K=4 and 5 get eliminated because they don’t follow this condition.\n",
    "- There shouldn’t be wide fluctuations in the size of the clusters. The width of the clusters represents the number of data points. For K=2, the blue cluster has almost twice the width as compared to the green cluster. This blue cluster gets broken down into two sub-clusters for K=3, and thus forms clusters of uniform size.\n",
    "\n",
    "#####  Elbow Method\n",
    "\n",
    "The Elbow Method is another technique used to determine the optimal number of clusters in K-means clustering. It focuses on minimizing variance within each cluster.\n",
    "\n",
    "- **How it Works**: The method involves plotting the explained variance as a function of the number of clusters and picking the elbow of the curve as the number of clusters to use.\n",
    "- **Calculation**: It calculates the sum of squared distances of samples to their closest cluster center, varying the number of clusters `k` and observing the change in the sum of squared distances.\n",
    "- **Interpretation**: The point where the distortion/inertia starts to decrease more slowly (the 'elbow') is considered to be an indicator of the appropriate number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e167452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# Load the IRIS dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # Feature data\n",
    "y = iris.target  # Target labels (not used in K-means clustering)\n",
    "\n",
    "# Instantiate the KMeans model\n",
    "# random_state=42 is used for reproducibility of results\n",
    "km = KMeans(random_state=42)\n",
    "\n",
    "# Instantiate the KElbowVisualizer with the KMeans model\n",
    "# k=(2,10) indicates the range of number of clusters to try (from 2 to 10)\n",
    "visualizer = KElbowVisualizer(km, k=(2,10))\n",
    "\n",
    "# Fit the visualizer to the data\n",
    "# This will run K-means clustering for each value of k and calculate the distortion score for each\n",
    "visualizer.fit(X)\n",
    "\n",
    "# Render the plot\n",
    "# The Elbow plot displays the distortion score for each k\n",
    "# The point where the distortion score starts to level off ('elbow') is the recommended number of clusters\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1274695",
   "metadata": {},
   "source": [
    "#####  Comparing Silhouette Score and Elbow Method\n",
    "\n",
    "- **Objective**: While the Silhouette Score is focused on the quality of the clustering (i.e., how distinct the clusters are), the Elbow Method is more about minimizing variance within each cluster.\n",
    "- **Clarity of Indication**: The Elbow Method can sometimes be ambiguous, as the 'elbow' might not be very pronounced. The Silhouette Score, on the other hand, provides a more definitive numerical value that can be easier to interpret.\n",
    "- **Use Cases**: The Silhouette Score is particularly useful when the data is not too high-dimensional and when the clusters are expected to be dense and well separated. The Elbow Method is often used as a general guideline, especially when computational simplicity is desired.\n",
    "\n",
    "Both methods have their strengths and can be used complementarily. The Elbow Method can give a good initial estimate of the number of clusters, which can then be refined using the Silhouette Score for more precise and qualitative assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21968310",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go  #for 3D plot\n",
    "\n",
    "## K-means using k = 3\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "## 3D plot \n",
    "Scene = dict(xaxis = dict(title  = 'sepal_length -->'),yaxis = dict(title  = 'sepal_width--->'),zaxis = dict(title  = 'petal_length-->'))\n",
    "\n",
    "labels = kmeans.labels_\n",
    "trace = go.Scatter3d(x=X[:, 0], y=X[:, 1], z=X[:, 2], mode='markers',marker=dict(color = labels, size= 10, line=dict(color= 'black',width = 10)))\n",
    "layout = go.Layout(margin=dict(l=0,r=0),scene = Scene,height = 800,width = 800)\n",
    "data = [trace]\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182b7a90",
   "metadata": {},
   "source": [
    "#### Comparing K-means Clustering Predictions with Actual Labels\n",
    "\n",
    "In this analysis, we explore the effectiveness of K-means clustering, an unsupervised learning algorithm, in correctly identifying groups in the well-known Iris dataset. This dataset contains three species of Iris flowers, making it a great candidate to test the clustering capability of K-means.\n",
    "\n",
    "##### Process Overview\n",
    "\n",
    "1. **Data Loading**: We start by loading the Iris dataset, which includes features like sepal length, sepal width, petal length, and petal width.\n",
    "\n",
    "2. **K-means Clustering**: We apply the K-means clustering algorithm to this data. K-means is instructed to partition the data into three clusters, corresponding to the three species of Iris in the dataset.\n",
    "\n",
    "3. **Prediction and Alignment**: The K-means algorithm assigns each data point to one of these clusters. Since K-means is unsupervised, it doesn't use the actual labels (species) during clustering. Therefore, we align the predicted cluster labels with the actual labels for a meaningful comparison.\n",
    "\n",
    "######  Introduction to the Confusion Matrix\n",
    "\n",
    "The confusion matrix is a powerful tool for assessing the performance of a classification algorithm. It compares the actual labels of the data with the labels predicted by the model, providing a clear visual representation of the model's accuracy and misclassifications.\n",
    "\n",
    "- Each row of the matrix represents the instances of an actual class.\n",
    "- Each column represents the instances predicted by the model.\n",
    "- The diagonal elements show the number of correct classifications for each class.\n",
    "- The off-diagonal elements indicate the errors or misclassifications.\n",
    "\n",
    "By analyzing the confusion matrix, we can not only see how many instances were classified correctly, but also which specific classes are getting confused by the algorithm. This insight is valuable in understanding the performance and limitations of our clustering model.\n",
    "\n",
    "In the following Python script, we visualize this comparison using a confusion matrix to evaluate how well the K-means clustering has performed in distinguishing the different species of Iris flowers based on their features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be296b0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # Feature data\n",
    "y = iris.target  # Target labels\n",
    "\n",
    "# Apply K-means clustering\n",
    "# KMeans is initialized with 3 clusters and a fixed random state for reproducibility\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X)  # Fit KMeans model to the data\n",
    "y_kmeans = kmeans.predict(X)  # Predict the clusters for each data point\n",
    "\n",
    "# Function to align K-means labels with true labels\n",
    "def align_labels(kmeans_labels, true_labels):\n",
    "    \"\"\"\n",
    "    Aligns K-means cluster labels with true labels for maximum correspondence.\n",
    "    \n",
    "    :param kmeans_labels: Labels predicted by K-means\n",
    "    :param true_labels: Actual true labels of the data\n",
    "    :return: Aligned labels\n",
    "    \"\"\"\n",
    "    labels = np.zeros_like(kmeans_labels)\n",
    "    for i in range(3):\n",
    "        mask = (kmeans_labels == i)\n",
    "        # Assign the mode (most frequent label) of true labels to each cluster\n",
    "        labels[mask] = mode(true_labels[mask])[0]\n",
    "    return labels\n",
    "\n",
    "# Align the predicted labels with the true labels for meaningful comparison\n",
    "y_aligned = align_labels(y_kmeans, y)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y, y_aligned)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues',\n",
    "            xticklabels=iris.target_names,  # Set x-axis labels as target names\n",
    "            yticklabels=iris.target_names)  # Set y-axis labels as target names\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Clustered Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1051994",
   "metadata": {},
   "source": [
    "### K-means Clustering Exercise: Wine Dataset Analysis\n",
    "\n",
    "In this exercise, you'll apply K-means clustering to the Wine dataset, a popular dataset for classification tasks. This dataset contains the results of a chemical analysis of wines grown in the same region in Italy from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.\n",
    "\n",
    "#### Objective\n",
    "\n",
    "Your task is to use K-means clustering to analyze the Wine dataset and group wines based on their chemical compositions. After clustering, compare your results with the actual wine cultivars to assess the performance of the clustering algorithm.\n",
    "\n",
    "#### Dataset Overview\n",
    "\n",
    "- **Number of Instances**: 178\n",
    "- **Features**: 13 numerical features, representing different chemical constituents like alcohol, malic acid, color intensity, etc.\n",
    "- **Classes**: The dataset contains three types of wine, representing three different cultivars.\n",
    "\n",
    "#### Steps to Follow\n",
    "\n",
    "1. **Load the Dataset**: Import the Wine dataset from `sklearn.datasets`.\n",
    "2. **Data Exploration**: Familiarize yourself with the dataset. Check the number of features, instances, and explore basic statistics of the dataset.\n",
    "3. **Apply K-means Clustering**:\n",
    "   - Use the K-means algorithm from `sklearn.cluster`.\n",
    "   - Experiment with different numbers of clusters (start with `n_clusters=3`).\n",
    "   - Fit the model to the data and predict the cluster for each wine instance.\n",
    "4. **Evaluate the Clustering**:\n",
    "   - Use metrics like Silhouette Score or compare with the actual wine cultivars using a confusion matrix.\n",
    "   - Visualize the clusters if possible (you may need to use dimensionality reduction techniques like PCA for visualization).\n",
    "5. **Interpret Results**:\n",
    "   - Analyze the characteristics of each cluster.\n",
    "   - Determine if the clusters correspond well with the actual cultivars.\n",
    "   - Reflect on the effectiveness of K-means for this type of data.\n",
    "\n",
    "#### Questions for Further Analysis\n",
    "\n",
    "- How did the number of clusters (`n_clusters`) affect the clustering outcome?\n",
    "- Which features seem to contribute most to distinguishing between the different clusters?\n",
    "- Could any preprocessing steps (like scaling) improve the clustering results?\n",
    "\n",
    "This exercise will enhance your understanding of K-means clustering and its application in real-world datasets. It will also provide insights into the challenges of unsupervised learning, particularly in interpreting the results and evaluating the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2ac01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial code for the Wine dataset K-means clustering exercise\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Convert to DataFrame for easier data manipulation and visualization\n",
    "wine_df = pd.DataFrame(X, columns=wine.feature_names)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"First five rows of the dataset:\")\n",
    "print(wine_df.head())\n",
    "\n",
    "print(\"\\nDataset statistics:\")\n",
    "print(wine_df.describe())\n",
    "\n",
    "# Initial K-means clustering setup\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Predicting the clusters\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Calculating silhouette score\n",
    "silhouette_avg = silhouette_score(X, labels)\n",
    "print(\"\\nSilhouette Score for 3 clusters: \", silhouette_avg)\n",
    "\n",
    "# Adding cluster information to the DataFrame\n",
    "wine_df['cluster'] = labels\n",
    "\n",
    "# Initial visualization - pairplot (optional, based on time and requirement)\n",
    "# sns.pairplot(wine_df, hue='cluster')\n",
    "# plt.show()\n",
    "\n",
    "# The code snippet above sets the stage for the exercise. It includes loading the data, basic exploration,\n",
    "# applying K-means clustering, and calculating the silhouette score. Encourage your students to explore\n",
    "# further by varying the number of clusters, visualizing the data, and comparing the clustering results\n",
    "# with the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d02c56a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Instantiate the KMeans model\n",
    "# random_state=42 is used for reproducibility of results\n",
    "km = KMeans(random_state=42)\n",
    "\n",
    "# Instantiate the KElbowVisualizer with the KMeans model\n",
    "# k=(2,10) indicates the range of number of clusters to try (from 2 to 10)\n",
    "visualizer = KElbowVisualizer(km, k=(2,10))\n",
    "\n",
    "# Fit the visualizer to the data\n",
    "# This will run K-means clustering for each value of k and calculate the distortion score for each\n",
    "visualizer.fit(X)\n",
    "\n",
    "# Render the plot\n",
    "# The Elbow plot displays the distortion score for each k\n",
    "# The point where the distortion score starts to level off ('elbow') is the recommended number of clusters\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59af3427",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b44a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go  #for 3D plot\n",
    "\n",
    "## K-means using k = 3\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "## 3D plot \n",
    "Scene = dict(xaxis = dict(title  = 'sepal_length -->'),yaxis = dict(title  = 'sepal_width--->'),zaxis = dict(title  = 'petal_length-->'))\n",
    "\n",
    "labels = kmeans.labels_\n",
    "trace = go.Scatter3d(x=X[:, 0], y=X[:, 1], z=X[:, 2], mode='markers',marker=dict(color = labels, size= 10, line=dict(color= 'black',width = 10)))\n",
    "layout = go.Layout(margin=dict(l=0,r=0),scene = Scene,height = 800,width = 800)\n",
    "data = [trace]\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d9e21a",
   "metadata": {},
   "source": [
    "#### Applying K-means Clustering in Real Life\n",
    "\n",
    "K-means clustering is not just for academic study; it's also a practical tool used in many everyday business situations. Understanding how to use K-means in real-life scenarios can show its importance and how it can help in business.\n",
    "\n",
    "- **Customer Segmentation:** Businesses often use K-means to group customers based on things like what they buy, their preferences, and where they live. This helps businesses create special marketing plans, develop products, and offer services that appeal more to each group of customers.**Example:** A store groups its customers by looking at what they buy. This helps the store send marketing messages to each group that are more likely to get their interest.\n",
    "\n",
    "- **Managing Inventory:** K-means can help businesses handle their stock better. By grouping products based on how well they sell and other factors, businesses can keep the right amount of stock, lower costs, and make their supply chain better. **Example:** A clothing store uses K-means to sort clothes based on how quickly they sell and the time of year. This helps the store know how much of each type of clothing to keep in stock.\n",
    "\n",
    "- **Sorting Documents:** K-means is useful for organizing lots of documents. This is good for finding information quickly, keeping large databases organized, and making work with documents more efficient.**Example:** A law firm uses K-means to sort many case files. This makes it easier to find the right documents for each legal case.\n",
    "\n",
    "- **Studying the Market:** K-means can help understand market trends by analyzing data about the market. This is important for planning, entering new markets, and understanding competition.**Example**: A company that studies markets uses K-means to look at survey data. This helps them see different groups of customers and what they like in a specific area.\n",
    "\n",
    "**Conclusion:** K-means clustering helps businesses in many ways. It can find patterns in data and group complex data into useful categories. This helps in making good decisions and working more efficiently. As you learn about data analysis, knowing how to use K-means in these situations will be very useful for your career.\n",
    "\n",
    "In our exercises, we saw how K-means works with datasets like Iris and Wine. Now think about how you can use this method to solve real business problems. Think about possible uses in your own field or industry, and how grouping data can give new insights and help in making decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a71d521",
   "metadata": {},
   "source": [
    "### 2.2 Hierarchical Clustering Explored\n",
    "\n",
    "Hierarchical clustering is a nuanced and insightful method of cluster analysis, widely recognized for its application across various fields ranging from bioinformatics to social sciences. It operates fundamentally differently from partitioning methods like K-means, providing a hierarchy of clusters that can be visualized in a dendrogram.\n",
    "\n",
    "[Hierarchical clustering explained](https://www.youtube.com/watch?v=7xHsRkOdVwo&ab_channel=StatQuestwithJoshStarmer)\n",
    "\n",
    "![](https://www.kdnuggets.com/wp-content/uploads/c_unveiling_hidden_patterns_introduction_hierarchical_clustering_9.png)\n",
    "\n",
    "#### Theoretical Background of Hierarchical Clustering:\n",
    "\n",
    "1. **Concept and Types**:\n",
    "   - Hierarchical clustering builds nested clusters by either merging or dividing them successively. The process is visualized as a tree-like diagram called a dendrogram.\n",
    "   - **Agglomerative (Bottom-Up)**: This approach starts by considering each data point as a separate cluster and then merges them based on similarity. It's akin to building a family tree from children to parents ([Johnson, 1967](https://doi.org/10.1007/BF02289588)).\n",
    "   - **Divisive (Top-Down)**: Here, all data points begin in one cluster, which is recursively split into smaller clusters. It resembles dividing a tree into branches and then leaves.\n",
    "\n",
    "2. **Distance Metrics and Linkage Criteria**:\n",
    "   - **Distance Metrics**: These define how the 'distance' between data points is measured. Common metrics include Euclidean, Manhattan, and cosine similarity.\n",
    "   - **Linkage Criteria**: This criterion determines how to measure the distance between clusters. Options include:\n",
    "     - *Single Linkage*: Distance between the closest members of clusters.\n",
    "     - *Complete Linkage*: Distance between the farthest members.\n",
    "     - *Average Linkage*: Average distance across all pairs of members.\n",
    "     - *Ward’s Method*: Minimization of variance within each cluster ([Ward, 1963](https://www.jstor.org/stable/2528416)).\n",
    "\n",
    "#### Importance in Data Analysis:\n",
    "\n",
    "- **Versatility**: Can be applied to any dataset where a measure of distance can be defined.\n",
    "- **No Preset Clusters**: Unlike K-means, it does not require specifying the number of clusters beforehand, offering flexibility in determining the cluster granularity based on the dendrogram.\n",
    "- **Rich Data Interpretation**: The dendrogram presents a detailed picture of the data, revealing intricate structures and relationships ([Murtagh, F., & Contreras, P., 2012](https://link.springer.com/article/10.1007/s00357-012-9101-4)).\n",
    "\n",
    "Hierarchical clustering is extensively used in biology for the classification of genes and proteins ([Eisen et al., 1998](https://www.pnas.org/content/95/25/14863)), in linguistics for grouping similar languages, and in marketing for customer segmentation. Its ability to uncover detailed data relationships makes it a powerful tool for exploratory data analysis.\n",
    "\n",
    "In the upcoming practical exercise, we will apply hierarchical clustering to a dataset and explore how to interpret its hierarchical structure through dendrogram analysis. This hands-on experience is crucial for understanding how hierarchical clustering can be leveraged to extract meaningful insights from complex data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4226ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Hierarchical Clustering Example using the Iris Dataset\n",
    "\n",
    "# Importing necessary libraries\n",
    "from sklearn import datasets  # For loading the Iris dataset\n",
    "import matplotlib.pyplot as plt  # For plotting the dendrogram\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage  # For hierarchical clustering\n",
    "import numpy as np  # For numerical operations\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # The feature data (sepal length, sepal width, petal length, petal width)\n",
    "\n",
    "# Performing hierarchical clustering\n",
    "# 'linkage' is used to perform the hierarchical clustering\n",
    "# The 'ward' method is an approach to minimize the variance within each cluster\n",
    "linked = linkage(X, 'ward')\n",
    "\n",
    "# Creating a dendrogram to visualize the clustering\n",
    "# A dendrogram is a diagram that shows the hierarchical relationship between objects\n",
    "# It's often used in hierarchical clustering to visualize the structure of the clusters\n",
    "plt.figure(figsize=(10, 7))  # Setting the size of the plot\n",
    "dendrogram(linked,\n",
    "           orientation='top',  # The direction to plot the dendrogram\n",
    "           labels=np.array(iris.target),  # Labels for each point\n",
    "           distance_sort='descending',  # Sorting the distances in descending order\n",
    "           show_leaf_counts=True)  # Show the number of samples in each cluster\n",
    "plt.title('Hierarchical Clustering Dendrogram (Ward)')  # Title of the plot\n",
    "plt.xlabel('Sample Index')  # X-axis label\n",
    "plt.ylabel('Distance')  # Y-axis label representing the distances between clusters\n",
    "plt.show()  # Display the plot\n",
    "\n",
    "# In this plot, the y-axis represents the distance or dissimilarity between clusters.\n",
    "# The x-axis represents the individual samples or clusters (after merging).\n",
    "# The height of each U-shaped line represents the distance between the two data points being linked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91214b8e",
   "metadata": {},
   "source": [
    "Let's go back into **theory** again:\n",
    "\n",
    "#### **Agglomerative Hierarchical clustering Technique**\n",
    "\n",
    "In this technique, initially each data point is considered as an individual cluster. At each iteration, the similar clusters merge with other clusters until one cluster or K clusters are formed.\n",
    "\n",
    "The basic algorithm of Agglomerative is straight forward.\n",
    "- Compute the proximity matrix\n",
    "- Let each data point be a cluster\n",
    "- Repeat: Merge the two closest clusters and update the proximity matrix\n",
    "- Until only a single cluster remains\n",
    "\n",
    "Key operation is the computation of the proximity of two clusters\n",
    "\n",
    "To understand better let’s see a pictorial representation of the Agglomerative Hierarchical clustering Technique. Lets say we have six data points {A,B,C,D,E,F}.\n",
    "\n",
    "- **Step- 1**: In the initial step, we calculate the proximity of individual points and consider all the six data points as individual clusters as shown in the image below.\n",
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*3pMZjFiiaaLcfSZBKDjbXA.png)\n",
    "\n",
    "- **Step- 2**: In step two, similar clusters are merged together and formed as a single cluster. Let’s consider B,C, and D,E are similar clusters that are merged in step two. Now, we’re left with four clusters which are A, BC, DE, F.\n",
    "- **Step- 3**: We again calculate the proximity of new clusters and merge the similar clusters to form new clusters A, BC, DEF.\n",
    "- **Step- 4**: Calculate the proximity of the new clusters. The clusters DEF and BC are similar and merged together to form a new cluster. We’re now left with two clusters A, BCDEF.\n",
    "- **Step- 5**: Finally, all the clusters are merged together and form a single cluster.\n",
    "\n",
    "The Hierarchical clustering Technique can be visualized using a Dendrogram.\n",
    "\n",
    "A Dendrogram is a tree-like diagram that records the sequences of merges or splits.\n",
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*JPQRbJDw2E1_HEvwzVTDDw.jpeg)\n",
    "![](https://miro.medium.com/v2/resize:fit:1000/format:webp/1*fw1vlNtq2vPFmAXsBy1_dA.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12199cde",
   "metadata": {},
   "source": [
    "####  **Divisive Hierarchical clustering Technique**: \n",
    "\n",
    "Since the Divisive Hierarchical clustering Technique is not much used in the real world, I’ll give a brief of the Divisive Hierarchical clustering Technique.\n",
    "\n",
    "In simple words, we can say that the Divisive Hierarchical clustering is exactly the opposite of the Agglomerative Hierarchical clustering. In Divisive Hierarchical clustering, we consider all the data points as a single cluster and in each iteration, we separate the data points from the cluster which are not similar. Each data point which is separated is considered as an individual cluster. In the end, we’ll be left with n clusters.\n",
    "\n",
    "As we’re dividing the single clusters into n clusters, it is named as Divisive Hierarchical clustering.\n",
    "\n",
    "So, we’ve discussed the two types of the Hierarchical clustering Technique.\n",
    "\n",
    "But wait!! we’re still left with the important part of Hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16644c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Enhanced Hierarchical Clustering Example with Comparison to Actual Labels\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target  # Actual labels\n",
    "\n",
    "# Perform hierarchical clustering using the 'ward' method\n",
    "linked = linkage(X, 'ward')\n",
    "\n",
    "# Set a maximum distance (threshold) to cut the dendrogram\n",
    "max_distance = 10  # Adjust this threshold based on the dendrogram\n",
    "cluster_labels = fcluster(linked, max_distance, criterion='distance')\n",
    "\n",
    "# Plotting dendrogram with the threshold line\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)\n",
    "plt.axhline(y=max_distance, color='r', linestyle='--')\n",
    "plt.title('Hierarchical Clustering Dendrogram (Ward) with threshold')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Compare the obtained clusters with the actual labels\n",
    "cm = confusion_matrix(y, cluster_labels - 1)  # Adjusting labels to start from 0\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Print the number of clusters and cluster labels\n",
    "num_clusters = np.unique(cluster_labels).size\n",
    "print(f'Number of clusters formed: {num_clusters}')\n",
    "print(f'Cluster labels for each point: {cluster_labels}')\n",
    "\n",
    "# This comparison helps in evaluating the accuracy of hierarchical clustering \n",
    "# in grouping the data points as per the actual species in the Iris dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752b5d01",
   "metadata": {},
   "source": [
    "#### Deciding the Cutting Distance in Hierarchical Clustering\n",
    "\n",
    "Determining the right place to 'cut' the dendrogram in hierarchical clustering is crucial for defining the number of clusters. There are several numerical methods to guide this decision:\n",
    "\n",
    "##### 1. Visual Inspection\n",
    "- **Description**: This method involves looking at the dendrogram and identifying the largest vertical gap that isn't crossed by extended horizontal lines.\n",
    "- **Use**: It's used to find a natural division between clusters, where merging clusters significantly increases the distance.\n",
    "\n",
    "##### 2. Inconsistency Coefficient Method\n",
    "- **Description**: Calculates the inconsistency coefficient for each link, which is a measure of the dissimilarity of a link compared to the links below it.\n",
    "- **Use**: Links with high inconsistency coefficients suggest significant divisions and can be used to set the cutting threshold.\n",
    "\n",
    "##### 3. Silhouette Score\n",
    "- **Description**: Although more common in K-means, it measures how similar an object is to its own cluster compared to other clusters.\n",
    "- **Use**: By cutting the dendrogram at various heights and computing the silhouette score for each cut, the height yielding the best score indicates an optimal clustering.\n",
    "\n",
    "##### 4. Using Predefined Criteria\n",
    "- **Description**: Based on external business needs or specific research questions, the required number of clusters is predefined.\n",
    "- **Use**: This predefined number of clusters can determine the appropriate cutting distance on the dendrogram.\n",
    "\n",
    "##### 5. Trial and Error\n",
    "- **Description**: Involves experimenting with different distances, observing the resulting cluster formations, and choosing the one that seems most reasonable.\n",
    "- **Use**: This method is often used when other methods do not yield clear or satisfactory results.\n",
    "\n",
    "##### Conclusion\n",
    "Each of these methods offers a different approach to determining the number of clusters in hierarchical clustering. The choice of method may depend on the dataset's characteristics, the specific goals of the analysis, and the analyst's familiarity with the data. Understanding these methods can enhance the decision-making process in clustering tasks and lead to more meaningful clustering results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b240a0e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Python Code for Determining Cutting Distance in Hierarchical Clustering\n",
    "\n",
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, inconsistent\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# Perform hierarchical clustering using the 'ward' method\n",
    "Z = linkage(X, 'ward')\n",
    "\n",
    "# 1. Inconsistency Coefficient Method\n",
    "# The inconsistency coefficient compares the height of each link in a cluster hierarchy with the average height of links below it\n",
    "depth = 5  # Specify the depth for calculating inconsistency\n",
    "inconsistencies = inconsistent(Z, depth)\n",
    "inconsistency_threshold = np.average(inconsistencies[:, -1])  # Average inconsistency value\n",
    "clusters_icm = fcluster(Z, inconsistency_threshold, criterion='inconsistent')\n",
    "\n",
    "# 2. Silhouette Score Method\n",
    "# This approach calculates the silhouette score for each possible number of clusters and selects the one with the best score\n",
    "silhouette_scores = []\n",
    "range_n_clusters = list(range(2, 10))  # Range of possible clusters\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    clusters = fcluster(Z, n_clusters, criterion='maxclust')\n",
    "    score = silhouette_score(X, clusters)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "best_cluster_num = range_n_clusters[np.argmax(silhouette_scores)]  # Number of clusters with highest silhouette score\n",
    "clusters_ssm = fcluster(Z, best_cluster_num, criterion='maxclust')\n",
    "\n",
    "# Plotting the Silhouette Scores\n",
    "plt.plot(range_n_clusters, silhouette_scores)\n",
    "plt.title('Silhouette Score Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()\n",
    "\n",
    "# Results\n",
    "print(f'Optimal number of clusters (Inconsistency Coefficient Method): {np.unique(clusters_icm).size}')\n",
    "print(f'Optimal number of clusters (Silhouette Score Method): {best_cluster_num}')\n",
    "\n",
    "# These techniques provide a more data-driven approach to finding the optimal number of clusters \n",
    "# in hierarchical clustering, enhancing the accuracy and reliability of the clustering process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81898c21",
   "metadata": {},
   "source": [
    "#### Hierarchical Clustering vs K-means\n",
    "\n",
    "When it comes to cluster analysis in data science, both Hierarchical Clustering and K-means are popular choices. Understanding their differences is crucial for selecting the right method for a given dataset and analysis objective.\n",
    "\n",
    "##### Hierarchical Clustering\n",
    "\n",
    "- **Nature**: Builds a hierarchy of clusters either through a bottom-up (Agglomerative) or top-down (Divisive) approach.\n",
    "- **Number of Clusters**: Does not require the number of clusters to be specified in advance. The number of clusters can be determined by analyzing the dendrogram.\n",
    "- **Flexibility**: Offers more flexibility with respect to the number of clusters.\n",
    "- **Computational Complexity**: Generally more computationally intensive, especially for large datasets, due to the hierarchical nature of the algorithm.\n",
    "- **Interpretability**: The dendrogram provides a deep insight into the data structure.\n",
    "- **Use Cases**: Particularly useful in exploratory data analysis where the data structure and cluster relations are unknown.\n",
    "\n",
    "##### K-means Clustering\n",
    "\n",
    "- **Nature**: A centroid-based partitioning method that divides the data into non-overlapping subsets (clusters) without establishing any hierarchical structure.\n",
    "- **Number of Clusters**: Requires pre-specification of the number of clusters (k).\n",
    "- **Speed and Scalability**: Tends to be faster and more scalable for large datasets compared to hierarchical clustering.\n",
    "- **Simplicity**: The algorithm is easier to understand and implement.\n",
    "- **Use Cases**: Ideal for situations where the underlying structure of the data is known, or a specific number of clusters is desired for further analysis.\n",
    "\n",
    "##### Key Differences\n",
    "\n",
    "- **Algorithm Structure**: Hierarchical clustering creates a tree-like model of the data, while K-means partitions the data into distinct clusters.\n",
    "- **Determining the Number of Clusters**: Hierarchical clustering does not require you to specify the number of clusters beforehand, unlike K-means.\n",
    "- **Computational Resources**: Hierarchical clustering is more computationally demanding, making K-means a preferred choice for very large datasets.\n",
    "- **Output Interpretation**: The dendrogram of hierarchical clustering can provide more insights into the data, whereas K-means offers a straightforward partitioning of the data.\n",
    "\n",
    "##### Choosing Between Hierarchical Clustering and K-means\n",
    "\n",
    "The choice between hierarchical clustering and K-means should be based on the dataset size, the computational resources available, and the specific goals of the analysis. For exploratory analysis where the relationships between data points are of interest, hierarchical clustering is advantageous. On the other hand, K-means is suitable for large datasets where the goal is to partition the data into a predetermined number of clusters.\n",
    "\n",
    "Understanding these differences will help you select the most appropriate method for your clustering needs, ensuring effective and efficient data analysis.\n",
    "\n",
    "## Exercise: Hierarchical Clustering Practice\n",
    "\n",
    "### Objective:\n",
    "In this exercise, you will apply hierarchical clustering to a practical dataset. Your goal is to understand how hierarchical clustering works and how to interpret the results, particularly through the dendrogram.\n",
    "\n",
    "### Dataset: The Wine Dataset\n",
    "For this exercise, we will use the well-known Wine dataset. This dataset contains the results of a chemical analysis of wines grown in the same region in Italy from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.\n",
    "\n",
    "### Steps to Follow:\n",
    "\n",
    "1. **Load the Dataset**:\n",
    "   - Use `sklearn.datasets` to load the Wine dataset.\n",
    "   - Explore the dataset briefly (size, number of features).\n",
    "\n",
    "2. **Apply Hierarchical Clustering**:\n",
    "   - Use the `linkage` function from `scipy.cluster.hierarchy` to perform hierarchical clustering.\n",
    "   - Choose an appropriate linkage method (e.g., 'ward', 'single', 'complete').\n",
    "\n",
    "3. **Create and Analyze a Dendrogram**:\n",
    "   - Use the `dendrogram` function to visualize the cluster hierarchy.\n",
    "   - Determine the number of clusters by identifying the largest vertical distance that doesn't intersect any horizontal line (or use another method as discussed).\n",
    "\n",
    "4. **Compare Clusters with Actual Labels**:\n",
    "   - Use `fcluster` to form clusters at the chosen distance.\n",
    "   - Compare the clusters obtained with the actual labels (if available) using a confusion matrix.\n",
    "\n",
    "### Expected Outcomes:\n",
    "- A clear understanding of how hierarchical clustering groups data.\n",
    "- Insights into the differences in clustering results based on various linkage methods.\n",
    "- Ability to interpret a dendrogram and decide on the number of clusters.\n",
    "\n",
    "### Questions for Reflection:\n",
    "- How did the choice of linkage method affect the clustering results?\n",
    "- How closely do the formed clusters match the actual labels in the dataset?\n",
    "- What insights about the wine data can you infer from the clustering results?\n",
    "\n",
    "This exercise will strengthen your understanding of hierarchical clustering and its application in data analysis. It will also highlight the nuances of interpreting dendrograms and the importance of choosing the right linkage method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b73ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Python Code for Hierarchical Clustering Exercise using the Wine Dataset\n",
    "\n",
    "# Importing necessary libraries\n",
    "from sklearn import datasets\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data  # Feature data\n",
    "y = wine.target  # Actual labels\n",
    "\n",
    "# Perform hierarchical clustering using the 'ward' method\n",
    "linked = linkage(X, 'ward')\n",
    "\n",
    "# Plotting the dendrogram to visualize the clustering hierarchy\n",
    "plt.figure(figsize=(12, 7))\n",
    "dendrogram(linked, orientation='top', labels=y, distance_sort='descending', show_leaf_counts=True)\n",
    "plt.title('Hierarchical Clustering Dendrogram (Ward)')\n",
    "plt.xlabel('Sample Index or Cluster Size')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Choosing a distance threshold to determine the number of clusters\n",
    "# This threshold can be set by visually inspecting the dendrogram\n",
    "distance_threshold = 150  # This is an example value\n",
    "clusters = fcluster(linked, distance_threshold, criterion='distance')\n",
    "\n",
    "# Comparing the obtained clusters with the actual labels using a confusion matrix\n",
    "cm = confusion_matrix(y, clusters)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues')\n",
    "plt.title('Confusion Matrix between Hierarchical Clusters and Actual Labels')\n",
    "plt.xlabel('Predicted Clusters')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Additional instructions for the exercise can include experimenting with different linkage methods,\n",
    "# such as 'single', 'complete', 'average', and interpreting the impact on the dendrogram and clustering results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c352642e",
   "metadata": {},
   "source": [
    "### 2.3 Understanding and Implementing DBSCAN\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) stands out in the realm of clustering algorithms for its proficiency in handling complex datasets. It is uniquely capable of identifying clusters of various shapes and sizes, and is particularly adept at managing outliers and noise.\n",
    "\n",
    "**Example:** Consider a park with people scattered around: some in groups, some standing alone. DBSCAN identifies clusters of people standing close together (within a distance `eps` and in groups larger than `min_samples`). Those who are part of a group are in a cluster, while individuals far from any group are treated as outliers.\n",
    "\n",
    "[DBSCAN Link](https://www.youtube.com/watch?v=RDZUdRSDOok&ab_channel=StatQuestwithJoshStarmer)\n",
    "\n",
    "- **Comparison with K-means and Hierarchical Clustering:**\n",
    "\n",
    "    - **K-means**: A centroid-based clustering method that partitions the data into K clusters. K-means assumes clusters to be convex-shaped (like circles or spheres) and requires the number of clusters to be specified in advance. Unlike DBSCAN, K-means can struggle with non-convex clusters and is sensitive to outliers.\n",
    "  \n",
    "    - **Hierarchical Clustering**: Builds a tree of clusters either from bottom-up or top-down. It doesn't require pre-specifying the number of clusters, but it can be computationally intensive and may not effectively handle noise and outliers. Hierarchical clustering is useful for understanding the data's hierarchical structure but doesn't inherently categorize noise as DBSCAN does.\n",
    "\n",
    "    DBSCAN's ability to handle irregularly shaped clusters and identify outliers gives it a significant edge in real-world datasets where such features are common. This makes DBSCAN a highly valuable tool for exploratory data analysis, particularly in complex scenarios where K-means or hierarchical clustering might not yield optimal results.\n",
    "    \n",
    "![](https://miro.medium.com/v2/resize:fit:1400/1*rfi9uHjGPdNgXgxe9xWvVw.png)\n",
    "\n",
    "#### Simple Explanation of DBSCAN:\n",
    "\n",
    "1. **What DBSCAN Does**: It groups together points that are close to each other based on a set distance and minimum number of points. Points that are not in dense areas are marked as outliers.\n",
    "\n",
    "2. **Key Terms in DBSCAN**:\n",
    "   - **eps (ε)**: A distance measure that defines how close points should be to each other to be considered part of a cluster.\n",
    "   - **min_samples**: The minimum number of points required to form a dense region, which DBSCAN treats as a cluster.\n",
    "\n",
    "3. **Types of Points**:\n",
    "   - **Core Points**: A point that has at least a minimum number of other points (min_samples) within a certain distance (eps).\n",
    "   - **Border Points**: Points that are within the distance (eps) of a core point but don't have enough points nearby to be core points themselves.\n",
    "   - **Noise Points**: Points that are neither core nor border points.\n",
    "\n",
    "#### Why Use DBSCAN?\n",
    "\n",
    "- **Good for Complex Shapes**: Unlike K-means, DBSCAN doesn't assume that clusters are round, so it can find clusters of any shape.\n",
    "- **Handles Outliers Well**: It can identify and ignore outliers, making it robust in real-world data scenarios.\n",
    "- **No Need to Specify Cluster Number**: You don't need to know how many clusters you're looking for, which is a requirement in some other algorithms like K-means.\n",
    "\n",
    "#### Common Use Cases:\n",
    "\n",
    "- **Geographical Data Analysis**: Like identifying areas of similar land use in satellite images.\n",
    "- **Anomaly Detection**: Such as identifying fraudulent transactions in banking.\n",
    "\n",
    "#### Exercise: Implementing DBSCAN Clustering\n",
    "\n",
    "In this exercise, you will gain hands-on experience with the DBSCAN clustering algorithm using the Iris dataset. DBSCAN is renowned for its ability to identify clusters of arbitrary shape and for its robustness to outliers. This exercise will guide you through applying DBSCAN, tuning its parameters, and visualizing the results.\n",
    "\n",
    "##### Objective:\n",
    "Learn to apply the DBSCAN clustering algorithm to a real dataset, understand the importance of parameter tuning, and interpret the clustering outcome.\n",
    "\n",
    "##### Dataset: Iris Dataset\n",
    "The Iris dataset is a classic dataset in machine learning and statistics, featuring measurements of 150 iris flowers from three different species. You will use DBSCAN to cluster these flowers based on their petal and sepal measurements.\n",
    "\n",
    "##### Steps:\n",
    "\n",
    "1. **Load the Dataset**:\n",
    "   - Start by loading the Iris dataset. Consider standardizing the data, as DBSCAN's performance is affected by the scale of the dataset.\n",
    "\n",
    "2. **Apply DBSCAN**:\n",
    "   - Use the `DBSCAN` class from `sklearn.cluster` to cluster the dataset. Begin with `eps=0.5` and `min_samples=5` as starting parameters.\n",
    "\n",
    "3. **Parameter Tuning**:\n",
    "   - Experiment with different values of `eps` and `min_samples` to see how they affect the clustering results. Aim to find a balance where the clusters make intuitive sense, and noise is minimized.\n",
    "\n",
    "4. **Visualize the Results**:\n",
    "   - Visualize the clustering outcome using a scatter plot. Since the Iris dataset has four features, select two features for a 2D plot or explore other visualization techniques for higher dimensions.\n",
    "\n",
    "5. **Analyze the Outcome**:\n",
    "   - Evaluate the clustering by considering the number of clusters formed and the number of points marked as noise. Reflect on how DBSCAN's density-based approach compares to other clustering methods you've learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f867e988",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# It's often a good idea to scale the data for clustering algorithms\n",
    "X_scaled = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5053ad3a",
   "metadata": {},
   "source": [
    "Before applying DBSCAN, it's crucial to standardize the features of the dataset. This is achieved using `StandardScaler` from `sklearn.preprocessing`. Here's why and how it works:\n",
    "\n",
    "#### What Does StandardScaler Do?\n",
    "\n",
    "- **StandardScaler** adjusts the features of your dataset so they have a mean of 0 and a standard deviation of 1. This process is known as **standardization** or **z-score normalization**.\n",
    "- The formula used is: `(X - mean) / standard_deviation`, where `X` is the original feature value.\n",
    "\n",
    "#### Why Standardize?\n",
    "\n",
    "- **Equal Weight**: Many clustering algorithms, including DBSCAN, calculate distances between data points. If features are on different scales, larger-scale features might dominate the distance calculations, biasing the algorithm. Standardization ensures each feature contributes equally.\n",
    "- **Improved Convergence**: Algorithms converge faster when data is on a similar scale, especially important for large datasets or complex models.\n",
    "\n",
    "Example:\n",
    "Imagine a dataset with two features: height in meters and weight in kilograms. Without standardization, weight would disproportionately influence clustering because its values are numerically larger than height values. StandardScaler mitigates this issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying DBSCAN\n",
    "# eps: The maximum distance between two samples for them to be considered as in the same neighborhood\n",
    "# min_samples: The number of samples in a neighborhood for a point to be considered as a core point\n",
    "dbscan = DBSCAN(eps=0.55, min_samples=5)\n",
    "clusters = dbscan.fit_predict(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e926f715",
   "metadata": {},
   "source": [
    "In DBSCAN, two main parameters, `eps` and `min_samples`, play crucial roles in defining the clusters:\n",
    "\n",
    "#### `eps` (epsilon)\n",
    "- **Definition**: The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "- **Impact**: A smaller `eps` value makes the algorithm stricter about what constitutes a neighborhood, leading to more clusters. A larger `eps` increases the neighborhood size, potentially reducing the number of clusters and merging distinct clusters.\n",
    "- **Choosing `eps`**: Selecting the right `eps` value is critical and often dataset-specific. The k-distance graph method can help find a starting point.\n",
    "\n",
    "#### `min_samples`\n",
    "- **Definition**: The number of samples in a neighborhood for a point to be considered a core point. It includes the point itself.\n",
    "- **Impact**: Determines the minimum size of a dense region. Higher values indicate that more points are required to form a cluster, which can lead to fewer, larger clusters.\n",
    "- **Choosing `min_samples`**: This parameter reflects the minimum cluster size you expect to find meaningful. It's often set based on domain knowledge.\n",
    "\n",
    "Example:\n",
    "Using `eps=0.5` and `min_samples=5` means a point needs at least 4 neighbors within a radius of 0.5 units to be considered a core point of a cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2ccd00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting the clustering result\n",
    "plt.figure(figsize=(10, 7))\n",
    "# The Iris dataset has 4 features, for simplicity, we'll plot two dimensions\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap='viridis', marker='o', edgecolor='k', s=50)\n",
    "plt.title('DBSCAN Clustering of Iris Dataset')\n",
    "plt.xlabel('Feature 1 (scaled)')\n",
    "plt.ylabel('Feature 2 (scaled)')\n",
    "plt.show()\n",
    "\n",
    "# Analyzing the results\n",
    "# -1 in clusters represents outliers detected by DBSCAN\n",
    "n_clusters_ = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
    "n_noise_ = list(clusters).count(-1)\n",
    "\n",
    "print(f'Estimated number of clusters: {n_clusters_}')\n",
    "print(f'Estimated number of noise points: {n_noise_}')\n",
    "\n",
    "# DBSCAN has clustered the data into groups and identified any outliers.\n",
    "# The eps and min_samples parameters were chosen as starting values; tuning these parameters\n",
    "# may yield better clustering results for different datasets or specific requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f319a5",
   "metadata": {},
   "source": [
    "After running DBSCAN, analyzing the output helps understand the clustering structure:\n",
    "\n",
    "#### Outliers\n",
    "- Points labeled `-1` are considered outliers or noise by DBSCAN. This label means they didn't fit well into any cluster based on the provided `eps` and `min_samples`.\n",
    "\n",
    "#### Number of Clusters\n",
    "- `n_clusters_` is calculated by counting the unique cluster labels. Since outliers are labeled `-1`, they are excluded from the cluster count.\n",
    "- This count helps assess how fragmented the data was clustered, providing insight into the data's underlying structure.\n",
    "\n",
    "#### Noise Points\n",
    "- `n_noise_` represents the count of these outlier points. A high number of noise points might indicate too strict parameters (`eps` and `min_samples`), suggesting a need for adjustment.\n",
    "\n",
    "Example:\n",
    "If DBSCAN returns many points as noise, you might consider increasing `eps` or decreasing `min_samples` to allow more points to join clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d163cf7",
   "metadata": {},
   "source": [
    "## Determining `eps` in DBSCAN with the K-Distance Graph\n",
    "\n",
    "When using DBSCAN, one of the key parameters to set is `eps`, which defines the radius within which to search for neighboring points. A methodical way to choose an appropriate `eps` value is by using the **k-distance graph**, similar to the elbow method used in K-means for finding the optimal number of clusters.\n",
    "\n",
    "### What is the K-Distance Graph?\n",
    "\n",
    "The k-distance graph helps identify the optimal `eps` value by plotting the distance from each point in the dataset to its k-th nearest neighbor. Here's how to use it:\n",
    "\n",
    "#### Steps to Compute `eps`:\n",
    "\n",
    "1. **Select `k`**:\n",
    "   - The choice of `k` typically depends on your dataset and objectives. A common starting point is to set `k` equal to `min_samples` - 1.\n",
    "\n",
    "2. **Compute K-Distances**:\n",
    "   - For every point, calculate the distance to its k-th nearest neighbor.\n",
    "\n",
    "3. **Plot the K-Distance Graph**:\n",
    "   - Order points by their k-distance in ascending order and plot these distances. The graph usually shows a sharp bend or \"elbow,\" indicating where points start to be further away from their neighbors.\n",
    "\n",
    "4. **Identify `eps`**:\n",
    "   - The y-coordinate of the elbow point suggests a good value for `eps`. This distance marks the threshold at which point densities decrease sharply, signifying the border between cluster points and noise.\n",
    "\n",
    "### Why Use the K-Distance Graph?\n",
    "\n",
    "- **Systematic Approach**: It provides a data-driven method to set `eps`, reducing guesswork.\n",
    "- **Adaptability**: This technique is versatile, applicable across datasets with different characteristics.\n",
    "- **Insightful**: Offers visual insight into the data's structure, helping to understand the density distribution.\n",
    "\n",
    "### Considerations:\n",
    "\n",
    "- **Heuristic Method**: While useful, this approach may require experimentation with different values of `k` to refine the `eps` estimation.\n",
    "- **Varying Densities**: For datasets with significantly varying densities, consider analyzing different segments separately or adjusting parameters accordingly.\n",
    "\n",
    "By employing the k-distance graph, you can methodically select an `eps` value that enhances the performance of DBSCAN clustering, making it an invaluable tool in your data analysis toolkit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0a8ad7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Python Example: Using the k-distance Graph to Determine `eps` for DBSCAN\n",
    "\n",
    "# Importing necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# Setting 'k' equal to min_samples - 1 (assuming min_samples is 4 for this example)\n",
    "k = 4 - 1\n",
    "\n",
    "# Initialize NearestNeighbors with n_neighbors as 'k'\n",
    "# Use the 'ball_tree' algorithm for efficient distance computation\n",
    "nbrs = NearestNeighbors(n_neighbors=k, algorithm='ball_tree').fit(X)\n",
    "\n",
    "# Find the distance to the k-th nearest neighbor for each point\n",
    "# distances: Array of distances to k-th nearest neighbor\n",
    "# indices: The indices of the k-th nearest neighbor in the dataset\n",
    "distances, indices = nbrs.kneighbors(X)\n",
    "\n",
    "# Sort the distances\n",
    "sorted_distances = np.sort(distances[:, k-1], axis=0)\n",
    "\n",
    "# Plotting the k-distance Graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(sorted_distances)\n",
    "plt.title('K-Distance Graph')\n",
    "plt.xlabel('Points sorted by distance to k-th nearest neighbor')\n",
    "plt.ylabel('k-th nearest neighbor distance')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Identifying 'eps':\n",
    "# Look for the \"elbow\" in the plotted graph. The y-value at this point is a good starting value for 'eps'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85450844",
   "metadata": {},
   "source": [
    "#### Choosing `min_samples` for DBSCAN\n",
    "\n",
    "Selecting the right `min_samples` parameter in DBSCAN is crucial for defining the density threshold that determines what constitutes a cluster. Unlike the `eps` parameter, which can be estimated using the k-distance graph, `min_samples` often requires a more experimental approach, guided by your dataset's characteristics and your specific clustering goals.\n",
    "\n",
    "##### Guidelines for `min_samples`:\n",
    "\n",
    "1. **Based on Dimensionality**:\n",
    "   - A general rule of thumb is to set `min_samples` to `D + 1`, where `D` is the dimensionality of your dataset. This is the minimum number needed to form a dense region in a space of `D` dimensions.\n",
    "   - For datasets with higher dimensionality, consider using `2 * D` or higher to mitigate the curse of dimensionality.\n",
    "\n",
    "2. **Data Density and Size Considerations**:\n",
    "   - For larger datasets with expected dense clusters, a higher `min_samples` might be appropriate to avoid identifying too many small, possibly insignificant clusters.\n",
    "   - In contrast, for smaller datasets or those where finer cluster distinctions are valuable, a lower `min_samples` can help identify nuanced structures.\n",
    "\n",
    "3. **Noise Sensitivity**:\n",
    "   - Increasing `min_samples` enhances the algorithm's resilience to noise by demanding more points to constitute a cluster, thus classifying more points as outliers.\n",
    "   - Adjust `min_samples` based on your tolerance for noise and outliers in your clustering results.\n",
    "\n",
    "##### Practical Approach to Selection:\n",
    "\n",
    "- **Experimentation is Key**: Start with the dimensionality-based guideline and adjust based on the observed clustering quality and relevance.\n",
    "- **Evaluate Clustering Outcomes**: Use domain knowledge and, if applicable, internal metrics like silhouette scores to assess cluster validity. However, be cautious with silhouette scores as they may not fully capture the quality of non-spherical clusters.\n",
    "- **Iterative Refinement**: Fine-tune `min_samples` based on initial results. Too many outliers may suggest lowering `min_samples`, while too many small clusters or excessive fragmentation might indicate a need for increase.\n",
    "\n",
    "##### Example Scenario:\n",
    "\n",
    "If dealing with a 10-dimensional dataset, begin with `min_samples` set to 11 (`D + 1`). If clusters appear overly fragmented, consider increasing `min_samples` to 20 (`2 * D`) and re-evaluating the results.\n",
    "\n",
    "##### Conclusion:\n",
    "\n",
    "Determining the optimal `min_samples` involves balancing the algorithm's sensitivity to noise, the desired cluster granularity, and the inherent properties of your dataset. Starting with informed guidelines and refining through testing will lead you to more meaningful and robust DBSCAN clustering outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca85cb17",
   "metadata": {},
   "source": [
    "## Exercise: Implementing DBSCAN Clustering on the Wine Dataset\n",
    "\n",
    "### Objective:\n",
    "In this exercise, you'll apply the DBSCAN clustering algorithm to the Wine dataset, exploring how to tune its parameters effectively and interpret the results. The Wine dataset presents a more complex clustering challenge, making it an excellent opportunity to practice DBSCAN.\n",
    "\n",
    "### Dataset:\n",
    "**Wine Dataset**: Contains the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The dataset includes 13 different measurements taken for different constituents found in the three types of wine.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Load the Dataset**:\n",
    "   - Use `sklearn.datasets.load_wine()` to import the Wine dataset.\n",
    "\n",
    "2. **Preprocess the Data**:\n",
    "   - Standardize the dataset using `StandardScaler` to ensure each feature contributes equally to the distance calculations.\n",
    "\n",
    "3. **Apply DBSCAN**:\n",
    "   - Implement DBSCAN from `sklearn.cluster` with initial values of `eps=0.5` and `min_samples=5`. Adjust these parameters as needed based on the clustering results.\n",
    "\n",
    "4. **Parameter Tuning**:\n",
    "   - Experiment with varying `eps` and `min_samples` to see their impact on clustering. \n",
    "   - Consider employing the k-distance graph to determine a more appropriate `eps` value.\n",
    "\n",
    "5. **Evaluate and Analyze the Results**:\n",
    "   - Determine the number of clusters and noise points identified by DBSCAN.\n",
    "   - Visualize the clustering using a scatter plot or other suitable visualization techniques. Consider reducing dimensionality with PCA for visualization if necessary.\n",
    "   - Optionally, compare the DBSCAN-derived clusters with the actual wine cultivars to assess the clustering quality.\n",
    "\n",
    "6. **Reflection and Discussion**:\n",
    "   - Reflect on the DBSCAN parameter settings that yielded the most meaningful clustering results.\n",
    "   - Discuss the effectiveness of DBSCAN for the Wine dataset and any insights gained from the clustering.\n",
    "\n",
    "### Expected Outcome:\n",
    "\n",
    "Upon completing this exercise, you will:\n",
    "\n",
    "- Gain practical experience in applying DBSCAN to a complex dataset.\n",
    "- Understand the significance of the `eps` and `min_samples` parameters and how to tune them.\n",
    "- Learn to evaluate the quality of clusters formed by DBSCAN and interpret the algorithm's ability to handle noise and outliers.\n",
    "- Develop insights into the chemical composition of different wine cultivars based on unsupervised clustering results.\n",
    "\n",
    "This hands-on exercise will deepen your understanding of density-based clustering and its applications in analyzing real-world datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a81e668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa84e04",
   "metadata": {},
   "source": [
    "## 3. Profiling Clusters\n",
    "\n",
    "After identifying clusters within a dataset using clustering algorithms like K-means, DBSCAN, or Hierarchical Clustering, the next critical step is **Cluster Profiling**. This process involves analyzing each cluster to understand its defining characteristics and how it differs from other clusters. Profiling enables us to interpret the clusters in a meaningful way, often leading to actionable insights.\n",
    "\n",
    "[Article on cluster creation and cluster algorithms](https://medium.com/analytics-vidhya/clustering-and-profiling-customers-using-k-means-9afa4277427)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a8374",
   "metadata": {},
   "source": [
    "### 3.1 Deep Dive into Cluster Profiling\n",
    "\n",
    "#### The Importance of Cluster Profiling\n",
    "\n",
    "Cluster profiling serves multiple critical purposes in data analysis:\n",
    "\n",
    "- **Insightful Characterization**: It transforms abstract clusters into understandable groups with specific characteristics, making the data analysis actionable.\n",
    "- **Targeted Strategies**: Businesses and organizations can develop tailored strategies for different segments identified through clustering, optimizing resources and efforts.\n",
    "- **Enhanced Communication**: Clearly defined profiles allow for better communication of findings to stakeholders, facilitating strategic decisions.\n",
    "\n",
    "#### Strategies for Effective Cluster Profiling\n",
    "\n",
    "To profile clusters effectively, a combination of statistical analysis and visualization techniques is often employed. Here are some key strategies:\n",
    "\n",
    "1. **Statistical Summaries**:\n",
    "   - **Descriptive Statistics**: Calculate mean, median, standard deviation, and other statistics for numerical features within each cluster. This helps in understanding the central tendency and dispersion of data points.\n",
    "   - **Distribution Analysis**: Assess the distribution of data within clusters to identify skewness, outliers, or peculiar patterns that define each cluster.\n",
    "\n",
    "2. **Feature Analysis**:\n",
    "   - **Significant Features**: Identify which features significantly contribute to defining a cluster. This can be done through hypothesis testing or by examining feature importance in decision trees.\n",
    "   - **Relative Importance**: Compare the importance of features across clusters to understand what makes each cluster distinct.\n",
    "\n",
    "3. **Visualization Techniques**:\n",
    "   - **Box Plots and Histograms**: Visualize the distribution of features within each cluster to understand their spread and central values.\n",
    "   - **Radar Charts**: Useful for multi-dimensional data, allowing comparison of cluster averages across various features.\n",
    "   - **Heatmaps**: Compare clusters against key features to visualize how each cluster differs in terms of feature values.\n",
    "\n",
    "4. **Dimensionality Reduction**:\n",
    "   - **PCA (Principal Component Analysis)**: Use PCA to reduce the dimensionality of your data, making it easier to visualize and interpret clusters in a 2D or 3D space.\n",
    "   - **t-SNE or UMAP**: For more complex datasets, t-SNE or UMAP can provide a nuanced visualization of clusters in a reduced-dimensional space.\n",
    "\n",
    "5. **Cross-Tabulation for Categorical Data**:\n",
    "   - Analyze how categorical variables are distributed across clusters to find patterns or anomalies. This can reveal if certain categories are predominant in specific clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75705e",
   "metadata": {},
   "source": [
    "### 3.2 Advanced Techniques for Profiling Clusters\n",
    "\n",
    "After identifying clusters within your dataset, the next step is to understand and profile these clusters deeply. Profiling involves examining the characteristics and key features of each cluster to uncover insights. This section explores advanced techniques for profiling clusters, focusing on statistical summaries and visualization methods.\n",
    "\n",
    "#### Statistical Summaries for Cluster Profiling\n",
    "\n",
    "Statistical summaries provide a quantitative look into the properties of each cluster. Here's how to approach this:\n",
    "\n",
    "1. **Central Tendency and Dispersion**: For each cluster, calculate measures of central tendency (mean, median) and dispersion (variance, standard deviation) for all features. This helps in understanding the typical values and the spread of data within each cluster.\n",
    "\n",
    "2. **Distribution**: Assess the distribution of features within clusters using histograms or kernel density estimates. This can highlight skewness or the presence of outliers within clusters.\n",
    "\n",
    "3. **Comparison**: Compare these statistical measures across clusters to identify distinguishing features. For example, if one cluster has significantly higher mean values for a particular feature, this feature could be key to defining that cluster.\n",
    "\n",
    "#### Advanced Visualization Techniques\n",
    "\n",
    "Visual tools can bring data to life, making complex relationships more understandable. Here are some advanced visualization techniques for cluster profiling:\n",
    "\n",
    "1. **Box Plots**: Use box plots to visualize the distribution of features within each cluster. This can quickly highlight differences in median values and variability across clusters.\n",
    "\n",
    "2. **Radar Charts**: Radar charts are effective for multi-dimensional data, allowing you to plot the average values of features for each cluster on a circular graph. This makes it easy to see which features define each cluster.\n",
    "\n",
    "3. **Heatmaps**: Heatmaps can be used to visualize the mean values of features across clusters. This can help identify which features are most characteristic of each cluster.\n",
    "\n",
    "4. **Dimensionality Reduction**: Apply PCA (Principal Component Analysis) or t-SNE for a 2D or 3D visualization of the clusters. This can help in visualizing how well-separated the clusters are in a reduced-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e002e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load a dataset\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target  # Assuming 'y' are cluster labels from a previous clustering\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert to a DataFrame for easier manipulation\n",
    "df = pd.DataFrame(X_scaled, columns=data.feature_names)\n",
    "df['Cluster'] = y  # Add cluster labels to the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ab406",
   "metadata": {},
   "source": [
    "#### Insights from Cluster Profiling\n",
    "\n",
    "##### 1. Statistical Summaries\n",
    "\n",
    "The statistical summaries provide a foundational understanding of each cluster by computing the mean and standard deviation for each feature within each cluster. This analysis reveals:\n",
    "\n",
    "- **Central Tendencies**: The mean values give insight into the 'average' characteristics of each cluster, helping to identify the central tendencies of the features that define each group.\n",
    "- **Variability**: Standard deviation values illuminate the variability or dispersion of features within each cluster. A high standard deviation indicates that the data points in that cluster are spread out over a wide range of values, while a low standard deviation suggests that the points are clustered closely around the mean.\n",
    "\n",
    "These summaries are crucial for understanding the general behavior of each cluster and identifying distinguishing features that set each cluster apart from the others.\n",
    "\n",
    "##### 2. Visualization Techniques: Box Plots\n",
    "\n",
    "Box plots are an invaluable tool for visually summarizing the distribution of each feature within the clusters. From the box plots, you can expect to gain insights into:\n",
    "\n",
    "- **Distribution Spread**: How broadly the data points in each cluster spread across the feature's range. This can highlight clusters with wide variances in certain features.\n",
    "- **Outliers**: Points that fall outside the typical range of data, shown as dots beyond the 'whiskers' of the box plots. Outliers can indicate anomalies within clusters or features with high variability.\n",
    "- **Median Values**: The line within each box represents the median value of the feature in that cluster, offering a visual cue to the central tendency that complements the mean values from statistical summaries.\n",
    "- **Interquartile Range (IQR)**: The length of the box itself shows the IQR, the middle 50% of values for the feature. A longer box suggests a greater spread of middle values.\n",
    "\n",
    "By examining the box plots for each feature across clusters, you'll be able to visually compare clusters and understand which features contribute most to the differences between clusters. This method of visualization brings statistical summaries to life, providing a clear and immediate way to grasp the characteristics that define each cluster.\n",
    "\n",
    "Together, statistical summaries and box plots form a comprehensive picture of your clusters, combining quantitative analysis with visual exploration to deepen your understanding of the data's structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db9646f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Statistical Summaries\n",
    "# Compute mean and standard deviation for each cluster and feature\n",
    "cluster_summary = df.groupby('Cluster').agg(['mean', 'std']).transpose()\n",
    "\n",
    "# 2. Visualization Techniques\n",
    "# Box plots for each feature across clusters\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(data.feature_names):\n",
    "    plt.subplot(4, 4, i + 1)  # Adjust grid size based on the number of features\n",
    "    sns.boxplot(x='Cluster', y=feature, data=df)\n",
    "    plt.title(feature)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198d7fea",
   "metadata": {},
   "source": [
    "####  Visualizing Cluster Profiles with Radar Charts\n",
    "\n",
    "In the realm of cluster analysis, understanding the unique characteristics of each identified group is crucial for deriving actionable insights. One of the most effective ways to visualize and compare the multidimensional profiles of clusters is through radar charts. These charts offer a comprehensive view of how clusters differ across various features, enabling a deeper understanding of the data's underlying structure.\n",
    "\n",
    "#####  Objective of the Exercise\n",
    "\n",
    "The purpose of this exercise is to employ radar charts for visualizing the average values of features across different clusters. By plotting each cluster's profile in a radar chart, we can visually assess the defining characteristics of each group, highlighting their similarities and differences in a single, intuitive graphic.\n",
    "\n",
    "#####  Why Radar Charts?\n",
    "\n",
    "- **Multi-dimensional Comparison**: Radar charts allow us to compare several quantitative variables simultaneously, making them ideal for cluster profiling.\n",
    "- **Identify Unique Attributes**: By observing the shape formed by each cluster on the chart, we can quickly identify which features are most distinctive for each cluster.\n",
    "- **Enhanced Interpretability**: These charts make it easier to communicate complex multidimensional data in a way that's accessible to both technical and non-technical stakeholders.\n",
    "\n",
    "In the following Python example, we'll demonstrate how to create a radar chart for the clusters obtained from the Wine dataset using DBSCAN. This visualization will not only enhance our understanding of the clusters but also showcase the practical application of advanced data visualization techniques in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8a5914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Load and scale the Wine dataset\n",
    "wine_data = load_wine()\n",
    "X = wine_data.data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Assume DBSCAN clustering has been applied to obtain cluster labels\n",
    "# For demonstration, applying DBSCAN directly here\n",
    "dbscan = DBSCAN(eps=2, min_samples=5)\n",
    "clusters = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "# Adding cluster labels to the dataframe\n",
    "df = pd.DataFrame(X_scaled, columns=wine_data.feature_names)\n",
    "df['Cluster'] = clusters\n",
    "\n",
    "# Prepare data for the radar chart\n",
    "features = wine_data.feature_names\n",
    "means = df.groupby('Cluster').mean().reset_index()\n",
    "\n",
    "# Setup for radar chart\n",
    "labels = np.array(features)\n",
    "num_vars = len(labels)\n",
    "\n",
    "# Create angles for each axis in the plot (with one extra for the closing loop)\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "angles += angles[:1]  # Close the loop\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
    "\n",
    "# Plot each cluster's average feature values on the radar chart\n",
    "for i, row in means.iterrows():\n",
    "    data = row.drop('Cluster').values.tolist()\n",
    "    data += data[:1]  # Repeat the first value to close the loop\n",
    "    ax.plot(angles, data, label=f'Cluster {row.Cluster}')\n",
    "    ax.fill(angles, data, alpha=0.25)\n",
    "\n",
    "# Add feature names to each axis\n",
    "ax.set_xticks(angles[:-1])  # Set ticks to the number of features\n",
    "ax.set_xticklabels(labels, rotation='vertical')  # Ensure the labels match feature names\n",
    "\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a310282",
   "metadata": {},
   "source": [
    "#### Introduction to PCA for Dimensionality Reduction and Visualization\n",
    "\n",
    "In our exploration of cluster profiling and visualization, we've encountered datasets with multiple features that can make analysis complex, especially when trying to visualize or find patterns in high-dimensional space. To address this challenge, we introduce a powerful technique known as **Principal Component Analysis (PCA)**.\n",
    "\n",
    "##### What is PCA?\n",
    "\n",
    "PCA is a statistical procedure that transforms a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (accounting for as much of the variability in the data as possible), and each succeeding component, in turn, has the highest variance possible under the constraint that it is orthogonal to the preceding components.\n",
    "\n",
    "##### Why Use PCA?\n",
    "\n",
    "- **Simplification**: PCA reduces the dimensionality of the data, simplifying the complexity in high-dimensional datasets while retaining the variation present in the dataset.\n",
    "- **Visualization**: By reducing the dataset to 2 or 3 principal components, PCA enables us to visualize the data in a 2D or 3D space, making it easier to observe patterns, clusters, and relationships.\n",
    "- **Efficiency**: Reducing the number of variables can improve the efficiency of other computational analyses, such as clustering algorithms.\n",
    "\n",
    "##### PCA in Cluster Profiling:\n",
    "\n",
    "In the context of cluster profiling, PCA is particularly useful for visualizing how clusters formed in high-dimensional space are distributed in reduced dimensions. This can provide valuable insights into the separation and characteristics of clusters that might not be apparent in the original space.\n",
    "\n",
    "##### Upcoming Detailed Exploration:\n",
    "\n",
    "While we've briefly introduced PCA here and demonstrated its application in visualizing clusters, a more detailed exploration of PCA will be covered in the next section. We'll delve into the mathematics behind PCA, how to interpret its components, and discuss best practices for its application in data analysis.\n",
    "\n",
    "Stay tuned for a deeper understanding of PCA and how it can be leveraged to uncover hidden patterns in your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c5f60d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4. PCA for Dimensionality Reduction and Visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=df['Cluster'], palette='viridis', alpha=0.7)\n",
    "plt.title('Clusters in PCA-reduced Space')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# This script demonstrates statistical summaries, box plots for feature distributions,\n",
    "# a radar chart for comparing cluster averages, and PCA for visualizing clusters in reduced dimensionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc55dd56",
   "metadata": {},
   "source": [
    "## 4. Dimensionality Reduction with PCA\n",
    "\n",
    "When we work with data, sometimes we have too many details (or \"features\") to look at all at once. Imagine trying to understand a person based on thousands of facts about them. It can be overwhelming! That's where Principal Component Analysis (PCA) comes in. It's like finding the most important stories that summarize all those facts without losing the essence.\n",
    "\n",
    "**What is PCA?**\n",
    "\n",
    "PCA is a way to simplify your data. If your data is a huge, crowded room of information, PCA helps you find the few key speakers who tell you almost everything you need to know. It does this by combining many details into a few big ideas, which we call \"principal components.\"\n",
    "\n",
    "**How PCA Works**\n",
    "\n",
    "1. **Making Everything Comparable**: First, we ensure no single detail dominates by mistake, by scaling all the data. It's like making sure everyone speaks at a volume we can compare.\n",
    "2. **Finding Relationships**: Then, PCA looks at how these details relate to each other - which facts go hand in hand and which don't.\n",
    "3. **Picking the Leaders**: From this, PCA finds the \"leaders\" - the principal components that represent the most significant patterns in the data.\n",
    "4. **Choosing the Important Stories**: Finally, we pick the top few leaders. These leaders tell us the most important stories about our data.\n",
    "\n",
    "**Why Use PCA?**\n",
    "\n",
    "- **Less Noise**: PCA helps us focus on the big picture and ignore the unimportant, noisy details.\n",
    "- **Easier to See and Understand**: With fewer details, it's easier to visualize and understand the data.\n",
    "- **Faster Processing**: Less information means our data analysis can run faster.\n",
    "\n",
    "**Where Can We Use PCA?**\n",
    "\n",
    "- **Simplifying Data**: When we have too much information, PCA helps us keep only what's really important.\n",
    "- **Finding Patterns**: PCA can help discover patterns in the data that weren't obvious before.\n",
    "- **Improving Speed**: Before running complex analyses or machine learning, using PCA can make things quicker by simplifying the data first.\n",
    "\n",
    "[PCA link](https://www.youtube.com/watch?v=FgakZw6K1QQ&ab_channel=StatQuestwithJoshStarmer)\n",
    "![](https://miro.medium.com/v2/resize:fit:1400/1*ba0XpZtJrgh7UpzWcIgZ1Q.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79fc856",
   "metadata": {},
   "source": [
    "### 4.1 Mastering PCA\n",
    "\n",
    "Imagine you have a huge box full of various toys. To make sense of what's inside, you decide to sort them out. But instead of looking at each toy individually, you find a simpler way to organize them by their most noticeable features, like size or color. This is somewhat what PCA (Principal Component Analysis) does with data. It finds what's most important in a lot of information and helps us understand the big picture without getting lost in details.\n",
    "\n",
    "### What's PCA All About?\n",
    "\n",
    "PCA is a tool that helps us see patterns in data when there's just too much to look at. It's like using a map to navigate a city instead of wandering every street. Here's how it works:\n",
    "\n",
    "1. **Getting Everything on the Same Page**: Just like it's easier to compare things when they're on a similar scale, PCA starts by making sure all parts of our data can be compared fairly. This step is called standardization.\n",
    "   \n",
    "2. **Finding What Stands Out**: Imagine you could stretch a rubber band around all your data points. The longest side of the band points out what's most different among them. PCA looks for these differences, which we call \"principal components.\" The first principal component shows the biggest difference, the second one finds the next big difference, and so on.\n",
    "\n",
    "3. **Choosing the Best View**: Often, we only need the first few principal components to get a good idea of what our data is about. It's like picking the best photos from a trip to tell its story without showing every single picture.\n",
    "\n",
    "### Why Do We Use PCA?\n",
    "\n",
    "- **To Simplify**: PCA helps us focus on the big ideas in our data, making it easier to explore and understand.\n",
    "- **To See Patterns**: By reducing the clutter, PCA can reveal patterns we might not notice otherwise.\n",
    "- **To Speed Things Up**: Analyses that would take a long time with lots of data can run faster after PCA simplifies the data.\n",
    "\n",
    "### Where Can PCA Take Us?\n",
    "\n",
    "- **Spotting Groups**: PCA can show us natural groupings in our data that we might want to explore further.\n",
    "- **Reducing Noise**: Sometimes, data has a lot of random variation that can distract us. PCA helps by focusing on the patterns that matter.\n",
    "- **Preparing for Other Analysis**: With simpler data, we're better set up to use other tools and techniques for deeper insights.\n",
    "\n",
    "### Bringing PCA into Practice\n",
    "\n",
    "Now that we know why PCA is useful, we'll dive into how to use it with Python in our next steps. We'll take our complex data, apply PCA to highlight the most telling features, and visualize the essence of our data in a way that's easy to understand and share.\n",
    "\n",
    "Stay tuned as we break down these concepts into practical, step-by-step exercises that will empower you to master PCA in your data projects.\n",
    "\n",
    "[Useful matherial on PCA](https://medium.com/mlearning-ai/principal-component-analysis-pca-simplified-22ef97b0e1dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd46c8e",
   "metadata": {},
   "source": [
    "### 4.2 Hands-on PCA with the Iris Dataset\n",
    "\n",
    "Principal Component Analysis (PCA) is a powerful technique for dimensionality reduction, enhancing data visualization and computational efficiency. Here's a step-by-step guide to performing PCA on the Iris dataset, a popular dataset for machine learning tasks involving flower species classification based on physical measurements.\n",
    "\n",
    "### Steps to Perform PCA\n",
    "\n",
    "1. **Standardization**: Ensure each feature contributes equally by standardizing the data.\n",
    "\n",
    "2. **Covariance Matrix Computation**: Understand how features vary together.\n",
    "\n",
    "3. **Eigen Decomposition**: Identify the directions (principal components) that maximize the variance in the data.\n",
    "\n",
    "4. **Sort by Eigenvalues**: Rank the principal components by their importance.\n",
    "\n",
    "5. **Choose Principal Components**: Select a subset of principal components for further analysis.\n",
    "\n",
    "### Step-by-Step PCA on the Iris Dataset\n",
    "\n",
    "**1. Loading and Preparing the Data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2f3b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Visualize the data before standardization\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.kdeplot(data=pd.DataFrame(X, columns=feature_names))\n",
    "plt.title('Before Standardization')\n",
    "plt.xlabel('Feature Value')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(feature_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1e2df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Standardization\n",
    "# When analyzing data, we often find features measured in different units (e.g., kilograms, kilometers).\n",
    "# Directly comparing these features without standardization might lead to incorrect assumptions and results.\n",
    "# For example, without standardization, an algorithm might incorrectly interpret 100 grams as larger than 1 kilogram.\n",
    "# Standardization adjusts features so they have a mean of 0 and variance of 1, making different units directly comparable.\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Scale the data to mean=0 and variance=1\n",
    "\n",
    "# Visualize the data after standardization\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.kdeplot(data=pd.DataFrame(X_scaled, columns=feature_names))\n",
    "plt.title('After Standardization')\n",
    "plt.xlabel('Feature Value')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(feature_names)\n",
    "plt.show()\n",
    "\n",
    "# Note: Standardization might not be necessary if your variables are already on a similar scale. However, for PCA,\n",
    "# standardization ensures that each feature contributes equally to the analysis, preventing features with larger\n",
    "# scales from dominating the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5af35e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Compute the Covariance Matrix\n",
    "# The covariance matrix is like a mathematical tool that helps us understand\n",
    "# how different features in our dataset move together. Does one feature increase\n",
    "# when another one does? Or does it decrease? Or maybe there's no clear pattern?\n",
    "# That's what we're looking to find out with the covariance matrix.\n",
    "\n",
    "# Variance - Think of variance as a way to measure how spread out a set of data is.\n",
    "# Imagine you're looking at the heights of a group of people. If everyone is about\n",
    "# the same height, the variance is low. But if there are some really tall and some\n",
    "# really short people, the variance is high. It's a way to quantify how much the data\n",
    "# varies from the average.\n",
    "\n",
    "# Covariance - Now, let's say we're looking at both the heights and weights of people.\n",
    "# Covariance tells us if taller people tend to be heavier (positive covariance), if\n",
    "# taller people tend to be lighter (negative covariance), or if there's no clear trend\n",
    "# between height and weight (zero covariance).\n",
    "\n",
    "# Computing the Covariance Matrix\n",
    "cov_matrix = np.cov(X_scaled.T)  # Here, we're calculating the covariance matrix for our standardized data.\n",
    "# This matrix will tell us the covariance between pairs of features. For example,\n",
    "# cov_matrix[i, j] tells us the covariance between feature i and feature j. If we look\n",
    "# at the diagonal of this matrix (where i equals j), we find the variance of each feature.\n",
    "\n",
    "# Visualizing the concept of variance and covariance (optional visualization not provided in code)\n",
    "# Imagine plotting your data on a graph, with one feature on the x-axis and another on the y-axis.\n",
    "# The spread of the data in the direction of each axis represents the variance, and how the data\n",
    "# stretches diagonally across the graph can give you an idea of the covariance. A positive slope\n",
    "# indicates positive covariance, a negative slope indicates negative covariance, and a cloud of\n",
    "# points with no discernible slope indicates zero covariance.\n",
    "\n",
    "# Note: The actual plotting code for variance and covariance visualization is not included here.\n",
    "# To visualize these concepts, you might plot individual features against each other and observe\n",
    "# the spread and directionality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33806475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Eigen Decomposition\n",
    "# After figuring out how features in our dataset move with each other through the covariance matrix,\n",
    "# the next big step is eigen decomposition. This might sound complicated, but it's about finding\n",
    "# special values and vectors from our covariance matrix that tell us a lot about our data.\n",
    "\n",
    "# Eigenvalues and Eigenvectors - What are they?\n",
    "# Imagine you have a shape made out of a flexible material, and you can stretch it in different directions.\n",
    "# Eigenvectors are directions in which this shape can be stretched or squished, while eigenvalues tell you\n",
    "# how much it stretches or squishes in those directions. In our data, eigenvectors point to directions of\n",
    "# maximum variance (where the data is most spread out), and eigenvalues tell us how spread out the data is\n",
    "# in these directions.\n",
    "\n",
    "# Why are they important?\n",
    "# Eigenvectors guide us to understand the principal components (the new axes of our dataset) after PCA.\n",
    "# These components are new ways to look at our data, where each step down in components shows us the next\n",
    "# best angle to view our data spread. Eigenvalues help us decide which principal components are most\n",
    "# important by showing the amount of variance captured by each component.\n",
    "\n",
    "eigen_values, eigen_vectors = np.linalg.eig(cov_matrix)  # Decompose the covariance matrix to find eigenvalues and eigenvectors\n",
    "\n",
    "# Visualization Suggestion:\n",
    "# To visualize eigenvectors and eigenvalues, imagine plotting your standardized data on a graph. Draw an\n",
    "# arrow from the center of this plot in the direction where the data spreads out the most. This arrow is\n",
    "# your first eigenvector. The length of the arrow represents the eigenvalue - a longer arrow means more\n",
    "# variance in that direction. The second arrow (eigenvector) is drawn perpendicular to the first, again\n",
    "# showing another direction of data spread but less so than the first. This helps us visualize the\n",
    "# multidimensional data spread in two dimensions, simplifying our understanding of its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aebfbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the covariance matrix and perform eigen decomposition\n",
    "cov_matrix = np.cov(X_scaled.T)\n",
    "eigen_values, eigen_vectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Plot the standardized data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], alpha=0.7)\n",
    "\n",
    "# Calculate the mean of each feature for the origin of eigenvectors\n",
    "mean_vec = np.mean(X_scaled, axis=0)\n",
    "\n",
    "# Plot the eigenvectors\n",
    "for i in range(len(eigen_values)):\n",
    "    # Eigenvalues scale the eigenvectors (for visualization purposes)\n",
    "    vec_end = mean_vec + eigen_vectors[:, i] * 2 * np.sqrt(eigen_values[i])\n",
    "    \n",
    "    plt.annotate('', xy=vec_end, xytext=mean_vec,\n",
    "                 arrowprops=dict(facecolor='red', width=1.5, headwidth=7))\n",
    "    plt.text(vec_end[0], vec_end[1], f'PC{i+1}', color='red', fontsize=12)\n",
    "\n",
    "plt.xlabel('Feature 1 (standardized)')\n",
    "plt.ylabel('Feature 2 (standardized)')\n",
    "plt.title('Iris dataset with Eigenvectors')\n",
    "plt.grid(True)\n",
    "plt.axis('equal')  # Equal scaling for both axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2494192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Sort Eigenvalues and Eigenvectors\n",
    "# After obtaining the eigenvalues and eigenvectors from the eigen decomposition,\n",
    "# it's crucial to sort them to identify which principal components (directions of\n",
    "# maximum variance in the data) hold the most information.\n",
    "\n",
    "# Sorting eigenvalues and their corresponding eigenvectors by decreasing eigenvalues\n",
    "# helps us prioritize the components by importance. This ranking is essential because\n",
    "# it tells us which directions (eigenvectors) in the data capture the most variance,\n",
    "# and hence, the most information about how the data is distributed.\n",
    "\n",
    "eigen_values_sorted_indices = np.argsort(eigen_values)[::-1]  # Get indices of eigenvalues sorted in descending order\n",
    "eigen_values_sorted = eigen_values[eigen_values_sorted_indices]  # Sort the eigenvalues by these indices\n",
    "eigen_vectors_sorted = eigen_vectors[:, eigen_values_sorted_indices]  # Sort the eigenvectors by the same indices\n",
    "\n",
    "# Why Sort and Choose Principal Components?\n",
    "# Principal components are new variables created from linear combinations of the original variables.\n",
    "# The goal is to reduce dimensions (features) while preserving as much information as possible.\n",
    "\n",
    "# The eigenvectors with the highest eigenvalues are the most significant. They capture the most\n",
    "# variance (information) in the data. In contrast, eigenvectors with the lowest eigenvalues carry\n",
    "# the least information about the data distribution and can be dropped to reduce dimensionality.\n",
    "\n",
    "# For example, if we have three features contributing to a dataset and calculate three eigenvalues,\n",
    "# dropping the one with the lowest eigenvalue reduces our dataset's dimensionality. This process\n",
    "# simplifies the dataset while retaining the most crucial information.\n",
    "\n",
    "# Multiplying the original standardized data by the chosen eigenvectors (corresponding to the top\n",
    "# eigenvalues) gives us the principal components. These principal components are the new features\n",
    "# of our reduced-dimensionality dataset, offering a simplified yet informative view of our data.\n",
    "\n",
    "# Example: If our dataset includes features like age, height, and weight, and after PCA, we find that\n",
    "# height and weight have the highest eigenvalues, we might drop age from our analysis. By using the\n",
    "# eigenvectors associated with height and weight, we can construct principal components that best\n",
    "# represent the variance in our dataset, focusing on the most informative aspects of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0100a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_values_sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a175e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_values_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9411992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_vectors_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd9e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Select the Top Principal Components\n",
    "# After sorting the eigenvalues and eigenvectors, the next crucial step is to decide\n",
    "# how many principal components we want to keep. This decision directly impacts the\n",
    "# dimensionality of the transformed data and the amount of variance (information) retained.\n",
    "\n",
    "# Choosing the Number of Components:\n",
    "# The choice of N (the number of principal components) is often guided by the goal of\n",
    "# retaining a substantial portion of the data's original variance. By selecting the top\n",
    "# N eigenvectors associated with the largest eigenvalues, we ensure that we capture the\n",
    "# most significant patterns and structures in the data.\n",
    "\n",
    "n_components = 3  # Here, we choose 2 for a specific reason.\n",
    "\n",
    "# The choice of 2 principal components is particularly motivated by the ease of visualization.\n",
    "# Reducing data to 2 dimensions allows us to plot it on a standard 2D graph, making it possible\n",
    "# to visually assess patterns, clusters, and relationships between data points. This visual\n",
    "# inspection can provide intuitive insights that are not easily obtained from high-dimensional data.\n",
    "\n",
    "top_eigen_vectors = eigen_vectors_sorted[:, :n_components]  # Select the top 2 eigenvectors\n",
    "\n",
    "# Why Two Components?\n",
    "# Selecting two components is a common practice when our primary interest is in visualizing the data.\n",
    "# However, for other applications, such as maximizing variance retained or simplifying datasets for\n",
    "# machine learning models, we might choose more components based on cumulative variance explained.\n",
    "\n",
    "# Visualizing the Choice:\n",
    "# Often, the decision on the number of components to retain is supported by a scree plot — a graph\n",
    "# that shows the fraction of total variance retained versus the number of components. A 'elbow' in\n",
    "# this plot can suggest a good balance between retaining variance and reducing dimensions.\n",
    "\n",
    "# Implementing PCA with two components allows us to transform our high-dimensional data into a\n",
    "# 2D space where we can explore and interpret the data's structure visually. It simplifies complex\n",
    "# datasets while preserving the essence of the information, facilitating both analysis and storytelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bfb825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Project Data Onto Lower-Dimensional Space\n",
    "# After identifying the top principal components (eigenvectors), the next step is to use them\n",
    "# to transform our original, high-dimensional data into a new space with fewer dimensions. This\n",
    "# new space is defined by the principal components we've chosen as the most significant.\n",
    "\n",
    "# Why Project the Data?\n",
    "# Projecting the data onto a lower-dimensional space helps us achieve several key objectives:\n",
    "# 1. **Simplification**: It simplifies the dataset, making it easier to work with, especially for visualization and analysis.\n",
    "# 2. **Information Preservation**: Despite reducing the number of dimensions, this process retains the most critical aspects of the original data, as captured by the principal components.\n",
    "# 3. **Noise Reduction**: By focusing on the directions of maximum variance, we also tend to filter out noise and less relevant information.\n",
    "\n",
    "# How Does Projection Work?\n",
    "# The projection is accomplished by multiplying the original standardized data (X_scaled) with the matrix of selected eigenvectors (top_eigen_vectors).\n",
    "# This operation essentially reorients the data from the original axes to the new axes defined by the principal components, resulting in a transformed dataset that highlights the most significant relationships and patterns.\n",
    "\n",
    "X_pca = np.dot(X_scaled, top_eigen_vectors)  # Perform the projection\n",
    "\n",
    "# The Result - What Do We Get?\n",
    "# The result of this projection is a new version of our dataset (X_pca) where each data point has been repositioned in a two-dimensional space (if we chose 2 components).\n",
    "# This 2D space is defined by the two most significant directions of variance in the original data, as identified by PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0595e2d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Step 7: Visualizing the PCA-reduced Data\n",
    "# Visualize the data in the new PCA-reduced 2D space\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=iris.target, palette='viridis')\n",
    "plt.title('PCA of Iris Dataset')\n",
    "plt.xlabel('Principal Component 1')  # The first principal component\n",
    "plt.ylabel('Principal Component 2')  # The second principal component\n",
    "plt.legend(title='Species', labels=iris.target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c29ff",
   "metadata": {},
   "source": [
    "## 5. Association Rules Mining\n",
    "\n",
    "In the world of data mining, Association Rules Mining is a powerful technique used to find hidden patterns and relationships within large datasets. Imagine you're at a grocery store analyzing shopping baskets to find items that are often bought together. This information can help in various decisions like product placement, promotions, and inventory management. Association Rules Mining does exactly this, but on a much larger scale and across diverse datasets.\n",
    "\n",
    "[Link association rules](https://www.youtube.com/watch?v=WGlMlS_Yydk&ab_channel=AugmentedAI)\n",
    "\n",
    "### 5.1 Foundation of Association Rules\n",
    "\n",
    "Association Rules Mining is based on discovering interesting relationships among variables in large databases. These relationships are not based on inherent properties of the data items themselves but are derived from the data as it is grouped together in transactions.\n",
    "\n",
    "#### Key Concepts and Metrics:\n",
    "\n",
    "1. **Support**: \n",
    "   - **What is it?** Support measures how frequently an item or itemset appears in the dataset. In our grocery store example, if 100 people bought milk out of 1000 transactions, the support for milk is 10%.\n",
    "   - **Why is it important?** Support helps us filter out less frequent itemsets from further analysis, focusing on more common and potentially significant patterns.\n",
    "\n",
    "2. **Confidence**:\n",
    "   - **What is it?** Confidence measures how often items in Y appear in transactions that contain X. If out of those who bought milk, 30 also bought cookies, the confidence of the rule {Milk -> Cookies} is 30%.\n",
    "   - **Why is it important?** Confidence indicates the reliability of the inference made by the rule. High confidence rules are more likely to be of interest because they represent strong associations.\n",
    "\n",
    "3. **Lift**:\n",
    "   - **What is it?** Lift measures how much more often X and Y occur together than we would expect if they were statistically independent. If milk and cookies are bought together three times more than the independence assumption would suggest, the lift is 3.\n",
    "   - **Why is it important?** Lift helps identify the strength of a rule over the baseline probability of the occurrence of items. A lift value greater than 1 indicates a positive association between X and Y.\n",
    "\n",
    "#### Significance in Data Mining:\n",
    "\n",
    "Association rules are significant in data mining because they reveal insights that can lead to actionable knowledge. For instance, understanding that milk and cookies are frequently purchased together can lead to targeted marketing strategies, better store layouts, and optimized stocking policies.\n",
    "\n",
    "These rules are not just limited to retail but can be applied across various domains such as web usage mining, intrusion detection, and bioinformatics, demonstrating the versatile utility of association rules in discovering meaningful patterns hidden within large datasets.\n",
    "\n",
    "[Association Rule link](https://medium.com/analytics-vidhya/association-rule-mining-concept-and-implementation-28443d16f611)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4918a3c",
   "metadata": {},
   "source": [
    "## 5.2 Apriori Algorithm in Action\n",
    "\n",
    "Imagine you're trying to find patterns in what people buy together at a supermarket. You might notice that people who buy bread often buy milk too. The Apriori algorithm helps us find these patterns but without having to check every single combination by hand. It's like having a smart assistant that tells you the most common shopping combos.\n",
    "\n",
    "### What is the Apriori Algorithm?\n",
    "\n",
    "The Apriori algorithm is a classic method used in data mining to uncover associations between items. It’s like playing a detective game where you look for items that often appear together in shoppers' baskets.\n",
    "\n",
    "#### How Does It Work?\n",
    "\n",
    "1. **Setting the Stage**: First, imagine every purchase at a store as a \"basket\" with various items. The Apriori algorithm looks through these baskets to find combinations of items that frequently occur together.\n",
    "\n",
    "2. **Finding Matches**: It starts with single items and gradually pairs them with others to see how often they're bought together. For example, it first notices milk is bought often, then bread, and starts looking at how often milk and bread are bought together.\n",
    "\n",
    "3. **Applying Rules**: The algorithm uses two main rules: \n",
    "   - **The frequency rule (support)**: If lots of people are buying milk, then milk is considered important.\n",
    "   - **The confidence rule**: If every time someone buys milk, they also buy bread, then the combination of milk and bread is significant.\n",
    "\n",
    "4. **Building Up**: From there, it keeps adding more items to the mix, like adding eggs to our milk and bread combo, always checking to make sure these new combos are still common enough.\n",
    "\n",
    "### Why Use the Apriori Algorithm?\n",
    "\n",
    "- **Efficiency**: It smartly reduces the number of combinations it needs to check, making it faster than looking at every possible combination.\n",
    "- **Simplicity**: Its steps are straightforward, making it easy to understand and apply, even for beginners.\n",
    "- **Insightful**: It can reveal surprising patterns in data that might not be obvious at first glance.\n",
    "\n",
    "### Real-World Application: Retail Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfb8fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Step 1: Prepare the Dataset\n",
    "# Let's simulate a small dataset of transactions in a retail setting\n",
    "transactions = [\n",
    "    ['Milk', 'Bread', 'Eggs'],\n",
    "    ['Milk', 'Bread'],\n",
    "    ['Bread', 'Eggs'],\n",
    "    ['Milk', 'Eggs'],\n",
    "    ['Bread', 'Eggs', 'Beer'],\n",
    "    ['Milk', 'Bread', 'Eggs', 'Beer']\n",
    "]\n",
    "\n",
    "# The TransactionEncoder transforms the dataset into a one-hot encoded DataFrame\n",
    "# Each column represents an item, and each row represents a transaction\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Step 2: Applying the Apriori Algorithm\n",
    "# Use the apriori function to find frequent itemsets\n",
    "# min_support is set to 0.5, meaning only itemsets appearing in at least 50% of transactions will be considered\n",
    "frequent_itemsets = apriori(df, min_support=0.5, use_colnames=True)\n",
    "\n",
    "# Step 3: Generating Association Rules\n",
    "# From the frequent itemsets, generate association rules using the association_rules function\n",
    "# Here, we're interested in rules with a minimum confidence of 0.7\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
    "\n",
    "# Step 4: Analyzing the Results\n",
    "# Display the rules found by the algorithm\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets)\n",
    "print(\"\\nAssociation Rules:\")\n",
    "print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])\n",
    "\n",
    "# Explanation:\n",
    "# The output includes frequent itemsets and association rules with their respective support, confidence, and lift.\n",
    "# - Support: The proportion of transactions that contain the itemset.\n",
    "# - Confidence: The likelihood that buying the antecedents leads to the consequents.\n",
    "# - Lift: The increase in the ratio of the sale of consequents when the antecedents are sold.\n",
    "\n",
    "# Real-World Application Insight:\n",
    "# For instance, if 'Milk' and 'Bread' appear frequently together in the rules with high confidence,\n",
    "# a retail manager might consider placing these items closer in the store or promoting them together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f029c0cc",
   "metadata": {},
   "source": [
    "## 6. Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9852c8b",
   "metadata": {},
   "source": [
    "#### What is Anomaly Detection?\n",
    "\n",
    "Anomaly detection refers to the identification of rare items, events, or observations that deviate significantly from the majority of the data. These outliers can indicate critical incidents such as fraud, network intrusions, malfunctioning equipment, or simply rare but significant data points.\n",
    "\n",
    "#### Why is Anomaly Detection Important?\n",
    "\n",
    "* Fraud Detection: Identifying unusual transactions that might indicate fraudulent activity.\n",
    "\n",
    "* Network Security: Detecting unauthorized access or unusual patterns in network traffic.\n",
    "\n",
    "* Quality Control: Identifying defects or irregularities in manufacturing processes.\n",
    "\n",
    "* Medical Diagnosis: Detecting rare diseases or abnormalities in medical images.\n",
    "\n",
    "#### Techniques for Anomaly Detection\n",
    "\n",
    "**Statistical Methods:**\n",
    "\n",
    "- Z-Score: Measures how many standard deviations an element is from the mean.\n",
    "\n",
    "- IQR (Interquartile Range): Identifies outliers based on the spread of the middle 50% of data.\n",
    "\n",
    "- Gaussian Mixture Models (GMM): Uses probabilistic models to identify anomalies.\n",
    "\n",
    "**Proximity-Based Methods:**\n",
    "\n",
    "- k-Nearest Neighbors (k-NN): Anomalies are data points that have a sparse neighborhood.\n",
    "\n",
    "- Density-Based Spatial Clustering of Applications with Noise (DBSCAN): Identifies points in low-density regions as anomalies.\n",
    "\n",
    "**Clustering-Based Methods:**\n",
    "\n",
    "- k-Means: Points far from any cluster centroid are considered anomalies.\n",
    "\n",
    "- Hierarchical Clustering: Dendrogram analysis can help in spotting outliers.\n",
    "\n",
    "**Machine Learning Methods:**\n",
    "\n",
    "- Isolation Forest: Constructs decision trees to isolate anomalies.\n",
    "\n",
    "- One-Class SVM: Learns a decision function for a single class and identifies points that do not conform.\n",
    "\n",
    "**Neural Networks:**\n",
    "\n",
    "- Autoencoders: Learn to compress and reconstruct data; high reconstruction error indicates anomalies.\n",
    "\n",
    "- Recurrent Neural Networks (RNN): Useful for sequential data where anomalies disrupt normal patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414a69b7",
   "metadata": {},
   "source": [
    "### 6.1 Advanced Anomaly Detection Techniques\n",
    "\n",
    "#### Isolation Forest:\n",
    "\n",
    "* Principle: Anomalies are few and different. Isolation Forest isolates observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.\n",
    "\n",
    "* Process:\n",
    "    1. Construct multiple random decision trees.\n",
    "\n",
    "    2. Calculate the average path length of each observation to the root.\n",
    "\n",
    "    3. Shorter paths indicate anomalies.\n",
    "\n",
    "* Advantages: Efficient for large datasets, works well with high-dimensional data.\n",
    "\n",
    "#### One-Class SVM:\n",
    "\n",
    "* Principle: SVM (Support Vector Machine) constructs a boundary that encompasses the majority of the data points in the feature space.\n",
    "\n",
    "* Process:\n",
    "    1. Map the data into a high-dimensional feature space.\n",
    "\n",
    "    2. Find the maximum margin hyperplane that best separates the data from the origin.\n",
    "\n",
    "    3. Data points on one side of the hyperplane are considered normal, and those on the other side are anomalies.\n",
    "\n",
    "* Advantages: Effective in high-dimensional space, robust to the curse of dimensionality.\n",
    "\n",
    "### Practical Considerations\n",
    "\n",
    "- Choice of Method: Depends on the nature of the data, the type of anomalies expected, and the computational resources available.\n",
    "\n",
    "- Parameter Tuning: Methods like Isolation Forest and One-Class SVM require careful tuning of parameters such as the number of trees or the kernel parameters.\n",
    "\n",
    "- Evaluation: Use metrics like Precision, Recall, F1-Score, and AUC-ROC to evaluate the performance of anomaly detection models.\n",
    "\n",
    "#### Example Workflow\n",
    "1. Data Preprocessing: Clean and normalize data.\n",
    "\n",
    "2. Model Selection: Choose the appropriate anomaly detection method.\n",
    "\n",
    "3. Training: Fit the model to the training data.\n",
    "\n",
    "4. Evaluation: Assess model performance using validation data.\n",
    "\n",
    "5. Deployment: Apply the model to detect anomalies in new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d883cbb7",
   "metadata": {},
   "source": [
    "### 6.2 Practical Anomaly Detection in Python\n",
    "\n",
    "#### Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37acdddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "# Generate sample data\n",
    "X = np.random.rand(100, 2)\n",
    "\n",
    "# Introduce anomalies\n",
    "X = np.concatenate([X, np.array([[1.5, 1.5], [1.6, 1.6]])])\n",
    "\n",
    "# Train Isolation Forest\n",
    "model = IsolationForest(contamination=0.1)\n",
    "model.fit(X)\n",
    "\n",
    "# Predict anomalies\n",
    "predictions = model.predict(X)\n",
    "# -1 for anomalies, 1 for normal data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b30c52",
   "metadata": {},
   "source": [
    "#### One-Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "import numpy as np\n",
    "\n",
    "# Generate sample data\n",
    "X = np.random.rand(100, 2)\n",
    "\n",
    "# Introduce anomalies\n",
    "X = np.concatenate([X, np.array([[1.5, 1.5], [1.6, 1.6]])])\n",
    "\n",
    "# Train One-Class SVM\n",
    "model = OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n",
    "model.fit(X)\n",
    "\n",
    "# Predict anomalies\n",
    "predictions = model.predict(X)\n",
    "# -1 for anomalies, 1 for normal data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da9af9f",
   "metadata": {},
   "source": [
    "**Basically**: <p>\n",
    "Anomaly detection is a critical aspect of data analysis, especially in fields where identifying rare events is crucial. Understanding and applying different anomaly detection techniques, such as Isolation Forest and One-Class SVM, equips data scientists with the tools necessary to uncover significant outliers in their data. Through careful model selection, parameter tuning, and rigorous evaluation, effective anomaly detection systems can be developed to address various real-world challenges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
