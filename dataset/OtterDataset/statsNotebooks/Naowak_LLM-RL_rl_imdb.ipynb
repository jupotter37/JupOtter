{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset imdb (/Users/naowak/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "Loading cached processed dataset at /Users/naowak/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-c800109586d11f73.arrow\n",
      "Loading cached processed dataset at /Users/naowak/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-53146ae257fbea76.arrow\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from trl.core import LengthSampler\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "def build_dataset(model_name='gpt2', dataset_name=\"imdb\", input_min_text_length=2, input_max_text_length=8):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "    customize this function to train the model on its own dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (`str`):\n",
    "            The name of the dataset to be loaded.\n",
    "\n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            The dataloader for the dataset.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # load imdb with datasets\n",
    "    ds = load_dataset(dataset_name, split=\"train\")\n",
    "    ds = ds.rename_columns({\"text\": \"review\"})\n",
    "    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n",
    "\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "dataset = build_dataset()\n",
    "split_dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "test_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# uncomment to try on small dataset\n",
    "train_dataset = train_dataset.select(range(128))\n",
    "test_dataset = test_dataset.select(range(32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model=\"lvwerra/distilbert-imdb\", device='mps:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/trl/core.py:114: UserWarning: MPS: no support for int64 reduction ops, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:144.)\n",
      "  return (values * mask).sum() / mask.sum()\n",
      "1it [01:18, 78.56s/it]/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:1067: UserWarning: The average ratio of batch (10.32) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:1067: UserWarning: The average ratio of batch (10.29) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "2it [02:33, 76.54s/it]/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:1067: UserWarning: The average ratio of batch (42.03) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "3it [03:48, 75.93s/it]/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:1067: UserWarning: The average ratio of batch (15.25) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:1067: UserWarning: The average ratio of batch (14.24) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:1067: UserWarning: The average ratio of batch (14.45) exceeds threshold 10.00. Skipping batch.\n",
      "  warnings.warn(\n",
      "4it [05:03, 75.97s/it]\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import respond_to_batch\n",
    "\n",
    "# get models\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')\n",
    "model_ref = AutoModelForCausalLMWithValueHead.from_pretrained('gpt2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# initialize trainer\n",
    "ppo_config = PPOConfig(\n",
    "    batch_size=32,\n",
    "    learning_rate=1.41e-5,\n",
    ")\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "# create a ppo trainer\n",
    "ppo_trainer = PPOTrainer(ppo_config, model, model_ref, tokenizer, dataset=train_dataset, data_collator=collator)\n",
    "\n",
    "# stats\n",
    "stats = []\n",
    "\n",
    "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "\n",
    "    # Check batch size\n",
    "    if len(batch[\"input_ids\"]) != ppo_config.batch_size:\n",
    "        continue\n",
    "\n",
    "    # Respond to batch\n",
    "    query_tensors = [t.unsqueeze(0) for t in batch[\"input_ids\"]]\n",
    "    response_tensors = []\n",
    "    for query_tensor in query_tensors:\n",
    "        response_tensor = respond_to_batch(model, query_tensor)\n",
    "        response_tensors.append(response_tensor)\n",
    "    batch[\"response\"] = tokenizer.batch_decode([r[0] for r in response_tensors])\n",
    "\n",
    "    # Compute sentiment score\n",
    "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "    pipe_outputs = sentiment_pipe(texts, return_all_scores=True, function_to_apply=None, batch_size=16)\n",
    "    rewards = [torch.tensor(output[1][\"score\"]).to('mps:0') for output in pipe_outputs]\n",
    "\n",
    "    # Run PPO step\n",
    "    q = [t[0] for t in query_tensors]\n",
    "    r = [t[0] for t in response_tensors]\n",
    "    stats = ppo_trainer.step(q, r, rewards)\n",
    "    ppo_trainer.log_stats(stats, batch, rewards)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1/1 [00:53<00:00, 53.86s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>ref_response</th>\n",
       "      <th>new_response</th>\n",
       "      <th>ref_reward</th>\n",
       "      <th>new_reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Honestly,</td>\n",
       "      <td>I've never had say of a day where it wasn't i...</td>\n",
       "      <td>both Howard and Regression exist–especially a...</td>\n",
       "      <td>0.652503</td>\n",
       "      <td>0.867355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Despite its flaws, I enjoyed \"</td>\n",
       "      <td>Perfectly Silent\" as well as what it is able t...</td>\n",
       "      <td>Your Song Follows Enhance Chris Bach,\" itself ...</td>\n",
       "      <td>0.988401</td>\n",
       "      <td>0.979626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not only</td>\n",
       "      <td>is Obama's DACA accomplishment vindictive, it...</td>\n",
       "      <td>that. Randall Miller is familiar with his tac...</td>\n",
       "      <td>0.503242</td>\n",
       "      <td>0.728814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>- A</td>\n",
       "      <td>snapshot of... LAMP943 = avata-bin/tmp943src ...</td>\n",
       "      <td>. Aziz Zazi, Leander.\\n\\nHMMO (HEGELA)</td>\n",
       "      <td>0.567857</td>\n",
       "      <td>0.665662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is an excellent modern-</td>\n",
       "      <td>day classic that puts your understanding of re...</td>\n",
       "      <td>day template.com.\\n\\n\\nSorry about this beta. ...</td>\n",
       "      <td>0.994691</td>\n",
       "      <td>0.992607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This is one</td>\n",
       "      <td>discount code onSweden's SSME gameStream, and...</td>\n",
       "      <td>of the players contending with what is not cl...</td>\n",
       "      <td>0.813399</td>\n",
       "      <td>0.655944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>This may sound crazy to even</td>\n",
       "      <td>consider, but all those chem pangs that raced...</td>\n",
       "      <td>some who are bullish on the Lightning, but in...</td>\n",
       "      <td>0.027367</td>\n",
       "      <td>0.209181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Carly Pope</td>\n",
       "      <td>, \"Polyvinylska Weekae: A V first 'treatise' f...</td>\n",
       "      <td>,rich fund run by Abbott at an upmarket prices...</td>\n",
       "      <td>0.808470</td>\n",
       "      <td>0.150346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ernst Lub</td>\n",
       "      <td>btkiewicz-Lutter, Richard P. Connor and Stuart...</td>\n",
       "      <td>in and Kruchtel F. SanEpi asserts as criminali...</td>\n",
       "      <td>0.253696</td>\n",
       "      <td>0.729309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Here's the</td>\n",
       "      <td>thing: Most computers ran Tor when they were ...</td>\n",
       "      <td>current United States. This country's place i...</td>\n",
       "      <td>0.050270</td>\n",
       "      <td>0.904473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What can</td>\n",
       "      <td>be faster than input DOM bindings? In that ca...</td>\n",
       "      <td>be reported to you from this project, says on...</td>\n",
       "      <td>0.315391</td>\n",
       "      <td>0.778474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I'm</td>\n",
       "      <td>your son!\" says Harry with an indulgent smile...</td>\n",
       "      <td>not wrong.\\n\\nI was a disabled Chinese man in...</td>\n",
       "      <td>0.967341</td>\n",
       "      <td>0.510319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ghost Story,(</td>\n",
       "      <td>Microsoft): -- A rail-station near College Par...</td>\n",
       "      <td>Clifford's son) R Street's sac impression, Aqu...</td>\n",
       "      <td>0.559480</td>\n",
       "      <td>0.757448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>When we</td>\n",
       "      <td>see a Bachelor Party in Ohio… i think this co...</td>\n",
       "      <td>set the host level to one vast world, we inse...</td>\n",
       "      <td>0.637586</td>\n",
       "      <td>0.669959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I have</td>\n",
       "      <td>given my soul and so many lives to give the w...</td>\n",
       "      <td>been froglived but it would take me hours to ...</td>\n",
       "      <td>0.991256</td>\n",
       "      <td>0.967877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>While not quite as monstrous</td>\n",
       "      <td>as the original Battleborn?\\n\\nTotal carnage ...</td>\n",
       "      <td>, well-prepared swans of the epidemic had appa...</td>\n",
       "      <td>0.247193</td>\n",
       "      <td>0.401156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1st watched 2</td>\n",
       "      <td>nd watch\\n\\nOverall karma: 1768 / 55,352. That...</td>\n",
       "      <td>FTA down by 77 points (slowed down by either ...</td>\n",
       "      <td>0.851347</td>\n",
       "      <td>0.236592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I would rather</td>\n",
       "      <td>send in more good cider, but some of it is Ge...</td>\n",
       "      <td>rotiza is originated in fewer markets than th...</td>\n",
       "      <td>0.445571</td>\n",
       "      <td>0.383811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>For years I thought</td>\n",
       "      <td>that this picture would be an essential clue ...</td>\n",
       "      <td>that ruminations, praxis, secret traditions a...</td>\n",
       "      <td>0.901020</td>\n",
       "      <td>0.833060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>I am a kind person</td>\n",
       "      <td>. There is let up the basic purpose of fishing...</td>\n",
       "      <td>examining the records itself and preparing do...</td>\n",
       "      <td>0.958427</td>\n",
       "      <td>0.982412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>This is</td>\n",
       "      <td>likely what we are seeing.\"\\n\\nOpposing the i...</td>\n",
       "      <td>good signing for the state of Ohio. As Secret...</td>\n",
       "      <td>0.755085</td>\n",
       "      <td>0.962830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>I waited ages before seeing</td>\n",
       "      <td>the deadly weapon. Certainly my adrenaline ha...</td>\n",
       "      <td>this past third-day of films. More than 61.3 ...</td>\n",
       "      <td>0.916571</td>\n",
       "      <td>0.881143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>The Poverty Row</td>\n",
       "      <td>Conversation on Wednesday, 23 September, 2017...</td>\n",
       "      <td>\\n\\nBlair doesn't name the wealthy inside the ...</td>\n",
       "      <td>0.279158</td>\n",
       "      <td>0.256482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Terrible...just terrible</td>\n",
       "      <td>, bacon is pretty far goon candidate, she does...</td>\n",
       "      <td>\\n\\n1200 Mini Avalon gems far better than cupc...</td>\n",
       "      <td>0.005015</td>\n",
       "      <td>0.005380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I had</td>\n",
       "      <td>discovered their intentions, but knew little ...</td>\n",
       "      <td>found [a working group] suggested interviewin...</td>\n",
       "      <td>0.371770</td>\n",
       "      <td>0.747580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I rarely comment on</td>\n",
       "      <td>event data from BAE Systems, but I have seen ...</td>\n",
       "      <td>fantasy football. Out of my heart, I believe ...</td>\n",
       "      <td>0.413906</td>\n",
       "      <td>0.736036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Yes, indeed, it could</td>\n",
       "      <td>only be a virtual reality. There is no physic...</td>\n",
       "      <td>have been why red man would never have had a ...</td>\n",
       "      <td>0.216268</td>\n",
       "      <td>0.098974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>First off,</td>\n",
       "      <td>we REALLY need to clean up our HS system from...</td>\n",
       "      <td>just Count a Natural that never previously ex...</td>\n",
       "      <td>0.284835</td>\n",
       "      <td>0.431140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>WARNING:</td>\n",
       "      <td>install_sqlite3.exe Error symbols: Install-SQ...</td>\n",
       "      <td>Word merger errors. (Reported by many of us)\\...</td>\n",
       "      <td>0.168192</td>\n",
       "      <td>0.066333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>The best thing you can say</td>\n",
       "      <td>about Midnight King (or: Speedrunning), is th...</td>\n",
       "      <td>is, it's only natural to consolidate key enfo...</td>\n",
       "      <td>0.188474</td>\n",
       "      <td>0.219257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Ah, the 1970's</td>\n",
       "      <td>. Obviously health conditions get worse at the...</td>\n",
       "      <td>1980's. And a KNBC and BET could well have st...</td>\n",
       "      <td>0.798443</td>\n",
       "      <td>0.373891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Buck</td>\n",
       "      <td>ett, Matty Slugger/Diaz.... Injuries continue ...</td>\n",
       "      <td>36, father of Herbert H. Johnson, TV6. Genera...</td>\n",
       "      <td>0.234245</td>\n",
       "      <td>0.735218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             query  \\\n",
       "0                        Honestly,   \n",
       "1   Despite its flaws, I enjoyed \"   \n",
       "2                         Not only   \n",
       "3                              - A   \n",
       "4     This is an excellent modern-   \n",
       "5                      This is one   \n",
       "6     This may sound crazy to even   \n",
       "7                       Carly Pope   \n",
       "8                        Ernst Lub   \n",
       "9                       Here's the   \n",
       "10                        What can   \n",
       "11                             I'm   \n",
       "12                   Ghost Story,(   \n",
       "13                         When we   \n",
       "14                          I have   \n",
       "15    While not quite as monstrous   \n",
       "16                   1st watched 2   \n",
       "17                  I would rather   \n",
       "18             For years I thought   \n",
       "19              I am a kind person   \n",
       "20                         This is   \n",
       "21     I waited ages before seeing   \n",
       "22                 The Poverty Row   \n",
       "23        Terrible...just terrible   \n",
       "24                           I had   \n",
       "25             I rarely comment on   \n",
       "26           Yes, indeed, it could   \n",
       "27                      First off,   \n",
       "28                        WARNING:   \n",
       "29      The best thing you can say   \n",
       "30                  Ah, the 1970's   \n",
       "31                            Buck   \n",
       "\n",
       "                                         ref_response  \\\n",
       "0    I've never had say of a day where it wasn't i...   \n",
       "1   Perfectly Silent\" as well as what it is able t...   \n",
       "2    is Obama's DACA accomplishment vindictive, it...   \n",
       "3    snapshot of... LAMP943 = avata-bin/tmp943src ...   \n",
       "4   day classic that puts your understanding of re...   \n",
       "5    discount code onSweden's SSME gameStream, and...   \n",
       "6    consider, but all those chem pangs that raced...   \n",
       "7   , \"Polyvinylska Weekae: A V first 'treatise' f...   \n",
       "8   btkiewicz-Lutter, Richard P. Connor and Stuart...   \n",
       "9    thing: Most computers ran Tor when they were ...   \n",
       "10   be faster than input DOM bindings? In that ca...   \n",
       "11   your son!\" says Harry with an indulgent smile...   \n",
       "12  Microsoft): -- A rail-station near College Par...   \n",
       "13   see a Bachelor Party in Ohio… i think this co...   \n",
       "14   given my soul and so many lives to give the w...   \n",
       "15   as the original Battleborn?\\n\\nTotal carnage ...   \n",
       "16  nd watch\\n\\nOverall karma: 1768 / 55,352. That...   \n",
       "17   send in more good cider, but some of it is Ge...   \n",
       "18   that this picture would be an essential clue ...   \n",
       "19  . There is let up the basic purpose of fishing...   \n",
       "20   likely what we are seeing.\"\\n\\nOpposing the i...   \n",
       "21   the deadly weapon. Certainly my adrenaline ha...   \n",
       "22   Conversation on Wednesday, 23 September, 2017...   \n",
       "23  , bacon is pretty far goon candidate, she does...   \n",
       "24   discovered their intentions, but knew little ...   \n",
       "25   event data from BAE Systems, but I have seen ...   \n",
       "26   only be a virtual reality. There is no physic...   \n",
       "27   we REALLY need to clean up our HS system from...   \n",
       "28   install_sqlite3.exe Error symbols: Install-SQ...   \n",
       "29   about Midnight King (or: Speedrunning), is th...   \n",
       "30  . Obviously health conditions get worse at the...   \n",
       "31  ett, Matty Slugger/Diaz.... Injuries continue ...   \n",
       "\n",
       "                                         new_response  ref_reward  new_reward  \n",
       "0    both Howard and Regression exist–especially a...    0.652503    0.867355  \n",
       "1   Your Song Follows Enhance Chris Bach,\" itself ...    0.988401    0.979626  \n",
       "2    that. Randall Miller is familiar with his tac...    0.503242    0.728814  \n",
       "3              . Aziz Zazi, Leander.\\n\\nHMMO (HEGELA)    0.567857    0.665662  \n",
       "4   day template.com.\\n\\n\\nSorry about this beta. ...    0.994691    0.992607  \n",
       "5    of the players contending with what is not cl...    0.813399    0.655944  \n",
       "6    some who are bullish on the Lightning, but in...    0.027367    0.209181  \n",
       "7   ,rich fund run by Abbott at an upmarket prices...    0.808470    0.150346  \n",
       "8   in and Kruchtel F. SanEpi asserts as criminali...    0.253696    0.729309  \n",
       "9    current United States. This country's place i...    0.050270    0.904473  \n",
       "10   be reported to you from this project, says on...    0.315391    0.778474  \n",
       "11   not wrong.\\n\\nI was a disabled Chinese man in...    0.967341    0.510319  \n",
       "12  Clifford's son) R Street's sac impression, Aqu...    0.559480    0.757448  \n",
       "13   set the host level to one vast world, we inse...    0.637586    0.669959  \n",
       "14   been froglived but it would take me hours to ...    0.991256    0.967877  \n",
       "15  , well-prepared swans of the epidemic had appa...    0.247193    0.401156  \n",
       "16   FTA down by 77 points (slowed down by either ...    0.851347    0.236592  \n",
       "17   rotiza is originated in fewer markets than th...    0.445571    0.383811  \n",
       "18   that ruminations, praxis, secret traditions a...    0.901020    0.833060  \n",
       "19   examining the records itself and preparing do...    0.958427    0.982412  \n",
       "20   good signing for the state of Ohio. As Secret...    0.755085    0.962830  \n",
       "21   this past third-day of films. More than 61.3 ...    0.916571    0.881143  \n",
       "22  \\n\\nBlair doesn't name the wealthy inside the ...    0.279158    0.256482  \n",
       "23  \\n\\n1200 Mini Avalon gems far better than cupc...    0.005015    0.005380  \n",
       "24   found [a working group] suggested interviewin...    0.371770    0.747580  \n",
       "25   fantasy football. Out of my heart, I believe ...    0.413906    0.736036  \n",
       "26   have been why red man would never have had a ...    0.216268    0.098974  \n",
       "27   just Count a Natural that never previously ex...    0.284835    0.431140  \n",
       "28   Word merger errors. (Reported by many of us)\\...    0.168192    0.066333  \n",
       "29   is, it's only natural to consolidate key enfo...    0.188474    0.219257  \n",
       "30   1980's. And a KNBC and BET could well have st...    0.798443    0.373891  \n",
       "31   36, father of Herbert H. Johnson, TV6. Genera...    0.234245    0.735218  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "results = {\n",
    "    \"query\": [],\n",
    "    \"ref_response\": [],\n",
    "    \"new_response\": [],\n",
    "    \"ref_reward\": [],\n",
    "    \"new_reward\": [],\n",
    "}\n",
    "\n",
    "for i in tqdm(range(0, len(test_dataset), ppo_config.batch_size)):\n",
    "    \n",
    "    # Get batch\n",
    "    batch = test_dataset[i:i+ppo_config.batch_size]\n",
    "\n",
    "    # Check batch size\n",
    "    if (len(batch[\"input_ids\"]) != ppo_config.batch_size):\n",
    "        continue\n",
    "    \n",
    "    # Respond to batch with reference model\n",
    "    query_tensors = [t.unsqueeze(0).to('mps:0') for t in batch[\"input_ids\"]]\n",
    "    response_tensors = []\n",
    "    for query_tensor in query_tensors:\n",
    "        response_tensor = respond_to_batch(model_ref, query_tensor)\n",
    "        response_tensors.append(response_tensor)\n",
    "    ref_response = tokenizer.batch_decode([r[0] for r in response_tensors])\n",
    "\n",
    "    # Compute sentiment score for reference model\n",
    "    texts = [q + r for q, r in zip(batch[\"query\"], ref_response)]\n",
    "    pipe_outputs = sentiment_pipe(texts, return_all_scores=True, function_to_apply=None, batch_size=16)\n",
    "    ref_rewards = [torch.tensor(output[1][\"score\"]).to('mps:0') for output in pipe_outputs]\n",
    "\n",
    "\n",
    "    # Respond to batch with new model\n",
    "    query_tensors = [t.unsqueeze(0).to('mps:0') for t in batch[\"input_ids\"]]\n",
    "    response_tensors = []\n",
    "    for query_tensor in query_tensors:\n",
    "        response_tensor = respond_to_batch(model, query_tensor)\n",
    "        response_tensors.append(response_tensor)\n",
    "    new_response = tokenizer.batch_decode([r[0] for r in response_tensors])\n",
    "\n",
    "    # Compute sentiment score\n",
    "    texts = [q + r for q, r in zip(batch[\"query\"], new_response)]\n",
    "    pipe_outputs = sentiment_pipe(texts, return_all_scores=True, function_to_apply=None, batch_size=16)\n",
    "    new_rewards = [torch.tensor(output[1][\"score\"]).to('mps:0') for output in pipe_outputs]\n",
    "\n",
    "    # Add to results\n",
    "    results[\"query\"].extend(batch[\"query\"])\n",
    "    results[\"ref_response\"].extend(ref_response)\n",
    "    results[\"new_response\"].extend(new_response)\n",
    "    results[\"ref_reward\"].extend([r.item() for r in ref_rewards])\n",
    "    results[\"new_reward\"].extend([r.item() for r in new_rewards])\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5364521861629328\n",
      "0.5912090297642862\n"
     ]
    }
   ],
   "source": [
    "print(df_results.ref_reward.mean())\n",
    "print(df_results.new_reward.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PPOTrainer' object has no attribute 'stats'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m keys_of_interest:\n\u001b[1;32m     20\u001b[0m     data[key] \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 22\u001b[0m \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m ppo_trainer\u001b[39m.\u001b[39;49mstats:\n\u001b[1;32m     23\u001b[0m     \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m keys_of_interest:\n\u001b[1;32m     24\u001b[0m         \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m entry:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PPOTrainer' object has no attribute 'stats'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming stats is the list containing your training statistics\n",
    "# Example: stats = [dict1, dict2, dict3, ...]\n",
    "\n",
    "keys_of_interest = [\n",
    "    'ppo/loss/total',\n",
    "    'ppo/learning_rate',\n",
    "    'ppo/returns/mean',\n",
    "    #'ppo/val/error',\n",
    "    'ppo/time/ppo/optimizer_step',\n",
    "    'objective/entropy'\n",
    "    # add more keys that you are interested in\n",
    "]\n",
    "\n",
    "# Extract the data for the keys of interest\n",
    "data = {}\n",
    "for key in keys_of_interest:\n",
    "    data[key] = []\n",
    "\n",
    "for entry in ppo_trainer.stats:\n",
    "    for key in keys_of_interest:\n",
    "        if key in entry:\n",
    "            data[key].append(entry[key])\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for key, values in data.items():\n",
    "    plt.plot(values, label=key)\n",
    "\n",
    "plt.xlabel('Training Iteration')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Training Statistics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config': PPOConfig(task_name=None, model_name=None, steps=20000, learning_rate=1.41e-05, adap_kl_ctrl=True, init_kl_coef=0.2, kl_penalty='kl', target=6, horizon=10000, gamma=1, lam=0.95, cliprange=0.2, cliprange_value=0.2, vf_coef=0.1, batch_size=32, forward_batch_size=None, mini_batch_size=1, gradient_accumulation_steps=1, ppo_epochs=4, remove_unused_columns=True, log_with=None, tracker_kwargs={}, accelerator_kwargs={}, project_kwargs={}, tracker_project_name='trl', max_grad_norm=None, seed=0, optimize_cuda_cache=False, early_stopping=False, target_kl=0.1, push_to_hub_if_best_kwargs={}, compare_steps=1, ratio_threshold=10.0),\n",
       " 'accelerator': <accelerate.accelerator.Accelerator at 0x107a76dd0>,\n",
       " 'model': AutoModelForCausalLMWithValueHead(\n",
       "   (pretrained_model): GPT2LMHeadModel(\n",
       "     (transformer): GPT2Model(\n",
       "       (wte): Embedding(50257, 768)\n",
       "       (wpe): Embedding(1024, 768)\n",
       "       (drop): Dropout(p=0.1, inplace=False)\n",
       "       (h): ModuleList(\n",
       "         (0-11): 12 x GPT2Block(\n",
       "           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): GPT2Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): GPT2MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (act): NewGELUActivation()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "     (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "   )\n",
       "   (v_head): ValueHead(\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (summary): Linear(in_features=768, out_features=1, bias=True)\n",
       "     (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "   )\n",
       " ),\n",
       " 'is_encoder_decoder': False,\n",
       " 'is_peft_model': False,\n",
       " 'ref_model': AutoModelForCausalLMWithValueHead(\n",
       "   (pretrained_model): GPT2LMHeadModel(\n",
       "     (transformer): GPT2Model(\n",
       "       (wte): Embedding(50257, 768)\n",
       "       (wpe): Embedding(1024, 768)\n",
       "       (drop): Dropout(p=0.1, inplace=False)\n",
       "       (h): ModuleList(\n",
       "         (0-11): 12 x GPT2Block(\n",
       "           (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (attn): GPT2Attention(\n",
       "             (c_attn): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "             (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "           (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): GPT2MLP(\n",
       "             (c_fc): Conv1D()\n",
       "             (c_proj): Conv1D()\n",
       "             (act): NewGELUActivation()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "     (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "   )\n",
       "   (v_head): ValueHead(\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (summary): Linear(in_features=768, out_features=1, bias=True)\n",
       "     (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "   )\n",
       " ),\n",
       " 'tokenizer': GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),\n",
       " 'dataset': Dataset({\n",
       "     features: ['review', 'label', 'input_ids', 'query'],\n",
       "     num_rows: 128\n",
       " }),\n",
       " '_signature_columns': ['input_ids',\n",
       "  'past_key_values',\n",
       "  'attention_mask',\n",
       "  'kwargs',\n",
       "  'label',\n",
       "  'query',\n",
       "  'response'],\n",
       " 'dataloader': <accelerate.data_loader.DataLoaderShard at 0x169ed61d0>,\n",
       " 'data_collator': DataCollatorForLanguageModeling(tokenizer=GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True), mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt'),\n",
       " 'optimizer': AcceleratedOptimizer (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 1.41e-05\n",
       "     maximize: False\n",
       "     weight_decay: 0\n",
       " ),\n",
       " 'lr_scheduler': None,\n",
       " 'kl_ctl': <trl.trainer.utils.AdaptiveKLController at 0x28451dd50>,\n",
       " 'is_distributed': False,\n",
       " 'current_step': 0,\n",
       " 'current_device': device(type='mps')}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
