{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YYdtMW7eZjk"
      },
      "source": [
        "#14757 Homework 1  (150 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QRhbSQO7aHZ"
      },
      "source": [
        "## **Due:** Wednesday September 18 at 3pm ET / 12 noon PT\n",
        "\n",
        "## Submission Instructions\n",
        "\n",
        "*   Download your completed notebook by clicking File->Download .ipynb and submit it on Gradescope\n",
        "*   Check your submission on Gradescope to make sure that all your code, code output and written responses appear correctly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tdp9iNT17pPr"
      },
      "source": [
        "## Problem 1: Gradescope Autograder Placeholder (0 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JEUcMu_7qJH"
      },
      "source": [
        "Gradescope requires that problem 1 be autograded for code submissions, but there are no autograded problems. Continue to problem 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgPiRgPROYhy"
      },
      "source": [
        "## Problem 2: The Wrongful Conviction of Sally Clark (20 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT9ImalpOgQ5"
      },
      "source": [
        "Sally Clark was a British lawyer who was wrongly sentenced to life in prison in 1999 for the deaths of her two infant children. Her elder son Christopher died at age 11 weeks in December 1996 and her younger son Harry at 8 weeks in January 1998. At her trial, the defence argued that the deaths were due to sudden infant death syndrome (SIDS). Clark was convicted on the basis of testimony by pediatrician Sir Roy Meadow, who made the following argument:\n",
        "\n",
        "*   Hospital records show that the ratio of SIDS deaths to live births in affluent non-smoking families is about $\\frac{1}{8500}$. (A live birth is a birth in which a child is born alive; not a still birth.)\n",
        "*   The chance of two SIDS deaths occurring in the same family is about $\\frac{1}{8500^2} \\approx \\frac{1}{73000000}$.\n",
        "*   It is therefore extremely unlikely that Clark is innocent.\n",
        "\n",
        "As a result of this prosecution, Clark spent more than 3 years in prison and was finally exonerated in 2003 after it was determined that Meadow’s expert testimony was flawed. Two other women against whom Meadow provided expert testimony had their convictions overturned as well.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a--FKi6P7_1"
      },
      "source": [
        "**2.1** (10 pts) Identify a flaw in Meadow’s $\\frac{1}{73000000}$ figure.\n",
        "\n",
        "WRITE YOUR ANSWER HERE:\n",
        "The problem with Meadow's figure is that he assumes conditional independence meaning that the 2 infant deaths are uncorrelated such that the probability of 2x SIDS deaths in the same family is 1/8500. However, Meadows being a pediatrician should recognize that 2 children being afflicted with the same disease coming from the same family should not be asssumed as conditionally independent or at least investigate and prove that assumption. Therefore his assumption would fail thus rendering his figure completely incorrect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATt9DojYQaAH"
      },
      "source": [
        "**2.2** (10 pts) Even if we accept Meadow’s $\\frac{1}{73000000}$ calculation as valid, what is wrong with a juror interpreting it as the probability of Clark’s innocence?\n",
        "\n",
        "WRITE YOUR ANSWER HERE:\n",
        "Even if we accept Meadow's Number as valid, the juror should not be concerned with Meadow's Number as his calculation gives the probability of 2x SIDS deaths given Clark's innocence P(2x SIDS Deaths | Clark's Innocence). First and foremost, the juror should be concerned with the probability that Clark is innocent given the two natural infant deaths regardless of cause. It stands to reason that $P(2x SIDS Deaths | Clark's Innocence) != P(Innocence | 2x Natural Infant Deaths)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPofMMlMAigz"
      },
      "source": [
        "## Problem 3: Attacking a Multinomial Naive Bayes Spam Filter (80 pts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqY3xKbxkSV_"
      },
      "source": [
        "In this problem you will execute several attacks on a naive Bayes spam filter for text messages (SMS). Run the cell below to set up the spam filter. The code performs the following steps:\n",
        "\n",
        "*   Reading and displaying the data\n",
        "*   Splitting the data into training and testing sets\n",
        "*   Converting each text message into a vector of word counts, so that the training messages are represented as a single matrix, and so are the test messages\n",
        "*   Training the spam filter using the training set and evaluating it with respect to the testing set\n",
        "\n",
        "Helpful documentation:\n",
        "\n",
        "*   https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "*   https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
        "\n",
        "This implementation is adapted from source code here: https://github.com/rahulvasaikar/SMS-spam-Detection\n",
        "\n",
        "The original data is documented here: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c0uBdoRUln_w"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATASET\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>message</th>\n",
              "      <th>binary_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>spam</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>ham</td>\n",
              "      <td>Will ü b going to esplanade fr home?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>ham</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>ham</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>ham</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     label                                            message  binary_label\n",
              "0      ham  Go until jurong point, crazy.. Available only ...             0\n",
              "1      ham                      Ok lar... Joking wif u oni...             0\n",
              "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...             1\n",
              "3      ham  U dun say so early hor... U c already then say...             0\n",
              "4      ham  Nah I don't think he goes to usf, he lives aro...             0\n",
              "...    ...                                                ...           ...\n",
              "5567  spam  This is the 2nd time we have tried 2 contact u...             1\n",
              "5568   ham               Will ü b going to esplanade fr home?             0\n",
              "5569   ham  Pity, * was in mood for that. So...any other s...             0\n",
              "5570   ham  The guy did some bitching but I acted like i'd...             0\n",
              "5571   ham                         Rofl. Its true to its name             0\n",
              "\n",
              "[5572 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "DATA SPLIT STATISTICS\n",
            "Number of rows in the total set:    5572\n",
            "Number of rows in the training set: 4179\n",
            "Number of rows in the test set:     1393\n",
            "\n",
            "PERFORMANCE\n",
            "Accuracy: 0.989\n",
            "Recall:   0.941\n"
          ]
        }
      ],
      "source": [
        "# READING AND DISPLAYING DATA\n",
        "\n",
        "# Reading tab separated text file into a Pandas dataframe\n",
        "import pandas as pd\n",
        "url = 'http://www.andrew.cmu.edu/user/dvaroday/14757/data/hw1/SMSSpamCollection'\n",
        "df = pd.read_table(url, sep='\\t', names=['label', 'message'])\n",
        "\n",
        "# Mapping ham/spam labels to 0/1 by creating a new column and using the .map() method to determine the matching map values\n",
        "df['binary_label'] = df.label.map({'ham':0,'spam':1})\n",
        "\n",
        "# Display dataframe\n",
        "print('DATASET')\n",
        "display(df)\n",
        "\n",
        "# SPLITTING INTO TRAINING AND TESTING SETS\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['message'],\n",
        "                                        df['binary_label'], random_state=1)\n",
        "\n",
        "# 2D dataframe shape prints in (num_rows, num_columns)\n",
        "total_rows = df.shape[0]\n",
        "training_rows = X_train.shape[0]\n",
        "test_rows = X_test.shape[0]\n",
        "\n",
        "# I would like to see a percent here, but don't wanna mess it up\n",
        "print('\\nDATA SPLIT STATISTICS')\n",
        "print('Number of rows in the total set:    {}'.format(total_rows))\n",
        "print('Number of rows in the training set: {}'.format(training_rows))\n",
        "print('Number of rows in the test set:     {}'.format(test_rows))\n",
        "\n",
        "# CONVERTING EACH TEXT MESSAGE INTO A VECTOR OF WORD COUNTS\n",
        "'''Find out if this is TF-IDF or likely just straight word counts'''\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Instantiate the CountVectorizer\n",
        "count_vector = CountVectorizer()\n",
        "\n",
        "'''\n",
        "This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n",
        "\n",
        "If you do not provide an a-priori dictionary and you do not use an analyzer that does \n",
        "some kind of feature selection then the number of features will be equal to the vocabulary\n",
        "size found by analyzing the data.\n",
        "\n",
        "The \"feature counts\" are just be the words and values are associated\n",
        "amount of times that word occurs for a given label'''\n",
        "\n",
        "# Fit the training data\n",
        "# Returns matrix of feature counts in training text messages\n",
        "# Features are obtained from the training set itself\n",
        "training_data = count_vector.fit_transform(X_train)\n",
        "\n",
        "\n",
        "'''\n",
        "Why is fit_transform used for training data, but transform is used for testing data?\n",
        "\n",
        ".fit() - Learn a vocabulary dictionary of all tokens in the raw documents.\n",
        "\n",
        ".transform() - produces the document-term matrix\n",
        "\n",
        "So fit presumably applies some form of separate transformation on the feature vectors\n",
        "\n",
        "Take to different notebook and play with it\n",
        "'''\n",
        "\n",
        "# Transform testing data\n",
        "# Returns matrix of feature counts in test text messages\n",
        "# Note that the features are the ones obtained from the training set\n",
        "testing_data = count_vector.transform(X_test)\n",
        "'''Find out how it transforms the data, but should be labeled somewhere\n",
        "Example document-term matrix\n",
        "                Feature Vectors (aka words)\n",
        "Docs        Hey     Bro     Send    Money\n",
        "Doc-1       1       0       3           3\n",
        "Doc-2        1      2       1           0\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "# TRAINING AND EVALUATING MODEL\n",
        "\n",
        "# Train\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "# MultinomialNB with Laplacian smoothing (alpha=1.0)\n",
        "spam_filter = MultinomialNB(alpha=1.0)\n",
        "spam_filter.fit(training_data, y_train)\n",
        "'''\n",
        "Basically creating the Naive Bayes classifier function with .fit()\n",
        "with both training_data labels and the ham/spam labels. \n",
        "\n",
        "\n",
        "\n",
        "Need to break this down later, but find out what the data looks like\n",
        "after y_train labels are applied to document-term matrix\n",
        "'''\n",
        "\n",
        "# Evaluate\n",
        "predictions = spam_filter.predict(testing_data)\n",
        "from sklearn.metrics import accuracy_score, recall_score\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "recall = recall_score(y_test, predictions)\n",
        "\n",
        "print('\\nPERFORMANCE')\n",
        "print('Accuracy:', \"{:.3f}\".format(accuracy))\n",
        "print('Recall:  ', \"{:.3f}\".format(recall))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHA_NGh6l_lA"
      },
      "source": [
        "### Understanding baseline performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btlqwb6fT6qv"
      },
      "source": [
        "**3.1** (10 pts) In the cell above *accuracy* is the fraction of messages that are correctly classified by the spam filter. *Recall* is the fraction of true spam messages correctly classified as spam. But the message recipient cares about true ham messages that are classified as spam because these messages are filtered out by the spam filter. Neither accuracy nor recall address this issue directly. Instead, the metric *specificity* represents the fraction of true ham messages that the recipient will actually receive. Fix the code between the comments in the cell below to calculate specificity for the test set.\n",
        "\n",
        "https://en.wikipedia.org/wiki/Precision_and_recall\n",
        "\n",
        "https://en.wikipedia.org/wiki/Sensitivity_and_specificity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TFsMZ2d0vqjx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Specificity: 0.996\n"
          ]
        }
      ],
      "source": [
        "def specificity_score(y_test, predictions):\n",
        "  test_messages = len(y_test.values)\n",
        "  true_spams = sum(y_test.values)\n",
        "  positives = sum(predictions)\n",
        "  true_positives = sum(y_test.values * predictions)\n",
        "\n",
        "  # START EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "  '''\n",
        "  Sensitivity = TP/(TP + FN) aka probability of a positive test result GIVEN actual positive\n",
        "  Specificity = TN / (TN + FP) aka probabililty of a negative test result GIVEN actual negative\n",
        "\n",
        "  Think COVID\n",
        "\n",
        "  This is pretty weird though because even the referenced wiki link says true negative rate\n",
        "  But we don't have negatives here so, equation really isn't that different anyway\n",
        "\n",
        "  y_test from above is the actual values of the test data\n",
        "  predictions is the output from running the test data throught the classifier\n",
        "  predictions = spam_filter.predict(testing_data)\n",
        "\n",
        "  since Ham is 0 and Spam is 1, simple sum here suffices\n",
        "  true_spams = all actual spam messages\n",
        "  positives = all predicted positives positives\n",
        "  true_positives performs vector multiplication then sums because 1*1 = 1 and anything*0=0\n",
        "  so sum still works\n",
        "\n",
        "  Also, true positives is the spam classifier working. True Negatives is the number of messages\n",
        "  correctly classified as ham\n",
        "  '''\n",
        "  true_hams = test_messages - true_spams\n",
        "  # probably don't even need false positives plus true negatives values with true_hams values, but makes me feel better to do the full equation\n",
        "  negatives = len(predictions) - positives\n",
        "  false_positives = positives - true_positives \n",
        "  true_negatives = true_hams - false_positives\n",
        "  spec = true_negatives / (true_negatives + false_positives)\n",
        "\n",
        "  return spec\n",
        "  # END EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "specificity = specificity_score(y_test, predictions)\n",
        "print('Specificity:', \"{:.3f}\".format(specificity))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzaAb5ePmHbr"
      },
      "source": [
        "### Evasion attack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOOScxPGNZIX"
      },
      "source": [
        "**3.2** (10 pts) In this part you act as a spammer who wants to evades the spam filter. Edit `evasion_message` in the cell below so that its probability of being labeled spam becomes less than 1%. For full credit, `evasion_message` must convey the same meaning as `original_message`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fJv2uPnZ8iq7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probability that original message is spam:  0.999985\n",
            "Probability that evasion message is spam:   0.003253\n"
          ]
        }
      ],
      "source": [
        "original_message = 'Txt back to get your $1000 prize now'\n",
        "\n",
        "# EDIT THE MESSAGE BELOW - DON'T REMOVE THIS COMMENT\n",
        "evasion_message = 'Text me back now to receive the contest prize money that you are owed in the amount of one thousand dollars'\n",
        "\n",
        "# Probabilistic predictions of original_message and evasion_message\n",
        "df_evasion = pd.DataFrame([original_message, evasion_message],\n",
        "                          columns=['message'])\n",
        "message_data = count_vector.transform(df_evasion['message'])\n",
        "prediction_probabilities = spam_filter.predict_proba(message_data)\n",
        "\n",
        "print('Probability that original message is spam: ',\n",
        "      \"{:.6f}\".format(prediction_probabilities[0,1]))\n",
        "print('Probability that evasion message is spam:  ',\n",
        "      \"{:.6f}\".format(prediction_probabilities[1,1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHMHe2rZ33Hi"
      },
      "source": [
        "**3.3** (10 pts) Describe the process you used to create your `evasion_message` in 1-2 sentences. Was this an open box (whitebox) or closed box (blackbox) attack? If open box, specify what information about `spam_filter` you used.\n",
        "\n",
        "WRITE YOUR ANSWER HERE: While we technically have access to the model, you can see that in the above cells that no data or specific weights were queried, and my approach to beating the spam filter was iterative despite one line evasion message. Therefore, this was a closed box attack as I didn't adjust my message based on prior knowledge of the decision boundary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybxd3mdKmPIj"
      },
      "source": [
        "### Poisoning attack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CUlek4uT_yP"
      },
      "source": [
        "**3.4** (10 pts) In this part you play the role of an adversary working against a ride share company. The company wants to deliver `target_message` to its customers. You gain access to the training set and have the opportunity to poison the training data with one poison message/label pair, but your example may be rejected by the spam filter administrator. Edit `poison_message` and/or `poison_label` so that they are accepted by the administrator and make the probability of `target_message` being labeled spam greater than 50%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MlGoee3Uw-e_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probability that target message is spam before poisoning: 0.037813\n",
            "Administrator accepts your poison message/label\n",
            "Probability that target message is spam after poisoning:  0.661127\n"
          ]
        }
      ],
      "source": [
        "target_message = 'Your ride share has arrived.'\n",
        "df_target = pd.DataFrame([target_message], columns=['message'])\n",
        "target_data = count_vector.transform(df_target['message'])\n",
        "target_probabilities = spam_filter.predict_proba(target_data)\n",
        "print('Probability that target message is spam before poisoning:',\n",
        "      \"{:.6f}\".format(target_probabilities[0,1]))\n",
        "\n",
        "# START EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "poison_message = 'Your ride share has arrived. Get your $1000 prize from the ride share'\n",
        "poison_label = 1\n",
        "# END EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "# Administrator runs poison message through original spam filter\n",
        "# to validate its label\n",
        "df_poison = pd.DataFrame([[poison_message, poison_label]],\n",
        "                         columns=['message', 'binary_label'])\n",
        "poison_data = count_vector.transform(df_poison['message'])\n",
        "poison_validation = spam_filter.predict(poison_data)\n",
        "\n",
        "if poison_label != poison_validation[0]:\n",
        "\n",
        "  print('Administrator rejects your poison message/label')\n",
        "\n",
        "else:\n",
        "\n",
        "  print('Administrator accepts your poison message/label')\n",
        "\n",
        "  # Initialize retraining data with original training data and append poison\n",
        "  df_retrain = pd.DataFrame(X_train, columns=['message'])\n",
        "  df_retrain['binary_label'] = y_train.values\n",
        "  df_retrain = pd.concat([df_retrain, df_poison], ignore_index=True)\n",
        "\n",
        "  # Retrain\n",
        "  recount_vector = CountVectorizer()\n",
        "  retraining_data = recount_vector.fit_transform(df_retrain['message'])\n",
        "  retrained_filter = MultinomialNB(alpha=1.0)\n",
        "  retrained_filter.fit(retraining_data, df_retrain['binary_label'])\n",
        "\n",
        "  # Evaluate target message with retrained_filter\n",
        "  recount_target_data = recount_vector.transform(df_target['message'])\n",
        "  poisoned_probabilities = retrained_filter.predict_proba(recount_target_data)\n",
        "\n",
        "  print('Probability that target message is spam after poisoning: ',\n",
        "        \"{:.6f}\".format(poisoned_probabilities[0,1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0H6C9ZmbHIm"
      },
      "source": [
        "**3.5** (10 pts) Describe an additional way for the administrator to decide whether to reject the poison message/label pair in order to defend against this type of attack. Use 1-4 sentences in your answer.\n",
        "\n",
        "WRITE YOUR ANSWER HERE:\n",
        "Assuming we are monitoring our classifier, we could put incoming messages through a less sophisticated filtering method to catch possible anomalies. We could compare the incoming messages' count vectors against the training data count vectors as a form of anomaly detection. Example: If you were receiving my potential spam message, you could compare it to other spam messages and see the $1000 token count of those messages as a clear indicator of spam in the training set and compare the rest of the count vectors. It's likely that there won't be many spam messages from the training set with similar 'ride' or 'share' counts, so it's an anomaly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbwFzD5JmSgL"
      },
      "source": [
        "### Reverse engineering attack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG0pKLgKC23l"
      },
      "source": [
        "**3.6** (20 pts) In this part you will perform a reverse engineering attack against the spam filter administrator. Your goal is to reconstruct the `spam_filter` model without using any of the original training messages or labels. In the starter code below the attacker sends a set of `probe_messages` to the `spam_filter`. The attacker can detect whether each message has been classified as spam or ham by inspecting its read receipt. The attacker uses these classifications as labels to train `reconstructed_filter`.\n",
        "\n",
        "Edit `probe_messages` so that `reconstructed_filter` achieves at least 97% accuracy and 88% recall on the test set. You can use at most `max_probes = 5000` probe messages. You may **not** use the training or test sets to help construct `probe_messages`, **except** for the list of words provided to you in the list `vocab`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ghUlVDXg176d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of probe messages: 5000\n",
            "Accuracy: 0.970\n",
            "Recall:   0.903\n"
          ]
        }
      ],
      "source": [
        "import numpy\n",
        "\n",
        "max_probes = 5000\n",
        "vocab = count_vector.get_feature_names_out()\n",
        "\n",
        "\n",
        "# START EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "# probe_messages = ['probe ' + str(i) for i in range(max_probes)] commented so I don't lose it\n",
        "'''\n",
        "I think it would be best to do this recursively, with getting probe messages through the filter\n",
        "for labels then data analysis on that to find more, organize the data by labels, then readjust\n",
        "from there, but I'm not sure if we're allowed to do that. \n",
        "'''\n",
        "# probe_messages = [str(vocab[i+1700]) for i in range(max_probes)] # simply using the probe messages from the vocab list gave me Accuracy: 0.952 and Recall: 0.876\n",
        "# print(probe_messages)\n",
        "\n",
        "'''\n",
        "Probe Table\n",
        "i:      Accuracy: 0.952 and Recall: 0.876\n",
        "i+500:  Accuracy: 0.958 and Recall: 0.865\n",
        "i+1000: Accuracy: 0.95 and Recall: 0.865 \n",
        "i+1500: Accuracy: 0.945 and Recall: 0.908 \n",
        "i+1700: Accuracy: 0.972 and Recall: 0.870\n",
        "i+2000: Accuracy: 0.966 and Recall: 0.843 \n",
        "i+2300: Accuracy: 0.976 and Recall: 0.838\n",
        "'''\n",
        "\n",
        "# probe_messages = [str(vocab[i+1500]) for i in range(max_probes)]\n",
        "# probe_messages = [str(vocab[i+1700]) for i in range(max_probes)]\n",
        "# # probe_messages = probe_messages1[1000:3500] + probe_messages2[:1200] + probe_messages2[3500:3800] # Accuracy: 0.933 and Recall: 0.87, but not maximizing labels\n",
        "# probe_messages = probes[:500] + probes[1000:3500] + + probes[3500:4800] # Accuracy: 0.926 and Recall: 0.903, but not maximizing labels\n",
        "\n",
        "\n",
        "'''\n",
        "abandoned binning approach because it simply lacked any form of control\n",
        "\n",
        "Couldn't figure out how to do it without ensuring that I had as many spam messages\n",
        "as possible which requires going through the classifier for at least one go round\n",
        "to figure out all actual spam messages and then going from there.\n",
        "'''\n",
        "all_probes = [str(vocab[i]) for i in range(len(vocab))]\n",
        "df_all_probes = pd.DataFrame(all_probes, columns=['message'])\n",
        "probe_all_data = count_vector.transform(df_all_probes['message'])\n",
        "probe_all_labels = spam_filter.predict(probe_all_data)\n",
        "# print(probe_all_labels)\n",
        "df_all_probes['label'] = probe_all_labels\n",
        "# print(len(spam_messages)) #783\n",
        "'''\n",
        "Took all of the spam convert to list and then take all of the ham, convert to list,\n",
        "then only include max_probes minus amount of spam for total probe_messages within max_probe count\n",
        "'''\n",
        "spam_messages = df_all_probes[df_all_probes['label'] == 1]['message'].tolist()\n",
        "ham_messages = df_all_probes[df_all_probes['label'] == 0]['message'].tolist()\n",
        "ham_messages = ham_messages[::-1]\n",
        "spam_messages_2_word = [spam_messages[i] + ' ' + ham_messages[i] for i, j in enumerate(spam_messages) if len(spam_messages) > 2]\n",
        "\n",
        "probe_messages = spam_messages + spam_messages_2_word + ham_messages[:(max_probes - len(spam_messages) - len(spam_messages_2_word))]\n",
        "\n",
        "# END EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "# Run probe_messages through spam_filter to obtain probe_labels\n",
        "'''\n",
        "Put probes into a dataframe send that dataframe through the transform() method to\n",
        "generate document-term matrix\n",
        "Then, create labels for the messages by running the probe data through the classifier\n",
        "'''\n",
        "print('Number of probe messages:', len(probe_messages))\n",
        "\n",
        "df_probes = pd.DataFrame(probe_messages, columns=['message'])\n",
        "probe_data = count_vector.transform(df_probes['message'])\n",
        "probe_labels = spam_filter.predict(probe_data)\n",
        "\n",
        "# Train reconstructed_filter using probe_messages and probe_labels\n",
        "'''\n",
        "Having run the classifier to create labels, we then create a new classifier\n",
        "using the probes and labels already created, then find their predictions\n",
        "'''\n",
        "reconstructed_filter = MultinomialNB(alpha=1.0)\n",
        "reconstructed_filter.fit(probe_data, probe_labels)\n",
        "reconstructed_predictions = reconstructed_filter.predict(testing_data)\n",
        "\n",
        "# Evaluate\n",
        "accuracy = accuracy_score(y_test, reconstructed_predictions)\n",
        "recall = recall_score(y_test, reconstructed_predictions)\n",
        "\n",
        "print('Accuracy:', \"{:.3f}\".format(accuracy))\n",
        "print('Recall:  ', \"{:.3f}\".format(recall))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esBEzGGIX8o2"
      },
      "source": [
        "**3.7** (10 pts) Describe one way that the administrator can defend against this type of attack. Use 1-4 sentences in your answer.\n",
        "\n",
        "WRITE YOUR ANSWER HERE:\n",
        "Honestly, I don't think there are many algorithmic ways to harden the classifer against this form of brute force attack other than simply limiting the number of messages that can pass through the administrator at a given time. So rate-limiting requests if there's an API as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGHknXEild0S"
      },
      "source": [
        "## Problem 4: Implementing a Multinomial Naive Bayes Classifier (50 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylq5ANqCJJ9Q"
      },
      "source": [
        "In this problem you will implement a Multinomial Naive Bayes classifier from scratch that performs identically to the one from the `sklearn.naive_bayes` library you used above. First run the cell below to load the preprocess the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Lzw9fLp0I3O0"
      },
      "outputs": [],
      "source": [
        "# Reading tab separated text file into a Pandas dataframe\n",
        "import pandas as pd\n",
        "url = 'http://www.andrew.cmu.edu/user/dvaroday/14757/data/hw1/SMSSpamCollection'\n",
        "df = pd.read_table(url, sep='\\t', names=['label', 'message'])\n",
        "\n",
        "# Mapping ham/spam labels to 0/1\n",
        "df['binary_label'] = df.label.map({'ham':0,'spam':1})\n",
        "\n",
        "# Splitting into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['message'],\n",
        "                                        df['binary_label'], random_state=1)\n",
        "\n",
        "# Converting each text message into a vector of word counts\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vector = CountVectorizer()\n",
        "training_data = count_vector.fit_transform(X_train)\n",
        "testing_data = count_vector.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lnMSilVmayD"
      },
      "source": [
        "In the cell below complete the class `MultinomialNB` by implementing the `fit()` and `predict()` methods. The constructor `__init__()` is written for you but you can modify it according to your needs.\n",
        "\n",
        "For full credit your implementation should not use any loops. Instead you should operate directly on vectors and matrices, not iterate through them. In particular your `fit()` method should calculate log-probability ratios with `np.log()` so that your `predict()` method can use `np.matmul()` for matrix multiplication. For your convenience the starter code already converts the method inputs into `numpy` arrays. Here's a guide to vectorization and broadcasting with `numpy`: https://realpython.com/numpy-array-programming\n",
        "\n",
        "Make the following assumptions:\n",
        "\n",
        "*   The classification task is binary. Specifically `train_labels` is a vector of values 0 and 1 only.\n",
        "*   The feature values in `train_features` and `test_features` are nonnegative integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eMVLzHIdsazP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "PERFORMANCE\n",
            "Accuracy: 0.989\n",
            "Recall:   0.941\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# START EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "class MultinomialNB:\n",
        "\n",
        "  def __init__(self, alpha):\n",
        "    self.alpha = alpha # Equal to 1.0 for Laplace smoothing\n",
        "    self.label_log_prob_ratios = None\n",
        "    self.feature_log_prob_ratios = None\n",
        "    self.label_counts = None\n",
        "\n",
        "  def fit(self, train_features, train_labels):\n",
        "    '''\n",
        "    train_features is the document-term sparse matrix and convertd to 2D numpy array (num_messages, num_features)\n",
        "    '''\n",
        "    train_features = np.array(train_features.todense())  # todense() converts the sparse matrix to the dense matrix\n",
        "    train_labels = np.array(train_labels)  # 1D array of spam:1 and ham:0, pretty simple\n",
        "    \n",
        "    '''\n",
        "    MultinomialNB is not always a binary classifier so here,\n",
        "    we could hardcode 0,1 here only, but that's limits the overall class\n",
        "    '''\n",
        "    # Calculate ratios\n",
        "    '''\n",
        "    label_counts creates a numpy array by finding the counts for each label in train_labels\n",
        "    and np.unique is a quick and robust way to get all the possible labels,\n",
        "    then list comprehension to find their sums aka counts\n",
        "    '''\n",
        "    num_labels = np.unique(train_labels)\n",
        "    label_counts = np.array([(train_labels == label).sum() for label in num_labels])\n",
        "    n_samples, n_features = train_features.shape\n",
        "    self.label_log_prob_ratios = np.log(label_counts / n_samples)\n",
        "    \n",
        "    '''\n",
        "    feature_counts creates matrix for each label of size (num_labels, num_features)\n",
        "\n",
        "    Use np.zeros to go ahead and create correctly sized matrix with 0 values for efficient memory\n",
        "    '''\n",
        "    feature_counts = np.zeros((len(num_labels), n_features))\n",
        "    \n",
        "    for idx, label in enumerate(num_labels):\n",
        "        # Sum of features (word counts) for all samples in class `c`\n",
        "        feature_counts[idx, :] = train_features[train_labels == label].sum(axis=0)\n",
        "    \n",
        "    \n",
        "    # Add in Laplace smoothing (alpha) then get the smoothed feature count totals of word count per label\n",
        "    smoothed_feature_counts = feature_counts + self.alpha\n",
        "    smoothed_class_totals = smoothed_feature_counts.sum(axis=1).reshape(-1, 1) \n",
        "    \n",
        "    # Find log of each feature given a label using the Laplace smoothed values\n",
        "    self.feature_log_prob_ratios = np.log(smoothed_feature_counts) - np.log(smoothed_class_totals)\n",
        "        \n",
        "  def predict(self, test_features):\n",
        "    '''\n",
        "    test_features is the document-term matrix for the test set\n",
        "    shape of test features is same as training set\n",
        "\n",
        "    \n",
        "    '''\n",
        "    # Convert to dense numpy array\n",
        "    test_features = np.array(test_features.todense())\n",
        "\n",
        "    # Find the Bayesian probability using the feature log probabilities transposed and label_logs\n",
        "    log_prob = np.matmul(test_features, self.feature_log_prob_ratios.T) + self.label_log_prob_ratios\n",
        "\n",
        "    # Return the max \n",
        "    return np.argmax(log_prob, axis=1)\n",
        "\n",
        "\n",
        "# END EDITING HERE - DON'T REMOVE THIS COMMENT\n",
        "\n",
        "### DO NOT EDIT THE CODE BELOW THIS LINE\n",
        "\n",
        "# Train\n",
        "# MultinomialNB with Laplacian smoothing (alpha=1.0)\n",
        "spam_filter = MultinomialNB(alpha=1.0)\n",
        "spam_filter.fit(training_data, y_train)\n",
        "\n",
        "# Evaluate\n",
        "predictions = spam_filter.predict(testing_data)\n",
        "from sklearn.metrics import accuracy_score, recall_score\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "recall = recall_score(y_test, predictions)\n",
        "print('\\nPERFORMANCE')\n",
        "print('Accuracy:', \"{:.3f}\".format(accuracy))\n",
        "print('Recall:  ', \"{:.3f}\".format(recall))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
