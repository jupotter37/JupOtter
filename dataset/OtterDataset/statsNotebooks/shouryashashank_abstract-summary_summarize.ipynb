{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing Text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.15.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 862031821031415827), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14247736103356357571), _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 3542614016, 6682711524476484399), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 17626999181137717918)]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "  devices = sess.list_devices()\n",
    "print (devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-1.0.3-cp37-cp37m-manylinux1_x86_64.whl (10.0 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/shourya/anaconda3/envs/tf15gpu/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/shourya/anaconda3/envs/tf15gpu/lib/python3.7/site-packages (from pandas) (1.18.1)\n",
      "Collecting pytz>=2017.2\n",
      "  Using cached pytz-2019.3-py2.py3-none-any.whl (509 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/shourya/anaconda3/envs/tf15gpu/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.0.3 pytz-2019.3\n"
     ]
    }
   ],
   "source": [
    "# ! pip install pandas\n",
    "! pip install pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insepcting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Summary</th>\n",
       "      <th>categories</th>\n",
       "      <th>Text</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>704.1291</td>\n",
       "      <td>projective hilbert space structures at excepti...</td>\n",
       "      <td>math-ph cond-mat.other math.mp quant-ph</td>\n",
       "      <td>a non-hermitian complex symmetric 2x2 matrix t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>705.1265</td>\n",
       "      <td>a noncommutative bohnenblust-spitzer identity ...</td>\n",
       "      <td>math.co hep-th math-ph math.mp math.ra</td>\n",
       "      <td>the bogoliubov recursion is a particular proce...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>705.4019</td>\n",
       "      <td>features of ion acceleration by circularly pol...</td>\n",
       "      <td>physics.plasm-ph</td>\n",
       "      <td>the characteristics of a mev ion source driven...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>706.0838</td>\n",
       "      <td>periodicity of ~155 days in solar electron flu...</td>\n",
       "      <td>astro-ph</td>\n",
       "      <td>in this paper we have investigated the occurre...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>706.1633</td>\n",
       "      <td>large attractive depletion interactions in sof...</td>\n",
       "      <td>cond-mat.soft cond-mat.stat-mech</td>\n",
       "      <td>we consider binary mixtures of soft repulsive ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                            Summary  \\\n",
       "0  704.1291  projective hilbert space structures at excepti...   \n",
       "1  705.1265  a noncommutative bohnenblust-spitzer identity ...   \n",
       "2  705.4019  features of ion acceleration by circularly pol...   \n",
       "3  706.0838  periodicity of ~155 days in solar electron flu...   \n",
       "4  706.1633  large attractive depletion interactions in sof...   \n",
       "\n",
       "                                categories  \\\n",
       "0  math-ph cond-mat.other math.mp quant-ph   \n",
       "1   math.co hep-th math-ph math.mp math.ra   \n",
       "2                         physics.plasm-ph   \n",
       "3                                 astro-ph   \n",
       "4         cond-mat.soft cond-mat.stat-mech   \n",
       "\n",
       "                                                Text Unnamed: 4 Unnamed: 5  \\\n",
       "0  a non-hermitian complex symmetric 2x2 matrix t...        NaN        NaN   \n",
       "1  the bogoliubov recursion is a particular proce...        NaN        NaN   \n",
       "2  the characteristics of a mev ion source driven...        NaN        NaN   \n",
       "3  in this paper we have investigated the occurre...        NaN        NaN   \n",
       "4  we consider binary mixtures of soft repulsive ...        NaN        NaN   \n",
       "\n",
       "  Unnamed: 6 Unnamed: 7  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "reviews = pd.read_csv(\"tr_physics.csv\") \n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review # 1\n",
      "projective hilbert space structures at exceptional points\n",
      "a non-hermitian complex symmetric 2x2 matrix toy model is used to study projective hilbert space structures in the vicinity of exceptional points (eps). the bi-orthogonal eigenvectors of a diagonalizable matrix are puiseux-expanded in terms of the root vectors at the ep. it is shown that the apparent contradiction between the two incompatible normalization conditions with finite and singular behavior in the ep-limit can be resolved by projectively extending the original hilbert space. the complementary normalization conditions correspond then to two different affine charts of this enlarged projective hilbert space. geometric phase and phase jump behavior are analyzed and the usefulness of the phase rigidity as measure for the distance to ep configurations is demonstrated. finally, ep-related aspects of pt-symmetrically extended quantum mechanics are discussed and a conjecture concerning the quantum brachistochrone problem is formulated.\n",
      "\n",
      "Review # 2\n",
      "a noncommutative bohnenblust-spitzer identity for rota-baxter algebras   solves bogoliubov's recursion\n",
      "the bogoliubov recursion is a particular procedure appearing in the process of renormalization in perturbative quantum field theory. it provides convergent expressions for otherwise divergent integrals. we develop here a theory of functional identities for noncommutative rota-baxter algebras which is shown to encode, among others, this process in the context of connes-kreimer's hopf algebra of renormalization. our results generalize the seminal cartier-rota theory of classical spitzer-type identities for commutative rota-baxter algebras. in the classical, commutative, case, these identities can be understood as deriving from the theory of symmetric functions. here, we show that an analogous property holds for noncommutative rota-baxter algebras. that is, we show that functional identities in the noncommutative setting can be derived from the theory of noncommutative symmetric functions. lie idempotents, and particularly the dynkin idempotent play a crucial role in the process. their action on the pro-unipotent groups such as those of perturbative renormalization is described in detail along the way.\n",
      "\n",
      "Review # 3\n",
      "features of ion acceleration by circularly polarized laser pulses\n",
      "the characteristics of a mev ion source driven by superintense, ultrashort laser pulses with circular polarization are studied by means of particle-in-cell simulations. predicted features include high efficiency, large ion density, low divergence and the possibility of femtosecond duration. a comparison with the case of linearly polarized pulses is made.\n",
      "\n",
      "Review # 4\n",
      "periodicity of ~155 days in solar electron fluence\n",
      "in this paper we have investigated the occurrence rate of high energetic(e>10 mev) solar electron flares measured by imp-8 spacecraft of nasa for solar cycle 21 (june, 1976 to august, 1986) first time by three different methods to detect periodicities accurately. power-spectrum analysis confirms a periodicity ~155 days which is in consistent with the result of chowdhury and ray (2006), that \"rieger periodicity\" was operated throughout the cycle 21 and it is independent on the energy of the electron fluxes.\n",
      "\n",
      "Review # 5\n",
      "large attractive depletion interactions in soft repulsive-sphere binary   mixtures\n",
      "we consider binary mixtures of soft repulsive spherical particles and calculate the depletion interaction between two big spheres mediated by the fluid of small spheres, using different theoretical and simulation methods. the validity of the theoretical approach, a virial expansion in terms of the density of the small spheres, is checked against simulation results. attention is given to the approach toward the hard-sphere limit, and to the effect of density and temperature on the strength of the depletion potential. our results indicate, surprisingly, that even a modest degree of softness in the pair potential governing the direct interactions between the particles may lead to a significantly more attractive total effective potential for the big spheres than in the hard-sphere case. this might lead to significant differences in phase behavior, structure and dynamics of a binary mixture of soft repulsive spheres. in particular, a perturbative scheme is applied to predict the phase diagram of an effective system of big spheres interacting via depletion forces for a size ratio of small and big spheres of 0.2; this diagram includes the usual fluid-solid transition but, in the soft-sphere case, the metastable fluid-fluid transition, which is probably absent in hard-sphere mixtures, is close to being stable with respect to direct fluid-solid coexistence. from these results the interesting possibility arises that, for sufficiently soft repulsive particles, this phase transition could become stable. possible implications for the phase behavior of real colloidal dispersions are discussed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspecting some of the reviews\n",
    "for i in range(5):\n",
    "    print(\"Review #\",i+1)\n",
    "    print(reviews.Summary[i])\n",
    "    print(reviews.Text[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove the stopwords from the texts because they do not provide much use for training our model. However, we will keep them for our summaries so that they sound more like natural phrases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries are complete.\n",
      "Texts are complete.\n"
     ]
    }
   ],
   "source": [
    "# Clean the summaries and texts\n",
    "clean_summaries = []\n",
    "for summary in reviews.Summary:\n",
    "    clean_summaries.append(clean_text(summary, remove_stopwords=False))\n",
    "print(\"Summaries are complete.\")\n",
    "\n",
    "clean_texts = []\n",
    "for text in reviews.Text:\n",
    "    clean_texts.append(clean_text(text))\n",
    "print(\"Texts are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/shourya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Review # 1\n",
      "projective hilbert space structures at exceptional points\n",
      "non hermitian complex symmetric 2x2 matrix toy model used study projective hilbert space structures vicinity exceptional points eps bi orthogonal eigenvectors diagonalizable matrix puiseux expanded terms root vectors ep shown apparent contradiction two incompatible normalization conditions finite singular behavior ep limit resolved projectively extending original hilbert space complementary normalization conditions correspond two different affine charts enlarged projective hilbert space geometric phase phase jump behavior analyzed usefulness phase rigidity measure distance ep configurations demonstrated finally ep related aspects pt symmetrically extended quantum mechanics discussed conjecture concerning quantum brachistochrone problem formulated\n",
      "\n",
      "Clean Review # 2\n",
      "a noncommutative bohnenblust spitzer identity for rota baxter algebras solves bogoliubov s recursion\n",
      "bogoliubov recursion particular procedure appearing process renormalization perturbative quantum field theory provides convergent expressions otherwise divergent integrals develop theory functional identities noncommutative rota baxter algebras shown encode among others process context connes kreimer hopf algebra renormalization results generalize seminal cartier rota theory classical spitzer type identities commutative rota baxter algebras classical commutative case identities understood deriving theory symmetric functions show analogous property holds noncommutative rota baxter algebras show functional identities noncommutative setting derived theory noncommutative symmetric functions lie idempotents particularly dynkin idempotent play crucial role process action pro unipotent groups perturbative renormalization described detail along way\n",
      "\n",
      "Clean Review # 3\n",
      "features of ion acceleration by circularly polarized laser pulses\n",
      "characteristics mev ion source driven superintense ultrashort laser pulses circular polarization studied means particle cell simulations predicted features include high efficiency large ion density low divergence possibility femtosecond duration comparison case linearly polarized pulses made\n",
      "\n",
      "Clean Review # 4\n",
      "periodicity of ~155 days in solar electron fluence\n",
      "paper investigated occurrence rate high energetic e>10 mev solar electron flares measured imp 8 spacecraft nasa solar cycle 21 june 1976 august 1986 first time three different methods detect periodicities accurately power spectrum analysis confirms periodicity ~155 days consistent result chowdhury ray 2006 rieger periodicity operated throughout cycle 21 independent energy electron fluxes\n",
      "\n",
      "Clean Review # 5\n",
      "large attractive depletion interactions in soft repulsive sphere binary mixtures\n",
      "consider binary mixtures soft repulsive spherical particles calculate depletion interaction two big spheres mediated fluid small spheres using different theoretical simulation methods validity theoretical approach virial expansion terms density small spheres checked simulation results attention given approach toward hard sphere limit effect density temperature strength depletion potential results indicate surprisingly even modest degree softness pair potential governing direct interactions particles may lead significantly attractive total effective potential big spheres hard sphere case might lead significant differences phase behavior structure dynamics binary mixture soft repulsive spheres particular perturbative scheme applied predict phase diagram effective system big spheres interacting via depletion forces size ratio small big spheres 0 2 diagram includes usual fluid solid transition soft sphere case metastable fluid fluid transition probably absent hard sphere mixtures close stable respect direct fluid solid coexistence results interesting possibility arises sufficiently soft repulsive particles phase transition could become stable possible implications phase behavior real colloidal dispersions discussed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the cleaned summaries and texts to ensure they have been cleaned well\n",
    "for i in range(5):\n",
    "    print(\"Clean Review #\",i+1)\n",
    "    print(clean_summaries[i])\n",
    "    print(clean_texts[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 109592\n"
     ]
    }
   ],
   "source": [
    "# Find the number of times each word was used and the size of the vocabulary\n",
    "word_counts = {}\n",
    "\n",
    "count_words(word_counts, clean_summaries)\n",
    "count_words(word_counts, clean_texts)\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 484557\n"
     ]
    }
   ],
   "source": [
    "# Load Conceptnet Numberbatch's (CN) embeddings, similar to GloVe, but probably better \n",
    "# (https://github.com/commonsense/conceptnet-numberbatch)\n",
    "embeddings_index = {}\n",
    "with open('numberbatch-en-17.02.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from CN: 2311\n",
      "Percent of words that are missing from vocabulary: 2.11%\n"
     ]
    }
   ],
   "source": [
    "# Find the number of words that are missing from CN, and are used more than our threshold.\n",
    "missing_words = 0\n",
    "threshold = 20\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from CN:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use a threshold of 20, so that words not in CN can be added to our word_embedding_matrix, but they need to be common enough in the reviews so that the model can understand their meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 109592\n",
      "Number of words we will use: 45714\n",
      "Percent of words we will use: 41.71%\n"
     ]
    }
   ],
   "source": [
    "# Limit the vocab that we will use to words that appear ≥ threshold or are in GloVe\n",
    "\n",
    "#dictionary to convert words to integers\n",
    "vocab_to_int = {} \n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1\n",
    "\n",
    "# Special tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45714\n"
     ]
    }
   ],
   "source": [
    "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "embedding_dim = 300\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in CN, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
    "    '''Convert words in text to an integer.\n",
    "       If word is not in vocab_to_int, use UNK's integer.\n",
    "       Total the number of words and UNKs.\n",
    "       Add EOS token to the end of texts'''\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "    return ints, word_count, unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 6355804\n",
      "Total number of UNKs in headlines: 153640\n",
      "Percent of words that are UNK: 2.42%\n"
     ]
    }
   ],
   "source": [
    "# Apply convert_to_ints to clean_summaries and clean_texts\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\n",
    "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lengths(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "             counts\n",
      "count  55521.000000\n",
      "mean      11.427514\n",
      "std        4.363274\n",
      "min        1.000000\n",
      "25%        8.000000\n",
      "50%       11.000000\n",
      "75%       14.000000\n",
      "max       50.000000\n",
      "\n",
      "Texts:\n",
      "             counts\n",
      "count  55521.000000\n",
      "mean     104.048162\n",
      "std       42.061018\n",
      "min        2.000000\n",
      "25%       73.000000\n",
      "50%       99.000000\n",
      "75%      132.000000\n",
      "max      333.000000\n"
     ]
    }
   ],
   "source": [
    "lengths_summaries = create_lengths(int_summaries)\n",
    "lengths_texts = create_lengths(int_texts)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_summaries.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165.0\n",
      "180.0\n",
      "203.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of texts\n",
    "print(np.percentile(lengths_texts.counts, 90))\n",
    "print(np.percentile(lengths_texts.counts, 95))\n",
    "print(np.percentile(lengths_texts.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.0\n",
      "19.0\n",
      "24.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of summaries\n",
    "print(np.percentile(lengths_summaries.counts, 90))\n",
    "print(np.percentile(lengths_summaries.counts, 95))\n",
    "print(np.percentile(lengths_summaries.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_counter(sentence):\n",
    "    '''Counts the number of time UNK appears in a sentence.'''\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11029\n",
      "11029\n"
     ]
    }
   ],
   "source": [
    "# Sort the summaries and texts by the length of the texts, shortest to longest\n",
    "# Limit the length of summaries and texts based on the min and max ranges.\n",
    "# Remove reviews that include too many UNKs\n",
    "\n",
    "sorted_summaries = []\n",
    "sorted_texts = []\n",
    "max_text_length = 84\n",
    "max_summary_length = 13\n",
    "min_length = 2\n",
    "unk_text_limit = 1\n",
    "unk_summary_limit = 0\n",
    "\n",
    "for length in range(min(lengths_texts.counts), max_text_length): \n",
    "    for count, words in enumerate(int_summaries):\n",
    "        if (len(int_summaries[count]) >= min_length and\n",
    "            len(int_summaries[count]) <= max_summary_length and\n",
    "            len(int_texts[count]) >= min_length and\n",
    "            unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
    "            unk_counter(int_texts[count]) <= unk_text_limit and\n",
    "            length == len(int_texts[count])\n",
    "           ):\n",
    "            sorted_summaries.append(int_summaries[count])\n",
    "            sorted_texts.append(int_texts[count])\n",
    "        \n",
    "# Compare lengths to ensure they match\n",
    "print(len(sorted_summaries))\n",
    "print(len(sorted_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    '''Create palceholders for inputs to the model'''\n",
    "    \n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
    "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
    "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    \n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
    "    '''Create the encoding layer'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                    cell_bw, \n",
    "                                                                    rnn_inputs,\n",
    "                                                                    sequence_length,\n",
    "                                                                    dtype=tf.float32)\n",
    "    # Join outputs since we are using a bidirectional RNN\n",
    "    enc_output = tf.concat(enc_output,2)\n",
    "    \n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
    "                            vocab_size, max_summary_length):\n",
    "    '''Create the training logits'''\n",
    "    \n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                        sequence_length=summary_length,\n",
    "                                                        time_major=False)\n",
    "\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                       training_helper,\n",
    "                                                       initial_state,\n",
    "                                                       output_layer) \n",
    "\n",
    "    training_logits, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                           output_time_major=False,\n",
    "                                                           impute_finished=True,\n",
    "                                                           maximum_iterations=max_summary_length)\n",
    "    return training_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
    "                             max_summary_length, batch_size):\n",
    "    '''Create the inference logits'''\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                end_token)\n",
    "                \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        initial_state,\n",
    "                                                        output_layer)\n",
    "                \n",
    "    inference_logits, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            output_time_major=False,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
    "#                    max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, inputs_length, targets_length, \n",
    "                   max_target_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
    "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
    "                                                     input_keep_prob = keep_prob)\n",
    "    \n",
    "    output_layer = Dense(vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                  enc_output,\n",
    "                                                  text_length,\n",
    "                                                  normalize=False,\n",
    "                                                  name='BahdanauAttention')\n",
    "\n",
    "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n",
    "                                                          attn_mech,\n",
    "                                                          rnn_size)\n",
    "            \n",
    "#     initial_state = tf.contrib.seq2seq.DynamicAttentionWrapperState(enc_state[0],\n",
    "#                                                                     _zero_state_tensors(rnn_size, \n",
    "#                                                                                         batch_size, \n",
    "#                                                                                         tf.float32)) \n",
    "    initial_state = dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state[0])\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "#         training_logits = training_decoding_layer(dec_embed_input, \n",
    "#                                                   summary_length, \n",
    "#                                                   dec_cell, \n",
    "#                                                   initial_state,\n",
    "#                                                   output_layer,\n",
    "#                                                   vocab_size, \n",
    "#                                                   max_summary_length)\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                            sequence_length=targets_length,\n",
    "                                                            time_major=False)\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                           training_helper,\n",
    "                                                           initial_state,\n",
    "                                                           output_layer) \n",
    "        training_logits, _ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                                  output_time_major=False,\n",
    "                                                                  impute_finished=True,\n",
    "                                                                  maximum_iterations=max_target_length)\n",
    "        \n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "#         inference_logits = inference_decoding_layer(embeddings,  \n",
    "#                                                     vocab_to_int['<GO>'], \n",
    "#                                                     vocab_to_int['<EOS>'],\n",
    "#                                                     dec_cell, \n",
    "#                                                     initial_state, \n",
    "#                                                     output_layer,\n",
    "#                                                     max_summary_length,\n",
    "#                                                     batch_size)\n",
    "        start_tokens = tf.tile(tf.constant([vocab_to_int['<GO>']], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "        end_token = (tf.constant(vocab_to_int['<EOS>'], dtype=tf.int32))\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                    start_tokens,\n",
    "                                                                    end_token)\n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                            inference_helper,\n",
    "                                                            initial_state,\n",
    "                                                            output_layer)\n",
    "        inference_logits, _ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                                   output_time_major=False,\n",
    "                                                                   impute_finished=True,\n",
    "                                                                   maximum_iterations=max_target_length)\n",
    "\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, inputs_length, targets_length, \n",
    "#                    max_target_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers, direction):\n",
    "#     '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
    "    \n",
    "#     with tf.name_scope(\"RNN_Decoder_Cell\"):\n",
    "#         for layer in range(num_layers):\n",
    "#             with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "#                 lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "#                 dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
    "#                                                          input_keep_prob = keep_prob)\n",
    "    \n",
    "#     output_layer = Dense(vocab_size,\n",
    "#                          kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "#     attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "#                                                   enc_output,\n",
    "#                                                   inputs_length,\n",
    "#                                                   normalize=False,\n",
    "#                                                   name='BahdanauAttention')\n",
    "    \n",
    "#     with tf.name_scope(\"Attention_Wrapper\"):\n",
    "#         dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n",
    "#                                                               attn_mech,\n",
    "#                                                               rnn_size)\n",
    "    \n",
    "#     initial_state =  dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state)\n",
    "# #     initial_state = tf.contrib.seq2seq.DynamicAttentionWrapperState(enc_state,\n",
    "# #                                                                     _zero_state_tensors(rnn_size, \n",
    "# #                                                                                         batch_size, \n",
    "# #                                                                                         tf.float32))\n",
    "\n",
    "#     with tf.variable_scope(\"decode\"):\n",
    "# #         training_logits = training_decoding_layer(dec_embed_input, \n",
    "# #                                                   targets_length, \n",
    "# #                                                   dec_cell, \n",
    "# #                                                   initial_state,\n",
    "# #                                                   output_layer,\n",
    "# #                                                   vocab_size, \n",
    "# #                                                   max_target_length)\n",
    "#         training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "#                                                             sequence_length=targets_length,\n",
    "#                                                             time_major=False)\n",
    "#         training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "#                                                            training_helper,\n",
    "#                                                            initial_state,\n",
    "#                                                            output_layer) \n",
    "#         training_logits, _ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "#                                             output_time_major=False,\n",
    "#                                             impute_finished=True,\n",
    "#                                             maximum_iterations=max_target_length)\n",
    "        \n",
    "#     with tf.variable_scope(\"decode\", reuse=True):\n",
    "# #         inference_logits = inference_decoding_layer(embeddings,  \n",
    "# #                                                     vocab_to_int['<GO>'], \n",
    "# #                                                     vocab_to_int['<EOS>'],\n",
    "# #                                                     dec_cell, \n",
    "# #                                                     initial_state, \n",
    "# #                                                     output_layer,\n",
    "# #                                                     max_target_length,\n",
    "# #                                                     batch_size)\n",
    "#         start_tokens = tf.tile(tf.constant([vocab_to_int['<GO>']], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "#         end_token = (tf.constant(vocab_to_int['<EOS>'], dtype=tf.int32))\n",
    "#         inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "#                                                                     start_tokens,\n",
    "#                                                                     end_token)\n",
    "#         inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "#                                                             inference_helper,\n",
    "#                                                             initial_state,\n",
    "#                                                             output_layer)\n",
    "#         inference_logits, _ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "#                                             output_time_major=False,\n",
    "#                                             impute_finished=True,\n",
    "#                                             maximum_iterations=max_target_length)\n",
    "\n",
    "#     return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
    "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
    "    embeddings = word_embedding_matrix\n",
    "    \n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
    "    \n",
    "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "    \n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
    "                                                        embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        text_length, \n",
    "                                                        summary_length, \n",
    "                                                        max_summary_length,\n",
    "                                                        rnn_size, \n",
    "                                                        vocab_to_int, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(summaries, texts, batch_size):\n",
    "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(texts)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        texts_batch = texts[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "        \n",
    "        pad_texts_lengths = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lengths.append(len(text))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-31-9390cb26606a>:7: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-31-9390cb26606a>:20: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/shourya/anaconda3/envs/tf15gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/shourya/anaconda3/envs/tf15gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /home/shourya/anaconda3/envs/tf15gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/shourya/anaconda3/envs/tf15gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Graph is built.\n"
     ]
    }
   ],
   "source": [
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      text_length,\n",
    "                                                      summary_length,\n",
    "                                                      max_summary_length,\n",
    "                                                      len(vocab_to_int)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      vocab_to_int,\n",
    "                                                      batch_size)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Graph is built.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I am training this model on my MacBook Pro, it would take me days if I used the whole dataset. For this reason, I am only going to use a subset of the data, so that I can train it over night. Normally I use [FloydHub's](https://www.floydhub.com/) services for my GPU needs, but it would take quite a bit of time to upload the dataset and ConceptNet Numberbatch, so I'm not going to bother with that for this project.\n",
    "\n",
    "I chose not use use the start of the subset because I didn't want to make it too easy for my model. The texts that I am using are closer to the median lengths; I thought this would be more fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shortest text length: 7\n",
      "The longest text length: 10\n"
     ]
    }
   ],
   "source": [
    "# Subset the data for training\n",
    "start = 200\n",
    "end = start + 50000\n",
    "sorted_summaries_short = sorted_summaries[start:end]\n",
    "sorted_texts_short = sorted_texts[start:end]\n",
    "print(\"The shortest text length:\", len(sorted_summaries_short[0]))\n",
    "print(\"The longest text length:\",len(sorted_summaries_short[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/100 Batch   20/169 - Loss:  7.151, Seconds: 4.31\n",
      "Epoch   1/100 Batch   40/169 - Loss:  5.143, Seconds: 4.64\n",
      "Average loss for this update: 5.912\n",
      "New Record!\n",
      "Epoch   1/100 Batch   60/169 - Loss:  5.293, Seconds: 4.83\n",
      "Epoch   1/100 Batch   80/169 - Loss:  5.200, Seconds: 5.00\n",
      "Epoch   1/100 Batch  100/169 - Loss:  5.226, Seconds: 5.11\n",
      "Average loss for this update: 5.241\n",
      "New Record!\n",
      "Epoch   1/100 Batch  120/169 - Loss:  5.240, Seconds: 5.30\n",
      "Epoch   1/100 Batch  140/169 - Loss:  5.224, Seconds: 5.51\n",
      "Epoch   1/100 Batch  160/169 - Loss:  5.157, Seconds: 5.60\n",
      "Average loss for this update: 5.175\n",
      "New Record!\n",
      "Epoch   2/100 Batch   20/169 - Loss:  4.736, Seconds: 4.33\n",
      "Epoch   2/100 Batch   40/169 - Loss:  4.544, Seconds: 4.63\n",
      "Average loss for this update: 4.663\n",
      "New Record!\n",
      "Epoch   2/100 Batch   60/169 - Loss:  4.738, Seconds: 4.81\n",
      "Epoch   2/100 Batch   80/169 - Loss:  4.686, Seconds: 5.03\n",
      "Epoch   2/100 Batch  100/169 - Loss:  4.751, Seconds: 5.12\n",
      "Average loss for this update: 4.746\n",
      "No Improvement.\n",
      "Epoch   2/100 Batch  120/169 - Loss:  4.784, Seconds: 5.29\n",
      "Epoch   2/100 Batch  140/169 - Loss:  4.791, Seconds: 5.42\n",
      "Epoch   2/100 Batch  160/169 - Loss:  4.753, Seconds: 5.65\n",
      "Average loss for this update: 4.757\n",
      "No Improvement.\n",
      "Epoch   3/100 Batch   20/169 - Loss:  4.479, Seconds: 4.34\n",
      "Epoch   3/100 Batch   40/169 - Loss:  4.292, Seconds: 4.65\n",
      "Average loss for this update: 4.403\n",
      "New Record!\n",
      "Epoch   3/100 Batch   60/169 - Loss:  4.459, Seconds: 4.83\n",
      "Epoch   3/100 Batch   80/169 - Loss:  4.406, Seconds: 4.94\n",
      "Epoch   3/100 Batch  100/169 - Loss:  4.467, Seconds: 5.17\n",
      "Average loss for this update: 4.456\n",
      "No Improvement.\n",
      "Epoch   3/100 Batch  120/169 - Loss:  4.467, Seconds: 5.34\n",
      "Epoch   3/100 Batch  140/169 - Loss:  4.478, Seconds: 5.48\n",
      "Epoch   3/100 Batch  160/169 - Loss:  4.453, Seconds: 5.63\n",
      "Average loss for this update: 4.449\n",
      "No Improvement.\n",
      "Epoch   4/100 Batch   20/169 - Loss:  4.239, Seconds: 4.33\n",
      "Epoch   4/100 Batch   40/169 - Loss:  4.055, Seconds: 4.63\n",
      "Average loss for this update: 4.172\n",
      "New Record!\n",
      "Epoch   4/100 Batch   60/169 - Loss:  4.248, Seconds: 4.81\n",
      "Epoch   4/100 Batch   80/169 - Loss:  4.172, Seconds: 5.01\n",
      "Epoch   4/100 Batch  100/169 - Loss:  4.248, Seconds: 5.23\n",
      "Average loss for this update: 4.236\n",
      "No Improvement.\n",
      "Epoch   4/100 Batch  120/169 - Loss:  4.259, Seconds: 5.29\n",
      "Epoch   4/100 Batch  140/169 - Loss:  4.264, Seconds: 5.48\n",
      "Epoch   4/100 Batch  160/169 - Loss:  4.235, Seconds: 5.65\n",
      "Average loss for this update: 4.233\n",
      "No Improvement.\n",
      "Epoch   5/100 Batch   20/169 - Loss:  4.035, Seconds: 4.33\n",
      "Epoch   5/100 Batch   40/169 - Loss:  3.870, Seconds: 4.59\n",
      "Average loss for this update: 3.977\n",
      "New Record!\n",
      "Epoch   5/100 Batch   60/169 - Loss:  4.049, Seconds: 4.87\n",
      "Epoch   5/100 Batch   80/169 - Loss:  3.962, Seconds: 5.00\n",
      "Epoch   5/100 Batch  100/169 - Loss:  4.023, Seconds: 5.15\n",
      "Average loss for this update: 4.023\n",
      "No Improvement.\n",
      "Epoch   5/100 Batch  120/169 - Loss:  4.056, Seconds: 5.37\n",
      "Epoch   5/100 Batch  140/169 - Loss:  4.064, Seconds: 5.53\n",
      "Epoch   5/100 Batch  160/169 - Loss:  4.046, Seconds: 5.59\n",
      "Average loss for this update: 4.037\n",
      "No Improvement.\n",
      "Epoch   6/100 Batch   20/169 - Loss:  3.854, Seconds: 4.41\n",
      "Epoch   6/100 Batch   40/169 - Loss:  3.673, Seconds: 4.59\n",
      "Average loss for this update: 3.788\n",
      "New Record!\n",
      "Epoch   6/100 Batch   60/169 - Loss:  3.865, Seconds: 4.82\n",
      "Epoch   6/100 Batch   80/169 - Loss:  3.742, Seconds: 4.98\n",
      "Epoch   6/100 Batch  100/169 - Loss:  3.851, Seconds: 5.14\n",
      "Average loss for this update: 3.831\n",
      "No Improvement.\n",
      "Epoch   6/100 Batch  120/169 - Loss:  3.876, Seconds: 5.38\n",
      "Epoch   6/100 Batch  140/169 - Loss:  3.870, Seconds: 5.50\n",
      "Epoch   6/100 Batch  160/169 - Loss:  3.842, Seconds: 5.64\n",
      "Average loss for this update: 3.842\n",
      "No Improvement.\n",
      "Epoch   7/100 Batch   20/169 - Loss:  3.649, Seconds: 4.33\n",
      "Epoch   7/100 Batch   40/169 - Loss:  3.506, Seconds: 4.61\n",
      "Average loss for this update: 3.6\n",
      "New Record!\n",
      "Epoch   7/100 Batch   60/169 - Loss:  3.667, Seconds: 4.96\n",
      "Epoch   7/100 Batch   80/169 - Loss:  3.555, Seconds: 4.99\n",
      "Epoch   7/100 Batch  100/169 - Loss:  3.660, Seconds: 5.16\n",
      "Average loss for this update: 3.638\n",
      "No Improvement.\n",
      "Epoch   7/100 Batch  120/169 - Loss:  3.690, Seconds: 5.28\n",
      "Epoch   7/100 Batch  140/169 - Loss:  3.700, Seconds: 5.47\n",
      "Epoch   7/100 Batch  160/169 - Loss:  3.649, Seconds: 5.58\n",
      "Average loss for this update: 3.665\n",
      "No Improvement.\n",
      "Epoch   8/100 Batch   20/169 - Loss:  3.438, Seconds: 4.38\n",
      "Epoch   8/100 Batch   40/169 - Loss:  3.336, Seconds: 4.56\n",
      "Average loss for this update: 3.407\n",
      "New Record!\n",
      "Epoch   8/100 Batch   60/169 - Loss:  3.467, Seconds: 4.85\n",
      "Epoch   8/100 Batch   80/169 - Loss:  3.364, Seconds: 4.95\n",
      "Epoch   8/100 Batch  100/169 - Loss:  3.480, Seconds: 5.24\n",
      "Average loss for this update: 3.445\n",
      "No Improvement.\n",
      "Epoch   8/100 Batch  120/169 - Loss:  3.483, Seconds: 5.38\n",
      "Epoch   8/100 Batch  140/169 - Loss:  3.507, Seconds: 5.50\n",
      "Epoch   8/100 Batch  160/169 - Loss:  3.474, Seconds: 5.62\n",
      "Average loss for this update: 3.474\n",
      "No Improvement.\n",
      "Epoch   9/100 Batch   20/169 - Loss:  3.245, Seconds: 4.33\n",
      "Epoch   9/100 Batch   40/169 - Loss:  3.156, Seconds: 4.58\n",
      "Average loss for this update: 3.228\n",
      "New Record!\n",
      "Epoch   9/100 Batch   60/169 - Loss:  3.308, Seconds: 4.82\n",
      "Epoch   9/100 Batch   80/169 - Loss:  3.182, Seconds: 5.00\n",
      "Epoch   9/100 Batch  100/169 - Loss:  3.301, Seconds: 5.13\n",
      "Average loss for this update: 3.264\n",
      "No Improvement.\n",
      "Epoch   9/100 Batch  120/169 - Loss:  3.289, Seconds: 5.36\n",
      "Epoch   9/100 Batch  140/169 - Loss:  3.324, Seconds: 5.43\n",
      "Epoch   9/100 Batch  160/169 - Loss:  3.286, Seconds: 5.59\n",
      "Average loss for this update: 3.291\n",
      "No Improvement.\n",
      "Epoch  10/100 Batch   20/169 - Loss:  3.067, Seconds: 4.33\n",
      "Epoch  10/100 Batch   40/169 - Loss:  2.942, Seconds: 4.65\n",
      "Average loss for this update: 3.035\n",
      "New Record!\n",
      "Epoch  10/100 Batch   60/169 - Loss:  3.107, Seconds: 4.81\n",
      "Epoch  10/100 Batch   80/169 - Loss:  3.009, Seconds: 5.02\n",
      "Epoch  10/100 Batch  100/169 - Loss:  3.117, Seconds: 5.13\n",
      "Average loss for this update: 3.079\n",
      "No Improvement.\n",
      "Epoch  10/100 Batch  120/169 - Loss:  3.098, Seconds: 5.27\n",
      "Epoch  10/100 Batch  140/169 - Loss:  3.149, Seconds: 5.45\n",
      "Epoch  10/100 Batch  160/169 - Loss:  3.139, Seconds: 5.58\n",
      "Average loss for this update: 3.122\n",
      "No Improvement.\n",
      "Epoch  11/100 Batch   20/169 - Loss:  2.894, Seconds: 4.39\n",
      "Epoch  11/100 Batch   40/169 - Loss:  2.813, Seconds: 4.61\n",
      "Average loss for this update: 2.872\n",
      "New Record!\n",
      "Epoch  11/100 Batch   60/169 - Loss:  2.920, Seconds: 4.79\n",
      "Epoch  11/100 Batch   80/169 - Loss:  2.862, Seconds: 4.94\n",
      "Epoch  11/100 Batch  100/169 - Loss:  2.945, Seconds: 5.16\n",
      "Average loss for this update: 2.92\n",
      "No Improvement.\n",
      "Epoch  11/100 Batch  120/169 - Loss:  2.939, Seconds: 5.29\n",
      "Epoch  11/100 Batch  140/169 - Loss:  2.993, Seconds: 5.43\n",
      "Epoch  11/100 Batch  160/169 - Loss:  2.972, Seconds: 5.61\n",
      "Average loss for this update: 2.961\n",
      "No Improvement.\n",
      "Epoch  12/100 Batch   20/169 - Loss:  2.751, Seconds: 4.33\n",
      "Epoch  12/100 Batch   40/169 - Loss:  2.657, Seconds: 4.64\n",
      "Average loss for this update: 2.722\n",
      "New Record!\n",
      "Epoch  12/100 Batch   60/169 - Loss:  2.774, Seconds: 4.76\n",
      "Epoch  12/100 Batch   80/169 - Loss:  2.694, Seconds: 5.67\n",
      "Epoch  12/100 Batch  100/169 - Loss:  2.806, Seconds: 5.10\n",
      "Average loss for this update: 2.768\n",
      "No Improvement.\n",
      "Epoch  12/100 Batch  120/169 - Loss:  2.799, Seconds: 5.22\n",
      "Epoch  12/100 Batch  140/169 - Loss:  2.822, Seconds: 5.41\n",
      "Epoch  12/100 Batch  160/169 - Loss:  2.805, Seconds: 5.58\n",
      "Average loss for this update: 2.801\n",
      "No Improvement.\n",
      "Epoch  13/100 Batch   20/169 - Loss:  2.600, Seconds: 4.27\n",
      "Epoch  13/100 Batch   40/169 - Loss:  2.541, Seconds: 4.77\n",
      "Average loss for this update: 2.593\n",
      "New Record!\n",
      "Epoch  13/100 Batch   60/169 - Loss:  2.654, Seconds: 4.70\n",
      "Epoch  13/100 Batch   80/169 - Loss:  2.574, Seconds: 5.03\n",
      "Epoch  13/100 Batch  100/169 - Loss:  2.656, Seconds: 5.17\n",
      "Average loss for this update: 2.632\n",
      "No Improvement.\n",
      "Epoch  13/100 Batch  120/169 - Loss:  2.661, Seconds: 5.35\n",
      "Epoch  13/100 Batch  140/169 - Loss:  2.692, Seconds: 5.42\n",
      "Epoch  13/100 Batch  160/169 - Loss:  2.663, Seconds: 5.63\n",
      "Average loss for this update: 2.666\n",
      "No Improvement.\n",
      "Epoch  14/100 Batch   20/169 - Loss:  2.468, Seconds: 4.31\n",
      "Epoch  14/100 Batch   40/169 - Loss:  2.406, Seconds: 4.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for this update: 2.465\n",
      "New Record!\n",
      "Epoch  14/100 Batch   60/169 - Loss:  2.542, Seconds: 4.78\n",
      "Epoch  14/100 Batch   80/169 - Loss:  2.462, Seconds: 4.94\n",
      "Epoch  14/100 Batch  100/169 - Loss:  2.534, Seconds: 5.15\n",
      "Average loss for this update: 2.514\n",
      "No Improvement.\n",
      "Epoch  14/100 Batch  120/169 - Loss:  2.547, Seconds: 5.31\n",
      "Epoch  14/100 Batch  140/169 - Loss:  2.579, Seconds: 5.43\n",
      "Epoch  14/100 Batch  160/169 - Loss:  2.566, Seconds: 5.57\n",
      "Average loss for this update: 2.564\n",
      "No Improvement.\n",
      "Epoch  15/100 Batch   20/169 - Loss:  2.349, Seconds: 4.35\n",
      "Epoch  15/100 Batch   40/169 - Loss:  2.305, Seconds: 4.60\n",
      "Average loss for this update: 2.351\n",
      "New Record!\n",
      "Epoch  15/100 Batch   60/169 - Loss:  2.426, Seconds: 4.86\n",
      "Epoch  15/100 Batch   80/169 - Loss:  2.327, Seconds: 4.98\n",
      "Epoch  15/100 Batch  100/169 - Loss:  2.433, Seconds: 5.13\n",
      "Average loss for this update: 2.4\n",
      "No Improvement.\n",
      "Epoch  15/100 Batch  120/169 - Loss:  2.444, Seconds: 5.36\n",
      "Epoch  15/100 Batch  140/169 - Loss:  2.467, Seconds: 5.45\n",
      "Epoch  15/100 Batch  160/169 - Loss:  2.432, Seconds: 5.57\n",
      "Average loss for this update: 2.449\n",
      "No Improvement.\n",
      "Epoch  16/100 Batch   20/169 - Loss:  2.243, Seconds: 4.32\n",
      "Epoch  16/100 Batch   40/169 - Loss:  2.206, Seconds: 4.62\n",
      "Average loss for this update: 2.249\n",
      "New Record!\n",
      "Epoch  16/100 Batch   60/169 - Loss:  2.318, Seconds: 4.88\n",
      "Epoch  16/100 Batch   80/169 - Loss:  2.220, Seconds: 5.05\n",
      "Epoch  16/100 Batch  100/169 - Loss:  2.310, Seconds: 5.14\n",
      "Average loss for this update: 2.289\n",
      "No Improvement.\n",
      "Epoch  16/100 Batch  120/169 - Loss:  2.333, Seconds: 5.27\n",
      "Epoch  16/100 Batch  140/169 - Loss:  2.354, Seconds: 5.47\n",
      "Epoch  16/100 Batch  160/169 - Loss:  2.324, Seconds: 5.58\n",
      "Average loss for this update: 2.331\n",
      "No Improvement.\n",
      "Epoch  17/100 Batch   20/169 - Loss:  2.157, Seconds: 4.34\n",
      "Epoch  17/100 Batch   40/169 - Loss:  2.123, Seconds: 4.62\n",
      "Average loss for this update: 2.158\n",
      "New Record!\n",
      "Epoch  17/100 Batch   60/169 - Loss:  2.213, Seconds: 4.76\n",
      "Epoch  17/100 Batch   80/169 - Loss:  2.133, Seconds: 4.94\n",
      "Epoch  17/100 Batch  100/169 - Loss:  2.228, Seconds: 5.15\n",
      "Average loss for this update: 2.204\n",
      "No Improvement.\n",
      "Epoch  17/100 Batch  120/169 - Loss:  2.239, Seconds: 5.31\n",
      "Epoch  17/100 Batch  140/169 - Loss:  2.256, Seconds: 5.42\n",
      "Epoch  17/100 Batch  160/169 - Loss:  2.234, Seconds: 5.55\n",
      "Average loss for this update: 2.237\n",
      "No Improvement.\n",
      "Epoch  18/100 Batch   20/169 - Loss:  2.068, Seconds: 4.33\n",
      "Epoch  18/100 Batch   40/169 - Loss:  2.027, Seconds: 4.60\n",
      "Average loss for this update: 2.073\n",
      "New Record!\n",
      "Epoch  18/100 Batch   60/169 - Loss:  2.142, Seconds: 4.82\n",
      "Epoch  18/100 Batch   80/169 - Loss:  2.064, Seconds: 4.97\n",
      "Epoch  18/100 Batch  100/169 - Loss:  2.120, Seconds: 5.11\n",
      "Average loss for this update: 2.111\n",
      "No Improvement.\n",
      "Epoch  18/100 Batch  120/169 - Loss:  2.139, Seconds: 5.29\n",
      "Epoch  18/100 Batch  140/169 - Loss:  2.148, Seconds: 5.46\n",
      "Epoch  18/100 Batch  160/169 - Loss:  2.119, Seconds: 5.58\n",
      "Average loss for this update: 2.129\n",
      "No Improvement.\n",
      "Epoch  19/100 Batch   20/169 - Loss:  1.982, Seconds: 4.33\n",
      "Epoch  19/100 Batch   40/169 - Loss:  1.945, Seconds: 4.64\n",
      "Average loss for this update: 1.987\n",
      "New Record!\n",
      "Epoch  19/100 Batch   60/169 - Loss:  2.050, Seconds: 4.80\n",
      "Epoch  19/100 Batch   80/169 - Loss:  1.956, Seconds: 4.95\n",
      "Epoch  19/100 Batch  100/169 - Loss:  2.042, Seconds: 5.16\n",
      "Average loss for this update: 2.016\n",
      "No Improvement.\n",
      "Epoch  19/100 Batch  120/169 - Loss:  2.034, Seconds: 5.32\n",
      "Epoch  19/100 Batch  140/169 - Loss:  2.052, Seconds: 5.45\n",
      "Epoch  19/100 Batch  160/169 - Loss:  2.037, Seconds: 5.63\n",
      "Average loss for this update: 2.035\n",
      "No Improvement.\n",
      "Epoch  20/100 Batch   20/169 - Loss:  1.908, Seconds: 4.46\n",
      "Epoch  20/100 Batch   40/169 - Loss:  1.871, Seconds: 4.57\n",
      "Average loss for this update: 1.906\n",
      "New Record!\n",
      "Epoch  20/100 Batch   60/169 - Loss:  1.961, Seconds: 4.82\n",
      "Epoch  20/100 Batch   80/169 - Loss:  1.890, Seconds: 4.93\n",
      "Epoch  20/100 Batch  100/169 - Loss:  1.959, Seconds: 5.15\n",
      "Average loss for this update: 1.941\n",
      "No Improvement.\n",
      "Epoch  20/100 Batch  120/169 - Loss:  1.962, Seconds: 5.30\n",
      "Epoch  20/100 Batch  140/169 - Loss:  1.988, Seconds: 5.48\n",
      "Epoch  20/100 Batch  160/169 - Loss:  1.964, Seconds: 5.63\n",
      "Average loss for this update: 1.968\n",
      "No Improvement.\n",
      "Epoch  21/100 Batch   20/169 - Loss:  1.851, Seconds: 4.31\n",
      "Epoch  21/100 Batch   40/169 - Loss:  1.817, Seconds: 4.61\n",
      "Average loss for this update: 1.851\n",
      "New Record!\n",
      "Epoch  21/100 Batch   60/169 - Loss:  1.905, Seconds: 4.83\n",
      "Epoch  21/100 Batch   80/169 - Loss:  1.826, Seconds: 5.01\n",
      "Epoch  21/100 Batch  100/169 - Loss:  1.887, Seconds: 5.13\n",
      "Average loss for this update: 1.872\n",
      "No Improvement.\n",
      "Epoch  21/100 Batch  120/169 - Loss:  1.886, Seconds: 5.32\n",
      "Epoch  21/100 Batch  140/169 - Loss:  1.924, Seconds: 5.44\n",
      "Epoch  21/100 Batch  160/169 - Loss:  1.875, Seconds: 5.61\n",
      "Average loss for this update: 1.89\n",
      "No Improvement.\n",
      "Epoch  22/100 Batch   20/169 - Loss:  1.766, Seconds: 4.33\n",
      "Epoch  22/100 Batch   40/169 - Loss:  1.730, Seconds: 4.60\n",
      "Average loss for this update: 1.768\n",
      "New Record!\n",
      "Epoch  22/100 Batch   60/169 - Loss:  1.838, Seconds: 4.79\n",
      "Epoch  22/100 Batch   80/169 - Loss:  1.773, Seconds: 4.98\n",
      "Epoch  22/100 Batch  100/169 - Loss:  1.805, Seconds: 5.58\n",
      "Average loss for this update: 1.812\n",
      "No Improvement.\n",
      "Epoch  22/100 Batch  120/169 - Loss:  1.815, Seconds: 5.26\n",
      "Epoch  22/100 Batch  140/169 - Loss:  1.864, Seconds: 5.45\n",
      "Epoch  22/100 Batch  160/169 - Loss:  1.822, Seconds: 5.60\n",
      "Average loss for this update: 1.823\n",
      "No Improvement.\n",
      "Epoch  23/100 Batch   20/169 - Loss:  1.742, Seconds: 4.32\n",
      "Epoch  23/100 Batch   40/169 - Loss:  1.671, Seconds: 4.60\n",
      "Average loss for this update: 1.72\n",
      "New Record!\n",
      "Epoch  23/100 Batch   60/169 - Loss:  1.774, Seconds: 4.87\n",
      "Epoch  23/100 Batch   80/169 - Loss:  1.696, Seconds: 4.98\n",
      "Epoch  23/100 Batch  100/169 - Loss:  1.757, Seconds: 5.11\n",
      "Average loss for this update: 1.745\n",
      "No Improvement.\n",
      "Epoch  23/100 Batch  120/169 - Loss:  1.752, Seconds: 5.27\n",
      "Epoch  23/100 Batch  140/169 - Loss:  1.763, Seconds: 5.46\n",
      "Epoch  23/100 Batch  160/169 - Loss:  1.750, Seconds: 5.65\n",
      "Average loss for this update: 1.75\n",
      "No Improvement.\n",
      "Epoch  24/100 Batch   20/169 - Loss:  1.685, Seconds: 4.36\n",
      "Epoch  24/100 Batch   40/169 - Loss:  1.624, Seconds: 5.69\n",
      "Average loss for this update: 1.67\n",
      "New Record!\n",
      "Epoch  24/100 Batch   60/169 - Loss:  1.719, Seconds: 4.78\n",
      "Epoch  24/100 Batch   80/169 - Loss:  1.649, Seconds: 4.95\n",
      "Epoch  24/100 Batch  100/169 - Loss:  1.679, Seconds: 5.15\n",
      "Average loss for this update: 1.682\n",
      "No Improvement.\n",
      "Epoch  24/100 Batch  120/169 - Loss:  1.700, Seconds: 5.29\n",
      "Epoch  24/100 Batch  140/169 - Loss:  1.731, Seconds: 5.45\n",
      "Epoch  24/100 Batch  160/169 - Loss:  1.680, Seconds: 5.61\n",
      "Average loss for this update: 1.697\n",
      "No Improvement.\n",
      "Epoch  25/100 Batch   20/169 - Loss:  1.620, Seconds: 4.34\n",
      "Epoch  25/100 Batch   40/169 - Loss:  1.569, Seconds: 4.64\n",
      "Average loss for this update: 1.604\n",
      "New Record!\n",
      "Epoch  25/100 Batch   60/169 - Loss:  1.640, Seconds: 4.82\n",
      "Epoch  25/100 Batch   80/169 - Loss:  1.598, Seconds: 4.97\n",
      "Epoch  25/100 Batch  100/169 - Loss:  1.641, Seconds: 5.53\n",
      "Average loss for this update: 1.637\n",
      "No Improvement.\n",
      "Epoch  25/100 Batch  120/169 - Loss:  1.664, Seconds: 5.64\n",
      "Epoch  25/100 Batch  140/169 - Loss:  1.675, Seconds: 5.42\n",
      "Epoch  25/100 Batch  160/169 - Loss:  1.639, Seconds: 5.53\n",
      "Average loss for this update: 1.652\n",
      "No Improvement.\n",
      "Epoch  26/100 Batch   20/169 - Loss:  1.558, Seconds: 4.27\n",
      "Epoch  26/100 Batch   40/169 - Loss:  1.523, Seconds: 4.54\n",
      "Average loss for this update: 1.558\n",
      "New Record!\n",
      "Epoch  26/100 Batch   60/169 - Loss:  1.617, Seconds: 4.79\n",
      "Epoch  26/100 Batch   80/169 - Loss:  1.557, Seconds: 4.92\n",
      "Epoch  26/100 Batch  100/169 - Loss:  1.579, Seconds: 5.09\n",
      "Average loss for this update: 1.587\n",
      "No Improvement.\n",
      "Epoch  26/100 Batch  120/169 - Loss:  1.594, Seconds: 5.23\n",
      "Epoch  26/100 Batch  140/169 - Loss:  1.610, Seconds: 5.39\n",
      "Epoch  26/100 Batch  160/169 - Loss:  1.584, Seconds: 5.52\n",
      "Average loss for this update: 1.59\n",
      "No Improvement.\n",
      "Epoch  27/100 Batch   20/169 - Loss:  1.507, Seconds: 4.28\n",
      "Epoch  27/100 Batch   40/169 - Loss:  1.476, Seconds: 4.53\n",
      "Average loss for this update: 1.51\n",
      "New Record!\n",
      "Epoch  27/100 Batch   60/169 - Loss:  1.574, Seconds: 4.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  27/100 Batch   80/169 - Loss:  1.528, Seconds: 4.95\n",
      "Epoch  27/100 Batch  100/169 - Loss:  1.547, Seconds: 5.11\n",
      "Average loss for this update: 1.552\n",
      "No Improvement.\n",
      "Epoch  27/100 Batch  120/169 - Loss:  1.544, Seconds: 5.24\n",
      "Epoch  27/100 Batch  140/169 - Loss:  1.558, Seconds: 5.38\n",
      "Epoch  27/100 Batch  160/169 - Loss:  1.535, Seconds: 5.57\n",
      "Average loss for this update: 1.537\n",
      "No Improvement.\n",
      "Epoch  28/100 Batch   20/169 - Loss:  1.480, Seconds: 4.33\n",
      "Epoch  28/100 Batch   40/169 - Loss:  1.448, Seconds: 4.56\n",
      "Average loss for this update: 1.473\n",
      "New Record!\n",
      "Epoch  28/100 Batch   60/169 - Loss:  1.512, Seconds: 4.76\n",
      "Epoch  28/100 Batch   80/169 - Loss:  1.481, Seconds: 4.89\n",
      "Epoch  28/100 Batch  100/169 - Loss:  1.488, Seconds: 5.07\n",
      "Average loss for this update: 1.499\n",
      "No Improvement.\n",
      "Epoch  28/100 Batch  120/169 - Loss:  1.492, Seconds: 5.25\n",
      "Epoch  28/100 Batch  140/169 - Loss:  1.511, Seconds: 5.38\n",
      "Epoch  28/100 Batch  160/169 - Loss:  1.483, Seconds: 5.50\n",
      "Average loss for this update: 1.489\n",
      "No Improvement.\n",
      "Epoch  29/100 Batch   20/169 - Loss:  1.448, Seconds: 4.27\n",
      "Epoch  29/100 Batch   40/169 - Loss:  1.363, Seconds: 4.55\n",
      "Average loss for this update: 1.423\n",
      "New Record!\n",
      "Epoch  29/100 Batch   60/169 - Loss:  1.473, Seconds: 6.00\n",
      "Epoch  29/100 Batch   80/169 - Loss:  1.445, Seconds: 4.94\n",
      "Epoch  29/100 Batch  100/169 - Loss:  1.465, Seconds: 5.13\n",
      "Average loss for this update: 1.463\n",
      "No Improvement.\n",
      "Epoch  29/100 Batch  120/169 - Loss:  1.453, Seconds: 5.29\n",
      "Epoch  29/100 Batch  140/169 - Loss:  1.471, Seconds: 5.55\n",
      "Epoch  29/100 Batch  160/169 - Loss:  1.448, Seconds: 7.70\n",
      "Average loss for this update: 1.451\n",
      "No Improvement.\n",
      "Epoch  30/100 Batch   20/169 - Loss:  1.396, Seconds: 4.35\n",
      "Epoch  30/100 Batch   40/169 - Loss:  1.336, Seconds: 4.60\n",
      "Average loss for this update: 1.381\n",
      "New Record!\n",
      "Epoch  30/100 Batch   60/169 - Loss:  1.425, Seconds: 4.88\n",
      "Epoch  30/100 Batch   80/169 - Loss:  1.393, Seconds: 4.97\n",
      "Epoch  30/100 Batch  100/169 - Loss:  1.432, Seconds: 5.19\n",
      "Average loss for this update: 1.419\n",
      "No Improvement.\n",
      "Epoch  30/100 Batch  120/169 - Loss:  1.410, Seconds: 5.27\n",
      "Epoch  30/100 Batch  140/169 - Loss:  1.432, Seconds: 5.48\n",
      "Epoch  30/100 Batch  160/169 - Loss:  1.412, Seconds: 5.59\n",
      "Average loss for this update: 1.414\n",
      "No Improvement.\n",
      "Epoch  31/100 Batch   20/169 - Loss:  1.387, Seconds: 4.33\n",
      "Epoch  31/100 Batch   40/169 - Loss:  1.317, Seconds: 4.60\n",
      "Average loss for this update: 1.357\n",
      "New Record!\n",
      "Epoch  31/100 Batch   60/169 - Loss:  1.378, Seconds: 4.80\n",
      "Epoch  31/100 Batch   80/169 - Loss:  1.358, Seconds: 4.97\n",
      "Epoch  31/100 Batch  100/169 - Loss:  1.371, Seconds: 5.15\n",
      "Average loss for this update: 1.374\n",
      "No Improvement.\n",
      "Epoch  31/100 Batch  120/169 - Loss:  1.375, Seconds: 5.30\n",
      "Epoch  31/100 Batch  140/169 - Loss:  1.386, Seconds: 5.46\n",
      "Epoch  31/100 Batch  160/169 - Loss:  1.370, Seconds: 5.60\n",
      "Average loss for this update: 1.373\n",
      "No Improvement.\n",
      "Epoch  32/100 Batch   20/169 - Loss:  1.304, Seconds: 4.33\n",
      "Epoch  32/100 Batch   40/169 - Loss:  1.272, Seconds: 4.61\n",
      "Average loss for this update: 1.301\n",
      "New Record!\n",
      "Epoch  32/100 Batch   60/169 - Loss:  1.340, Seconds: 4.80\n",
      "Epoch  32/100 Batch   80/169 - Loss:  1.323, Seconds: 4.95\n",
      "Epoch  32/100 Batch  100/169 - Loss:  1.340, Seconds: 5.16\n",
      "Average loss for this update: 1.336\n",
      "No Improvement.\n",
      "Epoch  32/100 Batch  120/169 - Loss:  1.335, Seconds: 5.30\n",
      "Epoch  32/100 Batch  140/169 - Loss:  1.361, Seconds: 5.46\n",
      "Epoch  32/100 Batch  160/169 - Loss:  1.341, Seconds: 5.61\n",
      "Average loss for this update: 1.343\n",
      "No Improvement.\n",
      "Epoch  33/100 Batch   20/169 - Loss:  1.289, Seconds: 4.33\n",
      "Epoch  33/100 Batch   40/169 - Loss:  1.247, Seconds: 4.63\n",
      "Average loss for this update: 1.276\n",
      "New Record!\n",
      "Epoch  33/100 Batch   60/169 - Loss:  1.300, Seconds: 4.79\n",
      "Epoch  33/100 Batch   80/169 - Loss:  1.270, Seconds: 4.96\n",
      "Epoch  33/100 Batch  100/169 - Loss:  1.276, Seconds: 5.16\n",
      "Average loss for this update: 1.284\n",
      "No Improvement.\n",
      "Epoch  33/100 Batch  120/169 - Loss:  1.289, Seconds: 5.27\n",
      "Epoch  33/100 Batch  140/169 - Loss:  1.299, Seconds: 5.46\n",
      "Epoch  33/100 Batch  160/169 - Loss:  1.283, Seconds: 5.66\n",
      "Average loss for this update: 1.285\n",
      "No Improvement.\n",
      "Epoch  34/100 Batch   20/169 - Loss:  1.259, Seconds: 4.37\n",
      "Epoch  34/100 Batch   40/169 - Loss:  1.203, Seconds: 4.62\n",
      "Average loss for this update: 1.238\n",
      "New Record!\n",
      "Epoch  34/100 Batch   60/169 - Loss:  1.269, Seconds: 4.88\n",
      "Epoch  34/100 Batch   80/169 - Loss:  1.247, Seconds: 4.97\n",
      "Epoch  34/100 Batch  100/169 - Loss:  1.247, Seconds: 5.15\n",
      "Average loss for this update: 1.261\n",
      "No Improvement.\n",
      "Epoch  34/100 Batch  120/169 - Loss:  1.283, Seconds: 5.31\n",
      "Epoch  34/100 Batch  140/169 - Loss:  1.269, Seconds: 5.46\n",
      "Epoch  34/100 Batch  160/169 - Loss:  1.275, Seconds: 5.55\n",
      "Average loss for this update: 1.271\n",
      "No Improvement.\n",
      "Epoch  35/100 Batch   20/169 - Loss:  1.202, Seconds: 4.34\n",
      "Epoch  35/100 Batch   40/169 - Loss:  1.174, Seconds: 4.62\n",
      "Average loss for this update: 1.198\n",
      "New Record!\n",
      "Epoch  35/100 Batch   60/169 - Loss:  1.228, Seconds: 4.87\n",
      "Epoch  35/100 Batch   80/169 - Loss:  1.208, Seconds: 4.90\n",
      "Epoch  35/100 Batch  100/169 - Loss:  1.233, Seconds: 5.17\n",
      "Average loss for this update: 1.233\n",
      "No Improvement.\n",
      "Epoch  35/100 Batch  120/169 - Loss:  1.250, Seconds: 6.03\n",
      "Epoch  35/100 Batch  140/169 - Loss:  1.240, Seconds: 5.40\n",
      "Epoch  35/100 Batch  160/169 - Loss:  1.234, Seconds: 5.61\n",
      "Average loss for this update: 1.232\n",
      "No Improvement.\n",
      "Epoch  36/100 Batch   20/169 - Loss:  1.194, Seconds: 4.30\n",
      "Epoch  36/100 Batch   40/169 - Loss:  1.154, Seconds: 4.57\n",
      "Average loss for this update: 1.182\n",
      "New Record!\n",
      "Epoch  36/100 Batch   60/169 - Loss:  1.203, Seconds: 4.76\n",
      "Epoch  36/100 Batch   80/169 - Loss:  1.178, Seconds: 4.93\n",
      "Epoch  36/100 Batch  100/169 - Loss:  1.191, Seconds: 5.09\n",
      "Average loss for this update: 1.187\n",
      "No Improvement.\n",
      "Epoch  36/100 Batch  120/169 - Loss:  1.195, Seconds: 5.28\n",
      "Epoch  36/100 Batch  140/169 - Loss:  1.193, Seconds: 5.36\n",
      "Epoch  36/100 Batch  160/169 - Loss:  1.201, Seconds: 5.55\n",
      "Average loss for this update: 1.194\n",
      "No Improvement.\n",
      "Epoch  37/100 Batch   20/169 - Loss:  1.176, Seconds: 4.28\n",
      "Epoch  37/100 Batch   40/169 - Loss:  1.128, Seconds: 4.56\n",
      "Average loss for this update: 1.152\n",
      "New Record!\n",
      "Epoch  37/100 Batch   60/169 - Loss:  1.165, Seconds: 4.75\n",
      "Epoch  37/100 Batch   80/169 - Loss:  1.149, Seconds: 4.92\n",
      "Epoch  37/100 Batch  100/169 - Loss:  1.170, Seconds: 5.07\n",
      "Average loss for this update: 1.17\n",
      "No Improvement.\n",
      "Epoch  37/100 Batch  120/169 - Loss:  1.189, Seconds: 5.22\n",
      "Epoch  37/100 Batch  140/169 - Loss:  1.176, Seconds: 5.38\n",
      "Epoch  37/100 Batch  160/169 - Loss:  1.168, Seconds: 5.55\n",
      "Average loss for this update: 1.17\n",
      "No Improvement.\n",
      "Epoch  38/100 Batch   20/169 - Loss:  1.145, Seconds: 4.29\n",
      "Epoch  38/100 Batch   40/169 - Loss:  1.084, Seconds: 4.54\n",
      "Average loss for this update: 1.122\n",
      "New Record!\n",
      "Epoch  38/100 Batch   60/169 - Loss:  1.143, Seconds: 4.75\n",
      "Epoch  38/100 Batch   80/169 - Loss:  1.111, Seconds: 4.89\n",
      "Epoch  38/100 Batch  100/169 - Loss:  1.143, Seconds: 5.06\n",
      "Average loss for this update: 1.133\n",
      "No Improvement.\n",
      "Epoch  38/100 Batch  120/169 - Loss:  1.146, Seconds: 5.23\n",
      "Epoch  38/100 Batch  140/169 - Loss:  1.155, Seconds: 5.37\n",
      "Epoch  38/100 Batch  160/169 - Loss:  1.145, Seconds: 5.52\n",
      "Average loss for this update: 1.143\n",
      "No Improvement.\n",
      "Epoch  39/100 Batch   20/169 - Loss:  1.107, Seconds: 4.28\n",
      "Epoch  39/100 Batch   40/169 - Loss:  1.059, Seconds: 4.55\n",
      "Average loss for this update: 1.085\n",
      "New Record!\n",
      "Epoch  39/100 Batch   60/169 - Loss:  1.107, Seconds: 4.79\n",
      "Epoch  39/100 Batch   80/169 - Loss:  1.100, Seconds: 4.89\n",
      "Epoch  39/100 Batch  100/169 - Loss:  1.114, Seconds: 5.10\n",
      "Average loss for this update: 1.112\n",
      "No Improvement.\n",
      "Epoch  39/100 Batch  120/169 - Loss:  1.106, Seconds: 5.31\n",
      "Epoch  39/100 Batch  140/169 - Loss:  1.120, Seconds: 5.37\n",
      "Epoch  39/100 Batch  160/169 - Loss:  1.120, Seconds: 5.55\n",
      "Average loss for this update: 1.112\n",
      "No Improvement.\n",
      "Epoch  40/100 Batch   20/169 - Loss:  1.097, Seconds: 4.30\n",
      "Epoch  40/100 Batch   40/169 - Loss:  1.041, Seconds: 4.53\n",
      "Average loss for this update: 1.08\n",
      "New Record!\n",
      "Epoch  40/100 Batch   60/169 - Loss:  1.114, Seconds: 4.77\n",
      "Epoch  40/100 Batch   80/169 - Loss:  1.049, Seconds: 4.90\n",
      "Epoch  40/100 Batch  100/169 - Loss:  1.113, Seconds: 5.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for this update: 1.087\n",
      "No Improvement.\n",
      "Epoch  40/100 Batch  120/169 - Loss:  1.069, Seconds: 5.25\n",
      "Epoch  40/100 Batch  140/169 - Loss:  1.075, Seconds: 5.40\n",
      "Epoch  40/100 Batch  160/169 - Loss:  1.083, Seconds: 5.54\n",
      "Average loss for this update: 1.072\n",
      "New Record!\n",
      "Epoch  41/100 Batch   20/169 - Loss:  1.052, Seconds: 4.28\n",
      "Epoch  41/100 Batch   40/169 - Loss:  1.022, Seconds: 4.58\n",
      "Average loss for this update: 1.047\n",
      "New Record!\n",
      "Epoch  41/100 Batch   60/169 - Loss:  1.077, Seconds: 4.78\n",
      "Epoch  41/100 Batch   80/169 - Loss:  1.058, Seconds: 4.96\n",
      "Epoch  41/100 Batch  100/169 - Loss:  1.073, Seconds: 5.09\n",
      "Average loss for this update: 1.064\n",
      "No Improvement.\n",
      "Epoch  41/100 Batch  120/169 - Loss:  1.058, Seconds: 5.28\n",
      "Epoch  41/100 Batch  140/169 - Loss:  1.056, Seconds: 5.42\n",
      "Epoch  41/100 Batch  160/169 - Loss:  1.057, Seconds: 5.54\n",
      "Average loss for this update: 1.056\n",
      "No Improvement.\n",
      "Epoch  42/100 Batch   20/169 - Loss:  1.027, Seconds: 4.30\n",
      "Epoch  42/100 Batch   40/169 - Loss:  1.007, Seconds: 4.54\n",
      "Average loss for this update: 1.027\n",
      "New Record!\n",
      "Epoch  42/100 Batch   60/169 - Loss:  1.055, Seconds: 4.99\n",
      "Epoch  42/100 Batch   80/169 - Loss:  1.004, Seconds: 4.98\n",
      "Epoch  42/100 Batch  100/169 - Loss:  1.034, Seconds: 5.13\n",
      "Average loss for this update: 1.023\n",
      "New Record!\n",
      "Epoch  42/100 Batch  120/169 - Loss:  1.017, Seconds: 5.34\n",
      "Epoch  42/100 Batch  140/169 - Loss:  1.047, Seconds: 5.45\n",
      "Epoch  42/100 Batch  160/169 - Loss:  1.043, Seconds: 5.58\n",
      "Average loss for this update: 1.039\n",
      "No Improvement.\n",
      "Epoch  43/100 Batch   20/169 - Loss:  0.992, Seconds: 4.36\n",
      "Epoch  43/100 Batch   40/169 - Loss:  0.985, Seconds: 4.60\n",
      "Average loss for this update: 0.994\n",
      "New Record!\n",
      "Epoch  43/100 Batch   60/169 - Loss:  1.018, Seconds: 4.85\n",
      "Epoch  43/100 Batch   80/169 - Loss:  1.004, Seconds: 4.99\n",
      "Epoch  43/100 Batch  100/169 - Loss:  1.003, Seconds: 5.14\n",
      "Average loss for this update: 1.012\n",
      "No Improvement.\n",
      "Epoch  43/100 Batch  120/169 - Loss:  1.015, Seconds: 5.30\n",
      "Epoch  43/100 Batch  140/169 - Loss:  1.016, Seconds: 5.49\n",
      "Epoch  43/100 Batch  160/169 - Loss:  1.032, Seconds: 5.60\n",
      "Average loss for this update: 1.018\n",
      "No Improvement.\n",
      "Epoch  44/100 Batch   20/169 - Loss:  0.971, Seconds: 4.40\n",
      "Epoch  44/100 Batch   40/169 - Loss:  0.961, Seconds: 4.64\n",
      "Average loss for this update: 0.974\n",
      "New Record!\n",
      "Epoch  44/100 Batch   60/169 - Loss:  1.003, Seconds: 4.86\n",
      "Epoch  44/100 Batch   80/169 - Loss:  0.976, Seconds: 4.99\n",
      "Epoch  44/100 Batch  100/169 - Loss:  0.997, Seconds: 5.15\n",
      "Average loss for this update: 0.993\n",
      "No Improvement.\n",
      "Epoch  44/100 Batch  120/169 - Loss:  0.987, Seconds: 5.30\n",
      "Epoch  44/100 Batch  140/169 - Loss:  1.015, Seconds: 5.43\n",
      "Epoch  44/100 Batch  160/169 - Loss:  0.995, Seconds: 5.57\n",
      "Average loss for this update: 0.995\n",
      "No Improvement.\n",
      "Epoch  45/100 Batch   20/169 - Loss:  0.961, Seconds: 4.35\n",
      "Epoch  45/100 Batch   40/169 - Loss:  0.946, Seconds: 4.59\n",
      "Average loss for this update: 0.966\n",
      "New Record!\n",
      "Epoch  45/100 Batch   60/169 - Loss:  1.002, Seconds: 4.84\n",
      "Epoch  45/100 Batch   80/169 - Loss:  0.966, Seconds: 4.97\n",
      "Epoch  45/100 Batch  100/169 - Loss:  0.968, Seconds: 5.16\n",
      "Average loss for this update: 0.974\n",
      "No Improvement.\n",
      "Epoch  45/100 Batch  120/169 - Loss:  0.970, Seconds: 5.31\n",
      "Epoch  45/100 Batch  140/169 - Loss:  0.991, Seconds: 5.45\n",
      "Epoch  45/100 Batch  160/169 - Loss:  0.986, Seconds: 5.61\n",
      "Average loss for this update: 0.981\n",
      "No Improvement.\n",
      "Epoch  46/100 Batch   20/169 - Loss:  0.929, Seconds: 4.35\n",
      "Epoch  46/100 Batch   40/169 - Loss:  0.922, Seconds: 4.61\n",
      "Average loss for this update: 0.944\n",
      "New Record!\n",
      "Epoch  46/100 Batch   60/169 - Loss:  0.993, Seconds: 4.83\n",
      "Epoch  46/100 Batch   80/169 - Loss:  0.962, Seconds: 4.96\n",
      "Epoch  46/100 Batch  100/169 - Loss:  0.967, Seconds: 5.13\n",
      "Average loss for this update: 0.969\n",
      "No Improvement.\n",
      "Epoch  46/100 Batch  120/169 - Loss:  0.956, Seconds: 5.30\n",
      "Epoch  46/100 Batch  140/169 - Loss:  0.965, Seconds: 6.80\n",
      "Epoch  46/100 Batch  160/169 - Loss:  0.973, Seconds: 5.60\n",
      "Average loss for this update: 0.961\n",
      "No Improvement.\n",
      "Epoch  47/100 Batch   20/169 - Loss:  0.930, Seconds: 4.32\n",
      "Epoch  47/100 Batch   40/169 - Loss:  0.904, Seconds: 4.65\n",
      "Average loss for this update: 0.93\n",
      "New Record!\n",
      "Epoch  47/100 Batch   60/169 - Loss:  0.969, Seconds: 5.03\n",
      "Epoch  47/100 Batch   80/169 - Loss:  0.945, Seconds: 4.95\n",
      "Epoch  47/100 Batch  100/169 - Loss:  0.940, Seconds: 5.14\n",
      "Average loss for this update: 0.945\n",
      "No Improvement.\n",
      "Epoch  47/100 Batch  120/169 - Loss:  0.923, Seconds: 5.29\n",
      "Epoch  47/100 Batch  140/169 - Loss:  0.941, Seconds: 5.49\n",
      "Epoch  47/100 Batch  160/169 - Loss:  0.948, Seconds: 5.59\n",
      "Average loss for this update: 0.938\n",
      "No Improvement.\n",
      "Epoch  48/100 Batch   20/169 - Loss:  0.909, Seconds: 4.62\n",
      "Epoch  48/100 Batch   40/169 - Loss:  0.862, Seconds: 4.95\n",
      "Average loss for this update: 0.895\n",
      "New Record!\n",
      "Epoch  48/100 Batch   60/169 - Loss:  0.928, Seconds: 4.76\n",
      "Epoch  48/100 Batch   80/169 - Loss:  0.909, Seconds: 4.90\n",
      "Epoch  48/100 Batch  100/169 - Loss:  0.949, Seconds: 5.05\n",
      "Average loss for this update: 0.931\n",
      "No Improvement.\n",
      "Epoch  48/100 Batch  120/169 - Loss:  0.908, Seconds: 5.28\n",
      "Epoch  48/100 Batch  140/169 - Loss:  0.938, Seconds: 5.42\n",
      "Epoch  48/100 Batch  160/169 - Loss:  0.920, Seconds: 5.52\n",
      "Average loss for this update: 0.924\n",
      "No Improvement.\n",
      "Epoch  49/100 Batch   20/169 - Loss:  0.894, Seconds: 4.31\n",
      "Epoch  49/100 Batch   40/169 - Loss:  0.877, Seconds: 4.54\n",
      "Average loss for this update: 0.892\n",
      "New Record!\n",
      "Epoch  49/100 Batch   60/169 - Loss:  0.923, Seconds: 4.78\n",
      "Epoch  49/100 Batch   80/169 - Loss:  0.889, Seconds: 4.96\n",
      "Epoch  49/100 Batch  100/169 - Loss:  0.913, Seconds: 5.11\n",
      "Average loss for this update: 0.911\n",
      "No Improvement.\n",
      "Epoch  49/100 Batch  120/169 - Loss:  0.898, Seconds: 5.25\n",
      "Epoch  49/100 Batch  140/169 - Loss:  0.907, Seconds: 5.37\n",
      "Epoch  49/100 Batch  160/169 - Loss:  0.887, Seconds: 5.54\n",
      "Average loss for this update: 0.89\n",
      "New Record!\n",
      "Epoch  50/100 Batch   20/169 - Loss:  0.874, Seconds: 4.32\n",
      "Epoch  50/100 Batch   40/169 - Loss:  0.837, Seconds: 4.58\n",
      "Average loss for this update: 0.863\n",
      "New Record!\n",
      "Epoch  50/100 Batch   60/169 - Loss:  0.894, Seconds: 4.76\n",
      "Epoch  50/100 Batch   80/169 - Loss:  0.869, Seconds: 4.93\n",
      "Epoch  50/100 Batch  100/169 - Loss:  0.898, Seconds: 5.08\n",
      "Average loss for this update: 0.891\n",
      "No Improvement.\n",
      "Epoch  50/100 Batch  120/169 - Loss:  0.882, Seconds: 5.25\n",
      "Epoch  50/100 Batch  140/169 - Loss:  0.913, Seconds: 5.40\n",
      "Epoch  50/100 Batch  160/169 - Loss:  0.883, Seconds: 5.54\n",
      "Average loss for this update: 0.892\n",
      "No Improvement.\n",
      "Epoch  51/100 Batch   20/169 - Loss:  0.861, Seconds: 4.28\n",
      "Epoch  51/100 Batch   40/169 - Loss:  0.810, Seconds: 4.54\n",
      "Average loss for this update: 0.849\n",
      "New Record!\n",
      "Epoch  51/100 Batch   60/169 - Loss:  0.891, Seconds: 4.78\n",
      "Epoch  51/100 Batch   80/169 - Loss:  0.875, Seconds: 4.92\n",
      "Epoch  51/100 Batch  100/169 - Loss:  0.866, Seconds: 5.08\n",
      "Average loss for this update: 0.876\n",
      "No Improvement.\n",
      "Epoch  51/100 Batch  120/169 - Loss:  0.864, Seconds: 5.27\n",
      "Epoch  51/100 Batch  140/169 - Loss:  0.887, Seconds: 5.38\n",
      "Epoch  51/100 Batch  160/169 - Loss:  0.885, Seconds: 5.57\n",
      "Average loss for this update: 0.876\n",
      "No Improvement.\n",
      "Epoch  52/100 Batch   20/169 - Loss:  0.828, Seconds: 4.28\n",
      "Epoch  52/100 Batch   40/169 - Loss:  0.797, Seconds: 4.54\n",
      "Average loss for this update: 0.826\n",
      "New Record!\n",
      "Epoch  52/100 Batch   60/169 - Loss:  0.874, Seconds: 5.50\n",
      "Epoch  52/100 Batch   80/169 - Loss:  0.824, Seconds: 4.92\n",
      "Epoch  52/100 Batch  100/169 - Loss:  0.867, Seconds: 5.10\n",
      "Average loss for this update: 0.852\n",
      "No Improvement.\n",
      "Epoch  52/100 Batch  120/169 - Loss:  0.841, Seconds: 5.25\n",
      "Epoch  52/100 Batch  140/169 - Loss:  0.879, Seconds: 5.41\n",
      "Epoch  52/100 Batch  160/169 - Loss:  0.870, Seconds: 5.56\n",
      "Average loss for this update: 0.863\n",
      "No Improvement.\n",
      "Epoch  53/100 Batch   20/169 - Loss:  0.845, Seconds: 4.26\n",
      "Epoch  53/100 Batch   40/169 - Loss:  0.805, Seconds: 4.58\n",
      "Average loss for this update: 0.835\n",
      "No Improvement.\n",
      "Epoch  53/100 Batch   60/169 - Loss:  0.870, Seconds: 4.75\n",
      "Epoch  53/100 Batch   80/169 - Loss:  0.824, Seconds: 4.91\n",
      "Epoch  53/100 Batch  100/169 - Loss:  0.836, Seconds: 5.11\n",
      "Average loss for this update: 0.84\n",
      "No Improvement.\n",
      "Epoch  53/100 Batch  120/169 - Loss:  0.835, Seconds: 5.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  53/100 Batch  140/169 - Loss:  0.855, Seconds: 5.42\n",
      "Epoch  53/100 Batch  160/169 - Loss:  0.850, Seconds: 5.53\n",
      "Average loss for this update: 0.845\n",
      "No Improvement.\n",
      "Epoch  54/100 Batch   20/169 - Loss:  0.824, Seconds: 4.27\n",
      "Epoch  54/100 Batch   40/169 - Loss:  0.776, Seconds: 4.55\n",
      "Average loss for this update: 0.809\n",
      "New Record!\n",
      "Epoch  54/100 Batch   60/169 - Loss:  0.839, Seconds: 4.75\n",
      "Epoch  54/100 Batch   80/169 - Loss:  0.827, Seconds: 4.90\n",
      "Epoch  54/100 Batch  100/169 - Loss:  0.835, Seconds: 5.07\n",
      "Average loss for this update: 0.832\n",
      "No Improvement.\n",
      "Epoch  54/100 Batch  120/169 - Loss:  0.814, Seconds: 5.22\n",
      "Epoch  54/100 Batch  140/169 - Loss:  0.837, Seconds: 5.51\n",
      "Epoch  54/100 Batch  160/169 - Loss:  0.831, Seconds: 6.48\n",
      "Average loss for this update: 0.827\n",
      "No Improvement.\n",
      "Epoch  55/100 Batch   20/169 - Loss:  0.797, Seconds: 4.34\n",
      "Epoch  55/100 Batch   40/169 - Loss:  0.763, Seconds: 4.60\n",
      "Average loss for this update: 0.788\n",
      "New Record!\n",
      "Epoch  55/100 Batch   60/169 - Loss:  0.822, Seconds: 4.81\n",
      "Epoch  55/100 Batch   80/169 - Loss:  0.798, Seconds: 4.94\n",
      "Epoch  55/100 Batch  100/169 - Loss:  0.821, Seconds: 5.19\n",
      "Average loss for this update: 0.815\n",
      "No Improvement.\n",
      "Epoch  55/100 Batch  120/169 - Loss:  0.793, Seconds: 5.26\n",
      "Epoch  55/100 Batch  140/169 - Loss:  0.819, Seconds: 5.47\n",
      "Epoch  55/100 Batch  160/169 - Loss:  0.813, Seconds: 5.59\n",
      "Average loss for this update: 0.806\n",
      "No Improvement.\n",
      "Epoch  56/100 Batch   20/169 - Loss:  0.786, Seconds: 4.31\n",
      "Epoch  56/100 Batch   40/169 - Loss:  0.770, Seconds: 4.59\n",
      "Average loss for this update: 0.786\n",
      "New Record!\n",
      "Epoch  56/100 Batch   60/169 - Loss:  0.810, Seconds: 4.80\n",
      "Epoch  56/100 Batch   80/169 - Loss:  0.789, Seconds: 4.98\n",
      "Epoch  56/100 Batch  100/169 - Loss:  0.795, Seconds: 5.15\n",
      "Average loss for this update: 0.79\n",
      "No Improvement.\n",
      "Epoch  56/100 Batch  120/169 - Loss:  0.772, Seconds: 5.31\n",
      "Epoch  56/100 Batch  140/169 - Loss:  0.795, Seconds: 5.43\n",
      "Epoch  56/100 Batch  160/169 - Loss:  0.812, Seconds: 5.62\n",
      "Average loss for this update: 0.8\n",
      "No Improvement.\n",
      "Epoch  57/100 Batch   20/169 - Loss:  0.785, Seconds: 4.35\n",
      "Epoch  57/100 Batch   40/169 - Loss:  0.739, Seconds: 4.60\n",
      "Average loss for this update: 0.767\n",
      "New Record!\n",
      "Epoch  57/100 Batch   60/169 - Loss:  0.789, Seconds: 4.85\n",
      "Epoch  57/100 Batch   80/169 - Loss:  0.790, Seconds: 5.00\n",
      "Epoch  57/100 Batch  100/169 - Loss:  0.790, Seconds: 5.14\n",
      "Average loss for this update: 0.792\n",
      "No Improvement.\n",
      "Epoch  57/100 Batch  120/169 - Loss:  0.767, Seconds: 5.34\n",
      "Epoch  57/100 Batch  140/169 - Loss:  0.788, Seconds: 5.44\n",
      "Epoch  57/100 Batch  160/169 - Loss:  0.771, Seconds: 5.62\n",
      "Average loss for this update: 0.774\n",
      "No Improvement.\n",
      "Epoch  58/100 Batch   20/169 - Loss:  0.771, Seconds: 4.33\n",
      "Epoch  58/100 Batch   40/169 - Loss:  0.733, Seconds: 4.61\n",
      "Average loss for this update: 0.762\n",
      "New Record!\n",
      "Epoch  58/100 Batch   60/169 - Loss:  0.785, Seconds: 4.95\n",
      "Epoch  58/100 Batch   80/169 - Loss:  0.749, Seconds: 4.96\n",
      "Epoch  58/100 Batch  100/169 - Loss:  0.754, Seconds: 5.17\n",
      "Average loss for this update: 0.756\n",
      "New Record!\n",
      "Epoch  58/100 Batch  120/169 - Loss:  0.749, Seconds: 5.29\n",
      "Epoch  58/100 Batch  140/169 - Loss:  0.757, Seconds: 5.52\n",
      "Epoch  58/100 Batch  160/169 - Loss:  0.769, Seconds: 5.67\n",
      "Average loss for this update: 0.761\n",
      "No Improvement.\n",
      "Epoch  59/100 Batch   20/169 - Loss:  0.740, Seconds: 4.35\n",
      "Epoch  59/100 Batch   40/169 - Loss:  0.727, Seconds: 4.62\n",
      "Average loss for this update: 0.74\n",
      "New Record!\n",
      "Epoch  59/100 Batch   60/169 - Loss:  0.763, Seconds: 4.85\n",
      "Epoch  59/100 Batch   80/169 - Loss:  0.749, Seconds: 5.00\n",
      "Epoch  59/100 Batch  100/169 - Loss:  0.768, Seconds: 5.11\n",
      "Average loss for this update: 0.758\n",
      "No Improvement.\n",
      "Epoch  59/100 Batch  120/169 - Loss:  0.739, Seconds: 5.33\n",
      "Epoch  59/100 Batch  140/169 - Loss:  0.775, Seconds: 5.42\n",
      "Epoch  59/100 Batch  160/169 - Loss:  0.760, Seconds: 5.59\n",
      "Average loss for this update: 0.763\n",
      "No Improvement.\n",
      "Epoch  60/100 Batch   20/169 - Loss:  0.749, Seconds: 4.34\n",
      "Epoch  60/100 Batch   40/169 - Loss:  0.706, Seconds: 4.71\n",
      "Average loss for this update: 0.734\n",
      "New Record!\n",
      "Epoch  60/100 Batch   60/169 - Loss:  0.754, Seconds: 4.88\n",
      "Epoch  60/100 Batch   80/169 - Loss:  0.733, Seconds: 5.08\n",
      "Epoch  60/100 Batch  100/169 - Loss:  0.754, Seconds: 5.16\n",
      "Average loss for this update: 0.747\n",
      "No Improvement.\n",
      "Epoch  60/100 Batch  120/169 - Loss:  0.735, Seconds: 5.68\n",
      "Epoch  60/100 Batch  140/169 - Loss:  0.747, Seconds: 5.85\n",
      "Epoch  60/100 Batch  160/169 - Loss:  0.743, Seconds: 5.55\n",
      "Average loss for this update: 0.741\n",
      "No Improvement.\n",
      "Epoch  61/100 Batch   20/169 - Loss:  0.752, Seconds: 4.43\n",
      "Epoch  61/100 Batch   40/169 - Loss:  0.690, Seconds: 4.54\n",
      "Average loss for this update: 0.731\n",
      "New Record!\n",
      "Epoch  61/100 Batch   60/169 - Loss:  0.761, Seconds: 4.79\n",
      "Epoch  61/100 Batch   80/169 - Loss:  0.739, Seconds: 4.98\n",
      "Epoch  61/100 Batch  100/169 - Loss:  0.722, Seconds: 5.10\n",
      "Average loss for this update: 0.734\n",
      "No Improvement.\n",
      "Epoch  61/100 Batch  120/169 - Loss:  0.728, Seconds: 5.24\n",
      "Epoch  61/100 Batch  140/169 - Loss:  0.729, Seconds: 5.39\n",
      "Epoch  61/100 Batch  160/169 - Loss:  0.759, Seconds: 5.56\n",
      "Average loss for this update: 0.741\n",
      "No Improvement.\n",
      "Epoch  62/100 Batch   20/169 - Loss:  0.729, Seconds: 4.29\n",
      "Epoch  62/100 Batch   40/169 - Loss:  0.689, Seconds: 4.56\n",
      "Average loss for this update: 0.718\n",
      "New Record!\n",
      "Epoch  62/100 Batch   60/169 - Loss:  0.744, Seconds: 4.79\n",
      "Epoch  62/100 Batch   80/169 - Loss:  0.710, Seconds: 4.90\n",
      "Epoch  62/100 Batch  100/169 - Loss:  0.711, Seconds: 5.10\n",
      "Average loss for this update: 0.716\n",
      "New Record!\n",
      "Epoch  62/100 Batch  120/169 - Loss:  0.716, Seconds: 5.27\n",
      "Epoch  62/100 Batch  140/169 - Loss:  0.708, Seconds: 5.39\n",
      "Epoch  62/100 Batch  160/169 - Loss:  0.747, Seconds: 5.54\n",
      "Average loss for this update: 0.722\n",
      "No Improvement.\n",
      "Epoch  63/100 Batch   20/169 - Loss:  0.717, Seconds: 4.41\n",
      "Epoch  63/100 Batch   40/169 - Loss:  0.703, Seconds: 4.56\n",
      "Average loss for this update: 0.712\n",
      "New Record!\n",
      "Epoch  63/100 Batch   60/169 - Loss:  0.721, Seconds: 4.77\n",
      "Epoch  63/100 Batch   80/169 - Loss:  0.704, Seconds: 4.94\n",
      "Epoch  63/100 Batch  100/169 - Loss:  0.695, Seconds: 5.13\n",
      "Average loss for this update: 0.709\n",
      "New Record!\n",
      "Epoch  63/100 Batch  120/169 - Loss:  0.722, Seconds: 5.26\n",
      "Epoch  63/100 Batch  140/169 - Loss:  0.716, Seconds: 8.48\n",
      "Epoch  63/100 Batch  160/169 - Loss:  0.729, Seconds: 5.54\n",
      "Average loss for this update: 0.715\n",
      "No Improvement.\n",
      "Epoch  64/100 Batch   20/169 - Loss:  0.715, Seconds: 4.29\n",
      "Epoch  64/100 Batch   40/169 - Loss:  0.695, Seconds: 4.54\n",
      "Average loss for this update: 0.717\n",
      "No Improvement.\n",
      "Epoch  64/100 Batch   60/169 - Loss:  0.742, Seconds: 4.78\n",
      "Epoch  64/100 Batch   80/169 - Loss:  0.676, Seconds: 4.92\n",
      "Epoch  64/100 Batch  100/169 - Loss:  0.712, Seconds: 5.14\n",
      "Average loss for this update: 0.701\n",
      "New Record!\n",
      "Epoch  64/100 Batch  120/169 - Loss:  0.698, Seconds: 5.30\n",
      "Epoch  64/100 Batch  140/169 - Loss:  0.689, Seconds: 5.38\n",
      "Epoch  64/100 Batch  160/169 - Loss:  0.697, Seconds: 5.55\n",
      "Average loss for this update: 0.691\n",
      "New Record!\n",
      "Epoch  65/100 Batch   20/169 - Loss:  0.706, Seconds: 4.27\n",
      "Epoch  65/100 Batch   40/169 - Loss:  0.662, Seconds: 4.54\n",
      "Average loss for this update: 0.69\n",
      "New Record!\n",
      "Epoch  65/100 Batch   60/169 - Loss:  0.711, Seconds: 4.77\n",
      "Epoch  65/100 Batch   80/169 - Loss:  0.673, Seconds: 4.89\n",
      "Epoch  65/100 Batch  100/169 - Loss:  0.690, Seconds: 5.12\n",
      "Average loss for this update: 0.684\n",
      "New Record!\n",
      "Epoch  65/100 Batch  120/169 - Loss:  0.666, Seconds: 5.25\n",
      "Epoch  65/100 Batch  140/169 - Loss:  0.693, Seconds: 5.39\n",
      "Epoch  65/100 Batch  160/169 - Loss:  0.691, Seconds: 5.52\n",
      "Average loss for this update: 0.689\n",
      "No Improvement.\n",
      "Epoch  66/100 Batch   20/169 - Loss:  0.659, Seconds: 4.27\n",
      "Epoch  66/100 Batch   40/169 - Loss:  0.651, Seconds: 4.54\n",
      "Average loss for this update: 0.664\n",
      "New Record!\n",
      "Epoch  66/100 Batch   60/169 - Loss:  0.690, Seconds: 4.78\n",
      "Epoch  66/100 Batch   80/169 - Loss:  0.670, Seconds: 4.90\n",
      "Epoch  66/100 Batch  100/169 - Loss:  0.682, Seconds: 5.08\n",
      "Average loss for this update: 0.679\n",
      "No Improvement.\n",
      "Epoch  66/100 Batch  120/169 - Loss:  0.667, Seconds: 5.20\n",
      "Epoch  66/100 Batch  140/169 - Loss:  0.666, Seconds: 5.36\n",
      "Epoch  66/100 Batch  160/169 - Loss:  0.689, Seconds: 5.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for this update: 0.679\n",
      "No Improvement.\n",
      "Epoch  67/100 Batch   20/169 - Loss:  0.656, Seconds: 4.29\n",
      "Epoch  67/100 Batch   40/169 - Loss:  0.642, Seconds: 4.55\n",
      "Average loss for this update: 0.652\n",
      "New Record!\n",
      "Epoch  67/100 Batch   60/169 - Loss:  0.670, Seconds: 4.75\n",
      "Epoch  67/100 Batch   80/169 - Loss:  0.659, Seconds: 4.92\n",
      "Epoch  67/100 Batch  100/169 - Loss:  0.678, Seconds: 5.09\n",
      "Average loss for this update: 0.673\n",
      "No Improvement.\n",
      "Epoch  67/100 Batch  120/169 - Loss:  0.660, Seconds: 5.20\n",
      "Epoch  67/100 Batch  140/169 - Loss:  0.668, Seconds: 5.38\n",
      "Epoch  67/100 Batch  160/169 - Loss:  0.657, Seconds: 5.53\n",
      "Average loss for this update: 0.659\n",
      "No Improvement.\n",
      "Epoch  68/100 Batch   20/169 - Loss:  0.681, Seconds: 4.29\n",
      "Epoch  68/100 Batch   40/169 - Loss:  0.626, Seconds: 4.54\n",
      "Average loss for this update: 0.654\n",
      "No Improvement.\n",
      "Epoch  68/100 Batch   60/169 - Loss:  0.665, Seconds: 4.77\n",
      "Epoch  68/100 Batch   80/169 - Loss:  0.661, Seconds: 4.93\n",
      "Epoch  68/100 Batch  100/169 - Loss:  0.662, Seconds: 5.09\n",
      "Average loss for this update: 0.665\n",
      "No Improvement.\n",
      "Epoch  68/100 Batch  120/169 - Loss:  0.655, Seconds: 5.22\n",
      "Epoch  68/100 Batch  140/169 - Loss:  0.649, Seconds: 5.39\n",
      "Epoch  68/100 Batch  160/169 - Loss:  0.655, Seconds: 5.54\n",
      "Average loss for this update: 0.65\n",
      "New Record!\n",
      "Epoch  69/100 Batch   20/169 - Loss:  0.663, Seconds: 4.28\n",
      "Epoch  69/100 Batch   40/169 - Loss:  0.622, Seconds: 4.56\n",
      "Average loss for this update: 0.647\n",
      "New Record!\n",
      "Epoch  69/100 Batch   60/169 - Loss:  0.666, Seconds: 4.81\n",
      "Epoch  69/100 Batch   80/169 - Loss:  0.650, Seconds: 4.91\n",
      "Epoch  69/100 Batch  100/169 - Loss:  0.657, Seconds: 5.08\n",
      "Average loss for this update: 0.656\n",
      "No Improvement.\n",
      "Epoch  69/100 Batch  120/169 - Loss:  0.647, Seconds: 5.27\n",
      "Epoch  69/100 Batch  140/169 - Loss:  0.673, Seconds: 5.42\n",
      "Epoch  69/100 Batch  160/169 - Loss:  0.676, Seconds: 5.55\n",
      "Average loss for this update: 0.666\n",
      "No Improvement.\n",
      "Epoch  70/100 Batch   20/169 - Loss:  0.634, Seconds: 4.29\n",
      "Epoch  70/100 Batch   40/169 - Loss:  0.616, Seconds: 4.55\n",
      "Average loss for this update: 0.634\n",
      "New Record!\n",
      "Epoch  70/100 Batch   60/169 - Loss:  0.662, Seconds: 4.77\n",
      "Epoch  70/100 Batch   80/169 - Loss:  0.646, Seconds: 4.93\n",
      "Epoch  70/100 Batch  100/169 - Loss:  0.659, Seconds: 5.09\n",
      "Average loss for this update: 0.654\n",
      "No Improvement.\n",
      "Epoch  70/100 Batch  120/169 - Loss:  0.635, Seconds: 5.25\n",
      "Epoch  70/100 Batch  140/169 - Loss:  0.643, Seconds: 5.36\n",
      "Epoch  70/100 Batch  160/169 - Loss:  0.619, Seconds: 5.55\n",
      "Average loss for this update: 0.629\n",
      "New Record!\n",
      "Epoch  71/100 Batch   20/169 - Loss:  0.633, Seconds: 4.29\n",
      "Epoch  71/100 Batch   40/169 - Loss:  0.590, Seconds: 4.59\n",
      "Average loss for this update: 0.619\n",
      "New Record!\n",
      "Epoch  71/100 Batch   60/169 - Loss:  0.641, Seconds: 4.76\n",
      "Epoch  71/100 Batch   80/169 - Loss:  0.632, Seconds: 4.90\n",
      "Epoch  71/100 Batch  100/169 - Loss:  0.642, Seconds: 5.07\n",
      "Average loss for this update: 0.64\n",
      "No Improvement.\n",
      "Epoch  71/100 Batch  120/169 - Loss:  0.633, Seconds: 5.22\n",
      "Epoch  71/100 Batch  140/169 - Loss:  0.620, Seconds: 5.40\n",
      "Epoch  71/100 Batch  160/169 - Loss:  0.632, Seconds: 5.52\n",
      "Average loss for this update: 0.627\n",
      "No Improvement.\n",
      "Epoch  72/100 Batch   20/169 - Loss:  0.629, Seconds: 4.29\n",
      "Epoch  72/100 Batch   40/169 - Loss:  0.602, Seconds: 4.53\n",
      "Average loss for this update: 0.619\n",
      "No Improvement.\n",
      "Epoch  72/100 Batch   60/169 - Loss:  0.632, Seconds: 4.84\n",
      "Epoch  72/100 Batch   80/169 - Loss:  0.612, Seconds: 4.89\n",
      "Epoch  72/100 Batch  100/169 - Loss:  0.621, Seconds: 5.08\n",
      "Average loss for this update: 0.624\n",
      "No Improvement.\n",
      "Epoch  72/100 Batch  120/169 - Loss:  0.630, Seconds: 5.45\n",
      "Epoch  72/100 Batch  140/169 - Loss:  0.628, Seconds: 6.49\n",
      "Epoch  72/100 Batch  160/169 - Loss:  0.645, Seconds: 5.66\n",
      "Average loss for this update: 0.634\n",
      "No Improvement.\n",
      "Epoch  73/100 Batch   20/169 - Loss:  0.633, Seconds: 4.34\n",
      "Epoch  73/100 Batch   40/169 - Loss:  0.589, Seconds: 4.59\n",
      "Average loss for this update: 0.617\n",
      "New Record!\n",
      "Epoch  73/100 Batch   60/169 - Loss:  0.636, Seconds: 4.87\n",
      "Epoch  73/100 Batch   80/169 - Loss:  0.620, Seconds: 5.00\n",
      "Epoch  73/100 Batch  100/169 - Loss:  0.622, Seconds: 5.13\n",
      "Average loss for this update: 0.625\n",
      "No Improvement.\n",
      "Epoch  73/100 Batch  120/169 - Loss:  0.615, Seconds: 5.32\n",
      "Epoch  73/100 Batch  140/169 - Loss:  0.630, Seconds: 5.44\n",
      "Epoch  73/100 Batch  160/169 - Loss:  0.636, Seconds: 5.61\n",
      "Average loss for this update: 0.628\n",
      "No Improvement.\n",
      "Epoch  74/100 Batch   20/169 - Loss:  0.628, Seconds: 4.35\n",
      "Epoch  74/100 Batch   40/169 - Loss:  0.573, Seconds: 4.60\n",
      "Average loss for this update: 0.609\n",
      "New Record!\n",
      "Epoch  74/100 Batch   60/169 - Loss:  0.631, Seconds: 4.83\n",
      "Epoch  74/100 Batch   80/169 - Loss:  0.598, Seconds: 4.95\n",
      "Epoch  74/100 Batch  100/169 - Loss:  0.621, Seconds: 5.17\n",
      "Average loss for this update: 0.615\n",
      "No Improvement.\n",
      "Epoch  74/100 Batch  120/169 - Loss:  0.620, Seconds: 5.29\n",
      "Epoch  74/100 Batch  140/169 - Loss:  0.606, Seconds: 5.46\n",
      "Epoch  74/100 Batch  160/169 - Loss:  0.611, Seconds: 5.60\n",
      "Average loss for this update: 0.609\n",
      "New Record!\n",
      "Epoch  75/100 Batch   20/169 - Loss:  0.619, Seconds: 4.91\n",
      "Epoch  75/100 Batch   40/169 - Loss:  0.581, Seconds: 4.61\n",
      "Average loss for this update: 0.602\n",
      "New Record!\n",
      "Epoch  75/100 Batch   60/169 - Loss:  0.610, Seconds: 4.81\n",
      "Epoch  75/100 Batch   80/169 - Loss:  0.587, Seconds: 4.96\n",
      "Epoch  75/100 Batch  100/169 - Loss:  0.604, Seconds: 5.22\n",
      "Average loss for this update: 0.6\n",
      "New Record!\n",
      "Epoch  75/100 Batch  120/169 - Loss:  0.598, Seconds: 5.31\n",
      "Epoch  75/100 Batch  140/169 - Loss:  0.588, Seconds: 5.46\n",
      "Epoch  75/100 Batch  160/169 - Loss:  0.602, Seconds: 5.62\n",
      "Average loss for this update: 0.595\n",
      "New Record!\n",
      "Epoch  76/100 Batch   20/169 - Loss:  0.607, Seconds: 4.38\n",
      "Epoch  76/100 Batch   40/169 - Loss:  0.555, Seconds: 4.60\n",
      "Average loss for this update: 0.587\n",
      "New Record!\n",
      "Epoch  76/100 Batch   60/169 - Loss:  0.612, Seconds: 4.82\n",
      "Epoch  76/100 Batch   80/169 - Loss:  0.587, Seconds: 4.94\n",
      "Epoch  76/100 Batch  100/169 - Loss:  0.598, Seconds: 5.21\n",
      "Average loss for this update: 0.6\n",
      "No Improvement.\n",
      "Epoch  76/100 Batch  120/169 - Loss:  0.603, Seconds: 5.30\n",
      "Epoch  76/100 Batch  140/169 - Loss:  0.587, Seconds: 5.44\n",
      "Epoch  76/100 Batch  160/169 - Loss:  0.580, Seconds: 5.58\n",
      "Average loss for this update: 0.587\n",
      "No Improvement.\n",
      "Epoch  77/100 Batch   20/169 - Loss:  0.604, Seconds: 4.34\n",
      "Epoch  77/100 Batch   40/169 - Loss:  0.567, Seconds: 4.63\n",
      "Average loss for this update: 0.585\n",
      "New Record!\n",
      "Epoch  77/100 Batch   60/169 - Loss:  0.598, Seconds: 4.87\n",
      "Epoch  77/100 Batch   80/169 - Loss:  0.583, Seconds: 4.98\n",
      "Epoch  77/100 Batch  100/169 - Loss:  0.588, Seconds: 5.12\n",
      "Average loss for this update: 0.591\n",
      "No Improvement.\n",
      "Epoch  77/100 Batch  120/169 - Loss:  0.578, Seconds: 5.28\n",
      "Epoch  77/100 Batch  140/169 - Loss:  0.584, Seconds: 5.49\n",
      "Epoch  77/100 Batch  160/169 - Loss:  0.583, Seconds: 5.62\n",
      "Average loss for this update: 0.578\n",
      "New Record!\n",
      "Epoch  78/100 Batch   20/169 - Loss:  0.585, Seconds: 4.33\n",
      "Epoch  78/100 Batch   40/169 - Loss:  0.548, Seconds: 4.66\n",
      "Average loss for this update: 0.572\n",
      "New Record!\n",
      "Epoch  78/100 Batch   60/169 - Loss:  0.589, Seconds: 5.12\n",
      "Epoch  78/100 Batch   80/169 - Loss:  0.567, Seconds: 5.34\n",
      "Epoch  78/100 Batch  100/169 - Loss:  0.576, Seconds: 5.14\n",
      "Average loss for this update: 0.576\n",
      "No Improvement.\n",
      "Epoch  78/100 Batch  120/169 - Loss:  0.575, Seconds: 5.28\n",
      "Epoch  78/100 Batch  140/169 - Loss:  0.583, Seconds: 5.39\n",
      "Epoch  78/100 Batch  160/169 - Loss:  0.575, Seconds: 5.51\n",
      "Average loss for this update: 0.579\n",
      "No Improvement.\n",
      "Epoch  79/100 Batch   20/169 - Loss:  0.573, Seconds: 4.27\n",
      "Epoch  79/100 Batch   40/169 - Loss:  0.552, Seconds: 4.56\n",
      "Average loss for this update: 0.572\n",
      "New Record!\n",
      "Epoch  79/100 Batch   60/169 - Loss:  0.593, Seconds: 4.78\n",
      "Epoch  79/100 Batch   80/169 - Loss:  0.573, Seconds: 4.91\n",
      "Epoch  79/100 Batch  100/169 - Loss:  0.585, Seconds: 5.09\n",
      "Average loss for this update: 0.581\n",
      "No Improvement.\n",
      "Epoch  79/100 Batch  120/169 - Loss:  0.574, Seconds: 5.25\n",
      "Epoch  79/100 Batch  140/169 - Loss:  0.560, Seconds: 5.40\n",
      "Epoch  79/100 Batch  160/169 - Loss:  0.553, Seconds: 5.58\n",
      "Average loss for this update: 0.56\n",
      "New Record!\n",
      "Epoch  80/100 Batch   20/169 - Loss:  0.585, Seconds: 4.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  80/100 Batch   40/169 - Loss:  0.523, Seconds: 4.54\n",
      "Average loss for this update: 0.556\n",
      "New Record!\n",
      "Epoch  80/100 Batch   60/169 - Loss:  0.572, Seconds: 5.48\n",
      "Epoch  80/100 Batch   80/169 - Loss:  0.576, Seconds: 5.56\n",
      "Epoch  80/100 Batch  100/169 - Loss:  0.552, Seconds: 5.13\n",
      "Average loss for this update: 0.571\n",
      "No Improvement.\n",
      "Epoch  80/100 Batch  120/169 - Loss:  0.563, Seconds: 5.23\n",
      "Epoch  80/100 Batch  140/169 - Loss:  0.555, Seconds: 5.42\n",
      "Epoch  80/100 Batch  160/169 - Loss:  0.559, Seconds: 5.52\n",
      "Average loss for this update: 0.555\n",
      "New Record!\n",
      "Epoch  81/100 Batch   20/169 - Loss:  0.565, Seconds: 4.30\n",
      "Epoch  81/100 Batch   40/169 - Loss:  0.527, Seconds: 4.54\n",
      "Average loss for this update: 0.551\n",
      "New Record!\n",
      "Epoch  81/100 Batch   60/169 - Loss:  0.563, Seconds: 4.76\n",
      "Epoch  81/100 Batch   80/169 - Loss:  0.533, Seconds: 4.93\n",
      "Epoch  81/100 Batch  100/169 - Loss:  0.557, Seconds: 5.12\n",
      "Average loss for this update: 0.55\n",
      "New Record!\n",
      "Epoch  81/100 Batch  120/169 - Loss:  0.548, Seconds: 5.23\n",
      "Epoch  81/100 Batch  140/169 - Loss:  0.538, Seconds: 5.43\n",
      "Epoch  81/100 Batch  160/169 - Loss:  0.543, Seconds: 5.55\n",
      "Average loss for this update: 0.54\n",
      "New Record!\n",
      "Epoch  82/100 Batch   20/169 - Loss:  0.553, Seconds: 4.29\n",
      "Epoch  82/100 Batch   40/169 - Loss:  0.536, Seconds: 4.54\n",
      "Average loss for this update: 0.545\n",
      "No Improvement.\n",
      "Epoch  82/100 Batch   60/169 - Loss:  0.548, Seconds: 4.76\n",
      "Epoch  82/100 Batch   80/169 - Loss:  0.542, Seconds: 4.93\n",
      "Epoch  82/100 Batch  100/169 - Loss:  0.532, Seconds: 5.10\n",
      "Average loss for this update: 0.541\n",
      "No Improvement.\n",
      "Epoch  82/100 Batch  120/169 - Loss:  0.530, Seconds: 5.25\n",
      "Epoch  82/100 Batch  140/169 - Loss:  0.542, Seconds: 5.38\n",
      "Epoch  82/100 Batch  160/169 - Loss:  0.564, Seconds: 5.56\n",
      "Average loss for this update: 0.544\n",
      "No Improvement.\n",
      "Epoch  83/100 Batch   20/169 - Loss:  0.553, Seconds: 4.27\n",
      "Epoch  83/100 Batch   40/169 - Loss:  0.523, Seconds: 4.59\n",
      "Average loss for this update: 0.541\n",
      "No Improvement.\n",
      "Epoch  83/100 Batch   60/169 - Loss:  0.548, Seconds: 4.76\n",
      "Epoch  83/100 Batch   80/169 - Loss:  0.537, Seconds: 4.92\n",
      "Epoch  83/100 Batch  100/169 - Loss:  0.522, Seconds: 5.10\n",
      "Average loss for this update: 0.535\n",
      "New Record!\n",
      "Epoch  83/100 Batch  120/169 - Loss:  0.540, Seconds: 5.24\n",
      "Epoch  83/100 Batch  140/169 - Loss:  0.535, Seconds: 5.39\n",
      "Epoch  83/100 Batch  160/169 - Loss:  0.545, Seconds: 5.58\n",
      "Average loss for this update: 0.54\n",
      "No Improvement.\n",
      "Epoch  84/100 Batch   20/169 - Loss:  0.546, Seconds: 4.29\n",
      "Epoch  84/100 Batch   40/169 - Loss:  0.509, Seconds: 4.58\n",
      "Average loss for this update: 0.535\n",
      "No Improvement.\n",
      "Epoch  84/100 Batch   60/169 - Loss:  0.550, Seconds: 4.76\n",
      "Epoch  84/100 Batch   80/169 - Loss:  0.534, Seconds: 4.92\n",
      "Epoch  84/100 Batch  100/169 - Loss:  0.539, Seconds: 5.07\n",
      "Average loss for this update: 0.536\n",
      "No Improvement.\n",
      "Epoch  84/100 Batch  120/169 - Loss:  0.532, Seconds: 5.22\n",
      "Epoch  84/100 Batch  140/169 - Loss:  0.535, Seconds: 5.38\n",
      "Epoch  84/100 Batch  160/169 - Loss:  0.525, Seconds: 5.55\n",
      "Average loss for this update: 0.53\n",
      "New Record!\n",
      "Epoch  85/100 Batch   20/169 - Loss:  0.545, Seconds: 4.28\n",
      "Epoch  85/100 Batch   40/169 - Loss:  0.502, Seconds: 4.52\n",
      "Average loss for this update: 0.527\n",
      "New Record!\n",
      "Epoch  85/100 Batch   60/169 - Loss:  0.542, Seconds: 4.76\n",
      "Epoch  85/100 Batch   80/169 - Loss:  0.527, Seconds: 4.92\n",
      "Epoch  85/100 Batch  100/169 - Loss:  0.532, Seconds: 5.12\n",
      "Average loss for this update: 0.534\n",
      "No Improvement.\n",
      "Epoch  85/100 Batch  120/169 - Loss:  0.519, Seconds: 5.23\n",
      "Epoch  85/100 Batch  140/169 - Loss:  0.533, Seconds: 5.40\n",
      "Epoch  85/100 Batch  160/169 - Loss:  0.528, Seconds: 5.53\n",
      "Average loss for this update: 0.526\n",
      "New Record!\n",
      "Epoch  86/100 Batch   20/169 - Loss:  0.539, Seconds: 4.34\n",
      "Epoch  86/100 Batch   40/169 - Loss:  0.500, Seconds: 4.64\n",
      "Average loss for this update: 0.522\n",
      "New Record!\n",
      "Epoch  86/100 Batch   60/169 - Loss:  0.533, Seconds: 4.76\n",
      "Epoch  86/100 Batch   80/169 - Loss:  0.533, Seconds: 4.89\n",
      "Epoch  86/100 Batch  100/169 - Loss:  0.532, Seconds: 5.08\n",
      "Average loss for this update: 0.538\n",
      "No Improvement.\n",
      "Epoch  86/100 Batch  120/169 - Loss:  0.529, Seconds: 5.25\n",
      "Epoch  86/100 Batch  140/169 - Loss:  0.526, Seconds: 5.43\n",
      "Epoch  86/100 Batch  160/169 - Loss:  0.519, Seconds: 5.55\n",
      "Average loss for this update: 0.517\n",
      "New Record!\n",
      "Epoch  87/100 Batch   20/169 - Loss:  0.541, Seconds: 4.29\n",
      "Epoch  87/100 Batch   40/169 - Loss:  0.504, Seconds: 4.57\n",
      "Average loss for this update: 0.524\n",
      "No Improvement.\n",
      "Epoch  87/100 Batch   60/169 - Loss:  0.535, Seconds: 4.79\n",
      "Epoch  87/100 Batch   80/169 - Loss:  0.528, Seconds: 4.91\n",
      "Epoch  87/100 Batch  100/169 - Loss:  0.524, Seconds: 5.08\n",
      "Average loss for this update: 0.53\n",
      "No Improvement.\n",
      "Epoch  87/100 Batch  120/169 - Loss:  0.531, Seconds: 5.26\n",
      "Epoch  87/100 Batch  140/169 - Loss:  0.520, Seconds: 5.40\n",
      "Epoch  87/100 Batch  160/169 - Loss:  0.526, Seconds: 5.54\n",
      "Average loss for this update: 0.525\n",
      "No Improvement.\n",
      "Epoch  88/100 Batch   20/169 - Loss:  0.548, Seconds: 4.29\n",
      "Epoch  88/100 Batch   40/169 - Loss:  0.485, Seconds: 4.54\n",
      "Average loss for this update: 0.518\n",
      "No Improvement.\n",
      "Epoch  88/100 Batch   60/169 - Loss:  0.532, Seconds: 4.75\n",
      "Epoch  88/100 Batch   80/169 - Loss:  0.510, Seconds: 4.90\n",
      "Epoch  88/100 Batch  100/169 - Loss:  0.521, Seconds: 5.07\n",
      "Average loss for this update: 0.52\n",
      "No Improvement.\n",
      "Epoch  88/100 Batch  120/169 - Loss:  0.508, Seconds: 5.23\n",
      "Epoch  88/100 Batch  140/169 - Loss:  0.510, Seconds: 5.42\n",
      "Epoch  88/100 Batch  160/169 - Loss:  0.523, Seconds: 5.58\n",
      "Average loss for this update: 0.512\n",
      "New Record!\n",
      "Epoch  89/100 Batch   20/169 - Loss:  0.511, Seconds: 4.27\n",
      "Epoch  89/100 Batch   40/169 - Loss:  0.483, Seconds: 4.57\n",
      "Average loss for this update: 0.501\n",
      "New Record!\n",
      "Epoch  89/100 Batch   60/169 - Loss:  0.517, Seconds: 4.77\n",
      "Epoch  89/100 Batch   80/169 - Loss:  0.500, Seconds: 4.88\n",
      "Epoch  89/100 Batch  100/169 - Loss:  0.526, Seconds: 5.09\n",
      "Average loss for this update: 0.52\n",
      "No Improvement.\n",
      "Epoch  89/100 Batch  120/169 - Loss:  0.526, Seconds: 5.24\n",
      "Epoch  89/100 Batch  140/169 - Loss:  0.507, Seconds: 5.39\n",
      "Epoch  89/100 Batch  160/169 - Loss:  0.524, Seconds: 5.52\n",
      "Average loss for this update: 0.514\n",
      "No Improvement.\n",
      "Epoch  90/100 Batch   20/169 - Loss:  0.510, Seconds: 4.30\n",
      "Epoch  90/100 Batch   40/169 - Loss:  0.476, Seconds: 4.54\n",
      "Average loss for this update: 0.501\n",
      "New Record!\n",
      "Epoch  90/100 Batch   60/169 - Loss:  0.526, Seconds: 4.77\n",
      "Epoch  90/100 Batch   80/169 - Loss:  0.502, Seconds: 4.91\n",
      "Epoch  90/100 Batch  100/169 - Loss:  0.504, Seconds: 5.11\n",
      "Average loss for this update: 0.508\n",
      "No Improvement.\n",
      "Epoch  90/100 Batch  120/169 - Loss:  0.497, Seconds: 5.26\n",
      "Epoch  90/100 Batch  140/169 - Loss:  0.517, Seconds: 5.41\n",
      "Epoch  90/100 Batch  160/169 - Loss:  0.495, Seconds: 5.56\n",
      "Average loss for this update: 0.499\n",
      "New Record!\n",
      "Epoch  91/100 Batch   20/169 - Loss:  0.505, Seconds: 4.29\n",
      "Epoch  91/100 Batch   40/169 - Loss:  0.470, Seconds: 4.54\n",
      "Average loss for this update: 0.492\n",
      "New Record!\n",
      "Epoch  91/100 Batch   60/169 - Loss:  0.505, Seconds: 4.76\n",
      "Epoch  91/100 Batch   80/169 - Loss:  0.485, Seconds: 4.94\n",
      "Epoch  91/100 Batch  100/169 - Loss:  0.496, Seconds: 5.10\n",
      "Average loss for this update: 0.492\n",
      "No Improvement.\n",
      "Epoch  91/100 Batch  120/169 - Loss:  0.491, Seconds: 5.21\n",
      "Epoch  91/100 Batch  140/169 - Loss:  0.501, Seconds: 5.40\n",
      "Epoch  91/100 Batch  160/169 - Loss:  0.503, Seconds: 5.50\n",
      "Average loss for this update: 0.5\n",
      "No Improvement.\n",
      "Epoch  92/100 Batch   20/169 - Loss:  0.517, Seconds: 4.28\n",
      "Epoch  92/100 Batch   40/169 - Loss:  0.468, Seconds: 4.63\n",
      "Average loss for this update: 0.495\n",
      "No Improvement.\n",
      "Epoch  92/100 Batch   60/169 - Loss:  0.506, Seconds: 4.88\n",
      "Epoch  92/100 Batch   80/169 - Loss:  0.499, Seconds: 5.01\n",
      "Epoch  92/100 Batch  100/169 - Loss:  0.492, Seconds: 5.09\n",
      "Average loss for this update: 0.5\n",
      "No Improvement.\n",
      "Epoch  92/100 Batch  120/169 - Loss:  0.499, Seconds: 5.26\n",
      "Epoch  92/100 Batch  140/169 - Loss:  0.496, Seconds: 5.37\n",
      "Epoch  92/100 Batch  160/169 - Loss:  0.500, Seconds: 5.56\n",
      "Average loss for this update: 0.496\n",
      "No Improvement.\n",
      "Epoch  93/100 Batch   20/169 - Loss:  0.503, Seconds: 4.28\n",
      "Epoch  93/100 Batch   40/169 - Loss:  0.461, Seconds: 4.55\n",
      "Average loss for this update: 0.487\n",
      "New Record!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  93/100 Batch   60/169 - Loss:  0.510, Seconds: 4.78\n",
      "Epoch  93/100 Batch   80/169 - Loss:  0.492, Seconds: 4.89\n",
      "Epoch  93/100 Batch  100/169 - Loss:  0.493, Seconds: 5.07\n",
      "Average loss for this update: 0.502\n",
      "No Improvement.\n",
      "Epoch  93/100 Batch  120/169 - Loss:  0.499, Seconds: 5.27\n",
      "Epoch  93/100 Batch  140/169 - Loss:  0.484, Seconds: 5.38\n",
      "Epoch  93/100 Batch  160/169 - Loss:  0.493, Seconds: 5.55\n",
      "Average loss for this update: 0.485\n",
      "New Record!\n",
      "Epoch  94/100 Batch   20/169 - Loss:  0.496, Seconds: 4.28\n",
      "Epoch  94/100 Batch   40/169 - Loss:  0.452, Seconds: 5.00\n",
      "Average loss for this update: 0.473\n",
      "New Record!\n",
      "Epoch  94/100 Batch   60/169 - Loss:  0.481, Seconds: 4.83\n",
      "Epoch  94/100 Batch   80/169 - Loss:  0.480, Seconds: 4.97\n",
      "Epoch  94/100 Batch  100/169 - Loss:  0.501, Seconds: 5.19\n",
      "Average loss for this update: 0.493\n",
      "No Improvement.\n",
      "Epoch  94/100 Batch  120/169 - Loss:  0.483, Seconds: 5.29\n",
      "Epoch  94/100 Batch  140/169 - Loss:  0.477, Seconds: 5.44\n",
      "Epoch  94/100 Batch  160/169 - Loss:  0.459, Seconds: 5.62\n",
      "Average loss for this update: 0.469\n",
      "New Record!\n",
      "Epoch  95/100 Batch   20/169 - Loss:  0.494, Seconds: 4.36\n",
      "Epoch  95/100 Batch   40/169 - Loss:  0.442, Seconds: 4.67\n",
      "Average loss for this update: 0.47\n",
      "No Improvement.\n",
      "Epoch  95/100 Batch   60/169 - Loss:  0.483, Seconds: 4.87\n",
      "Epoch  95/100 Batch   80/169 - Loss:  0.476, Seconds: 4.99\n",
      "Epoch  95/100 Batch  100/169 - Loss:  0.494, Seconds: 5.17\n",
      "Average loss for this update: 0.486\n",
      "No Improvement.\n",
      "Epoch  95/100 Batch  120/169 - Loss:  0.472, Seconds: 5.30\n",
      "Epoch  95/100 Batch  140/169 - Loss:  0.482, Seconds: 5.43\n",
      "Epoch  95/100 Batch  160/169 - Loss:  0.478, Seconds: 5.59\n",
      "Average loss for this update: 0.475\n",
      "No Improvement.\n",
      "Epoch  96/100 Batch   20/169 - Loss:  0.494, Seconds: 4.33\n",
      "Epoch  96/100 Batch   40/169 - Loss:  0.443, Seconds: 4.59\n",
      "Average loss for this update: 0.468\n",
      "New Record!\n",
      "Epoch  96/100 Batch   60/169 - Loss:  0.474, Seconds: 4.81\n",
      "Epoch  96/100 Batch   80/169 - Loss:  0.478, Seconds: 4.98\n",
      "Epoch  96/100 Batch  100/169 - Loss:  0.485, Seconds: 5.22\n",
      "Average loss for this update: 0.481\n",
      "No Improvement.\n",
      "Epoch  96/100 Batch  120/169 - Loss:  0.467, Seconds: 5.31\n",
      "Epoch  96/100 Batch  140/169 - Loss:  0.472, Seconds: 5.46\n",
      "Epoch  96/100 Batch  160/169 - Loss:  0.471, Seconds: 5.62\n",
      "Average loss for this update: 0.469\n",
      "No Improvement.\n",
      "Epoch  97/100 Batch   20/169 - Loss:  0.494, Seconds: 4.40\n",
      "Epoch  97/100 Batch   40/169 - Loss:  0.459, Seconds: 4.64\n",
      "Average loss for this update: 0.477\n",
      "No Improvement.\n",
      "Epoch  97/100 Batch   60/169 - Loss:  0.475, Seconds: 4.88\n",
      "Epoch  97/100 Batch   80/169 - Loss:  0.475, Seconds: 4.97\n",
      "Epoch  97/100 Batch  100/169 - Loss:  0.476, Seconds: 5.14\n",
      "Average loss for this update: 0.475\n",
      "No Improvement.\n",
      "Epoch  97/100 Batch  120/169 - Loss:  0.474, Seconds: 5.33\n",
      "Epoch  97/100 Batch  140/169 - Loss:  0.480, Seconds: 5.45\n",
      "Epoch  97/100 Batch  160/169 - Loss:  0.487, Seconds: 5.57\n",
      "Average loss for this update: 0.478\n",
      "No Improvement.\n",
      "Epoch  98/100 Batch   20/169 - Loss:  0.501, Seconds: 4.33\n",
      "Epoch  98/100 Batch   40/169 - Loss:  0.442, Seconds: 4.61\n",
      "Average loss for this update: 0.475\n",
      "No Improvement.\n",
      "Epoch  98/100 Batch   60/169 - Loss:  0.489, Seconds: 4.82\n",
      "Epoch  98/100 Batch   80/169 - Loss:  0.471, Seconds: 5.00\n",
      "Epoch  98/100 Batch  100/169 - Loss:  0.468, Seconds: 5.29\n",
      "Average loss for this update: 0.472\n",
      "No Improvement.\n",
      "Epoch  98/100 Batch  120/169 - Loss:  0.472, Seconds: 7.25\n",
      "Epoch  98/100 Batch  140/169 - Loss:  0.465, Seconds: 6.09\n",
      "Epoch  98/100 Batch  160/169 - Loss:  0.468, Seconds: 5.66\n",
      "Average loss for this update: 0.47\n",
      "No Improvement.\n",
      "Epoch  99/100 Batch   20/169 - Loss:  0.465, Seconds: 4.33\n",
      "Epoch  99/100 Batch   40/169 - Loss:  0.439, Seconds: 5.96\n",
      "Average loss for this update: 0.455\n",
      "New Record!\n",
      "Epoch  99/100 Batch   60/169 - Loss:  0.466, Seconds: 4.83\n",
      "Epoch  99/100 Batch   80/169 - Loss:  0.449, Seconds: 4.96\n",
      "Epoch  99/100 Batch  100/169 - Loss:  0.456, Seconds: 5.16\n",
      "Average loss for this update: 0.456\n",
      "No Improvement.\n",
      "Epoch  99/100 Batch  120/169 - Loss:  0.454, Seconds: 5.36\n",
      "Epoch  99/100 Batch  140/169 - Loss:  0.458, Seconds: 5.45\n",
      "Epoch  99/100 Batch  160/169 - Loss:  0.456, Seconds: 5.61\n",
      "Average loss for this update: 0.453\n",
      "New Record!\n",
      "Epoch 100/100 Batch   20/169 - Loss:  0.487, Seconds: 4.31\n",
      "Epoch 100/100 Batch   40/169 - Loss:  0.434, Seconds: 4.61\n",
      "Average loss for this update: 0.464\n",
      "No Improvement.\n",
      "Epoch 100/100 Batch   60/169 - Loss:  0.478, Seconds: 4.78\n",
      "Epoch 100/100 Batch   80/169 - Loss:  0.444, Seconds: 4.95\n",
      "Epoch 100/100 Batch  100/169 - Loss:  0.458, Seconds: 5.13\n",
      "Average loss for this update: 0.459\n",
      "No Improvement.\n",
      "Epoch 100/100 Batch  120/169 - Loss:  0.460, Seconds: 5.38\n",
      "Epoch 100/100 Batch  140/169 - Loss:  0.458, Seconds: 5.43\n",
      "Epoch 100/100 Batch  160/169 - Loss:  0.445, Seconds: 5.59\n",
      "Average loss for this update: 0.452\n",
      "New Record!\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 20 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 100 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "checkpoint = \"best_model.ckpt\" \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # If we want to continue training a previous session\n",
    "    #loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n",
    "    #loader.restore(sess, checkpoint)\n",
    "    \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learning_rate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(sorted_texts_short) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "            \n",
    "                    \n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "        \n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Our Own Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the quality of the summaries that this model can generate, you can either create your own review, or use a review from the dataset. You can set the length of the summary to a fixed value, or use a random value like I have here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_seq(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n",
      "Original Text: optical properties 001 oriented nbp single crystals studied wide spectral range 6 mev 3 ev room temperature 10 k itinerant carriers lead drude like contribution optical response identify two pronounced phonon modes interband transitions starting already rather low frequencies comparing experimental findings calculated interband optical conductivity assign features observed measured conductivity certain interband transitions particular find transitions electronic bands spilt spin orbit coupling dominate interband conductivity nbp 100 mev low temperatures momentum relaxing scattering rate itinerant carriers nbp small leading macroscopic characteristic length scales momentum relaxation approximately 0 5 \\mu\n",
      "\n",
      "Text\n",
      "  Word Ids:    [278, 479, 4786, 804, 4965, 463, 396, 4419, 2902, 203, 501, 126, 1874, 94, 9239, 2147, 266, 220, 986, 1847, 3137, 2961, 10113, 87, 2779, 278, 604, 11552, 43, 11638, 1034, 1184, 10875, 722, 5534, 21129, 6200, 108, 648, 3086, 903, 3468, 11462, 10875, 278, 1437, 22657, 19, 944, 5266, 1437, 1187, 10875, 722, 1595, 5461, 722, 418, 4513, 29966, 68, 937, 938, 13184, 10875, 1437, 4965, 222, 1874, 108, 2106, 454, 17207, 259, 3121, 1847, 3137, 4965, 1974, 2804, 1911, 3956, 173, 934, 454, 416, 11835, 699, 3799, 5376]\n",
      "  Input Words: optical properties 001 oriented nbp single crystals studied wide spectral range 6 mev 3 ev room temperature 10 k itinerant carriers lead drude like contribution optical response identify two pronounced phonon modes interband transitions starting already rather low frequencies comparing experimental findings calculated interband optical conductivity assign features observed measured conductivity certain interband transitions particular find transitions electronic bands spilt spin orbit coupling dominate interband conductivity nbp 100 mev low temperatures momentum relaxing scattering rate itinerant carriers nbp small leading macroscopic characteristic length scales momentum relaxation approximately 0 5 \\mu\n",
      "\n",
      "Summary\n",
      "  Word Ids:       [108, 44, 1010, 11, 3824]\n",
      "  Response Words: low dimensional localization for material\n"
     ]
    }
   ],
   "source": [
    "# Create your own review or use one from the dataset\n",
    "# input_sentence = \"I hope that you found this project to be rather interesting and informative. One of my main recommendations for working with this dataset and model is either use a GPU, a subset of the dataset, or plenty of time to train your model. As you might be able to expect, the model will not be able to make good predictions just by seeing many reviews, it needs so see the reviews many times to be able to understand the relationship between words and between descriptions & summaries.\"\n",
    "# text = text_to_seq(input_sentence)\n",
    "random = np.random.randint(0,len(clean_texts))\n",
    "input_sentence = clean_texts[random]\n",
    "text = text_to_seq(clean_texts[random])\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\"\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                      summary_length: [np.random.randint(5,8)], \n",
    "                                      text_length: [len(text)]*batch_size,\n",
    "                                      keep_prob: 1.0})[0] \n",
    "\n",
    "# Remove the padding from the tweet\n",
    "pad = vocab_to_int[\"<PAD>\"] \n",
    "\n",
    "print('Original Text:', input_sentence)\n",
    "\n",
    "print('\\nText')\n",
    "print('  Word Ids:    {}'.format([i for i in text]))\n",
    "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
    "\n",
    "print('\\nSummary')\n",
    "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html",
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Text\n",
    "  Word Ids:    [278, 479, 4786, 804, 4965, 463, 396, 4419, 2902, 203, 501, 126, 1874, 94, 9239, 2147, 266, 220, 986, 1847, 3137, 2961, 10113, 87, 2779, 278, 604, 11552, 43, 11638, 1034, 1184, 10875, 722, 5534, 21129, 6200, 108, 648, 3086, 903, 3468, 11462, 10875, 278, 1437, 22657, 19, 944, 5266, 1437, 1187, 10875, 722, 1595, 5461, 722, 418, 4513, 29966, 68, 937, 938, 13184, 10875, 1437, 4965, 222, 1874, 108, 2106, 454, 17207, 259, 3121, 1847, 3137, 4965, 1974, 2804, 1911, 3956, 173, 934, 454, 416, 11835, 699, 3799, 5376]\n",
    "  Input Words: optical properties 001 oriented nbp single crystals studied wide spectral range 6 mev 3 ev room temperature 10 k itinerant carriers lead drude like contribution optical response identify two pronounced phonon modes interband transitions starting already rather low frequencies comparing experimental findings calculated interband optical conductivity assign features observed measured conductivity certain interband transitions particular find transitions electronic bands spilt spin orbit coupling dominate interband conductivity nbp 100 mev low temperatures momentum relaxing scattering rate itinerant carriers nbp small leading macroscopic characteristic length scales momentum relaxation approximately 0 5 \\mu\n",
    "\n",
    "Summary\n",
    "  Word Ids:       [108, 44, 1010, 11, 3824]\n",
    "  Response Words: low dimensional localization for material\n",
    "  \n",
    "  \n",
    "Text\n",
    "  Word Ids:    [6252, 4722, 4723, 218, 3492, 213, 2546, 351, 2141, 709, 1017, 1859, 45710, 8501, 45710, 4863, 7778, 11971, 5200, 14040, 289, 1017, 1018, 844, 1166, 326, 1653, 3721, 218, 219, 944, 108, 1124, 490, 217, 1905, 5461, 115, 1017, 1859, 3189, 261, 1539, 4671, 14723, 5536, 4722, 4723, 3492, 22289, 2176, 385, 3492, 670, 580, 12900, 8361, 21348, 150, 3543, 8499, 1902, 5240, 21532, 1507, 3492, 12091, 1931, 21113, 34, 1993, 4722, 4723, 496, 3131, 10462, 7943, 5385, 9841, 1017, 1859, 9904, 7040, 5536, 3442, 999, 283, 219, 4928, 699, 3799, 54, 1933, 38, 999, 283, 3492, 1776, 12900, 4334, 12841, 4334, 11716, 22360, 27972, 5016, 7110, 4723, 5016, 6878, 9637, 5016, 6878, 4132, 2813, 1465, 732, 12878, 2531, 8586, 5385, 8101, 1017, 1859, 9904, 3103, 8289, 923, 8334, 13130, 263, 45710, 5568, 3799, 6877, 45710, 45710, 6878, 1017, 1859, 9904, 12878, 3531, 45710, 670, 278, 2592, 3856, 5690, 749, 7475, 7963, 432, 45710, 14409, 699, 91, 9904, 1519, 2695, 263, 45710, 5568, 3799, 6877, 45710, 45710, 6878, 19797, 29469, 8586]\n",
    "  Input Words: estimate 21 cm radio background accretion onto first intermediate mass black holes <UNK> 30 <UNK> 16 combining potentially optimistic plausible scenarios black hole formation growth empirical correlations luminosity radio emission observed low redshift active galactic nuclei find model black holes forming molecular cooling halos able produce 21 cm background exceeds cosmic microwave background cmb z \\approx 17 though models involving larger halo masses entirely excluded background could explain surprisingly large amplitude 21 cm absorption feature recently reported edges collaboration black holes would also produce significant x ray emission contribute 0 5 2 kev soft x ray background level \\approx 10^{ 13} 10^{ 12} erg sec ^{ 1} cm ^{ 2} deg ^{ 2} consistent existing constraints order avoid heating igm edges trough black holes would need obscured hydrogen column depths n <UNK> \\sim 5 \\times <UNK> <UNK> 2} black holes would avoid violating <UNK> cmb optical depth planck uv photon escape fractions f <UNK> \\lesssim 0 1 would natural result n <UNK> \\sim 5 \\times <UNK> <UNK> 2} imposed unheated igm\n",
    "\n",
    "Summary\n",
    "  Word Ids:       [79, 976, 20, 79, 217, 8751]\n",
    "  Response Words: the number of the galactic circulation\n",
    "  \n",
    "  \n",
    "Text\n",
    "  Word Ids:    [835, 5255, 4686, 4527, 894, 3050, 5392, 68, 5980, 10020, 5980, 1111, 2334, 240, 273, 21122, 3491, 1987, 16294, 1170, 266, 634, 220, 986, 13735, 4116, 68, 1707, 137, 54, 11478, 1338, 11478, 447, 5980, 291, 12091, 6576, 699, 11547, 3202, 91, 5376, 91, 5376, 3260, 634, 240, 6003, 220, 3940, 108, 266, 9783, 463, 2703, 68, 5441, 3101, 976, 4199, 3103, 25185, 2738, 220, 1607, 7040, 21122, 922, 1520, 5340, 9904, 14391, 1911, 5539]\n",
    "  Input Words: free falling nanodiamond containing nitrogen vacancy centre spin superposition experience superposition forces inhomogeneous magnetic field propose practical design brings internal temperature diamond 10 k extends expected spin coherence time 2 ms 500 ms spatial superposition distance could increased 0 05 nm 1 \\mu 1 \\mu diameter diamond magnetic inhomogeneity 10 ^4 low temperature allows single shot spin readout reducing number nanodiamonds need dropped factor 10 000 also propose solutions generic obstacle would prevent macroscopic superpositions\n",
    "\n",
    "Summary\n",
    "  Word Ids:       [664, 51, 68, 5980, 30, 652, 626]\n",
    "  Response Words: matter and spin superposition in vacuum experiment\n",
    "  \n",
    "Text\n",
    "  Word Ids:    [4432, 5538, 8103, 10496, 2329, 4525, 1646, 4513, 1446, 1959, 163, 3565, 2099, 6638, 12033, 2329, 43, 44, 1703, 302, 303, 7172, 1565, 2492, 5858, 21358, 1520, 475, 2329, 43, 44, 1703, 959, 325, 2, 137, 1418, 45710, 65, 1565, 2492, 2694, 779, 1528, 7527, 1703, 43, 475, 325, 4432, 43, 475, 542, 3705, 1565, 2492, 4063, 45710, 962, 5538, 3423, 21203, 43, 475, 542, 1565, 2492, 1567, 2237, 475, 3386, 6, 16625, 1466, 9054, 976, 1452, 11027, 54, 6718, 8617, 13062, 45710, 965, 17448, 325, 3705, 1565, 2492, 21122, 779, 576, 354, 43, 800, 767, 2922, 1565, 449, 916, 2074, 2952, 3350, 3672, 2682, 13911, 9054, 976, 3650, 131, 743, 3406, 976, 4513, 21826, 43, 580, 2237, 779, 1528, 7527, 475, 2329, 186, 17123, 17124, 2492, 1060, 54, 43, 4513, 1890, 1889, 1565, 2492, 13357, 542, 1060, 54, 699, 1060, 54, 91, 1152, 6345, 4513, 5419, 16923, 12033, 186, 17123, 17124, 2492, 8622, 2693, 12635, 6345, 4513, 21577, 5538, 8103, 976, 4513, 21826, 43, 21124, 8773, 1141, 475, 542, 12033, 186, 17123, 17124, 2492, 22851, 1152, 698, 65, 12033, 186, 732, 2329, 208, 2510, 3264, 2002]\n",
    "  Input Words: show wannier obstruction fragile topology nearly flat bands twisted bilayer graphene magic angle manifestations nontrivial topology two dimensional real wave functions characterized euler class prove examine generic band topology two dimensional real fermions systems space time inversion <UNK> symmetry euler class integer topological invariant classifying real two band systems show two band system nonzero euler class cannot <UNK> symmetric wannier representation moreover two band system euler class e {2} band crossing points whose total winding number equal 2e 2 thus conventional nielsen <UNK> theorem fails systems nonzero euler class propose topological phase transition two insulators carrying distinct euler classes described terms pair creation annihilation vortices accompanied winding number changes across dirac strings number bands bigger two z {2} topological invariant classifying band topology second stiefel whitney class w 2 two bands even odd euler class turn system w 2 0 w 2 1 additional trivial bands added although nontrivial second stiefel whitney class remains robust adding trivial bands impose wannier obstruction number bands bigger two however resulting multi band system nontrivial second stiefel whitney class supplemented additional chiral symmetry nontrivial second order topology associated corner charges guaranteed\n",
    "\n",
    "Summary\n",
    "  Word Ids:       [79, 2054, 779, 2329, 779, 2329]\n",
    "  Response Words: the enhanced topological topology topological topology\n",
    "1\n",
    "Text\n",
    "\n",
    "Text\n",
    "  Word Ids:    [1272, 139, 21241, 4842, 137, 1567, 304, 2, 16307, 12146, 5883, 2426, 4129, 9629, 644, 106, 11142, 45710, 13733, 18695, 45710, 9629, 14803, 14571, 45710, 41664, 9629, 45710, 5013, 2865, 5906, 275, 2961, 5150, 1292, 303, 6718, 4133, 350, 17751, 5488, 798, 2678, 21279, 138, 145, 3293, 5809, 9455, 869, 1631, 7587, 1997, 182, 1489, 1159, 139, 2535, 8191, 9841, 4571, 3459, 17697, 849, 41665, 8524, 818, 2318, 21138, 1663, 1307, 18953, 13526, 2481, 11839, 12072, 91, 7600, 869, 798, 5901, 224, 347, 734, 4527, 890, 629, 12273, 18588, 1141, 1848, 302, 4997, 45710, 6022, 54, 2678, 2558, 1152, 1141, 1848, 668, 7040, 2084, 4049, 444, 8085, 787, 3477, 20041, 9124, 48, 3650, 9846, 991, 2545, 9846, 8167, 6618, 5414, 94, 942, 411, 869, 21680, 10245, 16211, 2144, 2542, 12273, 8514, 16064, 8217, 5157, 247, 20352, 16211, 21680, 27740, 13383, 139, 262, 3393, 9027, 1707, 45710, 869, 3744, 9783, 6252, 3956, 4446, 173, 1993, 1410, 15050, 5839, 863, 3799, 1300, 45710, 296, 2485, 1352, 3393, 126, 7397, 9884, 2381, 2678, 2751, 4525, 1352, 1756, 21214]\n",
    "  Input Words: astronomical data typically irregular time e g space hipparcos tycho kepler gaia wise etc ground based ccd <UNK> asas crts <UNK> etc photographic harvard <UNK> odessa etc <UNK> surveys leads cancellation conditions lead orthogonality basic functions thus simplified methods give biased parameters approximations elaborated series algorithms programs statistically correct analysis applied 2000 variable stars different types data obtained international collaboration frame inter longitude astronomy ila campaign highlights studies presented extended list original publications main improvements done 1 periodogram analysis parameters determined complete set equations containing algebraic polynomial trend superimposed multi harmonic wave detrending <UNK> used 2 approximations use additional multi harmonic waves also special shapes patterns parts light curve correspond relatively fast changes minima eclipsing binaries minima maxima pulsating variables 3 auto correlation analysis acf taking account bias due trend removal previously subtraction sample mean taken account acf irregularly spaced data 4 signals bad coherence <UNK> analysis proposed allows estimate characteristic cycle length amplitude well provide realistic approximation 5 extension <UNK> type wavelet periodic signals 6 running parabola sine approximations aperiodic nearly periodic variations respectively\n",
    "\n",
    "Summary\n",
    "  Word Ids:       [79, 1736, 20, 3467, 275, 2998, 30]\n",
    "  Response Words: the statistics of initial conditions cross in\n",
    "\n",
    "\n",
    "Text\n",
    "  Word Ids:    [2432, 172, 14070, 3820, 1573, 3414, 9474, 3147, 8954, 387, 388, 659, 4432, 2432, 8249, 803, 3015, 45710, 3, 26597, 351, 732, 15360, 349, 2535, 2851, 2432, 172, 8825, 925, 872, 3820, 208, 4999, 5057, 3409, 4208, 2239, 45710, 26575, 349, 21138, 511, 12772, 2432, 15360, 172, 13060, 86, 2851, 841, 8000, 5092, 12618, 490, 118, 13116, 266]\n",
    "  Input Words: basal slip screw dislocations hexagonal closed packed titanium investigated ab initio calculations show basal dissociation highly unstable <UNK> structures dissociated first order pyramidal plane obtained mechanism basal slip corresponds migration partial dislocations associated stacking fault ribbon direction perpendicular <UNK> tion plane presented results indicate basal pyramidal slip operate peierls mechanism double kink nucleation equally active high enough temperature\n",
    "\n",
    "Summary\n",
    "  Word Ids:       [4966, 431, 23, 2147, 30, 1420, 1806]\n",
    "  Response Words: intrinsic induced by room in dense circuit"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Text\n",
    "  Word Ids:    [919, 3895, 378, 609, 835, 508, 545, 357, 127, 378, 245, 333, 14164, 273, 378, 245, 3337, 3731, 1070, 2460, 327, 3563, 452, 14535, 13977, 56, 168, 3034, 4152, 8615, 245, 3757, 2517, 14319, 126, 122, 8555, 1030, 56, 127, 12179, 5579, 4873, 8703, 357, 127, 452, 7961, 144, 3809, 60, 14246, 3034, 1975, 357, 127, 452, 5759, 3757, 56, 127, 2460, 4840, 1303, 32768, 2619, 9459, 10994, 346, 853, 14146, 144, 1860, 14789, 1426, 118, 423, 56, 127, 2460, 4840, 15306, 14563, 8703, 357, 127, 452, 110, 142, 3563, 452, 6300, 3337, 861, 54, 11028, 104, 1100, 7101, 14149, 5579, 11028, 88, 3372, 7643, 1150, 4381, 78, 3853, 810, 7961, 5027, 10005]\n",
    "  Input Words: paper studies boundary feedback stabilization class diagonal infinite dimensional boundary control systems studied setting boundary control input subject constant delay open loop system might exhibit finite number unstable modes proposed control design strategy consists two main steps first finite dimensional subsystem obtained truncation original infinite dimensional system ids via modal decomposition includes unstable components infinite dimensional system allows design finite dimensional delay controller means <UNK> transformation pole shifting theorem second shown via selection adequate lyapunov function 1 finite dimensional delay controller successfully stabilizes original infinite dimensional system 2 closed loop system exponentially input state stable iss respect distributed disturbances finally obtained iss property used derive small gain condition ensuring stability ids ode interconnection\n",
    "\n",
    "Summary\n",
    "  Word Ids:       [357, 609, 835, 1, 41, 4050]\n",
    "  Response Words: infinite feedback stabilization of a port\n",
    "    \n",
    "    \n",
    "\n",
    "Text\n",
    "  Word Ids:    [998, 3963, 14458, 12529, 17864, 4373, 5481, 4373, 2181, 17170, 17865, 12316, 337, 12529, 12792, 17864, 4373, 11710, 2768, 58, 2768, 276, 277, 2618, 704, 485, 15091, 2715, 5298, 4727, 10808, 4784, 607, 6060, 170, 2291, 7140, 486, 704, 7417, 17866, 17170]\n",
    "  Input Words: many species live colonies thrive collapse upon collapse individuals survive survivors start new colonies sites thrive collapse introduce spatial non spatial stochastic processes modeling population dynamic besides testing whether dispersion helps survival model experiencing large fluctuations obtain conditions population get extinct survive\n",
    "\n",
    "Summary\n",
    "  Word Ids:       [4372, 1, 4373, 24, 883]\n",
    "  Response Words: colonization of collapse and efficiency\n",
    "  \n",
    "  \n",
    "Text\n",
    "  Word Ids:    [1591, 842, 342, 458, 1070, 5889, 765, 14295, 126, 337, 426, 2731, 521, 988, 186, 1030, 7643, 186, 853, 3547, 5182, 1111, 9090, 284, 162, 569, 301, 14295, 186, 853, 3547, 5182, 1111, 754, 9090, 284, 301, 853, 5182, 426, 3547, 426, 847, 284, 301, 6856, 186, 13384, 1529]\n",
    "  Input Words: given complex square matrix constant row sum establish two new eigenvalue inclusion sets using bounds first derive bounds second largest smallest eigenvalues adjacency matrices k regular graphs establish bounds second largest smallest eigenvalues normalized adjacency matrices graphs second smallest eigenvalue largest eigenvalue laplacian matrices graphs sharpness bounds verified examples\n",
    "\n",
    "Summary\n",
    "  Word Ids:       [426, 186, 45, 284]\n",
    "  Response Words: eigenvalue bounds for matrices\n",
    "  \n",
    "  \n",
    "  \n",
    "Text\n",
    "  Word Ids:    [11710, 1615, 1774, 2429, 320, 1501, 828, 564, 6454, 583, 664, 2560, 2145, 66, 187, 1808, 458, 727, 1591, 1541, 876, 12726, 7471, 1049, 7954, 4343, 727, 1541, 876, 564, 8111, 664, 14315, 1240, 6038, 2429, 1591, 945, 14160, 16532, 7110, 14594, 5637, 14160, 14234, 794, 730, 583, 664, 828, 415, 14159, 11103, 8169, 3085, 76]\n",
    "  Input Words: introduce novel kind robustness linear programming solution x called robust optimal realizations objective functions coefficients constraint matrix entries given interval domains appropriate choices right hand side entries interval domains x remains optimal propose method check robustness given point also recommend suitable candidate found also discuss topological properties robust optimal solution set illustrate applicability concept transportation problem\n",
    "\n",
    "Summary\n",
    "  Word Ids:       [583, 664, 583, 664, 6044, 1953, 3190]\n",
    "  Response Words: robust optimal robust optimal stopping through forall\n",
    "1\n",
    "Text\n",
    "\n",
    "\n",
    "Text\n",
    "  Word Ids:    [1215, 1128, 11666, 32768, 2396, 346, 2147, 331, 297, 66, 8702, 6968, 32768, 2990, 2672, 988, 4638, 847, 32768, 3890, 32768, 3372, 1457, 32768, 2396, 346, 14308, 2074, 1662, 1660, 331, 2651, 640, 66, 2990, 2228, 919, 11086, 191, 550, 490, 308, 4875, 219, 14142, 346, 3890, 9650, 6968]\n",
    "  Input Words: l^2 version celebrated <UNK> carleman theorem regarding quasi analytic functions proved chernoff <UNK> \\mathbb r^d using iterates laplacian <UNK> ingham <UNK> used classical <UNK> carleman theorem relate decay fourier transform quasi analyticity integrable functions \\mathbb r paper extend theorems riemannian symmetric spaces noncompact type show theorem ingham follows chernoff\n",
    "\n",
    "Summary\n",
    "  Word Ids:       [6531, 1189, 69, 979, 1657, 2383]\n",
    "  Response Words: ruled numerical in hyperbolic kepler tomography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Examples of reviews and summaries:\n",
    "- Review(1): The coffee tasted great and was at such a good price! I highly recommend this to everyone!\n",
    "- Summary(1): great coffee\n",
    "\n",
    "\n",
    "- Review(2): This is the worst cheese that I have ever bought! I will never buy it again and I hope you won't either!\n",
    "- Summary(2): omg gross gross\n",
    "\n",
    "\n",
    "- Review(3): love individual oatmeal cups found years ago sam quit selling sound big lots quit selling found target expensive buy individually trilled get entire case time go anywhere need water microwave spoon know quaker flavor packets\n",
    "- Summary(3): love it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope that you found this project to be rather interesting and informative. One of my main recommendations for working with this dataset and model is either use a GPU, a subset of the dataset, or plenty of time to train your model. As you might be able to expect, the model will not be able to make good predictions just by seeing many reviews, it needs so see the reviews many times to be able to understand the relationship between words and between descriptions & summaries. \n",
    "\n",
    "In short, I'm pleased with how well this model performs. After creating numerous reviews and checking those from the dataset, I can happily say that most of the generated summaries are appropriate, some of them are great, and some of them make mistakes. I'll try to improve this model and if it gets better, I'll update my GitHub.\n",
    "\n",
    "Thanks for reading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
