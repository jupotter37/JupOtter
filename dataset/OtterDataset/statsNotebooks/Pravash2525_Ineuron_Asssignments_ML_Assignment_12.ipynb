{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "130dc62d",
   "metadata": {},
   "source": [
    "<h1> <u> <font color= green > ML_Assignment_12 </font> </u> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7291810",
   "metadata": {},
   "source": [
    "## 1. What is prior probability? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb9b782",
   "metadata": {},
   "source": [
    "> In Bayesian statistics, **prior probability** is the probability of an event before any evidence is taken into account. It is a subjective probability that is based on the knowledge and beliefs of the person making the estimate.\n",
    "\n",
    ">For example, suppose we  are trying to estimate the probability that a patient has a disease. we  might start with a prior probability of 20%. This means that we  believe there is a 20% chance that the patient has the disease before we  have any evidence. <br>\n",
    "After we collect some evidence, such as the patient's symptoms, we can update your prior probability using Bayes' theorem. Bayes' theorem tells us how to update your probability of an event based on new evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67077563",
   "metadata": {},
   "source": [
    "## 2. What is posterior probability? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ec733",
   "metadata": {},
   "source": [
    "> **Posterior probability** is the probability of an event after some evidence has been observed. It is calculated using Bayes' theorem, which is a formula that updates your prior probability based on new evidence.\n",
    "\n",
    "> For example, suppose we are trying to estimate the probability that a patient has a disease. we might start with a prior probability of 20%. This means that we believe there is a 20% chance that the patient has the disease before we have any evidence.\n",
    "> After we collect some evidence, such as the patient's symptoms, we can update our prior probability using Bayes' theorem. Bayes' theorem tells us how to update your probability of an event based on new evidence.\n",
    "\n",
    "> The posterior probability is a measure of how much oour belief in the event has changed after we have seen the evidence. It is a more accurate estimate of the probability of the event than the prior probability, because it takes into account the evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab13d77",
   "metadata": {},
   "source": [
    "## 3. What is likelihood probability? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f2a2eb",
   "metadata": {},
   "source": [
    "> **Likelihood probability** is the probability of observing the data given a particular hypothesis. It is often used in Bayesian statistics to update the prior probability of an event.\n",
    "\n",
    "> For example, suppose we are trying to estimate the probability that a patient has a disease. we might start with a prior probability of 20%. This means that we believe there is a 20% chance that the patient has the disease before we have any evidence.The likelihood probability is a measure of how likely the data is if the hypothesis is true. It is calculated by considering all the possible outcomes of the experiment and weighting them according to their probability.\n",
    "\n",
    "> In this example, the likelihood probability of the patient having the disease given their symptoms would be high. This is because the symptoms are consistent with the disease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625deab5",
   "metadata": {},
   "source": [
    "## 4. What is Naïve Bayes classifier? Why is it named so?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a0df4e",
   "metadata": {},
   "source": [
    "> A **Naive Bayes classifier** is a type of machine learning algorithm that is used for classification tasks. It is based on Bayes' theorem, which is a formula that updates prior probability based on new evidence.\n",
    "\n",
    "> **Naive Bayes classifiers** make the naive assumption that the features of a data point are independent of each other. This means that the probability of a data point belonging to a particular class is calculated by multiplying together the probabilities of each of the features belonging to that class.\n",
    "\n",
    "> The name **\"naive Bayes\"** comes from the fact that the algorithm makes the naive assumption that the features of a data point are independent of each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be12b69",
   "metadata": {},
   "source": [
    "## 5. What is optimal Bayes classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dbe65a",
   "metadata": {},
   "source": [
    "> An **optimal Bayes classifier** is a classifier that minimizes the Bayes error rate. The **Bayes error rate** is the probability that the classifier will misclassify a data point.\n",
    "\n",
    "> The **optimal Bayes classifier** is the classifier that makes the best use of the prior probabilities and the likelihood probabilities. It is the classifier that is most likely to correctly classify a data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b7dbfb",
   "metadata": {},
   "source": [
    "## 6. Write any two features of Bayesian learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a6f5c1",
   "metadata": {},
   "source": [
    "> Two features of Bayesian learning methods:\n",
    "> * **Probabilistic**: Bayesian learning methods treat all model parameters as probabilistic variables. This means that they can represent uncertainty in the model, which can be useful for tasks such as forecasting and decision-making.\n",
    "> * **Flexible**: Bayesian learning methods are very flexible and can be used to model a wide variety of data. This is because they do not make any assumptions about the distribution of the data, which allows them to adapt to the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0868e9c2",
   "metadata": {},
   "source": [
    "## 7. Define the concept of consistent learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedc0424",
   "metadata": {},
   "source": [
    "> In the field of machine learning, a **consistent learner** is a learning algorithm that will eventually converge to the correct hypothesis as the amount of training data increases.\n",
    "\n",
    "> In other words, a **consistent learner** will eventually learn the true underlying distribution of the data, even if it starts out with a very poor hypothesis. <br>\n",
    "> Another common definition is that a learner is consistent if the expected error rate tends to zero as the amount of training data increases.\n",
    "\n",
    "> examples of consistent learners:\n",
    "> * **Naive Bayes classifiers** are consistent learners, as they make the assumption that the features of a data point are independent of each other. This assumption is often not true, but it can still be a useful approximation in many cases.\n",
    "> * **Support vector machines** are also consistent learners, as they are based on a mathematical optimization problem that is guaranteed to converge to the correct solution as the amount of training data increases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ffc70f",
   "metadata": {},
   "source": [
    "## 8. Write any two strengths of Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c597779",
   "metadata": {},
   "source": [
    "> two strengths of Bayes classifier:\n",
    "> * **Interpretability**: Bayesian classifiers are often more interpretable than other machine learning algorithms. This is because they provide probabilities for each hypothesis, which can be used to understand the model's predictions.\n",
    "> * **Flexibility**: Bayesian classifiers are very flexible and can be used to model a wide variety of data. This is because they do not make any assumptions about the distribution of the data, which allows them to adapt to the data.\n",
    "> * **Scalability**: Bayes classifiers can be scaled to large datasets. This is because they do not rely on complex mathematical operations, which can be computationally expensive for large datasets.\n",
    "> * **Robustness**: Bayes classifiers are relatively robust to noise in the data. This is because they use probabilities to make predictions, which allows them to account for uncertainty in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea0b9b2",
   "metadata": {},
   "source": [
    "## 9. Write any two weaknesses of Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c87ee90",
   "metadata": {},
   "source": [
    ">  two weaknesses of Bayes classifier:\n",
    "> * **Dependence on priors**: The performance of Bayes classifiers can depend on the choice of priors. If the priors are not chosen carefully, then the classifier can perform poorly.\n",
    "> * **Computational complexity**: Bayes classifiers can be computationally expensive to train, especially for large datasets.\n",
    "> * **Not always optimal**: Bayes classifiers are not always the optimal classifier for a given task. There are other machine learning algorithms that can outperform Bayes classifiers in some cases.\n",
    "> * **Sensitive to rare events**: Bayes classifiers can be sensitive to rare events. This is because they use probabilities to make predictions, and if a rare event occurs, then the probability of predicting it correctly can be low.\n",
    "> * **Not suitable for continuous data**: Bayes classifiers are not suitable for continuous data. This is because they make the assumption that the features of a data point are discrete, which is not always the case for continuous data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fad878",
   "metadata": {},
   "source": [
    "## 10. Explain how Naïve Bayes classifier is used for\n",
    "1. Text classification\n",
    "2. Spam filtering\n",
    "3. Market sentiment analysis\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da08fd75",
   "metadata": {},
   "source": [
    "<b><i> Text classification </i></b> \n",
    "> * Naive Bayes classifiers are often used for text classification tasks, such as classifying emails as spam or ham, classifying news articles as political or sports, or classifying customer reviews as positive or negative.\n",
    "\n",
    "> * In text classification, Naive Bayes classifiers work by first creating a vocabulary of all the words that appear in the training data. Then, they calculate the probability of each word appearing in each class.\n",
    "\n",
    "> * Once the probabilities have been calculated, the Naive Bayes classifier can then be used to classify new emails. To classify a new email, the classifier simply calculates the probability of the email belonging to each class and then classifies the email to the class with the highest probability.\n",
    "---\n",
    "<b><i> Spam filtering </i></b> \n",
    "> * Naive Bayes classifiers are also often used for spam filtering tasks. Spam filtering is the task of automatically classifying emails as spam or ham.\n",
    "\n",
    "> * Spam emails often contain certain words or phrases that are not typically found in ham emails. For example, spam emails often contain words like \"free\", \"money\", and \"click here\".\n",
    "\n",
    "> * Naive Bayes classifiers can be used to identify these words and phrases in spam emails. By identifying these words and phrases, the Naive Bayes classifier can then classify new emails as spam or ham.\n",
    "---\n",
    "<b><i> Market sentiment analysis </i></b> \n",
    "> * Naive Bayes classifiers can also be used for market sentiment analysis tasks. Market sentiment analysis is the task of analyzing the sentiment of market participants, such as investors, traders, and analysts.\n",
    "\n",
    "> * Sentiment can be positive, negative, or neutral. Positive sentiment indicates that market participants are optimistic about the market, while negative sentiment indicates that market participants are pessimistic about the market.\n",
    "\n",
    "> * Naive Bayes classifiers can be used to identify the sentiment of market participants by analyzing the words and phrases that they use. For example, words like \"buy\", \"bullish\", and \"up\" are often associated with positive sentiment, while words like \"sell\", \"bearish\", and \"down\" are often associated with negative sentiment.\n",
    "\n",
    "> * By identifying the words and phrases that market participants use, the Naive Bayes classifier can then classify the sentiment of market participants as positive, negative, or neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d9f3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5936d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
