{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Distributions\n",
    "\n",
    "Here we we'll review the most important Statistical Distributions for hypothesis\n",
    "testing.\n",
    "\n",
    "## SciPy\n",
    "To perform Hypothesis Testing during could interviews, we would ideally be able \n",
    "to import **stats** function from **scipy** libraries to calculate de PDFs of\n",
    "the distributions. \n",
    "\n",
    "## Key Distributions and Use Cases\n",
    "\n",
    "### Normal distribution (stats.norm)\n",
    "* Used for z-tests or when sample means are normally distributed (Central Limit Theorem)\n",
    "- `stats.norm.cdf(x, loc=mean, scale=std_dev)`\n",
    "\n",
    "### t (stats.t)\n",
    "* Used for one-sample, two-sample, and paired t-tests. The t-distribution is used\n",
    "when the sample size is small (n<30) or if the population variance is unknown.\n",
    "- `stats.t.cdf(x, df)` where\n",
    "    * $df: Degrees of freedom.\n",
    "\n",
    "### Chi-Squared (stats.chi2)\n",
    "* Used to test whether observed data follows an expected distribution (goodness of fit) or to assess independence between categorical variables (e.g., contingency tables).\n",
    "- `stats.chi2.cdf(x,df)`\n",
    "\n",
    "\n",
    "### Binomial (stats.binom)\n",
    "* Useful when modeling **binary events** with fixed probabilities over a number of independent trials.\n",
    "- `stats.binom.cdf(k,n,p)`  where\n",
    "    * $n$: Number of trias\n",
    "    * $p$: probability of success in each trial\n",
    "    * $k$: Number of observed successes\n",
    "\n",
    "### Poison Distribution (stats.poison)\n",
    "* Model counts of events occurring over a fixed period or space, such as the \n",
    "number of website visits in an hour.\n",
    "- `stats.poisson.cdf(k,mu)` where\n",
    "    * $k$:Observed count\n",
    "    $ $\\mu$: Expected count (mean of the distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Sample T-Test\n",
    "The one-sample t-test tests whether the mean of a sample differs significantly \n",
    "from a known or hypothesized population mean. It assumes the data is normally \n",
    "distributed.\n",
    "\n",
    "* $H_0$ is that the sample mean $\\bar{x}$ is equal to the population mean $\\mu$.\n",
    "* $H_1$:   $\\bar{x} \\neq \\mu$\n",
    "\n",
    "Test statistic:\n",
    "\n",
    "$$ t = \\frac{\\bar{x}-\\mu}{s/\\sqrt{n} }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "class TTest:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.t_stat = None\n",
    "        self.p_value = None\n",
    "    \n",
    "    def one_sample_t_test(self, sample, mu, method = 'analytical') -> float:\n",
    "        \"\"\"\n",
    "        Perform a one-sample t-test to check if the sample mean is significantly\n",
    "        different from the population mean\n",
    "\n",
    "        Parameters:\n",
    "            sample: array-like. \n",
    "                The sample data.\n",
    "            mu: float.\n",
    "                The population mean, \n",
    "            method: string.\n",
    "                Either 'analytical' or 'monte_carlo' \n",
    "\n",
    "        Returns:\n",
    "            t-stat: float.\n",
    "                The estimated t-statistic\n",
    "            p-value: float.\n",
    "                The estimated p-value for the t-test. This is the probability of \n",
    "                obtaining test results *at least as extreme* as the observed in\n",
    "                the data\n",
    "        \"\"\"\n",
    "        if method not in ['analytical', 'monte_carlo']:\n",
    "            raise ValueError(\"method bust be one of ['analytical', 'monte_carlo']\")\n",
    "\n",
    "        # GUarantee that sample is a numpy array\n",
    "        X = np.array(sample)\n",
    "\n",
    "        # Sample size n and degrees of freedom df\n",
    "        n = len(X)\n",
    "        df = n-1\n",
    "        \n",
    "        # Compute sample statistics \n",
    "        X_bar = np.mean(X)  # Sample mean\n",
    "        X_std = np.std(X, ddof=1)  # Unbiased sample standard deviation\n",
    "\n",
    "        # t-statistic\n",
    "        self.t_stat = (X_bar - mu)/(X_std/(n**0.5))\n",
    "        \n",
    "        # Two-Tailed P-Value \n",
    "\n",
    "        if method == 'analytical':\n",
    "            self.p_value = 2 * (1 - stats.t.cdf( abs(self.t_stat), df)   )\n",
    "        else:\n",
    "            self.p_value = self.monte_carlo_t_test( mu, X_std, n, n_simulations = 10**5)\n",
    "\n",
    "        return self.t_stat, self.p_value\n",
    "\n",
    "        # Monte-Carlo\n",
    "    def monte_carlo_t_test(self, mu, X_std, n, n_simulations = 10**5):\n",
    "        \"\"\"\n",
    "        Monte Carlo simulation for one-sample t-test p-value estimation.\n",
    "\n",
    "        Parameters:\n",
    "            mu: float.\n",
    "                The population mean.\n",
    "            X_std: float.\n",
    "                The sample standard deviation.\n",
    "            n: int.\n",
    "                The sample size.\n",
    "            n_simulations: int.\n",
    "                The number of Monte Carlo simulations.\n",
    "\n",
    "        Returns:\n",
    "            p-value: float.\n",
    "                The estimated p-value based on the Monte Carlo simulation.\n",
    "        \"\"\"\n",
    "        counter = 0\n",
    "        \n",
    "        for _ in range(n_simulations):\n",
    "            random_sample = np.random.normal(mu, X_std, n )\n",
    "            sample_bar = np.mean(random_sample)  #Sample mean\n",
    "            sample_std = np.std(random_sample, ddof =1)\n",
    "            t_sample =  (sample_bar - mu)/(sample_std/(n**0.5) )\n",
    "\n",
    "            if abs(t_sample) >=  abs(self.t_stat):\n",
    "                counter += 1\n",
    "\n",
    "        # Return estimated p-value\n",
    "        return counter/n_simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Implementation - t-statistic: -1.14502, p-value: 0.26160\n",
      "Scipy Implementation - t-statistic: -1.14502, p-value: 0.26156\n"
     ]
    }
   ],
   "source": [
    "# Generate sample data\n",
    "np.random.seed(42)  # Ensure reproducibility\n",
    "sample_data = np.random.normal(loc=5, scale=2, size=30)  # Sample from N(5, 2^2)\n",
    "mu_population = 5  # Population mean to compare\n",
    "\n",
    "# 1. Use your TTest class\n",
    "ttest = TTest()\n",
    "my_t_stat, my_p_value = ttest.one_sample_t_test(sample_data, mu_population,method = 'monte_carlo')\n",
    "\n",
    "# 2. Use scipy's t-test for comparison\n",
    "scipy_t_stat, scipy_p_value = stats.ttest_1samp(sample_data, popmean=mu_population)\n",
    "\n",
    "# Print the results\n",
    "print(f\"My Implementation - t-statistic: {my_t_stat:.5f}, p-value: {my_p_value:.5f}\")\n",
    "print(f\"Scipy Implementation - t-statistic: {scipy_t_stat:.5f}, p-value: {scipy_p_value:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [2.5, 3.0, 2.8, 3.5, 3.1, 2.9, 3.0]\n",
    "ttest = TTest()\n",
    "t_stat, p_val = ttest.one_sample_t_test(sample, mu=3.0)\n",
    "print(\"T-Statistic:\", t_stat)\n",
    "print(\"P-Value:\", p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Sample T-Test (t-test for independent samples)\n",
    "The two-sample t-test compares the means of two independent samples to determine \n",
    "if they come from populations with the same mean. It assumes that both samples \n",
    "are normally distributed. \n",
    "\n",
    "The hypotheses are:\n",
    "\n",
    "* **Null Hypothesis**        $H_0$: $\\bar{X_1} = \\bar{X_2}$\n",
    "* **Alternative Hypothesis** $H_1$: $\\bar{X_1} \\neq \\bar{X_2}$\n",
    "\n",
    "There are **two main versions**:\n",
    "\n",
    "### 1. Pooled Variance\n",
    "This version assumes that both groups comes from  **populations with the same variance**.\n",
    "\n",
    "Test statistic:\n",
    "\n",
    "$$ t = \\frac{\\bar{X_1}-\\bar{X_2}}{ \\sqrt{s_p^2 (\\frac{1}{n_1} + \\frac{1}{n_2} )} }$$\n",
    "\n",
    "where the **pooled variance** $s_p^2$ is the weighted average of the variances: \n",
    "\n",
    "$$ s_p^2 = \\frac{(n_1 -1)  s_1^2 + (n_2 -1)  s_2^2 }{n_1 +n_2 -2} $$\n",
    "\n",
    "The weights are given by the degrees of freedom of each sample, ensuring that larger samples have more influence on the variance estimation:\n",
    "\n",
    "Sample Variance Formula:\n",
    "$$s_1^2 = \\frac{1}{n_1-1} \\sum_{i=1}^{n_1}  (x_{1i} -\\bar{x_1})^2$$\n",
    "\n",
    "The **degrees of freedom** for the test is $$df = n_1+n_2-2$$\n",
    "\n",
    "#### Why Pooled Variance?\n",
    "\n",
    "\n",
    "* Even though we assume that both samples are drawn from populations with the **same variance**, the **sample variances** may differ due to randomness. \n",
    "\n",
    "* The pooled variance s_p^2 provides a **more accurate estimate** of the true population variance by **combining information from both samples**. \n",
    "\n",
    "* The **weights based on degrees of freedom** ensure that larger samples influence the pooled variance more.\n",
    "\n",
    "### 2.Unequal Variance (Welch's t-tsets)\n",
    "\n",
    "The Welch's does not assume equal population variances. It is more robust when variances differ between the two samples.\n",
    "\n",
    "Test Statistics: \n",
    "\n",
    "$$ t_{\\text{Welch}} = \\frac{\\bar{X_1}-\\bar{X_2}}{  \\sqrt{ \\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2} }  }$$\n",
    "\n",
    "with the degrees of freedom defined by the equation (*forget about it...*)\n",
    "\n",
    "$$ df = \\frac{ (s_1^2/n_1 ) + (s_2^2/n_2) ^2 }{\\frac{1}{n_1-1} (\\frac{s_1^2}{n_1})^2   + \\frac{1}{n_2-1} (\\frac{s_2^2}{n_2})^2 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTest:\n",
    "\n",
    "    \n",
    "    def two_sample_t_test_monte_carlo(self, sample_1: list, sample_2: list) -> float:\n",
    "        \"\"\"\n",
    "        Perform a two-sample t-test to check if the sample mean is significantly\n",
    "        different from the population mean.\n",
    "        Assumption: the samples are drawn from populations with the same variance.\n",
    "\n",
    "        Parameters:\n",
    "            sample_1: array-like. \n",
    "                The first sample data.\n",
    "            sample_2: array-like. \n",
    "                The second sample data.\n",
    "            mu: float.\n",
    "                The population mean, \n",
    "\n",
    "        Returns:\n",
    "            t-stat: float.\n",
    "                The estimated t-statistic\n",
    "            p-value: float.\n",
    "                The estimated p-value for the t-test. This is the probability of \n",
    "                obtaining test results *at least as extreme* as the observed in\n",
    "                the data.\n",
    "        \"\"\"\n",
    "\n",
    "        # GUarantee that sample is a numpy array\n",
    "        X1= np.array(sample_1)\n",
    "        X2= np.array(sample_2)\n",
    "\n",
    "        # Sample size n and degrees of freedom df\n",
    "        n1, n2 = len(X1), len(X2)\n",
    "        df = n1 + n2 -2\n",
    "        \n",
    "        # Compute sample statistics \n",
    "        X1_bar = sum(X1)/n1                  #Sample mean\n",
    "        X2_bar = sum(X2)/n2                  #Sample mean\n",
    "\n",
    "        X1_var = sum( (X1-X1_bar)**2 )/(n1-1) # Sample variance\n",
    "        X2_var = sum( (X2-X2_bar)**2 )/(n2-1) # variance\n",
    "\n",
    "        pooled_variance = ( (n1-1)*X1_var + (n2-1)*X2_var ) / df\n",
    "        weighted_average = ( n1*X1_bar + n2*X2_bar ) / (n1+n2)\n",
    "        # t-statistic\n",
    "        denominator =  (pooled_variance* (1/n1 + 1/n2)) **(1/2)\n",
    "        self.t_stat = (X1_bar - X2_bar)/denominator\n",
    "\n",
    "\n",
    "        # Monte Carlo\n",
    "        n_simulations = 10**5\n",
    "        count = 0\n",
    "\n",
    "        for _ in range(n_simulations):\n",
    "\n",
    "            # Assuming X1_bar is the mean of X2\n",
    "            sample_X1 = np.random.normal(weighted_average, pooled_variance**0.5, n1)\n",
    "            sample_X2 = np.random.normal(weighted_average, pooled_variance**0.5, n2)\n",
    "\n",
    "            sample_X1_bar = np.sum(sample_X1)/(n1)\n",
    "            sample_X2_bar = np.sum(sample_X2)/(n2)  \n",
    "\n",
    "            sample_test = (sample_X1_bar - sample_X2_bar) / denominator\n",
    "\n",
    "            if abs(sample_test) > abs(self.t_stat):\n",
    "                count += 1\n",
    "\n",
    "        # p-value 2---\n",
    "        self.p_value = count/n_simulations\n",
    "\n",
    "        return self.t_stat, self.p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Statistic: 3.715152686079336, P-Value: 0.00016\n",
      "Scipy t-statistic: 3.71515, p-value: 0.00023\n"
     ]
    }
   ],
   "source": [
    "# Example data\n",
    "sample1 = [68, 78, 74, 72, 77]\n",
    "sample2 = [68, 65, 70, 67, 69]\n",
    "\n",
    "# Perform the t-test\n",
    "ttest = TTest()\n",
    "t_stat, p_val = ttest.two_sample_t_test_monte_carlo(sample1, sample2)\n",
    "print(f\"T-Statistic: {t_stat}, P-Value: {p_val}\")\n",
    "\n",
    "# 1. Scipy implementation of t-test\n",
    "scipy_t, scipy_p = stats.ttest_ind(sample1, sample2)\n",
    "print(f\"Scipy t-statistic: {scipy_t:.5f}, p-value: {scipy_p:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats \n",
    "class TTest:\n",
    "\n",
    "    \n",
    "    def two_sample_t_test(self, sample_1: list, sample_2: list) -> float:\n",
    "        \"\"\"\n",
    "        Perform a two-sample t-test to check if the sample mean is significantly\n",
    "        different from the population mean.\n",
    "        Assumption: the samples are drawn from populations with the same variance.\n",
    "\n",
    "        Parameters:\n",
    "            sample_1: array-like. \n",
    "                The first sample data.\n",
    "            sample_2: array-like. \n",
    "                The second sample data.\n",
    "            mu: float.\n",
    "                The population mean, \n",
    "\n",
    "        Returns:\n",
    "            t-stat: float.\n",
    "                The estimated t-statistic\n",
    "            p-value: float.\n",
    "                The estimated p-value for the t-test. This is the probability of \n",
    "                obtaining test results *at least as extreme* as the observed in\n",
    "                the data.\n",
    "        \"\"\"\n",
    "\n",
    "        # GUarantee that sample is a numpy array\n",
    "        X1= np.array(sample_1)\n",
    "        X2= np.array(sample_2)\n",
    "\n",
    "        # Sample size n and degrees of freedom df\n",
    "        n1, n2 = len(X1), len(X2)\n",
    "        df = n1 + n2 -2\n",
    "        \n",
    "        # Compute sample statistics \n",
    "        X1_bar = sum(X1)/n1                  #Sample mean\n",
    "        X2_bar = sum(X2)/n2                  #Sample mean\n",
    "\n",
    "        X1_var = sum( (X1-X1_bar)**2 )/(n1-1) # Sample variance\n",
    "        X2_var = sum( (X2-X2_bar)**2 )/(n2-1) # variance\n",
    "\n",
    "        pooled_variance = ( (n1-1)*X1_var + (n2-1)*X2_var ) / df\n",
    "\n",
    "        # t-statistic\n",
    "        denominator =  (pooled_variance* (1/n1 + 1/n2)) **(1/2)\n",
    "        self.t_stat = (X1_bar - X2_bar)/denominator\n",
    "        # p-value 2---\n",
    "        self.p_value = 2 * (1 - stats.t.cdf( abs(self.t_stat), df)   )\n",
    "\n",
    "        return self.t_stat, self.p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data\n",
    "sample1 = [75, 78, 74, 72, 77]\n",
    "sample2 = [68, 65, 70, 67, 69]\n",
    "\n",
    "# Perform the t-test\n",
    "ttest = TTest()\n",
    "t_stat, p_val = ttest.two_sample_t_test(sample1, sample2)\n",
    "print(f\"T-Statistic: {t_stat}, P-Value: {p_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anova (Analysis of Variance)\n",
    "See docs: https://docs.google.com/document/d/13b2W1HfUgijqNlsQ_3fQgJ6pIyJTJ5UdjuOSy2BXNW8/edit?usp=sharing\n",
    "\n",
    "ANOVA is used to determine if there are any statistically significant difference\n",
    "between the meaans of three or more groups. It compares the \n",
    "**variance between groups** with the **variance within groups** to check if at \n",
    "least one group's mean is different\n",
    "\n",
    "* $H_0$: All groups means are equal: $\\bar{x_1} = \\bar{x_2} = ...= \\bar{x_n}$\n",
    "* $H_1$: Att least one group mean is differente.\n",
    "\n",
    "\n",
    "The test statistic is the F-Statistic $$ \\frac{SSB/(k-1)}{SSW/(n-k)}   $$\n",
    "\n",
    "where $k$ is the number of groups, $n$ is the total number of observations \n",
    "across  all groups.  \n",
    "\n",
    "$SSB$ is the **Between-groups sum of squares**,  \n",
    "            $$ SSB = \\sum_{i=1:k} n_i (\\bar{x_i} -\\bar{x})^2  $$ \n",
    "\n",
    "where\n",
    " * $n_i$ is the number of observations in group $i$ \n",
    " * $\\bar{x_i}$: mean of group $i$ \n",
    " * $\\bar{x}$: overall mean across all groups\n",
    "\n",
    " and  SSW is the **Within groups sum of squares**,\n",
    "        $$ SSW = \\sum_{i=1:k} \\sum_{j \\text{in group i}} (x_{ij} - \\bar{x_i})^2   $$\n",
    "    \n",
    "where $x_{ij}$ is observation $j$ in group $i$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# The implementation comes from chatGPT. It is currently too hard for me. I suggest you skip it.\n",
    "class ANOVA:\n",
    "    def __init__(self):\n",
    "        self.f_statistic = None\n",
    "        self.p_value = None\n",
    "\n",
    "    def one_way_anova(self, *groups):\n",
    "        \"\"\"\n",
    "        Perform one-way ANOVA to test whether the means of multiple groups are equal.\n",
    "\n",
    "        Parameters:\n",
    "        *groups : array-like\n",
    "            Each argument represents a group of sample data.\n",
    "\n",
    "        Returns:\n",
    "        f_statistic : float\n",
    "            The computed F-statistic.\n",
    "        p_value : float\n",
    "            The p-value for the ANOVA test.\n",
    "        \"\"\"\n",
    "        # Calculate the total number of observations and number of groups\n",
    "        n_total = sum([len(group) for group in groups])\n",
    "        k = len(groups)\n",
    "\n",
    "        # Calculate the grand mean\n",
    "        grand_mean = np.mean([x for group in groups for x in group])\n",
    "\n",
    "        # Calculate Between-group sum of squares (SSB)\n",
    "        ssb = sum([len(group) * (np.mean(group) - grand_mean) ** 2 for group in groups])\n",
    "\n",
    "        # Calculate Within-group sum of squares (SSW)\n",
    "        ssw = sum([sum((x - np.mean(group)) ** 2 for x in group) for group in groups])\n",
    "\n",
    "        # Degrees of freedom\n",
    "        df_between = k - 1\n",
    "        df_within = n_total - k\n",
    "\n",
    "        # Mean squares\n",
    "        ms_between = ssb / df_between\n",
    "        ms_within = ssw / df_within\n",
    "\n",
    "        # F-statistic\n",
    "        self.f_statistic = ms_between / ms_within\n",
    "\n",
    "        # P-value (using F-distribution)\n",
    "        self.p_value = 1 - stats.f.cdf(self.f_statistic, df_between, df_within)\n",
    "\n",
    "        return self.f_statistic, self.p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi-Squared Test \n",
    "\n",
    "The Chi-squared tests are a family of statistical tests used for categorical data\n",
    "analysis. They evaluate the **association** or **independence** between categorical\n",
    "variables, or how well the observed data fits an expected distribution.\n",
    "\n",
    "The Chi-Squared tests are **non-parametric**, meaning that they don't assume the data follows a specific distribution,\n",
    "\n",
    "The **Goodness of Fit** test checks how well an observed distribution matches an \n",
    "expected one, while the **Independence Test\" evaluates whethet two categorical \n",
    "variables are independent. \n",
    "\n",
    "There are 3 main types of Chi-squared tests, but since 2 are very similar, I'm \n",
    "gonna treat it as a subcase.\n",
    "\n",
    "\n",
    "\n",
    "#### Test Statististic\n",
    "\n",
    "All Chi-squared test statistics are the given by:\n",
    "\n",
    "$$  \\chi^2 = \\sum \\frac{(O_i-E_i)^2}{E_i} $$\n",
    "\n",
    "where \n",
    "* $O_i$ is the Observed frequency (from sample)\n",
    "* $E_i$ is the Expected frequency \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Goodness of Fit\n",
    "\n",
    "Determines if a sample matches a specific distribution. It is used when there is \n",
    "**one categorical variable** and we want to see if the observed data fits a know\n",
    "distribution. For example, we can test if a dice is fair.\n",
    "\n",
    "#### Hypotheses:\n",
    "\n",
    "* $H_0$: The observed frequencies follow the specified distribution\n",
    "* $H_1$: The observed frequencies **do not** follow the specified distribution\n",
    "\n",
    "Example:\n",
    "\n",
    "* $H_0$: The die is fair (each outcome has equal probability).\n",
    "* $H_1$: The die is biased (at least one outcome has a different probability).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChiSquaredTests:\n",
    "    def chi_squared_goodness_of_fit(self, observed, expected):\n",
    "        \"\"\"\n",
    "        Perform a chi-squared goodness of fit test.\n",
    "        -----------\n",
    "        Parameters\n",
    "        -----------\n",
    "        observed: array-like.\n",
    "            The observed frequencies.\n",
    "        expected: array-like,\n",
    "            The expected frequency.\n",
    "        ----------\n",
    "        Returns\n",
    "        ----------\n",
    "        chi2_stat: float\n",
    "            The computed chi-squared statistic.\n",
    "        p-value: float\n",
    "            The estimated p-value for the test. This is the probability of \n",
    "            obtaining test results *at least as extreme* as the observed in\n",
    "            the data.\n",
    "        \"\"\"\n",
    "\n",
    "        #Guarantees that data is a numpy array\n",
    "        observed = np.array(observed)\n",
    "        expected = np.array(expected)\n",
    "    \n",
    "        if len(observed) != len(expected):\n",
    "            raise ValueError(\"Observed and expected arrays must have the same length.\")\n",
    "        if np.any(observed < 0) or np.any(expected < 0):\n",
    "            raise ValueError(\"Observed and expected frequencies must be non-negative.\")\n",
    "\n",
    "        # Avoid division by zero by adding a small epsilon\n",
    "        epsilon = 1e-10\n",
    "        expected = np.where(expected == 0, epsilon, expected)\n",
    "\n",
    "        # Compute test statistic and degrees of freedom (df)\n",
    "        chi2_stat =  sum( ((observed - expected)**2)/expected )\n",
    "        df = len(observed) -1 \n",
    "\n",
    "        p_value = 1 - stats.chi2.cdf(chi2_stat, df  )\n",
    "\n",
    "        return chi2_stat, p_value\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observed frequencies of a dice roll\n",
    "observed = [16, 18, 16, 14, 12, 24]\n",
    "\n",
    "# Expected frequencies if the die is fair\n",
    "expected = [100 / 6] * 6\n",
    "\n",
    "# Instantiate the ChiSquaredTest class\n",
    "chi_test = ChiSquaredTests()\n",
    "\n",
    "# Perform the chi-squared goodness of fit test\n",
    "chi2_stat, p_value = chi_test.chi_squared_goodness_of_fit(observed, expected)\n",
    "\n",
    "print(f\"Chi-Squared Statistic: {chi2_stat.round(3)}\")\n",
    "print(f\"P-Value: {p_value.round(3)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Since p-value is high, we do not reject the null hypothesis of a fair dice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note on the Chi-Squared Distribution\n",
    "\n",
    "Bellow are the PDF (left) and CDF (right) for the Chi-Squared distribution with\n",
    "different degrees of freedom. \n",
    "\n",
    "<img src=\"../images/chi_squared_pdf.png\" alt=\"left\" width=\"450\"/>\n",
    "\n",
    "<img src=\"../images/chi_squared_cdf.png\" alt=\"right\" width=\"450\"/>\n",
    "\n",
    "Since the $\\chi^2$ statistic is always non-negative, the **distribution is inherently one-sided**, focusing only on positive deviations from the expected values. \n",
    "\n",
    "Thus, the p-value for the Chi-Squared test is calculated using **only the right tail** of the distribution:\n",
    "$$ \\text{p-value} = 1 - CDF(\\chi^2, df) $$\n",
    "\n",
    "### Comparison with the T-Test\n",
    "\n",
    "In contrast, the **t-statistic** can be **positive or negative**, requiring a two-tailed test. Thus, the p-value calculation for a two-tailed t-test is:\n",
    "\n",
    "$$ \\text{p-value} =2 * (1 - CDF(|t|), df)) $$\n",
    "\n",
    "Since $(1 - CDF(|t|), df)$ gives the right-tail probability of observing a value\n",
    "greater that $|t|$, we need to multiply it by to to account for the left side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo for p-value Approximation\n",
    "Using Monte Carlo simulation is a practical alternative to compute the p-value \n",
    "if statistical libraries like scipy are restricted. This method is intuitive, demonstrates strong statistical reasoning, and aligns well with the \n",
    "problem-solving mindset that Google interviews often look for.\n",
    "\n",
    "## How it works\n",
    "\n",
    "1. Compute the **test statistic** for the observed data.\n",
    "\n",
    "2. Generate **simulated data** using a known distribution.\n",
    "\n",
    "3. Count how many simulated statistics are *as extreme* (greater or equal) to the observed test statistic.\n",
    "\n",
    "4. Estimate de **p-value**  as the proportion of such extreme results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChiSquaredTests:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.expected = None\n",
    "        self.observed_chi2_stat = None\n",
    "\n",
    "\n",
    "\n",
    "    def chi_squared_goodness_of_fit(self, observed, expected):\n",
    "        \"\"\"\n",
    "        Perform a chi-squared goodness of fit test using Monte Carlo Simulation.\n",
    "        -----------\n",
    "        Parameters\n",
    "        -----------\n",
    "        observed: array-like.\n",
    "            The observed frequencies.\n",
    "        expected: array-like,\n",
    "            The expected frequency.\n",
    "        ----------\n",
    "        Returns\n",
    "        ----------\n",
    "        chi2_stat: float\n",
    "            The computed chi-squared statistic.\n",
    "        p-value: float\n",
    "            The estimated p-value for the test. This is the probability of \n",
    "            obtaining test results *at least as extreme* as the observed in\n",
    "            the data.\n",
    "        \"\"\"\n",
    "\n",
    "        #Guarantees that data is a numpy array\n",
    "        observed = np.array(observed)\n",
    "        self.expected = np.array(expected)\n",
    "\n",
    "        # Compute test statisticfor the observed data\n",
    "        self.observed_chi2_stat =  sum( ((observed - self.expected)**2)/self.expected )\n",
    "        \n",
    "        p_value = self.monte_carlo_chi2(n_simulations = 10*12)\n",
    "\n",
    "        return self.observed_chi2_stat, p_value\n",
    "\n",
    "\n",
    "    def monte_carlo_chi2(self, n_simulations = 10000):\n",
    "\n",
    "        # Monte Carlo counter to estimate the p-value\n",
    "        more_extreme_count = 0\n",
    "\n",
    "        # Define the corresponding observations (can be any labels or indices)\n",
    "        labels = np.arange(len(self.expected))\n",
    "        probs = self.expected / np.sum(self.expected)   #normalized probabilities\n",
    "        for _ in range(n_simulations):    \n",
    "            \n",
    "            n_samples = np.sum(self.expected).astype(int)\n",
    "            simulated_outcomes = np.random.choice( labels, \n",
    "                                                   size = n_samples, \n",
    "                                                   p = probs)\n",
    "    \n",
    "            simulated_freq = np.bincount(simulated_outcomes, minlength=len(self.expected))\n",
    "\n",
    "            # Compute chi-squared statistic for simulated data\n",
    "            simulated_chi2_stat = np.sum((simulated_freq - self.expected ) ** 2 / (self.expected ))\n",
    "            if simulated_chi2_stat >= self.observed_chi2_stat:\n",
    "                more_extreme_count += 1\n",
    "\n",
    "        p_value = more_extreme_count/n_simulations\n",
    "\n",
    "        return p_value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Example usage\n",
    "observed = [8, 18, 16, 14, 12, 24]\n",
    "expected = [100 / 6] * 6\n",
    "\n",
    "# Adjust expected frequencies to match the sum of observed frequencies\n",
    "expected = np.array(expected) * (np.sum(observed) / np.sum(expected))\n",
    "\n",
    "# Instantiate and run Monte Carlo chi-squared test\n",
    "chi_test = ChiSquaredTests()\n",
    "chi2_stat, monte_carlo_p_value = chi_test.chi_squared_goodness_of_fit(observed, expected)\n",
    "\n",
    "print(f\"Monte Carlo p-value: {monte_carlo_p_value:.5f}\")\n",
    "\n",
    "# Compute chi-squared test using scipy.stats\n",
    "scipy_chi2_stat, scipy_p_value = stats.chisquare(f_obs=observed, f_exp=expected)\n",
    "\n",
    "print(f\"Scipy chi-squared statistic: {scipy_chi2_stat:.3f}\")\n",
    "print(f\"Scipy p-value: {scipy_p_value:.5f}\")\n",
    "\n",
    "# Compare results\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"Monte Carlo Chi-squared Statistic: {chi2_stat:.3f}\")\n",
    "print(f\"Monte Carlo p-value: {monte_carlo_p_value:.5f}\")\n",
    "print(f\"Scipy Chi-squared Statistic: {scipy_chi2_stat:.3f}\")\n",
    "print(f\"Scipy p-value: {scipy_p_value:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(observations,size= 10, p=frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = [100 / 6] * 6\n",
    "n_samples = 100\n",
    "labels = np.arange(len(expected))\n",
    "simulated_outcomes = np.random.choice(labels, size=n_samples, p=expected )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.92"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "75+(1.96*2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
