{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>It Starts with a Research Question...</h1>\n",
    "<img src='Long, So 263, Fig 8.png' width=\"66%\" height=\"66%\">\n",
    "<img src='Long, So 257, Fig 5.png' width=\"66%\" height=\"66%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literary Distinction (Probably)\n",
    "<ul><li>Preview</li>\n",
    "<li>Review</li>\n",
    "<li>Pre-Processing</li>\n",
    "<ul>\n",
    "<li>Import Corpus</li>\n",
    "<li>Stop Words</li>\n",
    "<li>Feature Selection</li></ul>\n",
    "<li>Classification</li>\n",
    "<ul>\n",
    "<li>Training, Feature Importance, & Prediction</li>\n",
    "<li>Literary Distinction</li>\n",
    "<li>Extra: Cross-Validation</li></ul>\n",
    "</ul>\n",
    "\n",
    "We will work through supervised machine learning via a classification task. This lesson is based on the paper by Hoyt Long and Richard So: [\"Literary Pattern Recognition: Modernism between Close Reading and Machine Learning.\"](https://www.journals.uchicago.edu/doi/abs/10.1086/684353)\n",
    "\n",
    "Although Long and So's study of modernist haiku motivates this lesson, a substantial portion of their corpus remains under copyright so they have not made it available publicly. Instead we will apply their methods to the corpus distributed by Ted Underwood and Jordan Sellers in support of their own literary historical study on nineteenth- and early-twentieth century volumes of poetry that were reviewed in prestigious magazines versus not at all. (The idea being that even a negative review indicates valuable, critical engagement.)\n",
    "\n",
    "In essence, our task will be to learn the vocabulary of literary prestige, rather than that of haiku. We will however be deliberate in using Long and So's methods, since they reflect assumptions about language that are more appropriate to a general introduction.\n",
    "\n",
    "Our task: build a machine learning classifier that will \"learn\" how to distinguish between prestigious poems and poems that have not been recognized as being prestigious. Our input will simply be 360 examples of poems that have been reviewed in literary journals - a mark of prestige (labeled *reviewed*), and 360 examples of poems that have not been reviewed (labeled *random*). We will then use an algorithm, in our case, the relatively simple *Naive Bayes Classifier* to determine what features, in our case words, distinguish reviewed poems from random poems.\n",
    "\n",
    "We will use the large and powerful Python library [Scikit-Learn](https://scikit-learn.org/stable/) to implement our classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jinyang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get texts of interest that belong to identifiably different categories\n",
    "\n",
    "unladen_swallow = 'high air-speed velocity'\n",
    "swallow_grasping_coconut = 'low air-speed velocity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>air-speed</th>\n",
       "      <th>velocity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>unladen</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coconut</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         high  low  air-speed  velocity\n",
       "unladen     1    0          1         1\n",
       "coconut     0    1          1         1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform them into a format scikit-learn can use\n",
    "\n",
    "columns = ['high','low','air-speed','velocity']\n",
    "indices = ['unladen', 'coconut']\n",
    "dtm = [[1,0,1,1],[0,1,1,1]]\n",
    "dtm_df = pandas.DataFrame(dtm, columns = columns, index = indices)\n",
    "\n",
    "dtm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Naive Bayes classifier\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(dtm,indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['unladen'], dtype='<U7')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a prediction!\n",
    "\n",
    "unknown_swallow = \"high velocity\"\n",
    "unknown_features = [1,0,0,1]\n",
    "\n",
    "nb.predict([unknown_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-Process\n",
    "\n",
    "In their paper, Long and So describe their pre-processing as consisting of three major steps: stop word removal, lemmatization of nouns, and feature selection (based on document frequency). In this workshop, we will focus on the first and third steps, since they can be integrated seamlessly with our workflow and Underwood and Sellers use them as well.\n",
    "\n",
    "Lemmatization -- the transformation of words into their dictionary forms; e.g. plural nouns become singular -- is particularly useful to Long and So, since they partly aim to study imagery. That is, they find it congenial to collapse the words <i>mountains</i> and <i>mountain</i> into the same token, since they express a similar image. For an introduction to Lemmatization (and a related technique, Stemming), see NLTK: http://www.nltk.org/book/ch03.html#sec-normalizing-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Corpus\n",
    "\n",
    "Note that due to issues of copyright, volumes' word order has not been retained, although their total word counts have been. Fortunately, our methods do not require word-order information.\n",
    "\n",
    "Underwood and Sellers's literary corpus has been divided into three folders: \"reviewed\", \"random\", \"canonic\". (The last of these are canonic poets but who did not have the opportunity to be reviewed, such as Emily Dickinson.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob #allows us to access our file structure (danger! it has access to your computer!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign file paths to each set of poems\n",
    "\n",
    "review_path = 'poems/reviewed/*.txt'\n",
    "random_path = 'poems/random/*.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lists of text files in each directory\n",
    "\n",
    "review_files = glob.glob(review_path)\n",
    "random_files = glob.glob(random_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['poems/reviewed/689 Hood, Thomas, The plea of the midsummer fairies 1827.txt',\n",
       " \"poems/reviewed/524 Mackay, Charles, A man's heart 1860.txt\",\n",
       " 'poems/reviewed/383 Colman, James F. The knightly heart, and other poems 1873.txt',\n",
       " 'poems/reviewed/580 Browning, Elizabeth Barrett, Poems 1853.txt',\n",
       " 'poems/reviewed/431 Myers, F. W. H. Poems 1870.txt',\n",
       " 'poems/reviewed/229 Dorr, Julia C. R. Poems 1892.txt',\n",
       " 'poems/reviewed/544 Tennyson, Alfred Tennyson, Idyls of the King 1859.txt',\n",
       " 'poems/reviewed/675 Davidson, Lucretia Maria, Amir Khan, and other poems 1829.txt',\n",
       " \"poems/reviewed/84 Sweeny, Mildred M' Neal. Men of no land, and other poems 1912.txt\",\n",
       " 'poems/reviewed/666 Lytton, Edward Bulwer Lytton, The Siamese twins 1831.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect\n",
    "\n",
    "review_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read-in texts as strings from each location using list comprehension\n",
    "\n",
    "\n",
    "\n",
    "review_texts = [open(file_name, encoding='utf-8').read() for file_name in review_files]\n",
    "random_texts = [open(file_name, encoding='utf-8').read() for file_name in random_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the the the the the the the the the the the the the the the the the the the the the the the the the '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect\n",
    "\n",
    "review_texts[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa - WTH is that? It's the text, but sorted. Most classification tasks, and much of machine learning as it's used in text analysis, is done on what is called a \"bag of words.\" It's just words and their counts, not words in context. So having it sorted this way is no issue, as we won't look at context.\n",
    "\n",
    "We'll look at words in context more next week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Reviewed Texts:\n",
      "357\n",
      "Number of Random Texts:\n",
      "352\n",
      "Number of TotalTexts:\n",
      "709\n"
     ]
    }
   ],
   "source": [
    "# Collect all texts in single list\n",
    "\n",
    "all_texts = review_texts + random_texts\n",
    "\n",
    "#let's make sure we understand what we're doing\n",
    "print(\"Number of Reviewed Texts:\")\n",
    "print(len(review_texts))\n",
    "\n",
    "print(\"Number of Random Texts:\")\n",
    "print(len(random_texts))\n",
    "\n",
    "print(\"Number of TotalTexts:\")\n",
    "print(len(all_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['poems/reviewed/689 Hood, Thomas, The plea of the midsummer fairies 1827.txt', \"poems/reviewed/524 Mackay, Charles, A man's heart 1860.txt\"]\n",
      "709\n"
     ]
    }
   ],
   "source": [
    "# Get all file names together\n",
    "\n",
    "all_file_names = review_files + random_files\n",
    "\n",
    "print(all_file_names[:2])\n",
    "print(len(all_file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reviewed', 'reviewed', 'reviewed', 'reviewed', 'reviewed']\n",
      "709\n"
     ]
    }
   ],
   "source": [
    "# Keep track of classes with labels\n",
    "\n",
    "all_labels = ['reviewed'] * len(review_texts) + ['random'] * len(random_texts) \n",
    "\n",
    "print(all_labels[:5])\n",
    "print(len(all_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We now have three lists of the same length:\n",
    "\n",
    "* `all_file_names` = a list of the file names\n",
    "* `all_texts` = a list where the contents of the file as a string associated with each filename is an element\n",
    "* `all_labels` = a list where each element is a label associated with the file\n",
    "\n",
    "The indices should match: all_file_names[0] should be the filename associated with the text all_texts[0] and the label all_labels[0], and so on.\n",
    "\n",
    "Take a second to really understand this data structure - it's important!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An aside: Stop Words\n",
    "\n",
    "<i>Stop words</i>, sometimes refered to as <i>function words</i>, include articles, prepositions, pronouns, and conjunctions among others. Although their frequencies encode information about textual features like authorship, they do not convey semantic meanings and are often removed before analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default scikit-learn uses this list of English stop words\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'a',\n",
       "           'about',\n",
       "           'above',\n",
       "           'across',\n",
       "           'after',\n",
       "           'afterwards',\n",
       "           'again',\n",
       "           'against',\n",
       "           'all',\n",
       "           'almost',\n",
       "           'alone',\n",
       "           'along',\n",
       "           'already',\n",
       "           'also',\n",
       "           'although',\n",
       "           'always',\n",
       "           'am',\n",
       "           'among',\n",
       "           'amongst',\n",
       "           'amoungst',\n",
       "           'amount',\n",
       "           'an',\n",
       "           'and',\n",
       "           'another',\n",
       "           'any',\n",
       "           'anyhow',\n",
       "           'anyone',\n",
       "           'anything',\n",
       "           'anyway',\n",
       "           'anywhere',\n",
       "           'are',\n",
       "           'around',\n",
       "           'as',\n",
       "           'at',\n",
       "           'back',\n",
       "           'be',\n",
       "           'became',\n",
       "           'because',\n",
       "           'become',\n",
       "           'becomes',\n",
       "           'becoming',\n",
       "           'been',\n",
       "           'before',\n",
       "           'beforehand',\n",
       "           'behind',\n",
       "           'being',\n",
       "           'below',\n",
       "           'beside',\n",
       "           'besides',\n",
       "           'between',\n",
       "           'beyond',\n",
       "           'bill',\n",
       "           'both',\n",
       "           'bottom',\n",
       "           'but',\n",
       "           'by',\n",
       "           'call',\n",
       "           'can',\n",
       "           'cannot',\n",
       "           'cant',\n",
       "           'co',\n",
       "           'con',\n",
       "           'could',\n",
       "           'couldnt',\n",
       "           'cry',\n",
       "           'de',\n",
       "           'describe',\n",
       "           'detail',\n",
       "           'do',\n",
       "           'done',\n",
       "           'down',\n",
       "           'due',\n",
       "           'during',\n",
       "           'each',\n",
       "           'eg',\n",
       "           'eight',\n",
       "           'either',\n",
       "           'eleven',\n",
       "           'else',\n",
       "           'elsewhere',\n",
       "           'empty',\n",
       "           'enough',\n",
       "           'etc',\n",
       "           'even',\n",
       "           'ever',\n",
       "           'every',\n",
       "           'everyone',\n",
       "           'everything',\n",
       "           'everywhere',\n",
       "           'except',\n",
       "           'few',\n",
       "           'fifteen',\n",
       "           'fifty',\n",
       "           'fill',\n",
       "           'find',\n",
       "           'fire',\n",
       "           'first',\n",
       "           'five',\n",
       "           'for',\n",
       "           'former',\n",
       "           'formerly',\n",
       "           'forty',\n",
       "           'found',\n",
       "           'four',\n",
       "           'from',\n",
       "           'front',\n",
       "           'full',\n",
       "           'further',\n",
       "           'get',\n",
       "           'give',\n",
       "           'go',\n",
       "           'had',\n",
       "           'has',\n",
       "           'hasnt',\n",
       "           'have',\n",
       "           'he',\n",
       "           'hence',\n",
       "           'her',\n",
       "           'here',\n",
       "           'hereafter',\n",
       "           'hereby',\n",
       "           'herein',\n",
       "           'hereupon',\n",
       "           'hers',\n",
       "           'herself',\n",
       "           'him',\n",
       "           'himself',\n",
       "           'his',\n",
       "           'how',\n",
       "           'however',\n",
       "           'hundred',\n",
       "           'i',\n",
       "           'ie',\n",
       "           'if',\n",
       "           'in',\n",
       "           'inc',\n",
       "           'indeed',\n",
       "           'interest',\n",
       "           'into',\n",
       "           'is',\n",
       "           'it',\n",
       "           'its',\n",
       "           'itself',\n",
       "           'keep',\n",
       "           'last',\n",
       "           'latter',\n",
       "           'latterly',\n",
       "           'least',\n",
       "           'less',\n",
       "           'ltd',\n",
       "           'made',\n",
       "           'many',\n",
       "           'may',\n",
       "           'me',\n",
       "           'meanwhile',\n",
       "           'might',\n",
       "           'mill',\n",
       "           'mine',\n",
       "           'more',\n",
       "           'moreover',\n",
       "           'most',\n",
       "           'mostly',\n",
       "           'move',\n",
       "           'much',\n",
       "           'must',\n",
       "           'my',\n",
       "           'myself',\n",
       "           'name',\n",
       "           'namely',\n",
       "           'neither',\n",
       "           'never',\n",
       "           'nevertheless',\n",
       "           'next',\n",
       "           'nine',\n",
       "           'no',\n",
       "           'nobody',\n",
       "           'none',\n",
       "           'noone',\n",
       "           'nor',\n",
       "           'not',\n",
       "           'nothing',\n",
       "           'now',\n",
       "           'nowhere',\n",
       "           'of',\n",
       "           'off',\n",
       "           'often',\n",
       "           'on',\n",
       "           'once',\n",
       "           'one',\n",
       "           'only',\n",
       "           'onto',\n",
       "           'or',\n",
       "           'other',\n",
       "           'others',\n",
       "           'otherwise',\n",
       "           'our',\n",
       "           'ours',\n",
       "           'ourselves',\n",
       "           'out',\n",
       "           'over',\n",
       "           'own',\n",
       "           'part',\n",
       "           'per',\n",
       "           'perhaps',\n",
       "           'please',\n",
       "           'put',\n",
       "           'rather',\n",
       "           're',\n",
       "           'same',\n",
       "           'see',\n",
       "           'seem',\n",
       "           'seemed',\n",
       "           'seeming',\n",
       "           'seems',\n",
       "           'serious',\n",
       "           'several',\n",
       "           'she',\n",
       "           'should',\n",
       "           'show',\n",
       "           'side',\n",
       "           'since',\n",
       "           'sincere',\n",
       "           'six',\n",
       "           'sixty',\n",
       "           'so',\n",
       "           'some',\n",
       "           'somehow',\n",
       "           'someone',\n",
       "           'something',\n",
       "           'sometime',\n",
       "           'sometimes',\n",
       "           'somewhere',\n",
       "           'still',\n",
       "           'such',\n",
       "           'system',\n",
       "           'take',\n",
       "           'ten',\n",
       "           'than',\n",
       "           'that',\n",
       "           'the',\n",
       "           'their',\n",
       "           'them',\n",
       "           'themselves',\n",
       "           'then',\n",
       "           'thence',\n",
       "           'there',\n",
       "           'thereafter',\n",
       "           'thereby',\n",
       "           'therefore',\n",
       "           'therein',\n",
       "           'thereupon',\n",
       "           'these',\n",
       "           'they',\n",
       "           'thick',\n",
       "           'thin',\n",
       "           'third',\n",
       "           'this',\n",
       "           'those',\n",
       "           'though',\n",
       "           'three',\n",
       "           'through',\n",
       "           'throughout',\n",
       "           'thru',\n",
       "           'thus',\n",
       "           'to',\n",
       "           'together',\n",
       "           'too',\n",
       "           'top',\n",
       "           'toward',\n",
       "           'towards',\n",
       "           'twelve',\n",
       "           'twenty',\n",
       "           'two',\n",
       "           'un',\n",
       "           'under',\n",
       "           'until',\n",
       "           'up',\n",
       "           'upon',\n",
       "           'us',\n",
       "           'very',\n",
       "           'via',\n",
       "           'was',\n",
       "           'we',\n",
       "           'well',\n",
       "           'were',\n",
       "           'what',\n",
       "           'whatever',\n",
       "           'when',\n",
       "           'whence',\n",
       "           'whenever',\n",
       "           'where',\n",
       "           'whereafter',\n",
       "           'whereas',\n",
       "           'whereby',\n",
       "           'wherein',\n",
       "           'whereupon',\n",
       "           'wherever',\n",
       "           'whether',\n",
       "           'which',\n",
       "           'while',\n",
       "           'whither',\n",
       "           'who',\n",
       "           'whoever',\n",
       "           'whole',\n",
       "           'whom',\n",
       "           'whose',\n",
       "           'why',\n",
       "           'will',\n",
       "           'with',\n",
       "           'within',\n",
       "           'without',\n",
       "           'would',\n",
       "           'yet',\n",
       "           'you',\n",
       "           'your',\n",
       "           'yours',\n",
       "           'yourself',\n",
       "           'yourselves'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect\n",
    "\n",
    "ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many are here?\n",
    "\n",
    "len(ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder: NLTK has its own collection of stop words\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pull up NLTK's list of English-language stop words\n",
    "\n",
    "stopwords.words('english')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many stop words are in the list?\n",
    "# Big difference!\n",
    "\n",
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Document-Term Matrix (DTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DTM is a different way of representing text, called *vector representation.* The goal is to turn the text into a matrix (or an array) of numbers. Once we have the texts in an array, or vector, format, we can do matrix manipulation and linear algebra on it (similar to what we did with relational data). To create the DTM we transform each document into a vector, where each number in the vector represents the count of a particular word. With a DTM, each row is a document, each column is a unique word (each unique word in the entire corpus gets a column), and the cells are the number of times that word appears in the document.\n",
    "\n",
    "To get a feel for this we'll jump right into an example.\n",
    "\n",
    "We've learned how to count each word using Python's NLTK. We could, then, construct a DTM manually. Luckily, however, `scikit-learn` with a built-in function to do this called `CountVectorizer()`.\n",
    "\n",
    "[Let's first look at the documentation for CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "\n",
    "We'll implement this by creating a `CountVectorizer()` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 127733)\t1439\n",
      "  (0, 4452)\t1207\n",
      "  (0, 90849)\t667\n",
      "  (0, 130798)\t559\n",
      "  (0, 64364)\t507\n",
      "  (0, 127641)\t433\n",
      "  (0, 146192)\t391\n",
      "  (0, 58162)\t348\n",
      "  (0, 59129)\t277\n",
      "  (0, 48348)\t244\n",
      "  (0, 17088)\t241\n",
      "  (0, 86910)\t217\n",
      "  (0, 91666)\t191\n",
      "  (0, 76426)\t172\n",
      "  (0, 3286)\t170\n",
      "  (0, 127841)\t164\n",
      "  (0, 67324)\t157\n",
      "  (0, 118589)\t149\n",
      "  (0, 49855)\t146\n",
      "  (0, 114724)\t139\n",
      "  (0, 6737)\t138\n",
      "  (0, 57296)\t128\n",
      "  (0, 17230)\t127\n",
      "  (0, 78471)\t121\n",
      "  (0, 67659)\t116\n",
      "  :\t:\n",
      "  (708, 112990)\t1\n",
      "  (708, 106449)\t1\n",
      "  (708, 105806)\t1\n",
      "  (708, 101706)\t1\n",
      "  (708, 100321)\t1\n",
      "  (708, 99364)\t1\n",
      "  (708, 98468)\t1\n",
      "  (708, 85654)\t1\n",
      "  (708, 80096)\t1\n",
      "  (708, 30364)\t1\n",
      "  (708, 70574)\t1\n",
      "  (708, 58852)\t1\n",
      "  (708, 58851)\t1\n",
      "  (708, 25937)\t1\n",
      "  (708, 24155)\t1\n",
      "  (708, 20554)\t1\n",
      "  (708, 19786)\t1\n",
      "  (708, 18197)\t1\n",
      "  (708, 13020)\t1\n",
      "  (708, 10633)\t1\n",
      "  (708, 5722)\t1\n",
      "  (708, 2681)\t1\n",
      "  (708, 2381)\t1\n",
      "  (708, 1247)\t1\n",
      "  (708, 1181)\t1\n"
     ]
    }
   ],
   "source": [
    "#import the function CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countvec = CountVectorizer()\n",
    "\n",
    "#use the fit_transform function to transform our text list (all_texts) into a DTM\n",
    "sklearn_dtm = countvec.fit_transform(all_texts)\n",
    "print(sklearn_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This format is called Compressed Sparse Format. How do we know what each number indicates? We can access the words themselves through the CountVectorizer function `get_feature_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '01' '02' '05' '0l' '0li' '0ltr' '0m' '0u' '0ug']\n"
     ]
    }
   ],
   "source": [
    "print(countvec.get_feature_names_out()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hybrid\n"
     ]
    }
   ],
   "source": [
    "print(countvec.get_feature_names_out()[61364])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "In the above DTM I included all words.\n",
    "\n",
    "Long and So did not use all of the words that appear in their respective corpora when constructing their matrices. The process of choosing which words to comprise the columns is referred to as <i>feature selection</i>.\n",
    "\n",
    "While there are several approaches one may take when selecting features, both of the literary studies under consideration use <i>document frequency</i> as the deciding criterion. The intuition is that a word that appears in a single text out of hundreds will not carry much weight when trying to determine the text's class membership.\n",
    "\n",
    "In order to be selected as a feature, Long and So require that words appear in at least 2 texts, whereas Underwood and Sellers require that a word appear in about a quarter of all texts. Although this is quite a large difference (a minimum of 2 texts vs. ~180 texts), it perhaps makes sense since the texts are of very different lengths: individual haiku vs entire volumes of poetry. The latter will have much greater overlap in its vocabulary.\n",
    "\n",
    "The process of feature selection is intimately tied to the object under study and the statistical model chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intitialize the function that will transform our list of texts to a DTM\n",
    "# 'min_df' and 'max_features' are arguments that enable flexible feature selection\n",
    "# 'binary' tells CountVectorizer only to record whether a word appeared in a text or not\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english', min_df=180, binary = True, max_features = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 652)\t1\n",
      "  (0, 692)\t1\n",
      "  (0, 1189)\t1\n",
      "  (0, 326)\t1\n",
      "  (0, 1179)\t1\n",
      "  (0, 1146)\t1\n",
      "  (0, 648)\t1\n",
      "  (0, 243)\t1\n",
      "  (0, 1173)\t1\n",
      "  (0, 1135)\t1\n",
      "  (0, 795)\t1\n",
      "  (0, 1192)\t1\n",
      "  (0, 1166)\t1\n",
      "  (0, 335)\t1\n",
      "  (0, 529)\t1\n",
      "  (0, 327)\t1\n",
      "  (0, 1191)\t1\n",
      "  (0, 981)\t1\n",
      "  (0, 644)\t1\n",
      "  (0, 128)\t1\n",
      "  (0, 677)\t1\n",
      "  (0, 537)\t1\n",
      "  (0, 782)\t1\n",
      "  (0, 497)\t1\n",
      "  (0, 640)\t1\n",
      "  :\t:\n",
      "  (708, 997)\t1\n",
      "  (708, 755)\t1\n",
      "  (708, 1245)\t1\n",
      "  (708, 1116)\t1\n",
      "  (708, 770)\t1\n",
      "  (708, 720)\t1\n",
      "  (708, 372)\t1\n",
      "  (708, 1255)\t1\n",
      "  (708, 887)\t1\n",
      "  (708, 413)\t1\n",
      "  (708, 168)\t1\n",
      "  (708, 117)\t1\n",
      "  (708, 55)\t1\n",
      "  (708, 1358)\t1\n",
      "  (708, 1345)\t1\n",
      "  (708, 1207)\t1\n",
      "  (708, 829)\t1\n",
      "  (708, 717)\t1\n",
      "  (708, 1127)\t1\n",
      "  (708, 220)\t1\n",
      "  (708, 1251)\t1\n",
      "  (708, 27)\t1\n",
      "  (708, 896)\t1\n",
      "  (708, 587)\t1\n",
      "  (708, 174)\t1\n"
     ]
    }
   ],
   "source": [
    "# Transform our texts to DTM\n",
    "\n",
    "a=cv.fit_transform(all_texts)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 0, 1, 0],\n",
       "       [0, 1, 1, ..., 1, 0, 1],\n",
       "       [1, 0, 1, ..., 1, 1, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 1, 0, 0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform our texts to a dense DTM\n",
    "\n",
    "#This format should make more sense - think rows and columns\n",
    "# Think back to social network analysis - this is the same data format we used to analyze social networks\n",
    "# Data science is great because it all uses the same data formats!\n",
    "\n",
    "cv.fit_transform(all_texts).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign this to a variable\n",
    "\n",
    "dtm = cv.fit_transform(all_texts).toarray()\n",
    "type(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['abide', 'abode', 'abroad', 'absence', 'absent', 'abyss',\n",
       "       'accents', 'accept', 'accursed', 'ache', 'aching', 'act', 'action',\n",
       "       'acts', 'adam', 'add', 'added', 'adieu', 'adore', 'adorn'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the column headings\n",
    "\n",
    "cv.get_feature_names_out()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign to a variable\n",
    "\n",
    "feature_list = cv.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place this in a dataframe for readability\n",
    "\n",
    "dtm_df = pandas.DataFrame(dtm, columns = feature_list, index = all_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abide</th>\n",
       "      <th>abode</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absence</th>\n",
       "      <th>absent</th>\n",
       "      <th>abyss</th>\n",
       "      <th>accents</th>\n",
       "      <th>accept</th>\n",
       "      <th>accursed</th>\n",
       "      <th>ache</th>\n",
       "      <th>...</th>\n",
       "      <th>yon</th>\n",
       "      <th>yonder</th>\n",
       "      <th>yore</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youth</th>\n",
       "      <th>youthful</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zephyr</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>poems/reviewed/689 Hood, Thomas, The plea of the midsummer fairies 1827.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poems/reviewed/524 Mackay, Charles, A man's heart 1860.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poems/reviewed/383 Colman, James F. The knightly heart, and other poems 1873.txt</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poems/reviewed/580 Browning, Elizabeth Barrett, Poems 1853.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poems/reviewed/431 Myers, F. W. H. Poems 1870.txt</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3606 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    abide  abode  abroad  \\\n",
       "poems/reviewed/689 Hood, Thomas, The plea of th...      0      0       1   \n",
       "poems/reviewed/524 Mackay, Charles, A man's hea...      0      1       1   \n",
       "poems/reviewed/383 Colman, James F. The knightl...      1      0       1   \n",
       "poems/reviewed/580 Browning, Elizabeth Barrett,...      0      1       1   \n",
       "poems/reviewed/431 Myers, F. W. H. Poems 1870.txt       1      0       0   \n",
       "\n",
       "                                                    absence  absent  abyss  \\\n",
       "poems/reviewed/689 Hood, Thomas, The plea of th...        1       1      1   \n",
       "poems/reviewed/524 Mackay, Charles, A man's hea...        1       0      1   \n",
       "poems/reviewed/383 Colman, James F. The knightl...        1       1      1   \n",
       "poems/reviewed/580 Browning, Elizabeth Barrett,...        1       1      0   \n",
       "poems/reviewed/431 Myers, F. W. H. Poems 1870.txt         0       0      0   \n",
       "\n",
       "                                                    accents  accept  accursed  \\\n",
       "poems/reviewed/689 Hood, Thomas, The plea of th...        1       0         0   \n",
       "poems/reviewed/524 Mackay, Charles, A man's hea...        0       1         0   \n",
       "poems/reviewed/383 Colman, James F. The knightl...        1       1         1   \n",
       "poems/reviewed/580 Browning, Elizabeth Barrett,...        1       1         1   \n",
       "poems/reviewed/431 Myers, F. W. H. Poems 1870.txt         0       0         1   \n",
       "\n",
       "                                                    ache  ...  yon  yonder  \\\n",
       "poems/reviewed/689 Hood, Thomas, The plea of th...     1  ...    0       0   \n",
       "poems/reviewed/524 Mackay, Charles, A man's hea...     0  ...    0       0   \n",
       "poems/reviewed/383 Colman, James F. The knightl...     0  ...    1       1   \n",
       "poems/reviewed/580 Browning, Elizabeth Barrett,...     1  ...    0       1   \n",
       "poems/reviewed/431 Myers, F. W. H. Poems 1870.txt      1  ...    0       0   \n",
       "\n",
       "                                                    yore  young  younger  \\\n",
       "poems/reviewed/689 Hood, Thomas, The plea of th...     0      1        0   \n",
       "poems/reviewed/524 Mackay, Charles, A man's hea...     0      1        1   \n",
       "poems/reviewed/383 Colman, James F. The knightl...     1      1        0   \n",
       "poems/reviewed/580 Browning, Elizabeth Barrett,...     1      1        1   \n",
       "poems/reviewed/431 Myers, F. W. H. Poems 1870.txt      0      1        0   \n",
       "\n",
       "                                                    youth  youthful  zeal  \\\n",
       "poems/reviewed/689 Hood, Thomas, The plea of th...      1         0     0   \n",
       "poems/reviewed/524 Mackay, Charles, A man's hea...      1         1     1   \n",
       "poems/reviewed/383 Colman, James F. The knightl...      1         1     1   \n",
       "poems/reviewed/580 Browning, Elizabeth Barrett,...      1         1     0   \n",
       "poems/reviewed/431 Myers, F. W. H. Poems 1870.txt       1         0     0   \n",
       "\n",
       "                                                    zephyr  zone  \n",
       "poems/reviewed/689 Hood, Thomas, The plea of th...       1     0  \n",
       "poems/reviewed/524 Mackay, Charles, A man's hea...       0     1  \n",
       "poems/reviewed/383 Colman, James F. The knightl...       1     1  \n",
       "poems/reviewed/580 Browning, Elizabeth Barrett,...       0     1  \n",
       "poems/reviewed/431 Myers, F. W. H. Poems 1870.txt        0     0  \n",
       "\n",
       "[5 rows x 3606 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the dataframe\n",
    "# The DTM, in all it's wonderful, full (memory-heavy) glory (don't use this format with large data)\n",
    "dtm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(709, 3606)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the dataframe's dimensions (# texts, # features)\n",
    "\n",
    "dtm_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Feature Importance, and Prediction\n",
    "\n",
    "Long and So selected a classification algorithm that specifically relies on <a href=\"https://en.wikipedia.org/wiki/Bayes%27_theorem\">Bayes' Theorem</a> to model relationships between textual features and categories in our corpus of poetry volumes. (See link for more information about the method and its assumptions.)\n",
    "\n",
    "Two ways that we learn about the model are its feature weights and predictions on new texts. The algorithm can explicity report to us which direction each word leans category-wise and how strongly. Based on those weights, it makes further predictions about the valences of previously unseen poetry volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the classifier and assign it to a variable\n",
    "\n",
    "#We use the fit function to fit the model to our data\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(dtm, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand-waving the underlying statistics here...\n",
    "\n",
    "def most_informative_features(text_class, vectorizer = cv, classifier = nb, top_n = 50):\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    class_index = np.where(classifier.classes_==(text_class))[0][0]\n",
    "    \n",
    "    class_prob_distro = np.exp(classifier.feature_log_prob_[class_index])\n",
    "    alt_class_prob_distro = np.exp(classifier.feature_log_prob_[1 - class_index])\n",
    "    \n",
    "    odds_ratios = class_prob_distro / alt_class_prob_distro\n",
    "    odds_with_fns = sorted(zip(odds_ratios, feature_names), reverse = True)\n",
    "    \n",
    "    return odds_with_fns[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2.2574987525392105, 'dusk'),\n",
       " (2.2162169507398977, 'windy'),\n",
       " (2.170421139765935, 'utterly'),\n",
       " (2.164877923481417, 'vague'),\n",
       " (2.1293064820834307, 'roofs'),\n",
       " (2.0836042941752964, 'perilous'),\n",
       " (2.077372177642372, 'visible'),\n",
       " (2.0427493080149963, 'yearned'),\n",
       " (2.0269008113545772, 'stair'),\n",
       " (2.012973640135456, 'shrank'),\n",
       " (1.9989807747124708, 'remembering'),\n",
       " (1.9695086222647848, 'sleepy'),\n",
       " (1.9656644284672604, 'moods'),\n",
       " (1.9154075671821176, 'whirled'),\n",
       " (1.900651280585991, 'alien'),\n",
       " (1.8638644816069039, 'stark'),\n",
       " (1.843996505786981, 'curled'),\n",
       " (1.8299760364867796, 'muttered'),\n",
       " (1.8252865650745336, 'ghosts'),\n",
       " (1.8148154163014614, 'wherewith'),\n",
       " (1.8108114109704032, 'passionate'),\n",
       " (1.810469296590852, 'folk'),\n",
       " (1.8065148052497686, 'miracle'),\n",
       " (1.7812555295135246, 'haunting'),\n",
       " (1.751051631404383, 'eclipse'),\n",
       " (1.749713929623325, 'topmost'),\n",
       " (1.746779616039069, 'smote'),\n",
       " (1.739799198775482, 'porch'),\n",
       " (1.723217764225026, 'shuddering'),\n",
       " (1.723046197342888, 'hedge'),\n",
       " (1.7145847176338114, 'younger'),\n",
       " (1.7088061461251791, 'vex'),\n",
       " (1.7078723722748321, 'touches'),\n",
       " (1.7078723722748321, 'heavily'),\n",
       " (1.702703267031872, 'loth'),\n",
       " (1.6970976595683918, 'salt'),\n",
       " (1.6967911029102367, 'whiteness'),\n",
       " (1.6913063241467854, 'hurrying'),\n",
       " (1.6676682203851265, 'hurt'),\n",
       " (1.6644518882339494, 'betwixt'),\n",
       " (1.6571038063193357, 'dragged'),\n",
       " (1.654008987008927, 'london'),\n",
       " (1.639640183067731, 'fluttered'),\n",
       " (1.636276818589641, 'shadowed'),\n",
       " (1.6349688435148297, 'threshold'),\n",
       " (1.6307371594492608, 'shaken'),\n",
       " (1.6289816336653362, 'deeps'),\n",
       " (1.6250168592499572, 'reeds'),\n",
       " (1.6248807378846293, 'leaned'),\n",
       " (1.6240690512247329, 'nightingale')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns feature name and odds ratio for a given class\n",
    "\n",
    "most_informative_features('reviewed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2.323890855405321, 'emblem'),\n",
       " (2.135859350418144, 'cheering'),\n",
       " (2.10956562735429, 'mission'),\n",
       " (2.109077414989705, 'caused'),\n",
       " (1.9534155016450552, 'united'),\n",
       " (1.9534155016450552, 'inspire'),\n",
       " (1.950679625592335, 'zephyr'),\n",
       " (1.9242916597934763, 'impart'),\n",
       " (1.908442196668769, 'display'),\n",
       " (1.8769673079432159, 'lasting'),\n",
       " (1.838894948755733, 'raging'),\n",
       " (1.834578293946445, 'unite'),\n",
       " (1.834578293946445, 'choicest'),\n",
       " (1.8296697232603005, 'fully'),\n",
       " (1.754989326392534, 'beaming'),\n",
       " (1.7461728906511993, 'varied'),\n",
       " (1.7145659923081469, 'saviour'),\n",
       " (1.7065300466472273, 'matchless'),\n",
       " (1.6989790287417097, 'peerless'),\n",
       " (1.6989790287417066, 'patriot'),\n",
       " (1.6989790287417066, 'dire'),\n",
       " (1.6932585606314674, 'jesus'),\n",
       " (1.6898447328882535, 'blessings'),\n",
       " (1.6757053434164795, 'firmly'),\n",
       " (1.6750497466467533, 'diamonds'),\n",
       " (1.6734688331149532, 'reigns'),\n",
       " (1.6612239392141168, 'climes'),\n",
       " (1.650436770777658, 'pleasing'),\n",
       " (1.6474948157495353, 'fetters'),\n",
       " (1.6423463944503165, 'views'),\n",
       " (1.641525631634502, 'lave'),\n",
       " (1.6393657294876147, 'joyful'),\n",
       " (1.6327850406089144, 'destruction'),\n",
       " (1.6263730873424889, 'lessons'),\n",
       " (1.6263730873424889, 'charming'),\n",
       " (1.6198151313451399, 'spotless'),\n",
       " (1.6180752654682917, 'dangers'),\n",
       " (1.6167703660606536, 'fondly'),\n",
       " (1.6161020029494277, 'object'),\n",
       " (1.6161020029494277, 'adorn'),\n",
       " (1.6114027901467767, 'manly'),\n",
       " (1.5998719187317758, 'twould'),\n",
       " (1.5990390858745465, 'combined'),\n",
       " (1.5945110625731234, 'unfurled'),\n",
       " (1.5941037800539477, 'wield'),\n",
       " (1.5922798626854662, 'alarms'),\n",
       " (1.5922798626854633, 'anxious'),\n",
       " (1.5857137601589286, 'wafted'),\n",
       " (1.5813008276167397, 'scenes'),\n",
       " (1.566137047070547, 'mr')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarly, for words that indicate 'random' class membership\n",
    "\n",
    "most_informative_features('random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load up two poems that aren't in the training set and make predictions\n",
    "# You could do the same thing by reading in files\n",
    "\n",
    "dickinson_canonic = \"\"\"Because I could not stop for Death â€“ \n",
    "He kindly stopped for me â€“  \n",
    "The Carriage held but just Ourselves â€“  \n",
    "And Immortality.\n",
    "\n",
    "We slowly drove â€“ He knew no haste\n",
    "And I had put away\n",
    "My labor and my leisure too,\n",
    "For His Civility â€“ \n",
    "\n",
    "We passed the School, where Children strove\n",
    "At Recess â€“ in the Ring â€“  \n",
    "We passed the Fields of Gazing Grain â€“  \n",
    "We passed the Setting Sun â€“ \n",
    "\n",
    "Or rather â€“ He passed us â€“ \n",
    "The Dews drew quivering and chill â€“ \n",
    "For only Gossamer, my Gown â€“ \n",
    "My Tippet â€“ only Tulle â€“ \n",
    "\n",
    "We paused before a House that seemed\n",
    "A Swelling of the Ground â€“ \n",
    "The Roof was scarcely visible â€“ \n",
    "The Cornice â€“ in the Ground â€“ \n",
    "\n",
    "Since then â€“ â€˜tis Centuries â€“ and yet\n",
    "Feels shorter than the Day\n",
    "I first surmised the Horsesâ€™ Heads \n",
    "Were toward Eternity â€“ \"\"\"\n",
    "\n",
    "\n",
    "anthem_patriotic = \"\"\"O! say can you see, by the dawn's early light,\n",
    "What so proudly we hailed at the twilight's last gleaming,\n",
    "Whose broad stripes and bright stars through the perilous fight,\n",
    "O'er the ramparts we watched, were so gallantly streaming?\n",
    "And the rockets' red glare, the bombs bursting in air,\n",
    "Gave proof through the night that our flag was still there;\n",
    "O! say does that star-spangled banner yet wave\n",
    "O'er the land of the free and the home of the brave?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform these into DTMs with the same feature-columns as previously\n",
    "# Remind yourself what the cv object is (see above, where we created it)\n",
    "\n",
    "unknown_dtm = cv.transform([dickinson_canonic,anthem_patriotic]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['reviewed', 'random'], dtype='<U8')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What does the classifier think?\n",
    "\n",
    "nb.predict(unknown_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26167891, 0.73832109],\n",
       "       [0.79942107, 0.20057893]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Although our classification is binary, Bayes theorem assigns\n",
    "# a probability of membership in either category\n",
    "\n",
    "# Just how confident is our classifier of its predictions?\n",
    "\n",
    "nb.predict_proba(unknown_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-Validation\n",
    "\n",
    "Just how good is our classifier? We can evaluate it by randomly selecting texts from each category and setting them aside before training. We then see how well the classifier predicts their (known) categories.\n",
    "\n",
    "Remember that if the classifier is trying to predict membership for just two categories, we would expect it to be correct about 50% of the time based on random chance. As a rule of thumb, if this kind of classifier has 65% accuracy or better under cross-validation, it has often identified a meaningful pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize the order of our texts\n",
    "import numpy\n",
    "randomized_review = numpy.random.permutation(review_texts)\n",
    "randomized_random = numpy.random.permutation(random_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90% index for random list:\n",
      "316\n",
      "\n",
      "637\n",
      "637\n",
      "72\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "# We'll train our classifier on the first 90% of texts in the randomized list\n",
    "# Then, we'll test it using the last 10%\n",
    "\n",
    "per90_review = int(len(randomized_review)*.90)\n",
    "per90_random = int(len(randomized_random)*.90)\n",
    "print(\"90% index for random list:\")\n",
    "print(per90_random)\n",
    "print()\n",
    "\n",
    "training_set = list(randomized_review[:per90_review]) + list(randomized_random[:per90_random])\n",
    "test_set = list(randomized_review[per90_review:]) + list(randomized_random[per90_random:])\n",
    "\n",
    "training_labels = ['reviewed'] * per90_review + ['random'] * per90_random\n",
    "test_labels = ['reviewed'] * (len(randomized_review) - per90_review) + ['random'] * (len(randomized_random) - per90_random)\n",
    "\n",
    "print(len(training_set))\n",
    "print(len(training_labels))\n",
    "\n",
    "print(len(test_set))\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform training and test texts into DTMs\n",
    "# Note that 'min_df' has been adjusted to one quarter of the size of the training set\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english', min_df = 162, binary=True)\n",
    "training_dtm = cv.fit_transform(training_set)\n",
    "test_dtm = cv.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6944444444444444"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train, Predict, Evaluate\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(training_dtm, training_labels)\n",
    "predictions = nb.predict(test_dtm)\n",
    "accuracy_score(predictions, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises!\n",
    "\n",
    "* Re-initialize the CountVectorizer function above with the the argument min_df = 1. How many unique words are in there in the total vocabulary of the corpus?\n",
    "\n",
    "* Repeat the exercise above with min_df = 360. (That is, words are only included if they appear in at least half of all documents.) What is the size of the vocabulary now? Does the list of these very common words look as you would expect?\n",
    "\n",
    "* What kinds of patterns do you notice among the 'most informative features' from our original model? Try looking at the top fifty most informative words for each category. Does this challenge what you think you know about literary prestige?\n",
    "\n",
    "* CHALLENGE: Another way to do cross-validation is to they do so by setting aside a single author's texts (one or more) from the training set and making a prediction for that author alone. After doing this for all authors, they tally the number of texts that were correctly predicted to calculate their overall accuracy. Implement this.\n",
    "    * Hint: look at the title of each text - they are all in a standarized format. Use your string splicing techniques to create a list, the same length as `all_texts` and `all_labels`, that contains the author name. Go from there to check how accurate the classifier is for each author, and the average accuracy overall all authors.\n",
    "    * Hint 2: You won't be able to actually complete this (it takes a long time!), but just work toward this. Write a few functions. Think through how to remove texts from lists, etc. It will help you get more comfortable with these data structures and these scikit-learn functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149636"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the function CountVectorizer\n",
    "cv_1 = CountVectorizer(stop_words = 'english', min_df=1, binary = True, max_features = None)\n",
    "dtm = cv_1.fit_transform(all_texts).toarray()\n",
    "feature_list = cv_1.get_feature_names_out()\n",
    "dtm_df = pandas.DataFrame(dtm, columns = feature_list, index = all_file_names)\n",
    "dtm_df.shape\n",
    "\n",
    "len(cv_1.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(709, 1362)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_360 = CountVectorizer(stop_words = 'english', min_df=360, binary = True, max_features = None)\n",
    "cv_360.fit_transform(all_texts)\n",
    "dtm = cv_360.fit_transform(all_texts).toarray()\n",
    "feature_list = cv_360.get_feature_names_out()\n",
    "dtm_df = pandas.DataFrame(dtm, columns = feature_list, index = all_file_names)\n",
    "dtm_df.shape\n",
    "#1362 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['abroad', 'afar', 'age', 'ages', 'ago', 'agony', 'ah', 'aid',\n",
       "       'air', 'alas', 'alike', 'altar', 'amid', 'ancient', 'angel',\n",
       "       'angels', 'angry', 'anguish', 'answer', 'answered'], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names_out()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_informative_features(text_class, vectorizer = cv_360, classifier = nb, top_n = 50):\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    class_index = np.where(classifier.classes_==(text_class))[0][0]\n",
    "    \n",
    "    class_prob_distro = np.exp(classifier.feature_log_prob_[class_index])\n",
    "    alt_class_prob_distro = np.exp(classifier.feature_log_prob_[1 - class_index])\n",
    "    \n",
    "    odds_ratios = class_prob_distro / alt_class_prob_distro\n",
    "    odds_with_fns = sorted(zip(odds_ratios, feature_names), reverse = True)\n",
    "    \n",
    "    return odds_with_fns[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2.323890855405321, 'shout'),\n",
       " (2.135859350418144, 'hands'),\n",
       " (2.109077414989705, 'gleams'),\n",
       " (1.908442196668769, 'race'),\n",
       " (1.834578293946445, 'hear'),\n",
       " (1.8296697232603005, 'wondering'),\n",
       " (1.754989326392534, 'chill'),\n",
       " (1.6989790287417066, 'proud'),\n",
       " (1.6898447328882535, 'doom'),\n",
       " (1.6757053434164795, 'thoughts'),\n",
       " (1.6750497466467533, 'pray'),\n",
       " (1.6612239392141168, 'humble'),\n",
       " (1.6474948157495353, 'swept'),\n",
       " (1.6327850406089144, 'pleasant'),\n",
       " (1.6263730873424889, 'growing'),\n",
       " (1.6180752654682917, 'mingled'),\n",
       " (1.6167703660606536, 'veil'),\n",
       " (1.6161020029494277, 'arch'),\n",
       " (1.5990390858745465, 'knows'),\n",
       " (1.5922798626854662, 'band'),\n",
       " (1.5922798626854633, 'better'),\n",
       " (1.5627739589269727, 'holds'),\n",
       " (1.557397443013232, 'best'),\n",
       " (1.548436583157001, 'short'),\n",
       " (1.5459178549811932, 'mourn'),\n",
       " (1.5423355721910534, 'answered'),\n",
       " (1.5392459576634272, 'close'),\n",
       " (1.5353736407888015, 'laughter'),\n",
       " (1.529565165476866, 'calling'),\n",
       " (1.5277640878607595, 'sweetly'),\n",
       " (1.527172160666705, 'stream'),\n",
       " (1.5130211250983385, 'heavens'),\n",
       " (1.5102035811037413, 'want'),\n",
       " (1.5046918892018997, 'drew'),\n",
       " (1.5024984607919845, 'hang'),\n",
       " (1.5003970643433269, 'left'),\n",
       " (1.4886292442308307, 'careless'),\n",
       " (1.483615489887124, 'sweetest'),\n",
       " (1.4818872639580434, 'high'),\n",
       " (1.4811612045440523, 'single'),\n",
       " (1.4811612045440523, 'shut'),\n",
       " (1.478741006497415, 'ones'),\n",
       " (1.469854630463565, 'whispered'),\n",
       " (1.4693872681009343, 'court'),\n",
       " (1.4666400162642095, 'onward'),\n",
       " (1.465785828718335, 'halls'),\n",
       " (1.4637357786082419, 'ah'),\n",
       " (1.4605258317253282, 'beauty'),\n",
       " (1.4502188594243894, 'hangs'),\n",
       " (1.4472784318910858, 'stars')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_informative_features('random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2.2574987525392105, 'sea'),\n",
       " (1.900651280585991, 'bare'),\n",
       " (1.843996505786981, 'march'),\n",
       " (1.810469296590852, 'used'),\n",
       " (1.751051631404383, 'shell'),\n",
       " (1.6644518882339494, 'dancing'),\n",
       " (1.6571038063193357, 'right'),\n",
       " (1.639640183067731, 'tune'),\n",
       " (1.6289816336653362, 'oft'),\n",
       " (1.6233658388189178, 'tears'),\n",
       " (1.616125134809911, 'ring'),\n",
       " (1.6041116850597574, 'felt'),\n",
       " (1.5975981270916337, 'fires'),\n",
       " (1.5918651194585287, 'vision'),\n",
       " (1.5910290558453675, 'know'),\n",
       " (1.5851766105532423, 'shook'),\n",
       " (1.5593780762042724, 'places'),\n",
       " (1.5585590751138287, 'dumb'),\n",
       " (1.5479885010398409, 'dust'),\n",
       " (1.5387392487250964, 'sweeter'),\n",
       " (1.5379255315126605, 'glowing'),\n",
       " (1.53433095547712, 'floating'),\n",
       " (1.5212756254734903, 'join'),\n",
       " (1.5185590618565723, 'looking'),\n",
       " (1.5060948287907157, 'mock'),\n",
       " (1.5022191344294444, 'wine'),\n",
       " (1.5022191344294444, 'foe'),\n",
       " (1.4992355810343545, 'saw'),\n",
       " (1.4954959911496868, 'rude'),\n",
       " (1.4869611376808562, 'sought'),\n",
       " (1.4772995708828047, 'maiden'),\n",
       " (1.4760703090357317, 'roses'),\n",
       " (1.4755033891884506, 'free'),\n",
       " (1.4714719591633458, 'wreath'),\n",
       " (1.468642205395724, 'fell'),\n",
       " (1.466944353135151, 'morn'),\n",
       " (1.4664839186238086, 'page'),\n",
       " (1.4630635479681267, 'sore'),\n",
       " (1.4622752594185755, 'forever'),\n",
       " (1.4555641541994153, 'dove'),\n",
       " (1.4484802098014218, 'street'),\n",
       " (1.4436333004764708, 'shalt'),\n",
       " (1.443174421487129, 'midnight'),\n",
       " (1.4423339005660516, 'kind'),\n",
       " (1.4375149139518832, 'chain'),\n",
       " (1.433322686148001, 'scene'),\n",
       " (1.430879629255392, 'plains'),\n",
       " (1.430022044820717, 'bread'),\n",
       " (1.4287880397372357, 'eye'),\n",
       " (1.42438485647012, 'hue')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_informative_features('reviewed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
