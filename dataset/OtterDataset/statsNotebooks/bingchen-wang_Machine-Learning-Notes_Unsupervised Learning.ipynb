{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83c8e7bc-a46d-47ba-917f-e76ce2239355",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "Author: Bingchen Wang\n",
    "\n",
    "Last Updated: 24 Sep, 2022\n",
    "\n",
    "---\n",
    "<nav>\n",
    "    <a href=\"../Machine%20Learning.ipynb\">Machine Learning</a>\n",
    "</nav>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e6f1e0-ec43-4fd8-a89d-79521ec8d067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel='stylesheet' type='text/css' media='screen' href='../styles/custom.css'>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<link rel='stylesheet' type='text/css' media='screen' href='../styles/custom.css'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7384be3b-c0fd-4bcc-8a58-32f768d6ed33",
   "metadata": {},
   "source": [
    "<section class = \"section--outline\">\n",
    "    <div class = \"outline--header\">Outline </div>\n",
    "    <div class = \"outline--content\">\n",
    "        <b>Concepts:</b>\n",
    "        <ul>\n",
    "            <li> <a href = \"#KMC\">K-means Clustering</a>\n",
    "            <li> <a href = \"#MoG\">Mixture of Gaussians</a>\n",
    "                <ul>\n",
    "                    <li> <a href = \"#AD\">Anomaly Detection</a>\n",
    "                    <li> <a href = \"#MoGM\">Mixture of Gaussians Model</a>\n",
    "                    <li> <a href = \"#EMA\">Expectation Maximization Algorithm</ul>\n",
    "            <li> <a href = \"#FA\">Factor Analysis</a>\n",
    "            <li> <a href = \"#PCA\">Principal Component Analysis</a>\n",
    "            <li> <a href = \"#ICA\">Independent Component Analysis</a>\n",
    "        </ul>\n",
    "        <b>Implementation:</b>\n",
    "        <ul>\n",
    "            <li> K-means Clustering\n",
    "                <ul>\n",
    "                    <li> <a href = \"./K-means Clustering/Numpy Implementation.ipynb\">Numpy Implementation</a>\n",
    "                    <li> <a href = \"./K-means Clustering/Sklearn Implementation.ipynb\">Sklearn Implementation</a>\n",
    "                </ul>\n",
    "            <li> Mixture of Gaussians\n",
    "                <ul>\n",
    "                    <li> <a href = \"./Mixture of Gaussians/Numpy Implementation.ipynb\">Numpy Implementation</a>\n",
    "                    <li> <a href = \"./Mixture of Gaussians/Sklearn Implementation.ipynb\">Sklearn Implementation</a>\n",
    "                </ul>\n",
    "            <li> Principal Component Analysis\n",
    "                <ul>\n",
    "                    <li> <a href = \"./Principal Component Analysis/Sklearn Implementation.ipynb\">Sklearn Implementation</a>\n",
    "                </ul>\n",
    "            <li> Independent Component Analysis\n",
    "                <ul>\n",
    "                    <li> <a href = \"./Independent Component Analysis/Sklearn Implementation.ipynb\">Sklearn Implementation</a>\n",
    "                </ul>\n",
    "        </ul>    \n",
    "    </div>\n",
    "</section>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfd5463-ca85-4044-9fb3-abdfdc233824",
   "metadata": {},
   "source": [
    "<a name = \"KMC\"></a>\n",
    "## K-means Clustering\n",
    "### Cost function\n",
    "$$\n",
    "J(\\mathbf{c},\\mathbf{\\mu}) = \\sum^m_{i=1}\\left\\Vert x^{(i)} - \\mu_{c^{(i)}}\\right\\Vert^2\n",
    "$$\n",
    "\n",
    "### Algorithm\n",
    "<section class = \"section--algorithm\">\n",
    "    <div class = \"algorithm--header\"> K-means Clustering Algorithm</div>\n",
    "    <div class = \"algorithm--content\">\n",
    "        Data $ \\{x^{(1)}, x^{(2)}, \\dots, x^{(m)}\\}$.\n",
    "        <blockquote>\n",
    "            Initialize cluster centroids $\\mu_1, \\mu_2, \\dots, \\mu_k \\in \\mathbb{R}^n$ <br>\n",
    "            <div class = \"alert alert-block alert-success\"><b>Note:</b> Usually randomly pick $k$ examples from the dataset to be the initial cluster centroids. </div>\n",
    "            Repeat until convergence:\n",
    "            <blockquote>\n",
    "                (a) <b>(colour the points)</b> Set $c^{(i)} := \\arg\\min_j \\left\\Vert x^{(i)} - \\mu_j \\right\\Vert_2$ <br>\n",
    "                (b) <b>(move the cluster centroids)</b> For $j = 1, 2, \\dots, k$, \n",
    "                $$\n",
    "                \\mu_j := \\frac{\\sum^m_{i=1} \\mathbb{1}\\{c^{(i)} = j \\} x^{(i)}}{\\sum^m_{i=1} \\mathbb{1}\\{c^{(i)} = j \\}}\n",
    "                $$\n",
    "            </blockquote>\n",
    "        </blockquote>\n",
    "    </div>\n",
    "</section>\n",
    "\n",
    "### Local minima\n",
    "**Q: Worry about local minima?** <br>\n",
    "A: Run the algorithm several times, say 10, 100, 1000 times, with different random initializations of cluster centroids. Pick the one that results in the lowest value for the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353c81e9-03d8-4bd7-8a08-cd431e86b56b",
   "metadata": {},
   "source": [
    "<a name = \"MoG\"></a>\n",
    "## Mixture of Gaussians\n",
    "\n",
    "<a name = \"AD\"></a>\n",
    "### Anomaly Detection\n",
    "<img align=\"right\" src=\"./images/Anomaly Detection.jpeg\" style=\"width:300px;\" >\n",
    "\n",
    "#### Supervised Learning vs Unsupervised Learning\n",
    "Supervised Learning when:\n",
    "- Lots of labelled data of both classes (normal and anomalous)\n",
    "- Future anomalies similar to the ones seen in the training set\n",
    "\n",
    "Unsupervised Learning when:\n",
    "- Unlabelled data or labelled data with few to none anomalies\n",
    "- Various types of anomalies, with unseen future anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a303409-6fd6-4632-acd3-270568f41b5f",
   "metadata": {},
   "source": [
    "<a name = \"MoGM\"></a>\n",
    "### Mixture of Gaussians Model\n",
    "Suppose there is a latent (hidden/unobserved) random variable $z$, and $x^{(i)}, z^{(i)}$ are distributed\n",
    "$$\n",
    "P(x^{(i)}, z^{(i)}) = P(x^{(i)}\\vert z^{(i)})P(z^{(i)})\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\begin{align}\n",
    "z^{(i)} \\sim & \\; \\text{Multinomial}(\\phi), z^{(i)} \\in \\{1, \\dots, k\\} \\\\\n",
    "x^{(i)}\\vert z^{(i)} \\sim & \\; \\mathcal{N}(\\mu_j, \\Sigma_j)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a15bf6-dec0-4d30-88b5-f3bfc0502741",
   "metadata": {},
   "source": [
    "<a name = \"EMA\"></a>\n",
    "### Expectation Maximization Algorithm\n",
    "<div class = \"alert alert-block alert-info\"><b>Intuition:</b> Like K-means but with soft assignments.</div>\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/Expectation maximization.jpeg\" style=\"width:50%;\" >\n",
    "</div>\n",
    "        \n",
    "<section class = \"section--algorithm\">\n",
    "    <div class = \"algorithm--header\"> Expectation Maximization Algorithm (for mixture of gaussians)</div>\n",
    "    <div class = \"algorithm--content\">\n",
    "        <b>E-step</b> (Guess the value of $z^{(i)}$'s)\n",
    "        <blockquote>\n",
    "            Set $$ \n",
    "            \\begin{align}\n",
    "            w_j^{(i)} =& Q_i(z^{(i)}=j) = P(z^{(i)} = j | x^{(i)}; \\mathbf{\\phi}, \\mathbf{\\mu}, \\mathbf{\\Sigma}) \\\\\n",
    "            =& \\frac{P(x^{(i)}\\vert z^{(i)} = j)P(z^{(i)} = j)}{\\sum^k_{l=1}P(x^{(i)}\\vert z^{(i)} = l)P(z^{(i)} = l)}\n",
    "            \\end{align}\n",
    "            $$\n",
    "            where\n",
    "            $$\n",
    "            \\begin{align}\n",
    "            P(z^{(i)} = j) = & \\phi_j  \\\\\n",
    "            P(x^{(i)}\\vert z^{(i)} = j) = & \\frac{1}{{(2\\pi)}^{n/2}\\vert\\Sigma_j\\vert^{1/2}} \\exp{\\left(-\\frac{1}{2}(x^{(i)}-\\mu_j)^T \\Sigma_j^{-1}(x^{(i)}-\\mu_j)\\right)}\n",
    "            \\end{align}\n",
    "            $$\n",
    "        </blockquote>\n",
    "        <b>M-step</b> (Update the gaussians)\n",
    "        <blockquote>\n",
    "        <div class= \"alert alert-block alert-success\">\n",
    "        $$\n",
    "        \\begin{align}\n",
    "        \\max_{\\phi, \\mu, \\Sigma}& \\Sigma_i \\Sigma_{z^{(i)}} Q_i(z^{(i)}) \\log\\left[\\frac{P(x^{(i)}, z^{(i)}; \\phi, \\mu, \\Sigma)}{Q_i(z^{(i)})}\\right] \\\\\n",
    "        &= \\Sigma_i \\Sigma_{j} w^{(i)}_j \\log\\left[\\frac{\\frac{1}{{(2\\pi)}^{n/2}\\vert\\Sigma_j\\vert^{1/2}} \\exp{\\left(-\\frac{1}{2}(x^{(i)}-\\mu_j)^T \\Sigma_j^{-1}(x^{(i)}-\\mu_j)\\right)}\\phi_j}{w^{(i)}_j}\\right]\n",
    "        \\end{align}\n",
    "        $$\n",
    "        </div> <br>\n",
    "        $$\n",
    "        \\begin{align}\n",
    "        \\phi_j :=& \\frac{1}{m} \\sum^m_{i=1} w_j^{(i)} \\\\\n",
    "        \\mu_j :=& \\frac{\\sum^m_{i=1}w_j^{(i)}x^{(i)}}{\\sum^m_{i=1}w_j^{(i)}} \\\\\n",
    "        \\Sigma_j :=& \\frac{\\sum^m_{i=1}w_j^{(i)}(x^{(i)} -\\mu_j)(x^{(i)} -\\mu_j)^T}{\\sum^m_{i=1}w_j^{(i)}}&\n",
    "        \\end{align}\n",
    "        $$\n",
    "        </blockquote>\n",
    "    </div>\n",
    "</section>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a3a417-a9c8-4876-8a6b-16af5fa39851",
   "metadata": {},
   "source": [
    "#### Prediction\n",
    "\n",
    "$$ P(x) = \\Sigma_k p(x,z = k) $$\n",
    "\n",
    "Criteria:\n",
    "\n",
    "$$\n",
    "\\left\\{ \\begin{array}{c c}\n",
    "P(x) \\geq \\epsilon & \\text{(ok)} \\\\\n",
    "P(x) < \\epsilon & \\text{(anomaly)}\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Choose $\\epsilon$ using CV with a labelled CV dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0363045-4500-4f78-903c-348f3ed6c8a4",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><font size=\"3\"><b>Why EM works</b></font></summary>\n",
    "    <br>\n",
    "    <section class = \"section--concept\">\n",
    "        <div class = \"concept--header\"> Jensen's Inequality</div>\n",
    "        <div class = \"concept--content\">\n",
    "        Let $f$ be a convex function. (e.g. $f^{\\prime\\prime}(x)\\geq 0$). Let $X$ be a random variable. Then,\n",
    "        $$\n",
    "        f(\\mathbb{E}[X]) \\leq \\mathbb{E}[f(X))]\n",
    "        $$\n",
    "        Further, if $f^{\\prime\\prime}(x) > 0$ ($f$ is strictly convex), then \n",
    "            $$E[f(X)] = f(E[X]) \\iff X \\text{ is constant.}$$ <br>\n",
    "        <div style = \"text-align: center;\">\n",
    "        <img src=\"./images/Jensen's inequality.jpeg\" style=\"width:300px;\" >\n",
    "        </div>\n",
    "        </div>\n",
    "    </section>\n",
    "    <br>\n",
    "    <b>Idea:</b> construct lower bounds and optimize lower bounds. <br>\n",
    "    <b>Construct lower bounds that are tight at the current $\\theta$:</b>\n",
    "    MLE: $$\n",
    "    \\begin{align}\n",
    "    \\max_\\theta  \\log (\\Pi_i P(x^{(i)};\\theta)) = &\n",
    "    \\Sigma_i \\log P(x^{(i)};\\theta) \\\\\n",
    "    = & \\Sigma_i \\log \\Sigma_{z^{(i)}}  P(x^{(i)}, z^{(i)};\\theta) \\\\\n",
    "    = & \\Sigma_i \\log \\Sigma_{z^{(i)}} Q_i(z^{(i)})  \\left[ \\frac{P(x^{(i)}, z^{(i)};\\theta)}{Q_i(z^{(i)})} \\right] \\; \\text{where $Q_i(z^{(i)})$ is a probability distribution (i.e., $\\Sigma_{z^{(i)}}Q_i(z^{(i)}) = 1$)} \\\\\n",
    "    = & \\Sigma_i \\log \\mathbb{E}_{z^{(i)} \\sim Q_i} \\left[\\frac{P(x^{(i)}, z^{(i)};\\theta)}{Q_i(z^{(i)})}\\right] \\\\\n",
    "    \\geq & \\Sigma_i \\mathbb{E}_{z^{(i)} \\sim Q_i} \\log \\left[\\frac{P(x^{(i)}, z^{(i)};\\theta)}{Q_i(z^{(i)})}\\right] \\; \\text{(Jensen's Inequality)} \\\\\n",
    "    = & \\Sigma_i \\Sigma_{z^{(i)}} Q_i(z^{(i)}) \\log \\left[\\frac{P(x^{(i)}, z^{(i)};\\theta)}{Q_i(z^{(i)})}\\right]\n",
    "    \\end{align}\n",
    "    $$\n",
    "    Want lower bounds to be tight at the current $\\theta$:\n",
    "    $$\n",
    "    \\log \\mathbb{E} \\left[\\frac{P(x^{(i)}, z^{(i)};\\theta)}{Q_i(z^{(i)})}\\right] = \\mathbb{E} \\log \\left[\\frac{P(x^{(i)}, z^{(i)};\\theta)}{Q_i(z^{(i)})}\\right]\n",
    "    $$\n",
    "    Since $\\log(\\cdot)$ is strictly concave, then it follows that:\n",
    "    $$\n",
    "    \\frac{P(x^{(i)}, z^{(i)};\\theta)}{Q_i(z^{(i)})} \\; \\text{is constant} \\implies Q_i(z^{(i)}) \\propto P(x^{(i)}, z^{(i)};\\theta)\n",
    "    $$\n",
    "    Since $Q_i$ is a pdf, it follows that $\\Sigma_{z^{(i)}}Q_i(z^{(i)}) = 1$. Then,\n",
    "    $$\n",
    "    Q_i(z^{(i)}) = \\frac{P(x^{(i)}, z^{(i)};\\theta)}{\\Sigma_{z^{(i)}}P(x^{(i)}, z^{(i)};\\theta)} = \\frac{P(x^{(i)}, z^{(i)};\\theta)}{P(x^{(i)};\\theta)} = P(z^{(i)} \\vert x^{(i)};\\theta)\n",
    "    $$\n",
    "    <b>Expectation Maximization:</b><br>\n",
    "    E-step: Set $Q_i(z^{(i)}) = P(z^{(i)}|x^{(i)};\\theta)$ <br>\n",
    "    M-step: $\\theta = \\arg\\max_\\theta \\Sigma_i\\Sigma_{z^{(i)}}Q_i(z^{(i)})\\log\\left[\\frac{P(x^{(i)}, z^{(i)};\\theta)}{Q_i(z^{(i)})}\\right]$ <br>\n",
    "    This shows that the EM algorithm is a maximum likelihood estimation algorithm, with optimization solved by constructing lower bounds and optimizing lower bounds.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a9c98-0fe7-4e65-a2ba-1aab688cd13f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a name = \"FA\"></a>\n",
    "## Factor Analysis\n",
    "\n",
    "<blockquote>\n",
    "Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors.  -- Wikipedia\n",
    "</blockquote>\n",
    "\n",
    "<blockquote>\n",
    "    <b>Factor Pricing Models in Asset Pricing Theory (APT)</b> (Financial Economics) <br>\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    x^i =& \\alpha_i + \\Sigma_{j=1}^M \\beta_{ij}f_j + \\epsilon^i \\\\\n",
    "    =& \\alpha_i + \\mathbf{\\beta}_i^\\prime \\mathbf{f} + \\epsilon^i \\\\\n",
    "    =& \\mathbb{E}[x^i] + \\Sigma_{j=1}^M \\beta_{ij}\\tilde{f_j} + \\epsilon^i \\; \\text{(by convention, $\\mathbb{E}[\\tilde{f}] = 0$)}\n",
    "    \\end{align}\n",
    "    $$\n",
    "    where $x^i$ is the return to asset $i$ and $f_j$'s are common factors, such as the market portfolio (\"the market\"), industry portfolios, or size and book to market portfolios, etc.\n",
    "    -- For more, see <a href =\"https://press.princeton.edu/books/hardcover/9780691121376/asset-pricing\">Cochrane (2005)<a>.\n",
    "</blockquote>    \n",
    "    \n",
    "### Factor Analysis Model\n",
    "    \n",
    "$\\mathbf{X}$ are **observed** variables of shape $(m,n)$ and $\\mathbf{Z}$ are **unobserved/latent** factors of shape $(m,d)$, where $d < n$. Denote a single vector of factors as $z \\in \\mathbb{R}^d$ and a single vector of variables as $x \\in \\mathbb{R}^n$.\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    z \\sim & \\mathcal{N}(0, I) \\\\\n",
    "    x = & \\mathbf{\\mu} +  \\Lambda z + \\epsilon, \\; \\epsilon \\sim \\mathcal{N}(0, \\Psi)\n",
    "    \\end{align}\n",
    "    $$\n",
    "    where $\\mu \\in \\mathbb{R}^n, \\;\\Lambda \\in \\mathbb{R}^{n\\times d},\\; \\Psi \\in \\mathbb{R}^{n \\times n}$ diagonal.\n",
    "    $$\n",
    "    \\left[\\begin{array}{c}\n",
    "    z \\\\\n",
    "    x\n",
    "    \\end{array}\\right] \\sim\n",
    "    \\mathcal{N}\\left(\\left[\\begin{array}{c}\n",
    "    0 \\\\\n",
    "    \\mu\n",
    "    \\end{array}\\right],\n",
    "    \\left[\\begin{array}{cc}\n",
    "    I & \\Lambda^T \\\\\n",
    "    \\Lambda & \\Psi + \\Lambda\\Lambda^T\n",
    "    \\end{array}\\right]\n",
    "    \\right)\n",
    "    $$\n",
    "    \n",
    "Conventionally, it is easier to work with the demeaned variables $\\tilde x = x - \\mathbb{E}[x]$ such that:<br>\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    z \\sim & \\mathcal{N}(0, I) \\\\\n",
    "    \\tilde x = &  \\Lambda z + \\epsilon, \\; \\epsilon \\sim \\mathcal{N}(0, \\Psi)\n",
    "    \\end{align}\n",
    "    $$<br>\n",
    "    $$\n",
    "    \\left[\\begin{array}{c}\n",
    "    z \\\\\n",
    "    \\tilde x\n",
    "    \\end{array}\\right] \\sim\n",
    "    \\mathcal{N}\\left(\\left[\\begin{array}{c}\n",
    "    0 \\\\\n",
    "    0\n",
    "    \\end{array}\\right],\n",
    "    \\left[\\begin{array}{cc}\n",
    "    I & \\Lambda^T \\\\\n",
    "    \\Lambda & \\Psi + \\Lambda\\Lambda^T\n",
    "    \\end{array}\\right]\n",
    "    \\right)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926363bf-e9f8-4036-b35d-7353296ef2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6fe4f53-e821-4855-b736-4efbdfaa3481",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><font size=\"3\"><b>Conditional Normal Distribution</b></font></summary>\n",
    "    <br>\n",
    "    <section class = \"section--concept\">\n",
    "        <div class = \"concept--header\"> Conditional Normal Distribution</div>\n",
    "        <div class = \"concept--content\">\n",
    "            Given a multivariate normal distribution\n",
    "            $$\n",
    "                \\left[\\begin{array}{c}\n",
    "                X_1 \\\\\n",
    "                X_2\n",
    "                \\end{array}\\right] \\sim\n",
    "                \\mathcal{N}\\left(\\left[\\begin{array}{c}\n",
    "                \\mu_1\\\\\n",
    "                \\mu_2\n",
    "                \\end{array}\\right],\n",
    "                \\left[\\begin{array}{cc}\n",
    "                \\Sigma_{11} & \\Sigma_{12} \\\\\n",
    "                \\Sigma_{21} & \\Sigma_{22}\n",
    "                \\end{array}\\right]\n",
    "                \\right),\n",
    "            $$\n",
    "            the conditional distribution of $X_1 \\vert X_2$ is normal with:\n",
    "            $$\n",
    "            \\begin{align}\n",
    "            \\mu_{1\\vert2} = \\mu_1 + \\Sigma_{12}\\Sigma_{22}^{-1}(X_2 - \\mu_2) \\\\\n",
    "            \\Sigma_{1\\vert2} = \\Sigma_{11} - \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}\n",
    "            \\end{align}\n",
    "            $$\n",
    "            (Derivation : <a href = \"https://stats.stackexchange.com/questions/30588/deriving-the-conditional-distributions-of-a-multivariate-normal-distribution\">here</a>)\n",
    "        </div>\n",
    "    </section>\n",
    "    <br>\n",
    "    It follows that: $z \\vert x$ is normal with:\n",
    "    $$\n",
    "    \\begin{align}\n",
    "    \\mu_{z\\vert x} = \\Lambda^T {(\\Psi + \\Lambda\\Lambda^T)}^{-1}(x - \\mu) \\\\\n",
    "    \\Sigma_{z\\vert x} = I - \\Lambda^T{(\\Psi + \\Lambda\\Lambda^T)}^{-1}\\Lambda\n",
    "    \\end{align}\n",
    "    $$\n",
    "    If we see $z \\vert x$ through the lens of <b>linear projection</b>, then\n",
    "    $$\n",
    "    \\mathbb{E}[z\\vert x] = \\beta (x - \\mu)\n",
    "    $$\n",
    "    where $\\beta \\equiv \\Lambda^T {(\\Psi + \\Lambda\\Lambda^T)}^{-1}$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d1c843-bf0f-4a2d-a098-03e3b13fe005",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Expectation Maximization for Factor Analysis\n",
    "Recall **EM**: (construct and optimise lower bounds)\n",
    "<blockquote>\n",
    "    <b>E-step</b>: compute $w_j^{(i)}= Q_i(z^{(i)}=j) = P(z^{(i)} = j\\vert x^{(i)})$. <br>\n",
    "    <b>M-step</b>: solve for $\\theta$ that maximises the log-likehood\n",
    "    $$\n",
    "    \\arg\\max_\\theta \\log(\\Pi_i^m P(x^{(i)}; \\theta))\n",
    "    = \\arg\\max_\\theta \\sum_i^m \\mathbb{E}_{z^{(i)}\\sim Q_i}[\\log(P(x^{(i)}; \\theta))]\n",
    "    $$\n",
    "</blockquote>\n",
    "Here, since $z$ follows a normal distribution, we can make use of the sufficient statistics (i.e. mean and variance). <br>\n",
    "<br>\n",
    "<section class = \"section--algorithm\">\n",
    "    <div class = \"algorithm--header\"> Expectation Maximization Algorithm (for factor analysis)</div>\n",
    "    <div class = \"algorithm--content\">\n",
    "        <b>E-step</b> (Estimate the first and second moments of the conditional distribution, i.e. $E[z^{(i)}\\vert  x^{(i)}]$ and $E[z^{(i)}z^{(i)\\prime} \\vert  x^{(i)}]$ using $\\Lambda$ and $\\Psi$)\n",
    "        <blockquote>\n",
    "        The variance is given by\n",
    "            $$\n",
    "            \\mathrm{Var}[z^{(i)} \\vert x^{(i)}] = \\Sigma_{z\\vert x^{(i)}} = I - \\Lambda^T{(\\Psi + \\Lambda\\Lambda^T)}^{-1}\\Lambda.\n",
    "            $$\n",
    "            <br>\n",
    "        <div class = \"alert alert-block alert-info\">\n",
    "            Recall that $\\mathrm{Var}[z] = \\mathbb{E}[zz^\\prime] - \\mathbb{E}[z]\\mathbb{E}[z^\\prime]$, which implies that:\n",
    "            $$\n",
    "            E[z^{(i)}z^{(i)\\prime} \\vert x^{(i)}] = \\mathrm{Var}[z^{(i)} \\vert x^{(i)}] + \\mathbb{E}[z^{(i)}\\vert  x^{(i)}]\\mathbb{E}[z^{(i)\\prime}\\vert  x^{(i)}]\n",
    "            $$\n",
    "        </div>\n",
    "        Thus, only need to estimate the conditional mean $\\mathbb{E}[z^{(i)}\\vert  x^{(i)}]$:\n",
    "            $$\n",
    "            \\hat \\mu_{z\\vert x^{(i)}} = \\Lambda^T {(\\Psi + \\Lambda\\Lambda^T)}^{-1}(x^{(i)} - \\hat \\mu).\n",
    "            $$\n",
    "        Estimate the mean $\\mu$ once before the EM as the sample mean:\n",
    "            $$\n",
    "            \\hat \\mu = \\frac{1}{m}\\sum_{i=1}^m x^{(i)}.\n",
    "            $$\n",
    "        </blockquote>\n",
    "        <b>M-step</b> (Update $\\Lambda$ and $\\Psi$)\n",
    "        <blockquote>\n",
    "        <div class= \"alert alert-block alert-success\">\n",
    "        $$\n",
    "        \\begin{align}\n",
    "        \\max_{\\Lambda, \\Psi}& \\sum_i \\mathbb{E}_{z^{(i)}\\vert x^{(i)}} \\log\\left[P(x^{(i)}; \\Lambda, \\Psi)\\right] \\\\\n",
    "        &= \\sum_i \\mathbb{E}_{z^{(i)}\\vert x^{(i)}} \\log\\left[ (2\\pi)^{-n/2} {\\vert\\Psi\\vert}^{-1/2} \\exp\\left\\{-\\frac{1}{2}{(x^{(i)} - \\Lambda z^{(i)})}^T\\Psi^{-1}(x^{(i)} - \\Lambda z^{(i)}) \\right\\}\\right] \\\\\n",
    "        &= -\\frac{mn}{2}\\log 2\\pi - \\frac{n}{2}\\log \\vert \\Psi \\vert - \\frac{1}{2}\\sum_i x^{(i)T}\\Psi^{-1}x^{(i)} + \\sum_i x^{(i)T}\\Psi^{-1}\\Lambda \\mathbb{E}[z^{(i)}\\vert x^{(i)}] - \\frac{1}{2}\\sum_i \\mathrm{tr}\\left[\\Lambda^T\\Psi^{-1}\\Lambda\\mathbb{E}[z^{(i)}z^{(i)T}\\vert x^{(i)}]\\right]\n",
    "        \\end{align}\n",
    "        $$\n",
    "        Solve the first order conditions (FOCs) w.r.t. $\\Lambda$ and $\\Psi$.\n",
    "        </div> <br>\n",
    "        $$\n",
    "        \\begin{align}\n",
    "        \\Lambda :=& \\left(\\sum^m_{i=1}x^{(i)}\\mathbb{E}[z^{(i)T}\\vert x^{(i)}]\\right){\\left(\\sum^m_{i=1}\\mathbb{E}[z^{(i)}z^{(i)T}\\vert x^{(i)}]\\right)}^{-1}  \\\\\n",
    "        \\Psi  :=& \\mathrm{diag}\\left(\\frac{1}{m}\\sum_i^m x^{(i)}x^{(i)T} - \\frac{1}{m}\\underbrace{\\left(\\sum_i^m x^{(i)}\\mathbb{E}[z^{(i)T}\\vert x^{(i)}]\\right){\\left(\\sum^m_{i=1}\\mathbb{E}[z^{(i)}z^{(i)T}\\vert x^{(i)}]\\right)}^{-1}}_{\\Lambda^{\\mathrm{new}}}\\left(\\sum_i^m \\mathbb{E}[z^{(i)}\\vert x^{(i)}]x^{(i)T}\\right)\\right)\n",
    "        \\end{align}\n",
    "        $$\n",
    "        where the $\\mathrm{diag}$ operator sets all of the off-diagonal elements of a matrix to $0$.\n",
    "        </blockquote>\n",
    "        (Original paper: <a href = \"https://mlg.eng.cam.ac.uk/zoubin/papers/tr-96-1.pdf\">here</a>)\n",
    "    </div>\n",
    "</section>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204ec9f6-9c1d-439f-8161-f20f80fe66d2",
   "metadata": {},
   "source": [
    "<a name = \"PCA\"></a>\n",
    "## Principal Component Analysis\n",
    "<blockquote>\n",
    "    The principal components of a set of data in $\\mathbb{R}^p$ provides a sequence of best linear approximations to that data, of all ranks $q \\leq p$. -- The Elements of Statistical Learning, p.534\n",
    "    <div style = \"text-align: center;\">\n",
    "    <img src=\"./images/PCA.jpeg\" style=\"width:80%;\" > <br>\n",
    "    Source: The Elements of Statistical Learning, p.536\n",
    "    </div>\n",
    "</blockquote>\n",
    "\n",
    "\n",
    "### Parameterization\n",
    "\n",
    "Denote the observations by $x_1, x_2, \\dots, x_N$, and consider the rank-$q$ linear model for representing them:\n",
    "$$\n",
    "f(\\lambda) = \\mu + \\mathbf{V}_q\\lambda\n",
    "$$\n",
    "where $\\mu$ is a location vector in $\\mathbb{R}^p$, $\\mathbf{V}_q$ is a $p \\times q$ matrix with $q$ orthogonal unit vectors as columns, and $\\lambda$ is $q$ vector of parameters (the $q$ principal components). $f(\\lambda)$ is a point on the affine hyperplane of rank $q$ defined by $f(\\cdot)$.\n",
    "\n",
    "### Reconstruction error\n",
    "The cost function of PCs:\n",
    "$$\n",
    "\\min_{\\mu,\\{\\lambda_i\\}, \\mathbf{V}_q} \\sum_{i=1}^N \\Vert x_i - \\mu - \\mathbf{V}_q\\lambda_i\\Vert^2\n",
    "$$\n",
    "Partial optimization:\n",
    "- optimise for $\\mu$ and $\\lambda_i$ keeping $\\mathbf{V}_q$ fixed.\n",
    "- plug in solutions for $\\mu$ and $\\lambda_i$ and optimise for $\\mathbf{V}_q$.\n",
    "Solutions:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\mu =& \\bar x \\\\\n",
    "\\hat \\lambda_i =& \\mathbf{V}_q^T(x_i - \\bar x)\n",
    "\\end{align}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\mathbf{H}_q = \\mathbf{V}_q \\mathbf{V}_q^T\n",
    "$$\n",
    "is a projection matrix, and maps each point $x_i$ onto the subspace spanned by the columns of $\\mathbf{V}_q$. $\\mathbf{V}_q$ is the first $q$ columns of $\\mathbf{V}$ in $\\mathbf{X}=\\mathbf{U}\\mathbf{D}\\mathbf{V}^T$.\n",
    "\n",
    "### Singular value decomposition\n",
    "Consider an $N \\times p$ matrix $\\mathbf{X}$:\n",
    "$$\\mathbf{X} =\\mathbf{U}\\mathbf{D}\\mathbf{V}^T $$\n",
    "where $\\mathbf{U}$ is an $N \\times p$ orthogonal matrix ($\\mathbf{U}^T\\mathbf{U} = \\mathbf{I}_p$) whose columns $\\mathbf{u}_j$ are called the *left singular vectors*; $\\mathbf{V}$ is a $p \\times p$ orthogonal matrix with columns $\\mathbf{v}_j$ called the *right singular vectors*, and $\\mathbf{D}$ is a $p \\times p$ diagonal matrix, with diagonal elments $d_1 \\geq d_2 \\geq \\cdots \\geq d_p \\geq 0$ known as the *singular values*. The columns of $\\mathbf{UD}$ are called the principal components of $\\mathbf{X}$.\n",
    "\n",
    "Conveniently, the $k$-th principal component is:\n",
    "$$\n",
    "\\mathbf{X} \\mathbf{v}_k = \\mathbf{u}_k d_k\n",
    "$$\n",
    "(using the orthogonality of $\\mathbf{V}$).\n",
    "\n",
    "### Evaluation metrics\n",
    "1. variation of data captured by the first few PCs.\n",
    "2. first few singular values in comparison with those obtained for equivalent uncorrelated data.\n",
    "\n",
    "<div style = \"text-align: center;\">\n",
    "    <img src=\"./images/Handwritten digits.png\" style=\"width:80%;\" > <br>\n",
    "    Applying PCA to the MNIST handwritten digits dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d2f19d-c44f-4d3f-b279-4bc0f5e80c96",
   "metadata": {},
   "source": [
    "<a name = \"ICA\"></a>\n",
    "## Independent Component Analysis\n",
    "\n",
    "Survey paper: <a href = \"https://www.cs.jhu.edu/~ayuille/courses/Stat161-261-Spring14/HyvO00-icatut.pdf\">Independent Component Analysis: A Tutorial (Hyvarinen and Oja)</a> \n",
    "\n",
    "### A latent variables model\n",
    "Recall the singular value decomposition:\n",
    "$$\n",
    "\\mathbf{X} = \\mathbf{UDV}^T\n",
    "$$\n",
    "Write $\\mathbf{S} = \\sqrt{N}\\mathbf{U}$ and $\\mathbf{A}^T = \\mathbf{DV}^T/\\sqrt{N}$ and we have a latent variables model:\n",
    "$$\n",
    "\\mathbf{X} = \\mathbf{SA}^T\n",
    "$$\n",
    "Assuming that the collumns of $\\mathbf{X}$ (and hence $\\mathbf{U}$) have mean zero, this implies that the columns of $\\mathbf{S}$ have *zero mean*, are *uncorrelated* and have *unit variance*. Consider a single observation $X$ of size $p \\times 1$. (i.e. $\\mathbf{X} = \\left[\\begin{array}{ccc} X_1 & \\cdots & X_3\\end{array}\\right]^T$.)\n",
    "$$\n",
    "X = \\mathbf{A}S\n",
    "$$\n",
    "where $S$ is $p \\times 1$. Alternatively, it can be expressed as a system of equations:\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "X_1 &=& a_{11} S_1 + a_{12} S_2 + \\cdots + a_{1p}S_p \\\\\n",
    "X_2 &=& a_{21} S_1 + a_{22} S_2 + \\cdots + a_{2p}S_p \\\\\n",
    "\\vdots & &\\vdots \\\\\n",
    "X_p &=& a_{p1} S_1 + a_{p2} S_2 + \\cdots + a_{pp}S_p \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "### Identifiability issse\n",
    "Consider any orthogonal $p \\times p$ matrix $\\mathbf{R}$.\n",
    "$$\n",
    "\\begin{align}\n",
    " X =& \\mathbf{A}S \\\\\n",
    "   =& \\mathbf{AR}^T\\mathbf{R}S \\\\\n",
    "   =& \\mathbf{A}^*S^*\n",
    "\\end{align}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\mathrm{Cov}(S^*) = \\mathbf{R}\\mathrm{Cov}(S)\\mathbf{R}^T = \\mathbf{I} = \\mathrm{Cov}(S)\n",
    "$$\n",
    "Therefore, there are many such decompositions and it is impossible to identify any particular latent variables as unique underlying sources.\n",
    "\n",
    "### Classical factor analysis model\n",
    "$$\n",
    "X = \\mathbf{A}_{p \\times q} S_{q \\times 1} + \\mathbf{\\epsilon}_{p \\times 1}\n",
    "$$\n",
    "where the $\\epsilon_{j}$ are uncorrelated zero-mean disturbances. The identifiability issue remains.\n",
    "\n",
    "### The independent component analysis model\n",
    "$$\n",
    "X = \\mathbf{A}S\n",
    "$$\n",
    "where $S_i$ are assumed to be **statistically independent** rather than uncorrelated. To avoid the identifiability issue, we need to assume that the $S_i$ are also **non-Gaussian**. (Note: multivariate Gaussian is determined up to its second moments.)\n",
    "<blockquote>\n",
    "ICA is able to perform <em>blind source separation</em> by exploring the independence and non-Gaussianity of the original sources. -- The Elements of Statistical Learning, p. 561\n",
    "</blockquote>\n",
    "\n",
    "### Approaches to ICA\n",
    "\n",
    "#### Differential entropy and mutual information\n",
    "Goals:\n",
    "- Minimize the mutual information (*Kullback-Leibler distance* between the density of a random vector and its independence counterpart).\n",
    "- Minimize the differential entropy (maximise the non-Gaussianity).\n",
    "\n",
    "Differential entropy of a random variable $Y$:\n",
    "\n",
    "$$\n",
    "H(Y) = - \\int g(y)\\log g(y) dy\n",
    "$$\n",
    "\n",
    "<div class = \"alert alert-block alert-info\"><b>Theorem (from information theory):</b> With a normal distribution, differential entropy is maximized for a given variance. A Gaussian random variable has the largest entropy amongst all random variables of equal variance, or, alternatively, the maximum entropy distribution under constraints of mean and variance is the Gaussian. <br> Proof: <a href=\"https://en.wikipedia.org/wiki/Differential_entropy\">here</a>.</div>\n",
    "\n",
    "Mutual information between the components of a random vector $Y$:\n",
    "$$\n",
    "I(Y) = \\sum_{j=1}^p H(Y_j) - H(Y)\n",
    "$$\n",
    "If $X$ has covariance $\\mathbf{I}$, and $S = \\mathbf{A}^T X$ with $\\mathbf{A}$ orthogonal (implied by $X$ has covariance $\\mathbf{I}$; see ESLII, p.560), then\n",
    "$$\n",
    "\\begin{align}\n",
    "I(S) =& \\sum_{j=1}^p H(S_j) - H(X) - \\log \\vert \\mathrm{det} \\mathbf{A}\\vert \\\\\n",
    " =& \\sum_{j=1}^p H(S_j) - H(X)\n",
    "\\end{align}\n",
    "$$\n",
    "- Finding an $\\mathbf{A}$ to minimise $I(S) = I(\\mathbf{A}^T X)$ looks for the orthogonal transformation that leads to the most independence between its components.\n",
    "- This is equivalent to minimising the sum of the entropies of the separate components of Y.\n",
    "- This amounts to maximising their departures from Gaussanity.\n",
    "\n",
    "#### Negentropy\n",
    "Goal:\n",
    "- Minimise the departure of $S_j$ from Gaussanity.\n",
    "\n",
    "The negentropy measure:\n",
    "\n",
    "$$\n",
    "J(Y_j) = H(Z_j) - H(Y_j)\n",
    "$$\n",
    "\n",
    "where $Y_j$ is a Gaussian random variable with the same variance as $Y_j$. Note that negentropy is non-negative, and measures the departure of $Y_j$ from Gaussanity. A simple approximation to negentropy proposed by <a href = \"https://www.cs.jhu.edu/~ayuille/courses/Stat161-261-Spring14/HyvO00-icatut.pdf\">Hyvarinen and Oja</a>:\n",
    "\n",
    "$$\n",
    "J(Y_j) \\approx [\\mathrm{E}G(Y_j) - \\mathrm{E}G(Z_j)]^2,\n",
    "$$\n",
    "where $G(x) = \\frac{1}{a}\\log\\cosh(ax)$ for $1\\leq a \\leq 2$. When applying to a sample of data, replace the expectations with sample averages. \n",
    "\n",
    "<blockquote>\n",
    "More classical (and less robust) measures are based on fourth moments, and hence look for departures from the Gaussian via kurtosis. See <a href = \"https://www.cs.jhu.edu/~ayuille/courses/Stat161-261-Spring14/HyvO00-icatut.pdf\">Hyvarinen and Oja</a> for more details. -- The Elements of Statistical Learning, p. 562\n",
    "</blockquote>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datascience)",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
