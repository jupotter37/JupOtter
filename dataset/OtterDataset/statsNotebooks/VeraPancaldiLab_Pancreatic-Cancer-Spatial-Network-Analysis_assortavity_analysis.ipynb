{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = 1000000000 \n",
    "from PIL import Image, ImageOps\n",
    "import fcsparser\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "sys.path.extend(['../../mosna/mosna'])\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "import copy\n",
    "from skimage import color\n",
    "import matplotlib as mpl\n",
    "import napari\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import loguniform\n",
    "import umap\n",
    "import hdbscan\n",
    "from sklearn.cluster import OPTICS, cluster_optics_dbscan\n",
    "from skimage import io\n",
    "from scipy.stats import ttest_ind, mannwhitneyu, ks_2samp \n",
    "import glob\n",
    "import re\n",
    "import mosna\n",
    "from tysserand import tysserand as ty\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "############ Make test networks ############\n",
    "\n",
    "def make_triangonal_net():\n",
    "    \"\"\"\n",
    "    Make a triangonal network.\n",
    "    \"\"\"\n",
    "    dict_nodes = {'x': [1,3,2],\n",
    "                  'y': [2,2,1],\n",
    "                  'a': [1,0,0],\n",
    "                  'b': [0,1,0],\n",
    "                  'c': [0,0,1]}\n",
    "    nodes = pd.DataFrame.from_dict(dict_nodes)\n",
    "    \n",
    "    data_edges = [[0,1],\n",
    "                  [1,2],\n",
    "                  [2,0]]\n",
    "    edges = pd.DataFrame(data_edges, columns=['source','target'])\n",
    "    \n",
    "    return nodes, edges\n",
    "\n",
    "def make_trigonal_net():\n",
    "    \"\"\"\n",
    "    Make a trigonal network.\n",
    "    \"\"\"\n",
    "    dict_nodes = {'x': [1,3,2,0,4,2],\n",
    "                  'y': [2,2,1,3,3,0],\n",
    "                  'a': [1,0,0,1,0,0],\n",
    "                  'b': [0,1,0,0,1,0],\n",
    "                  'c': [0,0,1,0,0,1]}\n",
    "    nodes = pd.DataFrame.from_dict(dict_nodes)\n",
    "    \n",
    "    data_edges = [[0,1],\n",
    "                  [1,2],\n",
    "                  [2,0],\n",
    "                  [0,3],\n",
    "                  [1,4],\n",
    "                  [2,5]]\n",
    "    edges = pd.DataFrame(data_edges, columns=['source','target'])\n",
    "    \n",
    "    return nodes, edges\n",
    "\n",
    "def make_P_net():\n",
    "    \"\"\"\n",
    "    Make a P-shaped network.\n",
    "    \"\"\"\n",
    "    dict_nodes = {'x': [0,0,0,0,1,1],\n",
    "                  'y': [0,1,2,3,3,2],\n",
    "                  'a': [1,0,0,0,0,0],\n",
    "                  'b': [0,0,0,0,1,0],\n",
    "                  'c': [0,1,1,1,0,1]}\n",
    "    nodes = pd.DataFrame.from_dict(dict_nodes)\n",
    "    \n",
    "    data_edges = [[0,1],\n",
    "                  [1,2],\n",
    "                  [2,3],\n",
    "                  [3,4],\n",
    "                  [4,5],\n",
    "                  [5,2]]\n",
    "    edges = pd.DataFrame(data_edges, columns=['source','target'])\n",
    "    \n",
    "    return nodes, edges\n",
    "\n",
    "def make_high_assort_net():\n",
    "    \"\"\"\n",
    "    Make a highly assortative network.\n",
    "    \"\"\"\n",
    "    dict_nodes = {'x': np.arange(12).astype(int),\n",
    "                  'y': np.zeros(12).astype(int),\n",
    "                  'a': [1] * 4 + [0] * 8,\n",
    "                  'b': [0] * 4 + [1] * 4 + [0] * 4,\n",
    "                  'c': [0] * 8 + [1] * 4}\n",
    "    nodes = pd.DataFrame.from_dict(dict_nodes)\n",
    "    \n",
    "    edges_block = np.vstack((np.arange(3), np.arange(3) +1)).T\n",
    "    data_edges = np.vstack((edges_block, edges_block + 4, edges_block + 8))\n",
    "    edges = pd.DataFrame(data_edges, columns=['source','target'])\n",
    "    \n",
    "    return nodes, edges\n",
    "\n",
    "def make_high_disassort_net():\n",
    "    \"\"\"\n",
    "    Make a highly dissassortative network.\n",
    "    \"\"\"\n",
    "    dict_nodes = {'x': [1,2,3,4,4,4,3,2,1,0,0,0],\n",
    "                  'y': [0,0,0,1,2,3,4,4,4,3,2,1],\n",
    "                  'a': [1,0,0] * 4,\n",
    "                  'b': [0,1,0] * 4,\n",
    "                  'c': [0,0,1] * 4}\n",
    "    nodes = pd.DataFrame.from_dict(dict_nodes)\n",
    "    \n",
    "    data_edges = np.vstack((np.arange(12), np.roll(np.arange(12), -1))).T\n",
    "    edges = pd.DataFrame(data_edges, columns=['source','target'])\n",
    "    \n",
    "    return nodes, edges\n",
    "\n",
    "def make_random_graph_2libs(nb_nodes=100, p_connect=0.1, attributes=['a', 'b', 'c'], multi_mod=False):\n",
    "    import networkx as nx\n",
    "    # initialize the network\n",
    "    G = nx.fast_gnp_random_graph(nb_nodes, p_connect, directed=False)\n",
    "    pos = nx.kamada_kawai_layout(G)\n",
    "    nodes = pd.DataFrame.from_dict(pos, orient='index', columns=['x','y'])\n",
    "    edges = pd.DataFrame(list(G.edges), columns=['source', 'target'])\n",
    "\n",
    "    # set attributes\n",
    "    if multi_mod:\n",
    "        nodes_class = np.random.randint(0, 2, size=(nb_nodes, len(attributes))).astype(bool)\n",
    "        nodes = nodes.join(pd.DataFrame(nodes_class, index=nodes.index, columns=attributes))\n",
    "    else:\n",
    "        nodes_class = np.random.choice(attributes, nb_nodes)\n",
    "        nodes = nodes.join(pd.DataFrame(nodes_class, index=nodes.index, columns=['nodes_class']))\n",
    "        nodes = nodes.join(pd.get_dummies(nodes['nodes_class']))\n",
    "\n",
    "    if multi_mod:\n",
    "        for col in attributes:\n",
    "        #     nx.set_node_attributes(G, df_nodes[col].to_dict(), col.replace('+','AND')) # only for glm extension file\n",
    "            nx.set_node_attributes(G, nodes[col].to_dict(), col)\n",
    "    else:\n",
    "        nx.set_node_attributes(G, nodes['nodes_class'].to_dict(), 'nodes_class')\n",
    "    \n",
    "    return nodes, edges, G\n",
    "\n",
    "############ Assortativity ############\n",
    "\n",
    "def count_edges_undirected(nodes, edges, attributes):\n",
    "    \"\"\"Compute the count of edges whose end nodes correspond to given attributes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nodes : dataframe\n",
    "        Attributes of all nodes\n",
    "    edges : dataframe\n",
    "        Edges between nodes given by their index\n",
    "    attributes: list\n",
    "        The attributes of nodes whose edges are selected\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    count : int\n",
    "       Count of edges\n",
    "    \"\"\"\n",
    "    \n",
    "    pairs = np.logical_or(np.logical_and(nodes.loc[edges['source'], attributes[0]].values, nodes.loc[edges['target'], attributes[1]].values),\n",
    "                          np.logical_and(nodes.loc[edges['target'], attributes[0]].values, nodes.loc[edges['source'], attributes[1]].values))\n",
    "    count = pairs.sum()\n",
    "    \n",
    "    return count\n",
    "\n",
    "def count_edges_directed(nodes, edges, attributes):\n",
    "    \"\"\"Compute the count of edges whose end nodes correspond to given attributes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nodes : dataframe\n",
    "        Attributes of all nodes\n",
    "    edges : dataframe\n",
    "        Edges between nodes given by their index\n",
    "    attributes: list\n",
    "        The attributes of nodes whose edges are selected\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    count : int\n",
    "       Count of edges\n",
    "    \"\"\"\n",
    "    \n",
    "    pairs = np.logical_and(nodes.loc[edges['source'], attributes[0]].values, nodes.loc[edges['target'], attributes[1]].values)\n",
    "    count = pairs.sum()\n",
    "    \n",
    "    return count\n",
    "\n",
    "def mixing_matrix(nodes, edges, attributes, normalized=True, double_diag=True):\n",
    "    \"\"\"Compute the mixing matrix of a network described by its `nodes` and `edges`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nodes : dataframe\n",
    "        Attributes of all nodes\n",
    "    edges : dataframe\n",
    "        Edges between nodes given by their index\n",
    "    attributes: list\n",
    "        Categorical attributes considered in the mixing matrix\n",
    "    normalized : bool (default=True)\n",
    "        Return counts if False or probabilities if True.\n",
    "    double_diag : bool (default=True)\n",
    "        If True elements of the diagonal are doubled like in NetworkX or iGraph \n",
    "       \n",
    "    Returns\n",
    "    -------\n",
    "    mixmat : array\n",
    "       Mixing matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    mixmat = np.zeros((len(attributes), len(attributes)))\n",
    "\n",
    "    for i in range(len(attributes)):\n",
    "        for j in range(i+1):\n",
    "            mixmat[i, j] = count_edges_undirected(nodes, edges, attributes=[attributes[i],attributes[j]])\n",
    "            mixmat[j, i] = mixmat[i, j]\n",
    "        \n",
    "    if double_diag:\n",
    "        for i in range(len(attributes)):\n",
    "            mixmat[i, i] += mixmat[i, i]\n",
    "            \n",
    "    if normalized:\n",
    "        mixmat = mixmat / mixmat.sum()\n",
    "    \n",
    "    return mixmat\n",
    "\n",
    "# NetworkX code:\n",
    "def attribute_ac(M):\n",
    "    \"\"\"Compute assortativity for attribute matrix M.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    M : numpy array or matrix\n",
    "        Attribute mixing matrix.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This computes Eq. (2) in Ref. [1]_ , (trace(e)-sum(e^2))/(1-sum(e^2)),\n",
    "    where e is the joint probability distribution (mixing matrix)\n",
    "    of the specified attribute.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] M. E. J. Newman, Mixing patterns in networks,\n",
    "       Physical Review E, 67 026126, 2003\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import numpy\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            \"attribute_assortativity requires NumPy: http://scipy.org/ \")\n",
    "    if M.sum() != 1.0:\n",
    "        M = M / float(M.sum())\n",
    "    M = numpy.asmatrix(M)\n",
    "    s = (M * M).sum()\n",
    "    t = M.trace()\n",
    "    r = (t - s) / (1 - s)\n",
    "    return float(r)\n",
    "\n",
    "def mixmat_to_df(mixmat, attributes):\n",
    "    \"\"\"\n",
    "    Make a dataframe of a mixing matrix.\n",
    "    \"\"\"\n",
    "    return pd.DataFrame(mixmat, columns=attributes, index=attributes)\n",
    "\n",
    "def mixmat_to_columns(mixmat):\n",
    "    \"\"\"\n",
    "    Flattens a mixing matrix taking only elements of the lower triangle and diagonal.\n",
    "    To revert this use `series_to_mixmat`.\n",
    "    \"\"\"\n",
    "    N = mixmat.shape[0]\n",
    "    val = []\n",
    "    for i in range(N):\n",
    "        for j in range(i+1):\n",
    "            val.append(mixmat[i,j])\n",
    "    return val\n",
    "\n",
    "def series_to_mixmat(series, medfix=' - ', discard=' Z'):\n",
    "    \"\"\"\n",
    "    Convert a 1D pandas series into a 2D dataframe.\n",
    "    To revert this use `mixmat_to_columns`.\n",
    "    \"\"\"\n",
    "    N = series.size\n",
    "    combi = [[x.split(medfix)[0].replace(discard, ''), x.split(medfix)[1].replace(discard, '')] for x in series.index]\n",
    "    # get unique elements of the list of mists\n",
    "    from itertools import chain \n",
    "    uniq = [*{*chain.from_iterable(combi)}]\n",
    "    mat = pd.DataFrame(data=None, index=uniq, columns=uniq)\n",
    "    for i in series.index:\n",
    "        x = i.split(medfix)[0].replace(discard, '')\n",
    "        y = i.split(medfix)[1].replace(discard, '')\n",
    "        val = series[i]\n",
    "        mat.loc[x, y] = val\n",
    "        mat.loc[y, x] = val\n",
    "    return mat\n",
    "\n",
    "def attributes_pairs(attributes, prefix='', medfix=' - ', suffix=''):\n",
    "    \"\"\"\n",
    "    Make a list of unique pairs of attributes.\n",
    "    Convenient to make the names of elements of the mixing matrix \n",
    "    that is flattened.\n",
    "    \"\"\"\n",
    "    N = len(attributes)\n",
    "    col = []\n",
    "    for i in range(N):\n",
    "        for j in range(i+1):\n",
    "            col.append(prefix + attributes[i] + medfix + attributes[j] + suffix)\n",
    "    return col\n",
    "\n",
    "def core_rand_mixmat(nodes, edges, attributes):\n",
    "    \"\"\"\n",
    "    Compute the mixing matrix of a network after nodes' attributes\n",
    "    are randomized once.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nodes : dataframe\n",
    "        Attributes of all nodes.\n",
    "    edges : dataframe\n",
    "        Edges between nodes given by their index.\n",
    "    attributes: list\n",
    "        Categorical attributes considered in the mixing matrix.\n",
    "       \n",
    "    Returns\n",
    "    -------\n",
    "    mixmat_rand : array\n",
    "       Mmixing matrix of the randomized network.\n",
    "    \"\"\"\n",
    "    nodes_rand = deepcopy(nodes)\n",
    "    nodes_rand[attributes] = shuffle(nodes_rand[attributes].values)\n",
    "    mixmat_rand = mixing_matrix(nodes_rand, edges, attributes)\n",
    "    return mixmat_rand\n",
    "\n",
    "def randomized_mixmat(nodes, edges, attributes, n_shuffle=50, parallel='max', memory_limit='50GB'):\n",
    "    \"\"\"Randomize several times a network by shuffling the nodes' attributes.\n",
    "    Then compute the mixing matrix and the corresponding assortativity coefficient.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nodes : dataframe\n",
    "        Attributes of all nodes.\n",
    "    edges : dataframe\n",
    "        Edges between nodes given by their index.\n",
    "    attributes: list\n",
    "        Categorical attributes considered in the mixing matrix.\n",
    "    n_shuffle : int (default=50)\n",
    "        Number of attributes permutations.\n",
    "    parallel : bool, int or str (default=\"max\")\n",
    "        How parallelization is performed.\n",
    "        If False, no parallelization is done.\n",
    "        If int, use this number of cores.\n",
    "        If 'max', use the maximum number of cores.\n",
    "        If 'max-1', use the max of cores minus 1.\n",
    "       \n",
    "    Returns\n",
    "    -------\n",
    "    mixmat_rand : array (n_shuffle x n_attributes x n_attributes)\n",
    "       Mixing matrices of each randomized version of the network\n",
    "    assort_rand : array  of size n_shuffle\n",
    "       Assortativity coefficients of each randomized version of the network\n",
    "    \"\"\"\n",
    "    \n",
    "    mixmat_rand = np.zeros((n_shuffle, len(attributes), len(attributes)))\n",
    "    assort_rand = np.zeros(n_shuffle)\n",
    "    \n",
    "    if parallel is False:\n",
    "        for i in tqdm(range(n_shuffle), desc=\"randomization\"):\n",
    "            mixmat_rand[i] = core_rand_mixmat(nodes, edges, attributes)\n",
    "            assort_rand[i] = attribute_ac(mixmat_rand[i])\n",
    "    else:\n",
    "        from multiprocessing import cpu_count\n",
    "        from dask.distributed import Client, LocalCluster\n",
    "        from dask import delayed\n",
    "        \n",
    "        # select the right number of cores\n",
    "        nb_cores = cpu_count()\n",
    "        if isinstance(parallel, int):\n",
    "            use_cores = min(parallel, nb_cores)\n",
    "        elif parallel == 'max-1':\n",
    "            use_cores = nb_cores - 1\n",
    "        elif parallel == 'max':\n",
    "            use_cores = nb_cores\n",
    "        # set up cluster and workers\n",
    "        cluster = LocalCluster(n_workers=use_cores, \n",
    "                               threads_per_worker=1,\n",
    "                               memory_limit=memory_limit)\n",
    "        client = Client(cluster)\n",
    "        \n",
    "        # store the matrices-to-be\n",
    "        mixmat_delayed = []\n",
    "        for i in range(n_shuffle):\n",
    "            mmd = delayed(core_rand_mixmat)(nodes, edges, attributes)\n",
    "            mixmat_delayed.append(mmd)\n",
    "        # evaluate the parallel computation and return is as a 3d array\n",
    "        mixmat_rand = delayed(np.array)(mixmat_delayed).compute()\n",
    "        # only the assortativity coeff is not parallelized\n",
    "        for i in range(n_shuffle):\n",
    "            assort_rand[i] = attribute_ac(mixmat_rand[i])\n",
    "        # close workers and cluster\n",
    "        client.close()\n",
    "        cluster.close()\n",
    "            \n",
    "    return mixmat_rand, assort_rand\n",
    "\n",
    "def zscore(mat, mat_rand, axis=0, return_stats=False):\n",
    "    rand_mean = mat_rand.mean(axis=axis)\n",
    "    rand_std = mat_rand.std(axis=axis)\n",
    "    zscore = (mat - rand_mean) / rand_std\n",
    "    if return_stats:\n",
    "        return rand_mean, rand_std, zscore\n",
    "    else:\n",
    "        return zscore\n",
    "    \n",
    "def select_pairs_from_coords(coords_ids, pairs, how='inner', return_selector=False):\n",
    "    \"\"\"\n",
    "    Select edges related to specific nodes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    coords_ids : array\n",
    "        Indices or ids of nodes.\n",
    "    pairs : array\n",
    "        Edges defined as pairs of nodes ids.\n",
    "    how : str (default='inner')\n",
    "        If 'inner', only edges that have both source and target \n",
    "        nodes in coords_ids are select. If 'outer', edges that \n",
    "        have at least a node in coords_ids are selected.\n",
    "    return_selector : bool (default=False)\n",
    "        If True, only the boolean mask is returned.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pairs_selected : array\n",
    "        Edges having nodes in coords_ids.\n",
    "    select : array\n",
    "        Boolean array to select latter on edges.\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> coords_ids = np.array([5, 6, 7])\n",
    "    >>> pairs = np.array([[1, 2],\n",
    "                          [3, 4],\n",
    "                          [5, 6],\n",
    "                          [7, 8]])\n",
    "    >>> select_pairs_from_coords(coords_ids, pairs, how='inner')\n",
    "    array([[5, 6]])\n",
    "    >>> select_pairs_from_coords(coords_ids, pairs, how='outer')\n",
    "    array([[5, 6],\n",
    "           [7, 8]])\n",
    "    \"\"\"\n",
    "    \n",
    "    select_source = np.in1d(pairs[:, 0], coords_ids)\n",
    "    select_target = np.in1d(pairs[:, 1], coords_ids)\n",
    "    if how == 'inner':\n",
    "        select = np.logical_and(select_source, select_target)\n",
    "    elif how == 'outer':\n",
    "        select = np.logical_or(select_source, select_target)\n",
    "    if return_selector:\n",
    "        return select\n",
    "    pairs_selected = pairs[select, :]\n",
    "    return pairs_selected\n",
    "\n",
    "def sample_assort_mixmat(nodes, edges, attributes, sample_id=None ,n_shuffle=50, \n",
    "                         parallel='max', memory_limit='50GB'):\n",
    "    \"\"\"\n",
    "    Computed z-scored assortativity and mixing matrix elements for \n",
    "    a network of a single sample.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nodes : dataframe\n",
    "        Attributes of all nodes.\n",
    "    edges : dataframe\n",
    "        Edges between nodes given by their index.\n",
    "    attributes: list\n",
    "        Categorical attributes considered in the mixing matrix.\n",
    "    sample_id : str\n",
    "        Name of the analyzed sample.\n",
    "    n_shuffle : int (default=50)\n",
    "        Number of attributes permutations.\n",
    "    parallel : bool, int or str (default=\"max\")\n",
    "        How parallelization is performed.\n",
    "        If False, no parallelization is done.\n",
    "        If int, use this number of cores.\n",
    "        If 'max', use the maximum number of cores.\n",
    "        If 'max-1', use the max of cores minus 1.\n",
    "    memory_limit : str (default='50GB')\n",
    "        Dask memory limit for parallelization.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    sample_stats : dataframe\n",
    "        Network's statistics including total number of nodes, attributes proportions,\n",
    "        assortativity and mixing matrix elements, both raw and z-scored.\n",
    "    \"\"\"\n",
    "    \n",
    "    col_sample = (['id', '# total'] +\n",
    "                 ['% ' + x for x in attributes] +\n",
    "                 ['assort', 'assort MEAN', 'assort STD', 'assort Z'] +\n",
    "                 attributes_pairs(attributes, prefix='', medfix=' - ', suffix=' RAW') +\n",
    "                 attributes_pairs(attributes, prefix='', medfix=' - ', suffix=' MEAN') +\n",
    "                 attributes_pairs(attributes, prefix='', medfix=' - ', suffix=' STD') +\n",
    "                 attributes_pairs(attributes, prefix='', medfix=' - ', suffix=' Z'))\n",
    "    \n",
    "    if sample_id is None:\n",
    "        sample_id = 'None'\n",
    "    # Network statistics\n",
    "    mixmat = mixing_matrix(nodes, edges, attributes)\n",
    "    assort = attribute_ac(mixmat)\n",
    "\n",
    "    # ------ Randomization ------\n",
    "    print(f\"randomization\")\n",
    "    np.random.seed(0)\n",
    "    mixmat_rand, assort_rand = randomized_mixmat(nodes, edges, attributes, n_shuffle=n_shuffle, parallel=False)\n",
    "    mixmat_mean, mixmat_std, mixmat_zscore = zscore(mixmat, mixmat_rand, return_stats=True)\n",
    "    assort_mean, assort_std, assort_zscore = zscore(assort, assort_rand, return_stats=True)\n",
    "\n",
    "    # Reformat sample's network's statistics\n",
    "    nb_nodes = len(nodes)\n",
    "    sample_data = ([sample_id, nb_nodes] +\n",
    "                   [nodes[col].sum()/nb_nodes for col in attributes] +\n",
    "                   [assort, assort_mean, assort_std, assort_zscore] +\n",
    "                   mixmat_to_columns(mixmat) +\n",
    "                   mixmat_to_columns(mixmat_mean) +\n",
    "                   mixmat_to_columns(mixmat_std) +\n",
    "                   mixmat_to_columns(mixmat_zscore))\n",
    "    sample_stats = pd.DataFrame(data=sample_data, index=col_sample).T\n",
    "    return sample_stats\n",
    "\n",
    "def _select_nodes_edges_from_group(nodes, edges, group, groups):\n",
    "    \"\"\"\n",
    "    Select nodes and edges related to a given group of nodes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nodes : dataframe\n",
    "        Attributes of all nodes.\n",
    "    edges : dataframe\n",
    "        Edges between nodes given by their index.\n",
    "    group: int or str\n",
    "        Group of interest. \n",
    "    groups: pd.Series\n",
    "        Group identifier of each node. \n",
    "    \n",
    "    Returns\n",
    "    ------\n",
    "    nodes_sel : dataframe\n",
    "        Nodes belonging to the group.\n",
    "    edges_sel : dataframe\n",
    "        Edges belonging to the group.\n",
    "    \"\"\"\n",
    "    select = groups == group\n",
    "    nodes_sel = nodes.loc[select, :]\n",
    "    nodes_ids = np.where(select)[0]\n",
    "    edges_selector = select_pairs_from_coords(nodes_ids, edges.values, return_selector=True)\n",
    "    edges_sel = edges.loc[edges_selector, :]\n",
    "    return nodes_sel, edges_sel\n",
    "    \n",
    "def batch_assort_mixmat(nodes, edges, attributes, groups, n_shuffle=50,\n",
    "                        parallel='max', memory_limit='50GB',\n",
    "                        save_intermediate_results=False, dir_save_interm='~'):\n",
    "    \"\"\"\n",
    "    Computed z-scored assortativity and mixing matrix elements for all\n",
    "    samples in a batch, cohort or other kind of groups.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nodes : dataframe\n",
    "        Attributes of all nodes.\n",
    "    edges : dataframe\n",
    "        Edges between nodes given by their index.\n",
    "    attributes: list\n",
    "        Categorical attributes considered in the mixing matrix.\n",
    "    groups: pd.Series\n",
    "        Group identifier of each node. \n",
    "        It can be a patient or sample id, chromosome number, etc...\n",
    "    n_shuffle : int (default=50)\n",
    "        Number of attributes permutations.\n",
    "    parallel : bool, int or str (default=\"max\")\n",
    "        How parallelization is performed.\n",
    "        If False, no parallelization is done.\n",
    "        If int, use this number of cores.\n",
    "        If 'max', use the maximum number of cores.\n",
    "        If 'max-1', use the max of cores minus 1.\n",
    "    memory_limit : str (default='50GB')\n",
    "        Dask memory limit for parallelization.\n",
    "    save_intermediate_results : bool (default=False)\n",
    "        If True network statistics are saved for each group.\n",
    "    dir_save_interm : str (default='~')\n",
    "        Directory where intermediate group network statistics are saved.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    networks_stats : dataframe\n",
    "        Networks's statistics for all groups, including total number of nodes, \n",
    "        attributes proportions, assortativity and mixing matrix elements, \n",
    "        both raw and z-scored.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> nodes_high, edges_high = make_high_assort_net()\n",
    "    >>> nodes_low, edges_low = make_high_disassort_net()\n",
    "    >>> nodes = nodes_high.append(nodes_low, ignore_index=True)\n",
    "    >>> edges_low_shift = edges_low + nodes_high.shape[0]\n",
    "    >>> edges = edges_high.append(edges_low_shift)\n",
    "    >>> groups = pd.Series(['high'] * len(nodes_high) + ['low'] * len(nodes_low))\n",
    "    >>> net_stats = batch_assort_mixmat(nodes, edges, \n",
    "                                        attributes=['a', 'b', 'c'], \n",
    "                                        groups=groups, \n",
    "                                        parallel=False)\n",
    "\"\"\"\n",
    "    \n",
    "    if not isinstance(groups, pd.Series):\n",
    "        groups = pd.Series(groups).copy()\n",
    "    \n",
    "    groups_data = []\n",
    " \n",
    "    if parallel is False:\n",
    "        for group in tqdm(groups.unique(), desc='group'):\n",
    "            # select nodes and edges of a specific group\n",
    "            nodes_sel, edges_sel = _select_nodes_edges_from_group(nodes, edges, group, groups)\n",
    "            # compute network statistics\n",
    "            group_data = sample_assort_mixmat(nodes_sel, edges_sel, attributes, sample_id=group, \n",
    "                                              n_shuffle=n_shuffle, parallel=parallel, memory_limit=memory_limit)\n",
    "            if save_intermediate_results:\n",
    "                group_data.to_csv(os.path.join(dir_save_interm, 'network_statistics_group_{}.csv'.format(group)), \n",
    "                                  encoding='utf-8', \n",
    "                                  index=False)\n",
    "            groups_data.append(group_data)\n",
    "        networks_stats = pd.concat(groups_data, axis=0)\n",
    "    else:\n",
    "        from multiprocessing import cpu_count\n",
    "        from dask.distributed import Client, LocalCluster\n",
    "        from dask import delayed\n",
    "        \n",
    "        # select the right number of cores\n",
    "        nb_cores = cpu_count()\n",
    "        if isinstance(parallel, int):\n",
    "            use_cores = min(parallel, nb_cores)\n",
    "        elif parallel == 'max-1':\n",
    "            use_cores = nb_cores - 1\n",
    "        elif parallel == 'max':\n",
    "            use_cores = nb_cores\n",
    "        # set up cluster and workers\n",
    "        cluster = LocalCluster(n_workers=use_cores, \n",
    "                               threads_per_worker=1,\n",
    "                               memory_limit=memory_limit)\n",
    "        client = Client(cluster)\n",
    "        \n",
    "        for group in groups.unique():\n",
    "            # select nodes and edges of a specific group\n",
    "            nodes_edges_sel = delayed(_select_nodes_edges_from_group)(nodes, edges, group, groups)\n",
    "            # individual samples z-score stats are not parallelized over shuffling rounds\n",
    "            # because parallelization is already done over samples\n",
    "            group_data = delayed(sample_assort_mixmat)(nodes_edges_sel[0], nodes_edges_sel[1], attributes, sample_id=group, \n",
    "                                                       n_shuffle=n_shuffle, parallel=False) \n",
    "            groups_data.append(group_data)\n",
    "        # evaluate the parallel computation\n",
    "        networks_stats = delayed(pd.concat)(groups_data, axis=0, ignore_index=True).compute()\n",
    "    return networks_stats\n",
    "    \n",
    "############ Neighbors Aggegation Statistics ############\n",
    "\n",
    "def neighbors(pairs, n):\n",
    "    \"\"\"\n",
    "    Return the list of neighbors of a node in a network defined \n",
    "    by edges between pairs of nodes. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pairs : array_like\n",
    "        Pairs of nodes' id that define the network's edges.\n",
    "    n : int\n",
    "        The node for which we look for the neighbors.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    neigh : array_like\n",
    "        The indices of neighboring nodes.\n",
    "    \"\"\"\n",
    "    \n",
    "    left_neigh = pairs[pairs[:,1] == n, 0]\n",
    "    right_neigh = pairs[pairs[:,0] == n, 1]\n",
    "    neigh = np.hstack( (left_neigh, right_neigh) ).flatten()\n",
    "    \n",
    "    return neigh\n",
    "\n",
    "def neighbors_k_order(pairs, n, order):\n",
    "    \"\"\"\n",
    "    Return the list of up the kth neighbors of a node \n",
    "    in a network defined by edges between pairs of nodes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pairs : array_like\n",
    "        Pairs of nodes' id that define the network's edges.\n",
    "    n : int\n",
    "        The node for which we look for the neighbors.\n",
    "    order : int\n",
    "        Max order of neighbors.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    all_neigh : list\n",
    "        The list of lists of 1D array neighbor and the corresponding order\n",
    "    \n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> pairs = np.array([[0, 10],\n",
    "                        [0, 20],\n",
    "                        [0, 30],\n",
    "                        [10, 110],\n",
    "                        [10, 210],\n",
    "                        [10, 310],\n",
    "                        [20, 120],\n",
    "                        [20, 220],\n",
    "                        [20, 320],\n",
    "                        [30, 130],\n",
    "                        [30, 230],\n",
    "                        [30, 330],\n",
    "                        [10, 20],\n",
    "                        [20, 30],\n",
    "                        [30, 10],\n",
    "                        [310, 120],\n",
    "                        [320, 130],\n",
    "                        [330, 110]])\n",
    "    >>> neighbors_k_order(pairs, 0, 2)\n",
    "    [[array([0]), 0],\n",
    "     [array([10, 20, 30]), 1],\n",
    "     [array([110, 120, 130, 210, 220, 230, 310, 320, 330]), 2]]\n",
    "    \"\"\"\n",
    "    \n",
    "    # all_neigh stores all the unique neighbors and their oder\n",
    "    all_neigh = [[np.array([n]), 0]]\n",
    "    unique_neigh = np.array([n])\n",
    "    \n",
    "    for k in range(order):\n",
    "        # detected neighbor nodes at the previous order\n",
    "        last_neigh = all_neigh[k][0]\n",
    "        k_neigh = []\n",
    "        for node in last_neigh:\n",
    "            # aggregate arrays of neighbors for each previous order neighbor\n",
    "            neigh = np.unique(neighbors(pairs, node))\n",
    "            k_neigh.append(neigh)\n",
    "        # aggregate all unique kth order neighbors\n",
    "        if len(k_neigh) > 0:\n",
    "            k_unique_neigh = np.unique(np.concatenate(k_neigh, axis=0))\n",
    "            # select the kth order neighbors that have never been detected in previous orders\n",
    "            keep_neigh = np.in1d(k_unique_neigh, unique_neigh, invert=True)\n",
    "            k_unique_neigh = k_unique_neigh[keep_neigh]\n",
    "            # register the kth order unique neighbors along with their order\n",
    "            all_neigh.append([k_unique_neigh, k+1])\n",
    "            # update array of unique detected neighbors\n",
    "            unique_neigh = np.concatenate([unique_neigh, k_unique_neigh], axis=0)\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    return all_neigh\n",
    "\n",
    "def flatten_neighbors(all_neigh):\n",
    "    \"\"\"\n",
    "    Convert the list of neighbors 1D arrays with their order into\n",
    "    a single 1D array of neighbors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    all_neigh : list\n",
    "        The list of lists of 1D array neighbor and the corresponding order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    flat_neigh : array_like\n",
    "        The indices of neighboring nodes.\n",
    "        \n",
    "    Examples\n",
    "    --------\n",
    "    >>> all_neigh = [[np.array([0]), 0],\n",
    "                     [np.array([10, 20, 30]), 1],\n",
    "                     [np.array([110, 120, 130, 210, 220, 230, 310, 320, 330]), 2]]\n",
    "    >>> flatten_neighbors(all_neigh)\n",
    "    array([  0,  10,  20,  30, 110, 120, 130, 210, 220, 230, 310, 320, 330])\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    For future features it should return a 2D array of\n",
    "    nodes and their respective order.\n",
    "    \"\"\"\n",
    "    \n",
    "    list_neigh = []\n",
    "    for neigh, order in all_neigh:\n",
    "        list_neigh.append(neigh)\n",
    "    flat_neigh = np.concatenate(list_neigh, axis=0)\n",
    "\n",
    "    return flat_neigh\n",
    "\n",
    "def aggregate_k_neighbors(X, pairs, order=1, var_names=None, stat_funcs='default', stat_names='default', var_sep=' '):\n",
    "    \"\"\"\n",
    "    Compute the statistics on aggregated variables across\n",
    "    the k order neighbors of each node in a network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The data on which to compute statistics (mean, std, ...).\n",
    "    pairs : array_like\n",
    "        Pairs of nodes' id that define the network's edges.\n",
    "    order : int\n",
    "        Max order of neighbors.\n",
    "    var_names : list\n",
    "        Names of variables of X.\n",
    "    stat_funcs : str or list of functions\n",
    "        Statistics functions to use on aggregated data. If 'default' np.mean and np.std are use.\n",
    "        All functions are used with the `axis=0` argument.\n",
    "    stat_names : str or list of str\n",
    "        Names of the statistical functions used on aggregated data.\n",
    "        If 'default' 'mean' and 'std' are used.\n",
    "    var_sep : str\n",
    "        Separation between variables names and statistical functions names\n",
    "        Default is ' '.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    aggreg : dataframe\n",
    "        Neighbors Aggregation Statistics of X.\n",
    "        \n",
    "    Examples\n",
    "    --------\n",
    "    >>> x = np.arange(5)\n",
    "    >>> X = x[np.newaxis,:] + x[:,np.newaxis] * 10\n",
    "    >>> pairs = np.array([[0, 1],\n",
    "                          [2, 3],\n",
    "                          [3, 4]])\n",
    "    >>> aggreg = aggregate_k_neighbors(X, pairs, stat_funcs=[np.mean, np.max], stat_names=['mean', 'max'], var_sep=' - ')\n",
    "    >>> aggreg.values\n",
    "    array([[ 5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14.],\n",
    "           [ 5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14.],\n",
    "           [25., 26., 27., 28., 29., 30., 31., 32., 33., 34.],\n",
    "           [30., 31., 32., 33., 34., 40., 41., 42., 43., 44.],\n",
    "           [35., 36., 37., 38., 39., 40., 41., 42., 43., 44.]])\n",
    "    \"\"\"\n",
    "    \n",
    "    nb_obs = X.shape[0]\n",
    "    nb_var = X.shape[1]\n",
    "    if stat_funcs == 'default':\n",
    "        stat_funcs = [np.mean, np.std]\n",
    "        if stat_names == 'default':\n",
    "            stat_names = ['mean', 'std']\n",
    "    nb_funcs = len(stat_funcs)\n",
    "    aggreg = np.zeros((nb_obs, nb_var*nb_funcs))\n",
    "\n",
    "    for i in range(nb_obs):\n",
    "        all_neigh = neighbors_k_order(pairs, n=i, order=order)\n",
    "        neigh = flatten_neighbors(all_neigh)\n",
    "        for j, (stat_func, stat_name) in enumerate(zip(stat_funcs, stat_names)):\n",
    "            aggreg[i, j*nb_var : (j+1)*nb_var] = stat_func(X[neigh,:], axis=0)\n",
    "    \n",
    "    if var_names is None:\n",
    "        var_names = [str(i) for i in range(nb_var)]\n",
    "    columns = []\n",
    "    for stat_name in stat_names:\n",
    "        stat_str = var_sep + stat_name\n",
    "        columns = columns + [var + stat_str for var in var_names]\n",
    "    aggreg = pd.DataFrame(data=aggreg, columns=columns)\n",
    "    \n",
    "    return aggreg\n",
    "\n",
    "def make_cluster_cmap(labels, grey_pos='start'):\n",
    "    \"\"\"\n",
    "    Creates an appropriate colormap for a vector of cluster labels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : array_like\n",
    "        The labels of multiple clustered points\n",
    "    grey_pos: str\n",
    "        Where to put the grey color for the noise\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    cmap : matplotlib colormap object\n",
    "        A correct colormap\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> my_cmap = make_cluster_cmap(labels=np.array([-1,3,5,2,4,1,3,-1,4,2,5]))\n",
    "    \"\"\"\n",
    "    \n",
    "    from matplotlib.colors import ListedColormap\n",
    "    \n",
    "    if labels.max() < 9:\n",
    "        cmap = list(plt.get_cmap('tab10').colors)\n",
    "        if grey_pos == 'end':\n",
    "            cmap.append(cmap.pop(-3))\n",
    "        elif grey_pos == 'start':\n",
    "            cmap = [cmap.pop(-3)] + cmap\n",
    "        elif grey_pos == 'del':\n",
    "            del cmap[-3]\n",
    "    else:\n",
    "        cmap = list(plt.get_cmap('tab20').colors)\n",
    "        if grey_pos == 'end':\n",
    "            cmap.append(cmap.pop(-6))\n",
    "            cmap.append(cmap.pop(-6))\n",
    "        elif grey_pos == 'start':\n",
    "            cmap = [cmap.pop(-5)] + cmap\n",
    "            cmap = [cmap.pop(-5)] + cmap\n",
    "        elif grey_pos == 'del':\n",
    "            del cmap[-5]\n",
    "            del cmap[-5]\n",
    "    cmap = ListedColormap(cmap)\n",
    "    \n",
    "    return cmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/mouneem/PNETS-NETS/Data/'\n",
    "\n",
    "data_files = [f for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "mixmat_files = [f for f in data_files if re.match(r'mixmatLayer*', f)]\n",
    "mixmat_c1 = [f for f in mixmat_files if 'C1v1' in f ]\n",
    "mixmat_l1_c1 = [f for f in mixmat_c1 if 'Layer 1' in f ]\n",
    "\n",
    "mixmat_l1_c1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrib = pd.read_csv(data_path + 'mosna_nodesLayer 1NVA_21-003.IMMCORE.C1v1_18T044196-05-Z1-ImvessC1-4518.czi_Layer 1.csv')\n",
    "matrix = pd.read_csv(data_path + 'mixmatLayer 1NVA_21-003.IMMCORE.C1v1_18T044196-05-Z1-ImvessC1-4518.czi_Layer 1.csv')\n",
    "pairs = pd.read_csv(data_path + 'pairsLayer 1NVA_21-003.IMMCORE.C1v1_18T044196-05-Z1-ImvessC1-4518.czi_Layer 1.csv', usecols=[1,2]).values\n",
    "#print(pairs.head())\n",
    "markers = [\n",
    "    'Vimentin', 'SMA', 'B7H3', 'FoxP3', 'Lag3',\n",
    "    'CD4', 'CD16', 'CD56', 'OX40', 'PD1', 'CD31', 'PD-L1', 'EGFR', 'Ki67',\n",
    "    'CD209', 'CD11c', 'CD138', 'CD163', 'CD68', 'CSF-1R', 'CD8', 'CD3',\n",
    "    'IDO', 'Keratin17', 'CD63', 'CD45RO', 'CD20', 'p53', 'Beta catenin',\n",
    "    'HLA-DR', 'CD11b', 'CD45', 'H3K9ac', 'Pan-Keratin', 'H3K27me3',\n",
    "    'phospho-S6', 'MPO', 'Keratin6', 'HLA_Class_1',\n",
    "    'M1','M2','Other','TAM',\n",
    "    'C1','C2','C3','C4','C5','C6'\n",
    "]\n",
    "\n",
    "markers = [ 'M1','M2','Other','TAM',]\n",
    "\n",
    "X = attrib[markers].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_aggreg.to_csv( data_path + 'var_aggreg ' + 'NVA_21-003.IMMCORE.C1v1_18T044196-05-Z1-ImvessC1-4518.czi_Layer 1.csv', index=False)\n",
    "\n",
    "marker = '.'\n",
    "size_points = 10\n",
    "\n",
    "reducer = umap.UMAP(random_state=0)\n",
    "embedding = reducer.fit_transform(var_aggreg)\n",
    "embedding.shape\n",
    "\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], c='royalblue', marker=marker, s=size_points)\n",
    "title = \"Aggregated neighbors' data on all samples\"\n",
    "plt.title(title, fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/mouneem/PNETS-NETS/Data/'\n",
    "figs_path = '/home/mouneem/PNETS-NETS/Data/figs/'\n",
    "data_files = [f for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "save_dir = Path('/home/mouneem/PNETS-NETS/Data/')\n",
    "\n",
    "data_files[2]\n",
    "\n",
    "All_data_files = {\n",
    "    '1':[],\n",
    "    '2':[],\n",
    "    '3':[],\n",
    "    '4':[],\n",
    "    '5':[],\n",
    "    '6':[],\n",
    "    }\n",
    "\n",
    "for file in data_files:\n",
    "    filename = \" \".join( file.split(\" \")[1:] )\n",
    "    if len(filename)> 0:\n",
    "        layer = filename[0]\n",
    "        All_data_files[layer].append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shuffle = 20\n",
    "\n",
    "\n",
    "uniq_att = [ 'M1','M2','Other','TAM',]\n",
    "\n",
    "cohort = []\n",
    "\n",
    "col_cohort = (\n",
    "    ['patient', '# total'] +\n",
    "    ['% ' + x for x in uniq_att] +\n",
    "    ['assort', 'assort MEAN', 'assort STD', 'assort Z'] +\n",
    "    mosna.attributes_pairs(uniq_att, prefix='', medfix=' - ', suffix='') +\n",
    "    mosna.attributes_pairs(uniq_att, prefix='', medfix=' - ', suffix=' MEAN') +\n",
    "    mosna.attributes_pairs(uniq_att, prefix='', medfix=' - ', suffix=' STD') +\n",
    "    mosna.attributes_pairs(uniq_att, prefix='', medfix=' - ', suffix=' Z') \n",
    ")\n",
    "\n",
    "dt_string = now.strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "\n",
    "\n",
    "for layer in All_data_files:\n",
    "    for file in All_data_files[layer]:\n",
    "        if 'C1v1' in file:\n",
    "            print(file)\n",
    "            nodes = pd.read_csv(data_path + 'nodesLayer '+file)\n",
    "            if not 'M1' in nodes:\n",
    "                nodes['M1'] = 0\n",
    "            if not 'M2' in nodes:\n",
    "                nodes['M2'] = 0\n",
    "            if not 'TAM' in nodes:\n",
    "                nodes['TAM'] = 0\n",
    "            pairs = pd.read_csv(data_path + 'pairsLayer '+file)\n",
    "            pairs.columns = ['i','source', 'target']\n",
    "            # mixmat = pd.read_csv(data_path + 'mixmatLayer '+file)\n",
    "            mosna_nodes = pd.read_csv(data_path + 'mosna_nodesLayer '+file)\n",
    "            mixmat_zscore = pd.read_csv(data_path + 'mixmat_zscoreLayer '+file)\n",
    "            ## Assortativity\n",
    "\n",
    "            # Network statistics\n",
    "            coords = nodes.loc[:,['x','y']].values\n",
    "            pairs = ty.build_delaunay(coords)\n",
    "\n",
    "            edges = pd.DataFrame(data=pairs, columns=['source', 'target'])\n",
    "            attributes = mosna_nodes['Group'].unique()\n",
    "\n",
    "            attributes = uniq_att\n",
    "            mixmat = mixing_matrix(nodes, edges, attributes)\n",
    "            assort = attribute_ac(mixmat)\n",
    "\n",
    "            print(\"Assortativity by cell types: {}\".format(assort))\n",
    "\n",
    "            # ------ Randomization ------\n",
    "            mixmat_rand, assort_rand = randomized_mixmat(nodes, edges, attributes, n_shuffle=n_shuffle, parallel=False)\n",
    "            mixmat_mean, mixmat_std, mixmat_zscore = zscore(mixmat, mixmat_rand, return_stats=True)\n",
    "            assort_mean, assort_std, assort_zscore = zscore(assort, assort_rand, return_stats=True)\n",
    "            mixmat = mixmat_to_df(mixmat, attributes)\n",
    "            mixmat_zscore = mixmat_to_df(mixmat_zscore, attributes)\n",
    "            f, ax = plt.subplots(figsize=(9, 6))\n",
    "            sns.heatmap(mixmat, center=0, cmap=\"vlag\", annot=True, linewidths=.5, ax=ax)\n",
    "            plt.savefig(figs_path + f\"assortativity_sample-{file}\"+'.png', bbox_inches='tight', facecolor='white')\n",
    "\n",
    "                # Save results\n",
    "            n_cells = nodes.shape[0]\n",
    "            sample_data = (\n",
    "                [file, n_cells] +\n",
    "                [nodes[col].sum()/n_cells for col in uniq_att] +\n",
    "                [assort, assort_mean, assort_std, assort_zscore] +\n",
    "                mosna.mixmat_to_columns(mixmat.values) +\n",
    "                mosna.mixmat_to_columns(mixmat_mean) +\n",
    "                mosna.mixmat_to_columns(mixmat_std) +\n",
    "                mosna.mixmat_to_columns(mixmat_zscore.values)\n",
    "            )\n",
    "            cohort.append(sample_data)\n",
    "            # save each sample analysis to be robust to VPN timeouts\n",
    "            sample_data = pd.DataFrame(data=[sample_data], columns=col_cohort)\n",
    "            sample_data.to_csv(save_dir.joinpath(f\"sample_{file}_network_analysis_{dt_string}.csv\"), encoding='utf-8', index=False)\n",
    "\n",
    "\n",
    "cohort = pd.DataFrame(cohort, columns=col_cohort)\n",
    "cohort.to_csv(save_dir.joinpath('all_samples_network_analysis_'+dt_string+'.csv'), encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorized mixing matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mosna_path = data_path\n",
    "\n",
    "data_files = [f for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "save_dir = Path('/home/mouneem/PNETS-NETS/Data/')\n",
    "\n",
    "layer = 'Layer 2'\n",
    "combo = 'C1v1'\n",
    "mosnas = [ f  for f in listdir(data_path) if isfile(join(data_path, f))  and combo in f and layer in f  and 'mixmatL' in f  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FISRT SAMPLE\n",
    "\n",
    "\n",
    "mosnafile = mosnas[0]\n",
    "mixmat = pd.read_csv(data_path + mosnafile ,index_col=0 )\n",
    "keep = np.triu(np.ones(mixmat.shape)).astype('bool').reshape(mixmat.size)\n",
    "MAT = pd.DataFrame(mixmat.stack())\n",
    "MAT.to_csv(layer + combo + 'out.csv')\n",
    "MAT = pd.read_csv(layer + combo + 'out.csv')\n",
    "\n",
    "MAT.columns = ['X','Y','Value']\n",
    "if combo == 'C1v1':\n",
    "    di = {'M1': \"M1\", \"M2\": 'M2', 'TAM' : 'TAM', 'Other':\"Other\"}\n",
    "else:\n",
    "    di = {'C1': \"Cancer\", \"C2\": 'CD8 T-Cell', 'C3' : 'CD4 T-Cell', 'C4':'B Cell', 'Other':\"Other\",'C5':'CD3+CD20+' }\n",
    "\n",
    "MAT = MAT.replace( {\"Y\": di })\n",
    "MAT = MAT.replace( {\"X\": di })\n",
    "\n",
    "MAT[\"comb\"] = MAT[\"X\"].astype(str) + \" / \" + MAT[\"Y\"].astype(str)\n",
    "\n",
    "MAT[\"Value\"]=(MAT[\"Value\"]-MAT[\"Value\"].min())/(MAT[\"Value\"].max()-MAT[\"Value\"].min())\n",
    "file = mosnafile.replace('mixmat'+layer,'').replace('.csv','')\n",
    "print(file)\n",
    "MAT['sample'] = file\n",
    "FullMatrix = MAT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mosnafile in mosnas:\n",
    "    mixmat = pd.read_csv(data_path + mosnafile ,index_col=0 )\n",
    "    keep = np.triu(np.ones(mixmat.shape)).astype('bool').reshape(mixmat.size)\n",
    "    MAT = pd.DataFrame(mixmat.stack())\n",
    "    MAT.to_csv(layer + combo + 'out.csv')\n",
    "    MAT = pd.read_csv(layer + combo + 'out.csv')\n",
    "\n",
    "    MAT.columns = ['X','Y','Value']\n",
    "    if combo == 'C1v1':\n",
    "        di = {'M1': \"M1\", \"M2\": 'M2', 'TAM' : 'TAM', 'Other':\"Other\"}\n",
    "    else:\n",
    "        di = {'C1': \"Cancer\", \"C2\": 'CD8 T-Cell', 'C3' : 'CD4 T-Cell', 'C4':'B Cell', 'Other':\"Other\",'C5':'CD3+CD20+' }\n",
    "\n",
    "    MAT = MAT.replace( {\"Y\": di })\n",
    "    MAT = MAT.replace( {\"X\": di })\n",
    "\n",
    "    MAT[\"comb\"] = MAT[\"X\"].astype(str) + \" / \" + MAT[\"Y\"].astype(str)\n",
    "\n",
    "    MAT[\"Value\"]=(MAT[\"Value\"]-MAT[\"Value\"].min())/(MAT[\"Value\"].max()-MAT[\"Value\"].min())\n",
    "    file = mosnafile.replace('mixmat'+layer,'').replace('.csv','')\n",
    "    print(file)\n",
    "    MAT['sample'] = file\n",
    "    FullMatrix = FullMatrix.append(pd.DataFrame(data = MAT))\n",
    "\n",
    "\n",
    "    print(mosnafile)\n",
    "\n",
    "\n",
    "FullMatrix.to_csv('FullMatrix.csv')\n",
    "\n",
    "FullMatrix = FullMatrix[['Value', 'comb', 'sample']]\n",
    "print(FullMatrix.columns)\n",
    "FullMatrix2 = FullMatrix[ FullMatrix['comb'].isin(['Other / Other' , 'Other / CD8 T-Cell' , 'Other / CD4 T-Cell', 'Other / Cancer',\n",
    "                                                    'CD8 T-Cell / CD8 T-Cell' , 'CD8 T-Cell / CD4 T-Cell', 'CD8 T-Cell / Cancer',\n",
    "                                                    'CD4 T-Cell / CD4 T-Cell', 'CD4 T-Cell / Cancer',\n",
    "                                                    'Cancer / Cancer', \n",
    "                                                    'M1 / Other' , 'M1 / TAM', 'M1 / M2', 'M1 / M1',\n",
    "                                                    'Other / M2' , 'Other / TAM', 'Other / Other'\n",
    "                                                    'TAM / M2', 'TAM / TAM', \n",
    "                                                    'M2 / M2'\n",
    "                                                    ]) ]\n",
    "\n",
    "FullMatrix2.index = FullMatrix2[['sample']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Matrix = FullMatrix2.pivot_table(index=[\"sample\"], \n",
    "                    columns='comb', \n",
    "                    values='Value')\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "sns.heatmap(Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "Matrix2 = Matrix\n",
    "Matrix2.index = Matrix2.iloc[:,0]\n",
    "Matrix2 = Matrix2.iloc[:,1:]\n",
    "print(Matrix2.shape)\n",
    "print(Matrix2.head)\n",
    "\n",
    "sns.clustermap(Matrix2, yticklabels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vect. Matrix Multilayers Multi-comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mosna_path = data_path\n",
    "\n",
    "data_files = [f for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "save_dir = Path('/home/mouneem/PNETS-NETS/Data/')\n",
    "\n",
    "layers =[ \"Layer 1\", \"Layer 2\", \"Layer 3\", \"Layer 4\", \"Layer 5\"]\n",
    "combos = ['C1v1' , 'C2v1']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for combo in combos:\n",
    "    for layer in layers:\n",
    "        mosnas = [ f  for f in listdir(data_path) if isfile(join(data_path, f))  and combo in f and layer in f  and 'mixmatL' in f  ]\n",
    "                \n",
    "        mosnafile = mosnas[0]\n",
    "        mixmat = pd.read_csv(data_path + mosnafile ,index_col=0 )\n",
    "        keep = np.triu(np.ones(mixmat.shape)).astype('bool').reshape(mixmat.size)\n",
    "        MAT = pd.DataFrame(mixmat.stack())\n",
    "        MAT.to_csv(layer + combo + 'out.csv')\n",
    "        MAT = pd.read_csv(layer + combo + 'out.csv')\n",
    "\n",
    "        MAT.columns = ['X','Y','Value']\n",
    "        if combo == 'C1v1':\n",
    "            di = {'M1': \"M1\", \"M2\": 'M2', 'TAM' : 'TAM', 'Other':\"Other\"}\n",
    "        else:\n",
    "            di = {'C1': \"Cancer\", \"C2\": 'CD8 T-Cell', 'C3' : 'CD4 T-Cell', 'C4':'B Cell', 'Other':\"Other\",'C5':'CD3+CD20+' }\n",
    "\n",
    "        MAT = MAT.replace( {\"Y\": di })\n",
    "        MAT = MAT.replace( {\"X\": di })\n",
    "\n",
    "        MAT[\"comb\"] = MAT[\"X\"].astype(str) + \" / \" + MAT[\"Y\"].astype(str)\n",
    "\n",
    "        MAT[\"Value\"]=(MAT[\"Value\"]-MAT[\"Value\"].min())/(MAT[\"Value\"].max()-MAT[\"Value\"].min())\n",
    "        file = mosnafile.replace('mixmat'+layer,'').replace('.csv','')\n",
    "        MAT['sample'] = file\n",
    "        FullMatrix = MAT\n",
    "\n",
    "        for mosnafile in mosnas:\n",
    "            mixmat = pd.read_csv(data_path + mosnafile ,index_col=0 )\n",
    "            keep = np.triu(np.ones(mixmat.shape)).astype('bool').reshape(mixmat.size)\n",
    "            MAT = pd.DataFrame(mixmat.stack())\n",
    "            MAT.to_csv(layer + combo + 'out.csv')\n",
    "            MAT = pd.read_csv(layer + combo + 'out.csv')\n",
    "\n",
    "            MAT.columns = ['X','Y','Value']\n",
    "            if combo == 'C1v1':\n",
    "                di = {'M1': \"M1\", \"M2\": 'M2', 'TAM' : 'TAM', 'Other':\"Other\"}\n",
    "            else:\n",
    "                di = {'C1': \"Cancer\", \"C2\": 'CD8 T-Cell', 'C3' : 'CD4 T-Cell', 'C4':'B Cell', 'Other':\"Other\",'C5':'CD3+CD20+' }\n",
    "\n",
    "            MAT = MAT.replace( {\"Y\": di })\n",
    "            MAT = MAT.replace( {\"X\": di })\n",
    "\n",
    "            MAT[\"comb\"] = MAT[\"X\"].astype(str) + \" / \" + MAT[\"Y\"].astype(str)\n",
    "\n",
    "            MAT[\"Value\"]=(MAT[\"Value\"]-MAT[\"Value\"].min())/(MAT[\"Value\"].max()-MAT[\"Value\"].min())\n",
    "            file = mosnafile.replace('mixmat'+layer,'').replace('.csv','')\n",
    "            MAT['sample'] = file\n",
    "            FullMatrix = FullMatrix.append(pd.DataFrame(data = MAT))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        FullMatrix.to_csv(layer + combo + '_FullMatrix.csv')\n",
    "\n",
    "        FullMatrix = FullMatrix[['Value', 'comb', 'sample']]\n",
    "        FullMatrix2 = FullMatrix[ FullMatrix['comb'].isin(['Other / Other' , 'Other / CD8 T-Cell' , 'Other / CD4 T-Cell', 'Other / Cancer',\n",
    "                                                            'CD8 T-Cell / CD8 T-Cell' , 'CD8 T-Cell / CD4 T-Cell', 'CD8 T-Cell / Cancer',\n",
    "                                                            'CD4 T-Cell / CD4 T-Cell', 'CD4 T-Cell / Cancer',\n",
    "                                                            'Cancer / Cancer', \n",
    "                                                            'M1 / Other' , 'M1 / TAM', 'M1 / M2', 'M1 / M1',\n",
    "                                                            'Other / M2' , 'Other / TAM', 'Other / Other'\n",
    "                                                            'TAM / M2', 'TAM / TAM', \n",
    "                                                            'M2 / M2'\n",
    "                                                            ]) ]\n",
    "\n",
    "        FullMatrix2.index = FullMatrix2[['sample']]\n",
    "\n",
    "        Matrix = FullMatrix2.pivot_table(index=[\"sample\"], \n",
    "                            columns='comb', \n",
    "                            values='Value')\n",
    "\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(20, 5))\n",
    "\n",
    "        sns.heatmap(Matrix)\n",
    "        plt.figure(figsize=(20, 5))\n",
    "\n",
    "        Matrix2 = Matrix\n",
    "        Matrix2.index = Matrix2.iloc[:,0]\n",
    "        Matrix2 = Matrix2.iloc[:,1:]\n",
    "\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(20, 5))\n",
    "\n",
    "        sns.heatmap(Matrix)\n",
    "        #plt.savefig( data_path + 'figs/cluster/' +  layer + combo + 'heatmap.png')\n",
    "        plt.clf()\n",
    "        Matrix2 = Matrix\n",
    "        Matrix2.index = Matrix2.iloc[:,0]\n",
    "        Matrix2 = Matrix2.iloc[:,1:]\n",
    "        print(Matrix2)\n",
    "        Matrix2 = Matrix2.dropna(how='any')    #to drop if any value in the row has a nan\n",
    "        # Matrix2 = Matrix2.dropna(how='all')    #to drop if all values in the row are nan\n",
    "        sns.clustermap(Matrix2, yticklabels=False)\n",
    "        try:\n",
    "            plt.savefig( data_path + 'figs/cluster/' +  layer + combo + 'clustermap2.png')\n",
    "\n",
    "        except:\n",
    "            print(combo , layer)\n",
    "     \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36be1d41563bd3663c41850d6a635654ce06e8923be3538928e3f1a2a7aabf0a"
  },
  "kernelspec": {
   "display_name": "spatial-networks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
