{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 3 [15% of your grade, 70 points in total]\n",
    "\n",
    "Hi! Welcome to assignment 3. Here, we are going to build a simple automatic speech recognition (ASR) system using the SpeechBrain framework, and check your understanding of some important concepts related to ASR. This assignment constitutes 15% of your final grade.\n",
    "\n",
    "You are required to:\n",
    "- Finish this notebook. Successfully run all the code cells and answer all the questions.\n",
    "- When you need to embed screenshot in the notebook, put the picture in './resources'.\n",
    "\n",
    "**Submission**\n",
    "After finishing, **zip the whole assignment directory (but please exclude \"datasets\" directory)**, then submit to Canvas. **Naming: \"eXXXXXXX_Name_Assignment3.zip\"**.\n",
    "\n",
    "**Late Policy**\n",
    "Please submit before **Wednesday, Recess Week, 27 September 2023, 23:59**. For each late day, your will get -25% marks.\n",
    "\n",
    "**Honor Code**\n",
    "Note that plagiarism will not be condoned. You may discuss the questions with your classmates or search on the internet for references, but you MUST NOT submit your code/answers that is copied directly from other sources. If you referred to the code or tutorial somewhere, please explicitly attribute the source somewhere in your code, e.g., in the comment.\n",
    "\n",
    "**Note** You might need to restart the jupyter kernel to clear the imported py files before running some code cells.\n",
    "\n",
    "**Useful Resources**\n",
    "- (Paper) [Recent Advances in End-to-End Automatic Speech Recognition](https://arxiv.org/abs/2111.01690)\n",
    "- (Code) [SpeechBrain ASR from Scratch](https://colab.research.google.com/drive/1aFgzrUv3udM_gNJNUoLaHIm78QHtxdIz?usp=sharing#scrollTo=IVCCe6cXPzJ0)\n",
    "- (Video) [End-to-End Models for Speech Processing](https://www.youtube.com/watch?v=3MjIkWxXigM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "We will continue using the same conda environment as the assignment 2, but some additional packages are needed.\n",
    "1. Enter the conda environment by:\n",
    "\n",
    "        conda activate 4347\n",
    "2. Install packages\n",
    "\n",
    "        # Install SpeechBrain and other libraries\n",
    "        pip install -r requirements.txt\n",
    "\n",
    "        # Install CMU Dictionary\n",
    "        python\n",
    "        nltk.download('cmudict')\n",
    "        exit()\n",
    "\n",
    "3. When you run this notebook in your IDE, switch the interpreter to the 4347 conda environment.\n",
    "4. You may be prompted to install the jupyter package. Click \"confirm\" in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Section 1 - Automatic Speech Recognition (ASR) [28 mark(s)]\n",
    "An automatic speech ASR system recognize spoken words from audio. If we build it using singing data, it becomes a lyric transcription system. As you have learned in the lecture, in recent decades, the performance of ASR systems has advanced significantly thanks to end-to-end (E2E) ASR models and large-scale open-source datasets.\n",
    "\n",
    "We are not going to build a well-performed E2E ASR system in this assignment because it's too demanding for both computation resources and scale of data. Instead, we will\n",
    "- Use phoneme as the recognition unit. In English, they have tighter relationship with the pronunciation, hence is less data-demanding.\n",
    "- Use a simple model with a toy dataset.\n",
    "- Train the model from scratch.\n",
    "- Decode the output without language model.\n",
    "\n",
    "This is just for simplicity and let you know the general idea of ASR system and SpeechBrain framework, but not what we do to solve real-world problems. For current state-of-the-art ASR systems, they tend to\n",
    "- Use grapheme (e.g., character, word, sub-word) as the recognition unit. This make the recognition workflow simpler.\n",
    "- Use huge models with huge datasets.\n",
    "- Transfer learning is commonly adopted -- systems are first trained with large-scale corpus from various domains, or even unlabeled data (audio-only, no text annotation), and then fine-tuned with some domain-specific labeled data.\n",
    "- Language models participate in the decoding process, making the output with higher fluency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since we will be using phoneme as the target for the dataset, our goal is to recognize a sequence of spoken phonemes from audio. But many speech dataset do not provide phoneme annotation (as in this assignment). So we need to obtain the phoneme sequence from sentences ourselves.\n",
    "\n",
    "### Task 1: Prepare phoneme annotation  [4 mark(s)]\n",
    "1. Please finish the code of PhonemeUtil Class in utils.py, so that you can pass the below tests. Please using the CMU Dictionary in nltk to obtain the pronunciation. Use the first pronunciation if multiple ones exists for a word. If a word is not in the dictionary, mark its phoneme as \"\\<UNK\\>\".  **[2 mark(s)]**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-09-18T04:43:51.103667500Z",
     "start_time": "2023-09-18T04:43:49.724287400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations!\n"
     ]
    }
   ],
   "source": [
    "#NOTE TO TA: Ensure nltk.download('punkt') is also done in your machine along with nltk.download('cmudict')\n",
    "from utils import *\n",
    "phoneme_util = PhonemeUtil()\n",
    "sentences = [\n",
    "    \"This is a test asdfdsaf\",\n",
    "    \"For you phoneme tool\",\n",
    "    \"thhat ensure you can get\",\n",
    "    \"Correct labels\",\n",
    "]\n",
    "out = [phoneme_util.word_to_phoneme_sequence(s) for s in sentences]\n",
    "ans = [['DH', 'IH', 'S', 'IH', 'Z', 'AH', 'T', 'EH', 'S', 'T', '<UNK>'], ['F', 'AO', 'R', 'Y', 'UW', 'F', 'OW', 'N', 'IY', 'M', 'T', 'UW', 'L'], ['<UNK>', 'EH', 'N', 'SH', 'UH', 'R', 'Y', 'UW', 'K', 'AE', 'N', 'G', 'EH', 'T'], ['K', 'ER', 'EH', 'K', 'T', 'L', 'EY', 'B', 'AH', 'L', 'Z']]\n",
    "for i,j in zip(out, ans):\n",
    "    assert i == j\n",
    "print('Congratulations!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. Run the code below to obtain phoneme annotation for tiny LibriSpeech dataset. After this, the phoneme annotations will be stored to 'phn' property in the annotation files for each audio.  **[2 mark(s)]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-09-18T04:44:05.272623500Z",
     "start_time": "2023-09-18T04:44:04.427041200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats!\n"
     ]
    }
   ],
   "source": [
    "#NOTE: At the end of this the /datasets/tiny_librispeech/annotations/x.json contains the phonemes also!\n",
    "phoneme_util = PhonemeUtil()\n",
    "dataset_dir = './datasets/tiny_librispeech'\n",
    "annot_dir_complete = jpath(dataset_dir, 'annotation_word')\n",
    "annot_dir_word = jpath(dataset_dir, 'annotation')\n",
    "if not os.path.exists(annot_dir_word):\n",
    "    os.mkdir(annot_dir_word)\n",
    "splits = ['train', 'valid', 'test']\n",
    "for split in splits:\n",
    "    annot_fp_old = jpath(annot_dir_complete, split+'.json')\n",
    "    annot_fp_new = jpath(annot_dir_word, split+'.json')\n",
    "    data = read_json(annot_fp_old)\n",
    "    for id in data:\n",
    "        entry = data[id]\n",
    "        sentence = entry['words']\n",
    "        phonemes = phoneme_util.word_to_phoneme_sequence(sentence)\n",
    "        data[id]['phn'] = ' '.join(phonemes)\n",
    "    save_json(data, annot_fp_new)\n",
    "data = read_json(jpath(dataset_dir, 'annotation', 'test.json'))\n",
    "\n",
    "t = 'R AA B AH N <UNK> S AO DH AE T HH IH Z D AW T S AH V W AA R AH N T AH N HH AE D B IH N AH N F EH R AH N D HH IY B IH K EY M AH SH EY M D AH V HH IH M S EH L F F AO R HH AA R B ER IH NG DH EH M'\n",
    "assert data['61-70970-0036']['phn'] == t\n",
    "print('Congrats!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 2: Prepare tokenizer [3 mark(s)]\n",
    "In both training and inference, a tokenizer help to convert labels (in our case, phoneme annotations) from text to integer numbers so that the model can handle them easily.\n",
    "\n",
    "1. Please finish the code of PhonemeTonekizer Class in utils.py so that it can pass the cell below. **[3 mark(s)]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-09-18T05:08:39.474236200Z",
     "start_time": "2023-09-18T05:08:38.544405400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats!\n"
     ]
    }
   ],
   "source": [
    "from utils import PhonemeTokenizer\n",
    "tokenizer = PhonemeTokenizer()\n",
    "assert len(tokenizer.vocab) == 41\n",
    "assert tokenizer.token_to_id['<UNK>'] == 40\n",
    "assert tokenizer.id_to_token[0] == '<blank>'\n",
    "\n",
    "phn_seqs = [\n",
    "    ['CH', 'AO', 'B', 'T', 'S', 'OY'],\n",
    "    ['B', 'AE', 'AA', 'AH', 'ER', 'TH'],\n",
    "    ['<UNK>', 'D', 'B', '<UNK>', 'HH', 'TH']\n",
    "]\n",
    "ans = [\n",
    "    [8, 4, 7, 31, 29, 26],\n",
    "    [7, 2, 1, 3, 12, 32],\n",
    "    [40, 9, 7, 40, 16, 32],\n",
    "]\n",
    "\n",
    "assert tokenizer.encode_seq(phn_seqs[0]) == ans[0]\n",
    "assert tokenizer.encode_seq(phn_seqs[1]) == ans[1]\n",
    "assert tokenizer.encode_seq(phn_seqs[2]) == ans[2]\n",
    "assert tokenizer.decode_seq(ans[0]) == phn_seqs[0]\n",
    "assert tokenizer.decode_seq(ans[1]) == phn_seqs[1]\n",
    "assert tokenizer.decode_seq(ans[2]) == phn_seqs[2]\n",
    "assert tokenizer.decode_seq_batch(ans) == phn_seqs\n",
    "\n",
    "print('Congrats!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3: ASR Baseline [8 mark(s)]\n",
    "\n",
    "We are now ready for building the first ASR system. Please finish the tasks below:\n",
    "\n",
    "1. The current code uses the validation set as the testing set, while the code for preparing the test data is missing. Please complete it. **[1 mark(s)]**\n",
    "2. Please use Checkpointer class of speechbrain to help you save the model with the lowest Phoneme Error Rate (PER) during training. Save the checkpoint under the directory \"results/baseline/best_ckpt\". **[1 mark(s)]**\n",
    "3. Load the best model (lowest PER) for evaluation, instead of using the model from the last epoch. **[1 mark(s)]**\n",
    "4. Please use speechbrain.utils.metric_stats.ErrorRateStats.write_stats to help you save the output of your model on the whole test set to help you know your model's performance better. In the output file, please use phoneme tokens instead of token ids (numbers). Save the file to \"results/baseline/results.txt\" **[1 mark(s)]**\n",
    "5. Please log your training, validation, and evaluation statistics to the result folder, in whatever way you like. **[1 mark(s)]**\n",
    "\n",
    "Run the training and testing by\n",
    "\n",
    "    python train.py hparam_baseline.yaml\n",
    "Expected PER: 90%.\n",
    "\n",
    "**NOTE**: Please keep the (1) training log, (2) model checkpoint and the (3) corresponding result files, when submitting you assignment. **[3 mark(s)]**\n",
    "\n",
    "**MY NOTE: The files are present in the ./results/baseline folder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 4: Modifying the Model [13 mark(s)]\n",
    "\n",
    "You may have spot some of the issues during the training, like the slow converging speed, overfitting, etc. Please make the following changes to your model by modifying the yaml file.\n",
    "1. (Please create a new .yaml file from the hparam_baseline.yaml, naming it hparam_modified.yaml) **[1 mark(s)]**\n",
    "2. Increase the N_epoch to 20. **[1 mark(s)]**\n",
    "3. Increase the learning rate to 5e-3 **[1 mark(s)]**\n",
    "4. Add weight decay = 0.1 to the optimizer **[1 mark(s)]**\n",
    "5. Add a variable named \"drop_p\", with value 0.2. **[1 mark(s)]**\n",
    "6. Add 3 dropout layers to the model, after act1, act2, and RNN. All with the same dropout rate of \"drop_p\" (you need to use a variable reference here). **[1 mark(s)]**\n",
    "7. Change the output_dir from \"results/baseline\" to \"results/drop0.2x2_lr0.0005_wd0.1\". **[1 mark(s)]**\n",
    "\n",
    "There are some other changes you need to make in the train.py file:\n",
    "1. Use the speechbrain.nnet.schedulers.NewBobScheduler to schedule the learning rate or training according to loss on validation set. If the validation loss did not decrease after an epoch of training, use that scheduler to adjust the learning rate. **[2 mark(s)]**\n",
    "2. Before the training of each epoch, print out and log the current learning rate. **[1 mark(s)]**\n",
    "\n",
    "Run the training and testing by\n",
    "\n",
    "    python train.py hparam_modified.yaml\n",
    "Expected PER: 65%.\n",
    "\n",
    "**NOTE**: Please keep the (1) training log, (2) model checkpoint and the (3) corresponding result files, when submitting you assignment. **[3 mark(s)]**\n",
    "\n",
    "**NOTE from TA: In Task 4, there is an inconsistancy of the folder naming and the learning rate. Please change the output_dir of the modified model to \"results/drop0.2x2_lr0.005_wd0.1\" instead.**\n",
    "\n",
    "**MY NOTE: THe files are present in ./results/drop0.2x2_lr0.005_wd0.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Section 2 - Questions [42 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### - Result Analysis [2 mark(s)]\n",
    "\n",
    "1. How does your system perform? Briefly introduce your system's performance with objective metric scores and the result file for the test set. **[2 mark(s)]**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "There are 2 systems trained which are the baseline model and the modified model.\n",
    "\n",
    "- Here is a brief explanation the required files which are saved in the path ./results/baseline && ./results/drop0.2x2_lr0.005_wd0.1: ( please check all the files present in these directories)\n",
    "    + <u>training log.txt</u>: Provides the training loss at each epoch along with the validation metric summary run at the end of each epoch. At the end of the file, the summary metrics for the best model at test time is also shown.\n",
    "    + <u>train_valid_stats.txt</u>: Provides both the training loss, validation loss and validation metrics at the end of each epoch. Contains the same information as 'training log.txt'\n",
    "    + <u>results.txt</u>: Provides a sample file showcasing the alignments at inference time on a series of sentences. It shows phoneme level comparisons of whether any insertions, deletions or substitutions happened along with the final testing evaluation metrics.\n",
    "\n",
    "- Here are the brief explanations of the results of each model.\n",
    "    + **a. Baseline model**\n",
    "        - As we can see from the files mentioned above for this model , the final training epoch's validation \"Word Error Rate\" (which in this case corresponds to the \"Phoneme Error Rate\" ) is around 73.8% and while evaluating the best model on the test data we can observe that the WER is around 80.33% while the SER is 100%\n",
    "        - This means that at a sentence level, the baseline model was not able to predict any sentence accurately as a whole but when it came to phoneme level predictions it only had an error rate of 80% on the test data.\n",
    "        - This model was only trained for 10 epochs and the checkpointed model at the end of the 10th epoch was the best model which was picked up for further evaluation on the test dataset.\n",
    "        - Also, we can see that our ASR model has a lot more deletions (399) and substitutions (71) when compared with insertions (7). Which means that this baseline model is predicted a lot lesser words ( i.e. more deletions) or predicting wrong phonemes( i.e. more substitutions) which also implies that this baseline model still has a long way to improve due to its poor performance.\n",
    "\n",
    "    + **b. Modified model**\n",
    "        - The architecture of this model is different from the base model as we additionally have 3 dropout layers, along with a weight decay in the Adam optimizer, a higher base learning rate of 0.005 and a NewBobScheduler which decreases the learning rate in the Adam optimizer if the validation loss ever decreases from its previous epoch's values.\n",
    "        - As we can see from the files mentioned above for this model , the final training epoch's validation \"Word Error Rate\" (which in this case corresponds to the \"Phoneme Error Rate\" ) is around 57% and while evaluating the best model on the test data we can observe that the WER is around 62.033% while the SER is 100%. These results are clearly better than the values in the baseline model but are still not yet good enough.\n",
    "        - Error rate for SER being 100% at a sentence level means that, even the modified model was not able to predict any sentence accurately as a whole but when it came to phoneme level predictions it only had an error rate of 62% on the test data.\n",
    "        - This model was only trained for 20 epochs. The best checkpointed model was picked up for further evaluation on the test dataset.\n",
    "        - In this particular ASR model we can see that the number of insertions , deletions and substitutions have significantly dropped from the base model (i.e. insertions (22), deletions (184), substitutions (160)) which implies that the model is now dropping lesser number of words but has different predictions at a phoneme level resulting in the substitutions increasing from 71 -> 160.\n",
    "        - That said, this model is still not performant enough for any practical application as there is still a lot of room for improvement of this model as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### - Tokenization [8 mark(s)]\n",
    "1. Do you think detecting phoneme sequence from speech recording is more difficult than detecting character or word sequence? Why? **[2 mark(s)]**\n",
    "2. For the task of speech recognition, what are the drawbacks of using phoneme as the detecting unit? **[2 mark(s)]**\n",
    "3. What is the advantage of sub-word tokenizer compared to word-level tokenizer? **[2 mark(s)]**\n",
    "4. If we are changing our tokenizer to the type of grapheme, which level do you think is the best, among {character, word, sub-word}? Please state your reason. **[2 mark(s)]**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "1. Detecting an entire word is a lot harder than detecting individual characters or phonemes. That said, the current formulation of phonemes assumes that people speaking english in all accents have the same pronunciation. Due to this simplifying assumption, the number of phonemes to deal with are a lot lesser than what we would have to deal with when considering the phonemes for all varied accents in the world. However, detecting phonemes is not an easy task as different speakers may pronounce phonemes differently, and there can be substantial variation in how individuals articulate the same phonemes. Additionally, unlike written text, where spaces indicate word boundaries and characters are well-defined, spoken language lacks clear boundaries between phonemes. This makes it challenging to segment speech into individual phonemes without contextual information.\n",
    "\n",
    "\n",
    "2. Here are some of the drawbacks of using phoneme as the detecting unit:\n",
    "    - It's more complicated because it deals with many tiny units.\n",
    "    - Sometimes, sounds can mean different things in different situations, making it hard to understand.\n",
    "    - People say the same sounds differently, making it tricky for one-size-fits-all systems.\n",
    "    - How sounds are spoken can change depending on nearby sounds, making it harder to separate them.\n",
    "    - It takes a lot of computing power and data.\n",
    "    - It often needs tweaking for different languages, speakers and accents.\n",
    "\n",
    "3. Sub-word tokenization, compared to word-level tokenization, has several benefits:\n",
    "    - It results in a smaller vocabulary, saving memory and making models easier to train.\n",
    "    - It focuses on common linguistic structures helps models generalize better.\n",
    "    - Sub-word tokenization retains character-level details, valuable for fine-grained text analysis.\n",
    "    - Sub-word tokenization can recognize and process rare/ infrequent words not found in its vocabulary, making it more adaptable to a wider range of terms.\n",
    "    - It effectively captures variations in word forms, such as tenses and plurals, by breaking them into common sub-word units which can help the model generalize better.\n",
    "    - A base model based on this tokenization could potentially be used as a base model to learn ASR systems in other similar languages which have similar sub words.\n",
    "\n",
    "4. A grapheme is the smallest unit of written language, like the building blocks of letters and symbols. Each letter in the alphabet and every symbol, like punctuation marks, is a grapheme. They come together to form words and sentences. When choosing the best level of grapheme tokenization (character, word, or sub-word), it depends on the specific task.\n",
    "    - <u>Character-Level Grapheme Tokenization</u>: This treats each character as a separate unit, making it versatile and suitable for languages with complex scripts. It's best when there is a need to analyze individual letters or work with languages where words don't have clear boundaries. It's also useful for tasks like text generation or text classification that focus on character-level patterns.\n",
    "    - <u>Word-Level Grapheme Tokenization</u>: This divides text into whole words, which are more interpretable and contextually meaningful. It's ideal when we need to understanding word meanings themselves, like in translation or sentiment analysis. It's also a good fit for languages with well-defined word boundaries.\n",
    "    - <u>Sub-Word-Level Grapheme Tokenization</u>: This balances characters and words by segmenting text into meaningful sub-word units. It's a commonly chosen option for many language processing tasks. It handles tricky out-of-vocabulary words, word variations, and multiple languages effectively. It's often used in machine translation, speech recognition, and text generation where adaptability to different languages matter.\n",
    "    - Overall, sub-word-level grapheme tokenization is often a practical choice because it combines the strengths of both characters and words while addressing challenges like handling new words and complex word forms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### - Modeling [7 mark(s)]\n",
    "Connectionist Temporal Classification (CTC) is a type of loss function that is commonly used in ASR, especially when we do not know the precise alignment between the annotation and the audio.\n",
    "\n",
    "1. Explain how does CTC deal with the misalignment issue between audio and annotation, i.e., the number of frames in the audio is much higher than the number of phoneme/character/sub-word/word in the annotation, and we do not know their correspondence. **[1 mark(s)]**\n",
    "2. Why does CTC need an additional blank token in the prediction? **[1 mark(s)]**\n",
    "3. Here are several decoded output from a CTC model. Write out their final recognition result. (\"-\" is CTC blank token, and \"_\" represent space) **[2 mark(s)]**\n",
    "\n",
    "    (1) heeel-ll-l_lllooo--wooooorld\n",
    "\n",
    "    (2) hhhhee-llow_wo--rr-rllll--dd\n",
    "    \n",
    "4. Recall the formula of CTC loss:\n",
    "   $$L_{CTC} = -log(\\sum_{\\pi \\in B^{-1}(W)} \\prod_{t=1}^Tp(\\pi_t|\\mathbf{x}_t))$$\n",
    "   Does this summation mark means that we have to list out all possible alignments between frames and texts, compute the probability for each pair, and add them together? Is there more efficient way to compute the CTC loss? If you think so, please briefly explain a more efficient algorithm. **[3 mark(s)]**\n",
    "\n",
    "Answer:\n",
    "1. CTC deals with misalignment between audio and annotation by allowing the model to predict not only the phonemes but also repetitions. This is represented by a special blank token (\"-\"). In this approach, words with blank tokens or repeated phonemes are all merged together as long as there are no space tokens in between. This way, it aligns the audio frames with the annotation without knowing their precise alignment mapping even though the number of audio frames are much larger than the number of phonemes in the annotation. In this process, we can either repeat the same phoneme multiple times as the output at each time step or instead output a \"blank token\". The model then learns to find the best alignment during training by adjusting the probabilities of inserting blank tokens and tokens corresponding to the desired output units.\n",
    "<br>\n",
    "\n",
    "2. The blank token in the CTC framework plays a crucial role as a separator between repeated units such as phonemes, characters, sub-words, or words. Its primary function is to assist the model in handling instances where the same unit occurs multiple times consecutively. Without the blank token, the model might encounter difficulties in distinguishing between repeated units. For e.g., in the word \"hello\"; the presence of the blank token enables the model to distinguish between the two \"l\" characters, preventing them from being merged into a single unit. This differentiation is essential for the model to generate the correct output sequence, as it helps control merging of repetitions effectively.\n",
    "<br>\n",
    "\n",
    "3.  (1) helll loworld\n",
    "    (2) helow worrld\n",
    "<br>\n",
    "\n",
    "4.\n",
    "    - Yes, in this case the summation means that we need to add the probabilities for each possible alignment as multiple alignments can give rise to the same word due to the nature of how \"blank tokens\"   and repeated tokens work.\n",
    "    - When computing this summation in a naive manner it is computationally very expensive however we can use dynamic programming to compute the CTC loss in a more efficient manner.\n",
    "    - The original implementation of CTC loss leverages dynamic programming techniques inspired by Hidden Markov Models (HMMs). Here is a high level overview of how it works:\n",
    "        - CTC adopts some concepts from HMMs, particularly the use of dynamic programming algorithms. For e.g. in HMMs, we use the forward and backward passes to efficiently compute probabilities while considering multiple possible alignments between audio frames and the target sequence.\n",
    "        - Here we cache the intermediate results, such as probabilities of reaching specific states or alignments, as we calculate them during the forward and backward passes.\n",
    "        - Since these probabilities are cached during computation of forward and backward algorithm, we can retrieve the cached value instead of recomputing it. This significantly speeds up the CTC loss computation and makes it feasible for training large ASR models on extensive datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### - Language Model [7 mark(s)]\n",
    "1. Consider the two sequences below:\n",
    "    - A: I like Singapore's weather.\n",
    "    - B: I Singapore like ? weathers.\n",
    "\n",
    "    For a well-trained language model, which sentence will have lower perplexity from this model? Why? **[1 mark(s)]**\n",
    "</br>\n",
    "\n",
    "2. Given the corpus below:\n",
    "\n",
    "            <s> I love to play football </s>\n",
    "            <s> He loves to watch football </s>\n",
    "            <s> I love to watch movies </s>\n",
    "            <s> She loves to play tennis </s>\n",
    "    (1) Assuming we are using a word-level tokenizer. <s> and </s> represent start and end of sentence token. Calculate the below bigram probability $P(B|A)$ by\n",
    "    $$\n",
    "    P(B|A) = \\frac{Count(A B)}{Count(A)}\n",
    "    $$\n",
    "    **[3 mark(s)]**\n",
    "    \n",
    "    a. P(love | I)\n",
    "\n",
    "    b. P(to | love)\n",
    "\n",
    "    c. P(football | play)\n",
    "    \n",
    "    d. P(movies | watch)\n",
    "   </br>\n",
    "   \n",
    "    (2) Use the probability you obtained above, calculate the probability of below sentences **[2 mark(s)]**\n",
    "    a. I love to watch football\n",
    "    b. She loves to play football\n",
    "   </br>\n",
    "   \n",
    "    (3) Why it's not a good idea to use a large n value for n-gram language models? **[1 mark(s)]**\n",
    "\n",
    "\n",
    "**Answer:**\n",
    "1. In Sentence A, \"I like Singapore's weather.\"; it is expected to have lower perplexity from a well-trained language model because it follows a regular sentence structure and makes logical and grammatical sense. Whereas in Sentence B, \"I Singapore like ? weathers.\" is likely to have higher perplexity because it contains jumbled words and lacks proper grammar, making it harder for the language model to predict the next word accurately. In general, lower perplexity means that the predicted sentence is better as it is in some sense inversely proportional to the Nth root of the joint probability distribution of the predicted sequence of words. Meaning that higher probability corresponds to the model being less \"perplexed\" or \"confused\". Here is the formula\n",
    "\n",
    "$$Perplexity = \\sqrt[n]{\\frac{1}{P(w1,w2,...wn)}}$$\n",
    "\n",
    "2. **NOTE: This is just using the same corrected formula provided by Longshen, even though it is not graded here are my answers**\n",
    "    (1).\n",
    "        a. P(love | I) = Count(I, love) / Count(I) = 2/2 = 1\n",
    "        b. P(to | love) = Count(love, to) / Count(love) = 2/2 = 1\n",
    "        c. P(football | play) = Count(play, football) / Count(play) = 1/2 = 0.5\n",
    "        d. P(movies | watch) = Count(watch, movies)/Count(watch) = 1/2 = 0.5\n",
    "\n",
    "    <br>\n",
    "\n",
    "    (2).a. P(\"I love to watch football) = P(I | \\<s\\>) * P(love | I) * P(to | love) * P(watch | to) * P(football | watch)\n",
    "                                    = 2/4 * 1 * 1 * 2/4 * 1/2\n",
    "                                    = 1/8\n",
    "    b. P(\"She loves to play football\") = P(She | \\<s\\>) * P(loves | She) * P(to | loves) * P(play | to) * P(football | play)\n",
    "                                    = 1/4 * 1 * 2/2 * 2/4* 1/2\n",
    "                                    = 1/16\n",
    "\n",
    "    <br>\n",
    "\n",
    "    (3). Although having a large n-value for n-grams might provide more context its generally not a good idea because:\n",
    "        * As n increases, the n-gram model considers longer sequences of words. This can lead to a significant increase in the number of unique n-grams in the training data.\n",
    "        * Many of these n-grams may occur very rarely or even be unique, resulting in sparse data.\n",
    "        * Sparse data can lead to poor model generalization because the model may not have enough information to make accurate predictions for these rare or unseen n-grams.\n",
    "        * In essence, a large n value can lead to data sparsity problems, making it challenging for the model to estimate probabilities accurately and potentially causing overfitting on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### - Beam Search [4 mark(s)]\n",
    "Assume we have a simplified language model that can predict the probability of next word. We have generated a start part of the sentence \"I want to\". Now we are using beam search to predict the rest of the sentence. Use letter \"G\" denote the generated part. Let's use beam size of 2 for this question.\n",
    "\n",
    "        Probability calculated by language model:\n",
    "        p(eat | G): 0.4\n",
    "        p(play | G): 0.3\n",
    "        p(go | G): 0.2\n",
    "        p(watch | G): 0.1\n",
    "        p(a sandwich | G eat): 0.5\n",
    "        p(dinner | G eat): 0.4\n",
    "        p(an apple | G eat): 0.1\n",
    "        p(football | G play): 0.6\n",
    "        p(games | G play): 0.4\n",
    "1. Let's continue the generation from G=\"I want to\". After the first step of beam search, what tokens will be selected, and what are the resulting candidate sequence? **[1 mark(s)]**\n",
    "2. In the 2nd step of beam search, what are the two beams starting with \"G eat\"? What are their probability respectively? **[1 mark(s)]**\n",
    "3. In the 2nd step of beam search, what are the two beams starting with \"G play\"? What are their probability respectively? **[1 mark(s)]**\n",
    "4. What are the resulting candidate sequence from the 2nd step of beam search? **[1 mark(s)]**\n",
    "\n",
    "**Answer**\n",
    "\n",
    "1. Since G = \"I want to\" the next possible tokens with a beam size of 2 are \"eat\" and \"play\" because P(eat|G) [0.4] and P(play|G) [0.3] have the highest probabilities after G is generated.\n",
    "2. In the second step , one possible branch is \"G eat\" ( i.e. I want to eat). From this point onwards the next possible tokens are \"a sandwich\" & \"dinner\" as p(a sandwich | G eat) [0.5] and p(dinner | G eat) have the highest probability when the \"G eat\" is already generated. Therefore the probability for \"a sandwich\" is 0.5 and the probability for \"dinner\" is 0.4\n",
    "3. In the second step , the other possible branch is \"G play\" (i.e. I want to play). From this point onwards the next possible tokens are \"football\" and \"games\" as their probabilities p(football | G play) and p(games | G play) are the highest given that \"G play\" is already generated. Therefore the probability for \"football\" is 0.6 and the probability for \"games\" is 0.4\n",
    "4. There are totally 4 possible resultant candidate sequences at the end of the 2nd step which are:\n",
    "    - When \"G eat\" is generated:\n",
    "        - I want to eat a sandwich\n",
    "        - I want to eat dinner\n",
    "    - When \"G play\" is generated:\n",
    "        - I want to play football.\n",
    "        - I want to play games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### - Word Error Rate [3 mark(s)]\n",
    "\n",
    "Consider an automatic speech recognition system that transcribes a spoken segment into text. We compare the transcription of the system with a human-annotated reference transcript to calculate the system's Word Error Rate.\n",
    "\n",
    "Reference Transcript:\n",
    "\"I am excited to learn about speech recognition.\"\n",
    "\n",
    "System's Transcription (Hypothesis):\n",
    "\"I am excited learn about speech recognise.\"\n",
    "\n",
    "1. Calculate the number of insertions, deletions, and substitutions. **[1 mark(s)]**\n",
    "2. Compute the Word Error Rate (WER) using the formula: **[1 mark(s)]**\n",
    "$$WER=\\frac{\\text{Insertions}+\\text{Deletions}+\\text{Substitutions}}{\\text{Number of words in Reference}}$$\n",
    "3. Why might WER be a more reasonable metric for ASR compared to a simple accuracy rate (correct words divided by total words)? **[1 mark(s)]**\n",
    "\n",
    "**Answer:**\n",
    "1. There is 1 deletion (Only the word \"to\" is missing in output) and 1 substitution (the word \"recognition\") and zero insertions. (i.e. I = 0, S = 1, D = 1)\n",
    "2. Here is the calculation\n",
    "    $$WER=\\frac{\\text{Insertions (0)}+\\text{Deletions (1)}+\\text{Substitutions (1)}}{\\text{Number of words in Reference (8)}}$$\n",
    "    $$WER=\\frac{2}{8}$$\n",
    "    $$WER=0.25$$\n",
    "3. WER is preferred over simple accuracy for the following reasons:\n",
    "    - WER considers insertions, deletions, and substitutions, making it better at detecting different types of errors in ASR systems. Basic accuracy rate treats all errors the same, which may not accurately reflect transcription quality.\n",
    "    - WER allows for variations in alignment of reference and output, therefore it is more forgiving in this regard.( e.g. alignment variations can happen because of different speaker rates or different accents or pauses etc)\n",
    "    - WER focuses on the correctness of the content, ensuring that even if a word is slightly mispronounced or replaced, the overall message is still captured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### - Possible Improvement [3 mark(s)]\n",
    "1. The performance of the recognition system in Section 1 might still have room to improve. What are possible reasons for the not-so-good performance, and directions of improvement? Please list 3 pairs of them. **[3 mark(s)]**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "There are several reasons for the below par performance, which are listed below along with other dimensions to explore for improving the model\n",
    "- We need more data to train on which can be achieved with data augmentation techniques where we can consider more different time-sliced windows in each training data point of audio / text pair.\n",
    "- It could also be the case that the dataset is not generalized enough to consider different accents as with different accents, the phoneme tokenization is also technically different. Therefore at inference time, if a person speaks with a different accent the machine might transcribe it incorrectly!\n",
    "- We could train our machine learning model to directly predict characters instead of phonemes as the concept of phonemes is something invented by humans and this biased way of thinking about the tokenization might actually hinder the model from learning a better underlying representation. Therefore, ignoring phonemes when using deeper machine learning models might lead to better results as the model might learn its own underlying representations when its not restricted with this concept of sticking to phonemes.\n",
    "- Other dimensions to consider for improvement would be to consider using more state of the art model architectures incorporating concepts like attention in order to learn the dependencies in a better manner\n",
    "- We could also consider building on top of existing pretrained models for getting a better performance.\n",
    "- Given that we know what the CTC loss formula is we can also add that loss term to the existing loss term between the encoder and decoder to consider minimizing the error in a more holistic end-to-end perspective.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### - Speech vs Singing [6 mark(s)]\n",
    "1. What are the properties that are different between audio of from speech recording and that of singing recording? What are the similar/same properties that are shared between them? **[2 mark(s)]**\n",
    "2. What are the properties that are different between spoken texts and lyrics? What are the similar/same properties that are shared between them? **[1 mark(s)]**\n",
    "3. Given the limited paired singing dataset of audio and lyric, how can we build a lyric transcription system with better performance? Please answer from 3 perspectives. **[3 mark(s)]**\n",
    "\n",
    "**Answer:**\n",
    "1. Listed below are the properties similar and different between speech and singing recording respectively:\n",
    "- **Similar Properties:**\n",
    "    + Both speech and music happen in the human hearing range (20 Hz -> 20k Hz)\n",
    "    + Both speech and singing have variations temporally. For e.g. rhythmic patterns and pauses in terms of duration, timing, and tempo.\n",
    "    + Both of them can have variations in loudness/amplitude ranging from low decibel to high decibels\n",
    "\n",
    "- **Different Properties:**\n",
    "    + Singing uses deliberate pitch manipulation for melodies, while speech has a smaller pitch range and depends on the speaker.\n",
    "    + Even though both contains phonetic features, in singing we often have to hold a particular phoneme at a particular note for a longer time and also sing it with some particular emotion as well. Whereas in speech the phonemes occur at a normal tempo which happens to be the rate at a which a person normall speaks.\n",
    "    + While speech has some kind of prosody and tempo it is not consistent and can very from person to person. Whereas, for singing it involves melodic tempo and typically has more consistent rhythm.\n",
    "    + Singing lyrics are generally more poetic and fit a particular time meter whereas regular speech need not necessarily be poetic in nature.\n",
    "    + While both speech and singing can convey emotion and expression, singing is more explicitly focused on emotional delivery through melody and musical techniques, while speech relies on intonation and prosody for emotional nuances.\n",
    "\n",
    "2. The only similar property is that both of them share features in the language dimension as both of them have phonemes, words, phrases and sentences in them which is used to convey some information or story.\n",
    "However, there quite a few differences which are listed below:\n",
    "- In terms of rhythm and tempo, singing lyrics have a fixed melodic rhythmic structure whereas spoken text may not.\n",
    "- In terms of pronunciation, some words in the singing lyrics might be pronounced differently for providing some emotion or special emphasis on the mood of the song whereas in terms of spoken text the pronounciation varies only with different accents but is more or less the same throughout.\n",
    "- In terms of melody, each word in the singing lyrics might be associated with a specific or multiple musical notes whereas each word in the spoken text may only have minor inflections in pitch or accent to emphasise a particular point but in general are not associated with any musical notes.\n",
    "\n",
    "3. For building a better lyric transcription model here are some of the things we can do:\n",
    "- We can perhaps do some kind of transformation/ preprocessing of the singing data and limit it to only a few pitches. This would be analogous to converting a singing lyric ASR system to a regular ASR system where we only try to detect spoken words. In this case, if we already had a performant ASR for spoken text (i.e. regular speech) we would essentially be converting the problem of singing detection to just speech detection\n",
    "- Another route to take would be to use ML algorithms to directly predict tokens at the character level rather than at the phoneme level. This way a more complex ML model will not be biased to our limited understanding of what phonemes are and might discover the hidden underlying patters by itself\n",
    "- Another key step we can do would be data augmentation. As given a pair of lyrics and its audio; we can make many time-sliced windows of varying lengths thus generating a lot more data for the model to learn from which would undoubtedly help any supervised learning algorithm perform better.\n",
    "- Apart from this we can perhaps try training more state of the art architectures using concepts like attention/ self-attention on already existing pretrained models (lets say even on a regular ASR only for speech detection) as that way the new model architecture will already have the benefit of learning from the weights and insights gained by the pretrained model before it.\n",
    "- Another approach to improve the lyric transcription would be to incorporate additional context in other modalities. For e.g. if we have the singing face or the lip movement, that might help the model to learn more underlying concepts better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### - Timing Survey [2 mark(s)]\n",
    "\n",
    "- What do you think is the most difficult part? Which part did you spent most time on it? **[1 mark(s)]**\n",
    "\n",
    "Answer:\n",
    "The entirety of section 2 was the hardest part as it required a deep understanding and revision of certain theoretical aspects of implementing ASR.\n",
    "\n",
    "</br>\n",
    "\n",
    "- How much time did you spent on the assignment? Please fill an estimated time here if you did not time yourself. **[1 mark(s)]**\n",
    "\n",
    "Answer: ~4 days\n",
    "\n",
    "</br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
