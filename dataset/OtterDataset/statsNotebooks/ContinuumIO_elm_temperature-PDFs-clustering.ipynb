{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering on Temperature PDF's\n",
    "\n",
    "This notebook shows how `elm`, Ensemble Learning Models, can be used for ensemble approaches to K-Means with a `Pipeline` of preprocessors, normlizers and transformers before K-Means and loading files from a multi-file NetCDF dataset.\n",
    "\n",
    "This notebook uses an ensemble approach to K-Means on those temperature spatial time series arrays.  The approach here is based on Loikith et al (2013)'s clustering of log probabilities of temperature anomalies.\n",
    "\n",
    "```text\n",
    "Classifying reanalysis surface temperature probability density functions (PDFs) over North America with cluster analysis\n",
    "P. C. Loikith, B. R. Lintner, J. Kim, H. Lee, J. D. Neelin, and D. E. Waliser\n",
    "\n",
    "GEOPHYSICAL RESEARCH LETTERS, VOL. 40, 3710â€“3714, doi:10.1002/grl.50688, 2013\n",
    "```\n",
    "\n",
    "### Setup with `xarray`, `numpy` and `matplotlib` imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure `dask-distributed` scheduler from environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DASK_SCHEDULER = os.environ.get('DASK_SCHEDULER', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from distributed import Client\n",
    "from distributed import local_client\n",
    "import dask\n",
    "\n",
    "client = Client(DASK_SCHEDULER) if DASK_SCHEDULER else Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set constants related to the data source\n",
    "\n",
    "This is a global dataset of temperature and other weather variables from the MERRA2 project.  If you have not yet downloaded the MERRA2 data source of this notebook and want to run the notebook locally, create a bash script like the following one to download the data set from [http://goldsmr4.gesdisc.eosdis.nasa.gov](http://goldsmr4.gesdisc.eosdis.nasa.gov):\n",
    "\n",
    "```bash\n",
    "SCRIPT_DIR=`dirname $(readlink -e \"$0\")`\n",
    "SPACIAL_URLS_FILE=\"$SCRIPT_DIR/MERRA2_M2T1NXSLV_west_pacific_urls.dat\"\n",
    "COOKIE_FILE=\"$HOME/.urs_cookies\"\n",
    "DEST_DIR=\"/mnt/efs\"\n",
    "USAGE=\"$0:\n",
    "    Data downloader for NASA SBIR project.\n",
    "\n",
    "usage: $0 [-h] [--cookie-file COOKIE_FILE] [--dest-dir DEST_DIR] [--spacial-urls SPACIAL_URLS_FILE]\n",
    "\n",
    "arguments:\n",
    "    -h, --help: show this help message and exit\n",
    "    -c, --cookie-file: (Optional) path to cookie file. Default: $HOME/.urs_cookies\n",
    "    -d, --dest-dir: (Optional) path to destination (download) directory. Default: /mnt/efs\n",
    "    -s, --spacial-urls: (Optional) path to data file containing spacial data URLs. Default: $SPACIAL_URLS_FILE\n",
    "\"\n",
    "while [ $# -gt 0 ]; do\n",
    "    case \"$1\" in\n",
    "        -c|--cookie-file)\n",
    "            COOKIE_FILE=\"$2\"\n",
    "            shift; shift\n",
    "            ;;\n",
    "        -d|--dest-dir)\n",
    "            DEST_DIR=\"$2\"\n",
    "            shift; shift\n",
    "            ;;\n",
    "        -s|--spacial-urls)\n",
    "            SPACIAL_URLS_FILE=\"$2\"\n",
    "            shift; shift\n",
    "            ;;\n",
    "        -h|--help)\n",
    "            echo \"$USAGE\"\n",
    "            exit 1\n",
    "            ;;\n",
    "        *)\n",
    "            echo \"Error: Invalid option \\\"$1\\\"\"\n",
    "            echo \"$USAGE\"\n",
    "            exit 1\n",
    "            ;;\n",
    "    esac\n",
    "done\n",
    "[ ! -f \"$COOKIE_FILE\" ] && echo \"Error: Non-existent cookie file provided\" && echo \"$USAGE\" && exit 1\n",
    "[ ! -d \"$DEST_DIR\" ] && echo \"Error: Non-existent destination dir provided\" && echo \"$USAGE\" && exit 1\n",
    "[ ! -f \"$SPACIAL_URLS_FILE\" ] && echo \"Error: Non-existent spacial URLs file provided\" && echo \"$USAGE\" && exit 1\n",
    "\n",
    "echo COOKIE_FILE = \"$COOKIE_FILE\"\n",
    "echo DEST_DIR = \"$DEST_DIR\"\n",
    "echo SPACIAL_URLS_FILE = \"$SPACIAL_URLS_FILE\"\n",
    "echo\n",
    "\n",
    "## Time period: Download the July and August data for all years\n",
    "##\n",
    "## Total wall clock time: 39m 4s\n",
    "## Downloaded: 2773 files, 3.5G in 26m 56s (2.23 MB/s)\n",
    "## real 39m4.178s\n",
    "## user 0m5.155s\n",
    "## sys  0m21.308s\n",
    "time sudo wget \\\n",
    "    --no-verbose \\\n",
    "    --load-cookies \"$COOKIE_FILE\" \\\n",
    "    --save-cookies \"$COOKIE_FILE\" \\\n",
    "    --auth-no-challenge=on \\\n",
    "    --keep-session-cookies \\\n",
    "    --recursive \\\n",
    "    --continue \\\n",
    "    --no-parent \\\n",
    "    --no-clobber \\\n",
    "    --relative \\\n",
    "    --accept '*0[78][0-9][0-9].nc4' \\\n",
    "    --directory-prefix=\"$DEST_DIR\" \\\n",
    "    \"http://goldsmr4.gesdisc.eosdis.nasa.gov/data/MERRA2/M2SDNXSLV.5.12.4/\"\n",
    "\n",
    "## Total wall clock time: 3m 25s\n",
    "## Downloaded: 113 files, 2.0G in 2m 39s (12.7 MB/s)\n",
    "## real 3m24.644s\n",
    "## user 0m1.314s\n",
    "## sys  0m11.405s\n",
    "time sudo wget \\\n",
    "    --no-verbose \\\n",
    "    --load-cookies \"$COOKIE_FILE\" \\\n",
    "    --save-cookies \"$COOKIE_FILE\" \\\n",
    "    --auth-no-challenge=on \\\n",
    "    --keep-session-cookies \\\n",
    "    --recursive \\\n",
    "    --continue \\\n",
    "    --no-parent \\\n",
    "    --no-clobber \\\n",
    "    --relative \\\n",
    "    --accept '*0[78].nc4' \\\n",
    "    --directory-prefix=\"$DEST_DIR\" \\\n",
    "    \"http://goldsmr4.gesdisc.eosdis.nasa.gov/data/MERRA2_MONTHLY/M2IMNXASM.5.12.4/\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some constants related to the data source\n",
    "\n",
    "*You will likely need to change the `MONTHLY_PATTERN` and `PATTERN` paths below if running the notebook locally*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FIRST_YEAR, LAST_YEAR = 1980, 2015 # the time domain of the input data\n",
    "\n",
    "# Glob file matching patterns\n",
    "PATTERN = '/mnt/efs/goldsmr4.gesdisc.eosdis.nasa.gov/data/MERRA2/M2SDNXSLV.5.12.4/{:04d}/{:02d}/*.nc4'\n",
    "\n",
    "MONTHLY_PATTERN = '/mnt/efs/goldsmr4.gesdisc.eosdis.nasa.gov/data/MERRA2_MONTHLY/M2IMNXASM.5.12.4/*/MERRA2_100.instM_2d_asm_Nx.*{:02d}.nc4'\n",
    "\n",
    "# The name of the NetCDF variable (xarray.DataArray) we will use\n",
    "TEMP_BAND = 'T2MMEAN'\n",
    "\n",
    "MONTH = 7 # just working with July\n",
    "\n",
    "YEARS = range(FIRST_YEAR, LAST_YEAR + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that our file pattern matches some NetCDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = glob.glob(PATTERN.format(2000, 7))\n",
    "g[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `earthio.load_array` to check one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from earthio import load_array\n",
    "example = load_array(g[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An `ElmStore` is returned \n",
    "\n",
    "Using the `data_vars` attribute `ElmStore`s inherit from `xarray.Dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example.data_vars "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `ElmStore` attributes\n",
    "\n",
    "The attribute `T2MMEAN` exists (an `xarray.DataArray`) because `T2MMEAN` was a \"variable\" name in the NetCDF we opened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example.T2MMEAN.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using summary statistics on a `DataArray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example.T2MMEAN.values.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with large temperature data sets, we can drop the other variables\n",
    "\n",
    "Later when we use [`xarray.open_mfdataset`](http://xarray.pydata.org/en/stable/generated/xarray.open_mfdataset.html) we can provide this list of variables to drop before concatenating temperature grids in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DROP_VARIABLES = [k for k in example.data_vars if k != TEMP_BAND]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for date components of file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_fname(f):\n",
    "    parts = f.split('.')\n",
    "    dt = parts[-2]\n",
    "    return int(dt[:4]), int(dt[4:6]), int(dt[6:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting monthly mean temperature\n",
    "Later we will use this monthly means `DataArray` for calculating residuals of the time series of grids (different of each time series point minus the long term mean for a pixel for July)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def month_means():\n",
    "    dask.set_options(get=dask.async.get_sync)\n",
    "    pat = MONTHLY_PATTERN.format(7)\n",
    "    fs = glob.glob(pat)\n",
    "    return xr.open_mfdataset(pat, \n",
    "                             lock=True,\n",
    "                             drop_variables=DROP_VARIABLES, \n",
    "                             concat_dim='time').mean(dim='time')\n",
    "\n",
    "MONTH_MEANS = month_means()\n",
    "MONTH_MEANS.T2M # This is the only attribute we will use (average temperature K)\n",
    "                # T2M is a name of a 'variable' in the underlying NetCDF files\n",
    "(MONTH_MEANS.T2M - 273.15).plot.pcolormesh()\n",
    "plt.title('Average temperatures for July (C)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `xarray.open_mfdataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(xr.open_mfdataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a `sampler` function\n",
    "\n",
    "A `sampler` given to `elm`'s fitting and prediction function should return\n",
    " * `X`, an `earthio.ElmStore` or `xarray.Dataset`\n",
    " * A tuple of `(X, y, sample_weight)` where X is as described above and `y` and `sample_weight` are either `None` or `numpy.ndarray`s of 1 dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def sampler(month, days, **kwargs):\n",
    "    with local_client() as lc:\n",
    "        dask.set_options(get=lc.get)\n",
    "        print('Sample - Month: {} Days: {}'.format(month, days))\n",
    "        files = []\n",
    "        for year in YEARS:\n",
    "            pattern = PATTERN.format(year, month)\n",
    "            fs = glob.glob(pattern)\n",
    "            dates = [split_fname(f) for f in fs]\n",
    "            keep = [idx for idx, d in enumerate(dates)\n",
    "                    if d[1] == month and d[2] in days]\n",
    "            files.extend(fs[idx] for idx in keep)\n",
    "        print('Sample {} files'.format(len(files)))\n",
    "        X = xr.open_mfdataset(files, lock=True, drop_variables=DROP_VARIABLES, concat_dim='time')\n",
    "        X.attrs['sample_kwargs'] = {'month': month, 'days': days}\n",
    "        X.attrs['band_order'] = [TEMP_BAND]\n",
    "        X.attrs['old_dims'] = [getattr(X, TEMP_BAND).dims[1:]]\n",
    "        X.attrs['old_coords'] = {k: v for k, v in X.coords.items()\n",
    "                                 if k in ('lon', 'lat',)}\n",
    "        return make_residuals(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Our sampler of files calls this `make_residuals` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from earthio import ElmStore\n",
    "def make_residuals(X, y=None, sample_weight=None, **kwargs):\n",
    "    \"\"\"Residuals of a spatial time series relative to a time series mean\n",
    "    \n",
    "    For each spatial point return (values - mean) where mean is the \n",
    "    mean of all points for a specific day-of-year\n",
    "    And values is the time series for a a given spatial lon, lat point\n",
    "    \n",
    "    Parameters:\n",
    "        X: ElmStore or xarray.Dataset\n",
    "        y: passed through\n",
    "        sample_weight: passed through\n",
    "        kwargs: Should contain \"month\" (integer) and \"days\" (list of ints)\n",
    "    Returns:\n",
    "        (Xnew, y, sample_weight)\n",
    "    \"\"\"\n",
    "    month = X.sample_kwargs['month']\n",
    "    days = X.sample_kwargs['days']\n",
    "    band_arr = getattr(X, TEMP_BAND)\n",
    "    date = pd.DatetimeIndex(tuple(pd.Timestamp(v) for v in band_arr.time.values))\n",
    "    arr = np.empty(band_arr.values.shape)\n",
    "    for year in YEARS:\n",
    "        for day in days:\n",
    "            idxes = np.where((date.day == day)&(date.year == year)&(date.month == month))[0]\n",
    "            slc = (idxes,\n",
    "                   slice(None),\n",
    "                   slice(None)\n",
    "                   )\n",
    "            one_day = band_arr.values[slc]\n",
    "            arr[slc] = (one_day - MONTH_MEANS.T2M.values)\n",
    "            assert np.abs(arr[slc].sum()) > 0\n",
    "    data_arr = xr.DataArray(arr, coords=band_arr.coords, dims=band_arr.dims, attrs=band_arr.attrs)\n",
    "    Xnew = ElmStore({TEMP_BAND: data_arr}, attrs=X.attrs, add_canvas=False)\n",
    "    del X\n",
    "    return (Xnew, y, sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature time series sampling\n",
    "\n",
    "Using the `sampler` function for the time series of July 2 data for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s, _, _ = sampler(7, [2]) # 2nd of July\n",
    "# we have only one DataArray - T2MMEAN\n",
    "s.T2MMEAN.std(axis=0).plot.pcolormesh(y='lat', x='lon')\n",
    "plt.title('Standard deviation of temperature resdiuals in time (C)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `elm.pipeline.steps` - Preprocessing and Transforms\n",
    "The next cells create some preprocessng and transformation steps we will use in the `elm.pipeline.Pipeline` a few cells later.\n",
    "Notes:\n",
    " * `elm.pipeline.steps.ModifySample` is a step that lets you call any function.  Your function should have the signature: `func(X, y=None, sample_weight=None, **kwargs):` and the function should return a tuple of `(X, y, sample_weight)` or just `X`, where in either case `X` is an `elm.readers.ElmStore` or `xarray.Dataset`.\n",
    " * `steps.Transform` takes a transformer instance like `IncrementalPCA()` and keyword argument `partial_fit_batches` which controls only the number of `partial_fit` operations for the `Transform` step itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from elm.pipeline import steps, Pipeline\n",
    "fixed_bins = steps.TSProbs(band=TEMP_BAND, num_bins=152, bin_size=0.5, log_probs=True)\n",
    "\n",
    "fixed_bins  # The repr below shows which parameters may be adjusted for \n",
    "            # time series binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting to multiple samples\n",
    "\n",
    "The `sampler` function we created above is called with two arguments, an integer for the month, and a list of days.  To fit to multiple samples, we need to pass in an `args_list` which is a list of 2-tuples in this case that are unpacked as arguments to each call to `sampler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "july_days = list(range(1, 32))\n",
    "np.random.shuffle(july_days)\n",
    "num_blocks = 4\n",
    "block_size = len(july_days) // num_blocks\n",
    "args_list = [(7, july_days[block: block_size + block])\n",
    "             for block in range(0, len(july_days), block_size)\n",
    "             if len(july_days) - block >= block_size]\n",
    "\n",
    "args_list # 4 samples of 7 random July days each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble varying `n_clusters`\n",
    "\n",
    "The next cell modifies the `ensemble_kwargs` argument to `elm.pipeline.train.train_step`.\n",
    "\n",
    " * `ensemble_init_func`: A function that takes \n",
    "   * `cls` (e.g. `sklearn.cluster.KMeans`), \n",
    "   * `model_kwargs` (the `cls`'s `__init__` keyword args), \n",
    "   * `**kwargs`, the `ensemble_kwargs` passed to `train_step`\n",
    " * `model_selection`: A function that determines how `Pipeline` instances are passed from generation to generation of ensemble.  \n",
    " * `model_selection_kwargs`: Passed to `model_selection`\n",
    "  * The model selection function should have a signature: `func(models, best_idxes=None, **kwargs)` \n",
    "  * `best_idxes` are indices in Pareto sorted best to worst fitness, where fitness is determined by `model_scoring` (see above in this notebook). \n",
    "  * `kwargs` will include `model_selection_kwargs` as well as `generation`, the current ensemble generation and `ngen` from the keywords given to `fit_ensemble`.\n",
    " * The `model_scoring` in this example uses `elm.model_selection.kmeans.kmeans_aic` to score the K-Means Akaike Information Criterion (AIC), a score which weights goodness-of-fit while penalizing for larger number of clusters. \n",
    " * `model_scoring` sets the weights for fitness sorting (minimize = -1 here to minimize AIC) as a list as long as the sequence of scores returned by the scoring function, defaulting to `model.score` if available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `elm.pipeline.Pipeline`\n",
    "\n",
    "To create a `Pipeline`, give a list of steps where each step is a class from `elm.pipeline.steps`, except the final step, which may be any estimator with a fit, transform, predict interface like most `scikit-learn` models.  Each step can be a tuple where the first item gives the step a name.\n",
    "\n",
    "There are two keyword arguments to `Pipeline`:\n",
    " * `scoring` a function called after each model fitting to score the model.  Here K-Means is scored with Akaike Information Criterion\n",
    " * `scoring_kwargs`: Should have a key `score_weights` and may also give the keys `needs_proba` or `needs_threshold`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from elm.model_selection.kmeans import kmeans_aic, kmeans_model_averaging\n",
    "pipe = Pipeline([('binning', fixed_bins), \n",
    "                 ('pca', steps.Transform(IncrementalPCA(), partial_fit_batches=2)),\n",
    "                 ('kmeans', MiniBatchKMeans())],\n",
    "                 scoring=kmeans_aic, \n",
    "                 scoring_kwargs=dict(score_weights=[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define `model_selection`, `ensemble_init_func` and other ensemble controls\n",
    "\n",
    "Here we define `ensemble_init_func` a function for initializing the ensemble with `Pipeline` instances of varying parameters and `model_selection`, a function which takes a list of `(tag, pipeline instance)` tuples and `best_idxes` (Pareto sorting indices) to modify the ensemble members carried forward to the next generation of `fit_ensemble`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_clusters_choices = range(4, 11)\n",
    "n_components_choices = range(3, 6)\n",
    "TOP_N = 4\n",
    "\n",
    "_num = 0\n",
    "\n",
    "def _next_name():\n",
    "    global _num\n",
    "    _num += 1\n",
    "    return 'new_model_{}'.format(_num)\n",
    "\n",
    "def new_pipe(pipe, num_bins, bin_size):\n",
    "    n_clusters = np.random.choice(n_clusters_choices)\n",
    "    n_components = np.random.choice(n_components_choices)      \n",
    "    print('New ensemble member - n_clusters: {} n_components: {} num_bins {}'.format(n_clusters, n_components, num_bins))\n",
    "    return pipe.new_with_params(kmeans__n_clusters=n_clusters,\n",
    "                                pca__n_components=n_components,\n",
    "                                binning__num_bins=num_bins,\n",
    "                                binning__bin_size=bin_size)\n",
    "\n",
    "def model_selection(models, best_idxes=None, **kwargs):\n",
    "    top = [models[idx] for idx in best_idxes[:TOP_N]]\n",
    "    if kwargs['generation'] < kwargs['ngen']:\n",
    "        tag, best = top[0]\n",
    "        new = [(_next_name(), new_pipe(best,\n",
    "                        best.get_params()['binning__num_bins'], \n",
    "                        best.get_params()['binning__bin_size']))\n",
    "               for idx in range(len(models) - len(top))]\n",
    "        return top + new\n",
    "    return best\n",
    "        \n",
    "def ensemble_init_func(pipe, **kwargs):\n",
    "    print('Calling ensemble_init_func with {} {} '.format(pipe, kwargs))\n",
    "    models = []\n",
    "    for num_bins, bin_size in zip([152, 76, 38], [0.5, 1, 2]):\n",
    "        for _ in range(5):\n",
    "            models.append(new_pipe(pipe, num_bins, bin_size))\n",
    "    return models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fit_ensemble` keyword arguments:\n",
    "\n",
    " * `ngen` is the number of generations where each generation has `model_selection` function called after it.\n",
    " * `ensemble_init_func` is a function returing a list of `elm.pipeline.Pipeline` instances\n",
    " * `model_selection` is a function to take a list of trained models and return a list of models\n",
    " * `model_selection_kwargs` are keyword arguments passed to `model_selection`\n",
    " * `models_share_sample=True`, the default, means to fit all ensemble members to one sample at a time in each generation, cycling to the next sample in the next generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble_kwargs = {\n",
    "    'ngen': len(args_list),\n",
    "    'ensemble_init_func': ensemble_init_func,\n",
    "    'model_selection': model_selection,\n",
    "    'model_selection_kwargs': {},\n",
    "    'models_share_sample': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call `fit_ensemble`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipe.fit_ensemble(sampler=sampler, args_list=args_list,client=client, **ensemble_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call `predict_many` with a `serialize` callable\n",
    "\n",
    "By default `predict_many` has the keyword argument `to_cube=True`, meaning to convert the 1-D prediction from the scikit-learn estimator to a 2-D raster with the coordinates of the input data.  \n",
    "\n",
    "Here we are calling `predict_many` with:\n",
    " * The `ensemble` keyword to control which models are used in prediction - by default it would have used `pipe.ensemble`, a list of 15 elements in this case, but we are limiting to the first two members\n",
    " * The `sampler` and `args_list` given to `fit_ensemble` (the `arg_list` or `sampler` may differ between fitting and prediction as long as the sampler function produces a sample of a consistent number of dimensions).\n",
    " * `serialize`, a function which serializes each prediction to avoid storing a large number of arrays in memory (the return value from `predict_many` is either a list of `ElmStore`s (y predictions) if `serialize` is `None`, otherwise a list of outputs from the `serialize` callable given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def serialize(y=None, X=None, tag=None, elm_predict_path='./predict'):\n",
    "    '''Example serialize callable for predict_many'''\n",
    "    # X is an ElmStore from the Pipeline and we have\n",
    "    # kept \"sample_kwargs\" in attrs\n",
    "    y.predict.plot.pcolormesh(levels=np.arange(np.max(y.predict.values)))\n",
    "    plt.title('Climatic regions based on {}'.format(X.sample_kwargs))\n",
    "    plt.show()\n",
    "    return True\n",
    "pred = pipe.predict_many(ensemble=pipe.ensemble[:2], sampler=sampler, args_list=args_list, serialize=serialize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirming we have 15 ensemble members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(pipe.ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the return values of `serialize` that reduced memory footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serializing the `Pipeline` for prediction later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(pipe, 'pipe.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = joblib.load('pipe.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
