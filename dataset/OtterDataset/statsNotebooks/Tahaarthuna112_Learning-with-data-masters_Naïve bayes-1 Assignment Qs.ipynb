{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d465ee3c-69e0-4ed8-b066-ca1e64d96cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Bayes' theorem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80b6f5d-a9b8-4308-9a2c-fd674ae6fe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes' theorem, named after the Reverend Thomas Bayes, is a fundamental theorem in probability theory that describes how to update the probability of a hypothesis in light of new evidence. Mathematically, it is represented as:\n",
    "\n",
    "\\[ P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)} \\]\n",
    "\n",
    "Where:\n",
    "- \\( P(A|B) \\) is the posterior probability of hypothesis A given the evidence B.\n",
    "- \\( P(B|A) \\) is the likelihood of the evidence B given that hypothesis A is true.\n",
    "- \\( P(A) \\) is the prior probability of hypothesis A before considering the evidence.\n",
    "- \\( P(B) \\) is the probability of the evidence B.\n",
    "\n",
    "In essence, Bayes' theorem provides a way to update our belief in a hypothesis (the posterior probability) based on new evidence. It's widely used in various fields, including statistics, machine learning, and artificial intelligence, particularly in tasks involving inference or making predictions based on uncertain information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da79e04-0613-468b-83bc-84bf75b87def",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the formula for Bayes' theorem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44513002-8b0c-471f-a110-40067970150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes' theorem is typically expressed as follows:\n",
    "\n",
    "\\[ P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)} \\]\n",
    "\n",
    "Where:\n",
    "- \\( P(A|B) \\) is the posterior probability of hypothesis A given the evidence B.\n",
    "- \\( P(B|A) \\) is the likelihood of the evidence B given that hypothesis A is true.\n",
    "- \\( P(A) \\) is the prior probability of hypothesis A before considering the evidence.\n",
    "- \\( P(B) \\) is the probability of the evidence B.\n",
    "\n",
    "This formula provides a way to update our belief in a hypothesis (the posterior probability) based on new evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b64cee1-ae2c-4f7e-9ec6-f3e05a7b8a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How is Bayes' theorem used in practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554c39c4-1b8e-43f9-a9d9-a620a3c6baf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes' theorem is used in various practical applications across different fields. Some common applications include:\n",
    "\n",
    "1. Medical Diagnosis: Bayes' theorem is used in medical diagnosis to update the probability of a disease given certain symptoms or test results. Physicians can use prior knowledge about the prevalence of a disease, the accuracy of a diagnostic test, and the patient's symptoms to calculate the probability of a patient having the disease.\n",
    "\n",
    "2. Spam Filtering: In email spam filtering, Bayes' theorem is employed to classify emails as either spam or non-spam. The algorithm learns from a set of training data, updating the probability of an email being spam based on the occurrence of certain words or features in the email content.\n",
    "\n",
    "3. Weather Forecasting: Bayes' theorem is utilized in weather forecasting to update the probability distribution of future weather conditions based on current observations and past data. Meteorologists incorporate data from various sources, such as satellite images, radar observations, and historical weather patterns, to make probabilistic forecasts.\n",
    "\n",
    "4. Machine Learning: Bayes' theorem serves as the foundation for Bayesian machine learning methods, such as Bayesian networks and Bayesian inference. These methods enable the modeling of uncertainty and the updating of beliefs based on observed data, making them valuable for tasks such as classification, regression, and clustering.\n",
    "\n",
    "5. Financial Modeling: In finance, Bayes' theorem can be applied to update the probability of various financial events based on new information, helping investors make informed decisions under uncertainty. Bayesian methods are also used in risk assessment, portfolio optimization, and algorithmic trading.\n",
    "\n",
    "6. Natural Language Processing: Bayes' theorem is used in natural language processing tasks such as text classification, sentiment analysis, and document clustering. It helps in determining the likelihood of a particular class (e.g., topic, sentiment) given the observed features in the text.\n",
    "\n",
    "Overall, Bayes' theorem provides a principled framework for updating beliefs in the face of uncertainty, making it a powerful tool in decision-making and inference in various real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b50dd09-b842-4543-836d-6e5eb06ea9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the relationship between Bayes' theorem and conditional probability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8669f034-0a75-44f1-9e2e-61b2cdf882b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes' theorem and conditional probability are closely related concepts, with Bayes' theorem essentially being an extension of conditional probability. \n",
    "\n",
    "Conditional probability is the probability of an event occurring given that another event has already occurred. It is represented as \\(P(A|B)\\), where \\(A\\) and \\(B\\) are events, and it can be calculated using the formula:\n",
    "\n",
    "\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\]\n",
    "\n",
    "Bayes' theorem, on the other hand, provides a way to update our beliefs about the probability of an event (hypothesis) given new evidence. It relates the conditional probability of an event to its prior probability and the likelihood of the evidence. Bayes' theorem is represented as:\n",
    "\n",
    "\\[P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\\]\n",
    "\n",
    "Where:\n",
    "- \\(P(A|B)\\) is the posterior probability of event \\(A\\) given event \\(B\\).\n",
    "- \\(P(B|A)\\) is the likelihood of event \\(B\\) given event \\(A\\).\n",
    "- \\(P(A)\\) is the prior probability of event \\(A\\).\n",
    "- \\(P(B)\\) is the probability of event \\(B\\).\n",
    "\n",
    "In summary, Bayes' theorem provides a way to update our prior beliefs (prior probability) about the occurrence of an event (hypothesis) given new evidence, by incorporating the likelihood of observing that evidence under different hypotheses (likelihood) and the overall probability of observing the evidence (marginal likelihood). Thus, Bayes' theorem and conditional probability are interrelated, with Bayes' theorem being a more general framework that encompasses conditional probability as a special case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af002159-ff23-4fb5-9845-6449e14ba26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d03a95-2ec9-4e47-a444-9d9448869724",
   "metadata": {},
   "outputs": [],
   "source": [
    "Choosing the appropriate type of Naive Bayes classifier for a given problem depends on several factors, including the nature of the problem, the characteristics of the data, and the assumptions that can be made about the data distribution. Here's a brief overview of the common types of Naive Bayes classifiers and considerations for choosing the right one:\n",
    "\n",
    "1. Gaussian Naive Bayes:\n",
    "   - Assumes that the features follow a Gaussian (normal) distribution.\n",
    "   - Suitable for continuous features.\n",
    "   - Appropriate when the features are continuous and have a roughly normal distribution.\n",
    "\n",
    "2. Multinomial Naive Bayes:\n",
    "   - Assumes that the features are generated from a multinomial distribution.\n",
    "   - Typically used for text classification tasks, where features represent word counts or frequencies.\n",
    "   - Suitable for features that describe the frequency of occurrences of different categories.\n",
    "\n",
    "3. Bernoulli Naive Bayes:\n",
    "   - Assumes that features are binary-valued (e.g., presence or absence).\n",
    "   - Suitable for binary feature vectors or text classification tasks with binary feature representations (e.g., bag-of-words).\n",
    "   - Appropriate when features are binary indicators of whether certain events occur or not.\n",
    "\n",
    "Choosing the right type of Naive Bayes classifier involves considering the following:\n",
    "\n",
    "- Data Distribution: Understanding the distribution of features in your dataset is crucial. If your features are continuous and approximately normally distributed, Gaussian Naive Bayes might be suitable. For binary features, Bernoulli Naive Bayes is appropriate. For features representing counts or frequencies (e.g., word counts), Multinomial Naive Bayes is often used.\n",
    "\n",
    "- Feature Types: Consider the nature of your features. Are they continuous, binary, or representing counts/frequencies? Choose a Naive Bayes classifier that aligns with the type of features in your dataset.\n",
    "\n",
    "- Assumptions: Naive Bayes classifiers make strong assumptions about the independence of features. While these assumptions might not hold true in real-world data, Naive Bayes classifiers can still perform well in practice. Consider whether the independence assumption is reasonable for your dataset.\n",
    "\n",
    "- Performance: It's essential to evaluate the performance of different Naive Bayes classifiers on your dataset using appropriate metrics (e.g., accuracy, precision, recall, F1-score). Choose the classifier that yields the best performance for your specific problem.\n",
    "\n",
    "- Scalability: Consider the scalability of the classifier with respect to the size of your dataset and the number of features. Some Naive Bayes classifiers might be more scalable than others, depending on the computational resources available.\n",
    "\n",
    "In summary, the choice of Naive Bayes classifier depends on a thorough understanding of the problem, the characteristics of the data, and the assumptions underlying each type of classifier. Experimentation and evaluation are key to selecting the most suitable classifier for your specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaa8211-b06b-45b8-8af2-15c5d5892b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Assignment:\n",
    "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive \n",
    "Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of \n",
    "each feature value for each class:\n",
    "\n",
    "Class\t X1=1 X1=2 \tX1=3 \tX2=1 \tX2=2 \tX2=3\t X2=4\n",
    "\n",
    " A\t 3\t 3\t 4\t 4\t 3\t 3\t 3\n",
    "\n",
    " B\t 2\t 2\t 1\t 2\t 2\t 2\t 3\n",
    "\n",
    "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance \n",
    "to belong to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4fdb95-216f-4e72-aa6b-5530b34725f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "To classify the new instance with features \\( X_1 = 3 \\) and \\( X_2 = 4 \\) using Naive Bayes, we need to calculate the posterior probabilities for each class \\( A \\) and \\( B \\) and then choose the class with the highest probability.\n",
    "\n",
    "Given:\n",
    "- Equal prior probabilities for each class (\\( P(A) = P(B) = 0.5 \\)).\n",
    "- We'll use the multinomial Naive Bayes assumption.\n",
    "\n",
    "First, we need to calculate the likelihoods of the features given each class. We'll assume Laplace smoothing (add-one smoothing) to avoid zero probabilities:\n",
    "\n",
    "\\[ P(X_1 = 3 | A) = \\frac{4 + 1}{16 + 7} = \\frac{5}{23} \\]\n",
    "\\[ P(X_1 = 3 | B) = \\frac{1 + 1}{16 + 7} = \\frac{2}{23} \\]\n",
    "\n",
    "\\[ P(X_2 = 4 | A) = \\frac{3 + 1}{16 + 7} = \\frac{4}{23} \\]\n",
    "\\[ P(X_2 = 4 | B) = \\frac{3 + 1}{16 + 7} = \\frac{4}{23} \\]\n",
    "\n",
    "Next, we calculate the likelihood of observing the feature values given each class:\n",
    "\n",
    "\\[ P(X_1 = 3, X_2 = 4 | A) = P(X_1 = 3 | A) \\times P(X_2 = 4 | A) = \\frac{5}{23} \\times \\frac{4}{23} = \\frac{20}{529} \\]\n",
    "\\[ P(X_1 = 3, X_2 = 4 | B) = P(X_1 = 3 | B) \\times P(X_2 = 4 | B) = \\frac{2}{23} \\times \\frac{4}{23} = \\frac{8}{529} \\]\n",
    "\n",
    "Now, we calculate the posterior probabilities using Bayes' theorem:\n",
    "\n",
    "\\[ P(A | X_1 = 3, X_2 = 4) = \\frac{P(X_1 = 3, X_2 = 4 | A) \\times P(A)}{P(X_1 = 3, X_2 = 4)} \\]\n",
    "\\[ P(B | X_1 = 3, X_2 = 4) = \\frac{P(X_1 = 3, X_2 = 4 | B) \\times P(B)}{P(X_1 = 3, X_2 = 4)} \\]\n",
    "\n",
    "Since the denominators are the same for both classes, we only need to compare the numerators. \n",
    "\n",
    "For class A:\n",
    "\\[ P(A | X_1 = 3, X_2 = 4) = \\frac{\\frac{20}{529} \\times 0.5}{P(X_1 = 3, X_2 = 4)} \\]\n",
    "\n",
    "For class B:\n",
    "\\[ P(B | X_1 = 3, X_2 = 4) = \\frac{\\frac{8}{529} \\times 0.5}{P(X_1 = 3, X_2 = 4)} \\]\n",
    "\n",
    "Now, we compare the posterior probabilities to determine the predicted class. We'll calculate \\( P(X_1 = 3, X_2 = 4) \\) separately and normalize the probabilities to sum to 1.\n",
    "\n",
    "\\[ P(X_1 = 3, X_2 = 4) = P(X_1 = 3, X_2 = 4 | A) \\times P(A) + P(X_1 = 3, X_2 = 4 | B) \\times P(B) \\]\n",
    "\\[ = \\frac{20}{529} \\times 0.5 + \\frac{8}{529} \\times 0.5 \\]\n",
    "\\[ = \\frac{28}{529} \\]\n",
    "\n",
    "\\[ P(A | X_1 = 3, X_2 = 4) = \\frac{\\frac{20}{529} \\times 0.5}{\\frac{28}{529}} = \\frac{20}{28} = \\frac{5}{7} \\]\n",
    "\\[ P(B | X_1 = 3, X_2 = 4) = \\frac{\\frac{8}{529} \\times 0.5}{\\frac{28}{529}} = \\frac{8}{28} = \\frac{2}{7} \\]\n",
    "\n",
    "Since \\( P(A | X_1 = 3, X_2 = 4) > P(B | X_1 = 3, X_2 = 4) \\), Naive Bayes would predict that the new instance belongs to class A."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
