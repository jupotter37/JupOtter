{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WEEK-14, ASS NO-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What are the key features of the wine quality data set? Discuss the importance of each feature in\n",
    "predicting the quality of wine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wine quality dataset is often used in machine learning and statistics for regression tasks and classification problems. The dataset typically includes various physicochemical properties of wine and a quality rating given by wine experts. Here are the key features commonly found in the wine quality dataset, along with their importance in predicting wine quality:\n",
    "\n",
    "### Key Features of the Wine Quality Dataset\n",
    "\n",
    "1. **Fixed Acidity**\n",
    "   - **Description**: This measures the acidity in the wine, expressed in grams per liter of tartaric acid.\n",
    "   - **Importance**: Higher acidity can contribute to a wine's freshness and crispness. It influences taste balance and can affect aging potential.\n",
    "\n",
    "2. **Volatile Acidity**\n",
    "   - **Description**: This refers to the amount of acetic acid in wine, which can lead to an unpleasant vinegar taste if too high.\n",
    "   - **Importance**: Volatile acidity is crucial for quality control; high levels can indicate spoilage or poor winemaking practices. \n",
    "\n",
    "3. **Citric Acid**\n",
    "   - **Description**: A natural acid found in citrus fruits; in wines, it contributes to the freshness and complexity of flavor.\n",
    "   - **Importance**: A higher citric acid content can improve the flavor profile and reduce the perception of harshness in a wine.\n",
    "\n",
    "4. **Residual Sugar**\n",
    "   - **Description**: The amount of sugar remaining after fermentation, expressed in grams per liter.\n",
    "   - **Importance**: Sugar levels can influence sweetness, which is a significant factor in wine taste. Too much residual sugar may indicate poor fermentation.\n",
    "\n",
    "5. **Chlorides**\n",
    "   - **Description**: This measures the salt content in wine, expressed in grams per liter.\n",
    "   - **Importance**: Chloride levels can impact the mouthfeel and balance of wine. Excessive levels may indicate contamination or poor wine quality.\n",
    "\n",
    "6. **Free Sulfur Dioxide**\n",
    "   - **Description**: This represents the amount of sulfur dioxide that is free to act as a preservative in the wine.\n",
    "   - **Importance**: It helps to protect the wine from oxidation and spoilage, influencing the wine's shelf life and overall quality.\n",
    "\n",
    "7. **Total Sulfur Dioxide**\n",
    "   - **Description**: The total amount of sulfur dioxide, including both free and bound forms.\n",
    "   - **Importance**: Total sulfur dioxide is critical for understanding a wineâ€™s preservation state. High levels can indicate excessive use of preservatives.\n",
    "\n",
    "8. **Density**\n",
    "   - **Description**: This measures the mass of wine per unit volume, which can indicate sugar and alcohol content.\n",
    "   - **Importance**: Density can provide insights into the wine's alcohol level and sugar content, both of which affect quality perception.\n",
    "\n",
    "9. **pH**\n",
    "   - **Description**: This indicates the acidity or alkalinity of the wine, on a scale of 0 to 14.\n",
    "   - **Importance**: pH plays a vital role in stability and aging potential. It affects flavor perception and microbial stability.\n",
    "\n",
    "10. **Alcohol Content**\n",
    "    - **Description**: This is the percentage of alcohol by volume in the wine.\n",
    "    - **Importance**: Alcohol content significantly affects flavor and mouthfeel. Higher alcohol levels may lead to a fuller body and can influence the overall quality rating.\n",
    "\n",
    "11. **Quality**\n",
    "    - **Description**: This is the target variable, typically a score between 0 and 10 given by wine experts.\n",
    "    - **Importance**: The quality rating is what the predictive models aim to forecast based on the physicochemical features.\n",
    "\n",
    "### Importance of Each Feature\n",
    "\n",
    "- **Predictive Power**: Each feature contributes differently to the prediction of wine quality. Features like volatile acidity and alcohol content often have a strong correlation with quality ratings.\n",
    "- **Sensory Attributes**: Features influence sensory perceptions such as taste, aroma, and mouthfeel, which are crucial for quality assessment.\n",
    "- **Quality Control**: Understanding these features can aid winemakers in producing higher quality wines and maintaining consistency.\n",
    "- **Consumer Preference**: Knowledge of how these features correlate with quality can help in market positioning and meeting consumer expectations.\n",
    "\n",
    "By analyzing the relationships between these features and the quality ratings, machine learning models can be trained to predict wine quality based on its chemical composition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. How did you handle missing data in the wine quality data set during the feature engineering process?\n",
    "Discuss the advantages and disadvantages of different imputation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling missing data is a crucial step in the feature engineering process, as it can significantly impact the quality of a machine learning model. Here are common strategies for handling missing data in the wine quality dataset, along with their advantages and disadvantages:\n",
    "\n",
    "### Common Techniques for Handling Missing Data\n",
    "\n",
    "1. **Removal of Missing Values**\n",
    "   - **Description**: Remove rows (or columns) that contain missing values.\n",
    "   - **Advantages**:\n",
    "     - Simple and straightforward.\n",
    "     - Avoids introducing bias from imputation.\n",
    "   - **Disadvantages**:\n",
    "     - Can lead to loss of valuable data, especially if the missing values are significant.\n",
    "     - Reduces the dataset size, which may impact model performance, especially with smaller datasets.\n",
    "\n",
    "2. **Mean/Median/Mode Imputation**\n",
    "   - **Description**: Replace missing values with the mean, median, or mode of the respective feature.\n",
    "     - **Mean**: Used for continuous variables.\n",
    "     - **Median**: More robust to outliers than mean; suitable for skewed distributions.\n",
    "     - **Mode**: Commonly used for categorical variables.\n",
    "   - **Advantages**:\n",
    "     - Simple to implement and computationally efficient.\n",
    "     - Preserves the dataset size.\n",
    "   - **Disadvantages**:\n",
    "     - Can introduce bias, especially if the data is not missing completely at random.\n",
    "     - Reduces variability, which may affect model performance.\n",
    "\n",
    "3. **K-Nearest Neighbors (KNN) Imputation**\n",
    "   - **Description**: Replace missing values by finding the K-nearest neighbors in the dataset and using their values to estimate the missing data.\n",
    "   - **Advantages**:\n",
    "     - Considers the relationships between features, which can lead to more accurate imputations.\n",
    "     - Can handle both continuous and categorical data.\n",
    "   - **Disadvantages**:\n",
    "     - Computationally intensive, especially for large datasets.\n",
    "     - Performance depends on the choice of K; if K is too small or too large, it may lead to inaccurate imputations.\n",
    "\n",
    "4. **Regression Imputation**\n",
    "   - **Description**: Use regression models to predict and replace missing values based on other features in the dataset.\n",
    "   - **Advantages**:\n",
    "     - Takes into account relationships between features, potentially providing more accurate imputations.\n",
    "     - Can improve model performance if done correctly.\n",
    "   - **Disadvantages**:\n",
    "     - Introduces additional complexity to the modeling process.\n",
    "     - Risk of overfitting the model if not done carefully.\n",
    "\n",
    "5. **Multiple Imputation**\n",
    "   - **Description**: Generate multiple imputed datasets and combine results, using statistical methods to account for uncertainty.\n",
    "   - **Advantages**:\n",
    "     - Provides a more comprehensive way to handle missing data by reflecting the uncertainty associated with missing values.\n",
    "     - Can lead to better estimates of model parameters.\n",
    "   - **Disadvantages**:\n",
    "     - More complex and computationally intensive.\n",
    "     - Requires careful implementation and understanding of the underlying statistical concepts.\n",
    "\n",
    "6. **Using Algorithms that Support Missing Values**\n",
    "   - **Description**: Some machine learning algorithms (like certain tree-based methods) can handle missing values directly without needing imputation.\n",
    "   - **Advantages**:\n",
    "     - No need for prior imputation, simplifying the preprocessing step.\n",
    "     - Preserves all data points in the model training.\n",
    "   - **Disadvantages**:\n",
    "     - Not all algorithms can handle missing values.\n",
    "     - The performance may still vary depending on the dataset and the extent of missingness.\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. What are the key factors that affect students' performance in exams? How would you go about\n",
    "analyzing these factors using statistical techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key factors that can affect students' performance in exams typically include a variety of personal, social, and environmental variables. Here are some of the key factors and how you might analyze them using statistical techniques:\n",
    "\n",
    "### Key Factors Affecting Students' Exam Performance:\n",
    "\n",
    "1. **Study Time**\n",
    "   - **Impact**: The amount of time students spend studying is often directly related to exam performance.\n",
    "   - **Statistical Analysis**: Linear regression can be used to determine the correlation between study hours and exam scores.\n",
    "\n",
    "2. **Sleep Quality**\n",
    "   - **Impact**: Sleep affects cognitive function and memory retention, which are crucial for exam performance.\n",
    "   - **Statistical Analysis**: Use correlation tests (Pearson or Spearman) to assess the relationship between hours of sleep and exam scores.\n",
    "\n",
    "3. **Attendance**\n",
    "   - **Impact**: Students who attend classes regularly tend to perform better due to exposure to class material.\n",
    "   - **Statistical Analysis**: Logistic regression or ANOVA could be used to analyze the impact of attendance on exam results.\n",
    "\n",
    "4. **Socioeconomic Status (SES)**\n",
    "   - **Impact**: SES can affect access to resources like tutors, study materials, and a conducive learning environment.\n",
    "   - **Statistical Analysis**: A chi-square test or regression analysis could be employed to see how SES categories affect exam performance.\n",
    "\n",
    "5. **Parental Involvement**\n",
    "   - **Impact**: Parental involvement in a student's education may boost performance through guidance and motivation.\n",
    "   - **Statistical Analysis**: Use multiple regression models to determine the impact of parental involvement on performance.\n",
    "\n",
    "6. **Prior Academic Performance**\n",
    "   - **Impact**: Previous academic achievements may be indicative of future exam success.\n",
    "   - **Statistical Analysis**: Use correlation analysis or regression to examine how past performance (e.g., GPA) influences exam scores.\n",
    "\n",
    "7. **Mental Health**\n",
    "   - **Impact**: Anxiety, stress, and other mental health issues can severely affect performance.\n",
    "   - **Statistical Analysis**: Conduct t-tests or regression to analyze the impact of mental health scores (from surveys) on exam performance.\n",
    "\n",
    "8. **Extracurricular Activities**\n",
    "   - **Impact**: Involvement in extracurriculars can either positively (e.g., leadership skills) or negatively (e.g., time distraction) affect performance.\n",
    "   - **Statistical Analysis**: Use ANOVA or multiple regression to compare performance between students with different levels of involvement in extracurriculars.\n",
    "\n",
    "### Steps to Analyze These Factors Using Statistical Techniques:\n",
    "\n",
    "1. **Data Collection**:\n",
    "   - Collect relevant data on the factors such as hours studied, attendance, sleep quality, and exam scores.\n",
    "   - Use surveys, academic records, and other methods to gather this information.\n",
    "\n",
    "2. **Exploratory Data Analysis (EDA)**:\n",
    "   - Visualize the data using histograms, boxplots, and scatterplots to understand distributions and potential relationships.\n",
    "   - Calculate descriptive statistics like mean, median, and standard deviation.\n",
    "\n",
    "3. **Correlation Analysis**:\n",
    "   - Use Pearson correlation to examine the strength of the linear relationships between continuous variables like study time, sleep, and exam scores.\n",
    "   - Use Spearman correlation if you expect a monotonic but not necessarily linear relationship.\n",
    "\n",
    "4. **Regression Analysis**:\n",
    "   - **Linear Regression**: Use this technique to predict exam scores based on continuous independent variables such as study time and sleep.\n",
    "   - **Multiple Regression**: Analyze the combined impact of several factors (study time, attendance, parental involvement) on exam performance.\n",
    "   - **Logistic Regression**: If you're categorizing performance into \"Pass\" or \"Fail\", logistic regression can be used to model the probability of passing based on various factors.\n",
    "\n",
    "5. **ANOVA (Analysis of Variance)**:\n",
    "   - If you have categorical variables (e.g., different study methods or extracurricular involvement levels), ANOVA can help you compare exam performance across groups.\n",
    "\n",
    "6. **T-tests or Chi-Square Tests**:\n",
    "   - If you want to compare exam performance between two groups (e.g., students who sleep more than 7 hours vs. those who sleep less), use t-tests. Chi-square tests are useful for categorical data.\n",
    "\n",
    "7. **Factor Analysis or Principal Component Analysis (PCA)**:\n",
    "   - These techniques help in identifying which underlying factors (e.g., parental involvement, socioeconomic status) contribute the most to exam performance.\n",
    "\n",
    "By applying these statistical methods, you can gain insights into which factors most strongly affect studentsâ€™ exam performance, helping educators and policymakers design interventions to improve student outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Describe the process of feature engineering in the context of the student performance data set. How\n",
    "did you select and transform the variables for your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering is a crucial step in building predictive models, particularly for datasets like student performance data, where the raw data may need to be transformed to optimize the model's performance. Hereâ€™s an overview of how you could approach feature engineering in the context of a student performance dataset, which may contain variables like study time, attendance, parental involvement, and exam scores.\n",
    "\n",
    "### Steps in the Feature Engineering Process:\n",
    "\n",
    "1. **Understanding the Dataset**:\n",
    "   - The first step is to thoroughly explore the dataset and understand the nature of each variable. In the context of student performance data, common variables might include:\n",
    "     - Study time (continuous)\n",
    "     - Attendance (categorical or continuous)\n",
    "     - Parental involvement (categorical or ordinal)\n",
    "     - Socioeconomic status (categorical)\n",
    "     - Exam scores (continuous, target variable)\n",
    "     - Sleep quality (ordinal)\n",
    "     - Extracurricular activities (categorical)\n",
    "\n",
    "2. **Handling Missing Data**:\n",
    "   - **Imputation**: Use appropriate imputation techniques to fill in missing values (e.g., mean/median imputation for continuous variables, mode for categorical variables). For more complex relationships, consider KNN or regression imputation.\n",
    "   - **Handling Outliers**: Identify outliers using techniques like z-scores or IQR. Decide whether to remove, cap, or transform outliers.\n",
    "\n",
    "3. **Feature Selection**:\n",
    "   - **Correlation Analysis**: Perform correlation analysis to assess the relationship between independent variables and the target variable (exam scores). Remove features that have little to no correlation with exam scores, as they may not contribute to the modelâ€™s performance.\n",
    "   - **Variance Threshold**: Use a variance threshold to filter out variables with very low variance, which contribute little information to the model.\n",
    "   - **Domain Knowledge**: Use domain expertise to select relevant features. For example, parental involvement, study time, and attendance are likely to be important predictors of performance, while less relevant features (like a studentâ€™s favorite hobby) may be dropped.\n",
    "\n",
    "4. **Feature Transformation**:\n",
    "   - **Normalization/Standardization**: For continuous variables (e.g., study time, exam scores), normalize or standardize the data so that all features are on a similar scale. This is especially important for algorithms like linear regression or SVM.\n",
    "   - **Log Transformation**: If a variable (e.g., study time) is highly skewed, a log transformation can make it more normally distributed, which may improve model performance.\n",
    "   - **Polynomial Features**: Consider generating polynomial features (e.g., square of study time) if you suspect non-linear relationships between the features and the target variable.\n",
    "\n",
    "5. **Encoding Categorical Variables**:\n",
    "   - **Label Encoding**: For ordinal variables like parental involvement (e.g., low, medium, high), use label encoding to assign a meaningful numerical order.\n",
    "   - **One-Hot Encoding**: For nominal categorical variables like socioeconomic status or extracurricular activities (no inherent order), use one-hot encoding to create binary variables that represent each category.\n",
    "   - **Binary Encoding**: If there are many unique categories (e.g., school or location), binary encoding can reduce the dimensionality of the one-hot encoded features.\n",
    "\n",
    "6. **Interaction Features**:\n",
    "   - **Creating Interaction Features**: Look for relationships between variables. For example, you could create interaction terms between \"study time\" and \"attendance\" if you believe that higher attendance enhances the effect of study time on exam scores.\n",
    "   - **Pairwise Products**: In some cases, multiplying features can capture interactions. For example, the product of parental involvement and study time might be a strong predictor of performance.\n",
    "\n",
    "7. **Feature Binning**:\n",
    "   - **Discretization**: Convert continuous variables like study time into categorical bins (e.g., \"low\", \"medium\", \"high\" study time). This can help the model capture non-linear relationships.\n",
    "   - **Quantile Binning**: Binning students based on percentiles (e.g., top 10% for attendance) can provide additional insights.\n",
    "\n",
    "8. **Dimensionality Reduction**:\n",
    "   - **PCA (Principal Component Analysis)**: If the dataset has a large number of features, consider PCA to reduce dimensionality and eliminate redundant information while retaining the variance in the data.\n",
    "   - **Feature Importance**: After fitting an initial model (e.g., decision trees or random forest), analyze feature importance scores to remove less impactful features and improve computational efficiency.\n",
    "\n",
    "9. **Creating New Features**:\n",
    "   - **Aggregating Variables**: Create composite features. For example, you could sum up \"study time\" and \"sleep time\" to create a new feature representing total effective hours per day.\n",
    "   - **Ratios**: Calculate ratios such as \"study time per week\" to \"total free time\" or \"attendance rate,\" which may better represent the studentâ€™s commitment than the raw numbers.\n",
    "   - **Temporal Features**: If the dataset includes information on the time or date (e.g., when assignments were submitted), you can create new time-related features like \"submission time before deadline\" to see if this affects performance.\n",
    "\n",
    "### Feature Selection and Transformation in the Model\n",
    "\n",
    "1. **Preprocessing**: \n",
    "   - After selecting and transforming the features, split the data into training and testing sets.\n",
    "   - Apply preprocessing techniques like normalization or encoding within cross-validation to avoid data leakage.\n",
    "\n",
    "2. **Modeling**:\n",
    "   - Fit different models (linear regression, decision trees, random forests) to see which one performs best on the transformed features.\n",
    "   - Use feature importance metrics from models like random forests to further refine which features to keep or discard.\n",
    "\n",
    "3. **Evaluation**:\n",
    "   - Evaluate model performance using metrics like accuracy, mean squared error (for regression), or F1 score (for classification). Conduct cross-validation to avoid overfitting.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Load the wine quality data set and perform exploratory data analysis (EDA) to identify the distribution\n",
    "of each feature. Which feature(s) exhibit non-normality, and what transformations could be applied to\n",
    "these features to improve normality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conduct exploratory data analysis (EDA) on the wine quality dataset and identify the distribution of each feature, we'll need to follow a systematic approach. This process will include loading the dataset, visualizing the distribution of features, checking for non-normality, and suggesting transformations to improve normality. \n",
    "\n",
    "Hereâ€™s a detailed plan for performing EDA and addressing non-normality:\n",
    "\n",
    "### Steps for EDA on the Wine Quality Dataset:\n",
    "\n",
    "1. **Loading the Dataset**:\n",
    "   - The wine quality dataset typically contains features like acidity, sugar levels, pH, alcohol content, and the target variable (wine quality). \n",
    "   - Begin by loading the dataset into a DataFrame (using pandas in Python).\n",
    "   \n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   import seaborn as sns\n",
    "   import matplotlib.pyplot as plt\n",
    "   from scipy.stats import shapiro, normaltest\n",
    "   import numpy as np\n",
    "   \n",
    "   # Load the dataset\n",
    "   wine_data = pd.read_csv(\"winequality.csv\")  # Example filepath, adjust as necessary\n",
    "   ```\n",
    "\n",
    "2. **Exploratory Data Analysis (EDA)**:\n",
    "   \n",
    "   - **Summary Statistics**: Get an initial sense of each feature by computing summary statistics.\n",
    "   \n",
    "   ```python\n",
    "   # Summary statistics\n",
    "   print(wine_data.describe())\n",
    "   ```\n",
    "\n",
    "   This will give you the mean, median, variance, and percentiles for continuous features like `fixed acidity`, `residual sugar`, `alcohol`, etc.\n",
    "   \n",
    "   - **Visualization of Distributions**: Plot histograms, box plots, and density plots for each feature to visualize the data distribution.\n",
    "   \n",
    "   ```python\n",
    "   # Plot histograms for each feature\n",
    "   wine_data.hist(bins=15, figsize=(15, 10))\n",
    "   plt.show()\n",
    "   \n",
    "   # Use seaborn to plot density plots\n",
    "   for column in wine_data.columns:\n",
    "       sns.kdeplot(wine_data[column], shade=True)\n",
    "       plt.title(f'Density Plot of {column}')\n",
    "       plt.show()\n",
    "   ```\n",
    "\n",
    "3. **Identifying Non-Normal Features**:\n",
    "   \n",
    "   - Use the **Shapiro-Wilk test** or **D'Agostino and Pearsonâ€™s test** to statistically assess whether a feature follows a normal distribution. A small p-value (typically < 0.05) indicates that the data is not normally distributed.\n",
    "   \n",
    "   ```python\n",
    "   # Shapiro-Wilk test for normality\n",
    "   for column in wine_data.columns:\n",
    "       stat, p = shapiro(wine_data[column])\n",
    "       print(f'{column}: p-value={p}')\n",
    "   ```\n",
    "\n",
    "4. **Interpreting Results of Normality Tests**:\n",
    "   \n",
    "   Based on the histograms, density plots, and normality test p-values, certain features might exhibit non-normality. Common wine features that may exhibit non-normal distributions include:\n",
    "   - **Residual Sugar**: Tends to be right-skewed, as most wines have low sugar levels, but a few have much higher levels.\n",
    "   - **Alcohol**: Often right-skewed, with most wines having moderate alcohol content and a few having higher values.\n",
    "   - **pH**: May show slight skewness or deviations from normality.\n",
    "   - **Sulfur Dioxide Levels (Free SO2, Total SO2)**: Frequently skewed due to regulations or natural chemical processes in winemaking.\n",
    "   \n",
    "5. **Applying Transformations**:\n",
    "\n",
    "   For features exhibiting non-normality, consider the following transformations:\n",
    "\n",
    "   - **Log Transformation**: This can be applied to right-skewed distributions (like residual sugar and alcohol) to make them more symmetric.\n",
    "   \n",
    "     ```python\n",
    "     wine_data['log_residual_sugar'] = np.log1p(wine_data['residual_sugar'])  # Adding 1 to avoid log(0)\n",
    "     wine_data['log_alcohol'] = np.log1p(wine_data['alcohol'])\n",
    "     ```\n",
    "\n",
    "   - **Square Root Transformation**: Another option for mildly skewed data, especially when dealing with positive values.\n",
    "   \n",
    "     ```python\n",
    "     wine_data['sqrt_residual_sugar'] = np.sqrt(wine_data['residual_sugar'])\n",
    "     ```\n",
    "\n",
    "   - **Box-Cox Transformation**: This is a more flexible transformation that can handle both positive and negative skewness. It automatically determines the optimal transformation parameter.\n",
    "   \n",
    "     ```python\n",
    "     from scipy.stats import boxcox\n",
    "\n",
    "     # Apply Box-Cox transformation (only works on positive data)\n",
    "     wine_data['boxcox_sulfur_dioxide'] = boxcox(wine_data['total_sulfur_dioxide'] + 1)[0]\n",
    "     ```\n",
    "\n",
    "   - **Z-score Normalization**: For normally distributed features with outliers, scaling may help reduce the impact of outliers on the model.\n",
    "   \n",
    "     ```python\n",
    "     from sklearn.preprocessing import StandardScaler\n",
    "     scaler = StandardScaler()\n",
    "     wine_data[['scaled_fixed_acidity']] = scaler.fit_transform(wine_data[['fixed_acidity']])\n",
    "     ```\n",
    "\n",
    "6. **Re-Evaluating Normality**:\n",
    "   \n",
    "   After transforming the features, re-run the normality tests and plot the new distributions to ensure that the transformations have improved normality.\n",
    "   \n",
    "   ```python\n",
    "   # Density plot after transformation\n",
    "   sns.kdeplot(wine_data['log_residual_sugar'], shade=True)\n",
    "   plt.title('Density Plot of Log Transformed Residual Sugar')\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "### Conclusion:\n",
    "The EDA process on the wine quality dataset involves checking the distribution of features and applying necessary transformations to handle non-normality. Features like **residual sugar**, **alcohol**, and **sulfur dioxide levels** typically exhibit non-normality, and applying transformations such as log, square root, or Box-Cox can help improve normality, which in turn can enhance the performance of predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Using the wine quality data set, perform principal component analysis (PCA) to reduce the number of\n",
    "features. What is the minimum number of principal components required to explain 90% of the variance in\n",
    "the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform **Principal Component Analysis (PCA)** on the wine quality dataset and reduce the number of features, the goal is to identify the minimum number of principal components required to explain 90% of the variance in the data. PCA works by transforming the original correlated features into a set of uncorrelated principal components that capture most of the variance in the data.\n",
    "\n",
    "### Steps for Performing PCA on the Wine Quality Dataset:\n",
    "\n",
    "1. **Load the Wine Quality Dataset**:\n",
    "   - First, load the dataset into a pandas DataFrame and ensure that the data is ready for analysis.\n",
    "   \n",
    "2. **Data Preprocessing**:\n",
    "   - **Handling Missing Data**: Ensure that there are no missing values in the dataset.\n",
    "   - **Standardization**: Since PCA is sensitive to the scale of the features, standardize the data to have a mean of 0 and a standard deviation of 1. This ensures that all features contribute equally to the PCA.\n",
    "\n",
    "3. **Perform PCA**:\n",
    "   - Apply PCA to the standardized dataset to find the principal components and determine how many are needed to explain 90% of the variance.\n",
    "\n",
    "4. **Explained Variance**:\n",
    "   - Calculate the cumulative explained variance to determine how many principal components are required to explain 90% of the variance.\n",
    "\n",
    "Hereâ€™s how you can perform this in Python:\n",
    "\n",
    "### Code to Perform PCA\n",
    "\n",
    "```python\n",
    "# Step 1: Load the necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 2: Load the dataset\n",
    "wine_data = pd.read_csv(\"winequality.csv\")  # Replace with your dataset path\n",
    "\n",
    "# Step 3: Data Preprocessing (Standardization)\n",
    "# Select only the numeric features for PCA (exclude the target variable 'quality')\n",
    "features = wine_data.drop(columns=['quality'])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Step 4: Perform PCA\n",
    "pca = PCA(n_components=None)  # Set None to calculate all components\n",
    "pca.fit(features_scaled)\n",
    "\n",
    "# Step 5: Explained Variance\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance = explained_variance_ratio.cumsum()\n",
    "\n",
    "# Step 6: Plot the cumulative explained variance\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')\n",
    "plt.title('Cumulative Explained Variance by Principal Components')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 7: Find the minimum number of components to explain 90% variance\n",
    "for i, cumulative_var in enumerate(cumulative_variance):\n",
    "    if cumulative_var >= 0.90:\n",
    "        print(f'{i+1} components are required to explain 90% of the variance.')\n",
    "        break\n",
    "```\n",
    "\n",
    "### Interpretation of Results:\n",
    "\n",
    "1. **Standardization**: The features were standardized to have mean 0 and variance 1 to ensure that the PCA results are not dominated by features with larger scales (e.g., alcohol or sugar levels).\n",
    "\n",
    "2. **Explained Variance**:\n",
    "   - The `explained_variance_ratio_` gives the amount of variance explained by each principal component.\n",
    "   - The cumulative sum of explained variance shows how much total variance is explained as you add more principal components.\n",
    "   \n",
    "3. **Choosing Principal Components**:\n",
    "   - After calculating the cumulative explained variance, we can see how many principal components are required to explain 90% of the variance.\n",
    "   - The plotted curve will show the \"elbow point,\" where the curve flattens, indicating that adding more components beyond this point doesnâ€™t explain much more variance.\n",
    "\n",
    "4. **Results**:\n",
    "   - From the print statement, youâ€™ll get the minimum number of principal components that explain 90% of the variance. For example, if it prints that **7 components are required**, it means 7 out of the original features explain 90% of the variance, allowing us to reduce the dimensionality from the original number of features to just 7.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "By applying PCA to the wine quality dataset, you can reduce the number of features while retaining most of the information. The exact number of principal components required to explain 90% of the variance will depend on the dataset, but typically, PCA can drastically reduce the dimensionality while maintaining predictive power. This helps simplify the model and reduces the risk of overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
