{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\"> Natural Language Processing - 102 </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Program so far \n",
    "***\n",
    "\n",
    "* Python\n",
    "* Statistics\n",
    "* Supervised Machine Learning\n",
    "* Unsupervised Machine Learning\n",
    "* NLP 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda for the Day\n",
    "***\n",
    "\n",
    "- Topic Modeling\n",
    "2. POS Tagging\n",
    "- Chunking\n",
    "- Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "NLP is all about unstructured data, and one of the problem industry is facing today is about amount of data that any System has to process. Often its not practical to read through a huge volume of data and get some insights about that data. Consider google news, there are hundred of thousands of news get published on daily basis. So we need a way to group news with some keywords in order to understand what is going on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![alt text](../images/topic_modelling.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- One in red are classes, which are fixed and with the help of training data, we can build news classifier.\n",
    "- But one in green are topics, that are identified run time. And process of identification of topics is totally unsupervised. And Topic modelling is one the best way to understand, repersent any unstructured text without actually getting into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Topic Modelling__ as the name suggests, it is a process to automatically identify topics present in a text object and to derive hidden patterns exhibited by a text corpus. Thus, assisting better decision making.\n",
    "\n",
    "A __Topic__ can be defined as “a repeating pattern of co-occurring terms in a corpus”. A good topic model should result in – “health”, “doctor”, “patient”, “hospital” for a topic – Healthcare, and “farm”, “crops”, “wheat” for a topic – “Farming”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Applications of Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Document Clustering.\n",
    "    1. Group news.\n",
    "    2. Group emails.\n",
    "    3. Group similar medical notes etc.\n",
    "- Keywords Generation. Can be used for SEO.\n",
    "- Build WordCloud.\n",
    "- Build Search Engines.\n",
    "- Build knowledge-graph(aka ontologies).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How Topic Modelling Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Topics are generally important words in text. \n",
    "- Frequency count can be one of the way to identify topics.\n",
    "- TF-IDF can also be used for Topic Modelling.\n",
    "- Or most famous, LDA (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose you have the following set of sentences:\n",
    "\n",
    "- I like to eat broccoli and bananas.\n",
    "- I ate a banana and spinach smoothie for breakfast.\n",
    "- Chinchillas and kittens are cute.\n",
    "- My sister adopted a kitten yesterday.\n",
    "- Look at this cute hamster munching on a piece of broccoli.\n",
    "\n",
    "LDA will try to identify words which have been used in similar context and will calculate probability of occuring two words togther.\n",
    "In the above example, LDA will create topics like:\n",
    "    \n",
    "- Topic A: 30% broccoli, 15% bananas, 10% breakfast, 10% munching, … (at which point, you could interpret topic A to be about food)\n",
    "- Topic B: 20% chinchillas, 20% kittens, 20% cute, 15% hamster, … (at which point, you could interpret topic B to be about cute animals).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How LDA Works?\n",
    "\n",
    "LDA involves a detailed inderstanding of Baysian Probabilisitc Approach. However, here is an intuitive explanation of how LDA operates:\n",
    "\n",
    "Let's from the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic Modelling with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "gensim(https://radimrehurek.com/gensim/) package in python implements most of topic modelling algorithms.\n",
    "\n",
    "* We'll walk through a basic application of Topic Modeling with LDA\n",
    "* We'll also cover the basic NLP operations necessary for the application\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# create sample documents\n",
    "# We will use author data\n",
    "import nltk\n",
    "text=open(\"../data/C50train/AaronPressman/2537newsML.txt\").read()\n",
    "sents = nltk.sent_tokenize(text)\n",
    "\n",
    "# compile documents\n",
    "doc_complete =sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's fast-forward through pre-processing\n",
    "\n",
    "* After the processing, we'll have *texts* - a tokenized, stopped and stemmed list of words from a single document\n",
    "* Let’s fast forward and loop through all our documents and appended each one to *texts*\n",
    "* So now *texts* is a list of lists, one list for each of our original documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from pprint import pprint\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = set(stopwords.words('english'))\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "   \n",
    "# create sample documents\n",
    "# We will use author data\n",
    "import nltk\n",
    "text=open(\"../data/C50train/AaronPressman/2537newsML.txt\").read()\n",
    "sents = nltk.sent_tokenize(text)\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = sents\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### texts\n",
      "[['break', 'u', u'justic', u'depart', 'world', 'wide', 'web', 'site', 'last', 'week', u'highlight', 'internet', u'continu', u'vulner', u'hacker'], [u'unidentifi', u'hacker', u'gain', u'access', u'depart', 'web', 'page', 'august', '16', u'replac', 'hate', u'fill', u'diatrib', u'label', u'depart', u'injustic', u'includ', 'swastika', u'pictur', 'adolf', 'hitler'], [u'justic', u'offici', u'quickli', u'pull', 'plug', u'vandalis', 'page', u'secur', u'flaw', u'allow', u'hacker', 'gain', u'entri', u'like', 'exist', u'thousand', u'corpor', u'govern', 'web', u'site', u'secur', u'expert', 'said'], ['vast', u'major', u'site', u'vulner', 'said', 'richard', 'power', 'senior', 'analyst', u'comput', u'secur', u'institut'], [u'justic', u'depart', u'singl'], [u'justic', u'depart', u'offici', 'said', u'compromis', 'web', 'site', u'connect', u'comput', u'contain', u'sensit', u'file'], ['web', 'site', 'http', 'www', 'usdoj', 'gov', u'includ', u'copi', u'press', u'releas', u'speech', u'publicli', u'avail', u'inform'], [u'secur', 'breach', 'like', 'graffiti', u'outsid', u'build', 'spokesman', 'bert', 'brandenburg', 'said'], [u'organis', u'target', 'past'], ['last', 'year', 'nation', 'islam', 'million', 'man', 'march', 'web', 'site', u'vandalis'], [u'hacker', 'make', '250', '000', u'attempt', u'annual', 'break', 'u', u'militari', u'comput', u'accord', u'gener', u'account', u'offic', 'report'], [u'window', u'magazin', u'recent', 'found', u'secur', u'flaw', 'web', u'site', 'dozen', 'major', u'corpor'], ['web', u'spectacularli', u'insecur', 'editor', 'mike', 'elgan', 'said'], [u'reli', u'secur', u'hole', u'document', u'softwar', u'manufactur', u'month', 'earlier', u'magazin', u'specialist', u'abl', 'gain', u'variou', u'degre', u'unauthoris', u'access', u'differ', u'site'], ['elgan', 'said', u'hacker', u'exploit', u'flaw', u'motiv', 'anger', 'growth', u'commerci', 'internet'], ['common', 'theme', u'hacker', 'fed', 'non', u'hacker', 'internet', 'said'], [u'battl', u'complet', u'hopeless'], [u'secur', 'web', 'site', 'richard', 'power', 'said'], [u'kind', u'measur', 'take'], [u'corpor', u'institut', 'take', u'simpli', u'noth', 'bad', u'happen', 'yet'], [u'site', u'use', u'multipl', u'layer', u'secur', 'well', 'beyond', u'simpl', 'password', u'protect', 'keep', u'hacker'], ['one', 'site', u'mention', u'window', u'magazin', u'fidel', u'invest'], [u'fidel', 'site', u'advertis', 'mutual', u'fund', u'dissemin', u'inform', u'person', u'financ', 'contain', u'confidenti', u'custom', u'inform'], [u'fidel', u'offici', u'immedi', u'close', u'loophol', u'identifi', u'magazin', 'spokeswoman', 'said'], [u'multipl', u'secur', u'measur', u'previous', 'place', 'would', u'prevent', u'secur', 'breach', u'despit', 'hole', 'spokeswoman', u'ad']]\n",
      "\n",
      "##### The lines in texts\n",
      "['break', 'u', u'justic', u'depart', 'world', 'wide', 'web', 'site', 'last', 'week', u'highlight', 'internet', u'continu', u'vulner', u'hacker']\n",
      "[u'unidentifi', u'hacker', u'gain', u'access', u'depart', 'web', 'page', 'august', '16', u'replac', 'hate', u'fill', u'diatrib', u'label', u'depart', u'injustic', u'includ', 'swastika', u'pictur', 'adolf', 'hitler']\n",
      "[u'justic', u'offici', u'quickli', u'pull', 'plug', u'vandalis', 'page', u'secur', u'flaw', u'allow', u'hacker', 'gain', u'entri', u'like', 'exist', u'thousand', u'corpor', u'govern', 'web', u'site', u'secur', u'expert', 'said']\n",
      "['vast', u'major', u'site', u'vulner', 'said', 'richard', 'power', 'senior', 'analyst', u'comput', u'secur', u'institut']\n",
      "[u'justic', u'depart', u'singl']\n",
      "[u'justic', u'depart', u'offici', 'said', u'compromis', 'web', 'site', u'connect', u'comput', u'contain', u'sensit', u'file']\n",
      "['web', 'site', 'http', 'www', 'usdoj', 'gov', u'includ', u'copi', u'press', u'releas', u'speech', u'publicli', u'avail', u'inform']\n",
      "[u'secur', 'breach', 'like', 'graffiti', u'outsid', u'build', 'spokesman', 'bert', 'brandenburg', 'said']\n",
      "[u'organis', u'target', 'past']\n",
      "['last', 'year', 'nation', 'islam', 'million', 'man', 'march', 'web', 'site', u'vandalis']\n",
      "[u'hacker', 'make', '250', '000', u'attempt', u'annual', 'break', 'u', u'militari', u'comput', u'accord', u'gener', u'account', u'offic', 'report']\n",
      "[u'window', u'magazin', u'recent', 'found', u'secur', u'flaw', 'web', u'site', 'dozen', 'major', u'corpor']\n",
      "['web', u'spectacularli', u'insecur', 'editor', 'mike', 'elgan', 'said']\n",
      "[u'reli', u'secur', u'hole', u'document', u'softwar', u'manufactur', u'month', 'earlier', u'magazin', u'specialist', u'abl', 'gain', u'variou', u'degre', u'unauthoris', u'access', u'differ', u'site']\n",
      "['elgan', 'said', u'hacker', u'exploit', u'flaw', u'motiv', 'anger', 'growth', u'commerci', 'internet']\n",
      "['common', 'theme', u'hacker', 'fed', 'non', u'hacker', 'internet', 'said']\n",
      "[u'battl', u'complet', u'hopeless']\n",
      "[u'secur', 'web', 'site', 'richard', 'power', 'said']\n",
      "[u'kind', u'measur', 'take']\n",
      "[u'corpor', u'institut', 'take', u'simpli', u'noth', 'bad', u'happen', 'yet']\n",
      "[u'site', u'use', u'multipl', u'layer', u'secur', 'well', 'beyond', u'simpl', 'password', u'protect', 'keep', u'hacker']\n",
      "['one', 'site', u'mention', u'window', u'magazin', u'fidel', u'invest']\n",
      "[u'fidel', 'site', u'advertis', 'mutual', u'fund', u'dissemin', u'inform', u'person', u'financ', 'contain', u'confidenti', u'custom', u'inform']\n",
      "[u'fidel', u'offici', u'immedi', u'close', u'loophol', u'identifi', u'magazin', 'spokeswoman', 'said']\n",
      "[u'multipl', u'secur', u'measur', u'previous', 'place', 'would', u'prevent', u'secur', 'breach', u'despit', 'hole', 'spokeswoman', u'ad']\n"
     ]
    }
   ],
   "source": [
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "print(\"\\n##### texts\")\n",
    "print(texts)\n",
    "\n",
    "print(\"\\n##### The lines in texts\")\n",
    "for line in texts:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What's next?\n",
    "\n",
    "* To generate an LDA model, we need to understand how frequently each term occurs within each document\n",
    "* To do that, we need to construct a document-term matrix with a package called *gensim*\n",
    "\n",
    "# Topic Modeling with gensim\n",
    "\n",
    "## Getting started with gensim?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(175 unique tokens: [u'theme', u'identifi', u'offici', u'month', u'report']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The Dictionary() function traverses texts, assigning a unique integer id to each unique token while also collecting word counts and relevant statistics\n",
    "* To see each token’s unique integer id, try -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'theme': 129, u'identifi': 167, u'offici': 40, u'month': 117, u'report': 97, u'simpli': 139, u'hate': 28, u'yet': 140, u'islam': 88, u'web': 4, u'adolf': 29, u'sensit': 58, u'manufactur': 118, u'offic': 95, u'breach': 79, u'vulner': 13, u'swastika': 27, u'take': 137, u'non': 130, u'march': 83, u'variou': 112, u'wide': 1, u'financ': 159, u'kind': 136, u'nation': 85, u'break': 9, u'mention': 154, u'govern': 45, u'press': 64, u'world': 3, u'password': 150, u'vast': 55, u'measur': 135, u'gov': 72, u'like': 36, u'corpor': 41, u'elgan': 107, u'magazin': 101, u'editor': 109, u'contain': 61, u'found': 100, u'page': 21, u'prevent': 172, u'www': 62, u'replac': 19, u'continu': 8, u'past': 80, u'growth': 125, u'connect': 59, u'year': 86, u'close': 165, u'happen': 141, u'beyond': 144, u'hacker': 14, u'said': 35, u'expert': 37, u'spokesman': 75, u'abl': 115, u'bad': 138, u'label': 22, u'access': 23, u'use': 143, u'internet': 11, u'injustic': 15, u'insecur': 105, u'common': 131, u'specialist': 113, u'accord': 94, u'confidenti': 157, u'august': 17, u'power': 50, u'despit': 169, u'gener': 99, u'million': 84, u'fed': 128, u'vandalis': 42, u'unauthoris': 120, u'highlight': 6, u'releas': 71, u'advertis': 162, u'plug': 32, u'protect': 147, u'last': 12, u'justic': 5, u'militari': 91, u'annual': 93, u'keep': 149, u'degre': 116, u'000': 96, u'organis': 81, u'hopeless': 133, u'outsid': 78, u'compromis': 57, u'softwar': 110, u'major': 48, u'depart': 2, u'usdoj': 66, u'one': 153, u'exploit': 124, u'250': 98, u'hitler': 30, u'ad': 170, u'differ': 111, u'institut': 49, u'would': 171, u'pictur': 18, u'window': 103, u'custom': 160, u'avail': 67, u'reli': 119, u'includ': 25, u'recent': 102, u'quickli': 47, u'bert': 73, u'flaw': 34, u'thousand': 46, u'copi': 65, u'fund': 158, u'gain': 31, u'hole': 121, u'analyst': 54, u'motiv': 126, u'pull': 33, u'account': 89, u'immedi': 166, u'target': 82, u'16': 20, u'battl': 132, u'commerci': 127, u'spokeswoman': 164, u'publicli': 70, u'invest': 152, u'exist': 43, u'dissemin': 163, u'document': 122, u'layer': 148, u'comput': 52, u'person': 161, u'site': 7, u'graffiti': 76, u'file': 60, u'anger': 123, u'fill': 16, u'mutual': 156, u'multipl': 145, u'secur': 38, u'make': 92, u'brandenburg': 74, u'mike': 106, u'speech': 69, u'build': 77, u'diatrib': 26, u'place': 173, u'dozen': 104, u'simpl': 151, u'noth': 142, u'singl': 56, u'complet': 134, u'week': 0, u'http': 63, u'spectacularli': 108, u'fidel': 155, u'earlier': 114, u'unidentifi': 24, u'previous': 174, u'loophol': 168, u'man': 87, u'attempt': 90, u'richard': 51, u'entri': 39, u'well': 146, u'inform': 68, u'u': 10, u'allow': 44, u'senior': 53}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next, our dictionary must be converted into a [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1)]\n",
      "[(2, 2), (4, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1)]\n",
      "[(4, 1), (5, 1), (7, 1), (14, 1), (21, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 2), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1)]\n",
      "[(7, 1), (13, 1), (35, 1), (38, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1)]\n",
      "[(2, 1), (5, 1), (56, 1)]\n",
      "[(2, 1), (4, 1), (5, 1), (7, 1), (35, 1), (40, 1), (52, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1)]\n",
      "[(4, 1), (7, 1), (25, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1)]\n",
      "[(35, 1), (36, 1), (38, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1)]\n",
      "[(80, 1), (81, 1), (82, 1)]\n",
      "[(4, 1), (7, 1), (12, 1), (42, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1)]\n",
      "[(9, 1), (10, 1), (14, 1), (52, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1)]\n",
      "[(4, 1), (7, 1), (34, 1), (38, 1), (41, 1), (48, 1), (100, 1), (101, 1), (102, 1), (103, 1), (104, 1)]\n",
      "[(4, 1), (35, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1)]\n",
      "[(7, 1), (23, 1), (31, 1), (38, 1), (101, 1), (110, 1), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1)]\n",
      "[(11, 1), (14, 1), (34, 1), (35, 1), (107, 1), (123, 1), (124, 1), (125, 1), (126, 1), (127, 1)]\n",
      "[(11, 1), (14, 2), (35, 1), (128, 1), (129, 1), (130, 1), (131, 1)]\n",
      "[(132, 1), (133, 1), (134, 1)]\n",
      "[(4, 1), (7, 1), (35, 1), (38, 1), (50, 1), (51, 1)]\n",
      "[(135, 1), (136, 1), (137, 1)]\n",
      "[(41, 1), (49, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1)]\n",
      "[(7, 1), (14, 1), (38, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 1), (148, 1), (149, 1), (150, 1), (151, 1)]\n",
      "[(7, 1), (101, 1), (103, 1), (152, 1), (153, 1), (154, 1), (155, 1)]\n",
      "[(7, 1), (61, 1), (68, 2), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1)]\n",
      "[(35, 1), (40, 1), (101, 1), (155, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 1)]\n",
      "[(38, 2), (79, 1), (121, 1), (135, 1), (145, 1), (164, 1), (169, 1), (170, 1), (171, 1), (172, 1), (173, 1), (174, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "for line in corpus:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The doc2bow() function converts dictionary into a bag-of-words\n",
    "* The result, *corpus*, is a list of vectors equal to the number of documents\n",
    "* In each document vector is a series of tuples\n",
    "* The tuples are (term ID, term frequency) pairs\n",
    "* This includes terms that actually occur - terms that do not occur in a document will not appear in that document’s vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating the LDA Model\n",
    "\n",
    "*corpus* is a (sparse) document-term matrix and now we’re ready to generate an LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parameters to the LDA model\n",
    "\n",
    "https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "* num_topics\n",
    "    - required\n",
    "    - An LDA model requires the user to determine how many topics should be generated\n",
    "    - Our document set is small, so we’re only asking for three topics\n",
    "* id2word\n",
    "    - required\n",
    "    - The LdaModel class requires our previous dictionary to map ids to strings\n",
    "* passes\n",
    "    - optional\n",
    "    - The number of laps the model will take through corpus\n",
    "    - The greater the number of passes, the more accurate the model will be\n",
    "    - A lot of passes can be slow on a very large corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel(num_terms=175, num_topics=3, decay=0.5, chunksize=2000)\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.033*\"secur\" + 0.033*\"site\" + 0.023*\"web\" + 0.023*\"hacker\" + 0.023*\"depart\" + 0.017*\"comput\" + 0.017*\"said\" + 0.017*\"justic\" + 0.012*\"offici\" + 0.012*\"gain\"'), (1, u'0.033*\"said\" + 0.033*\"hacker\" + 0.033*\"site\" + 0.025*\"internet\" + 0.025*\"web\" + 0.025*\"secur\" + 0.018*\"flaw\" + 0.018*\"window\" + 0.018*\"magazin\" + 0.010*\"vulner\"'), (2, u'0.020*\"magazin\" + 0.020*\"site\" + 0.020*\"take\" + 0.020*\"web\" + 0.020*\"said\" + 0.011*\"fidel\" + 0.011*\"hole\" + 0.011*\"institut\" + 0.011*\"spokeswoman\" + 0.011*\"access\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, u'0.020*\"magazin\" + 0.020*\"site\" + 0.020*\"take\" + 0.020*\"web\" + 0.020*\"said\" + 0.011*\"fidel\" + 0.011*\"hole\" + 0.011*\"institut\" + 0.011*\"spokeswoman\" + 0.011*\"access\"')\n",
      "(0, u'0.033*\"secur\" + 0.033*\"site\" + 0.023*\"web\" + 0.023*\"hacker\" + 0.023*\"depart\" + 0.017*\"comput\" + 0.017*\"said\" + 0.017*\"justic\" + 0.012*\"offici\" + 0.012*\"gain\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in ldamodel.print_topics(num_topics=2):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, u'0.033*\"secur\" + 0.033*\"site\" + 0.023*\"web\"')\n",
      "(1, u'0.033*\"said\" + 0.033*\"hacker\" + 0.033*\"site\"')\n",
      "(2, u'0.020*\"magazin\" + 0.020*\"site\" + 0.020*\"take\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in ldamodel.print_topics(num_topics=3, num_words=3):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Within each topic are the three most probable words to appear in that topic\n",
    "\n",
    "## Topics in detail\n",
    "Let's now look at a topic in detail. Let us see how distinct the topics are, and if they seem to capture any context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.033*\"secur\" + 0.033*\"site\" + 0.023*\"web\" + 0.023*\"hacker\" + 0.023*\"depart\" + 0.017*\"comput\" + 0.017*\"said\" + 0.017*\"justic\" + 0.012*\"offici\" + 0.012*\"gain\"\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topic(topicno=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.033*\"said\" + 0.033*\"hacker\" + 0.033*\"site\" + 0.025*\"internet\" + 0.025*\"web\" + 0.025*\"secur\" + 0.018*\"flaw\" + 0.018*\"window\" + 0.018*\"magazin\" + 0.010*\"vulner\"\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topic(topicno=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020*\"magazin\" + 0.020*\"site\" + 0.020*\"take\" + 0.020*\"web\" + 0.020*\"said\" + 0.011*\"fidel\" + 0.011*\"hole\" + 0.011*\"institut\" + 0.011*\"spokeswoman\" + 0.011*\"access\"\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topic(topicno=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Do the topics make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, u'0.033*\"secur\" + 0.033*\"site\" + 0.023*\"web\"')\n",
      "(1, u'0.033*\"said\" + 0.033*\"hacker\" + 0.033*\"site\"')\n",
      "(2, u'0.020*\"magazin\" + 0.020*\"site\" + 0.020*\"take\"')\n"
     ]
    }
   ],
   "source": [
    "for topic in ldamodel.print_topics(num_topics=3, num_words=3):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Refining the model\n",
    "\n",
    "Two topics seems like a better fit for our documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, u'0.037*\"secur\" + 0.032*\"site\" + 0.018*\"magazin\" + 0.017*\"said\"')\n",
      "(1, u'0.028*\"web\" + 0.028*\"said\" + 0.027*\"hacker\" + 0.027*\"site\"')\n"
     ]
    }
   ],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)\n",
    "\n",
    "for topic in ldamodel.print_topics(num_topics=2, num_words=4):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try it with more passes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, u'0.023*\"site\" + 0.023*\"web\" + 0.017*\"take\" + 0.010*\"hole\"')\n",
      "(1, u'0.033*\"secur\" + 0.033*\"site\" + 0.030*\"said\" + 0.029*\"hacker\"')\n"
     ]
    }
   ],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=200)\n",
    "\n",
    "for topic in ldamodel.print_topics(num_topics=2, num_words=4):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predicting Topic for new documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "doc_f = \"Are Health professionals justified in saying that brocolli is good for your health?\" \n",
    "\n",
    "doc_set = [doc_f]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "infer = ldamodel[corpus[0]]\n",
    "\n",
    "# https://radimrehurek.com/gensim/wiki.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel(num_terms=175, num_topics=2, decay=0.5, chunksize=2000)\n"
     ]
    }
   ],
   "source": [
    "#Lets check by default LDA parameters\n",
    "print(ldamodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Tree Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One of the advance topic in NLP is Lexical Analysis of text wherein we try to analyze and understand text. This process is called deep tree parsing in NLP world where we try to analyze relationships amongst the text.\n",
    "- Text parsing is important when you want to know relationships in text. For example <i>Delhi is capital of India<i>, here Delhi and India are related and having a relationship <b>is capital of<b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (Det the) (N dog))\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (Det a) (N man) (PP (P in) (NP (Det the) (N park))))))\n",
      "(S\n",
      "  (NP (Det the) (N dog))\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP (Det a) (N man))\n",
      "    (PP (P in) (NP (Det the) (N park)))))\n"
     ]
    }
   ],
   "source": [
    "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  VP -> V NP | V NP PP\n",
    "  PP -> P NP\n",
    "  V -> \"saw\" | \"ate\" | \"walked\"\n",
    "  NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n",
    "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
    "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
    "  P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
    "  \"\"\")\n",
    "sent = \"the dog saw a man in the park\"\n",
    "tokens=nltk.word_tokenize(sent)\n",
    "rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
    "for tree in rd_parser.parse(tokens):\n",
    "    print(tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![alt text](../images/deep_parsing.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here as well we have to define our grammar, which looks quite tedious job. But there are other NLP packages such as Stanford CoreNLP which provides funcitons to generate parse tree from unstructured text without defining any grammar.\n",
    "- Parse tree provides us meaningful and true relations and also kind of relations they share. Also called facts.\n",
    "- Tree Parsing is used to build knowledge base from unstructured corpus. Check DbPedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Part of Speech tags are grammatical consituents (Noun, Verbs, Adverb, Adjectives) and this process of POS tagging classify tokens into their part-of-speech tags and label them according the tagset which is a collection of tags used for the pos tagging. Part-of-speech tagging also known as word classes or lexical categories. Here is the definition from wikipedia:\n",
    "    \n",
    "<i>In corpus linguistics, part-of-speech tagging (POS tagging or POST), also called grammatical tagging or word-category disambiguation, is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition, as well as its context—i.e. relationship with adjacent and related words in a phrase, sentence, or paragraph. A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.\n",
    "\n",
    "Once performed by hand, POS tagging is now done in the context of computational linguistics, using algorithms which associate discrete terms, as well as hidden parts of speech, in accordance with a set of descriptive tags. POS-tagging algorithms fall into two distinctive groups: rule-based and stochastic. E. Brill’s tagger, one of the first and most widely used English POS-taggers, employs rule-based algorithms.<i>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('Justice', 'NNP'),\n",
       " ('Department', 'NNP'),\n",
       " ('should', 'MD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('be', 'VB'),\n",
       " ('singled', 'VBN'),\n",
       " ('out', 'RP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These DT, NNP, MD etc are pos tags taken from the standard list of Penn TreeBank Tagsets. It can be found here\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "POS tagging is one of the basic and very important component of NLP, as NLP mainly works on linguistics, i.e. way of writing language and Grammar is important part of it. POS tagging in the world of NLP is solved problem and works well if language is written well formatted.\n",
    "\n",
    "POS tagging is also supervised learning solution that uses features like previous word, next word, is first letter capitalized etc.\n",
    "\n",
    "NLTK has a function to get pos tags and it works after tokenization process. \n",
    "\n",
    "In our problem of Author Identification, we can create multiple features using POS Tagging.\n",
    "1. Number of Nouns, Verbs, Adjectives etc.\n",
    "2. How many times sentence starts with Adverb. Meaning words like Basically, Typically etc.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n"
     ]
    }
   ],
   "source": [
    "# We can get more details about any POS tag using help funciton of NLTK as follows.\n",
    "nltk.help.upenn_tagset(\"RB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Chunking is a process of extracting phrases(aka chunks) from unstructured text. Instead of just simple tokens which may not repersent actual meaning of text, its advisable to use phrases such as \"New Delhi\" as a single word instead of New and Delhi separate words.\n",
    "\n",
    "- Chunking is done using linguistic rules(language grammar rules), such as when two proper nouns occur together, merge them to make a single word. For Example \"South Africa\".\n",
    "-  Chunking works on top of POS tagging, it uses pos-tags as input and provide chunks as output. \n",
    "-  Similar to POS tags, there are standard set of Chunk tags like Noun Phrase(NP), Verb Phrase (VP) etc.\n",
    "-  Most data scientist uses N-Grams instead of chunker, but n-grams ends up creating a lots and lots of meaningless words.\n",
    "-  Chunking is very important when you want to extract information from text such as Locations, Person Names etc. In NLP called Named Entity Extraction.\n",
    "-  In Author Identification, we can hvae features like how many Named entity author uses in a sentence.\n",
    "-  What kind of countries/continents, author mostly refer in his articles.\n",
    "\n",
    "There are a lot of libraries which gives phrases out-of-box such as Spacy or TextBlob. NLTK just provides a mechanism using regular expressions to generate chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Unidentified/JJ)\n",
      "  hackers/NNS\n",
      "  (VP gained/VBD)\n",
      "  (NP access/NN)\n",
      "  to/TO\n",
      "  (NP the/DT department/NN)\n",
      "  's/POS\n",
      "  (NP web/JJ page/NN)\n",
      "  (P on/IN)\n",
      "  August/NNP\n",
      "  16/CD\n",
      "  and/CC\n",
      "  (VP replaced/VBD)\n",
      "  it/PRP\n",
      "  (PP (P with/IN) (NP a/DT hate-filled/JJ diatribe/NN))\n",
      "  (VP labelled/VBD)\n",
      "  (NP the/DT)\n",
      "  Department/NNP\n",
      "  (P of/IN)\n",
      "  Injustice/NNP\n",
      "  that/WDT\n",
      "  (VP included/VBD)\n",
      "  (NP a/DT swastika/NN)\n",
      "  and/CC\n",
      "  (NP a/DT picture/NN)\n",
      "  (P of/IN)\n",
      "  Adolf/NNP\n",
      "  Hitler/NNP\n",
      "  ./.)\n",
      "(NP Unidentified/JJ)\n",
      "(VP gained/VBD)\n",
      "(NP access/NN)\n",
      "(NP the/DT department/NN)\n",
      "(NP web/JJ page/NN)\n",
      "(P on/IN)\n",
      "(VP replaced/VBD)\n",
      "(PP (P with/IN) (NP a/DT hate-filled/JJ diatribe/NN))\n",
      "(P with/IN)\n",
      "(NP a/DT hate-filled/JJ diatribe/NN)\n",
      "(VP labelled/VBD)\n",
      "(NP the/DT)\n",
      "(P of/IN)\n",
      "(VP included/VBD)\n",
      "(NP a/DT swastika/NN)\n",
      "(NP a/DT picture/NN)\n",
      "(P of/IN)\n"
     ]
    }
   ],
   "source": [
    "#Define your grammar using regular expressions\n",
    "#For example a phrase starting with determiners(The/an/a) followed by noun or adjective will be a noun phrase. such as \"a greedy dog\"\n",
    "parser = ('''\n",
    "    NP: {<DT>? <JJ>* <NN>*} # NP\n",
    "    P: {<IN>}           # Preposition\n",
    "    PP: {<P> <NP>}      # PP -> P NP\n",
    "    VP: {<V.*> <PP|RB|V.*>*}  # VP -> V (NP|PP)*\n",
    "    ''')\n",
    "line=\"Unidentified hackers gained access to the department's web page on August 16 and replaced it with a hate-filled diatribe labelled the Department of Injustice that included a swastika and a picture of Adolf Hitler.\"\n",
    "chunkParser = nltk.RegexpParser(parser)\n",
    "negation_result={}\n",
    "tagged = nltk.pos_tag(nltk.word_tokenize(line))\n",
    "tree = chunkParser.parse(tagged)\n",
    "negated_entity=\"\"\n",
    "negated_value=\"\"\n",
    "negation=None\n",
    "for subtree in tree.subtrees():\n",
    "    print subtree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
