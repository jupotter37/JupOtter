{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fab71e1-749d-42fd-b35b-e1190a878cb9",
   "metadata": {},
   "source": [
    "# Statistical Thinking in Python (Part 2)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "These are my notes for DataCamp's course [_Statistical Thinking in Python (Part 2)_](https://www.datacamp.com/courses/statistical-thinking-in-python-part-2).\n",
    "\n",
    "This course is presented by Justin Bois, Lecturer at the California Institute of Technology. Collaborators are Yashas Roy and Hugo Bowne-Anderson.\n",
    "\n",
    "Prerequisites:\n",
    "\n",
    "- [_Statistical Thinking in Python (Part 1)_](../Statistical%20Thinking%20in%20Python%20Part%201/Statistical%20Thinking%20in%20Python%20Part%201.ipynb)\n",
    "\n",
    "This course is no longer part of any skill or career track, but it's an excellent course.\n",
    "\n",
    "For bootstrap analysis, see the new course \"Sampling in Python\".\n",
    "\n",
    "Be careful using np.polynomial.Polynomial.fit()!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f29001f-5f4a-4dc8-80b8-08a5a06e29e7",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- Introduction to Probability Theory, STAT 414 and Pennsylvania State University: https://online.stat.psu.edu/stat414/.\n",
    "- The code that underlies least squares analysis in NumPy: https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html#numpy.linalg.lstsq\n",
    "- Least squares fitting: https://mmas.github.io/least-squares-fitting-numpy-scipy\n",
    "- Fitting polynomials in NumPy: https://numpy.org/doc/stable/reference/routines.polynomials.classes#fitting\n",
    "- scikit-learn linear regression versus NumPy polyfit (not very well-written but plenty of code): https://techflare.blog/scikit-learn-linearregression-vs-numpy-polyfit/\n",
    "- YouTube video about bootstrapping: https://www.youtube.com/watch?v=N4ZQQqyIf6k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3edfdb2-5b67-4320-88ea-b27bf4bdf90a",
   "metadata": {},
   "source": [
    "## Imports and Code Initialization\n",
    "\n",
    "Imports are collected here for convenience and clarity. Initialize the default random number generator. Add the `ecdf()` function from part one of this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015a2415-b861-47ab-a991-5a2be1cc5753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "\n",
    "# Use dark mode for plotting.\n",
    "plt.style.use(\"dark_background\")\n",
    "\n",
    "# Initialize the random number generator.\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "# Add a function that takes as input a 1-D array of data and returns the\n",
    "# x and y values of the ECDF.\n",
    "def ecdf(data):\n",
    "    \"\"\"\n",
    "    Compute the ECDF of a one-dimensional array of measurements.\n",
    "\n",
    "    x, y = ecdf(data)\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    x = np.sort(data)\n",
    "    y = np.arange(1, n + 1) / n\n",
    "    return x, y\n",
    "\n",
    "# Add a function for calculating and returning the Pearson\n",
    "# correlation coefficient.\n",
    "def pearson_r(x, y):\n",
    "    \"\"\"\n",
    "    Compute the Pearson correlation coefficient.\n",
    "    \n",
    "    r = pearson_r(x, y)\n",
    "    \"\"\"\n",
    "    corr_mat = np.corrcoef(x, y)\n",
    "    return corr_mat[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4964a-2887-422a-9fcf-8dbfa236275c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Sets\n",
    "\n",
    "| Data Set | File |\n",
    "| :--- | :--- |\n",
    "| 2008 election all data | 2008_all_states.csv |\n",
    "| 2008 election swing states | 2008_swing_states.csv |\n",
    "| Anscombe data | anscombe.csv |\n",
    "| Bee sperm counts | bee_sperm.csv |\n",
    "| Female literacy and fertility | female_literacy_fertility.csv |\n",
    "| Finch beaks (1975) | finch_beaks_1975.csv |\n",
    "| Finch beaks (2012) | finch_beaks_2012.csv |\n",
    "| Fortis beak depth heredity | fortis_beak_depth_heredity.csv |\n",
    "| Frog tongue data | frog_tongue.csv |\n",
    "| Major League Baseball no-hitters | mlb_nohitters.csv |\n",
    "| Scandens beak depth heredity | scandens_beak_depth_heredity.csv |\n",
    "| Sheffield Weather Station | sheffield_weather_station.csv |\n",
    "\n",
    "Although it is simple to load each data set into a pandas DataFrame, this course extracts the data into NumPy ndarrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0191e69-fc41-4e24-a163-72fa8527743b",
   "metadata": {},
   "source": [
    "### MLB No-Hitter Times\n",
    "\n",
    "Load the data into a NumPy ndarray named `nohitter_times`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79fc20b-f945-4414-81db-5cd36b188186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a numpy ndarray containing the number of games between no-hitters\n",
    "# from 1900 through 2015.\n",
    "\n",
    "# Load the dates and other information of no-hit baseball games.\n",
    "# The dates in the CSV file are encoded as \"18760715\", but pd.read_csv()\n",
    "# parses them correctly.\n",
    "nohitters = pd.read_csv(\"mlb_nohitters.csv\", parse_dates=[0])\n",
    "print(nohitters.info())\n",
    "print()\n",
    "print(nohitters.head())\n",
    "print()\n",
    "\n",
    "# Extract the number of games between no-hitters for the modern era\n",
    "# (1900-2015).\n",
    "# mengn stands for modern_era_nohitters_game_numbers; this is a pandas Series.\n",
    "mengn = nohitters[nohitters[\"date\"] > \"1900-01-01\"][\"game_number\"]\n",
    "\n",
    "# Create a numpy ndarray containing the number of games between no-hitters.\n",
    "nohitter_times = np.array([mengn.iloc[x] - mengn.iloc[x - 1] - 1 for x in range(1, len(mengn))])\n",
    "print(nohitter_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb9c14b-ae0d-4790-ad2f-5b9d8a583610",
   "metadata": {},
   "source": [
    "### Female Literacy and Fertility\n",
    "\n",
    "Load the data into NumPy ndarrays named `illiteracy` and `fertility`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d81743-424c-41b8-9849-1fa9ccc6c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a NumPy DataFrame.\n",
    "# flf: female literacy and fertility.\n",
    "# Add the thousands=\",\" argument to convert values in the population\n",
    "# column to int64 values instead of object values.\n",
    "flf = pd.read_csv(\"female_literacy_fertility.csv\", thousands=\",\")\n",
    "print(flf.info())\n",
    "print()\n",
    "print(flf.head())\n",
    "print()\n",
    "\n",
    "# Create numpy.ndarrays for illiteracy and fertility values.\n",
    "illiteracy = 100 - flf[\"female literacy\"].to_numpy()\n",
    "# print(type(illiteracy))\n",
    "# print(illiteracy)\n",
    "fertility = flf[\"fertility\"].to_numpy()\n",
    "# print(type(fertility))\n",
    "# print(fertility)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae1598e-70f8-4c5a-ba30-7f07dcea6641",
   "metadata": {},
   "source": [
    "### Anscombe's Quartet\n",
    "\n",
    "See https://en.wikipedia.org/wiki/Anscombe%27s_quartet and https://blog.revolutionanalytics.com/2017/05/the-datasaurus-dozen.html.\n",
    "\n",
    "The CSV file has two header lines.\n",
    "\n",
    "Store the data in two lists, `anscombe_x` and `anscombe_y`, where the item of each list is a NumPy ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9581167-a6f5-412e-90f6-291b5c2afafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "anscombe = pd.read_csv(\"anscombe.csv\", header=[0, 1])\n",
    "print(anscombe.info())\n",
    "print()\n",
    "print(anscombe)\n",
    "print()\n",
    "# Demonstrate how to extract data from the DataFrame.\n",
    "# print(anscombe[(\"0\", \"x\")])\n",
    "# print(anscombe[(\"0\", \"y\")])\n",
    "# Create lists of ndrarrays for use in the exercise.\n",
    "anscombe_x = [anscombe[(group, \"x\")].to_numpy() for group in [\"0\", \"1\", \"2\", \"3\"]]\n",
    "anscombe_y = [anscombe[(group, \"y\")].to_numpy() for group in [\"0\", \"1\", \"2\", \"3\"]]\n",
    "pprint.pprint(anscombe_x)\n",
    "print()\n",
    "pprint.pprint(anscombe_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933aa8fd-b931-4ad2-b230-ac9e3697b44e",
   "metadata": {},
   "source": [
    "### Michelson Speed of Light\n",
    "\n",
    "The chapter on bootstrapping made use of this data set from the \"Statistical Thinking in Python (Part 1)\" course; I copied the CSV file into the directory for this course. Create a numpy.ndarray containing the speed of light results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1779d2-c475-47f4-9049-1e605be90b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Michelson speed of light data.\n",
    "light = pd.read_csv(\"michelson_speed_of_light.csv\", index_col=0)\n",
    "print(light.info())\n",
    "print()\n",
    "print(light.head())\n",
    "print()\n",
    "michelson_speed_of_light = light[\"velocity of light in air (km/s)\"].to_numpy()\n",
    "print(michelson_speed_of_light)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839b4fe4-5ef5-4d62-8d00-86a114cbf520",
   "metadata": {},
   "source": [
    "### Rainfall at Sheffield Weather Station\n",
    "\n",
    "The data is in a CSV file, but the data is delimited by white space characters, and there are multiple header lines. Skip 8 rows, take the header from the 9th row, and start reading data at the 10th row\n",
    "\n",
    "The records are monthly; combine the rainfall values for a year into a total for the year to create the variable `rainfall`, a numpy.ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6031c0-273b-4978-bba6-1b452db5af17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file, using a regular expression separator of \"\\s+\".\n",
    "# The header line is on line 8.\n",
    "# Missing values are coded as \"---\".\n",
    "sheffield = pd.read_csv(\"sheffield_weather_station.csv\", sep=r\"\\s+\", header=8, na_values=\"---\")\n",
    "print(sheffield.info())\n",
    "print()\n",
    "print(sheffield.head())\n",
    "print()\n",
    "\n",
    "# We have rain in mm units by month; these need to be combined into\n",
    "# annual rainfall for the years 1883 through 2015.\n",
    "rainfall_list = []\n",
    "for year in range(1883, 2016):\n",
    "    annual_rain = sheffield[sheffield[\"yyyy\"] == year][\"rain\"].sum()\n",
    "    rainfall_list.append(annual_rain)\n",
    "# print(rainfall_list)\n",
    "# print()\n",
    "rainfall = np.array(rainfall_list)\n",
    "print(rainfall)\n",
    "print()\n",
    "\n",
    "# Extract data for rain_june and rain_november.\n",
    "rain_june = sheffield[(sheffield[\"mm\"] == 6) & (sheffield[\"yyyy\"] <= 2015)][\"rain\"].to_numpy()\n",
    "rain_november = sheffield[sheffield[\"mm\"] == 11][\"rain\"].to_numpy()\n",
    "print(rain_june)\n",
    "print()\n",
    "print(rain_november)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33be6a05-8f19-4163-a799-1cb9d399bd32",
   "metadata": {},
   "source": [
    "### 2008 Election (Swing States)\n",
    "\n",
    "This data, from Statistical Thinking in Python (Part 1), is used in a demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda4092-958d-4dae-9557-cc1f4987f99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 2008 election results for swing states into a pandas DataFrame.\n",
    "# Read the data from the CSV file.\n",
    "swing_states = pd.read_csv(\"2008_swing_states.csv\")\n",
    "print(swing_states.info())\n",
    "print()\n",
    "print(swing_states.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1621d63c-6875-47e1-a1b0-65a561f94832",
   "metadata": {},
   "source": [
    "### 2008 Election (All States)\n",
    "\n",
    "This data, from Statistical Thinking in Python (Part 1), is used for some extra work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b3ed9a-8bd9-4855-beb1-129c7629a662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 2008 election results for swing states into a pandas DataFrame.\n",
    "# Read the data from the CSV file.\n",
    "all_states = pd.read_csv(\"2008_all_states.csv\")\n",
    "print(all_states.info())\n",
    "print()\n",
    "print(all_states.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d12ed5-f27f-4817-ac60-6b45eb4babfb",
   "metadata": {},
   "source": [
    "### Frog Tongue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bb3e83-fcec-482d-a867-79bf2662a220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the frog tongue force data.\n",
    "frog_tongue = pd.read_csv(\"frog_tongue.csv\", header=14)\n",
    "print(frog_tongue.info())\n",
    "print()\n",
    "print(frog_tongue.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d8842-9b5b-4d32-bddd-916dc130e2b4",
   "metadata": {},
   "source": [
    "### Bee Sperm\n",
    "\n",
    "Straub, et al. (Proc. Roy. Soc. B, 2016) investigated the effects of neonicotinoids on the sperm of pollinating bees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9868c2dc-70f4-4a3b-92d2-d4caed0b5a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the CSV file.\n",
    "# Multiply the values by 2 to obtain the values in the course's arrays.\n",
    "bee_sperm = pd.read_csv(\"bee_sperm.csv\", header=3)\n",
    "print(bee_sperm.info())\n",
    "print()\n",
    "print(bee_sperm.head())\n",
    "control = bee_sperm[bee_sperm[\"Treatment\"] == \"Control\"][\"Alive Sperm Millions\"].to_numpy()\n",
    "control = control * 2\n",
    "print(control)\n",
    "print(np.mean(control))\n",
    "print()\n",
    "treated = bee_sperm[bee_sperm[\"Treatment\"] == \"Pesticide\"][\"Alive Sperm Millions\"].to_numpy()\n",
    "treated = treated * 2\n",
    "print(treated)\n",
    "print(np.mean(treated))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37fe603-2183-4576-bda4-82393e32b284",
   "metadata": {},
   "source": [
    "### Finch Beaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3567a0-8470-4b0c-ba0c-dcbd7da4de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame named beak_depth with columns \"year\" and \"beak_depth\".\n",
    "finch_beaks_1975 = pd.read_csv(\"finch_beaks_1975.csv\", header=0)\n",
    "scandens_beaks_1975 = finch_beaks_1975[finch_beaks_1975[\"species\"] == \"scandens\"].copy()\n",
    "scandens_beaks_1975[\"year\"] = 1975\n",
    "print(scandens_beaks_1975.info())\n",
    "print()\n",
    "print(scandens_beaks_1975.head())\n",
    "print()\n",
    "\n",
    "finch_beaks_2012 = pd.read_csv(\"finch_beaks_2012.csv\", header=0)\n",
    "scandens_beaks_2012 = finch_beaks_2012[finch_beaks_2012[\"species\"] == \"scandens\"].copy()\n",
    "scandens_beaks_2012[\"year\"] = 2012\n",
    "print(scandens_beaks_2012.info())\n",
    "print()\n",
    "print(scandens_beaks_2012.head())\n",
    "print()\n",
    "\n",
    "# Create the new DataFrame for beak length data.\n",
    "year_data = \\\n",
    "    np.concatenate(\n",
    "        (\n",
    "            scandens_beaks_1975[\"year\"].to_numpy(),\n",
    "            scandens_beaks_2012[\"year\"].to_numpy()\n",
    "        )\n",
    "    )\n",
    "beak_depth_data = \\\n",
    "    np.concatenate(\n",
    "        (\n",
    "            scandens_beaks_1975[\"Beak depth, mm\"].to_numpy(),\n",
    "            scandens_beaks_2012[\"bdepth\"].to_numpy()\n",
    "        )\n",
    "    )\n",
    "data_dict = {\n",
    "    \"beak_depth\": beak_depth_data,\n",
    "    \"year\": year_data\n",
    "}\n",
    "beak_depth = pd.DataFrame(data_dict)\n",
    "print(beak_depth.info())\n",
    "print()\n",
    "print(beak_depth.head())\n",
    "print()\n",
    "\n",
    "# Create NumPy arrays for beak depth and beak length by year.\n",
    "bl_1975 = scandens_beaks_1975[\"Beak length, mm\"].to_numpy()\n",
    "bd_1975 = scandens_beaks_1975[\"Beak depth, mm\"].to_numpy()\n",
    "bl_2012 = scandens_beaks_2012[\"blength\"].to_numpy()\n",
    "bd_2012 = scandens_beaks_2012[\"bdepth\"].to_numpy()\n",
    "print(bl_1975)\n",
    "print()\n",
    "print(bd_1975)\n",
    "print()\n",
    "print(bl_2012)\n",
    "print()\n",
    "print(bd_2012)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191b3ac9-4c19-4dc9-bbe9-e78ff8aa9a84",
   "metadata": {},
   "source": [
    "## Parameter Estimation by Optimization\n",
    "\n",
    "### Optimal Parameters\n",
    "\n",
    "scipy.stats and statsmodels are two good Python packages for statistical inference by optimization. This course, however, focuses on the use of hacker statistics, which is adaptable to a wide range of statistical problems.\n",
    "\n",
    "#### CDFs of Expected and Observed Games between No-Hitters (Extra)\n",
    "\n",
    "These are my thoughts before attempting the course's exercise:\n",
    "- The waiting time between no-hitters could be modeled using the exponential distribution.\n",
    "- What is the parameter of the exponential distribution?\n",
    "- Estimate the parameter.\n",
    "- Plot the ECDF.\n",
    "- Using the estimated parameters and hacker statistics, get a random sample from the distribution and plot it against the ECDF.\n",
    "- How do they compare?\n",
    "\n",
    "Look at the section in the \"Statistical Thinking in Python (Part 1)\" course about the exponential distribution. We need to estimate the parameter for the exponential distribution by calculating the mean number of games between no-hitters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f07833c-a398-49ae-a7f4-b822629cef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of nohitter_times as tau.\n",
    "# In modern baseball, there are 30 teams that play 162 games each.\n",
    "# Since each game involves 2 teams, there are 30 * 162 / 2 = 2,430 games\n",
    "# per season. This means there are about 3 nohitters per season on average.\n",
    "tau = nohitter_times.mean()\n",
    "print(\"mean of nohitter_times:\", tau)\n",
    "\n",
    "# Draw a random sample from the exponential distribution with parameter tau.\n",
    "inter_nohitter_time = rng.exponential(tau, size=100000)\n",
    "\n",
    "# Plot the CDF of the random sample and the ECDF of the observed data.\n",
    "# Use a line plot with the expected data.\n",
    "# The two CDFs are very similar.\n",
    "x_e, y_e = ecdf(inter_nohitter_time)\n",
    "plt.plot(x_e, y_e, label=\"expected\")\n",
    "x_o, y_o = ecdf(nohitter_times)\n",
    "plt.plot(x_o, y_o, marker=\".\", linestyle=\"none\", label=\"observed\")\n",
    "plt.xlabel(\"Games between nohitters\")\n",
    "plt.ylabel(\"CDF\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d790bbb5-30d1-4642-9bab-3bb6c560a9a3",
   "metadata": {},
   "source": [
    "#### How Often Do We Get No-Hitters? (PDF Plot) (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed51dc4e-8b75-4144-9ef9-30235e6239b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the PDF of the random sample and label axes.\n",
    "_ = plt.hist(inter_nohitter_time, bins=50, density=True, histtype=\"step\")\n",
    "_ = plt.xlabel('Games between no-hitters')\n",
    "_ = plt.ylabel('PDF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90100f85-a123-4de1-b93e-f66afa263b0f",
   "metadata": {},
   "source": [
    "#### Do the Data Follow Our Story? (ECDF Plot) (Exercise)\n",
    "\n",
    "This exercise repeats the work I did above plotting the ECDF of the observed and expected data.\n",
    "\n",
    "> It looks like no-hitters in the modern era of Major League Baseball are Exponentially distributed. Based on the story of the Exponential distribution, this suggests that they are a random process; when a no-hitter will happen is independent of when the last no-hitter was."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2547308-c6bc-41f9-a54f-c2ccd0d86a43",
   "metadata": {},
   "source": [
    "#### How Is this Parameter Optimal? (Exercise)\n",
    "\n",
    "Plot the theoretical (expected) CDFs from tau, tau / 2, and tau * 2 along with the observed CDF. The best match is when using tau to represent the parameter of the exponential distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c720e557-183f-4138-9d5f-f8a0e9a1e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take samples for tau / 2 and tau * 2.\n",
    "samples_half = rng.exponential(tau / 2, size=100000)\n",
    "samples_double = rng.exponential(tau * 2, size=100000)\n",
    "\n",
    "# Generate CDFs from these samples\n",
    "x_half, y_half = ecdf(samples_half)\n",
    "x_double, y_double = ecdf(samples_double)\n",
    "\n",
    "# Plot the theoretical CDFs and the observed CDF.\n",
    "plt.plot(x_e, y_e, label=\"expected (tau)\")\n",
    "plt.plot(x_half, y_half, label=\"expected (tau / 2)\")\n",
    "plt.plot(x_double, y_double, label=\"expected (tau * 2)\")\n",
    "plt.plot(x_o, y_o, marker='.', linestyle='none', label=\"observed\")\n",
    "plt.margins(0.02)\n",
    "plt.xlabel('Games between no-hitters')\n",
    "plt.ylabel('CDF')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f81ce2-3c8d-43ee-9228-d01126ceb1c8",
   "metadata": {},
   "source": [
    "### Linear Regression by Least Squares\n",
    "\n",
    "This course uses `np.polyfit()` to estimate the intercept and slope of a polynomial of degree 1 for fitting a line to the data. See https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html.\n",
    "\n",
    "```Python\n",
    "slope, intercept = np.polyfit(x, y, 1)\n",
    "```\n",
    "\n",
    "NumPy recommends use of `np.polynomial.Polynomial.fit()`; see https://numpy.org/doc/stable/reference/routines.polynomials.html. Note that the new method behaves very differently.\n",
    "\n",
    "```Python\n",
    "intercept, slope = np.polynomial.Polynomial.fit(x, y, 1).convert().coef\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd177ce-d8e9-4561-95b3-481b82af734c",
   "metadata": {},
   "source": [
    "#### EDA of literacy/fertility Data (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca0f2e3-7cfc-4079-b04d-fcf20b01e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For exploratory data analysis (EDA), plot the fertility rate as a function\n",
    "# of the illiteracy rate.\n",
    "plt.plot(illiteracy, fertility, marker='.', linestyle='none')\n",
    "plt.margins(0.02)\n",
    "plt.xlabel('Percent illiterate')\n",
    "plt.ylabel('Fertility')\n",
    "plt.show()\n",
    "\n",
    "# Show the Pearson correlation coefficient.\n",
    "print(\"{:.3f}\".format(pearson_r(illiteracy, fertility)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db723079-231c-4dd6-a495-377fcb56222a",
   "metadata": {},
   "source": [
    "#### Linear Regression of Fertility vs. Illiteracy (Exercise)\n",
    "\n",
    "This code uses `np.polyfit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc274089-9b79-44ae-82a4-c19a4949d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the illiteracy rate versus fertility.\n",
    "plt.plot(illiteracy, fertility, marker='.', linestyle='none')\n",
    "\n",
    "# Perform a linear regression using np.polyfit().\n",
    "slope, intercept = np.polyfit(illiteracy, fertility, 1)\n",
    "print('intercept = {:.3f} children per woman'.format(intercept))\n",
    "print('slope = {:.3f} children per woman / percent illiterate'.format(slope))\n",
    "\n",
    "# Plot the regression line.\n",
    "x = np.array([illiteracy.min(), illiteracy.max()])\n",
    "y = x * slope + intercept\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.xlabel('percent illiterate')\n",
    "plt.ylabel('fertility')\n",
    "plt.margins(0.02)\n",
    "plt.xticks(np.arange(0, 110, 10))\n",
    "plt.yticks(np.arange(0, 9, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffb219b-b112-4f00-95bb-9387bb17ee55",
   "metadata": {},
   "source": [
    "#### Linear Regression using `Polynomial.fit()` (Extra)\n",
    "\n",
    "This code uses `np.polynomial.Polynomial.fit().convert().coef` (which is ugly and hard to understand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6ca7d-7086-4336-8554-4b1f3581a72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the illiteracy rate versus fertility.\n",
    "plt.plot(illiteracy, fertility, marker='.', linestyle='none')\n",
    "\n",
    "# Perform a linear regression using Polynomial.fit().\n",
    "# The API is very ugly.\n",
    "# I do not understand why .convert() has to be used.\n",
    "# The model is y = b0 + b1 * x\n",
    "b0, b1 = np.polynomial.Polynomial.fit(illiteracy, fertility, 1).convert().coef\n",
    "print(\"intercept = {:.3f} children per woman\".format(b0))\n",
    "print(\"slope = {:.3f} children per woman / percent illiterate\".format(b1))\n",
    "\n",
    "# Plot the regression line.\n",
    "x_reg = np.array([illiteracy.min(), illiteracy.max()])\n",
    "y_reg = b0 + b1 * x_reg\n",
    "plt.plot(x_reg, y_reg)\n",
    "\n",
    "plt.xlabel('percent illiterate')\n",
    "plt.ylabel('fertility')\n",
    "plt.margins(0.02)\n",
    "plt.xticks(np.arange(0, 110, 10))\n",
    "plt.yticks(np.arange(0, 9, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc578fd-059c-473d-9ca8-3929f79eed18",
   "metadata": {},
   "source": [
    "This code sets the `domain` and `window` values when fitting, preventing scaling of `domain` to `window`. This is the best way to use the polynomial fitting methods because no conversion is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db891bd-877d-4a1a-97ca-ddf620e014c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the illiteracy rate versus fertility.\n",
    "plt.plot(illiteracy, fertility, marker='.', linestyle='none')\n",
    "\n",
    "# Perform a linear regression using Polynomial.fit().\n",
    "# Set the domain and the window to use the domain of the observed data.\n",
    "# The model is y = b0 + b1 * x\n",
    "domain = [illiteracy.min(), illiteracy.max()]\n",
    "b0_1, b1_1 = np.polynomial.Polynomial.fit(illiteracy, fertility, 1, domain=domain, window=domain)\n",
    "print('intercept = {:.3f} children per woman'.format(b0_1))\n",
    "print('slope = {:.3f} children per woman / percent illiterate'.format(b1_1))\n",
    "\n",
    "# Plot the regression line.\n",
    "x_reg = np.array([illiteracy.min(), illiteracy.max()])\n",
    "y_reg = b0_1 + b1_1 * x_reg\n",
    "plt.plot(x_reg, y_reg)\n",
    "\n",
    "plt.xlabel('percent illiterate')\n",
    "plt.ylabel('fertility')\n",
    "plt.margins(0.02)\n",
    "plt.xticks(np.arange(0, 110, 10))\n",
    "plt.yticks(np.arange(0, 9, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7cf816-0604-48c6-be0c-8922604fee14",
   "metadata": {},
   "source": [
    "#### How Is the Slope Fit Optimal? (Exercise)\n",
    "\n",
    "For different slopes, calculate the sum of squares of the residuals (RSS), and plot them. What is the slope for which the RSS is minimal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f714aae4-64c5-4c45-8385-456477d58ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above, slope represents the slope of the regression line.\n",
    "# Specify a range of slopes to consider: slope_vals.\n",
    "# I have increased the number of slope_vals to consider in the range\n",
    "# to get a better estimate of the slope_val that minimizes RSS.\n",
    "slope_vals = np.linspace(0, 0.1, 200001)\n",
    "\n",
    "# Initialize sum of square of residuals: rss.\n",
    "# This is an np.ndarray with the same shape as a_vals.\n",
    "rss = np.empty_like(slope_vals)\n",
    "\n",
    "# Compute sum of square of residuals for each value of a_vals.\n",
    "for i, slope_val in enumerate(slope_vals):\n",
    "    rss[i] = np.sum((fertility - (slope_val * illiteracy + intercept)) ** 2)\n",
    "\n",
    "# Plot the RSS as a function of a_vals.\n",
    "plt.plot(slope_vals, rss, '-')\n",
    "\n",
    "# Plot the slope.\n",
    "plt.plot([slope, slope], [100, 200])\n",
    "\n",
    "# Customize the figure and show it.\n",
    "plt.xlabel('Slope (children per woman / percent illiterate)')\n",
    "plt.ylabel('Sum of square of residuals')\n",
    "plt.show()\n",
    "\n",
    "# Find the index of the minimum value in rss. Use the index to obtain the\n",
    "# corresponding slope. Compare this to the slope obtained from the\n",
    "# linear regression.\n",
    "min_rss = rss[0]\n",
    "min_rss_index = 0\n",
    "for i, rss in enumerate(rss):\n",
    "    if rss < min_rss:\n",
    "        min_rss = rss\n",
    "        min_rss_index = i\n",
    "print(\"min_rss: {:.3f}\".format(min_rss))\n",
    "print(\"slope for min_rss: {:.3f}\".format(slope_vals[min_rss_index]))\n",
    "print(\"least squares slope: {:.3f}\".format(slope))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea216116-c4cb-456c-afa3-81e2fef2b28e",
   "metadata": {},
   "source": [
    "#### How is the Intercept Fit Optimal? (Extra)\n",
    "\n",
    "Show that the optimal intercept was calculated by testing different intercept values using RSS (residual sum of squares)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbdebd4-9f1a-43d8-be7d-221cda535b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above, intercept represents the intercept of the regression line.\n",
    "# Specify a range of intercepts to consider: intercept_vals.\n",
    "intercept_vals = np.linspace(1.80, 2.00, 100001)\n",
    "\n",
    "# Initialize sum of square of residuals: rss.\n",
    "# This is an np.ndarray with the same shape as intercept_vals.\n",
    "rss = np.empty_like(intercept_vals)\n",
    "\n",
    "# Compute sum of square of residuals for each value of intercept_vals.\n",
    "for i, intercept_val in enumerate(intercept_vals):\n",
    "    rss[i] = np.sum((fertility - (slope * illiteracy + intercept_val)) ** 2)\n",
    "\n",
    "# Plot the RSS as a function of intercept_vals.\n",
    "plt.plot(intercept_vals, rss, '-')\n",
    "plt.plot([intercept, intercept], [115, 117])\n",
    "plt.xlabel('Intercept (children per woman)')\n",
    "plt.ylabel('Sum of square of residuals')\n",
    "plt.show()\n",
    "\n",
    "# Find the index of the minimum value in rss. Use the index to obtain the\n",
    "# corresponding slope. Compare this to the slope obtained from the\n",
    "# linear regression.\n",
    "min_rss = rss[0]\n",
    "min_rss_index = 0\n",
    "for i, rss in enumerate(rss):\n",
    "    if rss < min_rss:\n",
    "        min_rss = rss\n",
    "        min_rss_index = i\n",
    "print(\"min_rss: {:.3f}\".format(min_rss))\n",
    "print(\"intercept for min_rss: {:.3f}\".format(intercept_vals[min_rss_index]))\n",
    "print(\"least squares intercept: {:.3f}\".format(intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a7d606-d020-414e-8a60-7d5445872002",
   "metadata": {},
   "source": [
    "Note that the minimum for both the slope and the intercept is the same. We can think of plotting these in 3-dimensional space, where we're looking for the joint parameters with a minimum RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a067665c-e7f1-481e-8d74-61ac4ad07389",
   "metadata": {},
   "source": [
    "### The Importance of EDA: Anscombe's Quartet\n",
    "\n",
    "See https://en.wikipedia.org/wiki/Anscombe%27s_quartet and https://blog.revolutionanalytics.com/2017/05/the-datasaurus-dozen.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb96aadf-8d01-4553-8062-c2fce96b9d61",
   "metadata": {},
   "source": [
    "#### The Importance of EDA (Exercise)\n",
    "\n",
    "After importing and cleaning your data, the first step is exploratory data analysis, because:\n",
    "- You can be protected from misinterpretation of the type demonstrated by Anscombe's quartet.\n",
    "- EDA provides a good starting point for planning the rest of your analysis.\n",
    "- EDA is not really any more difficult than any of the subsequent analysis, so there is no excuse for not exploring the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eaf27c-c0f1-4f48-8bf5-e724d5ee7532",
   "metadata": {},
   "source": [
    "#### Linear Regression on Appropriate Anscombe Data (Exercise)\n",
    "\n",
    "Using the data from the first member of the Anscombe Quartet, plot the data and the linear regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2eacec-06f5-4856-89ef-2b4a3b0c6085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the slope and intercept using ordinary least squares\n",
    "# linear regression.\n",
    "x = anscombe_x[0]\n",
    "y = anscombe_y[0]\n",
    "anscombe_slope, anscombe_intercept = np.polyfit(x, y, 1)\n",
    "print(\"slope: {:.4f}; intercept: {:.4f}\".format(anscombe_slope, anscombe_intercept))\n",
    "\n",
    "# Generate theoretical x and y data: x_theor, y_theor\n",
    "# from the slope and intercept.\n",
    "x_theor = np.array([min(x), max(x)])\n",
    "y_theor = x_theor * anscombe_slope + anscombe_intercept\n",
    "\n",
    "# Plot the Anscombe data and theoretical line\n",
    "plt.plot(x, y, marker=\".\", linestyle=\"none\")\n",
    "plt.plot(x_theor, y_theor, \"-\")\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b9189f-61ee-44e5-b1bd-c152488dfea1",
   "metadata": {},
   "source": [
    "#### Linear Regression on All Anscombe Data (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f9e450-b8d2-4c18-8e2f-09dc4ab10707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each of the four Anscombe data sets, calculate and print the slope\n",
    "# and intercept.\n",
    "for x, y in zip(anscombe_x, anscombe_y):\n",
    "    anscombe_slope2, anscombe_intercept2 = np.polyfit(x, y, 1)\n",
    "    print('slope: {:.4f}; intercept: {:.4f}'.format(anscombe_slope2, anscombe_intercept2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba0ca32-369d-4b32-8965-e0868fee05c7",
   "metadata": {},
   "source": [
    "#### Plot Anscombe's Quartet (Extra)\n",
    "\n",
    "Create a 2 x 2 plot containing Anscombe's Quartet. See the \"Introduction to Data Analysis with Matplotlib\" course for how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1f5233-c6db-4765-80ff-6fa4a3d51115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code uses loops to remove repetitive code.\n",
    "# Initialize plotting parameters.\n",
    "fig, ax = plt.subplots(2, 2)\n",
    "fig.set_size_inches((12, 9))\n",
    "xticks = np.arange(0, 21, 2)\n",
    "yticks = np.arange(0, 15, 2)\n",
    "marker = \".\"\n",
    "linestyle = \"none\"\n",
    "loc = \"lower right\"\n",
    "label = \"slope: {:.3f}, intercept: {:.3f}\"\n",
    "\n",
    "# row, column refer to the rows and columns in the figure.\n",
    "# i is the index to the data in anscombe_x and anscombe_y.\n",
    "for row in range(0, 2):\n",
    "    for col in range(0, 2):\n",
    "        # Fill each subplot.\n",
    "        i = row * 2 + col\n",
    "        \n",
    "        # Calculate the slope and intercept.\n",
    "        anscombe_slope3, anscombe_intercept3 = np.polyfit(anscombe_x[i], anscombe_y[i], 1)\n",
    "        \n",
    "        # Draw the regression lines.\n",
    "        x_theor = np.array([anscombe_x[i].min(), anscombe_x[i].max()])\n",
    "        y_theor = np.array(x_theor * anscombe_slope3 + anscombe_intercept3)\n",
    "        ax[row, col].plot(x_theor, y_theor, label=label.format(anscombe_slope3, anscombe_intercept3))\n",
    "        ax[row, col].legend(loc=loc)\n",
    "        \n",
    "        # Plot the data points.\n",
    "        ax[row, col].plot(anscombe_x[i], anscombe_y[i], marker=marker, linestyle=linestyle)\n",
    "        ax[row, col].set_xticks(xticks)\n",
    "        ax[row, col].set_yticks(yticks)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f28e48e-4922-421e-b463-921f8f07124a",
   "metadata": {},
   "source": [
    "## Bootstrap Confidence Intervals\n",
    "\n",
    "### Generating Bootstrap Replicates\n",
    "\n",
    "Bootstrapping is the use of resampled data to perform statistical inference. A bootstrap sample is created by sampling from the original data with replacement. A bootstrap replicate is a statistics computed from a bootstrap sample (a resampled array). This can be done many times to create a series of bootstrap replicates, which can be plotted as an ECDF.\n",
    "\n",
    "#### Using `rng.choice()` for Bootstrap Sampling (Demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affacd4c-0892-4dad-ab58-eb489fc756c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using rng.choice() to create a sample from an existing data set.\n",
    "original_data = [1, 2, 3, 4, 5]\n",
    "print(rng.choice(original_data, size=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b105f81f-eb25-431b-93d6-048eff445ee9",
   "metadata": {},
   "source": [
    "#### Bootstrap Sampling from Michelson's Speed of Light Data (Demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de823eb-927e-4252-a157-56e6d18aa036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparison, print summary statistics of the original data.\n",
    "print(\"empirical values\")\n",
    "print(np.mean(michelson_speed_of_light))\n",
    "print(np.median(michelson_speed_of_light))\n",
    "print(np.std(michelson_speed_of_light))\n",
    "print()\n",
    "\n",
    "# Create a bootstrap sample of Michelson's speed of light data.\n",
    "light_bs_sample = rng.choice(michelson_speed_of_light, size=100)\n",
    "# Compute bootstrap replicate statistics from the single bootstrap sample.\n",
    "print(\"bootstrap values\")\n",
    "print(np.mean(light_bs_sample))\n",
    "print(np.median(light_bs_sample))\n",
    "print(np.std(light_bs_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f52ae8-094e-4869-b48c-a1bef1cf9bec",
   "metadata": {},
   "source": [
    "#### Getting the Terminology Down (Exercise)\n",
    "\n",
    "If we have a data set with  repeated measurements, a bootstrap sample is an array of length _n_ that was drawn from the original data with replacement. What is a bootstrap replicate?\n",
    "\n",
    "It is a single value of a statistic computed from the bootstrap sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6717ccad-d648-4e79-8200-155dfb587c94",
   "metadata": {},
   "source": [
    "#### Bootstrapping by Hand (Exercise)\n",
    "\n",
    "How many unique bootstrap samples can be created from a data set containing `[-1, 0, 1]`?\n",
    "\n",
    "There are 3 * 3 * 3 choices, or 27 different bootstrap samples that can be obtained from the data. The largest mean would come from sample `[1, 1, 1]`, which has a mean of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17acee8-dc40-4721-ac61-d762b1ac94f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Visualizing Bootstrap Samples (Exercise)\n",
    "\n",
    "\"Notice how the bootstrap samples give an idea of how the distribution of rainfalls is spread.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6aa90d-6ef9-4845-b8b0-0510477c4fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ECDFs of 50 bootstrap samples of the original data.\n",
    "for i in range(50):\n",
    "    # Generate a bootstrap sample.\n",
    "    bs_sample = rng.choice(rainfall, size=len(rainfall))\n",
    "\n",
    "    # Compute and plot ECDF from bootstrap sample.\n",
    "    x, y = ecdf(bs_sample)\n",
    "    _ = plt.plot(x, y, marker='.', linestyle='none',\n",
    "                 color='gray', alpha=0.1)\n",
    "\n",
    "# Compute and plot ECDF from original data.\n",
    "x, y = ecdf(rainfall)\n",
    "_ = plt.plot(x, y, marker='.')\n",
    "\n",
    "# Make margins and label axes.\n",
    "plt.margins(0.02)\n",
    "_ = plt.xlabel('Yearly rainfall (mm)')\n",
    "_ = plt.ylabel('ECDF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c71afd5-709b-49f9-87fb-df11fbea837e",
   "metadata": {},
   "source": [
    "### Bootstrap Confidence Intervals\n",
    "\n",
    "#### Bootstrap Replicate Function (Demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db573c5d-c1e6-4ea3-b8f2-60103318510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bootstrap replicate from a 1-D array of data.\n",
    "# func is the function to apply to the bootstrap sample that computes\n",
    "# the statistic of interest (e.g., np.mean, np.median).\n",
    "def bootstrap_replicate_1d(data, func):\n",
    "    bs_sample = rng.choice(data, size=len(data))\n",
    "    return func(bs_sample)\n",
    "\n",
    "print(bootstrap_replicate_1d(michelson_speed_of_light, np.mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130dcb32-8d4f-42b6-a2cf-1c3bea078263",
   "metadata": {},
   "source": [
    "#### Create Bootstrap Replicates (Demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bac32d-8d87-45eb-9a56-74dd2fdf8675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bootstrap replicates.\n",
    "iterations = 10000\n",
    "light_bs_replicates = np.empty(iterations)   \n",
    "for i in range(iterations):\n",
    "    light_bs_replicates[i] = bootstrap_replicate_1d(michelson_speed_of_light, np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55d504d-8747-4bdb-ba87-418fa1e46420",
   "metadata": {},
   "source": [
    "#### Plot a Histogram of Bootstrap Replicates (Demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936efca9-ec45-4dc6-b61a-aeb625c27ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the bootstrap replicates.\n",
    "# Use 100 bins for 10000 samples.\n",
    "# By setting density=True, the histogram approximates a probability\n",
    "# density function.\n",
    "bins=100\n",
    "plt.hist(light_bs_replicates, bins=bins, density=True)\n",
    "plt.xlabel(\"Mean speed of light (km/s)\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67efe210-1445-454e-85c4-ef9684d127e6",
   "metadata": {},
   "source": [
    "#### Confidence Interval of a Statistic (Demonstration)\n",
    "\n",
    "If we repeated measurements over and over again, p% of the observed values would lie within the p% confidence interval. We can use `np.percentile()` to calculate the boundaries of the 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caab2966-c9a4-4b51-a6bd-a6a3eee18588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 95% confidence interval.\n",
    "# The speed of light in a vacuum is 299,792.458 km/s.\n",
    "# The speed of light in air is about 90 km/s (56 mi/s) slower than c.\n",
    "conf_int = np.percentile(light_bs_replicates, [2.5, 97.5])\n",
    "print(conf_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dc439a-1a1b-402f-ac35-ceda38f02ed1",
   "metadata": {},
   "source": [
    "#### Generating Many Bootstrap Replicates (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1d6114-fef1-4cca-91c8-70e26952d37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for generating many bootstrap replicates.\n",
    "def draw_bs_reps(data, func, size=1):\n",
    "    bs_replicates = np.empty(size)\n",
    "    for i in range(size):\n",
    "        bs_replicates[i] = bootstrap_replicate_1d(data, func)\n",
    "    return bs_replicates\n",
    "\n",
    "# Test the code: Draw the bootstrap replicates and plot the histogram as\n",
    "# a PDF.\n",
    "light_bs_replicates2 = draw_bs_reps(michelson_speed_of_light, np.mean, size=10000)\n",
    "bins=100\n",
    "plt.hist(light_bs_replicates2, bins=bins, density=True)\n",
    "plt.xlabel(\"Mean speed of light (km/s)\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.show()\n",
    "conf_int = np.percentile(light_bs_replicates2, [2.5, 97.5])\n",
    "print(conf_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4400ff5c-de79-4eb9-a34c-47a28c78e9f1",
   "metadata": {},
   "source": [
    "#### Bootstrap Replicates of the Mean and SEM (Exercise)\n",
    "\n",
    "This exercise demonstrates two ways to calculate the standard error of the mean, one from theory and one from bootstrap replicates. The two values are very close. This exercise also shows that the distribution of the bootstrap replicates (means) is well approximated by the normal distribution, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86149f14-a4b1-484c-92d3-ba1614be42d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 10,000 bootstrap replicates of the mean: bs_replicates\n",
    "rainfall_bs_replicates = draw_bs_reps(rainfall, np.mean, size=10000)\n",
    "\n",
    "# Compute and print SEM from the standard deviation of rainfall.\n",
    "sem = np.std(rainfall) / np.sqrt(len(rainfall))\n",
    "print(\"Computed SEM: {:.2f}\".format(sem))\n",
    "\n",
    "# Compute and print the standard deviation of bootstrap replicates.\n",
    "# This is another estimate of the standard error of the mean!\n",
    "bs_std = np.std(rainfall_bs_replicates)\n",
    "print(\"Standard deviation of bootstrap replicate means: {:.2f}\".format(bs_std))\n",
    "\n",
    "# Make a histogram of the results\n",
    "_ = plt.hist(rainfall_bs_replicates, bins=50, density=True)\n",
    "_ = plt.xlabel('Mean annual rainfall (mm)')\n",
    "_ = plt.ylabel('PDF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0775e2b-923f-49ae-88f4-ae6f02fa0536",
   "metadata": {},
   "source": [
    "#### Confidence Intervals of Rainfall Data (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c286da-9978-4f65-b264-bf4c7742e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 95% confidence interval for mean annual rainfall.\n",
    "percentile_low, percentile_high = np.percentile(rainfall_bs_replicates, [2.5, 97.5])\n",
    "print(\"95% confidence interval: {:.1f} - {:.1f}\".format(percentile_low, percentile_high))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1f4d88-8557-47e5-afa6-4707ccd2a53b",
   "metadata": {},
   "source": [
    "#### Bootstrap Replicates of Other Statistics (Exercise)\n",
    "\n",
    "We saw in the previous exercise that the mean is normally distributed. This is not necessarily true of other statistics, but we can use hacker statistics to explore the distribution of other statistics using bootstrap replicates.\n",
    "\n",
    "In this exercise, it appears that the variance of the rainfall data is skewed to the right.\n",
    "\n",
    "> This is not normally distributed, as it has a longer tail to the right. Note that you can also compute a confidence interval on the variance, or any other statistic, using `np.percentile()` with your bootstrap replicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607f3715-3b2c-442f-a94e-14e8357dd3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of bootstrap replicates of the variance of rainfall\n",
    "# in Sheffield. Convert units from square mm to square cm.\n",
    "rainfall_bs_replicates2 = draw_bs_reps(rainfall, np.var, size=10000)\n",
    "rainfall_bs_replicates2 = rainfall_bs_replicates2 / 100\n",
    "plt.hist(rainfall_bs_replicates2, density=True, bins=100)\n",
    "plt.xlabel(\"Variance of annual rainfall (square cm)\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a2358e-7b8c-4f82-9f6f-b086535b74c8",
   "metadata": {},
   "source": [
    "#### Confidence Interval on the Rate of No-Hitters (Exercise)\n",
    "\n",
    "Generate 10,000 bootstrap replicates of the optimal parameter tau. Plot a histogram of your replicates and report a 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2174e1-8544-45c3-a663-09ad9c6a4c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 10,000 bootstrap replicates of the optimal parameter tau. Plot a\n",
    "# histogram of your replicates and report a 95% confidence interval.\n",
    "nohitter_bs_replicates = draw_bs_reps(nohitter_times, np.mean, size=10000)\n",
    "plt.hist(nohitter_bs_replicates, density=True, bins=100)\n",
    "plt.xlabel(r\"$\\tau$\" + \" (mean games between no-hitters)\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.show()\n",
    "conf_interval = np.percentile(nohitter_bs_replicates, [2.5, 97.5])\n",
    "print(\"95% confidence interval of tau: {:.1f} - {:.1f} games\".format(conf_interval[0], conf_interval[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecafb43-9b4a-4eda-843d-ff498ca83aca",
   "metadata": {},
   "source": [
    "### Pairs Bootstrap\n",
    "\n",
    "Pairs bootstrap makes the least assumptions about the data. Each bootstrap sample contains the data for a county, where the data pair is the percentage of the vote for Obama and the total vote in the county in thousands of votes.\n",
    "\n",
    "#### Linear Regression Model of 2008 Swing State Voting Data (Demonstration)\n",
    "\n",
    "This code recreates the 2008 swing state voting plot used in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be421bd4-9106-418e-af5d-a4263e3a37f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the percent of votes for Obama as a function of total votes\n",
    "# (thousands).\n",
    "\n",
    "# Create the data inputs.\n",
    "obama_percent = swing_states[\"dem_votes\"] * 100 / swing_states[\"total_votes\"]\n",
    "total_votes_thousands = swing_states[\"total_votes\"] / 1000\n",
    "\n",
    "# Create a linear regression model.\n",
    "slope_reg, intercept_reg = np.polyfit(total_votes_thousands, obama_percent, 1)\n",
    "\n",
    "# Plot the regression line.\n",
    "x = np.array([0, total_votes_thousands.max()])\n",
    "y_reg = x * slope_reg + intercept_reg\n",
    "plt.plot(x, y_reg)\n",
    "\n",
    "# Plot the data points.\n",
    "plt.plot(total_votes_thousands, obama_percent, marker='.', linestyle='none')\n",
    "\n",
    "# Adjust the plot and show it.\n",
    "plt.margins(0.02)\n",
    "plt.xticks(np.arange(0, 1000, 100))\n",
    "plt.yticks(np.arange(0, 110, 10))\n",
    "plt.xlabel(\"Total votes (thousands)\")\n",
    "plt.ylabel(\"Percent of Votes for Obama\")\n",
    "plt.show()\n",
    "\n",
    "# Print the parameters of the regression model.\n",
    "print(\"slope: {:.3f}\".format(slope_reg))\n",
    "print(\"intercept: {:.3f}\".format(intercept_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1ae375-6525-4148-9bff-b0625d296a7d",
   "metadata": {},
   "source": [
    "When working with the voting data, we need to resample the data for the counties to get two values, the percentage of votes for Obama and the total votes in thousands. A bootstrap sample is obtained by creating a random sample of the indexes of the data and using the index to obtain the two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3453973f-d8fb-474c-8bcf-423294a8dd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bootstrap replicates of the linear models.\n",
    "# Use obama_percent and total_votes_thousands as the sources for the bootstrap\n",
    "# samples.\n",
    "# I wrote all of this code before doing the exercises.\n",
    "# I revised the code while watching the video.\n",
    "size = 1000\n",
    "indices = np.arange(len(obama_percent))\n",
    "slope_reps = np.empty(size)\n",
    "intercept_reps = np.empty(size)\n",
    "for i in range(size):\n",
    "    # Build the random sample.\n",
    "    # This operation is vectorized by NumPy.\n",
    "    bs_indices = rng.choice(indices, len(indices))\n",
    "    obama_percent_sample = obama_percent[bs_indices]\n",
    "    total_votes_thousands_sample = total_votes_thousands[bs_indices]\n",
    "    \n",
    "    # Create the linear regression model and save the parameters.\n",
    "    slope_rep, intercept_rep = np.polyfit(total_votes_thousands_sample, obama_percent_sample, 1)\n",
    "    slope_reps[i] = slope_rep\n",
    "    intercept_reps[i] = intercept_rep\n",
    "\n",
    "# Plot the regression lines from the replicates.\n",
    "for i in np.arange(len(slope_reps)):\n",
    "    y_rep = x * slope_reps[i] + intercept_reps[i]\n",
    "    plt.plot(x, y_rep, color=\"gray\", alpha=0.1)\n",
    "\n",
    "# Plot the original regression line over the top of the other lines.\n",
    "# Otherwise, the original regression line is obscured.\n",
    "plt.plot(x, y_reg)\n",
    "\n",
    "# Plot the points over the regression lines.\n",
    "plt.plot(total_votes_thousands, obama_percent, marker=\".\", linestyle=\"none\")\n",
    "\n",
    "# Adjust the plot and show it.\n",
    "plt.margins(0.02)\n",
    "plt.xticks(np.arange(0, 1000, 100))\n",
    "plt.yticks(np.arange(0, 110, 10))\n",
    "plt.xlabel(\"Total votes (thousands)\")\n",
    "plt.ylabel(\"Percent of Votes for Obama\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5896e40e-ec68-4956-9a65-969ef46b1d25",
   "metadata": {},
   "source": [
    "#### Variation in the Intercept Replicates (Extra)\n",
    "\n",
    "Zoom in on the intercept to visualize the variation in the intercept values for the bootstrap replicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7d9a7a-17d9-4f88-9c10-a9d2a04a4601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the regression lines of the bootstrap replicates.\n",
    "for i in np.arange(len(slope_reps)):\n",
    "    y_rep = x * slope_reps[i] + intercept_reps[i]\n",
    "    plt.plot(x, y_rep, color=\"gray\", alpha=0.1)\n",
    "\n",
    "# Plot the regression line over the replicate regression lines to make\n",
    "# it more visible.\n",
    "plt.plot(x, y_reg)\n",
    "\n",
    "# Plot the points over the regression lines.\n",
    "plt.plot(total_votes_thousands, obama_percent, marker=\".\", linestyle=\"none\")\n",
    "\n",
    "# Adjust the plot and show it.\n",
    "plt.margins(0.02)\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(37, 42)\n",
    "plt.xticks(np.arange(0, 11, 1))\n",
    "plt.yticks(np.arange(36, 44, 1))\n",
    "plt.xlabel(\"Total votes (thousands)\")\n",
    "plt.ylabel(\"Percent of Votes for Obama\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eef4986-bbab-4bed-be85-d2f421a032ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the slope bootstrap replicates.\n",
    "plt.hist(slope_reps, density=True, bins=30)\n",
    "plt.plot([slope_reg, slope_reg], [0, 100])\n",
    "plt.xlabel(\"Slope\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.show()\n",
    "\n",
    "# Print the 95% confidence interval for the slope.\n",
    "print(\"regression slope: {:.3f}\".format(slope_reg))\n",
    "print(\n",
    "    \"95% confidence interval of the regression slope: {:.3f}-{:.3f}.\".format(\n",
    "        np.percentile(slope_reps, 2.5),\n",
    "        np.percentile(slope_reps, 97.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2cdab5-de8b-40d5-abcd-ef334f03f005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the intercept bootstrap replicates.\n",
    "plt.hist(intercept_reps, density=True, bins=30)\n",
    "plt.plot([intercept_reg, intercept_reg], [0, 0.6])\n",
    "plt.xlabel(\"Intercept\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.show()\n",
    "print(\"regression intercept: {:.3f}\".format(intercept_reg))\n",
    "print(\n",
    "    \"95% confidence interval of the intercept: {:.3f}-{:.3f}.\".format(\n",
    "        np.percentile(intercept_reps, 2.5),\n",
    "        np.percentile(intercept_reps, 97.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485ff7db-0f95-4e93-80d4-20c55f2810df",
   "metadata": {},
   "source": [
    "#### A Function to Do Pairs Bootstrap (Exercise)\n",
    "\n",
    "Write a function for resampling pairs of data given the inputs `x`, `y`, and `size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd7fa1-6847-435f-8d7b-d2ff0c2eee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bs_pairs_linreg(x, y, size=1):\n",
    "    \"\"\"\n",
    "    Draw bootstrap linear regression pairs for b0 and b1 parameters.\n",
    "    Linear regression model is y = b0 + b1 * x\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    x : array of float\n",
    "        x axis data\n",
    "    y : array of float\n",
    "        y axis data\n",
    "    size : int\n",
    "        number of bootstrap replicates to generate\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    b0_replicates : array of float\n",
    "        bootstrap replicates of linear regression b0 parameter (intercept)\n",
    "    b1_replicates : array of float\n",
    "        bootstrap replicates of linear regression b1 parameter (slope)\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    # Generate the bootstrap replicates of the b0 and b1 parameters.\n",
    "    b0_replicates, b1_replicates = draw_bs_pairs_linreg(x, y, size=1000)\n",
    "    # Plot the linear regression lines of the bootstrap replicates.\n",
    "    lin_reg_domain = np.array([x.min(), x.max()])\n",
    "    for i in range(len(b0_replicates)):\n",
    "        lin_reg_range = b0_replicates[i] + b1_replicates[i] * lin_reg_domain\n",
    "        plt.plot(lin_reg_domain, lin_reg_range, color=\"gray\", alpha=0.1)\n",
    "    \"\"\"\n",
    "    indices = np.arange(len(x))\n",
    "    domain = [x.min(), x.max()]\n",
    "    b0_replicates = np.empty(size)\n",
    "    b1_replicates = np.empty(size)\n",
    "    for i in range(size):\n",
    "        # Build the bootstrap samples from pairs of x and y values.\n",
    "        bootstrap_indices = rng.choice(indices, size=len(indices))\n",
    "        x_bootstrap_sample = x[bootstrap_indices]\n",
    "        y_bootstrap_sample = y[bootstrap_indices]\n",
    "\n",
    "        # Create the linear regression model and save the parameters.\n",
    "        b0, b1 = \\\n",
    "            np.polynomial.Polynomial.fit(\n",
    "                x_bootstrap_sample,\n",
    "                y_bootstrap_sample,\n",
    "                1,\n",
    "                domain=domain,\n",
    "                window=domain)\n",
    "        b0_replicates[i] = b0\n",
    "        b1_replicates[i] = b1\n",
    "    return b0_replicates, b1_replicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6dcfd4-3b07-4476-9dab-7b7bf088bb6f",
   "metadata": {},
   "source": [
    "#### Test the Function to Do Pairs Bootstrap (Extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078a1b36-5f71-48ac-b00f-7491eaf99344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the bootstrap replicates.\n",
    "b0_replicates, b1_replicates = \\\n",
    "    draw_bs_pairs_linreg(total_votes_thousands, obama_percent, size=1000)\n",
    "\n",
    "# Plot the regression lines from the replicates.\n",
    "# The lines are plotted using two points, from the minimum x value to \n",
    "# the maximum x value recorded in domain.\n",
    "lin_reg_domain = np.array([total_votes_thousands.min(), total_votes_thousands.max()])\n",
    "for i in np.arange(len(b0_replicates)):\n",
    "    lin_reg_range = b0_replicates[i] + b1_replicates[i] * lin_reg_domain\n",
    "    plt.plot(lin_reg_domain, lin_reg_range, color=\"gray\", alpha=0.1)\n",
    "\n",
    "# Recreate the original regression line and plot it over the other lines.\n",
    "# Otherwise, the original regression line is obscured.\n",
    "b0, b1 = np.polynomial.Polynomial.fit(total_votes_thousands, obama_percent, 1, domain=domain, window=domain)\n",
    "lin_reg_range = b0 + b1 * lin_reg_domain\n",
    "plt.plot(lin_reg_domain, lin_reg_range)\n",
    "\n",
    "# Plot the points over the regression lines.\n",
    "plt.plot(total_votes_thousands, obama_percent, marker=\".\", linestyle=\"none\")\n",
    "\n",
    "# Adjust the plot and show it.\n",
    "plt.margins(0.02)\n",
    "plt.xticks(np.arange(0, 1000, 100))\n",
    "plt.yticks(np.arange(0, 110, 10))\n",
    "plt.xlabel(\"Total votes (thousands)\")\n",
    "plt.ylabel(\"Percent of Votes for Obama\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc6104a-0483-4d06-9424-c6a7ad04eaab",
   "metadata": {},
   "source": [
    "#### Pairs Bootstrap of Literacy/Fertility Data (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2de4cd-f83e-4340-a0f0-247454cc407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate replicates of slope and intercept using pairs bootstrap\n",
    "bs_intercept_reps, bs_slope_reps = draw_bs_pairs_linreg(illiteracy, fertility, size=1000)\n",
    "\n",
    "# Compute and print 95% CI for slope.\n",
    "percentiles = np.percentile(bs_slope_reps, [2.5, 97.5])\n",
    "print(\"95% confidence interval: {:.4f} - {:.4f}\".format(percentiles[0], percentiles[1]))\n",
    "\n",
    "# Plot the histogram\n",
    "_ = plt.hist(bs_slope_reps, bins=50, density=True)\n",
    "_ = plt.xlabel('slope')\n",
    "_ = plt.ylabel('PDF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc93b2c3-9a39-4671-9737-1177e9a0c693",
   "metadata": {},
   "source": [
    "#### Plotting Bootstrap Regressions (Exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54278ccf-6f47-4246-91fe-8283b2e5edba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 100 bootstrap replicate regression lines.\n",
    "\n",
    "# Create an array of two x values for drawing bootstrap regression lines.\n",
    "x = np.array([illiteracy.min(), illiteracy.max()])\n",
    "\n",
    "# Plot the bootstrap lines.\n",
    "for i in range(100):\n",
    "    plt.plot(\n",
    "        x, \n",
    "        bs_intercept_reps[i] + bs_slope_reps[i] * x,\n",
    "        linewidth=0.5,\n",
    "        alpha=0.2,\n",
    "        color='red')\n",
    "\n",
    "# Plot the data points as a scatter plot.    \n",
    "plt.plot(illiteracy, fertility, marker=\".\", linestyle=\"none\")\n",
    "\n",
    "# Customize the plot and show it.\n",
    "plt.xlabel('Illiteracy')\n",
    "plt.ylabel('Fertility')\n",
    "plt.xticks(np.arange(0, 110, 20))\n",
    "plt.yticks(np.arange(0, 9))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8522a474-e1e5-4249-8248-e8e7f624fe42",
   "metadata": {},
   "source": [
    "## Introduction to Hypothesis Testing\n",
    "\n",
    "### Formulating and Simulating a Hypothesis\n",
    "\n",
    "Ohio and Pennsylvania are similar states: They are located in the same region of the United States, they have liberal urban populations, and they have conservative rural populations. Jason's example hypothesis is that \"county-level voting in these two states have identical probability distributions.\" In fact, when we plotted the ECDFs for these two states in part 1 of this course, they were very similar.\n",
    "\n",
    "The hypothesis we are testing is the null hypothesis, which in this case is that there is no difference in county-level voting between Ohio and Pennsylvania.\n",
    "\n",
    "Plotting the ECDFs and examining the summary statistics show that the two states are similar, but is there a significant difference? We can't tell yet.\n",
    "\n",
    "These are my notes, not mentioned in the class at this point. No alternative hypothesis is proposed. One alternative hypothesis is that the share of votes for Democrats in Pennsylvania is greater than in Ohio; this is a one-tail test. Another alternative hypothesis is that the share of votes for Democrats in Pennsylvania is different (greater than or less than) from Ohio; this is a two-tail test.\n",
    "\n",
    "#### Comparing County-Level Voting in Ohio and Pennsylvania (Demonstration)\n",
    "\n",
    "Plotting the ECDFs of the Democratic share of the vote in individual counties shows that voting patterns in Ohio and Pennsylvania are very similar. Likewise, the summary statistics for the two states are very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d61eb1-5b77-4636-8a0f-c68a2b0ae03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ECDFs.\n",
    "ohio_votes = swing_states[swing_states[\"state\"] == \"OH\"]\n",
    "x_oh, y_oh = ecdf(ohio_votes[\"dem_share\"])\n",
    "penn_votes = swing_states[swing_states[\"state\"] == \"PA\"]\n",
    "x_pa, y_pa = ecdf(penn_votes[\"dem_share\"])\n",
    "plt.plot(x_oh, y_oh, marker=\".\", linestyle=\"none\", label=\"Ohio\")\n",
    "plt.plot(x_pa, y_pa, marker=\".\", linestyle=\"none\", label=\"Pennsylvania\")\n",
    "plt.xlabel(\"Democratic share of votes in county (%)\")\n",
    "plt.ylabel(\"ECDF\")\n",
    "plt.xticks(np.arange(0, 110, 10))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics.\n",
    "print(\"OH mean: {:.1f}%\".format(np.mean(x_oh)))\n",
    "print(\"PA mean: {:.1f}%\".format(np.mean(x_pa)))\n",
    "print(\"PA mean - OH mean: {:.1f}%\".format(np.mean(x_pa) - np.mean(x_oh)))\n",
    "print(\"OH median: {:.1f}%\".format(np.median(x_oh)))\n",
    "print(\"PA mean: {:.1f}%\".format(np.median(x_pa)))\n",
    "print(\"PA median - OH median: {:.1f}%\".format(np.median(x_pa) - np.median(x_oh)))\n",
    "print(\"OH standard deviation: {:.1f}%\".format(x_oh.std()))\n",
    "print(\"PA standard deviation: {:.1f}%\".format(x_pa.std()))\n",
    "print(\"PA stddev - OH stddev: {:.1f}%\".format(x_pa.std() - x_oh.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b510b9-bead-49aa-93fe-3b3893f2ce58",
   "metadata": {},
   "source": [
    "#### Generating a Permutation Sample (Demonstration)\n",
    "\n",
    "To test if the distributions are different, we combine the data sets (here for Ohio and Pennsylvania), permute the combined data set, and create two new data sets (that represent Ohio and Pennsylvania counties). We can compute the means of these two data sets and compare the distribution by creating many permutation samples. We test to see if the distributions of the means from the permutation samples overlap or are separate.\n",
    "\n",
    "The code below shows how to create the permutation samples and plot their ECDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24830ce-1416-4806-b0a6-9687a4d94659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a permutation sample.\n",
    "dem_share_both = np.concatenate((x_pa, x_oh))\n",
    "dem_share_perm = rng.permutation(dem_share_both)\n",
    "perm_sample_pa = dem_share_perm[:len(x_pa)]\n",
    "perm_sample_oh = dem_share_perm[len(x_pa):]\n",
    "\n",
    "# Plot the permuted data.\n",
    "x_pa_p, y_pa_p = ecdf(perm_sample_pa)\n",
    "x_oh_p, y_oh_p = ecdf(perm_sample_oh)\n",
    "plt.plot(x_oh_p, y_oh_p, marker=\".\", linestyle=\"none\", label=\"Ohio\")\n",
    "plt.plot(x_pa_p, y_pa_p, marker=\".\", linestyle=\"none\", label=\"Pennsylvania\")\n",
    "plt.xlabel(\"Permuted Democratic share of votes in county (%)\")\n",
    "plt.ylabel(\"ECDF\")\n",
    "plt.xticks(np.arange(0, 110, 10))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11cc765-d4a6-414c-b912-e0554476f84a",
   "metadata": {},
   "source": [
    "#### Generating a Permutation Sample (Exercise)\n",
    "\n",
    "Write a function that combines and permutes two data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a7f030-f6f7-4747-acbb-3939611224d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_sample(array1, array2):\n",
    "    \"\"\"Generate a permutation sample from two data sets.\"\"\"\n",
    "    # Combine the data, permute it, and return slices having sizes\n",
    "    # of the original data.\n",
    "    combined = np.concatenate((array1, array2))\n",
    "    permuted = rng.permutation(combined)\n",
    "    perm_sample_1 = permuted[:len(array1)]\n",
    "    perm_sample_2 = permuted[len(array1):]\n",
    "    return perm_sample_1, perm_sample_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0e895b-7634-4034-bf12-ea20d2383dc9",
   "metadata": {},
   "source": [
    "#### Visualize Permutation Sampling (Exercise)\n",
    "\n",
    "The following exercises use the Sheffield rainfall data for June and November, which we will see are different.\n",
    "\n",
    "> Notice that the permutation samples ECDFs overlap and give a purple haze. None of the ECDFs from the permutation samples overlap with the observed data, suggesting that the [null] hypothesis is not commensurate with the data. June and November rainfall are not identically distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b017a02b-2bf5-477b-87da-a3761d8d0b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine rain_june and rain_november from the Sheffield weather station and\n",
    "# permute the samples. Plot the ECDFs of the original data and of 50 permutted\n",
    "# sample.\n",
    "# This uses the rain_june and rain_november variables, which were initialized\n",
    "# in the Data Sets section near the top of this page.\n",
    "for i in range(50):\n",
    "    # Permut the samples and plot their ECDFs.\n",
    "    rain_june_perm, rain_november_perm = permutation_sample(rain_june, rain_november)\n",
    "    x_jun_p, y_jun_p = ecdf(rain_june_perm)\n",
    "    x_nov_p, y_nov_p = ecdf(rain_november_perm)\n",
    "    plt.plot(x_jun_p, y_jun_p, marker='.', linestyle='none',\n",
    "                 color='red', alpha=0.1)\n",
    "    plt.plot(x_nov_p, y_nov_p, marker='.', linestyle='none',\n",
    "                 color='blue', alpha=0.1)\n",
    "\n",
    "# Create and plot ECDFs from original data.\n",
    "x_jun, y_jun = ecdf(rain_june)\n",
    "x_nov, y_nov = ecdf(rain_november)\n",
    "plt.plot(x_jun, y_jun, marker='.', linestyle='none', color='red', label=\"June\")\n",
    "plt.plot(x_nov, y_nov, marker='.', linestyle='none', color='blue', label=\"November\")\n",
    "\n",
    "# Customize the plot and show it.\n",
    "plt.margins(0.02)\n",
    "plt.xlabel('Monthly rainfall (mm)')\n",
    "plt.ylabel('ECDF')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401d1341-a643-4a2d-853b-baabebd46e50",
   "metadata": {},
   "source": [
    "### Test Statistics and p-Values\n",
    "\n",
    "> A test statistic is a single number that can be computed from observed data and also from data you simulate under the null hypothesis. It serves as a basis of comparison between what the hypothesis predicts and what we actually observed. Importantly, you should choose your test statistic to be something that is pertinent to the question you are trying to answer with your hypothesis test, in this case, are the two states different? If they are identical, they should have the same mean vote share for Obama. So the difference in mean vote share should be zero. We will therefore choose the difference in means as our test statistic.\n",
    "\n",
    "#### Test Statistic for Voting Data (Demonstration)\n",
    "\n",
    "The difference in means for the permutation samples is called a permutation replicate (of the difference of the means)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843bdd05-c154-47b8-b839-af22b0f702f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the difference of the means of the permutation samples.\n",
    "print(\"permuted OH mean: {:.1f}%\".format(x_oh_p.mean()))\n",
    "print(\"permuted PA mean: {:.1f}%\".format(x_pa_p.mean()))\n",
    "print(\"permuted PA mean - OH mean: {:.1f}%\".format(x_pa_p.mean() - x_oh_p.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24205e1f-4317-49e9-94fb-2c9754e8fe39",
   "metadata": {},
   "source": [
    "#### Mean Vote Difference under the Null Hypothesis (Demonstration)\n",
    "\n",
    "Create 10,000 permutation replicates and plot them as a histogram. Compute the p-value, the probability that the difference in means is greater than or equal to the observed value given that the null hypothesis is true (that there is no difference in the means)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b06b815-a497-45de-8267-6ea6acd07475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the permutation replicates of the difference of the means for\n",
    "# Pennsylvania and Ohio and plot them as a histogram.\n",
    "size = 10000\n",
    "diff_means_perm_pa_oh_array = np.empty(size)\n",
    "for i in range(size):\n",
    "    perm_pa_votes, perm_oh_votes = \\\n",
    "        permutation_sample(penn_votes[\"dem_share\"], ohio_votes[\"dem_share\"])\n",
    "    diff_means_perm_pa_oh = perm_pa_votes.mean() - perm_oh_votes.mean()\n",
    "    diff_means_perm_pa_oh_array[i] = diff_means_perm_pa_oh\n",
    "\n",
    "# Plot the permutation replicates of the difference of means.\n",
    "plt.hist(diff_means_perm_pa_oh_array, bins=100, density=True)\n",
    "# Plot a vertical line showing the difference of means for the original data.\n",
    "diff_obs_means_pa_oh = x_pa.mean() - x_oh.mean()\n",
    "plt.plot([diff_obs_means_pa_oh, diff_obs_means_pa_oh], [0, 0.30], color=\"red\")\n",
    "plt.xlabel(\"PA - OH mean percent vote difference\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.show()\n",
    "\n",
    "# Print the p-value for a one-tail test.\n",
    "p_value_pa_oh_1_tail = 1 - (diff_means_perm_pa_oh_array < diff_obs_means_pa_oh).mean()\n",
    "print(\"p-value = {:.3f}\".format(p_value_pa_oh_1_tail))\n",
    "\n",
    "# Use scipy.stats.percentileofscore() to calculate the p-value.\n",
    "p_value_pa_oh_1_tail_scipy = 1 - scipy.stats.percentileofscore(diff_means_perm_pa_oh_array, diff_obs_means_pa_oh) / 100\n",
    "print(\"scipy p-value = {:.3f}\".format(p_value_pa_oh_1_tail_scipy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286d7aa6-d41d-4bb4-ba93-2ec2183164fc",
   "metadata": {},
   "source": [
    "#### Compare Ohio with Florida (Extra)\n",
    "\n",
    "The voting pattern in Florida appears to be different from Ohio; test this. The null hypothesis is that the means are not different. The alternative hypothesis is that the mean for Florida is less than the mean for Ohio. We reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59b1294-3783-4166-8e95-86cdac41ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare county-level voting in Ohio to Florida.\n",
    "flor_votes = swing_states[swing_states[\"state\"] == \"FL\"]\n",
    "x_fl, y_fl = ecdf(flor_votes[\"dem_share\"])\n",
    "plt.plot(x_oh, y_oh, marker=\".\", linestyle=\"none\", label=\"Ohio\")\n",
    "plt.plot(x_fl, y_fl, marker=\".\", linestyle=\"none\", label=\"Florida\")\n",
    "plt.xlabel(\"Democratic share of votes in county (%)\")\n",
    "plt.ylabel(\"ECDF\")\n",
    "plt.xticks(np.arange(0, 110, 10))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Create the permutation replicates of the difference of the means for\n",
    "# Florida and Ohio and plot them as a histogram.\n",
    "size = 10000\n",
    "diff_means_perm_oh_fl_array = np.empty(size)\n",
    "for i in range(size):\n",
    "    perm_oh_votes, perm_fl_votes = \\\n",
    "        permutation_sample(ohio_votes[\"dem_share\"], flor_votes[\"dem_share\"])\n",
    "    diff_means_perm_oh_fl = perm_oh_votes.mean() - perm_fl_votes.mean()\n",
    "    diff_means_perm_oh_fl_array[i] = diff_means_perm_oh_fl\n",
    "\n",
    "# Plot the permutation replicates of the difference of means.\n",
    "plt.hist(diff_means_perm_oh_fl_array, bins=100, density=True)\n",
    "# Plot a vertical line showing the difference of means for the original data.\n",
    "diff_obs_means_oh_fl = x_oh.mean() - x_fl.mean()\n",
    "plt.plot([diff_obs_means_oh_fl, diff_obs_means_oh_fl], [0, 0.30], color=\"red\")\n",
    "plt.xlabel(\"OH - FL mean percent vote difference\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.show()\n",
    "\n",
    "# Print the p-value for a one-tail test.\n",
    "p_value_oh_fl_1_tail = 1 - (diff_means_perm_oh_fl_array < diff_obs_means_oh_fl).mean()\n",
    "print(\"p-value = {:.4f}\".format(p_value_oh_fl_1_tail))\n",
    "\n",
    "# Use scipy.stats.percentileofscore() to calculate the p-value.\n",
    "p_value_oh_fl_1_tail_sciy = scipy.stats.percentileofscore(diff_means_perm_oh_fl_array, diff_obs_means_oh_fl) / 100\n",
    "print(\"scipy p-value = {:.4f}\".format(1 - p_value_oh_fl_1_tail_sciy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8361960d-5c7e-4463-8313-0e444892e58e",
   "metadata": {},
   "source": [
    "#### Two-Tail Tests of the Difference of the Means (Extra)\n",
    "\n",
    "This is my work. For the two-tail test, we can test whether the absolute value of the difference of the means is different from 0. The null hypothesis is that the difference is 0; the alternative hypothesis is that the absolute value of the difference is greater than zero.\n",
    "\n",
    "For Pennsylvania and Ohio, we do not reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6dd28e-dad7-4c3a-ad57-ea1fd8388273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For PA and OH.\n",
    "abs_diff_means_perm_pa_oh_array = np.abs(diff_means_perm_pa_oh_array)\n",
    "\n",
    "# Plot the permutation replicates of the absolute values of the difference\n",
    "# of means.\n",
    "plt.hist(abs_diff_means_perm_pa_oh_array, bins=100, density=True)\n",
    "# Plot a vertical line showing the difference of means for the original data.\n",
    "diff_obs_means_pa_oh = x_pa.mean() - x_oh.mean()\n",
    "plt.plot([diff_obs_means_pa_oh, diff_obs_means_pa_oh], [0, 0.60], color=\"red\")\n",
    "plt.xlabel(\"PA - OH mean percent vote difference\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.show()\n",
    "\n",
    "# Print the p-value for a two-tail test.\n",
    "p_value_pa_oh_2_tail = 1 - (abs_diff_means_perm_pa_oh_array < diff_obs_means_pa_oh).mean()\n",
    "print(\"p-value = {:.4f}\".format(p_value_pa_oh_2_tail))\n",
    "\n",
    "# Use scipy.stats.percentileofscore() to calculate the p-value.\n",
    "p_value_pa_oh_2_tail_scipy = 1 - (scipy.stats.percentileofscore(abs_diff_means_perm_pa_oh_array, diff_obs_means_pa_oh) / 100)\n",
    "print(\"scipy p-value = {:.4f}\".format(p_value_pa_oh_2_tail_scipy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239cc897-e8f9-4cdd-9fb8-d8c5fc43f0b3",
   "metadata": {},
   "source": [
    "For Ohio and Florida, for a two-tail test, we do not reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e0c0f1-8ead-428d-a569-8b9bb41b48b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For OH and FL.\n",
    "abs_diff_means_perm_oh_fl_array = np.abs(diff_means_perm_oh_fl_array)\n",
    "\n",
    "# Plot the permutation replicates of the absolute values of the difference\n",
    "# of means.\n",
    "plt.hist(abs_diff_means_perm_oh_fl_array, bins=100, density=True)\n",
    "# Plot a vertical line showing the difference of means for the original data.\n",
    "abs_obs_diff_means_oh_fl = x_oh.mean() - x_fl.mean()\n",
    "plt.plot([abs_obs_diff_means_oh_fl, abs_obs_diff_means_oh_fl], [0, 0.60], color=\"red\")\n",
    "plt.xlabel(\"OH - FL mean percent vote difference\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.show()\n",
    "\n",
    "# Print the p-value for a two-tail test.\n",
    "p_value_oh_fl_2_tail = 1 - (abs_diff_means_perm_oh_fl_array < abs_obs_diff_means_oh_fl).mean()\n",
    "print(\"p-value = {:.4f}\".format(p_value_oh_fl_2_tail))\n",
    "\n",
    "# Use scipy.stats.percentileofscore() to calculate the p-value.\n",
    "p_value_oh_fl_2_tail_scipy = 1 - (scipy.stats.percentileofscore(abs_diff_means_perm_oh_fl_array, diff_obs_means_oh_fl) / 100)\n",
    "print(\"scipy p-value = {:.4f}\".format(p_value_oh_fl_2_tail_scipy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eac490f-d122-45b1-8255-7784c13a1999",
   "metadata": {},
   "source": [
    "#### Test Statistics (Exercise)\n",
    "\n",
    "When performing hypothesis tests, your choice of test statistics should be pertinent to the question you are seeking to answer in your hypothesis test.\n",
    "\n",
    "#### What Is a p-Value? (Exercise)\n",
    "\n",
    "The p-value is generally a measure of the probability of observing a test statistic equally or more extreme than the one you observed, given that the null hypothesis is true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f341d4c9-64ef-494a-8586-9e606ef898ab",
   "metadata": {},
   "source": [
    "#### Generating Permutation Replicates (Exercise)\n",
    "\n",
    "Create a function for generating permutation replicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4af07b0-4de2-42c8-ac8d-ad123b845b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_perm_reps(array1, array2, func, size=1):\n",
    "    \"\"\"Generate multiple permutation replicates.\"\"\"\n",
    "    perm_replicates = np.empty(size)\n",
    "    for i in range(size):\n",
    "        # Generate permutation sample\n",
    "        perm_sample1, perm_sample2 = permutation_sample(array1, array2)\n",
    "        # Compute the test statistic\n",
    "        perm_replicates[i] = func(perm_sample1, perm_sample2)\n",
    "    return perm_replicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4404b42-224f-4da4-a39c-cde3b9acec87",
   "metadata": {},
   "source": [
    "#### Analyze Voting in Arbitrary Pairs of States (Extra)\n",
    "\n",
    "Use the utility functions `permutation_sample()` and `draw_perm_reps()` to repeat the analysis of voting in Pennsylvania and Ohio and other pairs of states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2859731-563e-4e91-a0b1-1536a3960d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that calculates the difference of means.\n",
    "def difference_of_means(array1, array2):\n",
    "    diff_means = array1.mean() - array2.mean()\n",
    "    return diff_means\n",
    "\n",
    "# Define a function that plots the ECDFs of the two states and\n",
    "# creates and plots difference of means replicates from permuted\n",
    "# samples.\n",
    "def plot_votes(df, state1, state2, size):\n",
    "    # Compute the observed difference of means and the replicate difference of\n",
    "    # means from permuted samples.\n",
    "    state1_dem_share = df[df[\"state\"] == state1][\"dem_share\"].to_numpy()\n",
    "    state2_dem_share = df[df[\"state\"] == state2][\"dem_share\"].to_numpy()\n",
    "    x_state1, y_state1 = ecdf(state1_dem_share)\n",
    "    x_state2, y_state2 = ecdf(state2_dem_share)\n",
    "    plt.plot(x_state1, y_state1, marker=\".\", linestyle=\"none\", label=state1)\n",
    "    plt.plot(x_state2, y_state2, marker=\".\", linestyle=\"none\", label=state2)\n",
    "    plt.xlabel(\"Democratic share of votes in county (%)\")\n",
    "    plt.ylabel(\"ECDF\")\n",
    "    plt.xticks(np.arange(0, 110, 10))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate the observed difference of means and the difference of means\n",
    "    # of permutated sample replicates.\n",
    "    obs_diff_means = difference_of_means(state1_dem_share, state2_dem_share)\n",
    "    diff_means_reps = draw_perm_reps(state1_dem_share, state2_dem_share, difference_of_means, size=size)\n",
    "    abs_diff_means_reps = np.abs(diff_means_reps)\n",
    "\n",
    "    # Plot a histogram.\n",
    "    y, x, _ = plt.hist(diff_means_reps, density=True, bins=100)\n",
    "    # Draw a vertical line for the observed difference of means.\n",
    "    plt.vlines(obs_diff_means, 0, max(y), colors=[\"red\"])\n",
    "    # Customize and show the plot.\n",
    "    plt.ylabel(\"PDF\")\n",
    "    plt.xlabel(state1 + \" - \" + state2 + \" mean percent vote difference\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the p-value for a two-tail test.\n",
    "    p_value_2_tail = 1 - (abs_diff_means_reps < np.abs(obs_diff_means)).mean()\n",
    "    print(\"p-value (2-tail) = {:.4f}\".format(p_value_2_tail))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851155da-d9e7-4324-ae86-90bccf39daf6",
   "metadata": {},
   "source": [
    "##### Pennsylvania and Ohio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f50968-c225-43ac-a4c0-6d33003fc6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_votes(all_states, \"PA\", \"OH\", 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8b0761-aa8b-42b5-b2c7-7d68487b3465",
   "metadata": {},
   "source": [
    "##### Ohio and Florida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6c74d5-3362-47b2-948f-a93506f645e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_votes(all_states, \"OH\", \"FL\", 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4da2a5-677c-46d8-ae95-8597f4ca2bd6",
   "metadata": {},
   "source": [
    "##### Washington and Oregon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf40971-1856-49c5-ae41-e29de9dead87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_votes(all_states, \"WA\", \"OR\", 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2836ac74-8393-4247-a86c-aff4a3151180",
   "metadata": {},
   "source": [
    "##### Massachusetts and Washington"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677c3695-ed8d-4fbc-975d-9e1a20ed996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_votes(all_states, \"MA\", \"WA\", 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483d5c37-1daa-4ed8-a4e6-778f4fddf49a",
   "metadata": {},
   "source": [
    "##### Massachusetts and Alabama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6977e5e4-d55e-4e42-bd7c-8ce2c8f2b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_votes(all_states, \"MA\", \"AL\", 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093f8e50-6923-4108-9c79-75bfe1319832",
   "metadata": {},
   "source": [
    "##### Hawaii and Utah\n",
    "\n",
    "These are the states with the most extreme voting patterns. It appears that Hawaii has only four counties; this distorts the PDF plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b5c70d-e55c-452b-87f4-354e0767b42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_votes(all_states, \"HI\", \"UT\", 10000) # The most extreme states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaea07c7-dba5-4564-895d-4b2ef4e49bf5",
   "metadata": {},
   "source": [
    "#### EDA before Hypothesis Testing (Exercise)\n",
    "\n",
    "> Kleinteich and Gorb (Sci. Rep., 4, 5225, 2014) performed an interesting experiment with South American horned frogs. They held a plate connected to a force transducer, along with a bait fly, in front of them. They then measured the impact force and adhesive force of the frog's tongue when it struck the target.\n",
    "\n",
    "> Frog A is an adult and Frog B is a juvenile. The researchers measured the impact force of 20 strikes for each frog. In the next exercise, we will test the hypothesis that the two frogs have the same distribution of impact forces.\n",
    "\n",
    "For the data used in the exercise, ID = \"A\" corresponds to ID = \"II\" in the original data file, and ID = \"B\" corresponds to ID = \"IV\". The impact_force values in the original file were divided by 1000 to convert the force units from mN to N.\n",
    "\n",
    "> Eyeballing it, it does not look like they come from the same distribution. Frog A, the adult, has three or four very hard strikes, and Frog B, the juvenile, has a couple weak ones. However, it is possible that with only 20 samples it might be too difficult to tell if they have difference distributions, so we should proceed with the hypothesis test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087f9385-15de-4505-9663-7da62206c436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a seaborn beeswarm plot of the original data.\n",
    "_ = sns.swarmplot(x=\"ID\", y=\"impact force (mN)\", data=frog_tongue)\n",
    "# Labels are provided using the column headings by default.\n",
    "plt.xlabel(\"Frog\")\n",
    "plt.ylabel(\"Impace force (mN)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7c98e2-db37-4ad1-af78-c8fc54a1dce3",
   "metadata": {},
   "source": [
    "#### Permutation Tests on Frog Data (Exercise)\n",
    "\n",
    "> The average strike force of Frog A was 0.71 Newtons (N), and that of Frog B was 0.42 N for a difference of 0.29 N. It is possible the frogs strike with the same force and this observed difference was by chance. You will compute the probability of getting at least a 0.29 N difference in mean strike force under the hypothesis that _the distributions of strike forces for the two frogs are identical_. We use a permutation test with a test statistic of the difference of means to test this hypothesis.\n",
    "\n",
    "> The p-value tells you that there is about a 0.6% chance that you would get the difference of means observed in the experiment if frogs were exactly the same. A p-value below 0.01 is typically said to be \"statistically significant,\" but: warning! warning! warning! You have computed a p-value; it is a number. I encourage you not to distill it to a yes-or-no phrase. p = 0.006 and p = 0.000000006 are both said to be \"statistically significant,\" but they are definitely not the same!\n",
    "\n",
    "We reject the null hypothesis that the means are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549fbe2a-a4bb-4073-a310-ae9a9a895965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sample permutations to test whether there is a significant different\n",
    "# in means. The null hypothesis is that there is no difference in means.\n",
    "# The alternative hypothesis is that the impact force for frog \"II\" is\n",
    "# greater than the impact force for frog \"IV\".\n",
    "\n",
    "# Prepare the data.\n",
    "force_a = frog_tongue[frog_tongue[\"ID\"] == \"II\"][\"impact force (mN)\"].to_numpy() / 1000\n",
    "force_b = frog_tongue[frog_tongue[\"ID\"] == \"IV\"][\"impact force (mN)\"].to_numpy() / 1000\n",
    "\n",
    "def diff_of_means(data_1, data_2):\n",
    "    \"\"\"Difference in means of two arrays.\"\"\"\n",
    "    diff = np.mean(data_1) - np.mean(data_2)\n",
    "    return diff\n",
    "\n",
    "# Compute difference of mean impact force from experiment.\n",
    "empirical_diff_means = diff_of_means(force_a, force_b)\n",
    "# Draw 10,000 permutation replicates: perm_replicates\n",
    "perm_replicates = draw_perm_reps(force_a, force_b, diff_of_means, size=10000)\n",
    "# Compute p-value: p\n",
    "p = np.sum(perm_replicates >= empirical_diff_means) / len(perm_replicates)\n",
    "print(\"p-value = {:.4f}\".format(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cc3062-dcde-4b92-9fda-9ae97182ffd3",
   "metadata": {},
   "source": [
    "### Bootstrap Hypothesis Tests\n",
    "\n",
    "Pipeline for hypothesis testing:\n",
    "- First, clearly state the null hypothesis. Stating the null hypothesis so that it is crystal clear is essential to be able to simulate it. \n",
    "- Next, define your test statistic.\n",
    "- Then generate many sets of simulated data assuming the null hypothesis is true.\n",
    "- Compute the test statistic for each simulated data set.\n",
    "- The p-value is then the fraction of your simulated data sets for which the test statistic is at least as extreme as for the real data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14098731-b962-4a63-9900-1abf1c50ab3c",
   "metadata": {},
   "source": [
    "#### Speed of Light (Demonstration)\n",
    "\n",
    "Michelson measured the speed of light (see previous work in this and the prequel to this course), and we have his individual measurements and his mean, 299,852.4 km/s. Newcomb also measured the speed of light, but we have only his mean value, 299,860 km/s. Are these significantly different? Our null hypothesis is that Michelson's speed of light is Newcomb's speed of light (no difference in the means). We can't do a difference of means test using permuted samples because we don't have Newcomb's individual measurements.\n",
    "\n",
    "I think what you should do here is take bootstrap samples with replacement of Michelson's data and compute the means of the sample replicates. Then compute the p-value for the mean being Newcomb's value. The null hypothesis is that Newcomb's value could be obtained from Michelson's data.\n",
    "\n",
    "We do not reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c82b53-a21e-4dd5-b1e0-b72f5ab8d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain 10,000 bootstrap replicates of the mean from Michelson's data.\n",
    "# light_bs_replicates contains 10,000 bootstrap replicate means from\n",
    "# Michelson's data, as calculated above.\n",
    "newcomb_mean = 299860\n",
    "light_p_value = np.mean(light_bs_replicates >= newcomb_mean)\n",
    "print(\"p-value = {:.4f}\".format(light_p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b231ce-41d0-4b65-afc1-937abaa980b6",
   "metadata": {},
   "source": [
    "This is not what Justin does. He shifts Michelson's data by subtracting the Michelson mean and adding the Newcomb mean to each observation. Justin then uses bootstrapping to create bootstrap sample replicates of the mean from data where the mean is Newcomb's value for the speed of light. He uses the shifted data to simulate the null hypothesis, which is that the speed of light from Michelson's data is actually Newcomb's value. He calculates the p-value of the difference of means being -7.6 km/s given that the null hypothesis is true.\n",
    "\n",
    "The p-value obtained this way is similar to the p-value I calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f843e9-6836-480c-bfb5-74a5534be5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift the Michelson data.\n",
    "michelson_mean = np.mean(michelson_speed_of_light)\n",
    "michelson_shifted = michelson_speed_of_light - michelson_mean + newcomb_mean\n",
    "print(\"newcomb_mean:\", newcomb_mean)\n",
    "print(\"michelson_mean:\", michelson_mean)\n",
    "print(\"michelson_adjusted_mean:\", np.mean(michelson_shifted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1049949a-9769-43cd-a396-0d59606e396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ECDFs of michelson_speed_of_light and michelson_adjusted.\n",
    "# Note the shifting is not as big as it is in the course's figure.\n",
    "x_m, y_m = ecdf(michelson_speed_of_light)\n",
    "x_ms, y_ms = ecdf(michelson_shifted)\n",
    "plt.plot(x_m, y_m, marker=\".\", linestyle=\"none\", label=\"Michelson\")\n",
    "plt.plot(x_ms, y_ms, marker=\".\", linestyle=\"none\", label=\"Michelson shifted\")\n",
    "plt.xlabel(\"Speed of light (km/s)\")\n",
    "plt.ylabel(\"ECDF\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5430ca-7b3e-4d5a-8706-3f851818bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bootstrap replicates of the difference of the mean of the bootstrap\n",
    "# sample obtained from the Michelson shifted data and Newcomb's mean.\n",
    "def diff_from_newcomb(data, newcomb_mean=299860):\n",
    "    return np.mean(data) - newcomb_mean\n",
    "\n",
    "# The difference here should be -7.6 km/s.\n",
    "diff_observed = diff_from_newcomb(michelson_speed_of_light)\n",
    "print(\"diff_observed: {:.2f} km/s\".format(diff_observed))\n",
    "\n",
    "# Draw bootstrap replicates of the difference of the means from Newcomb's\n",
    "# mean.\n",
    "shifted_bs_replicates = draw_bs_reps(michelson_shifted, diff_from_newcomb, size=10000)\n",
    "# Calculate the p-value for the difference being less than or equal to the\n",
    "# observed difference of -7.6 km/s.\n",
    "light_p_value2 = np.mean(shifted_bs_replicates <= diff_observed)\n",
    "print(\"p-value: {:.4f}\".format(light_p_value2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a214aa3d-5f31-4d24-a5fe-7298c1d927f1",
   "metadata": {},
   "source": [
    "#### A One-Sample Bootstrap Hypothesis Test (Exercise)\n",
    "\n",
    "> Another juvenile frog was studied, Frog C, and you want to see if Frog B and Frog C have similar impact forces. Unfortunately, you do not have Frog C's impact forces available, but you know they have a mean of 0.55 N. Because you don't have the original data, you cannot do a permutation test, and you cannot assess the hypothesis that the forces from Frog B and Frog C come from the same distribution. You will therefore test another, less restrictive hypothesis: The mean strike force of Frog B is equal to that of Frog C.\n",
    "\n",
    "> To set up the bootstrap hypothesis test, you will take the mean as our test statistic. Remember, your goal is to calculate the probability of getting a mean impact force less than or equal to what was observed for Frog B if the hypothesis that the true mean of Frog B's impact forces is equal to that of Frog C is true. You first translate all of the data of Frog B such that the mean is 0.55 N. This involves adding the mean force of Frog C and subtracting the mean force of Frog B from each measurement of Frog B. This leaves other properties of Frog B's distribution, such as the variance, unchanged.\n",
    "\n",
    "We reject the null hypothesis.\n",
    "\n",
    "> The low p-value suggests that the null hypothesis that Frog B and Frog C have the same mean impact force is false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e83b591-3d4e-4492-9537-3ec83996dfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frog C in the exercise is frog \"III\" in the data.\n",
    "mean_force_b = np.mean(force_b)\n",
    "print(\"mean_force_b: {:.3f}\".format(mean_force_b))\n",
    "force_c = frog_tongue[frog_tongue[\"ID\"] == \"III\"][\"impact force (mN)\"].to_numpy() / 1000\n",
    "mean_force_c = np.mean(force_c)\n",
    "print(\"mean_force_c: {:.3f}\".format(mean_force_c))\n",
    "\n",
    "# Translate the force_b data to have a mean of mean_force_c.\n",
    "translated_force_b = force_b - np.mean(force_b) + mean_force_c\n",
    "# Take bootstrap replicates of the mean.\n",
    "bs_replicates = draw_bs_reps(translated_force_b, np.mean, 10000)\n",
    "# Compute fraction of replicates that are less than the observed Frog B force: p\n",
    "p = np.sum(bs_replicates <= np.mean(force_b)) / 10000\n",
    "print('p = ', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19acd56-28ac-4f01-a702-5d93fa5bc2b8",
   "metadata": {},
   "source": [
    "#### A Two-Sample Bootstrap Hypothesis Test for Difference of Means (Exercise)\n",
    "\n",
    "> We now want to test the hypothesis that Frog A and Frog B have the same mean impact force, but not necessarily the same distribution, which is also impossible with a permutation test.\n",
    "\n",
    "The permutation test (see above) assumes that the distributions are equal. It combines the data, creates permutations of the combined data to simulate the original data sets, and computes a replicate statistic.\n",
    "\n",
    "> To do the two-sample bootstrap test, we shift both arrays to have the same mean, since we are simulating the hypothesis that their means are, in fact, equal. We then draw bootstrap samples out of the shifted arrays and compute the difference in means. This constitutes a bootstrap replicate, and we generate many of them. The p-value is the fraction of replicates with a difference in means greater than or equal to what was observed.\n",
    "\n",
    "We reject the null hypothesis.\n",
    "\n",
    "> You got a similar result as when you did the permutation test. Nonetheless, remember that it is important to carefully think about what question you want to ask. Are you only interested in the mean impact force, or in the distribution of impact forces?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aee731-5d74-41ee-bda6-192cc26b83e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the force data. Using the + operator adds the values to each\n",
    "# other; must use np.concatenate() here.\n",
    "forces_concat = np.concatenate((force_a, force_b))\n",
    "mean_force = np.mean(forces_concat)\n",
    "\n",
    "# Generate shifted arrays\n",
    "force_a_shifted = force_a - np.mean(force_a) + mean_force\n",
    "force_b_shifted = force_b - np.mean(force_b) + mean_force\n",
    "\n",
    "# Compute 10,000 bootstrap replicates from shifted arrays\n",
    "bs_replicates_a = draw_bs_reps(force_a_shifted, np.mean, size=10000)\n",
    "bs_replicates_b = draw_bs_reps(force_b_shifted, np.mean, size=10000)\n",
    "\n",
    "# Get replicates of difference of means: bs_replicates\n",
    "bs_replicates = bs_replicates_a - bs_replicates_b\n",
    "\n",
    "# Compute and print p-value: p\n",
    "empirical_diff_means = np.mean(force_a) - np.mean(force_b)\n",
    "print(\"empirical_diff_means = {:.3f}\".format(empirical_diff_means))\n",
    "p = sum(bs_replicates >= empirical_diff_means) / len(bs_replicates)\n",
    "print(\"p-value = {:.4f}\".format(p))\n",
    "p = np.mean(bs_replicates >= empirical_diff_means)\n",
    "print(\"p-value = {:.4f}\".format(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dce767c-7d28-4a06-9daa-bb1cd2432121",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hypothesis Test Examples\n",
    "\n",
    "### A/B Testing\n",
    "\n",
    "#### Results of an A/B Test (Demonstration)\n",
    "\n",
    "Page A had a click-through rate of 45/500; page B had a click-through rate of 67/500. What is the probability that this happened by chance, given that these represent the same distribution? This is a hypothesis test, and one way to study this question is through using a permutation test, which assumes the null hypothesis that the difference in pages had no effect on the click-through rate.\n",
    "\n",
    "Given the low p-value, we reject the null hypothesis that the click-through rates for the two pages could have happened by chance assuming the rates were not different for the two pages.\n",
    "\n",
    "Be warned that statistical significance does not mean practical significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a057a054-6a07-44fd-a21a-2eb01d7d9ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the data. 45/500 hits for page A; 67/500 hits for page B.\n",
    "clickthrough_a = np.array([1] * 45 + [0] * (500 - 45))\n",
    "print(\"page A hit rate:\", np.mean(clickthrough_a))\n",
    "clickthrough_b = np.array([1] * 67 + [0] * (500 - 67))\n",
    "print(\"page B hit rate:\", np.mean(clickthrough_b))\n",
    "clickthrough_combined = np.concatenate((clickthrough_a, clickthrough_b))\n",
    "print(\"combined hit rate:\", np.mean(clickthrough_combined))\n",
    "\n",
    "# Take permuted samples.\n",
    "def diff__of_means(array1, array2):\n",
    "    return np.mean(array1) - np.mean(array2)\n",
    "\n",
    "# Draw the permutation samples and plot them against the empirical difference\n",
    "# of means as a histogram.\n",
    "obs_diff_of_means = diff_of_means(clickthrough_b, clickthrough_a)\n",
    "print(\"obs_diff_of_means: {:.4f}\".format(obs_diff_of_means))\n",
    "diff_of_means_reps = draw_perm_reps(clickthrough_b, clickthrough_a, diff_of_means, size=10000)\n",
    "# Decide the bin boundaries for the histogram.\n",
    "# 56/500 - 56/500 = 0.000\n",
    "# 57/500 - 55/500 = 0.004\n",
    "# 58/500 - 54/500 = 0.008\n",
    "# etc.\n",
    "bins = np.arange(-.10, .10, .004)\n",
    "y, x, _ = plt.hist(diff_of_means_reps, bins=bins, density=True)\n",
    "plt.vlines(obs_diff_of_means, 0, max(y), colors=[\"red\"])\n",
    "plt.xlabel(\"Difference of means (click-through rate)\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.xticks(np.arange(-0.10, 0.11, .02))\n",
    "plt.show()\n",
    "\n",
    "# Calculate the p-value.\n",
    "p = np.mean(diff_of_means_reps >= obs_diff_of_means)\n",
    "print(\"p-value: {:.4f}\".format(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e090e6c0-3d7a-4162-88bf-c55305b628b8",
   "metadata": {},
   "source": [
    "#### The Vote for the Civil Rights Act in 1964 (Exercise)\n",
    "\n",
    "> The Civil Rights Act of 1964 was one of the most important pieces of legislation ever passed in the USA. Excluding \"present\" and \"abstain\" votes, 153 House Democrats and 136 Republicans voted yea. However, 91 Democrats and 35 Republicans voted nay. Did party affiliation make a difference in the vote?\n",
    "\n",
    "> To answer this question, you will evaluate the hypothesis that the party of a House member has no bearing on his or her vote. You will use the fraction of Democrats voting in favor as your test statistic and evaluate the probability of observing a fraction of Democrats voting in favor at least as small as the observed fraction of 153/244. (That's right, at least as small as. In 1964, it was the Democrats who were less progressive on civil rights issues.) To do this, permute the party labels of the House voters and then arbitrarily divide them into \"Democrats\" and \"Republicans\" and compute the fraction of Democrats voting yea.\n",
    "\n",
    "> This small p-value suggests that party identity had a lot to do with the voting. Importantly, the South had a higher fraction of Democrat representatives, and consequently also a more racist bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de0697c-2018-4294-aa78-90f07d495631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct arrays of data: dems, reps\n",
    "dems = np.array([True] * 153 + [False] * 91)\n",
    "reps = np.array([True] * 136 + [False] * 35)\n",
    "print(\"prop_dems = {:.4f}\".format(153 / (153 + 91)))\n",
    "print(\"prop_reps = {:.4f}\".format(136 / (136 + 35)))\n",
    "print(\"prop_all  = {:.4f}\".format((153 + 136) / (153 + 91 + 136 + 35)))\n",
    "\n",
    "def frac_yea_dems(dems, reps):\n",
    "    \"\"\"Compute fraction of Democrat yea votes.\"\"\"\n",
    "    frac = np.sum(dems) / len(dems)\n",
    "    return frac\n",
    "\n",
    "# Acquire permutation replicates representing the proportion of Democrats\n",
    "# who voted for the Civil Rights Act.\n",
    "perm_replicates = draw_perm_reps(dems, reps, frac_yea_dems, size=10000)\n",
    "y, x, _ = plt.hist(perm_replicates, bins=100, density=True)\n",
    "plt.vlines(frac_yea_dems(dems, reps), 0, max(y), colors=[\"red\"])\n",
    "plt.xlabel(\"Proportion of Democrats voting yea\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.show()\n",
    "p = np.sum(perm_replicates <= (153 / (153 + 91))) / len(perm_replicates)\n",
    "print('Democrats p-value = {:.5f}'.format(p))\n",
    "\n",
    "# Do it again for the Republicans.\n",
    "# The null hypothesis is no difference in voting proportion.\n",
    "def frac_yea_reps(dems, reps):\n",
    "    frac = np.sum(reps) / len(reps)\n",
    "    return frac\n",
    "\n",
    "perm_replicates2 = draw_perm_reps(dems, reps, frac_yea_reps, size=10000)\n",
    "y2, x2, _ = plt.hist(perm_replicates2, bins=100, density=True)\n",
    "plt.vlines(frac_yea_reps(dems, reps), 0, max(y2), colors=[\"red\"])\n",
    "plt.xlabel(\"Proportion of Republicans voting yea\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.show()\n",
    "p2 = np.sum(perm_replicates2 >= (135 / (136 + 35))) / len(perm_replicates2)\n",
    "print(\"Republicans p-value = {:.5f}\".format(p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5592ca9a-9b04-4b01-9ca0-ddad21ff6484",
   "metadata": {},
   "source": [
    "#### A Time-on-Website Analog Using Games between No-Hitters (Exercise)\n",
    "\n",
    "Instead of calculating time-on-website, we calculate the number of games between no-hitters in the dead ball era and the live ball era. The null hypothesis is that there is no difference -- that both data sets come from the same distribution.\n",
    "\n",
    "This is unlikely given the mean of nht_dead versus the mean of mht_live.\n",
    "\n",
    "> We return to the no-hitter data set. In 1920, Major League Baseball implemented important rule changes that ended the so-called dead ball era. Importantly, the pitcher was no longer allowed to spit on or scuff the ball, an activity that greatly favors pitchers. In this problem you will perform an A/B test to determine if these rule changes resulted in a slower rate of no-hitters (i.e., longer average time between no-hitters) using the difference in mean inter-no-hitter time as your test statistic.\n",
    "\n",
    "> Your p-value is 0.0001, which means that only one out of your 10,000 replicates had a result as extreme as the actual difference between the dead ball and live ball eras. This suggests strong statistical significance. Watch out, though, you could very well have gotten zero replicates that were as extreme as the observed value. This just means that the p-value is quite small, almost certainly smaller than 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2815156-048a-4eab-a9a9-b55f607dc725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the NumPy arrays.\n",
    "# Note that the first item in dead_ball in the course is -1; this must be\n",
    "# an error.\n",
    "dead_ball = nohitters[nohitters[\"date\"] <= \"1920-01-01\"][\"game_number\"]\n",
    "nht_dead = np.array([dead_ball.iloc[x] - dead_ball.iloc[x - 1] - 1 for x in range(1, len(dead_ball))])\n",
    "print(nht_dead)\n",
    "print()\n",
    "live_ball = nohitters[nohitters[\"date\"] > \"1920-01-01\"][\"game_number\"]\n",
    "nht_live = np.array([live_ball.iloc[x] - live_ball.iloc[x - 1] - 1 for x in range(1, len(live_ball))])\n",
    "print(nht_live)\n",
    "\n",
    "# Compute the observed difference in mean inter-no-hitter times.\n",
    "print(\"mean of nht_dead: {:.1f}\".format(np.mean(nht_dead)))\n",
    "print(\"mean of nht_live: {:.1f}\".format(np.mean(nht_live)))\n",
    "nht_diff_obs = diff_of_means(nht_dead, nht_live)\n",
    "print(\"nht_diff_obs: {:.1f}\".format(nht_diff_obs))\n",
    "# Acquire 10,000 permutation replicates of difference in mean no-hitter time.\n",
    "perm_replicates = draw_perm_reps(nht_dead, nht_live, diff_of_means, size=10000)\n",
    "p = sum(perm_replicates < nht_diff_obs) / len(perm_replicates)\n",
    "print(\"p-val = {:.4f}\".format(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a360bc77-52f2-4c81-a44c-6ae894e9cc0f",
   "metadata": {},
   "source": [
    "#### EDA of Time between No-Hitters for Dead Ball and Live Ball Eras (Exercise)\n",
    "\n",
    "As always, EDA should be performed before hypothesis tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2e1a6d-caa2-4e23-b622-a2e15fd5adf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ECDFs of nht_dead and nht_live.\n",
    "x_dead, y_dead = ecdf(nht_dead)\n",
    "x_live, y_live = ecdf(nht_live)\n",
    "plt.plot(x_dead, y_dead, marker=\".\", linestyle=\"none\", label=\"dead\")\n",
    "plt.plot(x_live, y_live, marker=\".\", linestyle=\"none\", label=\"live\")\n",
    "plt.xlabel(\"Games between no-hitters\")\n",
    "plt.ylabel(\"ECDF\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fab86ad-b7ba-445d-af68-883b4c964c9b",
   "metadata": {},
   "source": [
    "### Test of Correlation\n",
    "\n",
    "#### 2008 US Swing State Election Results (Demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453f97e8-6f69-4864-8d7c-3fcca19c4f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the Pearson correlation coefficient.\n",
    "swing_states_pearson_r = np.corrcoef(swing_states[[\"total_votes\", \"dem_share\"]], rowvar=False)[0, 1]\n",
    "print(\"swing_states_pearson_r: {:.4f}\".format(swing_states_pearson_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6292fb-c152-42b4-8775-2e18721abef4",
   "metadata": {},
   "source": [
    "#### Hypothesis Test of Correlation (Demonstration)\n",
    "\n",
    "How do we know if the correlation is real or if it happened by chance? We can carry out a hypothesis test on the correlation statistic. The null hypothesis is that there is no correlation between the number of votes in a county and the percentage of votes given to the Democratic candidate. We simulate the data assuming the null hypothesis is true. Use the Pearson correlation coefficient, rho, as the test statistic. The p-value is the fraction of replicates that have rho at least as large as observed.\n",
    "\n",
    "Jason tried this and was unable to find even a single instance by chance where the correlation coefficient was at least 0.5362.\n",
    "\n",
    "> This does not mean that the p-value is zero. It means that it is so low that we would have to generate an enormous number of replicates to have even one that has a test statistic sufficiently extreme. We conclude that the p-value is very very small and there is essentially no doubt that counties with higher vote count tended to vote for Obama. After all, that is how he won the election.\n",
    "\n",
    "I confirmed that the p-value < 0.000001 by running the permutation one million times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b18e8e-b243-4459-a021-de8f69036d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It should be sufficient to permute either dem_share or total_votes to break\n",
    "# the association of the two variables.\n",
    "dem_share_permuted = rng.permutation(swing_states[\"dem_share\"])\n",
    "total_votes_permuted = rng.permutation(swing_states[\"total_votes\"])\n",
    "rho1 = pearson_r(dem_share_permuted, swing_states[\"total_votes\"])\n",
    "print(\"rho1 = {:.4f}\".format(rho1))\n",
    "rho2 = pearson_r(swing_states[\"dem_share\"], total_votes_permuted)\n",
    "print(\"rho2 = {:.4f}\".format(rho2))\n",
    "rho3 = pearson_r(dem_share_permuted, total_votes_permuted)\n",
    "print(\"rho3 = {:.4f}\".format(rho3))\n",
    "\n",
    "# Do this 10000 times.\n",
    "size = 10000\n",
    "rho_replicates = np.empty(size)\n",
    "for i in range(size):\n",
    "    dem_share_permuted = rng.permutation(swing_states[\"dem_share\"])\n",
    "    rho = pearson_r(dem_share_permuted, swing_states[\"total_votes\"])\n",
    "    rho_replicates[i] = rho\n",
    "\n",
    "# Plot the results.\n",
    "y, x, _ = plt.hist(rho_replicates, bins=100, density=True)\n",
    "plt.vlines(swing_states_pearson_r, 0, np.max(y), colors=[\"red\"])\n",
    "plt.xlabel(\"Pearson r\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate the p-value.\n",
    "print(\"mean of rho replicates: {:.4f}\".format(np.mean(rho_replicates)))\n",
    "p = np.sum(rho_replicates >= swing_states_pearson_r) / len(rho_replicates)\n",
    "print(\"p-value: {:.6f}\".format(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079eddc7-89ac-4fa7-bc85-855e3b6fef9b",
   "metadata": {},
   "source": [
    "#### Simulating a Null Hypothesis Concerning Correlation (Exercise)\n",
    "\n",
    "> The observed correlation between female illiteracy and fertility in the data set of 162 countries may just be by chance; the fertility of a given country may actually be totally independent of its illiteracy. You will test this null hypothesis in the next exercise.\n",
    "\n",
    "> To do the test, you need to simulate the data assuming the null hypothesis is true.\n",
    "\n",
    "> Do a permutation test: Permute the illiteracy values but leave the fertility values fixed to generate a new set of (illiteracy, fertility) data.\n",
    "\n",
    "> [T]his exactly simulates the null hypothesis and does so more efficiently than the last option. It is exact because it uses all data and eliminates any correlation because which illiteracy value pairs to which fertility value is shuffled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ebd8b-31ad-4646-aa42-712de68141f6",
   "metadata": {},
   "source": [
    "#### Hypothesis Test on Pearson Correlation (Exercise)\n",
    "\n",
    "> The observed correlation between female illiteracy and fertility may just be by chance; the fertility of a given country may actually be totally independent of its illiteracy. You will test this hypothesis. To do so, permute the illiteracy values but leave the fertility values fixed. This simulates the hypothesis that they are totally independent of each other. For each permutation, compute the Pearson correlation coefficient and assess how many of your permutation replicates have a Pearson correlation coefficient greater than the observed one.\n",
    "\n",
    "Once again, the p-value is very low (<0.0001).\n",
    "\n",
    "> You got a p-value of zero. In hacker statistics, this means that your p-value is very low, since you never got a single replicate in the 10,000 you took that had a Pearson correlation greater than the observed one. You could try increasing the number of replicates you take to continue to move the upper bound on your p-value lower and lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f50508d-a28f-4d2d-b169-796073f370f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use permutation samples to create 10,000 replicates of the Pearson r\n",
    "# for the illiteracy and fertility data.\n",
    "r_obs = pearson_r(illiteracy, fertility)\n",
    "size = 10000\n",
    "perm_replicates = np.empty(size)\n",
    "for i in range(size):\n",
    "    illiteracy_permuted = rng.permutation(illiteracy)\n",
    "    perm_replicates[i] = pearson_r(illiteracy_permuted, fertility)\n",
    "if_p = sum(perm_replicates >= r_obs) / len(perm_replicates)\n",
    "print(\"p-value: {:.4f}\".format(if_p))\n",
    "\n",
    "# Plot the histogram.\n",
    "y_if, x_if, _ = plt.hist(perm_replicates, bins=100, density=True)\n",
    "plt.vlines(r_obs, 0, np.max(y_if), colors=[\"red\"])\n",
    "plt.xlabel(\"Pearson r\")\n",
    "plt.ylabel(\"PDF\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f5638-b534-4704-9274-3a7152633f88",
   "metadata": {},
   "source": [
    "#### Do Neonicotinoid Insecticides Have Unintended Consequences? (Exercise)\n",
    "\n",
    "The ECDFs show a pretty clear difference between the treatment and control; treated bees have fewer alive sperm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ad3d32-0240-4e0c-8557-60d776d5c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA on bee sperm data.\n",
    "x_c, y_c = ecdf(control)\n",
    "x_t, y_t = ecdf(treated)\n",
    "plt.plot(x_c, y_c, marker=\".\", linestyle=\"none\", label=\"control\")\n",
    "plt.plot(x_t, y_t, marker=\".\", linestyle=\"none\", label=\"treated\")\n",
    "plt.xlabel(\"Millions of alive sperm per mL\")\n",
    "plt.ylabel(\"ECDF\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccda40db-51a3-4273-86e0-15eb94a6b22c",
   "metadata": {},
   "source": [
    "#### Bootstrap Hypothesis Test on Bee Sperm Counts (Exercise)\n",
    "\n",
    "> [T]est the following hypothesis: On average, male bees treated with neonicotinoid insecticide have the same number of active sperm per milliliter of semen than do untreated male bees. You will use the difference of means as your test statistic.\n",
    "\n",
    "The p-value is very low. We reject the null hypothesis that the treated and control bees have the same number of active sperm per milliliter of semen.\n",
    "\n",
    "> The p-value is small, most likely less than 0.0001, since you never saw a bootstrap replicated with a difference of means at least as extreme as what was observed. In fact, when I did the calculation with 10 million replicates, I got a p-value of 2e-05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e78022-7411-43d3-95f9-c49b0528d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume treated and control have the same mean and distribution.\n",
    "# Shift the values to support this assumption.\n",
    "# I find this approach counter-intuitive.\n",
    "size = 1000000\n",
    "diff_means = diff_of_means(control, treated)\n",
    "\n",
    "# Shift the control and treated measurements so their means are identical.\n",
    "mean_count = np.mean(np.concatenate((control, treated)))\n",
    "control_shifted = control - np.mean(control) + mean_count\n",
    "treated_shifted = treated - np.mean(treated) + mean_count\n",
    "\n",
    "# Generate bootstrap replicates of the means, assuming the means of the\n",
    "# control and treated samples are identical.\n",
    "bs_reps_control = draw_bs_reps(control_shifted, np.mean, size=size)\n",
    "bs_reps_treated = draw_bs_reps(treated_shifted, np.mean, size=size)\n",
    "\n",
    "# Get replicates of difference of means.\n",
    "# Find the p-value.\n",
    "bs_replicates = bs_reps_control - bs_reps_treated\n",
    "p = np.sum(bs_replicates >= diff_means) / len(bs_replicates)\n",
    "print('p-value = {:.6f}'.format(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bb0b4a-92fd-4285-812a-d3e5bf9b2b7b",
   "metadata": {},
   "source": [
    "## A Case Study\n",
    "\n",
    "### Finch Beaks And the Need for Statistics\n",
    "\n",
    "Since 1973, Peter and Rosemary Grant have been studying ground finches on the island of Daphne Major Island in the Galpagos Islands. In 2014 they published the book _40 Years of Evolution: Darwin's Finches on Daphne Major Island_, Princeton University Press. They made their data available at the Dryad Data Repository, http://dx.doi.org/10.5061/dryad.g6g3h.\n",
    "\n",
    "We will start with an investigation of how beak depth of _Geospiza scandens_ has changed over time, looking at beak depth data from 1975 and 2012. We will create parameter estimates of mean beak depth for those years. Finally, we will do a hypothesis test of whether beak depth has changed from 1975 to 2012.\n",
    "\n",
    "#### EDA of Beak Depths of Darwin's Finches (Exercise)\n",
    "\n",
    "> For your first foray into the Darwin finch data, you will study how the beak depth (the distance, top to bottom, of a closed beak) of the finch species _Geospiza scandens_ has changed over time. The Grants have noticed some changes of beak geometry depending on the types of seeds available on the island, and they also noticed that there was some interbreeding with another major species on Daphne Major, _Geospiza fortis_. These effects can lead to changes in the species over time.\n",
    "\n",
    "> In the next few problems, you will look at the beak depth of _G. scandens_ on Daphne Major in 1975 and in 2012. To start with, let's plot all of the beak depth measurements in 1975 and 2012 in a bee swarm plot.\n",
    "\n",
    "> It is kind of hard to see if there is a clear difference between the 1975 and 2012 data set. Eyeballing it, it appears as though the mean of the 2012 data set might be slightly higher, and it might have a bigger variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8e661b-b28d-4044-ae78-3943aa8f1f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bee swarm plot.\n",
    "_ = sns.swarmplot(x=\"year\", y=\"beak_depth\", data=beak_depth)\n",
    "_ = plt.xlabel(\"Year\")\n",
    "_ = plt.ylabel(\"Beak depth (mm)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e62df7b-b492-402f-987f-91565cdf7c86",
   "metadata": {},
   "source": [
    "#### ECDFs of Beak Depths (Exercise)\n",
    "\n",
    "> The differences are much clearer in the ECDF. The mean is larger in the 2012 data, and the variance does appear larger as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfdb722-2c3c-4d44-83e2-3fb75064046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ECDFs of the beak depth data.\n",
    "x_1975, y_1975 = ecdf(bd_1975)\n",
    "x_2012, y_2012 = ecdf(bd_2012)\n",
    "plt.plot(x_1975, y_1975, marker=\".\", linestyle=\"none\", label=\"1975\")\n",
    "plt.plot(x_2012, y_2012, marker=\".\", linestyle=\"none\", label=\"2012\")\n",
    "plt.xlabel(\"Beak depth (mm)\")\n",
    "plt.ylabel(\"ECDF\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106f2d13-08b5-49ba-b3d6-4701a2f680a2",
   "metadata": {},
   "source": [
    "#### Parameter Estimates of Beak Depths (Exercise)\n",
    "\n",
    "Since the 95% confidence interval does not include 0, we can reject the null hypothesis that the means are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31ba326-d806-4bfb-a055-b8949db2bd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the difference of the sample means.\n",
    "obs_mean_diff = np.mean(bd_2012) - np.mean(bd_1975)\n",
    "\n",
    "# Get bootstrap replicates of means.\n",
    "bs_replicates_1975 = draw_bs_reps(bd_1975, np.mean, size=10000)\n",
    "bs_replicates_2012 = draw_bs_reps(bd_2012, np.mean, size=10000)\n",
    "\n",
    "# Compute samples of difference of means.\n",
    "bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975\n",
    "\n",
    "# Compute 95% confidence interval: conf_int\n",
    "conf_int = np.percentile(bs_diff_replicates, [2.5, 97.5])\n",
    "print('difference of means = {:.3f} mm'.format(obs_mean_diff))\n",
    "print('95% confidence interval = {:.3f}-{:.3f} mm'.format(conf_int[0], conf_int[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29141b9-e696-4221-a706-e2ee50dfaa9b",
   "metadata": {},
   "source": [
    "#### Hypothesis Test: Are Beaks Deeper in 2012? (Exercise)\n",
    "\n",
    "> Your plot of the ECDF and determination of the confidence interval make it pretty clear that the beaks of _G. scandens_ on Daphne Major have gotten deeper. But is it possible that this effect is just due to random chance? In other words, what is the probability that we would get the observed difference in mean beak depth if the means were the same?\n",
    "\n",
    "> Be careful! The hypothesis we are testing is not that the beak depths come from the same distribution. For that we could use a permutation test. The hypothesis is that the means are equal. To perform this hypothesis test, we need to shift the two data sets so that they have the same mean and then use bootstrap sampling to compute the difference of means.\n",
    "\n",
    "> We get a p-value of 0.0034, which suggests that there is a statistically significant difference. But remember: it is very important to know how different they are! In the previous exercise, you got a difference of 0.2 mm between the means. You should combine this with the statistical significance. Changing by 0.2 mm in 37 years is substantial by evolutionary standards. If it kept changing at that rate, the beak depth would double in only 400 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba884b0f-e03e-42cd-942e-88d8b6360bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean of combined data set.\n",
    "combined_mean = np.mean(np.concatenate((bd_1975, bd_2012)))\n",
    "print(\"combined_mean: {:.4f}\".format(combined_mean))\n",
    "\n",
    "# Shift the samples so they have the same means.\n",
    "bd_1975_shifted = bd_1975 - np.mean(bd_1975) + combined_mean\n",
    "print(\"np.mean(bd_1975_shifted): {:.4f}\".format(np.mean(bd_1975_shifted)))\n",
    "bd_2012_shifted = bd_2012 - np.mean(bd_2012) + combined_mean\n",
    "print(\"np.mean(bd_2012_shifted): {:.4f}\".format(np.mean(bd_2012_shifted)))\n",
    "print()\n",
    "\n",
    "# Get bootstrap replicates of shifted data sets.\n",
    "# Compute replicates of the difference of means.\n",
    "# Compute the p-value and print it.\n",
    "bs_replicates_1975 = draw_bs_reps(bd_1975_shifted, np.mean, size=10000)\n",
    "bs_replicates_2012 = draw_bs_reps(bd_2012_shifted, np.mean, size=10000)\n",
    "bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975\n",
    "p = np.sum(bs_diff_replicates >= obs_mean_diff) / len(bs_diff_replicates)\n",
    "print('p = {:.4f}'.format(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edde1256-22b5-4982-9432-e5fc358c29f0",
   "metadata": {},
   "source": [
    "### Variation in Beak Shapes\n",
    "\n",
    "Are beak depth and beak length changing together? We can use linear regression to investigate this question. We will make use of the `draw_bs_pairs_linreg()` function we wrote earlier.\n",
    "\n",
    "#### EDA of Beak Length and Depth (Exercise)\n",
    "\n",
    "> In looking at the plot, we see that beaks got deeper (the red points are higher up in the y-direction), but not really longer. If anything, they got a bit shorter, since the red dots are to the left of the blue dots. So, it does not look like the beaks kept the same shape; they became shorter and deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4650f49-b78e-4ee1-b866-ae98618b843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make scatter plot of 1975 data\n",
    "_ = plt.plot(bl_1975, bd_1975, marker='.',\n",
    "             linestyle='None', color=\"blue\", alpha=0.5, label=\"1975\")\n",
    "\n",
    "# Make scatter plot of 2012 data\n",
    "_ = plt.plot(bl_2012, bd_2012, marker='.',\n",
    "            linestyle='None', color=\"red\", alpha=0.5, label=\"2012\")\n",
    "\n",
    "# Label axes and make legend\n",
    "_ = plt.xlabel('Beak length (mm)')\n",
    "_ = plt.ylabel('Beak depth (mm)')\n",
    "_ = plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c483170e-958d-44a3-9891-f1099d05f114",
   "metadata": {},
   "source": [
    "### Calculation of Heritability\n",
    "\n",
    "### Final Thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4224c92-c086-49e9-9236-12e23147bfb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
